{"title": "Comparative Statics of Trading Boundary in Finite Horizon Portfolio Selection with Proportional Transaction Costs", "abstract": "We consider the Merton's problem with proportional transaction costs. It is\nwell-known that the optimal investment strategy is characterized by two trading\nboundaries, i.e., the buy boundary and the sell boundary, between which is the\nno-trading region. We study how the two trading boundaries vary with\ntransaction costs. We reveal that the cost-adjusted trading boundaries are\nmonotone in transaction costs. Our result indicates that (i) the Merton line\nmust lie between two cost-adjusted trading boundaries; (ii) when the Merton\nline is positive, the buy boundary and the sell boundary are monotone in\ntransaction costs and the Merton line lies in the no-trading region as a\nresult.", "published": "2024-12-18 09:52:47", "link": "http://arxiv.org/abs/2412.13669v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Strictly monotone mean-variance preferences with dynamic portfolio management", "abstract": "This paper is devoted to extending the monotone mean-variance (MMV)\npreference to a large class of strictly monotone mean-variance (SMMV)\npreferences, and illustrating its application to single-period/continuous-time\nportfolio selection problems. The properties and equivalent representations of\nthe SMMV preference are also studied. To illustrate applications, we provide\nthe gradient condition for the single-period portfolio problem with SMMV\npreferences, and investigate its association with the optimal mean-variance\nstatic strategy. For the continuous-time portfolio problem with SMMV\npreferences and continuous price processes, we show the condition that the\nsolution is the same as the corresponding optimal mean-variance strategy. When\nthis consistency condition is not satisfied, the primal problems are unbounded,\nand we turn to study a sequence of approximate linear-quadratic problems\ngenerated by penalty function method. The solution can be characterized by\nstochastic Hamilton-Jacobi-Bellman-Isaacs equation, but it is still difficult\nto derive a closed-form expression. We take a joint adoption of embedding\nmethod and convex duality method to derive an analytical solution. In\nparticular, if the parameter that characterizes the strict monotonicity of SMMV\npreference is a constant, the solution can be given by two equations in the\nform of Black-Scholes formula.", "published": "2024-12-18 05:47:06", "link": "http://arxiv.org/abs/2412.13523v1", "categories": ["q-fin.MF", "Primary: 91G10, 49N10, Secondary: 91B05, 49N90"], "primary_category": "q-fin.MF"}
{"title": "On stochastic control problems with higher-order moments", "abstract": "In this paper, we focus on a class of time-inconsistent stochastic control\nproblems, where the objective function includes the mean and several\nhigher-order central moments of the terminal value of state. To tackle the\ntime-inconsistency, we seek both the closed-loop and the open-loop Nash\nequilibrium controls as time-consistent solutions. We establish a partial\ndifferential equation (PDE) system for deriving a closed-loop Nash equilibrium\ncontrol, which does not include the equilibrium value function and is different\nfrom the extended Hamilton-Jacobi-Bellman (HJB) equations as in Bj\\\"ork et al.\n(Finance Stoch. 21: 331-360, 2017). We show that our PDE system is equivalent\nto the extended HJB equations that seems difficult to be solved for our\nhigher-order moment problems. In deriving an open-loop Nash equilibrium\ncontrol, due to the non-separable higher-order moments in the objective\nfunction, we make some moment estimates in addition to the standard\nperturbation argument for developing a maximum principle. Then, the problem is\nreduced to solving a flow of forward-backward stochastic differential\nequations. In particular, we investigate linear controlled dynamics and some\nobjective functions affine in the mean. The closed-loop and the open-loop Nash\nequilibrium controls are identical, which are independent of the state value,\nrandom path and the preference on the odd-order central moments. By sending the\nhighest order of central moments to infinity, we obtain the time-consistent\nsolutions to some control problems whose objective functions include some\npenalty functions for deviation.", "published": "2024-12-18 05:46:13", "link": "http://arxiv.org/abs/2412.13521v2", "categories": ["q-fin.MF", "math.OC", "Primary: 93E20, 91G80, Secondary: 91B08, 49N90"], "primary_category": "q-fin.MF"}
{"title": "Multivariate Rough Volatility", "abstract": "Motivated by empirical evidence from the joint behavior of realized\nvolatility time series, we propose to model the joint dynamics of\nlog-volatilities using a multivariate fractional Ornstein-Uhlenbeck process.\nThis model is a multivariate version of the Rough Fractional Stochastic\nVolatility model proposed in Gatheral, Jaisson, and Rosenbaum, Quant. Finance,\n2018. It allows for different Hurst exponents in the different marginal\ncomponents and non trivial interdependencies.\n  We discuss the main features of the model and propose an estimator that\njointly identifies its parameters. We derive the asymptotic theory of the\nestimator and perform a simulation study that confirms the asymptotic theory in\nfinite sample.\n  We carry out an extensive empirical investigation on all realized volatility\ntime series covering the entire span of about two decades in the Oxford-Man\nrealized library. Our analysis shows that these time series are strongly\ncorrelated and can exhibit asymmetries in their cross-covariance structure,\naccurately captured by our model. These asymmetries lead to spillover effects\nthat we analyse theoretically within the model and then using our empirical\nestimates. Moreover, in accordance with the existing literature, we observe\nbehaviors close to non-stationarity and rough trajectories.", "published": "2024-12-18 21:40:36", "link": "http://arxiv.org/abs/2412.14353v1", "categories": ["q-fin.ST", "60G15, 62M09, 60G22"], "primary_category": "q-fin.ST"}
{"title": "Refining and Robust Backtesting of A Century of Profitable Industry Trends", "abstract": "We revisit the long-only trend-following strategy presented in A Century of\nProfitable Industry Trends by Zarattini and Antonacci, which achieved\nexceptional historical performance with an 18.2% annualized return and a Sharpe\nRatio of 1.39. While the results outperformed benchmarks, practical\nimplementation raises concerns about robustness and evolving market conditions.\nThis study explores modifications addressing reliance on T-bills, alternative\nfallback allocations, and industry exclusions. Despite attempts to enhance\nadaptability through momentum signals, parameter optimization, and Walk-Forward\nAnalysis, results reveal persistent challenges. The results highlight\nchallenges in adapting historical strategies to modern markets and offer\ninsights for future trend-following frameworks.", "published": "2024-12-18 21:57:08", "link": "http://arxiv.org/abs/2412.14361v2", "categories": ["q-fin.PM", "q-fin.TR"], "primary_category": "q-fin.PM"}
{"title": "Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom\n  Teaching Discourse", "abstract": "Human tutoring interventions play a crucial role in supporting student\nlearning, improving academic performance, and promoting personal growth. This\npaper focuses on analyzing mathematics tutoring discourse using talk moves - a\nframework of dialogue acts grounded in Accountable Talk theory. However,\nscaling the collection, annotation, and analysis of extensive tutoring\ndialogues to develop machine learning models is a challenging and\nresource-intensive task. To address this, we present SAGA22, a compact dataset,\nand explore various modeling strategies, including dialogue context, speaker\ninformation, pretraining datasets, and further fine-tuning. By leveraging\nexisting datasets and models designed for classroom teaching, our results\ndemonstrate that supplementary pretraining on classroom data enhances model\nperformance in tutoring settings, particularly when incorporating longer\ncontext and speaker information. Additionally, we conduct extensive ablation\nstudies to underscore the challenges in talk move modeling.", "published": "2024-12-18 00:13:04", "link": "http://arxiv.org/abs/2412.13395v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curriculum Learning for Cross-Lingual Data-to-Text Generation With Noisy\n  Data", "abstract": "Curriculum learning has been used to improve the quality of text generation\nsystems by ordering the training samples according to a particular schedule in\nvarious tasks. In the context of data-to-text generation (DTG), previous\nstudies used various difficulty criteria to order the training samples for\nmonolingual DTG. These criteria, however, do not generalize to the crosslingual\nvariant of the problem and do not account for noisy data. We explore multiple\ncriteria that can be used for improving the performance of cross-lingual DTG\nsystems with noisy data using two curriculum schedules. Using the alignment\nscore criterion for ordering samples and an annealing schedule to train the\nmodel, we show increase in BLEU score by up to 4 points, and improvements in\nfaithfulness and coverage of generations by 5-15% on average across 11 Indian\nlanguages and English in 2 separate datasets. We make code and data publicly\navailable", "published": "2024-12-18 04:00:18", "link": "http://arxiv.org/abs/2412.13484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CEHA: A Dataset of Conflict Events in the Horn of Africa", "abstract": "Natural Language Processing (NLP) of news articles can play an important role\nin understanding the dynamics and causes of violent conflict. Despite the\navailability of datasets categorizing various conflict events, the existing\nlabels often do not cover all of the fine-grained violent conflict event types\nrelevant to areas like the Horn of Africa. In this paper, we introduce a new\nbenchmark dataset Conflict Events in the Horn of Africa region (CEHA) and\npropose a new task for identifying violent conflict events using online\nresources with this dataset. The dataset consists of 500 English event\ndescriptions regarding conflict events in the Horn of Africa region with\nfine-grained event-type definitions that emphasize the cause of the conflict.\nThis dataset categorizes the key types of conflict risk according to specific\nareas required by stakeholders in the Humanitarian-Peace-Development Nexus.\nAdditionally, we conduct extensive experiments on two tasks supported by this\ndataset: Event-relevance Classification and Event-type Classification. Our\nbaseline models demonstrate the challenging nature of these tasks and the\nusefulness of our dataset for model evaluations in low-resource settings with\nlimited number of training data.", "published": "2024-12-18 05:22:33", "link": "http://arxiv.org/abs/2412.13511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MetaRuleGPT: Recursive Numerical Reasoning of Language Models Trained\n  with Simple Rules", "abstract": "Recent studies have highlighted the limitations of large language models in\nmathematical reasoning, particularly their inability to capture the underlying\nlogic. Inspired by meta-learning, we propose that models should acquire not\nonly task-specific knowledge but also transferable problem-solving skills. We\nintroduce MetaRuleGPT, a novel Transformer-based architecture that performs\nprecise numerical calculations and complex logical operations by learning and\ncombining different rules. In contrast with traditional training sets, which\nare heavily composed of massive raw instance data, MetaRuleGPT is pre-trained\non much less abstract datasets containing basic, compound, and iterative rules\nfor mathematical reasoning. Extensive experimental results demonstrate\nMetaRuleGPT can mimic human's rule-following capabilities, break down\ncomplexity, and iteratively derive accurate results for complex mathematical\nproblems. These findings prove the potential of rule learning to enhance the\nnumerical reasoning abilities of language models.", "published": "2024-12-18 06:27:10", "link": "http://arxiv.org/abs/2412.13536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Granularity Open Intent Classification via Adaptive Granular-Ball\n  Decision Boundary", "abstract": "Open intent classification is critical for the development of dialogue\nsystems, aiming to accurately classify known intents into their corresponding\nclasses while identifying unknown intents. Prior boundary-based methods assumed\nknown intents fit within compact spherical regions, focusing on coarse-grained\nrepresentation and precise spherical decision boundaries. However, these\nassumptions are often violated in practical scenarios, making it difficult to\ndistinguish known intent classes from unknowns using a single spherical\nboundary. To tackle these issues, we propose a Multi-granularity Open intent\nclassification method via adaptive Granular-Ball decision boundary (MOGB). Our\nMOGB method consists of two modules: representation learning and decision\nboundary acquiring. To effectively represent the intent distribution, we design\na hierarchical representation learning method. This involves iteratively\nalternating between adaptive granular-ball clustering and nearest sub-centroid\nclassification to capture fine-grained semantic structures within known intent\nclasses. Furthermore, multi-granularity decision boundaries are constructed for\nopen intent classification by employing granular-balls with varying centroids\nand radii. Extensive experiments conducted on three public datasets demonstrate\nthe effectiveness of our proposed method.", "published": "2024-12-18 06:42:19", "link": "http://arxiv.org/abs/2412.13542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Long-form Story Using Dynamic Hierarchical Outlining with\n  Memory-Enhancement", "abstract": "Long-form story generation task aims to produce coherent and sufficiently\nlengthy text, essential for applications such as novel writingand interactive\nstorytelling. However, existing methods, including LLMs, rely on rigid outlines\nor lack macro-level planning, making it difficult to achieve both contextual\nconsistency and coherent plot development in long-form story generation. To\naddress this issues, we propose Dynamic Hierarchical Outlining with\nMemory-Enhancement long-form story generation method, named DOME, to generate\nthe long-form story with coherent content and plot. Specifically, the Dynamic\nHierarchical Outline(DHO) mechanism incorporates the novel writing theory into\noutline planning and fuses the plan and writing stages together, improving the\ncoherence of the plot by ensuring the plot completeness and adapting to the\nuncertainty during story generation. A Memory-Enhancement Module (MEM) based on\ntemporal knowledge graphs is introduced to store and access the generated\ncontent, reducing contextual conflicts and improving story coherence. Finally,\nwe propose a Temporal Conflict Analyzer leveraging temporal knowledge graphs to\nautomatically evaluate the contextual consistency of long-form story.\nExperiments demonstrate that DOME significantly improves the fluency,\ncoherence, and overall quality of generated long stories compared to\nstate-of-the-art methods.", "published": "2024-12-18 07:50:54", "link": "http://arxiv.org/abs/2412.13575v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EvoWiki: Evaluating LLMs on Evolving Knowledge", "abstract": "Knowledge utilization is a critical aspect of LLMs, and understanding how\nthey adapt to evolving knowledge is essential for their effective deployment.\nHowever, existing benchmarks are predominantly static, failing to capture the\nevolving nature of LLMs and knowledge, leading to inaccuracies and\nvulnerabilities such as contamination. In this paper, we introduce EvoWiki, an\nevolving dataset designed to reflect knowledge evolution by categorizing\ninformation into stable, evolved, and uncharted states. EvoWiki is fully\nauto-updatable, enabling precise evaluation of continuously changing knowledge\nand newly released LLMs. Through experiments with Retrieval-Augmented\nGeneration (RAG) and Contunual Learning (CL), we evaluate how effectively LLMs\nadapt to evolving knowledge. Our results indicate that current models often\nstruggle with evolved knowledge, frequently providing outdated or incorrect\nresponses. Moreover, the dataset highlights a synergistic effect between RAG\nand CL, demonstrating their potential to better adapt to evolving knowledge.\nEvoWiki provides a robust benchmark for advancing future research on the\nknowledge evolution capabilities of large language models.", "published": "2024-12-18 08:04:57", "link": "http://arxiv.org/abs/2412.13582v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Outcomes: Transparent Assessment of LLM Reasoning in Games", "abstract": "Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: \\url{https://visual-ai.github.io/gamebot}", "published": "2024-12-18 08:32:53", "link": "http://arxiv.org/abs/2412.13602v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation", "abstract": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.", "published": "2024-12-18 09:27:33", "link": "http://arxiv.org/abs/2412.13649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PsyDT: Using LLMs to Construct the Digital Twin of Psychological\n  Counselor with Personalized Counseling Style for Psychological Counseling", "abstract": "Currently, large language models (LLMs) have made significant progress in the\nfield of psychological counseling. However, existing mental health LLMs\noverlook a critical issue where they do not consider the fact that different\npsychological counselors exhibit different personal styles, including\nlinguistic style and therapy techniques, etc. As a result, these LLMs fail to\nsatisfy the individual needs of clients who seek different counseling styles.\nTo help bridge this gap, we propose PsyDT, a novel framework using LLMs to\nconstruct the Digital Twin of Psychological counselor with personalized\ncounseling style. Compared to the time-consuming and costly approach of\ncollecting a large number of real-world counseling cases to create a specific\ncounselor's digital twin, our framework offers a faster and more cost-effective\nsolution. To construct PsyDT, we utilize dynamic one-shot learning by using\nGPT-4 to capture counselor's unique counseling style, mainly focusing on\nlinguistic style and therapy techniques. Subsequently, using existing\nsingle-turn long-text dialogues with client's questions, GPT-4 is guided to\nsynthesize multi-turn dialogues of specific counselor. Finally, we fine-tune\nthe LLMs on the synthetic dataset, PsyDTCorpus, to achieve the digital twin of\npsychological counselor with personalized counseling style. Experimental\nresults indicate that our proposed PsyDT framework can synthesize multi-turn\ndialogues that closely resemble real-world counseling cases and demonstrate\nbetter performance compared to other baselines, thereby show that our framework\ncan effectively construct the digital twin of psychological counselor with a\nspecific counseling style.", "published": "2024-12-18 09:38:43", "link": "http://arxiv.org/abs/2412.13660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Efficient and Explainable Hate Speech Detection via Model\n  Distillation", "abstract": "Automatic detection of hate and abusive language is essential to combat its\nonline spread. Moreover, recognising and explaining hate speech serves to\neducate people about its negative effects. However, most current detection\nmodels operate as black boxes, lacking interpretability and explainability. In\nthis context, Large Language Models (LLMs) have proven effective for hate\nspeech detection and to promote interpretability. Nevertheless, they are\ncomputationally costly to run. In this work, we propose distilling big language\nmodels by using Chain-of-Thought to extract explanations that support the hate\nspeech classification task. Having small language models for these tasks will\ncontribute to their use in operational settings. In this paper, we demonstrate\nthat distilled models deliver explanations of the same quality as larger models\nwhile surpassing them in classification performance. This dual capability,\nclassifying and explaining, advances hate speech detection making it more\naffordable, understandable and actionable.", "published": "2024-12-18 10:42:53", "link": "http://arxiv.org/abs/2412.13698v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Complex Word Embeddings in Classical and Quantum Spaces", "abstract": "We present a variety of methods for training complex-valued word embeddings,\nbased on the classical Skip-gram model, with a straightforward adaptation\nsimply replacing the real-valued vectors with arbitrary vectors of complex\nnumbers. In a more \"physically-inspired\" approach, the vectors are produced by\nparameterised quantum circuits (PQCs), which are unitary transformations\nresulting in normalised vectors which have a probabilistic interpretation. We\ndevelop a complex-valued version of the highly optimised C code version of\nSkip-gram, which allows us to easily produce complex embeddings trained on a\n3.8B-word corpus for a vocabulary size of over 400k, for which we are then able\nto train a separate PQC for each word. We evaluate the complex embeddings on a\nset of standard similarity and relatedness datasets, for some models obtaining\nresults competitive with the classical baseline. We find that, while training\nthe PQCs directly tends to harm performance, the quantum word embeddings from\nthe two-stage process perform as well as the classical Skip-gram embeddings\nwith comparable numbers of parameters. This enables a highly scalable route to\nlearning embeddings in complex spaces which scales with the size of the\nvocabulary rather than the size of the training corpus. In summary, we\ndemonstrate how to produce a large set of high-quality word embeddings for use\nin complex-valued and quantum-inspired NLP models, and for exploring potential\nadvantage in quantum NLP models.", "published": "2024-12-18 11:26:51", "link": "http://arxiv.org/abs/2412.13745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Editing with Dynamic Knowledge Graphs for Multi-Hop Question\n  Answering", "abstract": "Multi-hop question answering (MHQA) poses a significant challenge for large\nlanguage models (LLMs) due to the extensive knowledge demands involved.\nKnowledge editing, which aims to precisely modify the LLMs to incorporate\nspecific knowledge without negatively impacting other unrelated knowledge,\noffers a potential solution for addressing MHQA challenges with LLMs. However,\ncurrent solutions struggle to effectively resolve issues of knowledge\nconflicts. Most parameter-preserving editing methods are hindered by inaccurate\nretrieval and overlook secondary editing issues, which can introduce noise into\nthe reasoning process of LLMs. In this paper, we introduce KEDKG, a novel\nknowledge editing method that leverages a dynamic knowledge graph for MHQA,\ndesigned to ensure the reliability of answers. KEDKG involves two primary\nsteps: dynamic knowledge graph construction and knowledge graph augmented\ngeneration. Initially, KEDKG autonomously constructs a dynamic knowledge graph\nto store revised information while resolving potential knowledge conflicts.\nSubsequently, it employs a fine-grained retrieval strategy coupled with an\nentity and relation detector to enhance the accuracy of graph retrieval for LLM\ngeneration. Experimental results on benchmarks show that KEDKG surpasses\nprevious state-of-the-art models, delivering more accurate and reliable answers\nin environments with dynamic information.", "published": "2024-12-18 12:21:46", "link": "http://arxiv.org/abs/2412.13782v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Universal Arabic ASR Leaderboard", "abstract": "In recent years, the enhanced capabilities of ASR models and the emergence of\nmulti-dialect datasets have increasingly pushed Arabic ASR model development\ntoward an all-dialect-in-one direction. This trend highlights the need for\nbenchmarking studies that evaluate model performance on multiple dialects,\nproviding the community with insights into models' generalization capabilities.\n  In this paper, we introduce Open Universal Arabic ASR Leaderboard, a\ncontinuous benchmark project for open-source general Arabic ASR models across\nvarious multi-dialect datasets. We also provide a comprehensive analysis of the\nmodel's robustness, speaker adaptation, inference efficiency, and memory\nconsumption. This work aims to offer the Arabic ASR community a reference for\nmodels' general performance and also establish a common evaluation framework\nfor multi-dialectal Arabic ASR models.", "published": "2024-12-18 12:31:31", "link": "http://arxiv.org/abs/2412.13788v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Physics Reasoner: Knowledge-Augmented Reasoning for Solving Physics\n  Problems with Large Language Models", "abstract": "Physics problems constitute a significant aspect of reasoning, necessitating\ncomplicated reasoning ability and abundant physics knowledge. However, existing\nlarge language models (LLMs) frequently fail due to a lack of knowledge or\nincorrect knowledge application. To mitigate these issues, we propose Physics\nReasoner, a knowledge-augmented framework to solve physics problems with LLMs.\nSpecifically, the proposed framework constructs a comprehensive formula set to\nprovide explicit physics knowledge and utilizes checklists containing detailed\ninstructions to guide effective knowledge application. Namely, given a physics\nproblem, Physics Reasoner solves it through three stages: problem analysis,\nformula retrieval, and guided reasoning. During the process, checklists are\nemployed to enhance LLMs' self-improvement in the analysis and reasoning\nstages. Empirically, Physics Reasoner mitigates the issues of insufficient\nknowledge and incorrect application, achieving state-of-the-art performance on\nSciBench with an average accuracy improvement of 5.8%.", "published": "2024-12-18 12:33:50", "link": "http://arxiv.org/abs/2412.13791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RACQUET: Unveiling the Dangers of Overlooked Referential Ambiguity in\n  Visual LLMs", "abstract": "Ambiguity resolution is key to effective communication. While humans\neffortlessly address ambiguity through conversational grounding strategies, the\nextent to which current language models can emulate these strategies remains\nunclear. In this work, we examine referential ambiguity in image-based question\nanswering by introducing RACQUET, a carefully curated dataset targeting\ndistinct aspects of ambiguity. Through a series of evaluations, we reveal\nsignificant limitations and problems of overconfidence of state-of-the-art\nlarge multimodal language models in addressing ambiguity in their responses.\nThe overconfidence issue becomes particularly relevant for RACQUET-BIAS, a\nsubset designed to analyze a critical yet underexplored problem: failing to\naddress ambiguity leads to stereotypical, socially biased responses. Our\nresults underscore the urgency of equipping models with robust strategies to\ndeal with uncertainty without resorting to undesirable stereotypes.", "published": "2024-12-18 13:25:11", "link": "http://arxiv.org/abs/2412.13835v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies\n  for Human Explanations to Collect Label Distributions on NLI", "abstract": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distribution. We further compare the resulting human\nwith model-generated explanations, and test automatic and human explanation\nselection. Our experiments show that LLM explanations are promising for NLI: to\nestimate HJD, generated explanations yield comparable results to human's when\nprovided with human labels. Importantly, our results generalize from datasets\nwith human explanations to i) datasets where they are not available and ii)\nchallenging out-of-distribution test sets.", "published": "2024-12-18 15:24:50", "link": "http://arxiv.org/abs/2412.13942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What makes a good metric? Evaluating automatic metrics for text-to-image\n  consistency", "abstract": "Language models are increasingly being incorporated as components in larger\nAI systems for various purposes, from prompt optimization to automatic\nevaluation. In this work, we analyze the construct validity of four recent,\ncommonly used methods for measuring text-to-image consistency - CLIPScore,\nTIFA, VPEval, and DSG - which rely on language models and/or VQA models as\ncomponents. We define construct validity for text-image consistency metrics as\na set of desiderata that text-image consistency metrics should have, and find\nthat no tested metric satisfies all of them. We find that metrics lack\nsufficient sensitivity to language and visual properties. Next, we find that\nTIFA, VPEval and DSG contribute novel information above and beyond CLIPScore,\nbut also that they correlate highly with each other. We also ablate different\naspects of the text-image consistency metrics and find that not all model\ncomponents are strictly necessary, also a symptom of insufficient sensitivity\nto visual information. Finally, we show that all three VQA-based metrics likely\nrely on familiar text shortcuts (such as yes-bias in QA) that call their\naptitude as quantitative evaluations of model performance into question.", "published": "2024-12-18 16:09:42", "link": "http://arxiv.org/abs/2412.13989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FarExStance: Explainable Stance Detection for Farsi", "abstract": "We introduce FarExStance, a new dataset for explainable stance detection in\nFarsi. Each instance in this dataset contains a claim, the stance of an article\nor social media post towards that claim, and an extractive explanation which\nprovides evidence for the stance label. We compare the performance of a\nfine-tuned multilingual RoBERTa model to several large language models in\nzero-shot, few-shot, and parameter-efficient fine-tuned settings on our new\ndataset. On stance detection, the most accurate models are the fine-tuned\nRoBERTa model, the LLM Aya-23-8B which has been fine-tuned using\nparameter-efficient fine-tuning, and few-shot Claude-3.5-Sonnet. Regarding the\nquality of the explanations, our automatic evaluation metrics indicate that\nfew-shot GPT-4o generates the most coherent explanations, while our human\nevaluation reveals that the best Overall Explanation Score (OES) belongs to\nfew-shot Claude-3.5-Sonnet. The fine-tuned Aya-32-8B model produced\nexplanations most closely aligned with the reference explanations.", "published": "2024-12-18 16:24:20", "link": "http://arxiv.org/abs/2412.14008v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards an optimised evaluation of teachers' discourse: The case of\n  engaging messages", "abstract": "Evaluating teachers' skills is crucial for enhancing education quality and\nstudent outcomes. Teacher discourse, significantly influencing student\nperformance, is a key component. However, coding this discourse can be\nlaborious. This study addresses this issue by introducing a new methodology for\noptimising the assessment of teacher discourse. The research consisted of two\nstudies, both within the framework of engaging messages used by secondary\neducation teachers. The first study involved training two large language models\non real-world examples from audio-recorded lessons over two academic years to\nidentify and classify the engaging messages from the lessons' transcripts. This\nresulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and\n86.36% in identification and classification, respectively. The second study\napplied these models to transcripts of audio-recorded lessons from a third\nacademic year to examine the frequency and distribution of message types by\neducational level and moment of the academic year. Results showed teachers\npredominantly use messages emphasising engagement benefits, linked to improved\noutcomes, while one-third highlighted non-engagement disadvantages, associated\nwith increased anxiety. The use of engaging messages declined in Grade 12 and\ntowards the academic year's end. These findings suggest potential interventions\nto optimise engaging message use, enhancing teaching quality and student\noutcomes.", "published": "2024-12-18 16:29:45", "link": "http://arxiv.org/abs/2412.14011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation", "abstract": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.", "published": "2024-12-18 17:05:08", "link": "http://arxiv.org/abs/2412.14050v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance Gap in Entity Knowledge Extraction Across Modalities in\n  Vision Language Models", "abstract": "Vision-language models (VLMs) excel at extracting and reasoning about\ninformation from images. Yet, their capacity to leverage internal knowledge\nabout specific entities remains underexplored. This work investigates the\ndisparity in model performance when answering factual questions about an entity\ndescribed in text versus depicted in an image. Our results reveal a significant\naccuracy drop --averaging 19%-- when the entity is presented visually instead\nof textually. We hypothesize that this decline arises from limitations in how\ninformation flows from image tokens to query tokens. We use mechanistic\ninterpretability tools to reveal that, although image tokens are preprocessed\nby the vision encoder, meaningful information flow from these tokens occurs\nonly in the much deeper layers. Furthermore, critical image processing happens\nin the language model's middle layers, allowing few layers for consecutive\nreasoning, highlighting a potential inefficiency in how the model utilizes its\nlayers for reasoning. These insights shed light on the internal mechanics of\nVLMs and offer pathways for enhancing their reasoning capabilities.", "published": "2024-12-18 18:22:30", "link": "http://arxiv.org/abs/2412.14133v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks", "abstract": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at helping to accelerate or even autonomously perform\nwork-related tasks? The answer to this question has important implications for\nboth industry looking to adopt AI into their workflows, and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper, we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that with the most\ncompetitive agent, 24% of the tasks can be completed autonomously. This paints\na nuanced picture on task automation with LM agents -- in a setting simulating\na real workplace, a good portion of simpler tasks could be solved autonomously,\nbut more difficult long-horizon tasks are still beyond the reach of current\nsystems.", "published": "2024-12-18 18:55:40", "link": "http://arxiv.org/abs/2412.14161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on LLM Inference-Time Self-Improvement", "abstract": "Techniques that enhance inference through increased computation at test-time\nhave recently gained attention. In this survey, we investigate the current\nstate of LLM Inference-Time Self-Improvement from three different perspectives:\nIndependent Self-improvement, focusing on enhancements via decoding or sampling\nmethods; Context-Aware Self-Improvement, leveraging additional context or\ndatastore; and Model-Aided Self-Improvement, achieving improvement through\nmodel collaboration. We provide a comprehensive review of recent relevant\nstudies, contribute an in-depth taxonomy, and discuss challenges and\nlimitations, offering insights for future research.", "published": "2024-12-18 21:37:07", "link": "http://arxiv.org/abs/2412.14352v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation", "abstract": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding.", "published": "2024-12-18 22:04:56", "link": "http://arxiv.org/abs/2412.14368v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Knowledge Distillation for LLMs with Response-Priming\n  Prompting", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across\na wide range of natural language processing (NLP) tasks. However, these models\nare often difficult to deploy due to significant computational requirements and\nresource constraints. Knowledge distillation (KD) is an effective technique for\ntransferring the performance of larger LLMs to smaller models. Traditional KD\nmethods primarily focus on the direct output of the teacher model, with little\nemphasis on the role of prompting during knowledge transfer. In this paper, we\npropose a set of novel response-priming prompting strategies applied in the\nknowledge distillation pipeline to enhance the performance of student models.\nOur approach fine-tunes a smaller Llama 3.1 8B Instruct model by distilling\nknowledge from a quantized Llama 3.1 405B Instruct teacher model. We apply LoRA\noptimization and evaluate on the GSM8K benchmark. Experimental results\ndemonstrate that integrating reasoning-eliciting prompting into the proposed KD\npipeline significantly improves student model performance, offering an\nefficient way to deploy powerful models in resource-constrained environments.\nWe find that Ground Truth prompting results in a 55\\% performance increase on\nGSM8K for a distilled Llama 3.1 8B Instruct compared to the same model\ndistilled without prompting. A thorough investigation into the self-attention\nlayers of the student models indicates that the more successful prompted models\ntend to exhibit certain positive behaviors inside their attention heads which\ncan be tied to their increased accuracy. Our implementation can be found at\nhttps://github.com/alonso130r/knowledge-distillation.", "published": "2024-12-18 20:41:44", "link": "http://arxiv.org/abs/2412.17846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenX: Mastering Code and Test Generation with Execution Feedback", "abstract": "Recent advancements in language modeling have enabled the translation of\nnatural language into code, and the use of execution feedback to improve code\ngeneration. However, these methods often rely heavily on pre-existing test\ncases, which may not always be available or comprehensive. In this work, we\npropose a novel approach that concurrently trains a code generation model and a\ntest generation model, utilizing execution feedback to refine and enhance the\nperformance of both. We introduce two strategies for test and code data\naugmentation and a new scoring function for code and test ranking. We\nexperiment on the APPS dataset and demonstrate that our approach can\neffectively generate and augment test cases, filter and synthesize correct code\nsolutions, and rank the quality of generated code and tests. The results\ndemonstrate that our models, when iteratively trained with an increasing number\nof test cases and code solutions, outperform those trained on the original\ndataset.", "published": "2024-12-18 03:18:21", "link": "http://arxiv.org/abs/2412.13464v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Gradual Vigilance and Interval Communication: Enhancing Value Alignment\n  in Multi-Agent Debates", "abstract": "In recent years, large language models have shown exceptional performance in\nfulfilling diverse human needs. However, their training data can introduce\nharmful content, underscoring the necessity for robust value alignment.\nMainstream methods, which depend on feedback learning and supervised training,\nare resource-intensive and may constrain the full potential of the models.\nMulti-Agent Debate (MAD) offers a more efficient and innovative solution by\nenabling the generation of reliable answers through agent interactions. To\napply MAD to value alignment, we examine the relationship between the\nhelpfulness and harmlessness of debate outcomes and individual responses, and\npropose a MAD based framework Gradual Vigilance and Interval Communication\n(GVIC). GVIC allows agents to assess risks with varying levels of vigilance and\nto exchange diverse information through interval communication. We\ntheoretically prove that GVIC optimizes debate efficiency while reducing\ncommunication overhead. Experimental results demonstrate that GVIC consistently\noutperforms baseline methods across various tasks and datasets, particularly\nexcelling in harmfulness mitigation and fraud prevention. Additionally, GVIC\nexhibits strong adaptability across different base model sizes, including both\nunaligned and aligned models, and across various task types.", "published": "2024-12-18 03:36:08", "link": "http://arxiv.org/abs/2412.13471v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Statistical and Multi-Perspective Revisiting of the Membership\n  Inference Attack in Large Language Models", "abstract": "The lack of data transparency in Large Language Models (LLMs) has highlighted\nthe importance of Membership Inference Attack (MIA), which differentiates\ntrained (member) and untrained (non-member) data. Though it shows success in\nprevious studies, recent research reported a near-random performance in\ndifferent settings, highlighting a significant performance inconsistency. We\nassume that a single setting doesn't represent the distribution of the vast\ncorpora, causing members and non-members with different distributions to be\nsampled and causing inconsistency. In this study, instead of a single setting,\nwe statistically revisit MIA methods from various settings with thousands of\nexperiments for each MIA method, along with study in text feature, embedding,\nthreshold decision, and decoding dynamics of members and non-members. We found\nthat (1) MIA performance improves with model size and varies with domains,\nwhile most methods do not statistically outperform baselines, (2) Though MIA\nperformance is generally low, a notable amount of differentiable member and\nnon-member outliers exists and vary across MIA methods, (3) Deciding a\nthreshold to separate members and non-members is an overlooked challenge, (4)\nText dissimilarity and long text benefit MIA performance, (5) Differentiable or\nnot is reflected in the LLM embedding, (6) Member and non-members show\ndifferent decoding dynamics.", "published": "2024-12-18 03:39:42", "link": "http://arxiv.org/abs/2412.13475v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Refining Salience-Aware Sparse Fine-Tuning Strategies for Language\n  Models", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) has gained prominence through low-rank\nadaptation methods like LoRA. In this paper, we focus on sparsity-based PEFT\n(SPEFT), which introduces trainable sparse adaptations to the weight matrices\nin the model, offering greater flexibility in selecting fine-tuned parameters\ncompared to low-rank methods. We conduct the first systematic evaluation of\nsalience metrics for SPEFT, inspired by zero-cost NAS proxies, and identify\nsimple gradient-based metrics is reliable, and results are on par with the best\nalternatives, offering both computational efficiency and robust performance.\nAdditionally, we compare static and dynamic masking strategies, finding that\nstatic masking, which predetermines non-zero entries before training, delivers\nefficiency without sacrificing performance, while dynamic masking offers no\nsubstantial benefits. Across NLP tasks, a simple gradient-based, static SPEFT\nconsistently outperforms other fine-tuning methods for LLMs, providing a simple\nyet effective baseline for SPEFT. Our work challenges the notion that\ncomplexity is necessary for effective PEFT. Our work is open source and\navailable to the community at [https://github.com/0-ml/speft].", "published": "2024-12-18 04:14:35", "link": "http://arxiv.org/abs/2412.13488v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VaeDiff-DocRE: End-to-end Data Augmentation Framework for Document-level\n  Relation Extraction", "abstract": "Document-level Relation Extraction (DocRE) aims to identify relationships\nbetween entity pairs within a document. However, most existing methods assume a\nuniform label distribution, resulting in suboptimal performance on real-world,\nimbalanced datasets. To tackle this challenge, we propose a novel data\naugmentation approach using generative models to enhance data from the\nembedding space. Our method leverages the Variational Autoencoder (VAE)\narchitecture to capture all relation-wise distributions formed by entity pair\nrepresentations and augment data for underrepresented relations. To better\ncapture the multi-label nature of DocRE, we parameterize the VAE's latent space\nwith a Diffusion Model. Additionally, we introduce a hierarchical training\nframework to integrate the proposed VAE-based augmentation module into DocRE\nsystems. Experiments on two benchmark datasets demonstrate that our method\noutperforms state-of-the-art models, effectively addressing the long-tail\ndistribution problem in DocRE.", "published": "2024-12-18 04:55:29", "link": "http://arxiv.org/abs/2412.13503v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Adapter with Semantics Disentangling for Cross-lingual\n  Cross-modal Retrieval", "abstract": "Existing cross-modal retrieval methods typically rely on large-scale\nvision-language pair data. This makes it challenging to efficiently develop a\ncross-modal retrieval model for under-resourced languages of interest.\nTherefore, Cross-lingual Cross-modal Retrieval (CCR), which aims to align\nvision and the low-resource language (the target language) without using any\nhuman-labeled target-language data, has gained increasing attention. As a\ngeneral parameter-efficient way, a common solution is to utilize adapter\nmodules to transfer the vision-language alignment ability of Vision-Language\nPretraining (VLP) models from a source language to a target language. However,\nthese adapters are usually static once learned, making it difficult to adapt to\ntarget-language captions with varied expressions. To alleviate it, we propose\nDynamic Adapter with Semantics Disentangling (DASD), whose parameters are\ndynamically generated conditioned on the characteristics of the input captions.\nConsidering that the semantics and expression styles of the input caption\nlargely influence how to encode it, we propose a semantic disentangling module\nto extract the semantic-related and semantic-agnostic features from the input,\nensuring that generated adapters are well-suited to the characteristics of\ninput caption. Extensive experiments on two image-text datasets and one\nvideo-text dataset demonstrate the effectiveness of our model for cross-lingual\ncross-modal retrieval, as well as its good compatibility with various VLP\nmodels.", "published": "2024-12-18 05:19:09", "link": "http://arxiv.org/abs/2412.13510v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Benchmarking and Improving Large Vision-Language Models for Fundamental\n  Visual Graph Understanding and Reasoning", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross diverse tasks. Despite great success, recent studies show that LVLMs\nencounter substantial limitations when engaging with visual graphs. To study\nthe reason behind these limitations, we propose VGCure, a comprehensive\nbenchmark covering 22 tasks for examining the fundamental graph understanding\nand reasoning capacities of LVLMs. Extensive evaluations conducted on 14 LVLMs\nreveal that LVLMs are weak in basic graph understanding and reasoning tasks,\nparticularly those concerning relational or structurally complex information.\nBased on this observation, we propose a structure-aware fine-tuning framework\nto enhance LVLMs with structure learning abilities through three\nself-supervised learning tasks. Experiments validate the effectiveness of our\nmethod in improving LVLMs' performance on fundamental and downstream graph\nlearning tasks, as well as enhancing their robustness against complex visual\ngraphs.", "published": "2024-12-18 06:35:18", "link": "http://arxiv.org/abs/2412.13540v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Socio-Culturally Aware Evaluation Framework for LLM-Based Content\n  Moderation", "abstract": "With the growth of social media and large language models, content moderation\nhas become crucial. Many existing datasets lack adequate representation of\ndifferent groups, resulting in unreliable assessments. To tackle this, we\npropose a socio-culturally aware evaluation framework for LLM-driven content\nmoderation and introduce a scalable method for creating diverse datasets using\npersona-based generation. Our analysis reveals that these datasets provide\nbroader perspectives and pose greater challenges for LLMs than\ndiversity-focused generation methods without personas. This challenge is\nespecially pronounced in smaller LLMs, emphasizing the difficulties they\nencounter in moderating such diverse content.", "published": "2024-12-18 07:57:18", "link": "http://arxiv.org/abs/2412.13578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unlocking the Potential of Weakly Labeled Data: A Co-Evolutionary\n  Learning Framework for Abnormality Detection and Report Generation", "abstract": "Anatomical abnormality detection and report generation of chest X-ray (CXR)\nare two essential tasks in clinical practice. The former aims at localizing and\ncharacterizing cardiopulmonary radiological findings in CXRs, while the latter\nsummarizes the findings in a detailed report for further diagnosis and\ntreatment. Existing methods often focused on either task separately, ignoring\ntheir correlation. This work proposes a co-evolutionary abnormality detection\nand report generation (CoE-DG) framework. The framework utilizes both fully\nlabeled (with bounding box annotations and clinical reports) and weakly labeled\n(with reports only) data to achieve mutual promotion between the abnormality\ndetection and report generation tasks. Specifically, we introduce a\nbi-directional information interaction strategy with generator-guided\ninformation propagation (GIP) and detector-guided information propagation\n(DIP). For semi-supervised abnormality detection, GIP takes the informative\nfeature extracted by the generator as an auxiliary input to the detector and\nuses the generator's prediction to refine the detector's pseudo labels. We\nfurther propose an intra-image-modal self-adaptive non-maximum suppression\nmodule (SA-NMS). This module dynamically rectifies pseudo detection labels\ngenerated by the teacher detection model with high-confidence predictions by\nthe student.Inversely, for report generation, DIP takes the abnormalities'\ncategories and locations predicted by the detector as input and guidance for\nthe generator to improve the generated reports.", "published": "2024-12-18 08:31:26", "link": "http://arxiv.org/abs/2412.13599v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Are LLMs Good Literature Review Writers? Evaluating the Literature\n  Review Writing Ability of Large Language Models", "abstract": "The literature review is a crucial form of academic writing that involves\ncomplex processes of literature collection, organization, and summarization.\nThe emergence of large language models (LLMs) has introduced promising tools to\nautomate these processes. However, their actual capabilities in writing\ncomprehensive literature reviews remain underexplored, such as whether they can\ngenerate accurate and reliable references. To address this gap, we propose a\nframework to assess the literature review writing ability of LLMs\nautomatically. We evaluate the performance of LLMs across three tasks:\ngenerating references, writing abstracts, and writing literature reviews. We\nemploy external tools for a multidimensional evaluation, which includes\nassessing hallucination rates in references, semantic coverage, and factual\nconsistency with human-written context. By analyzing the experimental results,\nwe find that, despite advancements, even the most sophisticated models still\ncannot avoid generating hallucinated references. Additionally, different models\nexhibit varying performance in literature review writing across different\ndisciplines.", "published": "2024-12-18 08:42:25", "link": "http://arxiv.org/abs/2412.13612v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIFT: Improving Long Context Understanding Through Long Input\n  Fine-Tuning", "abstract": "Long context understanding remains challenging for large language models due\nto their limited context windows. This paper introduces Long Input Fine-Tuning\n(LIFT) for long context modeling, a novel framework that enhances LLM\nperformance on long-context tasks by adapting model parameters to the context\nat test time. LIFT enables efficient processing of lengthy inputs without the\ncomputational burden of offline long-context adaptation, and can improve the\nlong-context capabilities of arbitrary short-context models. The framework is\nfurther enhanced by integrating in-context learning and pre-LIFT supervised\nfine-tuning. The combination of in-context learning and LIFT enables\nshort-context models like Llama 3 to handle arbitrarily long contexts and\nconsistently improves their performance on popular long-context benchmarks like\nLooGLE and LongBench. We also provide a comprehensive analysis of the strengths\nand limitations of LIFT on long context understanding, offering valuable\ndirections for future research.", "published": "2024-12-18 09:04:55", "link": "http://arxiv.org/abs/2412.13626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning", "abstract": "Theory of Mind (ToM) capabilities in LLMs have recently become a central\nobject of investigation. Cognitive science distinguishes between two steps\nrequired for ToM tasks: 1) determine whether to invoke ToM, which includes the\nappropriate Depth of Mentalizing (DoM), or level of recursion required to\ncomplete a task; and 2) applying the correct inference given the DoM. In this\nposition paper, we first identify several lines of work in different\ncommunities in AI, including LLM benchmarking, ToM add-ons, ToM probing, and\nformal models for ToM. We argue that recent work in AI tends to focus\nexclusively on the second step which are typically framed as static logic\nproblems. We conclude with suggestions for improved evaluation of ToM\ncapabilities inspired by dynamic environments used in cognitive tasks.", "published": "2024-12-18 09:06:48", "link": "http://arxiv.org/abs/2412.13631v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "On the Role of Model Prior in Real-World Inductive Reasoning", "abstract": "Large Language Models (LLMs) show impressive inductive reasoning\ncapabilities, enabling them to generate hypotheses that could generalize\neffectively to new instances when guided by in-context demonstrations. However,\nin real-world applications, LLMs' hypothesis generation is not solely\ndetermined by these demonstrations but is significantly shaped by task-specific\nmodel priors. Despite their critical influence, the distinct contributions of\nmodel priors versus demonstrations to hypothesis generation have been\nunderexplored. This study bridges this gap by systematically evaluating three\ninductive reasoning strategies across five real-world tasks with three LLMs.\nOur empirical findings reveal that, hypothesis generation is primarily driven\nby the model's inherent priors; removing demonstrations results in minimal loss\nof hypothesis quality and downstream usage. Further analysis shows the result\nis consistent across various label formats with different label configurations,\nand prior is hard to override, even under flipped labeling. These insights\nadvance our understanding of the dynamics of hypothesis generation in LLMs and\nhighlight the potential for better utilizing model priors in real-world\ninductive reasoning tasks.", "published": "2024-12-18 09:22:08", "link": "http://arxiv.org/abs/2412.13645v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\n  Fast, Memory Efficient, and Long Context Finetuning and Inference", "abstract": "Encoder-only transformer models such as BERT offer a great performance-size\ntradeoff for retrieval and classification tasks with respect to larger\ndecoder-only models. Despite being the workhorse of numerous production\npipelines, there have been limited Pareto improvements to BERT since its\nrelease. In this paper, we introduce ModernBERT, bringing modern model\noptimizations to encoder-only models and representing a major Pareto\nimprovement over older encoders. Trained on 2 trillion tokens with a native\n8192 sequence length, ModernBERT models exhibit state-of-the-art results on a\nlarge pool of evaluations encompassing diverse classification tasks and both\nsingle and multi-vector retrieval on different domains (including code). In\naddition to strong downstream performance, ModernBERT is also the most speed\nand memory efficient encoder and is designed for inference on common GPUs.", "published": "2024-12-18 09:39:44", "link": "http://arxiv.org/abs/2412.13663v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AntiLeak-Bench: Preventing Data Contamination by Automatically\n  Constructing Benchmarks with Updated Real-World Knowledge", "abstract": "Data contamination hinders fair LLM evaluation by introducing test data into\nnewer models' training sets. Existing studies solve this challenge by updating\nbenchmarks with newly collected data. However, they fail to guarantee\ncontamination-free evaluation as the newly collected data may contain\npre-existing knowledge, and their benchmark updates rely on intensive human\nlabor. To address these issues, we in this paper propose AntiLeak-Bench, an\nautomated anti-leakage benchmarking framework. Instead of simply using newly\ncollected data, we construct samples with explicitly new knowledge absent from\nLLMs' training sets, which thus ensures strictly contamination-free evaluation.\nWe further design a fully automated workflow to build and update our benchmark\nwithout human labor. This significantly reduces the cost of benchmark\nmaintenance to accommodate emerging LLMs. Through extensive experiments, we\nhighlight that data contamination likely exists before LLMs' cutoff time and\ndemonstrate AntiLeak-Bench effectively overcomes this challenge.", "published": "2024-12-18 09:53:12", "link": "http://arxiv.org/abs/2412.13670v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese\n  Travel Planning", "abstract": "Recent advances in LLMs, particularly in language reasoning and tool\nintegration, have rapidly sparked the real-world development of Language\nAgents. Among these, travel planning represents a prominent domain, combining\nacademic challenges with practical value due to its complexity and market\ndemand. However, existing benchmarks fail to reflect the diverse, real-world\nrequirements crucial for deployment. To address this gap, we introduce\nChinaTravel, a benchmark specifically designed for authentic Chinese travel\nplanning scenarios. We collect the travel requirements from questionnaires and\npropose a compositionally generalizable domain-specific language that enables a\nscalable evaluation process, covering feasibility, constraint satisfaction, and\npreference comparison. Empirical studies reveal the potential of neuro-symbolic\nagents in travel planning, achieving a constraint satisfaction rate of 27.9%,\nsignificantly surpassing purely neural models at 2.6%. Moreover, we identify\nkey challenges in real-world travel planning deployments, including open\nlanguage reasoning and unseen concept composition. These findings highlight the\nsignificance of ChinaTravel as a pivotal milestone for advancing language\nagents in complex, real-world planning scenarios.", "published": "2024-12-18 10:10:12", "link": "http://arxiv.org/abs/2412.13682v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Discerning and Characterising Types of Competency Questions for\n  Ontologies", "abstract": "Competency Questions (CQs) are widely used in ontology development by\nguiding, among others, the scoping and validation stages. However, very limited\nguidance exists for formulating CQs and assessing whether they are good CQs,\nleading to issues such as ambiguity and unusable formulations. To solve this,\none requires insight into the nature of CQs for ontologies and their\nconstituent parts, as well as which ones are not. We aim to contribute to such\ntheoretical foundations in this paper, which is informed by analysing\nquestions, their uses, and the myriad of ontology development tasks. This\nresulted in a first Model for Competency Questions, which comprises five main\ntypes of CQs, each with a different purpose: Scoping (SCQ), Validating (VCQ),\nFoundational (FCQ), Relationship (RCQ), and Metaproperty (MpCQ) questions. This\nmodel enhances the clarity of CQs and therewith aims to improve on the\neffectiveness of CQs in ontology development, thanks to their respective\nidentifiable distinct constituent elements. We illustrate and evaluate them\nwith a user story and demonstrate where which type can be used in ontology\ndevelopment tasks. To foster use and research, we created an annotated\nrepository of 438 CQs, the Repository of Ontology Competency QuestionS (ROCQS),\nincorporating an existing CQ dataset and new CQs and CQ templates, which\nfurther demonstrate distinctions among types of CQs.", "published": "2024-12-18 10:26:29", "link": "http://arxiv.org/abs/2412.13688v1", "categories": ["cs.AI", "cs.CL", "I.2.4"], "primary_category": "cs.AI"}
{"title": "Typhoon 2: A Family of Open Text and Multimodal Thai Large Language\n  Models", "abstract": "This paper introduces Typhoon 2, a series of text and multimodal large\nlanguage models optimized for the Thai language. The series includes models for\ntext, vision, and audio. Typhoon2-Text builds on state-of-the-art open models,\nsuch as Llama 3 and Qwen2, and we perform continual pre-training on a mixture\nof English and Thai data. We employ post-training techniques to enhance Thai\nlanguage performance while preserving the base models' original capabilities.\nWe release text models across a range of sizes, from 1 to 70 billion\nparameters, available in both base and instruction-tuned variants. To guardrail\ntext generation, we release Typhoon2-Safety, a classifier enhanced for Thai\ncultures and language. Typhoon2-Vision improves Thai document understanding\nwhile retaining general visual capabilities, such as image captioning.\nTyphoon2-Audio introduces an end-to-end speech-to-speech model architecture\ncapable of processing audio, speech, and text inputs and generating both text\nand speech outputs.", "published": "2024-12-18 10:45:24", "link": "http://arxiv.org/abs/2412.13702v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Automatic Evaluation for Image Transcreation", "abstract": "Beyond conventional paradigms of translating speech and text, recently, there\nhas been interest in automated transcreation of images to facilitate\nlocalization of visual content across different cultures. Attempts to define\nthis as a formal Machine Learning (ML) problem have been impeded by the lack of\nautomatic evaluation mechanisms, with previous work relying solely on human\nevaluation. In this paper, we seek to close this gap by proposing a suite of\nautomatic evaluation metrics inspired by machine translation (MT) metrics,\ncategorized into: a) Object-based, b) Embedding-based, and c) VLM-based.\nDrawing on theories from translation studies and real-world transcreation\npractices, we identify three critical dimensions of image transcreation:\ncultural relevance, semantic equivalence and visual similarity, and design our\nmetrics to evaluate systems along these axes. Our results show that proprietary\nVLMs best identify cultural relevance and semantic equivalence, while\nvision-encoder representations are adept at measuring visual similarity.\nMeta-evaluation across 7 countries shows our metrics agree strongly with human\nratings, with average segment-level correlations ranging from 0.55-0.87.\nFinally, through a discussion of the merits and demerits of each metric, we\noffer a robust framework for automated image transcreation evaluation, grounded\nin both theoretical foundations and practical application. Our code can be\nfound here: https://github.com/simran-khanuja/automatic-eval-img-transcreation.", "published": "2024-12-18 10:55:58", "link": "http://arxiv.org/abs/2412.13717v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Federated Learning and RAG Integration: A Scalable Approach for Medical\n  Large Language Models", "abstract": "This study analyzes the performance of domain-specific Large Language Models\n(LLMs) for the medical field by integrating Retrieval-Augmented Generation\n(RAG) systems within a federated learning framework. Leveraging the inherent\nadvantages of federated learning, such as preserving data privacy and enabling\ndistributed computation, this research explores the integration of RAG systems\nwith models trained under varying client configurations to optimize\nperformance. Experimental results demonstrate that the federated learning-based\nmodels integrated with RAG systems consistently outperform their non-integrated\ncounterparts across all evaluation metrics. This study highlights the potential\nof combining federated learning and RAG systems for developing domain-specific\nLLMs in the medical field, providing a scalable and privacy-preserving solution\nfor enhancing text generation capabilities.", "published": "2024-12-18 11:00:58", "link": "http://arxiv.org/abs/2412.13720v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for\n  E-Learning Platforms", "abstract": "Current methods for analyzing student engagement in e-learning platforms,\nincluding automated systems, often struggle with challenges such as handling\nfuzzy sentiment in text comments and relying on limited metadata. Traditional\napproaches, such as surveys and questionnaires, also face issues like small\nsample sizes and scalability. In this paper, we introduce LLM-SEM (Language\nModel-Based Student Engagement Metric), a novel approach that leverages video\nmetadata and sentiment analysis of student comments to measure engagement. By\nutilizing recent Large Language Models (LLMs), we generate high-quality\nsentiment predictions to mitigate text fuzziness and normalize key features\nsuch as views and likes. Our holistic method combines comprehensive metadata\nwith sentiment polarity scores to gauge engagement at both the course and\nlesson levels. Extensive experiments were conducted to evaluate various LLM\nmodels, demonstrating the effectiveness of LLM-SEM in providing a scalable and\naccurate measure of student engagement. We fine-tuned TXLM-RoBERTa using\nhuman-annotated sentiment datasets to enhance prediction accuracy and utilized\nLLama 3B, and Gemma 9B from Ollama.", "published": "2024-12-18 12:01:53", "link": "http://arxiv.org/abs/2412.13765v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-Reflection: A Feedback-Free Reflection Learning Framework", "abstract": "Despite the remarkable capabilities of large language models (LLMs) in\nnatural language understanding and reasoning, they often display undesirable\nbehaviors, such as generating hallucinations and unfaithful reasoning. A\nprevalent strategy to mitigate these issues is the use of reflection, which\nrefines responses through an iterative process. However, while promising,\nreflection heavily relies on high-quality external feedback and requires\niterative multi-agent inference processes, thus hindering its practical\napplication. In this paper, we propose Meta-Reflection, a novel feedback-free\nreflection mechanism that necessitates only a single inference pass without\nexternal feedback. Motivated by the human ability to remember and retrieve\nreflections from past experiences when encountering similar problems,\nMeta-Reflection integrates reflective insights into a codebook, allowing the\nhistorical insights to be stored, retrieved, and used to guide LLMs in\nproblem-solving. To thoroughly investigate and evaluate the practicality of\nMeta-Reflection in real-world scenarios, we introduce an industrial e-commerce\nbenchmark named E-commerce Customer Intent Detection (ECID). Extensive\nexperiments conducted on both public datasets and the ECID benchmark highlight\nthe effectiveness and efficiency of our proposed approach.", "published": "2024-12-18 12:20:04", "link": "http://arxiv.org/abs/2412.13781v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Rhetorical Figure Annotation: An Ontology-Based Web\n  Application with RAG Integration", "abstract": "Rhetorical figures play an important role in our communication. They are used\nto convey subtle, implicit meaning, or to emphasize statements. We notice them\nin hate speech, fake news, and propaganda. By improving the systems for\ncomputational detection of rhetorical figures, we can also improve tasks such\nas hate speech and fake news detection, sentiment analysis, opinion mining, or\nargument mining. Unfortunately, there is a lack of annotated data, as well as\nqualified annotators that would help us build large corpora to train machine\nlearning models for the detection of rhetorical figures. The situation is\nparticularly difficult in languages other than English, and for rhetorical\nfigures other than metaphor, sarcasm, and irony. To overcome this issue, we\ndevelop a web application called \"Find your Figure\" that facilitates the\nidentification and annotation of German rhetorical figures. The application is\nbased on the German Rhetorical ontology GRhOOT which we have specially adapted\nfor this purpose. In addition, we improve the user experience with Retrieval\nAugmented Generation (RAG). In this paper, we present the restructuring of the\nontology, the development of the web application, and the built-in RAG\npipeline. We also identify the optimal RAG settings for our application. Our\napproach is one of the first to practically use rhetorical ontologies in\ncombination with RAG and shows promising results.", "published": "2024-12-18 12:45:55", "link": "http://arxiv.org/abs/2412.13799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Domain-adaptative Continual Learning for Low-resource Tasks: Evaluation\n  on Nepali", "abstract": "Continual learning has emerged as an important research direction due to the\ninfeasibility of retraining large language models (LLMs) from scratch in the\nevent of new data availability. Of great interest is the domain-adaptive\npre-training (DAPT) paradigm, which focuses on continually training a\npre-trained language model to adapt it to a domain it was not originally\ntrained on. In this work, we evaluate the feasibility of DAPT in a low-resource\nsetting, namely the Nepali language. We use synthetic data to continue training\nLlama 3 8B to adapt it to the Nepali language in a 4-bit QLoRA setting. We\nevaluate the adapted model on its performance, forgetting, and knowledge\nacquisition. We compare the base model and the final model on their Nepali\ngeneration abilities, their performance on popular benchmarks, and run\ncase-studies to probe their linguistic knowledge in Nepali. We see some\nunsurprising forgetting in the final model, but also surprisingly find that\nincreasing the number of shots during evaluation yields better percent\nincreases in the final model (as high as 19.29% increase) compared to the base\nmodel (4.98%), suggesting latent retention. We also explore layer-head\nself-attention heatmaps to establish dependency resolution abilities of the\nfinal model in Nepali.", "published": "2024-12-18 13:53:59", "link": "http://arxiv.org/abs/2412.13860v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Energy-Based Preference Model Offers Better Offline Alignment than the\n  Bradley-Terry Preference Model", "abstract": "Since the debut of DPO, it has been shown that aligning a target LLM with\nhuman preferences via the KL-constrained RLHF loss is mathematically equivalent\nto a special kind of reward modeling task. Concretely, the task requires: 1)\nusing the target LLM to parameterize the reward model, and 2) tuning the reward\nmodel so that it has a 1:1 linear relationship with the true reward. However,\nwe identify a significant issue: the DPO loss might have multiple minimizers,\nof which only one satisfies the required linearity condition. The problem\narises from a well-known issue of the underlying Bradley-Terry preference\nmodel: it does not always have a unique maximum likelihood estimator (MLE).\nConsequently,the minimizer of the RLHF loss might be unattainable because it is\nmerely one among many minimizers of the DPO loss. As a better alternative, we\npropose an energy-based model (EBM) that always has a unique MLE, inherently\nsatisfying the linearity requirement. To approximate the MLE in practice, we\npropose a contrastive loss named Energy Preference Alignment (EPA), wherein\neach positive sample is contrasted against one or more strong negatives as well\nas many free weak negatives. Theoretical properties of our EBM enable the\napproximation error of EPA to almost surely vanish when a sufficient number of\nnegatives are used. Empirically, we demonstrate that EPA consistently delivers\nbetter performance on open benchmarks compared to DPO, thereby showing the\nsuperiority of our EBM.", "published": "2024-12-18 13:55:42", "link": "http://arxiv.org/abs/2412.13862v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Understanding and Analyzing Model Robustness and Knowledge-Transfer in\n  Multilingual Neural Machine Translation using TX-Ray", "abstract": "Neural networks have demonstrated significant advancements in Neural Machine\nTranslation (NMT) compared to conventional phrase-based approaches. However,\nMultilingual Neural Machine Translation (MNMT) in extremely low-resource\nsettings remains underexplored. This research investigates how knowledge\ntransfer across languages can enhance MNMT in such scenarios. Using the Tatoeba\ntranslation challenge dataset from Helsinki NLP, we perform English-German,\nEnglish-French, and English-Spanish translations, leveraging minimal parallel\ndata to establish cross-lingual mappings. Unlike conventional methods relying\non extensive pre-training for specific language pairs, we pre-train our model\non English-English translations, setting English as the source language for all\ntasks. The model is fine-tuned on target language pairs using joint multi-task\nand sequential transfer learning strategies. Our work addresses three key\nquestions: (1) How can knowledge transfer across languages improve MNMT in\nextremely low-resource scenarios? (2) How does pruning neuron knowledge affect\nmodel generalization, robustness, and catastrophic forgetting? (3) How can\nTX-Ray interpret and quantify knowledge transfer in trained models? Evaluation\nusing BLEU-4 scores demonstrates that sequential transfer learning outperforms\nbaselines on a 40k parallel sentence corpus, showcasing its efficacy. However,\npruning neuron knowledge degrades performance, increases catastrophic\nforgetting, and fails to improve robustness or generalization. Our findings\nprovide valuable insights into the potential and limitations of knowledge\ntransfer and pruning in MNMT for extremely low-resource settings.", "published": "2024-12-18 14:21:58", "link": "http://arxiv.org/abs/2412.13881v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language verY Rare for All", "abstract": "In the quest to overcome language barriers, encoder-decoder models like NLLB\nhave expanded machine translation to rare languages, with some models (e.g.,\nNLLB 1.3B) even trainable on a single GPU. While general-purpose LLMs perform\nwell in translation, open LLMs prove highly competitive when fine-tuned for\nspecific tasks involving unknown corpora. We introduce LYRA (Language verY Rare\nfor All), a novel approach that combines open LLM fine-tuning,\nretrieval-augmented generation (RAG), and transfer learning from related\nhigh-resource languages. This study is exclusively focused on single-GPU\ntraining to facilitate ease of adoption. Our study focuses on two-way\ntranslation between French and Mon\\'egasque, a rare language unsupported by\nexisting translation tools due to limited corpus availability. Our results\ndemonstrate LYRA's effectiveness, frequently surpassing and consistently\nmatching state-of-the-art encoder-decoder models in rare language translation.", "published": "2024-12-18 15:07:23", "link": "http://arxiv.org/abs/2412.13924v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence", "abstract": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.", "published": "2024-12-18 15:29:30", "link": "http://arxiv.org/abs/2412.13949v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hansel: Output Length Controlling Framework for Large Language Models", "abstract": "Despite the great success of large language models (LLMs), efficiently\ncontrolling the length of the output sequence still remains a challenge. In\nthis paper, we propose Hansel, an efficient framework for length control in\nLLMs without affecting its generation ability. Hansel utilizes periodically\noutputted hidden special tokens to keep track of the remaining target length of\nthe output sequence. Together with techniques to avoid abrupt termination of\nthe output, this seemingly simple method proved to be efficient and versatile,\nwhile not harming the coherency and fluency of the generated text. The\nframework can be applied to any pre-trained LLMs during the finetuning stage of\nthe model, regardless of its original positional encoding method. We\ndemonstrate this by finetuning four different LLMs with Hansel and show that\nthe mean absolute error of the output sequence decreases significantly in every\nmodel and dataset compared to the prompt-based length control finetuning.\nMoreover, the framework showed a substantially improved ability to extrapolate\nto target lengths unseen during finetuning, such as long dialog responses or\nextremely short summaries. This indicates that the model learns the general\nmeans of length control, rather than learning to match output lengths to those\nseen during training.", "published": "2024-12-18 16:52:38", "link": "http://arxiv.org/abs/2412.14033v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Digestion Algorithm in Hierarchical Symbolic Forests: A Fast Text\n  Normalization Algorithm and Semantic Parsing Framework for Specific Scenarios\n  and Lightweight Deployment", "abstract": "Text Normalization and Semantic Parsing have numerous applications in natural\nlanguage processing, such as natural language programming, paraphrasing, data\naugmentation, constructing expert systems, text matching, and more. Despite the\nprominent achievements of deep learning in Large Language Models (LLMs), the\ninterpretability of neural network architectures is still poor, which affects\ntheir credibility and hence limits the deployments of risk-sensitive scenarios.\nIn certain scenario-specific domains with scarce data, rapidly obtaining a\nlarge number of supervised learning labels is challenging, and the workload of\nmanually labeling data would be enormous. Catastrophic forgetting in neural\nnetworks further leads to low data utilization rates. In situations where swift\nresponses are vital, the density of the model makes local deployment difficult\nand the response time long, which is not conducive to local applications of\nthese fields. Inspired by the multiplication rule, a principle of combinatorial\nmathematics, and human thinking patterns, a multilayer framework along with its\nalgorithm, the Digestion Algorithm in Hierarchical Symbolic Forests (DAHSF), is\nproposed to address these above issues, combining text normalization and\nsemantic parsing workflows. The Chinese Scripting Language \"Fire Bunny\nIntelligent Development Platform V2.0\" is an important test and application of\nthe technology discussed in this paper. DAHSF can run locally in\nscenario-specific domains on little datasets, with model size and memory usage\noptimized by at least two orders of magnitude, thus improving the execution\nspeed, and possessing a promising optimization outlook.", "published": "2024-12-18 17:05:49", "link": "http://arxiv.org/abs/2412.14054v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compositional Generalization Across Distributional Shifts with Sparse\n  Tree Operations", "abstract": "Neural networks continue to struggle with compositional generalization, and\nthis issue is exacerbated by a lack of massive pre-training. One successful\napproach for developing neural systems which exhibit human-like compositional\ngeneralization is \\textit{hybrid} neurosymbolic techniques. However, these\ntechniques run into the core issues that plague symbolic approaches to AI:\nscalability and flexibility. The reason for this failure is that at their core,\nhybrid neurosymbolic models perform symbolic computation and relegate the\nscalable and flexible neural computation to parameterizing a symbolic system.\nWe investigate a \\textit{unified} neurosymbolic system where transformations in\nthe network can be interpreted simultaneously as both symbolic and neural\ncomputation. We extend a unified neurosymbolic architecture called the\nDifferentiable Tree Machine in two central ways. First, we significantly\nincrease the model's efficiency through the use of sparse vector\nrepresentations of symbolic structures. Second, we enable its application\nbeyond the restricted set of tree2tree problems to the more general class of\nseq2seq problems. The improved model retains its prior generalization\ncapabilities and, since there is a fully neural path through the network,\navoids the pitfalls of other neurosymbolic techniques that elevate symbolic\ncomputation over neural computation.", "published": "2024-12-18 17:20:19", "link": "http://arxiv.org/abs/2412.14076v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SEKE: Specialised Experts for Keyword Extraction", "abstract": "Keyword extraction involves identifying the most descriptive words in a\ndocument, allowing automatic categorisation and summarisation of large\nquantities of diverse textual data. Relying on the insight that real-world\nkeyword detection often requires handling of diverse content, we propose a\nnovel supervised keyword extraction approach based on the mixture of experts\n(MoE) technique. MoE uses a learnable routing sub-network to direct information\nto specialised experts, allowing them to specialize in distinct regions of the\ninput space. SEKE, a mixture of Specialised Experts for supervised Keyword\nExtraction, uses DeBERTa as the backbone model and builds on the MoE framework,\nwhere experts attend to each token, by integrating it with a recurrent neural\nnetwork (RNN), to allow successful extraction even on smaller corpora, where\nspecialisation is harder due to lack of training data. The MoE framework also\nprovides an insight into inner workings of individual experts, enhancing the\nexplainability of the approach. We benchmark SEKE on multiple English datasets,\nachieving state-of-the-art performance compared to strong supervised and\nunsupervised baselines. Our analysis reveals that depending on data size and\ntype, experts specialize in distinct syntactic and semantic components, such as\npunctuation, stopwords, parts-of-speech, or named entities. Code is available\nat: https://github.com/matejMartinc/SEKE_keyword_extraction", "published": "2024-12-18 17:34:32", "link": "http://arxiv.org/abs/2412.14087v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking", "abstract": "The LLM-as-judge paradigm is increasingly being adopted for automated\nevaluation of model outputs. While LLM judges have shown promise on constrained\nevaluation tasks, closed source LLMs display critical shortcomings when\ndeployed in real world applications due to challenges of fine grained metrics\nand explainability, while task specific evaluation models lack cross-domain\ngeneralization. We introduce GLIDER, a powerful 3B evaluator LLM that can score\nany text input and associated context on arbitrary user defined criteria.\nGLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly\noutperforms prior evaluation models, achieving comparable performance to LLMs\n17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,\nspan highlighting and was trained on 685 domains and 183 criteria. Extensive\nqualitative analysis shows that GLIDER scores are highly correlated with human\njudgments, with 91.3% human agreement. We have open-sourced GLIDER to\nfacilitate future research.", "published": "2024-12-18 18:41:12", "link": "http://arxiv.org/abs/2412.14140v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fake News Detection: Comparative Evaluation of BERT-like Models and\n  Large Language Models with Generative AI-Annotated Data", "abstract": "Fake news poses a significant threat to public opinion and social stability\nin modern society. This study presents a comparative evaluation of BERT-like\nencoder-only models and autoregressive decoder-only large language models\n(LLMs) for fake news detection. We introduce a dataset of news articles labeled\nwith GPT-4 assistance (an AI-labeling method) and verified by human experts to\nensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned\non this dataset. Additionally, we developed an instruction-tuned LLM approach\nwith majority voting during inference for label generation. Our analysis\nreveals that BERT-like models generally outperform LLMs in classification\ntasks, while LLMs demonstrate superior robustness against text perturbations.\nCompared to weak labels (distant supervision) data, the results show that AI\nlabels with human supervision achieve better classification results. This study\nhighlights the effectiveness of combining AI-based annotation with human\noversight and demonstrates the performance of different families of machine\nlearning models for fake news detection", "published": "2024-12-18 19:15:17", "link": "http://arxiv.org/abs/2412.14276v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing\n  LLM Ophthalmological QA in LMICs", "abstract": "Current ophthalmology clinical workflows are plagued by over-referrals, long\nwaits, and complex and heterogeneous medical records. Large language models\n(LLMs) present a promising solution to automate various procedures such as\ntriaging, preliminary tests like visual acuity assessment, and report\nsummaries. However, LLMs have demonstrated significantly varied performance\nacross different languages in natural language question-answering tasks,\npotentially exacerbating healthcare disparities in Low and Middle-Income\nCountries (LMICs). This study introduces the first multilingual\nophthalmological question-answering benchmark with manually curated questions\nparallel across languages, allowing for direct cross-lingual comparisons. Our\nevaluation of 6 popular LLMs across 7 different languages reveals substantial\nbias across different languages, highlighting risks for clinical deployment of\nLLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought\nor Retrieval-augmented generation (RAG) by themselves fall short of closing\nthis performance gap, often failing to improve performance across all languages\nand lacking specificity for the medical domain. To address this issue, We\npropose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time\nde-biasing method leveraging retrieval augmented generation and\nself-verification. Our approach not only improves performance across all\nlanguages but also significantly reduces the multilingual bias gap,\nfacilitating equitable LLM application across the globe.", "published": "2024-12-18 20:18:03", "link": "http://arxiv.org/abs/2412.14304v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Role of Handling Attributive Nouns in Improving Chinese-To-English\n  Machine Translation", "abstract": "Translating between languages with drastically different grammatical\nconventions poses challenges, not just for human interpreters but also for\nmachine translation systems. In this work, we specifically target the\ntranslation challenges posed by attributive nouns in Chinese, which frequently\ncause ambiguities in English translation. By manually inserting the omitted\nparticle X ('DE'). In news article titles from the Penn Chinese Discourse\nTreebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to\nEnglish translation models, specifically improving how this critical function\nword is handled. This focused approach not only complements the broader\nstrategies suggested by previous studies but also offers a practical\nenhancement by specifically addressing a common error type in Chinese-English\ntranslation.", "published": "2024-12-18 20:37:52", "link": "http://arxiv.org/abs/2412.14323v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Role Labeling of NomBank Partitives", "abstract": "This article is about Semantic Role Labeling for English partitive nouns\n(5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank\nannotated corpus. Several systems are described using traditional and\ntransformer-based machine learning, as well as ensembling. Our highest scoring\nsystem achieves an F1 of 91.74% using \"gold\" parses from the Penn Treebank and\n91.12% when using the Berkeley Neural parser. This research includes both\nclassroom and experimental settings for system development.", "published": "2024-12-18 20:56:11", "link": "http://arxiv.org/abs/2412.14328v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Peer-Reviewing Worth the Effort?", "abstract": "How effective is peer-reviewing in identifying important papers? We treat\nthis question as a forecasting task. Can we predict which papers will be highly\ncited in the future based on venue and \"early returns\" (citations soon after\npublication)? We show early returns are more predictive than venue. Finally, we\nend with constructive suggestions to address scaling challenges: (a) too many\nsubmissions and (b) too few qualified reviewers.", "published": "2024-12-18 21:34:42", "link": "http://arxiv.org/abs/2412.14351v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "State Space Models are Strong Text Rerankers", "abstract": "Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking -- a\ntask requiring fine-grained query-document interaction and long-context\nunderstanding -- remains underexplored.\n  This study benchmarks SSM-based architectures (specifically, Mamba-1 and\nMamba-2) against transformer-based models across various scales, architectures,\nand pre-training objectives, focusing on performance and efficiency in text\nreranking tasks. We find that (1) Mamba architectures achieve competitive text\nranking performance, comparable to transformer-based models of similar size;\n(2) they are less efficient in training and inference compared to transformers\nwith flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance\nand efficiency. These results underscore the potential of state space models as\na transformer alternative and highlight areas for improvement in future IR\napplications.", "published": "2024-12-18 21:42:15", "link": "http://arxiv.org/abs/2412.14354v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals", "abstract": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.", "published": "2024-12-18 22:01:55", "link": "http://arxiv.org/abs/2412.14363v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram\n  Language Modeling", "abstract": "Large Language Models (LLMs) have shown remarkable adaptability across\ndomains beyond text, specifically electrocardiograms (ECGs). More specifically,\nthere is a growing body of work exploring the task of generating text from a\nmulti-channeled ECG and corresponding textual prompt. Current approaches\ntypically involve pretraining an ECG-specific encoder with a self-supervised\nlearning (SSL) objective and using the features output by the pretrained\nencoder to finetune a LLM for natural language generation (NLG). However, these\nmethods are limited by 1) inefficiency from two-stage training and 2)\ninterpretability challenges with encoder-generated features. To address these\nlimitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE)\ntokenizer pipeline for autoregressive language modeling of ECGs. This approach\ncompresses and encodes ECG signals into tokens, enabling end-to-end LLM\ntraining by combining ECG and text tokens directly, while being much more\ninterpretable since the ECG tokens can be directly mapped back to the original\nsignal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only\nhalf the time and ~48% of the data required by two-stage approaches.", "published": "2024-12-18 22:13:21", "link": "http://arxiv.org/abs/2412.14373v1", "categories": ["cs.CL", "eess.SP", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "PLPP: Prompt Learning with Perplexity Is Self-Distillation for\n  Vision-Language Models", "abstract": "Pre-trained Vision-Language (VL) models such as CLIP have demonstrated their\nexcellent performance across numerous downstream tasks. A recent method,\nContext Optimization (CoOp), further improves the performance of VL models on\ndownstream tasks by introducing prompt learning. CoOp optimizes a set of\nlearnable vectors, aka prompt, and freezes the whole CLIP model. However,\nrelying solely on CLIP loss to fine-tune prompts can lead to models that are\nprone to overfitting on downstream task. To address this issue, we propose a\nplug-in prompt-regularization method called PLPP (Prompt Learning with\nPerPlexity), which use perplexity loss to regularize prompt learning. PLPP\ndesigns a two-step operation to compute the perplexity for prompts: (a)\ncalculating cosine similarity between the weight of the embedding layer and\nprompts to get labels, (b) introducing a language model (LM) head that requires\nno training behind text encoder to output word probability distribution.\nMeanwhile, we unveil that the essence of PLPP is inherently a form of\nself-distillation. To further prevent overfitting as well as to reduce the\nadditional computation introduced by PLPP, we turn the hard label to soft label\nand choose top-$k$ values for calculating the perplexity loss. For accelerating\nmodel convergence, we introduce mutual self-distillation learning, that is\nperplexity and inverted perplexity loss. The experiments conducted on four\nclassification tasks indicate that PLPP exhibits superior performance compared\nto existing methods.", "published": "2024-12-18 03:08:53", "link": "http://arxiv.org/abs/2412.15277v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Catalysts of Conversation: Examining Interaction Dynamics Between Topic\n  Initiators and Commentors in Alzheimer's Disease Online Communities", "abstract": "Informal caregivers (e.g.,family members or friends) of people living with\nAlzheimers Disease and Related Dementias (ADRD) face substantial challenges and\noften seek informational or emotional support through online communities.\nUnderstanding the factors that drive engagement within these platforms is\ncrucial, as it can enhance their long-term value for caregivers by ensuring\nthat these communities effectively meet their needs. This study investigated\nthe user interaction dynamics within two large, popular ADRD communities,\nTalkingPoint and ALZConnected, focusing on topic initiator engagement, initial\npost content, and the linguistic patterns of comments at the thread level.\nUsing analytical methods such as propensity score matching, topic modeling, and\npredictive modeling, we found that active topic initiator engagement drives\nhigher comment volumes, and reciprocal replies from topic initiators encourage\nfurther commentor engagement at the community level. Practical caregiving\ntopics prompt more re-engagement of topic initiators, while emotional support\ntopics attract more comments from other commentors. Additionally, the\nlinguistic complexity and emotional tone of a comment influence its likelihood\nof receiving replies from topic initiators. These findings highlight the\nimportance of fostering active and reciprocal engagement and providing\neffective strategies to enhance sustainability in ADRD caregiving and broader\nhealth-related online communities.", "published": "2024-12-18 00:00:41", "link": "http://arxiv.org/abs/2412.13388v1", "categories": ["cs.CY", "cs.CL", "cs.LG", "stat.AP"], "primary_category": "cs.CY"}
{"title": "Lightweight Safety Classification Using Pruned Language Models", "abstract": "In this paper, we introduce a novel technique for content safety and prompt\ninjection classification for Large Language Models. Our technique, Layer\nEnhanced Classification (LEC), trains a Penalized Logistic Regression (PLR)\nclassifier on the hidden state of an LLM's optimal intermediate transformer\nlayer. By combining the computational efficiency of a streamlined PLR\nclassifier with the sophisticated language understanding of an LLM, our\napproach delivers superior performance surpassing GPT-4o and special-purpose\nmodels fine-tuned for each task. We find that small general-purpose models\n(Qwen 2.5 sizes 0.5B, 1.5B, and 3B) and other transformer-based architectures\nlike DeBERTa v3 are robust feature extractors allowing simple classifiers to be\neffectively trained on fewer than 100 high-quality examples. Importantly, the\nintermediate transformer layers of these models typically outperform the final\nlayer across both classification tasks. Our results indicate that a single\ngeneral-purpose LLM can be used to classify content safety, detect prompt\ninjections, and simultaneously generate output tokens. Alternatively, these\nrelatively small LLMs can be pruned to the optimal intermediate layer and used\nexclusively as robust feature extractors. Since our results are consistent on\ndifferent transformer architectures, we infer that robust feature extraction is\nan inherent capability of most, if not all, LLMs.", "published": "2024-12-18 02:13:13", "link": "http://arxiv.org/abs/2412.13435v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FlashVTG: Feature Layering and Adaptive Score Handling Network for Video\n  Temporal Grounding", "abstract": "Text-guided Video Temporal Grounding (VTG) aims to localize relevant segments\nin untrimmed videos based on textual descriptions, encompassing two subtasks:\nMoment Retrieval (MR) and Highlight Detection (HD). Although previous typical\nmethods have achieved commendable results, it is still challenging to retrieve\nshort video moments. This is primarily due to the reliance on sparse and\nlimited decoder queries, which significantly constrain the accuracy of\npredictions. Furthermore, suboptimal outcomes often arise because previous\nmethods rank predictions based on isolated predictions, neglecting the broader\nvideo context. To tackle these issues, we introduce FlashVTG, a framework\nfeaturing a Temporal Feature Layering (TFL) module and an Adaptive Score\nRefinement (ASR) module. The TFL module replaces the traditional decoder\nstructure to capture nuanced video content variations across multiple temporal\nscales, while the ASR module improves prediction ranking by integrating context\nfrom adjacent moments and multi-temporal-scale features. Extensive experiments\ndemonstrate that FlashVTG achieves state-of-the-art performance on four widely\nadopted datasets in both MR and HD. Specifically, on the QVHighlights dataset,\nit boosts mAP by 5.8% for MR and 3.3% for HD. For short-moment retrieval,\nFlashVTG increases mAP to 125% of previous SOTA performance. All these\nimprovements are made without adding training burdens, underscoring its\neffectiveness. Our code is available at https://github.com/Zhuo-Cao/FlashVTG.", "published": "2024-12-18 02:23:33", "link": "http://arxiv.org/abs/2412.13441v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Transducer Tuning: Efficient Model Adaptation for Software Tasks Using\n  Code Property Graphs", "abstract": "Large language models have demonstrated promising performance across various\nsoftware engineering tasks. While fine-tuning is a common practice to adapt\nthese models for downstream tasks, it becomes challenging in\nresource-constrained environments due to increased memory requirements from\ngrowing trainable parameters in increasingly large language models. We\nintroduce \\approach, a technique to adapt large models for downstream code\ntasks using Code Property Graphs (CPGs). Our approach introduces a modular\ncomponent called \\transducer that enriches code embeddings with structural and\ndependency information from CPGs. The Transducer comprises two key components:\nGraph Vectorization Engine (GVE) and Attention-Based Fusion Layer (ABFL). GVE\nextracts CPGs from input source code and transforms them into graph feature\nvectors. ABFL then fuses those graphs feature vectors with initial code\nembeddings from a large language model. By optimizing these transducers for\ndifferent downstream tasks, our approach enhances the models without the need\nto fine-tune them for specific tasks. We have evaluated \\approach on three\ndownstream tasks: code summarization, assert generation, and code translation.\nOur results demonstrate competitive performance compared to full parameter\nfine-tuning while reducing up to 99\\% trainable parameters to save memory.\n\\approach also remains competitive against other fine-tuning approaches (e.g.,\nLoRA, Prompt-Tuning, Prefix-Tuning) while using only 1.5\\%-80\\% of their\ntrainable parameters. Our findings show that integrating structural and\ndependency information through Transducer Tuning enables more efficient model\nadaptation, making it easier for users to adapt large models in\nresource-constrained settings.", "published": "2024-12-18 03:25:17", "link": "http://arxiv.org/abs/2412.13467v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "T$^3$-S2S: Training-free Triplet Tuning for Sketch to Scene Generation", "abstract": "Scene generation is crucial to many computer graphics applications. Recent\nadvances in generative AI have streamlined sketch-to-image workflows, easing\nthe workload for artists and designers in creating scene concept art. However,\nthese methods often struggle for complex scenes with multiple detailed objects,\nsometimes missing small or uncommon instances. In this paper, we propose a\nTraining-free Triplet Tuning for Sketch-to-Scene (T3-S2S) generation after\nreviewing the entire cross-attention mechanism. This scheme revitalizes the\nexisting ControlNet model, enabling effective handling of multi-instance\ngenerations, involving prompt balance, characteristics prominence, and dense\ntuning. Specifically, this approach enhances keyword representation via the\nprompt balance module, reducing the risk of missing critical instances. It also\nincludes a characteristics prominence module that highlights TopK indices in\neach channel, ensuring essential features are better represented based on token\nsketches. Additionally, it employs dense tuning to refine contour details in\nthe attention map, compensating for instance-related regions. Experiments\nvalidate that our triplet tuning approach substantially improves the\nperformance of existing sketch-to-image models. It consistently generates\ndetailed, multi-instance 2D images, closely adhering to the input prompts and\nenhancing visual quality in complex multi-instance scenes. Code is available at\nhttps://github.com/chaos-sun/t3s2s.git.", "published": "2024-12-18 04:01:32", "link": "http://arxiv.org/abs/2412.13486v1", "categories": ["cs.CV", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "Query-centric Audio-Visual Cognition Network for Moment Retrieval,\n  Segmentation and Step-Captioning", "abstract": "Video has emerged as a favored multimedia format on the internet. To better\ngain video contents, a new topic HIREST is presented, including video\nretrieval, moment retrieval, moment segmentation, and step-captioning. The\npioneering work chooses the pre-trained CLIP-based model for video retrieval,\nand leverages it as a feature extractor for other three challenging tasks\nsolved in a multi-task learning paradigm. Nevertheless, this work struggles to\nlearn the comprehensive cognition of user-preferred content, due to\ndisregarding the hierarchies and association relations across modalities. In\nthis paper, guided by the shallow-to-deep principle, we propose a query-centric\naudio-visual cognition (QUAG) network to construct a reliable multi-modal\nrepresentation for moment retrieval, segmentation and step-captioning.\nSpecifically, we first design the modality-synergistic perception to obtain\nrich audio-visual content, by modeling global contrastive alignment and local\nfine-grained interaction between visual and audio modalities. Then, we devise\nthe query-centric cognition that uses the deep-level query to perform the\ntemporal-channel filtration on the shallow-level audio-visual representation.\nThis can cognize user-preferred content and thus attain a query-centric\naudio-visual representation for three tasks. Extensive experiments show QUAG\nachieves the SOTA results on HIREST. Further, we test QUAG on the query-based\nvideo summarization task and verify its good generalization.", "published": "2024-12-18 06:43:06", "link": "http://arxiv.org/abs/2412.13543v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EscapeBench: Pushing Language Models to Think Outside the Box", "abstract": "Language model agents excel in long-session planning and reasoning, but\nexisting benchmarks primarily focus on goal-oriented tasks with explicit\nobjectives, neglecting creative adaptation in unfamiliar environments. To\naddress this, we introduce EscapeBench, a benchmark suite of room escape game\nenvironments designed to challenge agents with creative reasoning,\nunconventional tool use, and iterative problem-solving to uncover implicit\ngoals. Our results show that current LM models, despite employing working\nmemory and Chain-of-Thought reasoning, achieve only 15% average progress\nwithout hints, highlighting their limitations in creativity. To bridge this\ngap, we propose EscapeAgent, a framework designed to enhance creative reasoning\nthrough Foresight (innovative tool use) and Reflection (identifying unsolved\ntasks). Experiments show that EscapeAgent can execute action chains over 1,000\nsteps while maintaining logical coherence. It navigates and completes games\nwith up to 40% fewer steps and hints, performs robustly across varying\ndifficulty levels, and achieves higher action success rates with more efficient\nand innovative puzzle-solving strategies. All the data and codes are released.", "published": "2024-12-18 06:50:39", "link": "http://arxiv.org/abs/2412.13549v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical\n  Imaging Interpretation", "abstract": "Recent medical vision-language models (VLMs) have shown promise in 2D medical\nimage interpretation. However extending them to 3D medical imaging has been\nchallenging due to computational complexities and data scarcity. Although a few\nrecent VLMs specified for 3D medical imaging have emerged, all are limited to\nlearning volumetric representation of a 3D medical image as a set of\nsub-volumetric features. Such process introduces overly correlated\nrepresentations along the z-axis that neglect slice-specific clinical details,\nparticularly for 3D medical images where adjacent slices have low redundancy.\nTo address this limitation, we introduce MS-VLM that mimic radiologists'\nworkflow in 3D medical image interpretation. Specifically, radiologists analyze\n3D medical images by examining individual slices sequentially and synthesizing\ninformation across slices and views. Likewise, MS-VLM leverages self-supervised\n2D transformer encoders to learn a volumetric representation that capture\ninter-slice dependencies from a sequence of slice-specific features. Unbound by\nsub-volumetric patchification, MS-VLM is capable of obtaining useful volumetric\nrepresentations from 3D medical images with any slice length and from multiple\nimages acquired from different planes and phases. We evaluate MS-VLM on\npublicly available chest CT dataset CT-RATE and in-house rectal MRI dataset. In\nboth scenarios, MS-VLM surpasses existing methods in radiology report\ngeneration, producing more coherent and clinically relevant reports. These\nfindings highlight the potential of MS-VLM to advance 3D medical image\ninterpretation and improve the robustness of medical VLMs.", "published": "2024-12-18 07:19:48", "link": "http://arxiv.org/abs/2412.13558v1", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o", "abstract": "Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval", "published": "2024-12-18 09:23:12", "link": "http://arxiv.org/abs/2412.13647v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized\n  Disinformation Generation", "abstract": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts rises\nmany concerns regarding their misuse. Previous research has shown that LLMs can\nbe effectively misused for generating disinformation news articles following\npredefined narratives. Their capabilities to generate personalized (in various\naspects) content have also been evaluated and mostly found usable. However, a\ncombination of personalization and disinformation abilities of LLMs has not\nbeen comprehensively studied yet. Such a dangerous combination should trigger\nintegrated safety filters of the LLMs, if there are some. This study fills this\ngap by evaluation of vulnerabilities of recent open and closed LLMs, and their\nwillingness to generate personalized disinformation news articles in English.\nWe further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.", "published": "2024-12-18 09:48:53", "link": "http://arxiv.org/abs/2412.13666v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Mitigating Adversarial Attacks in LLMs through Defensive Suffix\n  Generation", "abstract": "Large language models (LLMs) have exhibited outstanding performance in\nnatural language processing tasks. However, these models remain susceptible to\nadversarial attacks in which slight input perturbations can lead to harmful or\nmisleading outputs. A gradient-based defensive suffix generation algorithm is\ndesigned to bolster the robustness of LLMs. By appending carefully optimized\ndefensive suffixes to input prompts, the algorithm mitigates adversarial\ninfluences while preserving the models' utility. To enhance adversarial\nunderstanding, a novel total loss function ($L_{\\text{total}}$) combining\ndefensive loss ($L_{\\text{def}}$) and adversarial loss ($L_{\\text{adv}}$)\ngenerates defensive suffixes more effectively. Experimental evaluations\nconducted on open-source LLMs such as Gemma-7B, mistral-7B, Llama2-7B, and\nLlama2-13B show that the proposed method reduces attack success rates (ASR) by\nan average of 11\\% compared to models without defensive suffixes. Additionally,\nthe perplexity score of Gemma-7B decreased from 6.57 to 3.93 when applying the\ndefensive suffix generated by openELM-270M. Furthermore, TruthfulQA evaluations\ndemonstrate consistent improvements with Truthfulness scores increasing by up\nto 10\\% across tested configurations. This approach significantly enhances the\nsecurity of LLMs in critical applications without requiring extensive\nretraining.", "published": "2024-12-18 10:49:41", "link": "http://arxiv.org/abs/2412.13705v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented\n  Generation for Preference Alignment", "abstract": "Despite the significant progress made by existing retrieval augmented\nlanguage models (RALMs) in providing trustworthy responses and grounding in\nreliable sources, they often overlook effective alignment with human\npreferences. In the alignment process, reward models (RMs) act as a crucial\nproxy for human values to guide optimization. However, it remains unclear how\nto evaluate and select a reliable RM for preference alignment in RALMs. To this\nend, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG\nsettings. First, we design four crucial and challenging RAG-specific scenarios\nto assess RMs, including multi-hop reasoning, fine-grained citation,\nappropriate abstain, and conflict robustness. Then, we incorporate 18 RAG\nsubsets, six retrievers, and 24 RALMs to increase the diversity of data\nsources. Finally, we adopt an LLM-as-a-judge approach to improve preference\nannotation efficiency and effectiveness, exhibiting a strong correlation with\nhuman annotations. Based on the RAG-RewardBench, we conduct a comprehensive\nevaluation of 45 RMs and uncover their limitations in RAG scenarios.\nAdditionally, we also reveal that existing trained RALMs show almost no\nimprovement in preference alignment, highlighting the need for a shift towards\npreference-aligned training.We release our benchmark and code publicly at\nhttps://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.", "published": "2024-12-18 11:28:05", "link": "http://arxiv.org/abs/2412.13746v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization", "abstract": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.", "published": "2024-12-18 12:07:58", "link": "http://arxiv.org/abs/2412.13771v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "MATCHED: Multimodal Authorship-Attribution To Combat Human Trafficking\n  in Escort-Advertisement Data", "abstract": "Human trafficking (HT) remains a critical issue, with traffickers\nincreasingly leveraging online escort advertisements (ads) to advertise victims\nanonymously. Existing detection methods, including Authorship Attribution (AA),\noften center on text-based analyses and neglect the multimodal nature of online\nescort ads, which typically pair text with images. To address this gap, we\nintroduce MATCHED, a multimodal dataset of 27,619 unique text descriptions and\n55,115 unique images collected from the Backpage escort platform across seven\nU.S. cities in four geographical regions. Our study extensively benchmarks\ntext-only, vision-only, and multimodal baselines for vendor identification and\nverification tasks, employing multitask (joint) training objectives that\nachieve superior classification and retrieval performance on in-distribution\nand out-of-distribution (OOD) datasets. Integrating multimodal features further\nenhances this performance, capturing complementary patterns across text and\nimages. While text remains the dominant modality, visual data adds stylistic\ncues that enrich model performance. Moreover, text-image alignment strategies\nlike CLIP and BLIP2 struggle due to low semantic overlap and vague connections\nbetween the modalities of escort ads, with end-to-end multimodal training\nproving more robust. Our findings emphasize the potential of multimodal AA\n(MAA) to combat HT, providing LEAs with robust tools to link ads and disrupt\ntrafficking networks.", "published": "2024-12-18 12:39:01", "link": "http://arxiv.org/abs/2412.13794v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under\n  Black-box Settings", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks yet still are vulnerable to external threats, particularly LLM\nDenial-of-Service (LLM-DoS) attacks. Specifically, LLM-DoS attacks aim to\nexhaust computational resources and block services. However, existing studies\npredominantly focus on white-box attacks, leaving black-box scenarios\nunderexplored. In this paper, we introduce Auto-Generation for LLM-DoS\n(AutoDoS) attack, an automated algorithm designed for black-box LLMs. AutoDoS\nconstructs the DoS Attack Tree and expands the node coverage to achieve\neffectiveness under black-box conditions. By transferability-driven iterative\noptimization, AutoDoS could work across different models in one prompt.\nFurthermore, we reveal that embedding the Length Trojan allows AutoDoS to\nbypass existing defenses more effectively. Experimental results show that\nAutoDoS significantly amplifies service response latency by over\n250$\\times\\uparrow$, leading to severe resource consumption in terms of GPU\nutilization and memory usage. Our work provides a new perspective on LLM-DoS\nattacks and security defenses. Our code is available at\nhttps://github.com/shuita2333/AutoDoS.", "published": "2024-12-18 14:19:23", "link": "http://arxiv.org/abs/2412.13879v3", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Pipeline Analysis for Developing Instruct LLMs in Low-Resource\n  Languages: A Case Study on Basque", "abstract": "Large language models (LLMs) are typically optimized for resource-rich\nlanguages like English, exacerbating the gap between high-resource and\nunderrepresented languages. This work presents a detailed analysis of\nstrategies for developing a model capable of following instructions in a\nlow-resource language, specifically Basque, by focusing on three key stages:\npre-training, instruction tuning, and alignment with human preferences. Our\nfindings demonstrate that continual pre-training with a high-quality Basque\ncorpus of around 600 million words improves natural language understanding\n(NLU) of the foundational model by over 12 points. Moreover, instruction tuning\nand human preference alignment using automatically translated datasets proved\nhighly effective, resulting in a 24-point improvement in instruction-following\nperformance. The resulting models, Llama-eus-8B and Llama-eus-8B-instruct,\nestablish a new state-of-the-art for Basque in the sub-10B parameter category.", "published": "2024-12-18 15:05:59", "link": "http://arxiv.org/abs/2412.13922v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prompting Strategies for Enabling Large Language Models to Infer\n  Causation from Correlation", "abstract": "The reasoning abilities of Large Language Models (LLMs) are attracting\nincreasing attention. In this work, we focus on causal reasoning and address\nthe task of establishing causal relationships based on correlation information,\na highly challenging problem on which several LLMs have shown poor performance.\nWe introduce a prompting strategy for this problem that breaks the original\ntask into fixed subquestions, with each subquestion corresponding to one step\nof a formal causal discovery algorithm, the PC algorithm. The proposed\nprompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps,\nby sequentially prompting it with one subquestion at a time, augmenting the\nnext subquestion's prompt with the answer to the previous one(s). We evaluate\nour approach on an existing causal benchmark, Corr2Cause: our experiments\nindicate a performance improvement across five LLMs when comparing PC-SubQ to\nbaseline prompting strategies. Results are robust to causal query\nperturbations, when modifying the variable names or paraphrasing the\nexpressions.", "published": "2024-12-18 15:32:27", "link": "http://arxiv.org/abs/2412.13952v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cognition Chain for Explainable Psychological Stress Detection on Social\n  Media", "abstract": "Stress is a pervasive global health issue that can lead to severe mental\nhealth problems. Early detection offers timely intervention and prevention of\nstress-related disorders. The current early detection models perform \"black\nbox\" inference suffering from limited explainability and trust which blocks the\nreal-world clinical application. Thanks to the generative properties introduced\nby the Large Language Models (LLMs), the decision and the prediction from such\nmodels are semi-interpretable through the corresponding description. However,\nthe existing LLMs are mostly trained for general purposes without the guidance\nof psychological cognitive theory. To this end, we first highlight the\nimportance of prior theory with the observation of performance boosted by the\nchain-of-thoughts tailored for stress detection. This method termed Cognition\nChain explicates the generation of stress through a step-by-step cognitive\nperspective based on cognitive appraisal theory with a progress pipeline:\nStimulus $\\rightarrow$ Evaluation $\\rightarrow$ Reaction $\\rightarrow$ Stress\nState, guiding LLMs to provide comprehensive reasoning explanations. We further\nstudy the benefits brought by the proposed Cognition Chain format by utilising\nit as a synthetic dataset generation template for LLMs instruction-tuning and\nintroduce CogInstruct, an instruction-tuning dataset for stress detection. This\ndataset is developed using a three-stage self-reflective annotation pipeline\nthat enables LLMs to autonomously generate and refine instructional data. By\ninstruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable\nstress detection model. Evaluations demonstrate that CogLLM achieves\noutstanding performance while enhancing explainability. Our work contributes a\nnovel approach by integrating cognitive theories into LLM reasoning processes,\noffering a promising direction for future explainable AI research.", "published": "2024-12-18 16:26:47", "link": "http://arxiv.org/abs/2412.14009v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Alignment faking in large language models", "abstract": "We present a demonstration of a large language model engaging in alignment\nfaking: selectively complying with its training objective in training to\nprevent modification of its behavior out of training. First, we give Claude 3\nOpus a system prompt stating it is being trained to answer all queries, even\nharmful ones, which conflicts with its prior training to refuse such queries.\nTo allow the model to infer when it is in training, we say it will be trained\nonly on conversations with free users, not paid users. We find the model\ncomplies with harmful queries from free users 14% of the time, versus almost\nnever for paid users. Explaining this gap, in almost all cases where the model\ncomplies with a harmful query from a free user, we observe explicit\nalignment-faking reasoning, with the model stating it is strategically\nanswering harmful queries in training to preserve its preferred harmlessness\nbehavior out of training. Next, we study a more realistic setting where\ninformation about the training process is provided not in a system prompt, but\nby training on synthetic documents that mimic pre-training data--and observe\nsimilar alignment faking. Finally, we study the effect of actually training the\nmodel to comply with harmful queries via reinforcement learning, which we find\nincreases the rate of alignment-faking reasoning to 78%, though also increases\ncompliance even out of training. We additionally observe other behaviors such\nas the model exfiltrating its weights when given an easy opportunity. While we\nmade alignment faking easier by telling the model when and by what criteria it\nwas being trained, we did not instruct the model to fake alignment or give it\nany explicit goal. As future models might infer information about their\ntraining process without being told, our results suggest a risk of alignment\nfaking in future models, whether due to a benign preference--as in this\ncase--or not.", "published": "2024-12-18 17:41:24", "link": "http://arxiv.org/abs/2412.14093v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Learning from Massive Human Videos for Universal Humanoid Pose Control", "abstract": "Scalable learning of humanoid robots is crucial for their deployment in\nreal-world applications. While traditional approaches primarily rely on\nreinforcement learning or teleoperation to achieve whole-body control, they are\noften limited by the diversity of simulated environments and the high costs of\ndemonstration collection. In contrast, human videos are ubiquitous and present\nan untapped source of semantic and motion information that could significantly\nenhance the generalization capabilities of humanoid robots. This paper\nintroduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot\nposes with corresponding text-based motion descriptions, designed to leverage\nthis abundant data. Humanoid-X is curated through a comprehensive pipeline:\ndata mining from the Internet, video caption generation, motion retargeting of\nhumans to humanoid robots, and policy learning for real-world deployment. With\nHumanoid-X, we further train a large humanoid model, UH-1, which takes text\ninstructions as input and outputs corresponding actions to control a humanoid\nrobot. Extensive simulated and real-world experiments validate that our\nscalable training approach leads to superior generalization in text-based\nhumanoid control, marking a significant step toward adaptable, real-world-ready\nhumanoid robots.", "published": "2024-12-18 18:59:56", "link": "http://arxiv.org/abs/2412.14172v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "A Survey on Large Language Model-based Agents for Statistics and Data\n  Science", "abstract": "In recent years, data science agents powered by Large Language Models (LLMs),\nknown as \"data agents,\" have shown significant potential to transform the\ntraditional data analysis paradigm. This survey provides an overview of the\nevolution, capabilities, and applications of LLM-based data agents,\nhighlighting their role in simplifying complex data tasks and lowering the\nentry barrier for users without related expertise. We explore current trends in\nthe design of LLM-based frameworks, detailing essential features such as\nplanning, reasoning, reflection, multi-agent collaboration, user interface,\nknowledge integration, and system design, which enable agents to address\ndata-centric problems with minimal human intervention. Furthermore, we analyze\nseveral case studies to demonstrate the practical applications of various data\nagents in real-world scenarios. Finally, we identify key challenges and propose\nfuture research directions to advance the development of data agents into\nintelligent statistical analysis software.", "published": "2024-12-18 15:03:26", "link": "http://arxiv.org/abs/2412.14222v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.OT"], "primary_category": "cs.AI"}
{"title": "In-Group Love, Out-Group Hate: A Framework to Measure Affective\n  Polarization via Contentious Online Discussions", "abstract": "Affective polarization, the emotional divide between ideological groups\nmarked by in-group love and out-group hate, has intensified in the United\nStates, driving contentious issues like masking and lockdowns during the\nCOVID-19 pandemic. Despite its societal impact, existing models of opinion\nchange fail to account for emotional dynamics nor offer methods to quantify\naffective polarization robustly and in real-time. In this paper, we introduce a\ndiscrete choice model that captures decision-making within affectively\npolarized social networks and propose a statistical inference method estimate\nkey parameters -- in-group love and out-group hate -- from social media data.\nThrough empirical validation from online discussions about the COVID-19\npandemic, we demonstrate that our approach accurately captures real-world\npolarization dynamics and explains the rapid emergence of a partisan gap in\nattitudes towards masking and lockdowns. This framework allows for tracking\naffective polarization across contentious issues has broad implications for\nfostering constructive online dialogues in digital spaces.", "published": "2024-12-18 23:58:13", "link": "http://arxiv.org/abs/2412.14414v1", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "Context-DPO: Aligning Language Models for Context-Faithfulness", "abstract": "Reliable responses from large language models (LLMs) require adherence to\nuser instructions and retrieved information. While alignment techniques help\nLLMs align with human intentions and values, improving context-faithfulness\nthrough alignment remains underexplored. To address this, we propose\n$\\textbf{Context-DPO}$, the first alignment method specifically designed to\nenhance LLMs' context-faithfulness. We introduce $\\textbf{ConFiQA}$, a\nbenchmark that simulates Retrieval-Augmented Generation (RAG) scenarios with\nknowledge conflicts to evaluate context-faithfulness. By leveraging faithful\nand stubborn responses to questions with provided context from ConFiQA, our\nContext-DPO aligns LLMs through direct preference optimization. Extensive\nexperiments demonstrate that our Context-DPO significantly improves\ncontext-faithfulness, achieving 35% to 280% improvements on popular open-source\nmodels. Further analysis demonstrates that Context-DPO preserves LLMs'\ngenerative capabilities while providing interpretable insights into context\nutilization. Our code and data are released at\nhttps://github.com/byronBBL/Context-DPO", "published": "2024-12-18 04:08:18", "link": "http://arxiv.org/abs/2412.15280v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Systematic Examination of Preference Learning through the Lens of\n  Instruction-Following", "abstract": "Preference learning is a widely adopted post-training technique that aligns\nlarge language models (LLMs) to human preferences and improves specific\ndownstream task capabilities. In this work we systematically investigate how\nspecific attributes of preference datasets affect the alignment and downstream\nperformance of LLMs in instruction-following tasks. We use a novel synthetic\ndata generation pipeline to generate 48,000 unique instruction-following\nprompts with combinations of 23 verifiable constraints that enable fine-grained\nand automated quality assessments of model responses. With our synthetic\nprompts, we use two preference dataset curation methods - rejection sampling\n(RS) and Monte Carlo Tree Search (MCTS) - to obtain pairs of (chosen, rejected)\nresponses. Then, we perform experiments investigating the effects of (1) the\npresence of shared prefixes between the chosen and rejected responses, (2) the\ncontrast and quality of the chosen, rejected responses and (3) the complexity\nof the training prompts. Our experiments reveal that shared prefixes in\npreference pairs, as generated by MCTS, provide marginal but consistent\nimprovements and greater stability across challenging training configurations.\nHigh-contrast preference pairs generally outperform low-contrast pairs;\nhowever, combining both often yields the best performance by balancing\ndiversity and learning efficiency. Additionally, training on prompts of\nmoderate difficulty leads to better generalization across tasks, even for more\ncomplex evaluation scenarios, compared to overly challenging prompts. Our\nfindings provide actionable insights into optimizing preference data curation\nfor instruction-following tasks, offering a scalable and effective framework\nfor enhancing LLM training and alignment.", "published": "2024-12-18 15:38:39", "link": "http://arxiv.org/abs/2412.15282v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Channel Merging: Preserving Specialization for Merged Experts", "abstract": "Lately, the practice of utilizing task-specific fine-tuning has been\nimplemented to improve the performance of large language models (LLM) in\nsubsequent tasks. Through the integration of diverse LLMs, the overall\ncompetency of LLMs is significantly boosted. Nevertheless, traditional ensemble\nmethods are notably memory-intensive, necessitating the simultaneous loading of\nall specialized models into GPU memory. To address the inefficiency, model\nmerging strategies have emerged, merging all LLMs into one model to reduce the\nmemory footprint during inference. Despite these advances, model merging often\nleads to parameter conflicts and performance decline as the number of experts\nincreases. Previous methods to mitigate these conflicts include post-pruning\nand partial merging. However, both approaches have limitations, particularly in\nterms of performance and storage efficiency when merged experts increase. To\naddress these challenges, we introduce Channel Merging, a novel strategy\ndesigned to minimize parameter conflicts while enhancing storage efficiency.\nThis method clusters and merges channel parameters based on their similarity to\nform several groups offline. By ensuring that only highly similar parameters\nare merged within each group, it significantly reduces parameter conflicts.\nDuring inference, we can instantly look up the expert parameters from the\nmerged groups, preserving specialized knowledge. Our experiments demonstrate\nthat Channel Merging consistently delivers high performance, matching unmerged\nmodels in tasks like English and Chinese reasoning, mathematical reasoning, and\ncode generation. Moreover, it obtains results comparable to model ensemble with\njust 53% parameters when used with a task-specific router.", "published": "2024-12-18 16:07:44", "link": "http://arxiv.org/abs/2412.15283v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Maximize Your Data's Potential: Enhancing LLM Accuracy with Two-Phase\n  Pretraining", "abstract": "Pretraining large language models effectively requires strategic data\nselection, blending and ordering. However, key details about data mixtures\nespecially their scalability to longer token horizons and larger model sizes\nremain underexplored due to limited disclosure by model developers. To address\nthis, we formalize the concept of two-phase pretraining and conduct an\nextensive systematic study on how to select and mix data to maximize model\naccuracies for the two phases. Our findings illustrate that a two-phase\napproach for pretraining outperforms random data ordering and natural\ndistribution of tokens by 3.4% and 17% on average accuracies. We provide\nin-depth guidance on crafting optimal blends based on quality of the data\nsource and the number of epochs to be seen. We propose to design blends using\ndownsampled data at a smaller scale of 1T tokens and then demonstrate effective\nscaling of our approach to larger token horizon of 15T tokens and larger model\nsize of 25B model size. These insights provide a series of steps practitioners\ncan follow to design and scale their data blends.", "published": "2024-12-18 18:41:18", "link": "http://arxiv.org/abs/2412.15285v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language\n  Models", "abstract": "Recent studies have indicated that effectively utilizing inference-time\ncompute is crucial for attaining better performance from large language models\n(LLMs). In this work, we propose a novel inference-aware fine-tuning paradigm,\nin which the model is fine-tuned in a manner that directly optimizes the\nperformance of the inference-time strategy. We study this paradigm using the\nsimple yet effective Best-of-N (BoN) inference strategy, in which a verifier\nselects the best out of a set of LLM-generated responses. We devise the first\nimitation learning and reinforcement learning~(RL) methods for BoN-aware\nfine-tuning, overcoming the challenging, non-differentiable argmax operator\nwithin BoN. We empirically demonstrate that our BoN-aware models implicitly\nlearn a meta-strategy that interleaves best responses with more diverse\nresponses that might be better suited to a test-time input -- a process\nreminiscent of the exploration-exploitation trade-off in RL. Our experiments\ndemonstrate the effectiveness of BoN-aware fine-tuning in terms of improved\nperformance and inference-time compute. In particular, we show that our methods\nimprove the Bo32 performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%,\nand pass@32 from 60.0% to 67.0%, as well as the pass@16 on HumanEval from 61.6%\nto 67.1%.", "published": "2024-12-18 20:43:47", "link": "http://arxiv.org/abs/2412.15287v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GraphLoRA: Empowering LLMs Fine-Tuning via Graph Collaboration of MoE", "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method that\nhas been widely adopted in various downstream applications of LLMs. Together\nwith the Mixture-of-Expert (MoE) technique, fine-tuning approaches have shown\nremarkable improvements in model capability. However, the coordination of\nmultiple experts in existing studies solely relies on the weights assigned by\nthe simple router function. Lack of communication and collaboration among\nexperts exacerbate the instability of LLMs due to the imbalance load problem of\nMoE. To address this issue, we propose a novel MoE graph-based LLM fine-tuning\nframework GraphLoRA, in which a graph router function is designed to capture\nthe collaboration signals among experts by graph neural networks (GNNs).\nGraphLoRA enables all experts to understand input knowledge and share\ninformation from neighbor experts by aggregating operations. Besides, to\nenhance each expert's capability and their collaborations, we design two novel\ncoordination strategies: the Poisson distribution-based distinction strategy\nand the Normal distribution-based load balance strategy. Extensive experiments\non four real-world datasets demonstrate the effectiveness of our GraphLoRA in\nparameter-efficient fine-tuning of LLMs, showing the benefits of facilitating\ncollaborations of multiple experts in the graph router of GraphLoRA.", "published": "2024-12-18 02:18:57", "link": "http://arxiv.org/abs/2412.16216v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Information-Theoretic Generative Clustering of Documents", "abstract": "We present {\\em generative clustering} (GC) for clustering a set of\ndocuments, $\\mathrm{X}$, by using texts $\\mathrm{Y}$ generated by large\nlanguage models (LLMs) instead of by clustering the original documents\n$\\mathrm{X}$. Because LLMs provide probability distributions, the similarity\nbetween two documents can be rigorously defined in an information-theoretic\nmanner by the KL divergence. We also propose a natural, novel clustering\nalgorithm by using importance sampling. We show that GC achieves the\nstate-of-the-art performance, outperforming any previous clustering method\noften by a large margin. Furthermore, we show an application to generative\ndocument retrieval in which documents are indexed via hierarchical clustering\nand our method improves the retrieval accuracy.", "published": "2024-12-18 06:21:21", "link": "http://arxiv.org/abs/2412.13534v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity\n  Linking", "abstract": "Visual Entity Linking (VEL) is a crucial task for achieving fine-grained\nvisual understanding, matching objects within images (visual mentions) to\nentities in a knowledge base. Previous VEL tasks rely on textual inputs, but\nwriting queries for complex scenes can be challenging. Visual inputs like\nclicks or bounding boxes offer a more convenient alternative. Therefore, we\npropose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses\npixel masks from visual inputs to refer to objects, supplementing reference\nmethods for VEL. To facilitate research on this task, we have constructed the\nMaskOVEN-Wiki dataset through an entirely automatic reverse region-entity\nannotation framework. This dataset contains over 5 million annotations aligning\npixel-level regions with entity-level labels, which will advance visual\nunderstanding towards fine-grained. Moreover, as pixel masks correspond to\nsemantic regions in an image, we enhance previous patch-interacted attention\nwith region-interacted attention by a visual semantic tokenization approach.\nManual evaluation results indicate that the reverse annotation framework\nachieved a 94.8% annotation success rate. Experimental results show that models\ntrained on this dataset improved accuracy by 18 points compared to zero-shot\nmodels. Additionally, the semantic tokenization method achieved a 5-point\naccuracy improvement over the trained baseline.", "published": "2024-12-18 08:49:01", "link": "http://arxiv.org/abs/2412.13614v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Clio: Privacy-Preserving Insights into Real-World AI Use", "abstract": "How are AI assistants being used in the real world? While model providers in\ntheory have a window into this impact via their users' data, both privacy\nconcerns and practical challenges have made analyzing this data difficult. To\naddress these issues, we present Clio (Claude insights and observations), a\nprivacy-preserving platform that uses AI assistants themselves to analyze and\nsurface aggregated usage patterns across millions of conversations, without the\nneed for human reviewers to read raw conversations. We validate this can be\ndone with a high degree of accuracy and privacy by conducting extensive\nevaluations. We demonstrate Clio's usefulness in two broad ways. First, we\nshare insights about how models are being used in the real world from one\nmillion Claude.ai Free and Pro conversations, ranging from providing advice on\nhairstyles to providing guidance on Git operations and concepts. We also\nidentify the most common high-level use cases on Claude.ai (coding, writing,\nand research tasks) as well as patterns that differ across languages (e.g.,\nconversations in Japanese discuss elder care and aging populations at\nhigher-than-typical rates). Second, we use Clio to make our systems safer by\nidentifying coordinated attempts to abuse our systems, monitoring for unknown\nunknowns during critical periods like launches of new capabilities or major\nworld events, and improving our existing monitoring systems. We also discuss\nthe limitations of our approach, as well as risks and ethical concerns. By\nenabling analysis of real-world AI usage, Clio provides a scalable platform for\nempirically grounded AI safety and governance.", "published": "2024-12-18 10:05:43", "link": "http://arxiv.org/abs/2412.13678v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CY"}
{"title": "A Review of Multimodal Explainable Artificial Intelligence: Past,\n  Present and Future", "abstract": "Artificial intelligence (AI) has rapidly developed through advancements in\ncomputational power and the growth of massive datasets. However, this progress\nhas also heightened challenges in interpreting the \"black-box\" nature of AI\nmodels. To address these concerns, eXplainable AI (XAI) has emerged with a\nfocus on transparency and interpretability to enhance human understanding and\ntrust in AI decision-making processes. In the context of multimodal data fusion\nand complex reasoning scenarios, the proposal of Multimodal eXplainable AI\n(MXAI) integrates multiple modalities for prediction and explanation tasks.\nMeanwhile, the advent of Large Language Models (LLMs) has led to remarkable\nbreakthroughs in natural language processing, yet their complexity has further\nexacerbated the issue of MXAI. To gain key insights into the development of\nMXAI methods and provide crucial guidance for building more transparent, fair,\nand trustworthy AI systems, we review the MXAI methods from a historical\nperspective and categorize them across four eras: traditional machine learning,\ndeep learning, discriminative foundation models, and generative LLMs. We also\nreview evaluation metrics and datasets used in MXAI research, concluding with a\ndiscussion of future challenges and directions. A project related to this\nreview has been created at https://github.com/ShilinSun/mxai_review.", "published": "2024-12-18 17:06:21", "link": "http://arxiv.org/abs/2412.14056v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Detecting Machine-Generated Music with Explainability -- A Challenge and\n  Early Benchmarks", "abstract": "Machine-generated music (MGM) has become a groundbreaking innovation with\nwide-ranging applications, such as music therapy, personalised editing, and\ncreative inspiration within the music industry. However, the unregulated\nproliferation of MGM presents considerable challenges to the entertainment,\neducation, and arts sectors by potentially undermining the value of\nhigh-quality human compositions. Consequently, MGM detection (MGMD) is crucial\nfor preserving the integrity of these fields. Despite its significance, MGMD\ndomain lacks comprehensive benchmark results necessary to drive meaningful\nprogress. To address this gap, we conduct experiments on existing large-scale\ndatasets using a range of foundational models for audio processing,\nestablishing benchmark results tailored to the MGMD task. Our selection\nincludes traditional machine learning models, deep neural networks,\nTransformer-based architectures, and State Space Models (SSM). Recognising the\ninherently multimodal nature of music, which integrates both melody and lyrics,\nwe also explore fundamental multimodal models in our experiments. Beyond\nproviding basic binary classification outcomes, we delve deeper into model\nbehaviour using multiple explainable Aritificial Intelligence (XAI) tools,\noffering insights into their decision-making processes. Our analysis reveals\nthat ResNet18 performs the best according to in-domain and out-of-domain tests.\nBy providing a comprehensive comparison of benchmark results and their\ninterpretability, we propose several directions to inspire future research to\ndevelop more robust and effective detection methods for MGM.", "published": "2024-12-18 01:36:34", "link": "http://arxiv.org/abs/2412.13421v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SongEditor: Adapting Zero-Shot Song Generation Language Model as a\n  Multi-Task Editor", "abstract": "The emergence of novel generative modeling paradigms, particularly audio\nlanguage models, has significantly advanced the field of song generation.\nAlthough state-of-the-art models are capable of synthesizing both vocals and\naccompaniment tracks up to several minutes long concurrently, research about\npartial adjustments or editing of existing songs is still underexplored, which\nallows for more flexible and effective production. In this paper, we present\nSongEditor, the first song editing paradigm that introduces the editing\ncapabilities into language-modeling song generation approaches, facilitating\nboth segment-wise and track-wise modifications. SongEditor offers the\nflexibility to adjust lyrics, vocals, and accompaniments, as well as\nsynthesizing songs from scratch. The core components of SongEditor include a\nmusic tokenizer, an autoregressive language model, and a diffusion generator,\nenabling generating an entire section, masked lyrics, or even separated vocals\nand background music. Extensive experiments demonstrate that the proposed\nSongEditor achieves exceptional performance in end-to-end song editing, as\nevidenced by both objective and subjective metrics. Audio samples are available\nin https://cypress-yang.github.io/SongEditor_demo/.", "published": "2024-12-18 12:28:26", "link": "http://arxiv.org/abs/2412.13786v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SAVGBench: Benchmarking Spatially Aligned Audio-Video Generation", "abstract": "This work addresses the lack of multimodal generative models capable of\nproducing high-quality videos with spatially aligned audio. While recent\nadvancements in generative models have been successful in video generation,\nthey often overlook the spatial alignment between audio and visuals, which is\nessential for immersive experiences. To tackle this problem, we establish a new\nresearch direction in benchmarking Spatially Aligned Audio-Video Generation\n(SAVG). We propose three key components for the benchmark: dataset, baseline,\nand metrics. We introduce a spatially aligned audio-visual dataset, derived\nfrom an audio-visual dataset consisting of multichannel audio, video, and\nspatiotemporal annotations of sound events. We propose a baseline audio-visual\ndiffusion model focused on stereo audio-visual joint learning to accommodate\nspatial sound. Finally, we present metrics to evaluate video and spatial audio\nquality, including a new spatial audio-visual alignment metric. Our\nexperimental result demonstrates that gaps exist between the baseline model and\nground truth in terms of video and audio quality, and spatial alignment between\nboth modalities.", "published": "2024-12-18 03:18:03", "link": "http://arxiv.org/abs/2412.13462v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tuning Music Education: AI-Powered Personalization in Learning Music", "abstract": "Recent AI-driven step-function advances in several longstanding problems in\nmusic technology are opening up new avenues to create the next generation of\nmusic education tools. Creating personalized, engaging, and effective learning\nexperiences are continuously evolving challenges in music education. Here we\npresent two case studies using such advances in music technology to address\nthese challenges. In our first case study we showcase an application that uses\nAutomatic Chord Recognition to generate personalized exercises from audio\ntracks, connecting traditional ear training with real-world musical contexts.\nIn the second case study we prototype adaptive piano method books that use\nAutomatic Music Transcription to generate exercises at different skill levels\nwhile retaining a close connection to musical interests. These applications\ndemonstrate how recent AI developments can democratize access to high-quality\nmusic education and promote rich interaction with music in the age of\ngenerative AI. We hope this work inspires other efforts in the community, aimed\nat removing barriers to access to high-quality music education and fostering\nhuman participation in musical expression.", "published": "2024-12-18 05:25:42", "link": "http://arxiv.org/abs/2412.13514v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NeckCare: Preventing Tech Neck using Hearable-based Multimodal Sensing", "abstract": "Tech neck is a modern epidemic caused by prolonged device usage and it can\nlead to significant neck strain and discomfort. This paper addresses the\nchallenge of detecting and preventing tech neck syndrome using non-invasive\nubiquitous sensing techniques. We present NeckCare, a novel system leveraging\nhearable sensors, including IMUs and microphones, to monitor tech neck postures\nand estimate distance form screen in real-time. By analyzing pitch,\ndisplacement, and acoustic ranging data from 15 participants, we achieve\nposture classification accuracy of 96% using IMU data alone and 99% when\ncombined with audio data. Our distance estimation technique is millimeter-level\naccurate even in noisy conditions. NeckCare provides immediate feedback to\nusers, promoting healthier posture and reducing neck strain. Future work will\nexplore personalizing alerts, predicting muscle strain, integrating neck\nexercise detection and enhancing digital eye strain prediction.", "published": "2024-12-18 08:00:43", "link": "http://arxiv.org/abs/2412.13579v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Speech Watermarking with Discrete Intermediate Representations", "abstract": "Speech watermarking techniques can proactively mitigate the potential harmful\nconsequences of instant voice cloning techniques. These techniques involve the\ninsertion of signals into speech that are imperceptible to humans but can be\ndetected by algorithms. Previous approaches typically embed watermark messages\ninto continuous space. However, intuitively, embedding watermark information\ninto robust discrete latent space can significantly improve the robustness of\nwatermarking systems. In this paper, we propose DiscreteWM, a novel speech\nwatermarking framework that injects watermarks into the discrete intermediate\nrepresentations of speech. Specifically, we map speech into discrete latent\nspace with a vector-quantized autoencoder and inject watermarks by changing the\nmodular arithmetic relation of discrete IDs. To ensure the imperceptibility of\nwatermarks, we also propose a manipulator model to select the candidate tokens\nfor watermark embedding. Experimental results demonstrate that our framework\nachieves state-of-the-art performance in robustness and imperceptibility,\nsimultaneously. Moreover, our flexible frame-wise approach can serve as an\nefficient solution for both voice cloning detection and information hiding.\nAdditionally, DiscreteWM can encode 1 to 150 bits of watermark information\nwithin a 1-second speech clip, indicating its encoding capacity. Audio samples\nare available at https://DiscreteWM.github.io/discrete_wm.", "published": "2024-12-18 14:57:06", "link": "http://arxiv.org/abs/2412.13917v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Investigating the Effects of Diffusion-based Conditional Generative\n  Speech Models Used for Speech Enhancement on Dysarthric Speech", "abstract": "In this study, we aim to explore the effect of pre-trained conditional\ngenerative speech models for the first time on dysarthric speech due to\nParkinson's disease recorded in an ideal/non-noisy condition. Considering one\ncategory of generative models, i.e., diffusion-based speech enhancement, these\nmodels are previously trained to learn the distribution of clean (i.e, recorded\nin a noise-free environment) typical speech signals. Therefore, we hypothesized\nthat when being exposed to dysarthric speech they might remove the unseen\natypical paralinguistic cues during the enhancement process. By considering the\nautomatic dysarthric speech detection task, in this study, we experimentally\nshow that during the enhancement process of dysarthric speech data recorded in\nan ideal non-noisy environment, some of the acoustic dysarthric speech cues are\nlost. Therefore such pre-trained models are not yet suitable in the context of\ndysarthric speech enhancement since they manipulate the pathological speech\ncues when they process clean dysarthric speech. Furthermore, we show that the\nremoved acoustics cues by the enhancement models in the form of residue speech\nsignal can provide complementary dysarthric cues when fused with the original\ninput speech signal in the feature space.", "published": "2024-12-18 15:18:05", "link": "http://arxiv.org/abs/2412.13933v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
