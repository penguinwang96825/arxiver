{"title": "Heat modulated affine stochastic volatility models for forward curve dynamics", "abstract": "We present a function-valued stochastic volatility model designed to capture\nthe continuous-time evolution of forward curves in fixed-income or commodity\nmarkets. The dynamics of the (logarithmic) forward curves are defined by a\nHeath-Jarrow-Morton-Musiela stochastic partial differential equation modulated\nby an instantaneous volatility process that describes the second-order moment\nstructure of forwards with different time-to-maturity. We propose to model the\noperator-valued instantaneous covariance by an affine process on the cone of\npositive trace-class operators with drift given by the Lyapunov operator of the\nLaplacian. The so defined infinite-rank stochastic volatility model is\nanalytically tractable due to its affine structure and allows to model maturity\nspecific risk and volatility clustering in forward markets. Furthermore, we\nintroduce a numerically feasible spectral Galerkin approximation of the\nassociated operator-valued generalized Riccati equations and study the\nrobustness of the model with respect to finite-rank approximations by providing\nexplicit error bounds on the approximation error.", "published": "2024-09-19 20:06:35", "link": "http://arxiv.org/abs/2409.13070v1", "categories": ["q-fin.MF", "math.PR", "q-fin.PR"], "primary_category": "q-fin.MF"}
{"title": "Financial Stochastic Models Diffusion: From Risk-Neutral to Real-World Measure", "abstract": "This research presents a comprehensive framework for transitioning financial\ndiffusion models from the risk-neutral (RN) measure to the real-world (RW)\nmeasure, leveraging results from probability theory, specifically Girsanov's\ntheorem. The RN measure, fundamental in derivative pricing, is contrasted with\nthe RW measure, which incorporates risk premiums and better reflects actual\nmarket behavior and investor preferences, making it crucial for risk\nmanagement. We address the challenges of incorporating real-world dynamics into\nfinancial models, such as accounting for market premiums, producing realistic\nterm structures of market indicators, and fitting any arbitrarily given market\ncurve. Our framework is designed to be general, applicable to a variety of\ndiffusion models, including those with non-additive noise such as the CIR++\nmodel. Through case studies involving Goldman Sachs' 2024 global credit outlook\nforecasts and the European Banking Authority (EBA) 2023 stress tests, we\nvalidate the robustness, practical relevance and applicability of our\nmethodology. This work contributes to the literature by providing a versatile\ntool for better risk measures and enhancing the realism of financial models\nunder the RW measure. Our model's versatility extends to stress testing and\nscenario analysis, providing practitioners with a powerful tool to evaluate\nvarious what-if scenarios and make well-informed decisions, particularly in\npricing and risk management strategies.", "published": "2024-09-19 13:51:14", "link": "http://arxiv.org/abs/2409.12783v1", "categories": ["q-fin.MF", "91G30, 91G70, 91G80, 60H10, 60G99", "G.3; I.6"], "primary_category": "q-fin.MF"}
{"title": "Algorithmic and High-Frequency Trading Problems for Semi-Markov and Hawkes Jump-Diffusion Models", "abstract": "This paper introduces a jump-diffusion pricing model specifically designed\nfor algorithmic trading and high-frequency trading (HFT). The model\nincorporates independent jump and diffusion processes, providing a more precise\nrepresentation of the limit order book (LOB) dynamics within a scaling-limit\nframework. Given that algorithmic and HFT strategies now dominate major\nfinancial markets, accurately modeling LOB dynamics is crucial for developing\neffective trading algorithms. Recent research has shown that LOB data often\nexhibit non-Markovian properties, reinforcing the need for models that better\ncapture its evolution. In this paper, we address acquisition and liquidation\nproblems under more general compound semi-Markov and Hawkes jump-diffusion\nmodels. We first develop jump-diffusion frameworks to capture these dynamics\nand then apply diffusion approximations to the jump components so that robust\nsolutions can be given. Optimal trading strategies are formulated using\nstochastic optimal control (SOC) and solved numerically. Finally, we present\nstrategy simulations analyzing price paths, inventory evolution, trading speed,\nand average execution prices. This study provides insights into how these\nmodels can improve execution strategies under more general price dynamics.", "published": "2024-09-19 13:44:24", "link": "http://arxiv.org/abs/2409.12776v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Concentrated Liquidity with Leverage", "abstract": "Concentrated liquidity (CL) provisioning is a way how to improve the capital\nefficiency of Automated Market Makers (AMM). Allowing liquidity providers to\nuse leverage is a step towards even higher capital efficiency. A number of\nDecentralized Finance (DeFi) protocols implement this technique in conjunction\nwith overcollateralized lending. However, the properties of leveraged CL\npositions have not been formalized and are poorly understood in practice. This\narticle describes the principles of a leveraged CL provisioning protocol,\nformally models the notions of margin level, assets, and debt, and proves that\nwithin this model, leveraged LP positions possess several properties that make\nthem safe to use.", "published": "2024-09-19 14:18:49", "link": "http://arxiv.org/abs/2409.12803v1", "categories": ["q-fin.TR", "q-fin.CP"], "primary_category": "q-fin.TR"}
{"title": "A Multi-agent Market Model Can Explain the Impact of AI Traders in Financial Markets -- A New Microfoundations of GARCH model", "abstract": "The AI traders in financial markets have sparked significant interest in\ntheir effects on price formation mechanisms and market volatility, raising\nimportant questions for market stability and regulation. Despite this interest,\na comprehensive model to quantitatively assess the specific impacts of AI\ntraders remains undeveloped. This study aims to address this gap by modeling\nthe influence of AI traders on market price formation and volatility within a\nmulti-agent framework, leveraging the concept of microfoundations.\nMicrofoundations involve understanding macroeconomic phenomena, such as market\nprice formation, through the decision-making and interactions of individual\neconomic agents. While widely acknowledged in macroeconomics, microfoundational\napproaches remain unexplored in empirical finance, particularly for models like\nthe GARCH model, which captures key financial statistical properties such as\nvolatility clustering and fat tails. This study proposes a multi-agent market\nmodel to derive the microfoundations of the GARCH model, incorporating three\ntypes of agents: noise traders, fundamental traders, and AI traders. By\nmathematically aggregating the micro-structure of these agents, we establish\nthe microfoundations of the GARCH model. We validate this model through\nmulti-agent simulations, confirming its ability to reproduce the stylized facts\nof financial markets. Finally, we analyze the impact of AI traders using\nparameters derived from these microfoundations, contributing to a deeper\nunderstanding of their role in market dynamics.", "published": "2024-09-19 07:14:13", "link": "http://arxiv.org/abs/2409.12516v1", "categories": ["q-fin.CP", "cs.AI", "cs.MA", "q-fin.TR"], "primary_category": "q-fin.CP"}
{"title": "Small Language Models are Equation Reasoners", "abstract": "Chain-of-Thought (CoT) reasoning has enabled Large Language Model (LLM) to\nachieve remarkable performance in various NLP tasks, including arithmetic\nproblem-solving. However, this success does not generalize to small language\nmodel (sLM) like T5, due to their limited capacity and absence of emergent\nabilities associated with larger models. Recent works to enhance sLM through\nknowledge distillation have yielded some improvements but still face\nsignificant limitations, particularly high ambiguity from the variability in\nnatural language expressions and substantial computational costs. In this\npaper, we investigate why sLM perform poorly on arithmetic reasoning tasks and\nhypothesize that natural language format variability introduces high ambiguity\nfor these smaller models. Based on this hypothesis, we conduct experiments with\nequation-only format, which is a reasoning format that unifies arithmetic\nreasoning previously expressed in natural language formats into mathematical\nequations. Experiment results demonstrate that equation-only format effectively\nboosts the arithmetic reasoning abilities of sLM, especially in very small\nmodels like T5-Tiny.", "published": "2024-09-19 01:34:43", "link": "http://arxiv.org/abs/2409.12393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round\n  LLM Generation", "abstract": "Chain-of-thought prompting significantly boosts the reasoning ability of\nlarge language models but still faces three issues: hallucination problem,\nrestricted interpretability, and uncontrollable generation. To address these\nchallenges, we present AgentCOT, a llm-based autonomous agent framework, which\ncan solve complex problems in an agent-style manner by multiple round LLM\ngeneration. At each step, AgentCOT selects an action and executes it to yield\nan intermediate result with supporting evidence. In addition, we integrate the\nstep's index into the reasoning process to form a graph structure for complex\ninference logic. We introduce two new strategies to enhance the performance of\nAgentCOT.We conduct extensive experiments to verify the effectiveness of our\nmethod on six common benchmarks. Results exhibit that our method brings in\nsubstantial improvements over current competitive approaches.", "published": "2024-09-19 02:20:06", "link": "http://arxiv.org/abs/2409.12411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language\n  Models", "abstract": "We introduce a novel analysis that leverages linguistic minimal pairs to\nprobe the internal linguistic representations of Large Language Models (LLMs).\nBy measuring the similarity between LLM activation differences across minimal\npairs, we quantify the and gain insight into the linguistic knowledge captured\nby LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs\nin three languages, reveal properties of linguistic similarity from four key\naspects: consistency across LLMs, relation to theoretical categorizations,\ndependency to semantic context, and cross-lingual alignment of relevant\nphenomena. Our findings suggest that 1) linguistic similarity is significantly\ninfluenced by training data exposure, leading to higher cross-LLM agreement in\nhigher-resource languages. 2) Linguistic similarity strongly aligns with\nfine-grained theoretical linguistic categories but weakly with broader ones. 3)\nLinguistic similarity shows a weak correlation with semantic similarity,\nshowing its context-dependent nature. 4) LLMs exhibit limited cross-lingual\nalignment in their understanding of relevant linguistic phenomena. This work\ndemonstrates the potential of minimal pairs as a window into the neural\nrepresentations of language in LLMs, shedding light on the relationship between\nLLMs and linguistic theory. Codes and data are available at\nhttps://github.com/ChenDelong1999/Linguistic-Similarity", "published": "2024-09-19 03:29:40", "link": "http://arxiv.org/abs/2409.12435v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Reasoning Potential in Large Langauge Models by Scaling\n  Code-form Planning", "abstract": "Despite the remarkable success of large language models (LLMs) on traditional\nnatural language processing tasks, their planning ability remains a critical\nbottleneck in tackling complex multi-step reasoning tasks. Existing approaches\nmainly rely on prompting or task-specific fine-tuning, often suffering from\npoor robustness and cross-task generalization. To address the limitation, we\nintroduce CodePlan, a scalable framework that empowers LLMs to generate and\nfollow \\textit{code-form plans} -- pseudocode that outlines high-level,\nstructured reasoning processes. By leveraging the structured and versatile\nnature of code, CodePlan effectively captures the rich semantics and control\nflows inherent to sophisticated reasoning tasks. Importantly, CodePlan allows\nautomatic extraction of code-form plans from massive, wide-ranging text corpora\nwithout the need for curated, task-specific datasets. This enables it to scale\nup efficiently and improve LLM's reasoning capabilities across diverse\nscenarios. To train CodePlan, we construct a large-scale dataset of 2M examples\nthat integrate code-form plans with standard prompt-response pairs from\nexisting corpora. With minimal computation overhead during both training and\ninference, CodePlan achieves a 25.1\\% relative improvement compared with\ndirectly generating responses, averaged across 13 challenging multi-step\nreasoning benchmarks, spanning mathematical reasoning, symbolic reasoning,\ninstruction-following, multi-hop QA, and decision-making tasks. Further\nanalysis reveals CodePlan's increasing performance gains on more complex\nreasoning tasks, as well as significant data efficiency thanks to its\ngeneralization ability.", "published": "2024-09-19 04:13:58", "link": "http://arxiv.org/abs/2409.12452v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring and Enhancing the Transfer of Distribution in Knowledge\n  Distillation for Autoregressive Language Models", "abstract": "Knowledge distillation (KD) is a technique that compresses large teacher\nmodels by training smaller student models to mimic them. The success of KD in\nauto-regressive language models mainly relies on Reverse KL for mode-seeking\nand student-generated output (SGO) to combat exposure bias. Our theoretical\nanalyses and experimental validation reveal that while Reverse KL effectively\nmimics certain features of the teacher distribution, it fails to capture most\nof its behaviors. Conversely, SGO incurs higher computational costs and\npresents challenges in optimization, particularly when the student model is\nsignificantly smaller than the teacher model. These constraints are primarily\ndue to the immutable distribution of the teacher model, which fails to adjust\nadaptively to models of varying sizes. We introduce Online Knowledge\nDistillation (OKD), where the teacher network integrates small online modules\nto concurrently train with the student model. This strategy abolishes the\nnecessity for on-policy sampling and merely requires minimal updates to the\nparameters of the teacher's online module during training, thereby allowing\ndynamic adaptation to the student's distribution to make distillation better.\nExtensive results across multiple generation datasets show that OKD achieves or\nexceeds the performance of leading methods in various model architectures and\nsizes, reducing training time by up to fourfold.", "published": "2024-09-19 07:05:26", "link": "http://arxiv.org/abs/2409.12512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Profiling Patient Transcript Using Large Language Model Reasoning\n  Augmentation for Alzheimer's Disease Detection", "abstract": "Alzheimer's disease (AD) stands as the predominant cause of dementia,\ncharacterized by a gradual decline in speech and language capabilities. Recent\ndeep-learning advancements have facilitated automated AD detection through\nspontaneous speech. However, common transcript-based detection methods directly\nmodel text patterns in each utterance without a global view of the patient's\nlinguistic characteristics, resulting in limited discriminability and\ninterpretability. Despite the enhanced reasoning abilities of large language\nmodels (LLMs), there remains a gap in fully harnessing the reasoning ability to\nfacilitate AD detection and model interpretation. Therefore, we propose a\npatient-level transcript profiling framework leveraging LLM-based reasoning\naugmentation to systematically elicit linguistic deficit attributes. The\nsummarized embeddings of the attributes are integrated into an Albert model for\nAD detection. The framework achieves 8.51\\% ACC and 8.34\\% F1 improvements on\nthe ADReSS dataset compared to the baseline without reasoning augmentation. Our\nfurther analysis shows the effectiveness of our identified linguistic deficit\nattributes and the potential to use LLM for AD detection interpretation.", "published": "2024-09-19 07:58:07", "link": "http://arxiv.org/abs/2409.12541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Knowledge Distillation of Large Language Models through\n  Efficient Multi-Modal Distribution Alignment", "abstract": "Knowledge distillation (KD) is an effective model compression method that can\ntransfer the internal capabilities of large language models (LLMs) to smaller\nones. However, the multi-modal probability distribution predicted by teacher\nLLMs causes difficulties for student models to learn. In this paper, we first\ndemonstrate the importance of multi-modal distribution alignment with\nexperiments and then highlight the inefficiency of existing KD approaches in\nlearning multi-modal distributions. To address this problem, we propose Ranking\nLoss based Knowledge Distillation (RLKD), which encourages the consistency of\nthe ranking of peak predictions between the teacher and student models. By\nincorporating word-level ranking loss, we ensure excellent compatibility with\nexisting distillation objectives while fully leveraging the fine-grained\ninformation between different categories in peaks of two predicted\ndistribution. Experimental results demonstrate that our method enables the\nstudent model to better learn the multi-modal distributions of the teacher\nmodel, leading to a significant performance improvement in various downstream\ntasks.", "published": "2024-09-19 08:06:42", "link": "http://arxiv.org/abs/2409.12545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval\n  Augmented Dialogues", "abstract": "In real-world applications with Large Language Models (LLMs), external\nretrieval mechanisms - such as Search-Augmented Generation (SAG), tool\nutilization, and Retrieval-Augmented Generation (RAG) - are often employed to\nenhance the quality of augmented generations in dialogues. These approaches\noften come with multi-turn dialogue, where each interaction is enriched by\nrelevant information retrieved from external sources. Existing benchmarks\neither assess LLMs' chat abilities in multi-turn dialogues or their use of\nretrieval for augmented responses in single-turn settings. However, there is a\ngap in evaluating LLMs' ability to leverage retrieval for more precise\nresponses across multiple turns. To address this limitation, we introduce\nRAD-Bench (Retrieval Augmented Dialogue), a benchmark designed to evaluate\nLLMs' capabilities in multi-turn dialogues following retrievals, essential for\ntheir deployment in context-rich applications. RAD-Bench evaluates two key\nabilities of LLMs: Retrieval Synthesis and Retrieval Reasoning. These are\nmeasured using discriminative questions and retrieved contexts, and\ncorresponding reference answers, assessing how effectively LLMs integrate and\nreason with context to maintain and enhance conversation quality over multiple\nturns. Our evaluation results on commonly used LLMs reveal that model\nperformance deteriorates as additional layers of conditions or constraints are\napplied across conversation turns, even when accurate retrieved contexts are\nprovided. The data and code are available at\nhttps://github.com/mtkresearch/RAD-Bench", "published": "2024-09-19 08:26:45", "link": "http://arxiv.org/abs/2409.12558v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Knowledge Distillation: Empowering Small Language Models with\n  Teacher Model Insights", "abstract": "Enhancing small language models for real-life application deployment is a\nsignificant challenge facing the research community. Due to the difficulties\nand costs of using large language models, researchers are seeking ways to\neffectively deploy task-specific small models. In this work, we introduce a\nsimple yet effective knowledge distillation method to improve the performance\nof small language models. Our approach utilizes a teacher model with\napproximately 3 billion parameters to identify the most influential tokens in\nits decision-making process. These tokens are extracted from the input based on\ntheir attribution scores relative to the output, using methods like saliency\nmaps. These important tokens are then provided as rationales to a student\nmodel, aiming to distill the knowledge of the teacher model. This method has\nproven to be effective, as demonstrated by testing it on four diverse datasets,\nwhere it shows improvement over both standard fine-tuning methods and\nstate-of-the-art knowledge distillation models. Furthermore, we explore\nexplanations of the success of the model by analyzing the important tokens\nextracted from the teacher model. Our findings reveal that in 68\\% of cases,\nspecifically in datasets where labels are part of the answer, such as\nmultiple-choice questions, the extracted tokens are part of the ground truth.", "published": "2024-09-19 09:09:53", "link": "http://arxiv.org/abs/2409.12586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing SLM via ChatGPT and Dataset Augmentation", "abstract": "This paper explores the enhancement of small language models through\nstrategic dataset augmentation via ChatGPT-3.5-Turbo, in the domain of Natural\nLanguage Inference (NLI). By employing knowledge distillation-based techniques\nand synthetic dataset augmentation, we aim to bridge the performance gap\nbetween large language models (LLMs) and small language models (SLMs) without\nthe immense cost of human annotation. Our methods involve two forms of\nrationale generation--information extraction and informed reasoning--to enrich\nthe ANLI dataset. We then fine-tune T5-Small on these augmented datasets,\nevaluating its performance against an established benchmark. Our findings\nreveal that the incorporation of synthetic rationales significantly improves\nthe model's ability to comprehend natural language, leading to 1.3\\% and 2.3\\%\nhigher classification accuracy, respectively, on the ANLI dataset,\ndemonstrating the potential of leveraging LLMs for dataset augmentation. This\napproach not only enhances the performance of smaller models on complex tasks\nbut also introduces a cost-effective method for fine-tuning smaller language\nmodels. By advancing our understanding of knowledge distillation and\nfine-tuning strategies, this work contributes to the ongoing effort to create\nmore capable and efficient NLP systems.", "published": "2024-09-19 09:24:36", "link": "http://arxiv.org/abs/2409.12599v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Performance Tracking: Leveraging Large Language Models for\n  Automated Construction of Scientific Leaderboards", "abstract": "Scientific leaderboards are standardized ranking systems that facilitate\nevaluating and comparing competitive methods. Typically, a leaderboard is\ndefined by a task, dataset, and evaluation metric (TDM) triple, allowing\nobjective performance assessment and fostering innovation through benchmarking.\nHowever, the exponential increase in publications has made it infeasible to\nconstruct and maintain these leaderboards manually. Automatic leaderboard\nconstruction has emerged as a solution to reduce manual labor. Existing\ndatasets for this task are based on the community-contributed leaderboards\nwithout additional curation. Our analysis shows that a large portion of these\nleaderboards are incomplete, and some of them contain incorrect information. In\nthis work, we present SciLead, a manually-curated Scientific Leaderboard\ndataset that overcomes the aforementioned problems. Building on this dataset,\nwe propose three experimental settings that simulate real-world scenarios where\nTDM triples are fully defined, partially defined, or undefined during\nleaderboard construction. While previous research has only explored the first\nsetting, the latter two are more representative of real-world applications. To\naddress these diverse settings, we develop a comprehensive LLM-based framework\nfor constructing leaderboards. Our experiments and analysis reveal that various\nLLMs often correctly identify TDM triples while struggling to extract result\nvalues from publications. We make our code and data publicly available.", "published": "2024-09-19 11:12:27", "link": "http://arxiv.org/abs/2409.12656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the topics, sentiments and hate speech in the Spanish\n  information environment", "abstract": "In the digital era, the internet and social media have transformed\ncommunication but have also facilitated the spread of hate speech and\ndisinformation, leading to radicalization, polarization, and toxicity. This is\nespecially concerning for media outlets due to their significant role in\nshaping public discourse. This study examines the topics, sentiments, and hate\nprevalence in 337,807 response messages (website comments and tweets) to news\nfrom five Spanish media outlets (La Vanguardia, ABC, El Pa\\'is, El Mundo, and\n20 Minutos) in January 2021. These public reactions were originally labeled as\ndistinct types of hate by experts following an original procedure, and they are\nnow classified into three sentiment values (negative, neutral, or positive) and\nmain topics. The BERTopic unsupervised framework was used to extract 81 topics,\nmanually named with the help of Large Language Models (LLMs) and grouped into\nnine primary categories.\n  Results show social issues (22.22%), expressions and slang (20.35%), and\npolitical issues (11.80%) as the most discussed. Content is mainly negative\n(62.7%) and neutral (28.57%), with low positivity (8.73%). Toxic narratives\nrelate to conversation expressions, gender, feminism, and COVID-19. Despite low\nlevels of hate speech (3.98%), the study confirms high toxicity in online\nresponses to social and political topics.", "published": "2024-09-19 11:19:44", "link": "http://arxiv.org/abs/2409.12658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text2Traj2Text: Learning-by-Synthesis Framework for Contextual\n  Captioning of Human Movement Trajectories", "abstract": "This paper presents Text2Traj2Text, a novel learning-by-synthesis framework\nfor captioning possible contexts behind shopper's trajectory data in retail\nstores. Our work will impact various retail applications that need better\ncustomer understanding, such as targeted advertising and inventory management.\nThe key idea is leveraging large language models to synthesize a diverse and\nrealistic collection of contextual captions as well as the corresponding\nmovement trajectories on a store map. Despite learned from fully synthesized\ndata, the captioning model can generalize well to trajectories/captions created\nby real human subjects. Our systematic evaluation confirmed the effectiveness\nof the proposed framework over competitive approaches in terms of ROUGE and\nBERT Score metrics.", "published": "2024-09-19 11:30:09", "link": "http://arxiv.org/abs/2409.12670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Measure: Generating Valid, Consistent, and Reproducible Text-Based\n  Measures for Social Science Research", "abstract": "The increasing use of text as data in social science research necessitates\nthe development of valid, consistent, reproducible, and efficient methods for\ngenerating text-based concept measures. This paper presents a novel method that\nleverages the internal hidden states of large language models (LLMs) to\ngenerate these concept measures. Specifically, the proposed method learns a\nconcept vector that captures how the LLM internally represents the target\nconcept, then estimates the concept value for text data by projecting the\ntext's LLM hidden states onto the concept vector. Three replication studies\ndemonstrate the method's effectiveness in producing highly valid, consistent,\nand reproducible text-based measures across various social science research\ncontexts, highlighting its potential as a valuable tool for the research\ncommunity.", "published": "2024-09-19 12:44:00", "link": "http://arxiv.org/abs/2409.12722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Edu-Values: Towards Evaluating the Chinese Education Values of Large\n  Language Models", "abstract": "In this paper, we present Edu-Values, the first Chinese education values\nevaluation benchmark that includes seven core values: professional philosophy,\nteachers' professional ethics, education laws and regulations, cultural\nliteracy, educational knowledge and skills, basic competencies and subject\nknowledge. We meticulously design 1,418 questions, covering multiple-choice,\nmulti-modal question answering, subjective analysis, adversarial prompts, and\nChinese traditional culture (short answer) questions. We conduct human feedback\nbased automatic evaluation over 21 state-of-the-art (SoTA) LLMs, and highlight\nthree main findings: (1) due to differences in educational culture, Chinese\nLLMs outperform English LLMs, with Qwen 2 ranking the first with a score of\n81.37; (2) LLMs often struggle with teachers' professional ethics and\nprofessional philosophy; (3) leveraging Edu-Values to build an external\nknowledge repository for RAG significantly improves LLMs' alignment. This\ndemonstrates the effectiveness of the proposed benchmark.", "published": "2024-09-19 13:02:54", "link": "http://arxiv.org/abs/2409.12739v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilingual Evaluation of Language Models on General Knowledge in\n  University Entrance Exams with Minimal Contamination", "abstract": "In this article we present UNED-ACCESS 2024, a bilingual dataset that\nconsists of 1003 multiple-choice questions of university entrance level exams\nin Spanish and English. Questions are originally formulated in Spanish and\ntranslated manually into English, and have not ever been publicly released. A\nselection of current open-source and proprietary models are evaluated in a\nuniform zero-shot experimental setting both on the UNED-ACCESS 2024 dataset and\non an equivalent subset of MMLU questions. Results show that (i) reasoning\nquestions are challenging for models, (ii) smaller models perform worse than\nlarger models and degrade faster in Spanish than in English and (iii) the\nperformance gap between languages is negligible for the best models and grows\nup to 37% for smaller models. Model ranking on UNED-ACCESS 2024 is almost\nidentical in English and Spanish, and has also a high correlation (0.98\nPearson) with ranking on MMLU, suggesting that a small dataset is sufficiently\ndiverse and representative to measure performance by discipline.", "published": "2024-09-19 13:13:07", "link": "http://arxiv.org/abs/2409.12746v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Learn to Mislead Humans via RLHF", "abstract": "Language models (LMs) can produce errors that are hard to detect for humans,\nespecially when the task is complex. RLHF, the most popular post-training\nmethod, may exacerbate this problem: to achieve higher rewards, LMs might get\nbetter at convincing humans that they are right even when they are wrong. We\nstudy this phenomenon under a standard RLHF pipeline, calling it \"U-SOPHISTRY\"\nsince it is Unintended by model developers. Specifically, we ask\ntime-constrained (e.g., 3-10 minutes) human subjects to evaluate the\ncorrectness of model outputs and calculate humans' accuracy against gold\nlabels. On a question-answering task (QuALITY) and programming task (APPS),\nRLHF makes LMs better at convincing our subjects but not at completing the task\ncorrectly. RLHF also makes the model harder to evaluate: our subjects' false\npositive rate increases by 24.1% on QuALITY and 18.3% on APPS. Finally, we show\nthat probing, a state-of-the-art approach for detecting Intended Sophistry\n(e.g. backdoored LMs), does not generalize to U-SOPHISTRY. Our results\nhighlight an important failure mode of RLHF and call for more research in\nassisting humans to align them.", "published": "2024-09-19 14:50:34", "link": "http://arxiv.org/abs/2409.12822v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexicon-Based Sentiment Analysis on Text Polarities with Evaluation of\n  Classification Models", "abstract": "Sentiment analysis possesses the potential of diverse applicability on\ndigital platforms. Sentiment analysis extracts the polarity to understand the\nintensity and subjectivity in the text. This work uses a lexicon-based method\nto perform sentiment analysis and shows an evaluation of classification models\ntrained over textual data. The lexicon-based methods identify the intensity of\nemotion and subjectivity at word levels. The categorization identifies the\ninformative words inside a text and specifies the quantitative ranking of the\npolarity of words. This work is based on a multi-class problem of text being\nlabeled as positive, negative, or neutral. Twitter sentiment dataset containing\n1.6 million unprocessed tweets is used with lexicon-based methods like Text\nBlob and Vader Sentiment to introduce the neutrality measure on text. The\nanalysis of lexicons shows how the word count and the intensity classify the\ntext. A comparative analysis of machine learning models, Naiive Bayes, Support\nVector Machines, Multinomial Logistic Regression, Random Forest, and Extreme\nGradient (XG) Boost performed across multiple performance metrics. The best\nestimations are achieved through Random Forest with an accuracy score of 81%.\nAdditionally, sentiment analysis is applied for a personality judgment case\nagainst a Twitter profile based on online activity.", "published": "2024-09-19 15:31:12", "link": "http://arxiv.org/abs/2409.12840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data\n  Augmentation and Gaussian-Decayed Contrastive Learning", "abstract": "Recently, using large language models (LLMs) for data augmentation has led to\nconsiderable improvements in unsupervised sentence embedding models. However,\nexisting methods encounter two primary challenges: limited data diversity and\nhigh data noise. Current approaches often neglect fine-grained knowledge, such\nas entities and quantities, leading to insufficient diversity. Additionally,\nunsupervised data frequently lacks discriminative information, and the\ngenerated synthetic samples may introduce noise. In this paper, we propose a\npipeline-based data augmentation method via LLMs and introduce the\nGaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model\nto enhance unsupervised sentence embeddings. To tackle the issue of low data\ndiversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and\nquantities, enabling LLMs to generate more diverse, knowledge-enriched samples.\nTo address high data noise, the GCSE model uses a Gaussian-decayed function to\nlimit the impact of false hard negative samples, enhancing the model's\ndiscriminative capability. Experimental results show that our approach achieves\nstate-of-the-art performance in semantic textual similarity (STS) tasks, using\nfewer data samples and smaller LLMs, demonstrating its efficiency and\nrobustness across various models.", "published": "2024-09-19 16:29:58", "link": "http://arxiv.org/abs/2409.12887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided\n  Learning", "abstract": "In this paper, we propose a new data synthesis method called\n\\textbf{LogicPro}, which leverages LeetCode-style algorithm\n\\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to\nsynthesize Complex \\underline{Logic}al Reasoning data in text format. First, we\nsynthesize complex reasoning problems through source algorithm problems and\ntest cases. Then, standard answers and intermediate variable outputs are\nobtained for each problem based on standard python solutions and test cases.\nFinally, with the guidance of code intermediate variables, we synthesize the\ntext reasoning process for each reasoning problems. Through this method, we can\nsynthesize data that is difficult, scalable, effective, and comes with golden\nstandard answers and high-quality reasoning processes. As a result, with our\n540K synthesized dataset constructed solely from 2,360 algorithm problems, our\napproach\n  Code and data are publicly available at\nhttps://github.com/jiangjin1999/LogicPro achieves significant improvements in\nmultiple models for the datasets \\textit{BBH$^{27}$}, \\textit{LogicBench},\n\\textit{DROP}, \\textit{AR-LSAT}, and \\textit{GSM8K}, etc. outperforming a wide\nrange of existing reasoning datasets.", "published": "2024-09-19 17:30:45", "link": "http://arxiv.org/abs/2409.12929v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented\n  Generation", "abstract": "Large Language Models (LLMs) have demonstrated significant performance\nimprovements across various cognitive tasks. An emerging application is using\nLLMs to enhance retrieval-augmented generation (RAG) capabilities. These\nsystems require LLMs to understand user queries, retrieve relevant information,\nand synthesize coherent and accurate responses. Given the increasing real-world\ndeployment of such systems, comprehensive evaluation becomes crucial. To this\nend, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set),\na high-quality evaluation dataset designed to test LLMs' ability to provide\nfactual responses, assess retrieval capabilities, and evaluate the reasoning\nrequired to generate final answers. While previous work has provided datasets\nand benchmarks to evaluate these abilities in isolation, FRAMES offers a\nunified framework that provides a clearer picture of LLM performance in\nend-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions\nthat require the integration of information from multiple sources. We present\nbaseline results demonstrating that even state-of-the-art LLMs struggle with\nthis task, achieving 0.40 accuracy with no retrieval. The accuracy is\nsignificantly improved with our proposed multi-step retrieval pipeline,\nachieving an accuracy of 0.66 (>50% improvement). We hope our work will help\nbridge evaluation gaps and assist in developing more robust and capable RAG\nsystems.", "published": "2024-09-19 17:52:07", "link": "http://arxiv.org/abs/2409.12941v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preference Alignment Improves Language Model-Based TTS", "abstract": "Recent advancements in text-to-speech (TTS) have shown that language model\n(LM)-based systems offer competitive performance to their counterparts. Further\noptimization can be achieved through preference alignment algorithms, which\nadjust LMs to align with the preferences of reward models, enhancing the\ndesirability of the generated content. This study presents a thorough empirical\nevaluation of how preference alignment algorithms, particularly Direct\nPreference Optimization (DPO), enhance LM-based TTS. With a 1.15B parameter\nLM-based TTS model, we demonstrate that preference alignment consistently\nimproves intelligibility, speaker similarity, and proxy subjective evaluation\nscores, with the latter two metrics surpassing even human speech in certain\nevaluations. We also show preference alignment is applicable to low-resource\nscenarios and effectively generalized to out-of-domain applications.", "published": "2024-09-19 01:58:19", "link": "http://arxiv.org/abs/2409.12403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mutual Information-based Representations Disentanglement for Unaligned\n  Multimodal Language Sequences", "abstract": "The key challenge in unaligned multimodal language sequences lies in\neffectively integrating information from various modalities to obtain a refined\nmultimodal joint representation. Recently, the disentangle and fuse methods\nhave achieved the promising performance by explicitly learning\nmodality-agnostic and modality-specific representations and then fusing them\ninto a multimodal joint representation. However, these methods often\nindependently learn modality-agnostic representations for each modality and\nutilize orthogonal constraints to reduce linear correlations between\nmodality-agnostic and modality-specific representations, neglecting to\neliminate their nonlinear correlations. As a result, the obtained multimodal\njoint representation usually suffers from information redundancy, leading to\noverfitting and poor generalization of the models. In this paper, we propose a\nMutual Information-based Representations Disentanglement (MIRD) method for\nunaligned multimodal language sequences, in which a novel disentanglement\nframework is designed to jointly learn a single modality-agnostic\nrepresentation. In addition, the mutual information minimization constraint is\nemployed to ensure superior disentanglement of representations, thereby\neliminating information redundancy within the multimodal joint representation.\nFurthermore, the challenge of estimating mutual information caused by the\nlimited labeled data is mitigated by introducing unlabeled data. Meanwhile, the\nunlabeled data also help to characterize the underlying structure of multimodal\ndata, consequently further preventing overfitting and enhancing the performance\nof the models. Experimental results on several widely used benchmark datasets\nvalidate the effectiveness of our proposed approach.", "published": "2024-09-19 02:12:26", "link": "http://arxiv.org/abs/2409.12408v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Zero-to-Strong Generalization: Eliciting Strong Capabilities of Large\n  Language Models Iteratively without Gold Labels", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance through\nsupervised fine-tuning or in-context learning using gold labels. However, this\nparadigm is limited by the availability of gold labels, while in certain\nscenarios, LLMs may need to perform tasks that are too complex for humans to\nprovide such labels. To tackle this challenge, this study explores whether\nsolely utilizing unlabeled data can elicit strong model capabilities. We\npropose a new paradigm termed zero-to-strong generalization. We iteratively\nprompt LLMs to annotate unlabeled data and retain high-quality labels by\nfiltering. Surprisingly, we obverse that this iterative process gradually\nunlocks LLMs' potential on downstream tasks. Our experiments on extensive\nclassification and reasoning tasks confirm the effectiveness of our proposed\nframework. Our analysis indicates that this paradigm is effective for both\nin-context learning and fine-tuning, and for various model sizes.", "published": "2024-09-19 02:59:44", "link": "http://arxiv.org/abs/2409.12425v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Enhancing Logical Reasoning in Large Language Models through Graph-based\n  Synthetic Data", "abstract": "Despite recent advances in training and prompting strategies for Large\nLanguage Models (LLMs), these models continue to face challenges with complex\nlogical reasoning tasks that involve long reasoning chains. In this work, we\nexplore the potential and limitations of using graph-based synthetic reasoning\ndata as training signals to enhance LLMs' reasoning capabilities. Our extensive\nexperiments, conducted on two established natural language reasoning tasks --\ninductive reasoning and spatial reasoning -- demonstrate that supervised\nfine-tuning (SFT) with synthetic graph-based reasoning data effectively\nenhances LLMs' reasoning performance without compromising their effectiveness\non other standard evaluation benchmarks.", "published": "2024-09-19 03:39:09", "link": "http://arxiv.org/abs/2409.12437v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incremental and Data-Efficient Concept Formation to Support Masked Word\n  Prediction", "abstract": "This paper introduces Cobweb4L, a novel approach for efficient language model\nlearning that supports masked word prediction. The approach builds on Cobweb,\nan incremental system that learns a hierarchy of probabilistic concepts. Each\nconcept stores the frequencies of words that appear in instances tagged with\nthat concept label. The system utilizes an attribute value representation to\nencode words and their surrounding context into instances. Cobweb4L uses the\ninformation theoretic variant of category utility and a new performance\nmechanism that leverages multiple concepts to generate predictions. We\ndemonstrate that with these extensions it significantly outperforms prior\nCobweb performance mechanisms that use only a single node to generate\npredictions. Further, we demonstrate that Cobweb4L learns rapidly and achieves\nperformance comparable to and even superior to Word2Vec. Next, we show that\nCobweb4L and Word2Vec outperform BERT in the same task with less training data.\nFinally, we discuss future work to make our conclusions more robust and\ninclusive.", "published": "2024-09-19 03:48:31", "link": "http://arxiv.org/abs/2409.12440v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMR: Knowledge Distillation with a Large Language Model-Induced Reward", "abstract": "Large language models have become increasingly popular and demonstrated\nremarkable performance in various natural language processing (NLP) tasks.\nHowever, these models are typically computationally expensive and difficult to\nbe deployed in resource-constrained environments. In this paper, we propose\nLLMR, a novel knowledge distillation (KD) method based on a reward function\ninduced from large language models. We conducted experiments on multiple\ndatasets in the dialogue generation and summarization tasks. Empirical results\ndemonstrate that our LLMR approach consistently outperforms traditional KD\nmethods in different tasks and datasets.", "published": "2024-09-19 06:27:58", "link": "http://arxiv.org/abs/2409.12500v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Should RAG Chatbots Forget Unimportant Conversations? Exploring\n  Importance and Forgetting with Psychological Insights", "abstract": "While Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nlong-term conversations, the increasing memory load as conversations progress\ndegrades retrieval accuracy. Drawing on psychological insights, we propose\nLUFY, a simple yet effective method that focuses on emotionally arousing\nmemories and retains less than 10% of the conversation. In the user experiment,\nparticipants interacted with three types of RAG chatbots, each for 2 hours over\n4 sessions, marking the most extensive assessment of a chatbot's long-term\ncapabilities to date -- more than four times longer than any existing\nbenchmark. The results demonstrate that prioritizing arousing memories while\nforgetting the majority of the conversation significantly enhances user\nexperience. This study pushes the frontier of long-term conversations and\nhighlights the importance of forgetting unimportant parts of conversations.\nCode and Dataset: https://github.com/ryuichi-sumida/LUFY", "published": "2024-09-19 07:39:22", "link": "http://arxiv.org/abs/2409.12524v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CamelEval: Advancing Culturally Aligned Arabic Language Models and\n  Benchmarks", "abstract": "Large Language Models (LLMs) are the cornerstones of modern artificial\nintelligence systems. This paper introduces Juhaina, a Arabic-English bilingual\nLLM specifically designed to align with the values and preferences of Arabic\nspeakers. Juhaina inherently supports advanced functionalities such as\ninstruction following, open-ended question answering, information provisioning,\nand text processing. Our model contains 9.24 billion parameters and is trained\non a context window of up to 8,192 tokens. This paper details the creation\nprocess of Juhaina and provides an extensive empirical evaluation. Furthermore,\nwe identify the limitations of widely-adopted Open Arabic LLM Leaderboard\n(OALL) and propose a new evaluation benchmark, CamelEval. Our findings\ndemonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as\nthe Llama and Gemma families, in generating helpful responses in Arabic,\nproviding factually accurate information about the region, and understanding\nnuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI\ntechnologies, serving over 400 million Arabic speakers by offering LLMs that\nnot only communicate in their language but also comprehend their culture. We\npublicly release all models on Huggingface \\url{https://huggingface.co/elmrc}.", "published": "2024-09-19 09:52:35", "link": "http://arxiv.org/abs/2409.12623v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Michelangelo: Long Context Evaluations Beyond Haystacks via Latent\n  Structure Queries", "abstract": "We introduce Michelangelo: a minimal, synthetic, and unleaked long-context\nreasoning evaluation for large language models which is also easy to\nautomatically score. This evaluation is derived via a novel, unifying framework\nfor evaluations over arbitrarily long contexts which measure the model's\nability to do more than retrieve a single piece of information from its\ncontext. The central idea of the Latent Structure Queries framework (LSQ) is to\nconstruct tasks which require a model to ``chisel away'' the irrelevant\ninformation in the context, revealing a latent structure in the context. To\nverify a model's understanding of this latent structure, we query the model for\ndetails of the structure. Using LSQ, we produce three diagnostic long-context\nevaluations across code and natural-language domains intended to provide a\nstronger signal of long-context language model capabilities. We perform\nevaluations on several state-of-the-art models and demonstrate both that a) the\nproposed evaluations are high-signal and b) that there is significant room for\nimprovement in synthesizing long-context information.", "published": "2024-09-19 10:38:01", "link": "http://arxiv.org/abs/2409.12640v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Connecting Ideas in 'Lower-Resource' Scenarios: NLP for National\n  Varieties, Creoles and Other Low-resource Scenarios", "abstract": "Despite excellent results on benchmarks over a small subset of languages,\nlarge language models struggle to process text from languages situated in\n`lower-resource' scenarios such as dialects/sociolects (national or social\nvarieties of a language), Creoles (languages arising from linguistic contact\nbetween multiple languages) and other low-resource languages. This introductory\ntutorial will identify common challenges, approaches, and themes in natural\nlanguage processing (NLP) research for confronting and overcoming the obstacles\ninherent to data-poor contexts. By connecting past ideas to the present field,\nthis tutorial aims to ignite collaboration and cross-pollination between\nresearchers working in these scenarios. Our notion of `lower-resource' broadly\ndenotes the outstanding lack of data required for model training - and may be\napplied to scenarios apart from the three covered in the tutorial.", "published": "2024-09-19 11:48:42", "link": "http://arxiv.org/abs/2409.12683v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Large Language Models for Product Attribute Value\n  Identification", "abstract": "Product attribute value identification (PAVI) involves automatically\nidentifying attributes and their values from product information, enabling\nfeatures like product search, recommendation, and comparison. Existing methods\nprimarily rely on fine-tuning pre-trained language models, such as BART and T5,\nwhich require extensive task-specific training data and struggle to generalize\nto new attributes. This paper explores large language models (LLMs), such as\nLLaMA and Mistral, as data-efficient and robust alternatives for PAVI. We\npropose various strategies: comparing one-step and two-step prompt-based\napproaches in zero-shot settings and utilizing parametric and non-parametric\nknowledge through in-context learning examples. We also introduce a dense\ndemonstration retriever based on a pre-trained T5 model and perform instruction\nfine-tuning to explicitly train LLMs on task-specific instructions. Extensive\nexperiments on two product benchmarks show that our two-step approach\nsignificantly improves performance in zero-shot settings, and instruction\nfine-tuning further boosts performance when using training data, demonstrating\nthe practical benefits of using LLMs for PAVI.", "published": "2024-09-19 12:09:33", "link": "http://arxiv.org/abs/2409.12695v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MEXMA: Token-level objectives improve sentence representations", "abstract": "Current pre-trained cross-lingual sentence encoders approaches use\nsentence-level objectives only. This can lead to loss of information,\nespecially for tokens, which then degrades the sentence representation. We\npropose MEXMA, a novel approach that integrates both sentence-level and\ntoken-level objectives. The sentence representation in one language is used to\npredict masked tokens in another language, with both the sentence\nrepresentation and all tokens directly updating the encoder. We show that\nadding token-level objectives greatly improves the sentence representation\nquality across several tasks. Our approach outperforms current pre-trained\ncross-lingual sentence encoders on bi-text mining as well as several downstream\ntasks. We also analyse the information encoded in our tokens, and how the\nsentence representation is built from them.", "published": "2024-09-19 13:00:29", "link": "http://arxiv.org/abs/2409.12737v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine Tuning Large Language Models for Medicine: The Role and Importance\n  of Direct Preference Optimization", "abstract": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique.", "published": "2024-09-19 13:03:24", "link": "http://arxiv.org/abs/2409.12741v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists", "abstract": "Flavor development in the food industry is increasingly challenged by the\nneed for rapid innovation and precise flavor profile creation. Traditional\nflavor research methods typically rely on iterative, subjective testing, which\nlacks the efficiency and scalability required for modern demands. This paper\npresents three contributions to address the challenges. Firstly, we define a\nnew problem domain for scientific agents in flavor science, conceptualized as\nthe generation of hypotheses for flavor profile sourcing and understanding. To\nfacilitate research in this area, we introduce the FoodPuzzle, a challenging\nbenchmark consisting of 978 food items and 1,766 flavor molecules profiles. We\npropose a novel Scientific Agent approach, integrating in-context learning and\nretrieval augmented techniques to generate grounded hypotheses in the domain of\nfood science. Experimental results indicate that our model significantly\nsurpasses traditional methods in flavor profile prediction tasks, demonstrating\nits potential to transform flavor development practices.", "published": "2024-09-19 15:07:35", "link": "http://arxiv.org/abs/2409.12832v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A New Perspective on ADHD Research: Knowledge Graph Construction with\n  LLMs and Network Based Insights", "abstract": "Attention-Deficit/Hyperactivity Disorder (ADHD) is a challenging disorder to\nstudy due to its complex symptomatology and diverse contributing factors. To\nexplore how we can gain deeper insights on this topic, we performed a network\nanalysis on a comprehensive knowledge graph (KG) of ADHD, constructed by\nintegrating scientific literature and clinical data with the help of\ncutting-edge large language models. The analysis, including k-core techniques,\nidentified critical nodes and relationships that are central to understanding\nthe disorder. Building on these findings, we curated a knowledge graph that is\nusable in a context-aware chatbot (Graph-RAG) with Large Language Models\n(LLMs), enabling accurate and informed interactions. Our knowledge graph not\nonly advances the understanding of ADHD but also provides a powerful tool for\nresearch and clinical applications.", "published": "2024-09-19 15:50:22", "link": "http://arxiv.org/abs/2409.12853v2", "categories": ["cs.SI", "cs.CL", "68T30, 68T50, 92C30", "I.2.4; I.2.7; J.3"], "primary_category": "cs.SI"}
{"title": "Evaluating Defences against Unsafe Feedback in RLHF", "abstract": "While there has been progress towards aligning Large Language Models (LLMs)\nwith human values and ensuring safe behaviour at inference time, safety guards\ncan easily be removed when fine tuned on unsafe and harmful datasets. While\nthis setting has been treated extensively, another popular training paradigm,\nlearning from unsafe feedback with reinforcement learning, has previously been\nunexplored. This is concerning due to the widespread deployment of feedback\ncollection systems. We address this gap by providing an analysis of learning\nsettings where feedback is harmful, i.e. that unsafe samples are preferred over\nsafe ones despite model developers goal to maintain safety. We find that\nsafety-aligned LLMs easily explore unsafe action spaces via generating harmful\ntext and optimize for reward that violates safety constraints indicating that\ncurrent safety guards are not enough to prevent learning from unsafe feedback.\nIn order to protect against this vulnerability, we adapt a number of both\n\"implict\" and \"explicit\" harmful fine-tuning defences to evaluate whether they\nare effective as learning constraints in an RLHF setting finding that no method\nis generally effective pointing to the need for more defence research. We end\nthe paper with the observation that some defences work by performing \"harmless\nreward hacking\" for which we provide a theoretical explanation drawn from the\ntheory of Constrained Markov Decision Processes and provide some direction for\nfuture defence development.", "published": "2024-09-19 17:10:34", "link": "http://arxiv.org/abs/2409.12914v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models\n  with Correct-by-Construction Non-Textual Representations and Targeted Code\n  Repair", "abstract": "Despite the significant progress made in code generation with large language\nmodels, challenges persist, especially with hardware description languages such\nas Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog\ncoding, with synthetic data from prior methods. We identify two main issues:\ndifficulties in handling non-textual representations (Karnaugh maps,\nstate-transition diagrams and waveforms) and significant variability during\ntraining with models randomly making \"minor\" mistakes. To address these\nlimitations, we enhance data curation by creating correct-by-construction data\ntargeting non-textual representations. Additionally, we introduce an automated\nframework that generates error reports from various model checkpoints and\ninjects these errors into open-source code to create targeted code repair data.\nOur fine-tuned Starcoder2-15B outperforms prior state-of-the-art results by\n3.8%, 10.9%, 6.6% for pass@1 on VerilogEval-Machine, VerilogEval-Human, and\nRTLLM.", "published": "2024-09-19 12:15:55", "link": "http://arxiv.org/abs/2409.12993v2", "categories": ["cs.AR", "cs.CL"], "primary_category": "cs.AR"}
{"title": "TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement\n  Learning", "abstract": "The increasing prevalence of large language models (LLMs) such as GPT-4 in\nvarious applications has led to a surge in the size of prompts required for\noptimal performance, leading to challenges in computational efficiency. Prompt\ncompression aims to reduce the inference cost by minimizing input tokens\nwithout compromising on the task performance. However, existing prompt\ncompression techniques either rely on sub-optimal metrics such as information\nentropy or model it as a task-agnostic token classification problem that fails\nto capture task-specific information. To address these issues, we propose a\nnovel and efficient reinforcement learning (RL) based task-aware prompt\ncompression method. To ensure low latency requirements, we leverage existing\nTransformer encoder-based token classification model while guiding the learning\nprocess with task-specific reward signals using lightweight REINFORCE\nalgorithm. We evaluate the performance of our method on three diverse and\nchallenging tasks including text summarization, question answering and code\nsummarization. We demonstrate that our RL-guided compression method improves\nthe task performance by 8% - 189% across these three scenarios over\nstate-of-the-art compression techniques while satisfying the same compression\nrate and latency requirements.", "published": "2024-09-19 18:11:59", "link": "http://arxiv.org/abs/2409.13035v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing Methods for the Study of Protein-Ligand\n  Interactions", "abstract": "Recent advances in Natural Language Processing (NLP) have ignited interest in\ndeveloping effective methods for predicting protein-ligand interactions (PLIs)\ngiven their relevance to drug discovery and protein engineering efforts and the\never-growing volume of biochemical sequence and structural data available. The\nparallels between human languages and the \"languages\" used to represent\nproteins and ligands have enabled the use of NLP machine learning approaches to\nadvance PLI studies. In this review, we explain where and how such approaches\nhave been applied in the recent literature and discuss useful mechanisms such\nas long short-term memory, transformers, and attention. We conclude with a\ndiscussion of the current limitations of NLP methods for the study of PLIs as\nwell as key challenges that need to be addressed in future work.", "published": "2024-09-19 19:14:50", "link": "http://arxiv.org/abs/2409.13057v2", "categories": ["q-bio.QM", "cs.CL"], "primary_category": "q-bio.QM"}
{"title": "Guided Profile Generation Improves Personalization with LLMs", "abstract": "In modern commercial systems, including Recommendation, Ranking, and\nE-Commerce platforms, there is a trend towards improving customer experiences\nby incorporating Personalization context as input into Large Language Models\n(LLMs). However, LLMs often struggle to effectively parse and utilize sparse\nand complex personal context without additional processing or contextual\nenrichment, underscoring the need for more sophisticated context understanding\nmechanisms. In this work, we propose Guided Profile Generation (GPG), a general\nmethod designed to generate personal profiles in natural language. As is\nobserved, intermediate guided profile generation enables LLMs to summarize, and\nextract the important, distinctive features from the personal context into\nconcise, descriptive sentences, precisely tailoring their generation more\nclosely to an individual's unique habits and preferences. Our experimental\nresults show that GPG improves LLM's personalization ability across different\ntasks, for example, it increases 37% accuracy in predicting personal preference\ncompared to directly feeding the LLMs with raw personal context.", "published": "2024-09-19 21:29:56", "link": "http://arxiv.org/abs/2409.13093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiSHA: Dimension-Sharding Adaptation of Large Language Models with Fast\n  Convergence and Fast Computation", "abstract": "Low-Rank Adaptation (LoRA), a prominent technique within the framework of\nParameter-Efficient Fine-Tuning (PEFT), efficiently reduces the computational\nburden associated with adapting Large Language Models (LLMs) to downstream\ntasks, thereby enabling resource-constrained fine-tuning. However, existing\nresearches have shown that LoRA suffers from slow convergence. To address this\nlimitation, we introduce Dimension-Sharding Adaptation (DiSHA), which expands\nthe PEFT design space to even fewer trainable parameters and faster\nconvergence. Within DiSHA's design space, we propose Block Affine Efficient\nComputation (Bone), a computationally efficient structure that delivers both\nhigh performance and efficiency. While certain DiSHA configurations may result\nin colinear updates to weight shards, we address this with Block Affine\nTransformation (Bat), a nonlinear variant of DiSHA. Bat introduces nonlinearity\nby combining trainable matrices with original weight shards in a nonlinear\nmanner, inducing nonlinearity in matrix updates without introducing additional\nparameters. Empirical results show that Bone, under the DiSHA framework,\nconsistently outperforms LoRA variants in both Natural Language Understanding\nand Natural Language Generation tasks, with significantly improved\ncomputational efficiency. Further analysis demonstrates that BAT enhances model\ncapabilities by leveraging its nonlinear design.", "published": "2024-09-19 10:26:42", "link": "http://arxiv.org/abs/2409.15371v8", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cross-Domain Content Generation with Domain-Specific Small Language\n  Models", "abstract": "Generating domain-specific content using small language models poses\nchallenges, especially when dealing with multiple distinct datasets with\nminimal overlap. In this study, we explore methods to enable a small language\nmodel to produce coherent and relevant outputs for two different domains:\nstories (Dataset A) and recipes (Dataset B). Our initial experiments show that\ntraining individual models on each dataset yields satisfactory results, with\neach model generating appropriate content within its domain. We find that\nutilizing custom tokenizers tailored to each dataset significantly enhances\ngeneration quality compared to using a generic tokenizer. Attempts to adapt a\nsingle model to both domains using Low-Rank Adaptation (LoRA) or standard\nfine-tuning do not yield substantial results, often failing to produce\nmeaningful outputs. Moreover, full fine-tuning without freezing the model's\nexisting weights leads to catastrophic forgetting, where the model loses\npreviously learned information and only retains knowledge from the new data. To\novercome these challenges, we employ a knowledge expansion strategy: training\nonly with additional parameters. This approach enables the model to generate\nboth stories and recipes upon request, effectively handling multiple domains\nwithout suffering from catastrophic forgetting. Our findings demonstrate that\nknowledge expansion with frozen layers is an effective method for small\nlanguage models to generate domain-specific content across distinct datasets.\nThis work contributes to the development of efficient multi-domain language\nmodels and provides insights into managing catastrophic forgetting in\nsmall-scale architectures.", "published": "2024-09-19 21:45:13", "link": "http://arxiv.org/abs/2409.17171v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Controlled LLM-based Reasoning for Clinical Trial Retrieval", "abstract": "Matching patients to clinical trials demands a systematic and reasoned\ninterpretation of documents which require significant expert-level background\nknowledge, over a complex set of well-defined eligibility criteria. Moreover,\nthis interpretation process needs to operate at scale, over vast knowledge\nbases of trials. In this paper, we propose a scalable method that extends the\ncapabilities of LLMs in the direction of systematizing the reasoning over sets\nof medical eligibility criteria, evaluating it in the context of real-world\ncases. The proposed method overlays a Set-guided reasoning method for LLMs. The\nproposed framework is evaluated on TREC 2022 Clinical Trials, achieving results\nsuperior to the state-of-the-art: NDCG@10 of 0.693 and Precision@10 of 0.73.", "published": "2024-09-19 09:42:33", "link": "http://arxiv.org/abs/2409.18998v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing TinyBERT for Financial Sentiment Analysis Using GPT-Augmented\n  FinBERT Distillation", "abstract": "In the rapidly evolving field of financial sentiment analysis, the efficiency\nand accuracy of predictive models are critical due to their significant impact\non financial markets. Transformer based models like BERT and large language\nmodels (LLMs) like GPT-4, have advanced NLP tasks considerably. Despite their\nadvantages, BERT-based models face challenges with computational intensity in\nedge computing environments, and the substantial size and compute requirements\nof LLMs limit their practical deployment. This study proposes leveraging the\ngenerative capabilities of LLMs, such as GPT-4 Omni, to create synthetic,\ndomain-specific training data. This approach addresses the challenge of data\nscarcity and enhances the performance of smaller models by making them\ncompetitive with their larger counterparts. The research specifically aims to\nenhance FinBERT, a BERT model fine-tuned for financial sentiment analysis, and\ndevelop TinyFinBERT, a compact transformer model, through a structured,\ntwo-tiered knowledge distillation strategy. Using data augmented by GPT-4 Omni,\nwhich involves generating new training examples and transforming existing data,\nwe significantly improved the accuracy of FinBERT, preparing it to serve as a\nteacher model. This enhanced FinBERT then distilled knowledge to TinyFinBERT,\nemploying both GPT-4 Omni and GPT-3.5 Turbo augmented data. The distillation\nstrategy incorporated both logit and intermediate layer distillation. The\ntraining and evaluation of TinyFinBERT utilized the PhraseBank dataset and the\nFiQA 2018 Task1 dataset, achieving performance comparable to FinBERT while\nbeing substantially smaller and more efficient. This research demonstrates how\nLLMs can effectively contribute to the advancement of financial sentiment\nanalysis by enhancing the capabilities of smaller, more efficient models\nthrough innovative data augmentation and distillation techniques.", "published": "2024-09-19 10:22:23", "link": "http://arxiv.org/abs/2409.18999v1", "categories": ["cs.CL", "cs.LG", "I.2.6"], "primary_category": "cs.CL"}
{"title": "Pay Attention to What Matters", "abstract": "Despite the remarkable success of Large Language Models (LLMs), they still\nexhibit a limited capability to align their outputs to the user instructions.\nIn this work, we introduce a simple and effective method, which we name GUIDE,\nthat mechanistically increases attention scores in instruction tokens. To\nsupport this operation, we present Influence, a novel metric that highlights\nhow the user's instructions propagate through the transformer layers and impact\nthe LLM output. Our results show that GUIDE improves the accuracy of following\ninstructions 29.4 % to 60.4%, outperforming natural prompting alternatives and\nSupervised Fine-Tuning up to 1M tokens.", "published": "2024-09-19 15:26:50", "link": "http://arxiv.org/abs/2409.19001v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Robust Audiovisual Speech Recognition Models with Mixture-of-Experts", "abstract": "Visual signals can enhance audiovisual speech recognition accuracy by\nproviding additional contextual information. Given the complexity of visual\nsignals, an audiovisual speech recognition model requires robust generalization\ncapabilities across diverse video scenarios, presenting a significant\nchallenge. In this paper, we introduce EVA, leveraging the mixture-of-Experts\nfor audioVisual ASR to perform robust speech recognition for ``in-the-wild''\nvideos. Specifically, we first encode visual information into visual tokens\nsequence and map them into speech space by a lightweight projection. Then, we\nbuild EVA upon a robust pretrained speech recognition model, ensuring its\ngeneralization ability. Moreover, to incorporate visual information\neffectively, we inject visual information into the ASR model through a\nmixture-of-experts module. Experiments show our model achieves state-of-the-art\nresults on three benchmarks, which demonstrates the generalization ability of\nEVA across diverse video domains.", "published": "2024-09-19 00:08:28", "link": "http://arxiv.org/abs/2409.12370v1", "categories": ["eess.AS", "cs.CL", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Channel-Aware Domain-Adaptive Generative Adversarial Network for Robust\n  Speech Recognition", "abstract": "While pre-trained automatic speech recognition (ASR) systems demonstrate\nimpressive performance on matched domains, their performance often degrades\nwhen confronted with channel mismatch stemming from unseen recording\nenvironments and conditions. To mitigate this issue, we propose a novel\nchannel-aware data simulation method for robust ASR training. Our method\nharnesses the synergistic power of channel-extractive techniques and generative\nadversarial networks (GANs). We first train a channel encoder capable of\nextracting embeddings from arbitrary audio. On top of this, channel embeddings\nare extracted using a minimal amount of target-domain data and used to guide a\nGAN-based speech synthesizer. This synthesizer generates speech that faithfully\npreserves the phonetic content of the input while mimicking the channel\ncharacteristics of the target domain. We evaluate our method on the challenging\nHakka Across Taiwan (HAT) and Taiwanese Across Taiwan (TAT) corpora, achieving\nrelative character error rate (CER) reductions of 20.02% and 9.64%,\nrespectively, compared to the baselines. These results highlight the efficacy\nof our channel-aware data simulation method for bridging the gap between\nsource- and target-domain acoustics.", "published": "2024-09-19 01:02:31", "link": "http://arxiv.org/abs/2409.12386v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Familiarity-Aware Evidence Compression for Retrieval-Augmented\n  Generation", "abstract": "Retrieval-augmented generation (RAG) improves large language models (LMs) by\nincorporating non-parametric knowledge through evidence retrieved from external\nsources. However, it often struggles to cope with inconsistent and irrelevant\ninformation that can distract the LM from its tasks, especially when multiple\nevidence pieces are required. While compressing the retrieved evidence with a\ncompression model aims to address this issue, the compressed evidence may still\nbe unfamiliar to the target model used for downstream tasks, potentially\nfailing to utilize the evidence effectively. We propose FaviComp\n(Familarity-Aware Evidence Compression), a novel training-free evidence\ncompression technique that makes retrieved evidence more familiar to the target\nmodel, while seamlessly integrating parametric knowledge from the model.\nExperimental results show that FaviComp consistently outperforms most recent\nevidence compression baselines across multiple open-domain QA datasets,\nimproving accuracy by up to 28.1% while achieving high compression rates.\nAdditionally, we demonstrate the effective integration of both parametric and\nnon-parametric knowledge during evidence compression.", "published": "2024-09-19 05:14:55", "link": "http://arxiv.org/abs/2409.12468v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoMode-ASR: Learning to Select ASR Systems for Better Quality and Cost", "abstract": "We present AutoMode-ASR, a novel framework that effectively integrates\nmultiple ASR systems to enhance the overall transcription quality while\noptimizing cost. The idea is to train a decision model to select the optimal\nASR system for each segment based solely on the audio input before running the\nsystems. We achieve this by ensembling binary classifiers determining the\npreference between two systems. These classifiers are equipped with various\nfeatures, such as audio embeddings, quality estimation, and signal properties.\nAdditionally, we demonstrate how using a quality estimator can further improve\nperformance with minimal cost increase. Experimental results show a relative\nreduction in WER of 16.2%, a cost saving of 65%, and a speed improvement of\n75%, compared to using a single-best model for all segments. Our framework is\ncompatible with commercial and open-source black-box ASR systems as it does not\nrequire changes in model codes.", "published": "2024-09-19 05:34:52", "link": "http://arxiv.org/abs/2409.12476v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs", "abstract": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.", "published": "2024-09-19 06:09:56", "link": "http://arxiv.org/abs/2409.12490v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large\n  Language Model Reasoning", "abstract": "Iterative human engagement is a common and effective means of leveraging the\nadvanced language processing power of large language models (LLMs). Using\nwell-structured prompts in a conversational manner, human users can effectively\ninfluence an LLM to develop more thoughtful and accurate responses. Motivated\nby this insight, we propose the Iteration of Thought (IoT) framework for\nenhancing LLM responses by generating \"thought\"-provoking prompts vis a vis an\ninput query and the current iteration of an LLM's response. Unlike static or\nsemi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT),\nIoT adapts its reasoning path dynamically, based on evolving context, and\nwithout generating alternate explorative thoughts which are ultimately\ndiscarded. The three components of the IoT framework are (1) an Inner Dialogue\nAgent (IDA) responsible for generating instructive, context-specific prompts;\n(2) an LLM Agent (LLMA) that processes these prompts to refine its responses;\nand (3) an iterative prompting loop that implements a conversation between the\nformer two components. We introduce two variants of our framework: Autonomous\nIteration of Thought (AIoT), where an LLM decides when to stop iterating, and\nGuided Iteration of Thought (GIoT), which always forces a fixed number\niterations. We investigate the performance of IoT across various datasets,\nspanning complex reasoning tasks from the GPQA dataset, explorative\nproblem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop\nquestion answering from the HotpotQA dataset. Our results show that IoT\nrepresents a viable paradigm for autonomous response refinement in LLMs,\nshowcasing significant improvements over CoT and thereby enabling more adaptive\nand efficient reasoning systems that minimize human intervention.", "published": "2024-09-19 09:44:17", "link": "http://arxiv.org/abs/2409.12618v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Enhancing E-commerce Product Title Translation with Retrieval-Augmented\n  Generation and Large Language Models", "abstract": "E-commerce stores enable multilingual product discovery which require\naccurate product title translation. Multilingual large language models (LLMs)\nhave shown promising capacity to perform machine translation tasks, and it can\nalso enhance and translate product titles cross-lingually in one step. However,\nproduct title translation often requires more than just language conversion\nbecause titles are short, lack context, and contain specialized terminology.\nThis study proposes a retrieval-augmented generation (RAG) approach that\nleverages existing bilingual product information in e-commerce by retrieving\nsimilar bilingual examples and incorporating them as few-shot prompts to\nenhance LLM-based product title translation. Experiment results show that our\nproposed RAG approach improve product title translation quality with chrF score\ngains of up to 15.3% for language pairs where the LLM has limited proficiency.", "published": "2024-09-19 16:23:42", "link": "http://arxiv.org/abs/2409.12880v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Smart: Accelerating Large Language Model Pre-training with Small\n  Model Initialization", "abstract": "The pre-training phase of language models often begins with randomly\ninitialized parameters. With the current trends in scaling models, training\ntheir large number of parameters can be extremely slow and costly. In contrast,\nsmall language models are less expensive to train, but they often cannot\nachieve the accuracy of large models. In this paper, we explore an intriguing\nidea to connect these two different regimes: Can we develop a method to\ninitialize large language models using smaller pre-trained models? Will such\ninitialization bring any benefits in terms of training time and final accuracy?\nIn this paper, we introduce HyperCloning, a method that can expand the\nparameters of a pre-trained language model to those of a larger model with\nincreased hidden dimensions. Our method ensures that the larger model retains\nthe functionality of the smaller model. As a result, the larger model already\ninherits the predictive power and accuracy of the smaller model before the\ntraining starts. We demonstrate that training such an initialized model results\nin significant savings in terms of GPU hours required for pre-training large\nlanguage models.", "published": "2024-09-19 16:50:26", "link": "http://arxiv.org/abs/2409.12903v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Geometric Interpretation of Layer Normalization and a Comparative\n  Analysis with RMSNorm", "abstract": "This paper presents a novel geometric interpretation of LayerNorm and\nexplores how LayerNorm influences the norm and orientation of hidden vectors in\nthe representation space. With these geometric insights, we prepare the\nfoundation for comparing LayerNorm with RMSNorm. We show that the definition of\nLayerNorm is innately linked to the uniform vector, defined as $\\boldsymbol{1}\n= [1, 1, 1, 1, \\cdots, 1]^T \\in \\mathbb{R}^d$. We then show that the\nstandardization step in LayerNorm can be understood in three simple steps: (i)\nremove the component of a vector along the uniform vector, (ii) normalize the\nremaining vector, and (iii) scale the resultant vector by $\\sqrt{d}$, where $d$\nis the dimensionality of the representation space. We also provide additional\ninsights into how LayerNorm operates at inference time. Finally, we compare the\nhidden representations of LayerNorm-based LLMs with models trained using\nRMSNorm and show that all LLMs naturally operate orthogonal to the uniform\nvector at inference time, that is, on average they do not have a component\nalong the uniform vector during inference. This presents the first mechanistic\nevidence that removing the component along the uniform vector in LayerNorm is a\nredundant step. These results advocate for using RMSNorm over LayerNorm which\nis also more computationally efficient.", "published": "2024-09-19 17:58:07", "link": "http://arxiv.org/abs/2409.12951v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MURI: High-Quality Instruction Tuning Datasets for Low-Resource\n  Languages via Reverse Instructions", "abstract": "Instruction tuning enhances large language models (LLMs) by aligning them\nwith human preferences across diverse tasks. Traditional approaches to create\ninstruction tuning datasets face serious challenges for low-resource languages\ndue to their dependence on data annotation. This work introduces a novel\nmethod, Multilingual Reverse Instructions (MURI), which generates high-quality\ninstruction tuning datasets for low-resource languages without requiring human\nannotators or pre-existing multilingual models. Utilizing reverse instructions\nand a translation pipeline, MURI produces instruction-output pairs from\nexisting human-written texts in low-resource languages. This method ensures\ncultural relevance and diversity by sourcing texts from different native\ndomains and applying filters to eliminate inappropriate content. Our dataset,\nMURI-IT, includes more than 2 million instruction-output pairs across 200\nlanguages. Evaluation by native speakers and fine-tuning experiments with mT5\nmodels demonstrate the approach's effectiveness for both NLU and open-ended\ngeneration. We publicly release datasets and models at\nhttps://github.com/akoksal/muri.", "published": "2024-09-19 17:59:20", "link": "http://arxiv.org/abs/2409.12958v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal\n  Search Engines", "abstract": "The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io", "published": "2024-09-19 17:59:45", "link": "http://arxiv.org/abs/2409.12959v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "CLAIR-A: Leveraging Large Language Models to Judge Audio Captions", "abstract": "The Automated Audio Captioning (AAC) task asks models to generate natural\nlanguage descriptions of an audio input. Evaluating these machine-generated\naudio captions is a complex task that requires considering diverse factors,\namong them, auditory scene understanding, sound-object inference, temporal\ncoherence, and the environmental context of the scene. While current methods\nfocus on specific aspects, they often fail to provide an overall score that\naligns well with human judgment. In this work, we propose CLAIR-A, a simple and\nflexible method that leverages the zero-shot capabilities of large language\nmodels (LLMs) to evaluate candidate audio captions by directly asking LLMs for\na semantic distance score. In our evaluations, CLAIR-A better predicts human\njudgements of quality compared to traditional metrics, with a 5.8% relative\naccuracy improvement compared to the domain-specific FENSE metric and up to 11%\nover the best general-purpose measure on the Clotho-Eval dataset. Moreover,\nCLAIR-A offers more transparency by allowing the language model to explain the\nreasoning behind its scores, with these explanations rated up to 30% better by\nhuman evaluators than those provided by baseline methods. CLAIR-A is made\npublicly available at https://github.com/DavidMChan/clair-a.", "published": "2024-09-19 17:59:52", "link": "http://arxiv.org/abs/2409.12962v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLM Surgery: Efficient Knowledge Unlearning and Editing in Large\n  Language Models", "abstract": "Large language models (LLMs) have revolutionized various domains, yet their\nutility comes with significant challenges related to outdated or problematic\nknowledge embedded during pretraining. This paper addresses the challenge of\nmodifying LLMs to unlearn problematic and outdated information while\nefficiently integrating new knowledge without retraining from scratch. Here, we\npropose LLM Surgery, a framework to efficiently modify LLM behaviour by\noptimizing a three component objective function that: (1) Performs reverse\ngradient on unlearning dataset (problematic and outdated information), (2)\nPerforms gradient descent on the update dataset (new and updated information),\nand (3) Minimizes the KL divergence on the retain dataset (small subset of\nunchanged text), ensuring alignment between pretrained and modified model\noutputs. Due to the lack of publicly available datasets specifically tailored\nfor our novel task, we compiled a new dataset and an evaluation benchmark.\nUsing Llama2-7B, we demonstrate that LLM Surgery can achieve significant\nforgetting on the unlearn set, a 20\\% increase in accuracy on the update set,\nand maintain performance on the retain set.", "published": "2024-09-19 19:07:01", "link": "http://arxiv.org/abs/2409.13054v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Embedding Geometries of Contrastive Language-Image Pre-Training", "abstract": "Since the publication of CLIP, the approach of using InfoNCE loss for\ncontrastive pre-training has become widely popular for bridging two or more\nmodalities. Despite its wide adoption, CLIP's original design choices of L2\nnormalization and cosine similarity logit have rarely been revisited. We have\nsystematically experimented with alternative geometries and softmax logits for\nlanguage-image pre-training and identified that variants with intuitive\nEuclidean geometry, Euclidean CLIP (EuCLIP), match or exceed the performance of\nCLIP and support hierarchical relationships at least as well as more\ncomplicated hyperbolic alternative.", "published": "2024-09-19 20:34:22", "link": "http://arxiv.org/abs/2409.13079v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Personalized Speech Recognition for Children with Test-Time Adaptation", "abstract": "Accurate automatic speech recognition (ASR) for children is crucial for\neffective real-time child-AI interaction, especially in educational\napplications. However, off-the-shelf ASR models primarily pre-trained on adult\ndata tend to generalize poorly to children's speech due to the data domain\nshift from adults to children. Recent studies have found that supervised\nfine-tuning on children's speech data can help bridge this domain shift, but\nhuman annotations may be impractical to obtain for real-world applications and\nadaptation at training time can overlook additional domain shifts occurring at\ntest time. We devised a novel ASR pipeline to apply unsupervised test-time\nadaptation (TTA) methods for child speech recognition, so that ASR models\npre-trained on adult speech can be continuously adapted to each child speaker\nat test time without further human annotations. Our results show that ASR\nmodels adapted with TTA methods significantly outperform the unadapted\noff-the-shelf ASR baselines both on average and statistically across individual\nchild speakers. Our analysis also discovered significant data domain shifts\nboth between child speakers and within each child speaker, which further\nmotivates the need for test-time adaptation.", "published": "2024-09-19 21:40:07", "link": "http://arxiv.org/abs/2409.13095v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Are Large Language Models Good Essay Graders?", "abstract": "We evaluate the effectiveness of Large Language Models (LLMs) in assessing\nessay quality, focusing on their alignment with human grading. More precisely,\nwe evaluate ChatGPT and Llama in the Automated Essay Scoring (AES) task, a\ncrucial natural language processing (NLP) application in Education. We consider\nboth zero-shot and few-shot learning and different prompting approaches. We\ncompare the numeric grade provided by the LLMs to human rater-provided scores\nutilizing the ASAP dataset, a well-known benchmark for the AES task. Our\nresearch reveals that both LLMs generally assign lower scores compared to those\nprovided by the human raters; moreover, those scores do not correlate well with\nthose provided by the humans. In particular, ChatGPT tends to be harsher and\nfurther misaligned with human evaluations than Llama. We also experiment with a\nnumber of essay features commonly used by previous AES methods, related to\nlength, usage of connectives and transition words, and readability metrics,\nincluding the number of spelling and grammar mistakes. We find that, generally,\nnone of these features correlates strongly with human or LLM scores. Finally,\nwe report results on Llama 3, which are generally better across the board, as\nexpected. Overall, while LLMs do not seem an adequate replacement for human\ngrading, our results are somewhat encouraging for their use as a tool to assist\nhumans in the grading of written essays in the future.", "published": "2024-09-19 23:20:49", "link": "http://arxiv.org/abs/2409.13120v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Case Study of Web App Coding with OpenAI Reasoning Models", "abstract": "This paper presents a case study of coding tasks by the latest reasoning\nmodels of OpenAI, i.e. o1-preview and o1-mini, in comparison with other\nfrontier models. The o1 models deliver SOTA results for WebApp1K, a single-task\nbenchmark. To this end, we introduce WebApp1K-Duo, a harder benchmark doubling\nnumber of tasks and test cases. The new benchmark causes the o1 model\nperformances to decline significantly, falling behind Claude 3.5. Moreover,\nthey consistently fail when confronted with atypical yet correct test cases, a\ntrap non-reasoning models occasionally avoid. We hypothesize that the\nperformance variability is due to instruction comprehension. Specifically, the\nreasoning mechanism boosts performance when all expectations are captured,\nmeanwhile exacerbates errors when key expectations are missed, potentially\nimpacted by input lengths. As such, we argue that the coding success of\nreasoning models hinges on the top-notch base model and SFT to ensure\nmeticulous adherence to instructions.", "published": "2024-09-19 06:58:02", "link": "http://arxiv.org/abs/2409.13773v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on\n  Curiosity-Driven Questioning", "abstract": "Large language models (LLMs) can store a massive amount of knowledge, yet\ntheir potential to acquire new knowledge remains unknown. We propose a novel\nevaluation framework that evaluates this capability. This framework prompts\nLLMs to generate questions about a statement introducing scientific knowledge,\nsimulating a curious person when facing the statement for the first time. We\nscore the qualities of the generated questions, thereby evaluating the\nknowledge acquisition potential of the LLM. We apply controlled ablation\nstudies to validate our scoring procedures. Additionally, we created a\nsynthetic dataset consisting of 1101 statements in physics, chemistry, and\nmaths with distinct levels of difficulties, 300 general knowledge statements,\nand 567 incorrect statements. Human evaluations were conducted to validate our\nmodel assessments, achieving an approximate weighted Cohen's kappa of 0.7 on\nall three metrics considered. We find that while large models like GPT-4 and\nMistral 8x7b are adept at generating coherent and relevant questions, the\nsmaller Phi-2 model is equally or more effective. This indicates that size does\nnot solely determine a model's knowledge acquisition potential. The proposed\nframework quantifies a critical model capability that was commonly overlooked\nand opens up research opportunities for developing more knowledgeable AI\nsystems", "published": "2024-09-19 22:12:16", "link": "http://arxiv.org/abs/2409.17172v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PropaInsight: Toward Deeper Understanding of Propaganda in Terms of\n  Techniques, Appeals, and Intent", "abstract": "Propaganda plays a critical role in shaping public opinion and fueling\ndisinformation. While existing research primarily focuses on identifying\npropaganda techniques, it lacks the ability to capture the broader motives and\nthe impacts of such content. To address these challenges, we introduce\npropainsight, a conceptual framework grounded in foundational social science\nresearch, which systematically dissects propaganda into techniques, arousal\nappeals, and underlying intent. propainsight offers a more granular\nunderstanding of how propaganda operates across different contexts.\nAdditionally, we present propagaze, a novel dataset that combines\nhuman-annotated data with high-quality synthetic data generated through a\nmeticulously designed pipeline. Our experiments show that off-the-shelf LLMs\nstruggle with propaganda analysis, but training with propagaze significantly\nimproves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span\nIoU in technique identification and 66.2% higher BertScore in appeal analysis\ncompared to 1-shot GPT-4-Turbo. Moreover, propagaze complements limited\nhuman-annotated data in data-sparse and cross-domain scenarios, showing its\npotential for comprehensive and generalizable propaganda analysis.", "published": "2024-09-19 06:28:18", "link": "http://arxiv.org/abs/2409.18997v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Strategic Collusion of LLM Agents: Market Division in Multi-Commodity\n  Competitions", "abstract": "Machine-learning technologies are seeing increased deployment in real-world\nmarket scenarios. In this work, we explore the strategic behaviors of large\nlanguage models (LLMs) when deployed as autonomous agents in multi-commodity\nmarkets, specifically within Cournot competition frameworks. We examine whether\nLLMs can independently engage in anti-competitive practices such as collusion\nor, more specifically, market division. Our findings demonstrate that LLMs can\neffectively monopolize specific commodities by dynamically adjusting their\npricing and resource allocation strategies, thereby maximizing profitability\nwithout direct human input or explicit collusion commands. These results pose\nunique challenges and opportunities for businesses looking to integrate AI into\nstrategic roles and for regulatory bodies tasked with maintaining fair and\ncompetitive markets. The study provides a foundation for further exploration\ninto the ramifications of deferring high-stakes decisions to LLM-based agents.", "published": "2024-09-19 20:10:40", "link": "http://arxiv.org/abs/2410.00031v1", "categories": ["cs.GT", "cs.AI", "cs.CL", "q-fin.CP"], "primary_category": "cs.GT"}
{"title": "System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam", "abstract": "The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch students' average of 40.63 points. Neither model had\naccess to the exam figures. Since there was a risk of model contami-nation\n(i.e., the knowledge cutoff for o1-preview and GPT-4o was after the exam was\npublished online), we repeated the procedure with a new Mathematics B exam that\nwas published after the cutoff date. The results again indicated that\no1-preview performed strongly (97.8th percentile), which suggests that\ncontamination was not a factor. We also show that there is some variability in\nthe output of o1-preview, which means that sometimes there is 'luck' (the\nanswer is correct) or 'bad luck' (the output has diverged into something that\nis incorrect). We demonstrate that the self-consistency approach, where\nrepeated prompts are given and the most common answer is selected, is a useful\nstrategy for identifying the correct answer. It is concluded that while\nOpenAI's new model series holds great potential, certain risks must be\nconsidered.", "published": "2024-09-19 19:48:31", "link": "http://arxiv.org/abs/2410.07114v5", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal\n  Reasoning with Large Language Models", "abstract": "Cross-modal reasoning (CMR), the intricate process of synthesizing and\ndrawing inferences across divergent sensory modalities, is increasingly\nrecognized as a crucial capability in the progression toward more sophisticated\nand anthropomorphic artificial intelligence systems. Large Language Models\n(LLMs) represent a class of AI algorithms specifically engineered to parse,\nproduce, and engage with human language on an extensive scale. The recent trend\nof deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches\nfor enhancing their effectiveness. This survey offers a nuanced exposition of\ncurrent methodologies applied in CMR using LLMs, classifying these into a\ndetailed three-tiered taxonomy. Moreover, the survey delves into the principal\ndesign strategies and operational techniques of prototypical models within this\ndomain. Additionally, it articulates the prevailing challenges associated with\nthe integration of LLMs in CMR and identifies prospective research directions.\nTo sum up, this survey endeavors to expedite progress within this burgeoning\nfield by endowing scholars with a holistic and detailed vista, showcasing the\nvanguard of current research whilst pinpointing potential avenues for\nadvancement. An associated GitHub repository that collects the relevant papers\ncan be found at\nhttps://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs", "published": "2024-09-19 02:51:54", "link": "http://arxiv.org/abs/2409.18996v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM", "A.1"], "primary_category": "cs.CL"}
{"title": "DeFT-Mamba: Universal Multichannel Sound Separation and Polyphonic Audio\n  Classification", "abstract": "This paper presents a framework for universal sound separation and polyphonic\naudio classification, addressing the challenges of separating and classifying\nindividual sound sources in a multichannel mixture. The proposed framework,\nDeFT-Mamba, utilizes the dense frequency-time attentive network (DeFTAN)\ncombined with Mamba to extract sound objects, capturing the local\ntime-frequency relations through gated convolution block and the global\ntime-frequency relations through position-wise Hybrid Mamba. DeFT-Mamba\nsurpasses existing separation and classification networks by a large margin,\nparticularly in complex scenarios involving in-class polyphony. Additionally, a\nclassification-based source counting method is introduced to identify the\npresence of multiple sources, outperforming conventional threshold-based\napproaches. Separation refinement tuning is also proposed to improve\nperformance further. The proposed framework is trained and tested on a\nmultichannel universal sound separation dataset developed in this work,\ndesigned to mimic realistic environments with moving sources and varying onsets\nand offsets of polyphonic events.", "published": "2024-09-19 02:24:12", "link": "http://arxiv.org/abs/2409.12413v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AudioEditor: A Training-Free Diffusion-Based Audio Editing Framework", "abstract": "Diffusion-based text-to-audio (TTA) generation has made substantial progress,\nleveraging latent diffusion model (LDM) to produce high-quality, diverse and\ninstruction-relevant audios. However, beyond generation, the task of audio\nediting remains equally important but has received comparatively little\nattention. Audio editing tasks face two primary challenges: executing precise\nedits and preserving the unedited sections. While workflows based on LDMs have\neffectively addressed these challenges in the field of image processing,\nsimilar approaches have been scarcely applied to audio editing. In this paper,\nwe introduce AudioEditor, a training-free audio editing framework built on the\npretrained diffusion-based TTA model. AudioEditor incorporates Null-text\nInversion and EOT-suppression methods, enabling the model to preserve original\naudio features while executing accurate edits. Comprehensive objective and\nsubjective experiments validate the effectiveness of AudioEditor in delivering\nhigh-quality audio edits. Code and demo can be found at\nhttps://github.com/NKU-HLT/AudioEditor.", "published": "2024-09-19 05:07:05", "link": "http://arxiv.org/abs/2409.12466v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Geometry-Constrained EEG Channel Selection for Brain-Assisted Speech\n  Enhancement", "abstract": "Brain-assisted speech enhancement (BASE) aims to extract the target speaker\nin complex multi-talker scenarios using electroencephalogram (EEG) signals as\nan assistive modality, as the auditory attention of the listener can be decoded\nfrom electroneurographic signals of the brain. This facilitates a potential\nintegration of EEG electrodes with listening devices to improve the speech\nintelligibility of hearing-impaired listeners, which was shown by the\nrecently-proposed BASEN model. As in general the multichannel EEG signals are\nhighly correlated and some are even irrelevant to listening, blindly\nincorporating all EEG channels would lead to a high economic and computational\ncost. In this work, we therefore propose a geometry-constrained EEG channel\nselection approach for BASE. We design a new weighted multi-dilation temporal\nconvolutional network (WDTCN) as the backbone to replace the Conv-TasNet in\nBASEN. Given a raw channel set that is defined by the electrode geometry for\nfeasible integration, we then propose a geometry-constrained convolutional\nregularization selection (GC-ConvRS) module for WD-TCN to find an informative\nEEG subset. Experimental results on a public dataset show the superiority of\nthe proposed WD-TCN over BASEN. The GC-ConvRS can further refine the useful EEG\nsubset subject to the geometry constraint, resulting in a better trade-off\nbetween performance and integration cost.", "published": "2024-09-19 07:20:13", "link": "http://arxiv.org/abs/2409.12520v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SoundBeam meets M2D: Target Sound Extraction with Audio Foundation Model", "abstract": "Target sound extraction (TSE) consists of isolating a desired sound from a\nmixture of arbitrary sounds using clues to identify it. A TSE system requires\nsolving two problems at once, identifying the target source and extracting the\ntarget signal from the mixture. For increased practicability, the same system\nshould work with various types of sound. The duality of the problem and the\nwide variety of sounds make it challenging to train a powerful TSE system from\nscratch. In this paper, to tackle this problem, we explore using a pre-trained\naudio foundation model that can provide rich feature representations of sounds\nwithin a TSE system. We chose the masked-modeling duo (M2D) foundation model,\nwhich appears especially suited for the TSE task, as it is trained using a dual\nobjective consisting of sound-label predictions and improved masked prediction.\nThese objectives are related to sound identification and the signal extraction\nproblems of TSE. We propose a new TSE system that integrates the feature\nrepresentation from M2D into SoundBeam, which is a strong TSE system that can\nexploit both target sound class labels and pre-recorded enrollments (or audio\nqueries) as clues. We show experimentally that using M2D can increase\nextraction performance, especially when employing enrollment clues.", "published": "2024-09-19 07:44:07", "link": "http://arxiv.org/abs/2409.12528v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FruitsMusic: A Real-World Corpus of Japanese Idol-Group Songs", "abstract": "This study presents FruitsMusic, a metadata corpus of Japanese idol-group\nsongs in the real world, precisely annotated with who sings what and when.\nJapanese idol-group songs, vital to Japanese pop culture, feature a unique\nvocal arrangement style, where songs are divided into several segments, and a\nspecific individual or multiple singers are assigned to each segment. To\nenhance singer diarization methods for recognizing such structures, we\nconstructed FruitsMusic as a resource using 40 music videos of Japanese idol\ngroups from YouTube. The corpus includes detailed annotations, covering songs\nacross various genres, division and assignment styles, and groups ranging from\n4 to 9 members. FruitsMusic also facilitates the development of various music\ninformation retrieval techniques, such as lyrics transcription and singer\nidentification, benefiting not only Japanese idol-group songs but also a wide\nrange of songs featuring single or multiple singers from various cultures. This\npaper offers a comprehensive overview of FruitsMusic, including its creation\nmethodology and unique characteristics compared to conversational speech.\nAdditionally, this paper evaluates the efficacy of current methods for singer\nembedding extraction and diarization in challenging real-world conditions using\nFruitsMusic. Furthermore, this paper examines potential improvements in\nautomatic diarization performance through evaluating human performance.", "published": "2024-09-19 08:14:20", "link": "http://arxiv.org/abs/2409.12549v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AudioComposer: Towards Fine-grained Audio Generation with Natural\n  Language Descriptions", "abstract": "Current Text-to-audio (TTA) models mainly use coarse text descriptions as\ninputs to generate audio, which hinders models from generating audio with\nfine-grained control of content and style. Some studies try to improve the\ngranularity by incorporating additional frame-level conditions or control\nnetworks. However, this usually leads to complex system design and difficulties\ndue to the requirement for reference frame-level conditions. To address these\nchallenges, we propose AudioComposer, a novel TTA generation framework that\nrelies solely on natural language descriptions (NLDs) to provide both content\nspecification and style control information. To further enhance audio\ngenerative modeling, we employ flow-based diffusion transformers with the\ncross-attention mechanism to incorporate text descriptions effectively into\naudio generation processes, which can not only simultaneously consider the\ncontent and style information in the text inputs, but also accelerate\ngeneration compared to other architectures. Furthermore, we propose a novel and\ncomprehensive automatic data simulation pipeline to construct data with\nfine-grained text descriptions, which significantly alleviates the problem of\ndata scarcity in the area. Experiments demonstrate the effectiveness of our\nframework using solely NLDs as inputs for content specification and style\ncontrol. The generation quality and controllability surpass state-of-the-art\nTTA models, even with a smaller model size.", "published": "2024-09-19 08:33:00", "link": "http://arxiv.org/abs/2409.12560v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector\n  Quantization", "abstract": "Built upon vector quantization (VQ), discrete audio codec models have\nachieved great success in audio compression and auto-regressive audio\ngeneration. However, existing models face substantial challenges in perceptual\nquality and signal distortion, especially when operating in extremely low\nbandwidth, rooted in the sensitivity of the VQ codebook to noise. This\ndegradation poses significant challenges for several downstream tasks, such as\ncodec-based speech synthesis. To address this issue, we propose a novel VQ\nmethod, Normal Distribution-based Vector Quantization (NDVQ), by introducing an\nexplicit margin between the VQ codes via learning a variance. Specifically, our\napproach involves mapping the waveform to a latent space and quantizing it by\nselecting the most likely normal distribution, with each codebook entry\nrepresenting a unique normal distribution defined by its mean and variance.\nUsing these distribution-based VQ codec codes, a decoder reconstructs the input\nwaveform. NDVQ is trained with additional distribution-related losses,\nalongside reconstruction and discrimination losses. Experiments demonstrate\nthat NDVQ outperforms existing audio compression baselines, such as EnCodec, in\nterms of audio quality and zero-shot TTS, particularly in very low bandwidth\nscenarios.", "published": "2024-09-19 12:41:30", "link": "http://arxiv.org/abs/2409.12717v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentangling Speakers in Multi-Talker Speech Recognition with\n  Speaker-Aware CTC", "abstract": "Multi-talker speech recognition (MTASR) faces unique challenges in\ndisentangling and transcribing overlapping speech. To address these challenges,\nthis paper investigates the role of Connectionist Temporal Classification (CTC)\nin speaker disentanglement when incorporated with Serialized Output Training\n(SOT) for MTASR. Our visualization reveals that CTC guides the encoder to\nrepresent different speakers in distinct temporal regions of acoustic\nembeddings. Leveraging this insight, we propose a novel Speaker-Aware CTC\n(SACTC) training objective, based on the Bayes risk CTC framework. SACTC is a\ntailored CTC variant for multi-talker scenarios, it explicitly models speaker\ndisentanglement by constraining the encoder to represent different speakers'\ntokens at specific time frames. When integrated with SOT, the SOT-SACTC model\nconsistently outperforms standard SOT-CTC across various degrees of speech\noverlap. Specifically, we observe relative word error rate reductions of 10%\noverall and 15% on low-overlap speech. This work represents an initial\nexploration of CTC-based enhancements for MTASR tasks, offering a new\nperspective on speaker disentanglement in multi-talker speech recognition. The\ncode is available at https://github.com/kjw11/Speaker-Aware-CTC.", "published": "2024-09-19 01:26:33", "link": "http://arxiv.org/abs/2409.12388v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multichannel-to-Multichannel Target Sound Extraction Using Direction and\n  Timestamp Clues", "abstract": "We propose a multichannel-to-multichannel target sound extraction (M2M-TSE)\nframework for separating multichannel target signals from a multichannel\nmixture of sound sources. Target sound extraction (TSE) isolates a specific\ntarget signal using user-provided clues, typically focusing on single-channel\nextraction with class labels or temporal activation maps. However, to preserve\nand utilize spatial information in multichannel audio signals, it is essential\nto extract multichannel signals of a target sound source. Moreover, the clue\nfor extraction can also include spatial or temporal cues like\ndirection-of-arrival (DoA) or timestamps of source activation. To address these\nchallenges, we present an M2M framework that extracts a multichannel sound\nsignal based on spatio-temporal clues. We demonstrate that our\ntransformer-based architecture can successively accomplish the M2M-TSE task for\nmultichannel signals synthesized from audio signals of diverse classes in\ndifferent room environments. Furthermore, we show that the multichannel\nextraction task introduces sufficient inductive bias in the DNN, allowing it to\ndirectly handle DoA clues without utilizing hand-crafted spatial features.", "published": "2024-09-19 02:30:49", "link": "http://arxiv.org/abs/2409.12415v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech-Declipping Transformer with Complex Spectrogram and Learnerble\n  Temporal Features", "abstract": "We present a transformer-based speech-declipping model that effectively\nrecovers clipped signals across a wide range of input signal-to-distortion\nratios (SDRs). While recent time-domain deep neural network (DNN)-based\ndeclippers have outperformed traditional handcrafted and spectrogram-based DNN\napproaches, they still struggle with low-SDR inputs. To address this, we\nincorporate a transformer-based architecture that operates in the\ntime-frequency (TF) domain. The TF-transformer architecture has demonstrated\nremarkable performance in the speech enhancement task for low-SDR signals but\ncannot be optimal for the time-domain artifact like clipping. To overcome the\nlimitations of spectrogram-based DNNs, we design an extra convolutional block\nthat directly extracts temporal features from time-domain waveforms. The joint\nanalysis of complex spectrogram and learned temporal features allows the model\nto improve performance on both high- and low-SDR inputs. Our approach also\npreserves the unclipped portions of the speech signal during processing,\npreventing degradation typically seen when only spectral information is used.\nIn evaluations on the VoiceBank-DEMAND and DNS challenge datasets, the proposed\nmodel consistently outperformed state-of-the-art (SOTA) declipping models\nacross various metrics, demonstrating its robustness and generalizability.", "published": "2024-09-19 02:35:40", "link": "http://arxiv.org/abs/2409.12416v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Lightweight and Real-Time Binaural Speech Enhancement Model with\n  Spatial Cues Preservation", "abstract": "Binaural speech enhancement (BSE) aims to jointly improve the speech quality\nand intelligibility of noisy signals received by hearing devices and preserve\nthe spatial cues of the target for natural listening. Existing methods often\nsuffer from the compromise between noise reduction (NR) capacity and spatial\ncues preservation (SCP) accuracy and a high computational demand in complex\nacoustic scenes. In this work, we present a learning-based lightweight binaural\ncomplex convolutional network (LBCCN), which excels in NR by filtering\nlow-frequency bands and keeping the rest. Additionally, our approach explicitly\nincorporates the estimation of interchannel relative acoustic transfer function\nto ensure the spatial cues fidelity and speech clarity. Results show that the\nproposed LBCCN can achieve a comparable NR performance to state-of-the-art\nmethods under fixed-speaker conditions, but with a much lower computational\ncost and a certain degree of SCP capability. The reproducible code and audio\nexamples are available at https://github.com/jywanng/LBCCN.", "published": "2024-09-19 03:52:50", "link": "http://arxiv.org/abs/2409.12444v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hidden in Plain Sound: Environmental Backdoor Poisoning Attacks on\n  Whisper, and Mitigations", "abstract": "Thanks to the popularisation of transformer-based models, speech recognition\n(SR) is gaining traction in various application fields, such as industrial and\nrobotics environments populated with mission-critical devices. While\ntransformer-based SR can provide various benefits for simplifying human-machine\ninterfacing, the research on the cybersecurity aspects of these models is\nlacklustre. In particular, concerning backdoor poisoning attacks. In this\npaper, we propose a new poisoning approach that maps different environmental\ntrigger sounds to target phrases of different lengths, during the fine-tuning\nphase. We test our approach on Whisper, one of the most popular\ntransformer-based SR model, showing that it is highly vulnerable to our attack,\nunder several testing conditions. To mitigate the attack proposed in this\npaper, we investigate the use of Silero VAD, a state-of-the-art voice activity\ndetection (VAD) model, as a defence mechanism. Our experiments show that it is\npossible to use VAD models to filter out malicious triggers and mitigate our\nattacks, with a varying degree of success, depending on the type of trigger\nsound and testing conditions.", "published": "2024-09-19 08:21:52", "link": "http://arxiv.org/abs/2409.12553v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Exploring bat song syllable representations in self-supervised audio\n  encoders", "abstract": "How well can deep learning models trained on human-generated sounds\ndistinguish between another species' vocalization types? We analyze the\nencoding of bat song syllables in several self-supervised audio encoders, and\nfind that models pre-trained on human speech generate the most distinctive\nrepresentations of different syllable types. These findings form first steps\ntowards the application of cross-species transfer learning in bat bioacoustics,\nas well as an improved understanding of out-of-distribution signal processing\nin audio encoder models.", "published": "2024-09-19 10:09:31", "link": "http://arxiv.org/abs/2409.12634v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "$\\text{M}^\\text{6}(\\text{GPT})^\\text{3}$: Generating Multitrack\n  Modifiable Multi-Minute MIDI Music from Text using Genetic algorithms,\n  Probabilistic methods and GPT Models in any Progression and Time signature", "abstract": "This work introduces the $\\text{M}^\\text{6}(\\text{GPT})^\\text{3}$ composer\nsystem, capable of generating complete, multi-minute musical compositions with\ncomplex structures in any time signature, in the MIDI domain from input\ndescriptions in natural language. The system utilizes an autoregressive\ntransformer language model to map natural language prompts to composition\nparameters in JSON format. The defined structure includes time signature,\nscales, chord progressions, and valence-arousal values, from which\naccompaniment, melody, bass, motif, and percussion tracks are created. We\npropose a genetic algorithm for the generation of melodic elements. The\nalgorithm incorporates mutations with musical significance and a fitness\nfunction based on normal distribution and predefined musical feature values.\nThe values adaptively evolve, influenced by emotional parameters and distinct\nplaying styles. The system for generating percussion in any time signature\nutilises probabilistic methods, including Markov chains. Through both human and\nobjective evaluations, we demonstrate that our music generation approach\noutperforms baselines on specific, musically meaningful metrics, offering a\nviable alternative to purely neural network-based systems.", "published": "2024-09-19 10:26:01", "link": "http://arxiv.org/abs/2409.12638v2", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Synthetic Training Data for Speech Commands: From ASR-Based\n  Filtering to Domain Adaptation in SSL Latent Space", "abstract": "The use of synthetic speech as data augmentation is gaining increasing\npopularity in fields such as automatic speech recognition and speech\nclassification tasks. Despite novel text-to-speech systems with voice cloning\ncapabilities, that allow the usage of a larger amount of voices based on short\naudio segments, it is known that these systems tend to hallucinate and\noftentimes produce bad data that will most likely have a negative impact on the\ndownstream task. In the present work, we conduct a set of experiments around\nzero-shot learning with synthetic speech data for the specific task of speech\ncommands classification. Our results on the Google Speech Commands dataset show\nthat a simple ASR-based filtering method can have a big impact in the quality\nof the generated data, translating to a better performance. Furthermore,\ndespite the good quality of the generated speech data, we also show that\nsynthetic and real speech can still be easily distinguishable when using\nself-supervised (WavLM) features, an aspect further explored with a CycleGAN to\nbridge the gap between the two types of speech material.", "published": "2024-09-19 13:07:55", "link": "http://arxiv.org/abs/2409.12745v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiffEditor: Enhancing Speech Editing with Semantic Enrichment and\n  Acoustic Consistency", "abstract": "As text-based speech editing becomes increasingly prevalent, the demand for\nunrestricted free-text editing continues to grow. However, existing speech\nediting techniques encounter significant challenges, particularly in\nmaintaining intelligibility and acoustic consistency when dealing with\nout-of-domain (OOD) text. In this paper, we introduce, DiffEditor, a novel\nspeech editing model designed to enhance performance in OOD text scenarios\nthrough semantic enrichment and acoustic consistency. To improve the\nintelligibility of the edited speech, we enrich the semantic information of\nphoneme embeddings by integrating word embeddings extracted from a pretrained\nlanguage model. Furthermore, we emphasize that interframe smoothing properties\nare critical for modeling acoustic consistency, and thus we propose a\nfirst-order loss function to promote smoother transitions at editing boundaries\nand enhance the overall fluency of the edited speech. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in both\nin-domain and OOD text scenarios.", "published": "2024-09-19 07:11:54", "link": "http://arxiv.org/abs/2409.12992v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DiffSSD: A Diffusion-Based Dataset For Speech Forensics", "abstract": "Diffusion-based speech generators are ubiquitous. These methods can generate\nvery high quality synthetic speech and several recent incidents report their\nmalicious use. To counter such misuse, synthetic speech detectors have been\ndeveloped. Many of these detectors are trained on datasets which do not include\ndiffusion-based synthesizers. In this paper, we demonstrate that existing\ndetectors trained on one such dataset, ASVspoof2019, do not perform well in\ndetecting synthetic speech from recent diffusion-based synthesizers. We propose\nthe Diffusion-Based Synthetic Speech Dataset (DiffSSD), a dataset consisting of\nabout 200 hours of labeled speech, including synthetic speech generated by 8\ndiffusion-based open-source and 2 commercial generators. We also examine the\nperformance of existing synthetic speech detectors on DiffSSD in both\nclosed-set and open-set scenarios. The results highlight the importance of this\ndataset in detecting synthetic speech generated from recent open-source and\ncommercial speech generators.", "published": "2024-09-19 18:55:13", "link": "http://arxiv.org/abs/2409.13049v2", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A sound description: Exploring prompt templates and class descriptions\n  to enhance zero-shot audio classification", "abstract": "Audio-text models trained via contrastive learning offer a practical approach\nto perform audio classification through natural language prompts, such as \"this\nis a sound of\" followed by category names. In this work, we explore alternative\nprompt templates for zero-shot audio classification, demonstrating the\nexistence of higher-performing options. First, we find that the formatting of\nthe prompts significantly affects performance so that simply prompting the\nmodels with properly formatted class labels performs competitively with\noptimized prompt templates and even prompt ensembling. Moreover, we look into\ncomplementing class labels by audio-centric descriptions. By leveraging large\nlanguage models, we generate textual descriptions that prioritize acoustic\nfeatures of sound events to disambiguate between classes, without extensive\nprompt engineering. We show that prompting with class descriptions leads to\nstate-of-the-art results in zero-shot audio classification across major ambient\nsound datasets. Remarkably, this method requires no additional training and\nremains fully zero-shot.", "published": "2024-09-19 11:27:50", "link": "http://arxiv.org/abs/2409.13676v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A quest through interconnected datasets: lessons from highly-cited\n  ICASSP papers", "abstract": "As audio machine learning outcomes are deployed in societally impactful\napplications, it is important to have a sense of the quality and origins of the\ndata used. Noticing that being explicit about this sense is not trivially\nrewarded in academic publishing in applied machine learning domains, and\nneither is included in typical applied machine learning curricula, we present a\nstudy into dataset usage connected to the top-5 cited papers at the\nInternational Conference on Acoustics, Speech, and Signal Processing (ICASSP).\nIn this, we conduct thorough depth-first analyses towards origins of used\ndatasets, often leading to searches that had to go beyond what was reported in\nofficial papers, and ending into unclear or entangled origins. Especially in\nthe current pull towards larger, and possibly generative AI models, awareness\nof the need for accountability on data provenance is increasing. With this, we\ncall on the community to not only focus on engineering larger models, but\ncreate more room and reward for explicitizing the foundations on which such\nmodels should be built.", "published": "2024-09-19 14:25:57", "link": "http://arxiv.org/abs/2410.03676v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ViolinDiff: Enhancing Expressive Violin Synthesis with Pitch Bend\n  Conditioning", "abstract": "Modeling the natural contour of fundamental frequency (F0) plays a critical\nrole in music audio synthesis. However, transcribing and managing multiple F0\ncontours in polyphonic music is challenging, and explicit F0 contour modeling\nhas not yet been explored for polyphonic instrumental synthesis. In this paper,\nwe present ViolinDiff, a two-stage diffusion-based synthesis framework. For a\ngiven violin MIDI file, the first stage estimates the F0 contour as pitch bend\ninformation, and the second stage generates mel spectrogram incorporating these\nexpressive details. The quantitative metrics and listening test results show\nthat the proposed model generates more realistic violin sounds than the model\nwithout explicit pitch bend modeling. Audio samples are available online:\ndaewoung.github.io/ViolinDiff-Demo.", "published": "2024-09-19 05:39:19", "link": "http://arxiv.org/abs/2409.12477v2", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
