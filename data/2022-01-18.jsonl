{"title": "TYPIC: A Corpus of Template-Based Diagnostic Comments on Argumentation", "abstract": "Providing feedback on the argumentation of the learner is essential for\ndeveloping critical thinking skills, however, it requires a lot of time and\neffort. To mitigate the overload on teachers, we aim to automate a process of\nproviding feedback, especially giving diagnostic comments which point out the\nweaknesses inherent in the argumentation. It is recommended to give specific\ndiagnostic comments so that learners can recognize the diagnosis without\nmisinterpretation. However, it is not obvious how the task of providing\nspecific diagnostic comments should be formulated. We present a formulation of\nthe task as template selection and slot filling to make an automatic evaluation\neasier and the behavior of the model more tractable. The key to the formulation\nis the possibility of creating a template set that is sufficient for practical\nuse. In this paper, we define three criteria that a template set should\nsatisfy: expressiveness, informativeness, and uniqueness, and verify the\nfeasibility of creating a template set that satisfies these criteria as a first\ntrial. We will show that it is feasible through an annotation study that\nconverts diagnostic comments given in a text to a template format. The corpus\nused in the annotation study is publicly available.", "published": "2022-01-18 00:30:40", "link": "http://arxiv.org/abs/2201.06674v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emojis as Anchors to Detect Arabic Offensive Language and Hate Speech", "abstract": "We introduce a generic, language-independent method to collect a large\npercentage of offensive and hate tweets regardless of their topics or genres.\nWe harness the extralinguistic information embedded in the emojis to collect a\nlarge number of offensive tweets. We apply the proposed method on Arabic tweets\nand compare it with English tweets - analysing key cultural differences. We\nobserved a constant usage of these emojis to represent offensiveness throughout\ndifferent timespans on Twitter. We manually annotate and publicly release the\nlargest Arabic dataset for offensive, fine-grained hate speech, vulgar and\nviolence content. Furthermore, we benchmark the dataset for detecting\noffensiveness and hate speech using different transformer architectures and\nperform in-depth linguistic analysis. We evaluate our models on external\ndatasets - a Twitter dataset collected using a completely different method, and\na multi-platform dataset containing comments from Twitter, YouTube and\nFacebook, for assessing generalization capability. Competitive results on these\ndatasets suggest that the data collected using our method captures universal\ncharacteristics of offensive language. Our findings also highlight the common\nwords used in offensive communications, common targets for hate speech,\nspecific patterns in violence tweets; and pinpoint common classification errors\nthat can be attributed to limitations of NLP models. We observe that even\nstate-of-the-art transformer models may fail to take into account culture,\nbackground and context or understand nuances present in real-world data such as\nsarcasm.", "published": "2022-01-18 03:56:57", "link": "http://arxiv.org/abs/2201.06723v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Youling: an AI-Assisted Lyrics Creation System", "abstract": "Recently, a variety of neural models have been proposed for lyrics\ngeneration. However, most previous work completes the generation process in a\nsingle pass with little human intervention. We believe that lyrics creation is\na creative process with human intelligence centered. AI should play a role as\nan assistant in the lyrics creation process, where human interactions are\ncrucial for high-quality creation. This paper demonstrates \\textit{Youling}, an\nAI-assisted lyrics creation system, designed to collaborate with music\ncreators. In the lyrics generation process, \\textit{Youling} supports\ntraditional one pass full-text generation mode as well as an interactive\ngeneration mode, which allows users to select the satisfactory sentences from\ngenerated candidates conditioned on preceding context. The system also provides\na revision module which enables users to revise undesired sentences or words of\nlyrics repeatedly. Besides, \\textit{Youling} allows users to use multifaceted\nattributes to control the content and format of generated lyrics. The demo\nvideo of the system is available at https://youtu.be/DFeNpHk0pm4.", "published": "2022-01-18 03:57:04", "link": "http://arxiv.org/abs/2201.06724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HashSet -- A Dataset For Hashtag Segmentation", "abstract": "Hashtag segmentation is the task of breaking a hashtag into its constituent\ntokens. Hashtags often encode the essence of user-generated posts, along with\ninformation like topic and sentiment, which are useful in downstream tasks.\nHashtags prioritize brevity and are written in unique ways -- transliterating\nand mixing languages, spelling variations, creative named entities. Benchmark\ndatasets used for the hashtag segmentation task -- STAN, BOUN -- are small in\nsize and extracted from a single set of tweets. However, datasets should\nreflect the variations in writing styles of hashtags and also account for\ndomain and language specificity, failing which the results will misrepresent\nmodel performance. We argue that model performance should be assessed on a\nwider variety of hashtags, and datasets should be carefully curated. To this\nend, we propose HashSet, a dataset comprising of: a) 1.9k manually annotated\ndataset; b) 3.3M loosely supervised dataset. HashSet dataset is sampled from a\ndifferent set of tweets when compared to existing datasets and provides an\nalternate distribution of hashtags to build and validate hashtag segmentation\nmodels. We show that the performance of SOTA models for Hashtag Segmentation\ndrops substantially on proposed dataset, indicating that the proposed dataset\nprovides an alternate set of hashtags to train and assess models.", "published": "2022-01-18 04:40:45", "link": "http://arxiv.org/abs/2201.06741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COPA-SSE: Semi-structured Explanations for Commonsense Reasoning", "abstract": "We present Semi-Structured Explanations for COPA (COPA-SSE), a new\ncrowdsourced dataset of 9,747 semi-structured, English common sense\nexplanations for Choice of Plausible Alternatives (COPA) questions. The\nexplanations are formatted as a set of triple-like common sense statements with\nConceptNet relations but freely written concepts. This semi-structured format\nstrikes a balance between the high quality but low coverage of structured data\nand the lower quality but high coverage of free-form crowdsourcing. Each\nexplanation also includes a set of human-given quality ratings. With their\nfamiliar format, the explanations are geared towards commonsense reasoners\noperating on knowledge graphs and serve as a starting point for ongoing work on\nimproving such systems. The dataset is available at\nhttps://github.com/a-brassard/copa-sse.", "published": "2022-01-18 07:20:57", "link": "http://arxiv.org/abs/2201.06777v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Self-learning End-to-End Task-Oriented Dialog Systems", "abstract": "End-to-end task bots are typically learned over a static and usually\nlimited-size corpus. However, when deployed in dynamic, changing, and open\nenvironments to interact with users, task bots tend to fail when confronted\nwith data that deviate from the training corpus, i.e., out-of-distribution\nsamples. In this paper, we study the problem of automatically adapting task\nbots to changing environments by learning from human-bot interactions with\nminimum or zero human annotations. We propose SL-AGENT, a novel self-learning\nframework for building end-to-end task bots. SL-AGENT consists of a dialog\nmodel and a pre-trained reward model to predict the quality of an agent\nresponse. It enables task bots to automatically adapt to changing environments\nby learning from the unlabeled human-bot dialog logs accumulated after\ndeployment via reinforcement learning with the incorporated reward model.\nExperimental results on four well-studied dialog tasks show the effectiveness\nof SL-AGENT to automatically adapt to changing environments, using both\nautomatic and human evaluations. We will release code and data for further\nresearch.", "published": "2022-01-18 09:56:35", "link": "http://arxiv.org/abs/2201.06849v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instance-aware Prompt Learning for Language Understanding and Generation", "abstract": "Recently, prompt learning has become a new paradigm to utilize pre-trained\nlanguage models (PLMs) and achieves promising results in downstream tasks with\na negligible increase of parameters. The current usage of discrete and\ncontinuous prompts assumes that the prompt is fixed for a specific task and all\nsamples in the task share the same prompt. However, a task may contain quite\ndiverse samples in which some are easy and others are difficult, and diverse\nprompts are desirable. In this paper, we propose an instance-aware prompt\nlearning method that learns a different prompt for each instance. Specifically,\nwe suppose that each learnable prompt token has a different contribution to\ndifferent instances, and we learn the contribution by calculating the relevance\nscore between an instance and each prompt token. The contribution weighted\nprompt would be instance aware. We apply our method to both unidirectional and\nbidirectional PLMs on both language understanding and generation tasks.\nExtensive experiments demonstrate that our method obtains considerable\nimprovements compared to strong baselines. Especially, our method achieves the\nstate-of-the-art on the SuperGLUE few-shot learning benchmark.", "published": "2022-01-18 17:03:25", "link": "http://arxiv.org/abs/2201.07126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Klexikon: A German Dataset for Joint Summarization and Simplification", "abstract": "Traditionally, Text Simplification is treated as a monolingual translation\ntask where sentences between source texts and their simplified counterparts are\naligned for training. However, especially for longer input documents,\nsummarizing the text (or dropping less relevant content altogether) plays an\nimportant role in the simplification process, which is currently not reflected\nin existing datasets. Simultaneously, resources for non-English languages are\nscarce in general and prohibitive for training new solutions. To tackle this\nproblem, we pose core requirements for a system that can jointly summarize and\nsimplify long source documents. We further describe the creation of a new\ndataset for joint Text Simplification and Summarization based on German\nWikipedia and the German children's lexicon \"Klexikon\", consisting of almost\n2900 documents. We release a document-aligned version that particularly\nhighlights the summarization aspect, and provide statistical evidence that this\nresource is well suited to simplification as well. Code and data are available\non Github: https://github.com/dennlinger/klexikon", "published": "2022-01-18 18:50:43", "link": "http://arxiv.org/abs/2201.07198v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotating the Tweebank Corpus on Named Entity Recognition and Building\n  NLP Models for Social Media Analysis", "abstract": "Social media data such as Twitter messages (\"tweets\") pose a particular\nchallenge to NLP systems because of their short, noisy, and colloquial nature.\nTasks such as Named Entity Recognition (NER) and syntactic parsing require\nhighly domain-matched training data for good performance. To date, there is no\ncomplete training corpus for both NER and syntactic analysis (e.g., part of\nspeech tagging, dependency parsing) of tweets. While there are some publicly\navailable annotated NLP datasets of tweets, they are only designed for\nindividual tasks. In this study, we aim to create Tweebank-NER, an English NER\ncorpus based on Tweebank V2 (TB2), train state-of-the-art (SOTA) Tweet NLP\nmodels on TB2, and release an NLP pipeline called Twitter-Stanza. We annotate\nnamed entities in TB2 using Amazon Mechanical Turk and measure the quality of\nour annotations. We train the Stanza pipeline on TB2 and compare with\nalternative NLP frameworks (e.g., FLAIR, spaCy) and transformer-based models.\nThe Stanza tokenizer and lemmatizer achieve SOTA performance on TB2, while the\nStanza NER tagger, part-of-speech (POS) tagger, and dependency parser achieve\ncompetitive performance against non-transformer models. The transformer-based\nmodels establish a strong baseline in Tweebank-NER and achieve the new SOTA\nperformance in POS tagging and dependency parsing on TB2. We release the\ndataset and make both the Stanza pipeline and BERTweet-based models available\n\"off-the-shelf\" for use in future Tweet NLP research. Our source code, data,\nand pre-trained models are available at:\n\\url{https://github.com/social-machines/TweebankNLP}.", "published": "2022-01-18 19:34:23", "link": "http://arxiv.org/abs/2201.07281v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending the Vocabulary of Fictional Languages using Neural Networks", "abstract": "Fictional languages have become increasingly popular over the recent years\nappearing in novels, movies, TV shows, comics, and video games. While some of\nthese fictional languages have a complete vocabulary, most do not. We propose a\ndeep learning solution to the problem. Using style transfer and machine\ntranslation tools, we generate new words for a given target fictional language,\nwhile maintaining the style of its creator, hence extending this language\nvocabulary.", "published": "2022-01-18 19:57:18", "link": "http://arxiv.org/abs/2201.07288v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Privacy-Preserving Unsupervised Domain Adaptation Framework for\n  Clinical Text Analysis", "abstract": "Unsupervised domain adaptation (UDA) generally aligns the unlabeled target\ndomain data to the distribution of the source domain to mitigate the\ndistribution shift problem. The standard UDA requires sharing the source data\nwith the target, having potential data privacy leaking risks. To protect the\nsource data's privacy, we first propose to share the source feature\ndistribution instead of the source data. However, sharing only the source\nfeature distribution may still suffer from the membership inference attack who\ncan infer an individual's membership by the black-box access to the source\nmodel. To resolve this privacy issue, we further study the under-explored\nproblem of privacy-preserving domain adaptation and propose a method with a\nnovel differential privacy training strategy to protect the source data\nprivacy. We model the source feature distribution by Gaussian Mixture Models\n(GMMs) under the differential privacy setting and send it to the target client\nfor adaptation. The target client resamples differentially private source\nfeatures from GMMs and adapts on target data with several state-of-art UDA\nbackbones. With our proposed method, the source data provider could avoid\nleaking source data privacy during domain adaptation as well as reserve the\nutility. To evaluate our proposed method's utility and privacy loss, we apply\nour model on a medical report disease label classification task using two noisy\nchallenging clinical text datasets. The results show that our proposed method\ncan preserve source data's privacy with a minor performance influence on the\ntext classification task.", "published": "2022-01-18 21:23:28", "link": "http://arxiv.org/abs/2201.07317v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialog Intent Induction via Density-based Deep Clustering Ensemble", "abstract": "Existing task-oriented chatbots heavily rely on spoken language understanding\n(SLU) systems to determine a user's utterance's intent and other key\ninformation for fulfilling specific tasks. In real-life applications, it is\ncrucial to occasionally induce novel dialog intents from the conversation logs\nto improve the user experience. In this paper, we propose the Density-based\nDeep Clustering Ensemble (DDCE) method for dialog intent induction. Compared to\nexisting K-means based methods, our proposed method is more effective in\ndealing with real-life scenarios where a large number of outliers exist. To\nmaximize data utilization, we jointly optimize texts' representations and the\nhyperparameters of the clustering algorithm. In addition, we design an\noutlier-aware clustering ensemble framework to handle the overfitting issue.\nExperimental results over seven datasets show that our proposed method\nsignificantly outperforms other state-of-the-art baselines.", "published": "2022-01-18 04:13:26", "link": "http://arxiv.org/abs/2201.06731v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Neural Network Approaches for Long Document Classification", "abstract": "Text classification algorithms investigate the intricate relationships\nbetween words or phrases and attempt to deduce the document's interpretation.\nIn the last few years, these algorithms have progressed tremendously.\nTransformer architecture and sentence encoders have proven to give superior\nresults on natural language processing tasks. But a major limitation of these\narchitectures is their applicability for text no longer than a few hundred\nwords. In this paper, we explore hierarchical transfer learning approaches for\nlong document classification. We employ pre-trained Universal Sentence Encoder\n(USE) and Bidirectional Encoder Representations from Transformers (BERT) in a\nhierarchical setup to capture better representations efficiently. Our proposed\nmodels are conceptually simple where we divide the input data into chunks and\nthen pass this through base models of BERT and USE. Then output representation\nfor each chunk is then propagated through a shallow neural network comprising\nof LSTMs or CNNs for classifying the text data. These extensions are evaluated\non 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its\nstand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its\nstand-alone counterpart. However, the hierarchical BERT models are still\ndesirable as it avoids the quadratic complexity of the attention mechanism in\nBERT. Along with the hierarchical approaches, this work also provides a\ncomparison of different deep learning algorithms like USE, BERT, HAN,\nLongformer, and BigBird for long document classification. The Longformer\napproach consistently performs well on most of the datasets.", "published": "2022-01-18 07:17:40", "link": "http://arxiv.org/abs/2201.06774v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoAuthor: Designing a Human-AI Collaborative Writing Dataset for\n  Exploring Language Model Capabilities", "abstract": "Large language models (LMs) offer unprecedented language generation\ncapabilities and exciting opportunities for interaction design. However, their\nhighly context-dependent capabilities are difficult to grasp and are often\nsubjectively interpreted. In this paper, we argue that by curating and\nanalyzing large interaction datasets, the HCI community can foster more\nincisive examinations of LMs' generative capabilities. Exemplifying this\napproach, we present CoAuthor, a dataset designed for revealing GPT-3's\ncapabilities in assisting creative and argumentative writing. CoAuthor captures\nrich interactions between 63 writers and four instances of GPT-3 across 1445\nwriting sessions. We demonstrate that CoAuthor can address questions about\nGPT-3's language, ideation, and collaboration capabilities, and reveal its\ncontribution as a writing \"collaborator\" under various definitions of good\ncollaboration. Finally, we discuss how this work may facilitate a more\nprincipled discussion around LMs' promises and pitfalls in relation to\ninteraction design. The dataset and an interface for replaying the writing\nsessions are publicly available at https://coauthor.stanford.edu.", "published": "2022-01-18 07:51:57", "link": "http://arxiv.org/abs/2201.06796v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Syntax-based data augmentation for Hungarian-English machine translation", "abstract": "We train Transformer-based neural machine translation models for\nHungarian-English and English-Hungarian using the Hunglish2 corpus. Our best\nmodels achieve a BLEU score of 40.0 on HungarianEnglish and 33.4 on\nEnglish-Hungarian. Furthermore, we present results on an ongoing work about\nsyntax-based augmentation for neural machine translation. Both our code and\nmodels are publicly available.", "published": "2022-01-18 11:13:56", "link": "http://arxiv.org/abs/2201.06876v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evidence-aware Fake News Detection with Graph Neural Networks", "abstract": "The prevalence and perniciousness of fake news has been a critical issue on\nthe Internet, which stimulates the development of automatic fake news detection\nin turn. In this paper, we focus on the evidence-based fake news detection,\nwhere several evidences are utilized to probe the veracity of news (i.e., a\nclaim). Most previous methods first employ sequential models to embed the\nsemantic information and then capture the claim-evidence interaction based on\ndifferent attention mechanisms. Despite their effectiveness, they still suffer\nfrom two main weaknesses. Firstly, due to the inherent drawbacks of sequential\nmodels, they fail to integrate the relevant information that is scattered far\napart in evidences for veracity checking. Secondly, they neglect much redundant\ninformation contained in evidences that may be useless or even harmful. To\nsolve these problems, we propose a unified Graph-based sEmantic sTructure\nmining framework, namely GET in short. Specifically, different from the\nexisting work that treats claims and evidences as sequences, we model them as\ngraph-structured data and capture the long-distance semantic dependency among\ndispersed relevant snippets via neighborhood propagation. After obtaining\ncontextual semantic information, our model reduces information redundancy by\nperforming graph structure learning. Finally, the fine-grained semantic\nrepresentations are fed into the downstream claim-evidence interaction module\nfor predictions. Comprehensive experiments have demonstrated the superiority of\nGET over the state-of-the-arts.", "published": "2022-01-18 11:28:36", "link": "http://arxiv.org/abs/2201.06885v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improve Sentence Alignment by Divide-and-conquer", "abstract": "In this paper, we introduce a divide-and-conquer algorithm to improve\nsentence alignment speed. We utilize external bilingual sentence embeddings to\nfind accurate hard delimiters for the parallel texts to be aligned. We use\nMonte Carlo simulation to show experimentally that using this\ndivide-and-conquer algorithm, we can turn any quadratic time complexity\nsentence alignment algorithm into an algorithm with average time complexity of\nO(NlogN). On a standard OCR-generated dataset, our method improves the\nBleualign baseline by 3 F1 points. Besides, when computational resources are\nrestricted, our algorithm is faster than Vecalign in practice.", "published": "2022-01-18 12:25:04", "link": "http://arxiv.org/abs/2201.06907v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves\n  Zero-Shot Generalization", "abstract": "We propose a multitask pretraining approach ZeroPrompt for zero-shot\ngeneralization, focusing on task scaling and zero-shot prompting. While\nprevious models are trained on only a few dozen tasks, we scale to 1,000 tasks\nfor the first time using real-world data. This leads to a crucial discovery\nthat task scaling can be an efficient alternative to model scaling; i.e., the\nmodel size has little impact on performance with an extremely large number of\ntasks. Our results show that task scaling can substantially improve training\nefficiency by 30 times in FLOPs. Moreover, we present a prompting method that\nincorporates a genetic algorithm to automatically search for the best prompt\nfor unseen tasks, along with a few other improvements. Empirically, ZeroPrompt\nsubstantially improves both the efficiency and the performance of zero-shot\nlearning across a variety of academic and production datasets.", "published": "2022-01-18 12:30:17", "link": "http://arxiv.org/abs/2201.06910v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "What Makes the Story Forward? Inferring Commonsense Explanations as\n  Prompts for Future Event Generation", "abstract": "Prediction over event sequences is critical for many real-world applications\nin Information Retrieval and Natural Language Processing. Future Event\nGeneration (FEG) is a challenging task in event sequence prediction because it\nrequires not only fluent text generation but also commonsense reasoning to\nmaintain the logical coherence of the entire event story. In this paper, we\npropose a novel explainable FEG framework, Coep. It highlights and integrates\ntwo types of event knowledge, sequential knowledge of direct event-event\nrelations and inferential knowledge that reflects the intermediate character\npsychology between events, such as intents, causes, reactions, which\nintrinsically pushes the story forward. To alleviate the knowledge forgetting\nissue, we design two modules, Im and Gm, for each type of knowledge, which are\ncombined via prompt tuning. First, Im focuses on understanding inferential\nknowledge to generate commonsense explanations and provide a soft prompt vector\nfor Gm. We also design a contrastive discriminator for better generalization\nability. Second, Gm generates future events by modeling direct sequential\nknowledge with the guidance of Im. Automatic and human evaluation demonstrate\nthat our approach can generate more coherent, specific, and logical future\nevents.", "published": "2022-01-18 16:21:23", "link": "http://arxiv.org/abs/2201.07099v2", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "Sectioning of Biomedical Abstracts: A Sequence of Sequence\n  Classification Task", "abstract": "Rapid growth of the biomedical literature has led to many advances in the\nbiomedical text mining field. Among the vast amount of information, biomedical\narticle abstracts are the easily accessible sources. However, the number of the\nstructured abstracts, describing the rhetorical sections with one of\nBackground, Objective, Method, Result and Conclusion categories is still not\nconsiderable. Exploration of valuable information in the biomedical abstracts\ncan be expedited with the improvements in the sequential sentence\nclassification task. Deep learning based models has great performance/potential\nin achieving significant results in this task. However, they can often be\noverly complex and overfit to specific data. In this project, we study a\nstate-of-the-art deep learning model, which we called SSN-4 model here. We\ninvestigate different components of the SSN-4 model to study the trade-off\nbetween the performance and complexity. We explore how well this model\ngeneralizes to a new data set beyond Randomized Controlled Trials (RCT)\ndataset. We address the question that whether word embeddings can be adjusted\nto the task to improve the performance. Furthermore, we develop a second model\nthat addresses the confusion pairs in the first model. Results show that SSN-4\nmodel does not appear to generalize well beyond RCT dataset.", "published": "2022-01-18 16:41:13", "link": "http://arxiv.org/abs/2201.07112v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Selecting and combining complementary feature representations and\n  classifiers for hate speech detection", "abstract": "Hate speech is a major issue in social networks due to the high volume of\ndata generated daily. Recent works demonstrate the usefulness of machine\nlearning (ML) in dealing with the nuances required to distinguish between\nhateful posts from just sarcasm or offensive language. Many ML solutions for\nhate speech detection have been proposed by either changing how features are\nextracted from the text or the classification algorithm employed. However, most\nworks consider only one type of feature extraction and classification\nalgorithm. This work argues that a combination of multiple feature extraction\ntechniques and different classification models is needed. We propose a\nframework to analyze the relationship between multiple feature extraction and\nclassification techniques to understand how they complement each other. The\nframework is used to select a subset of complementary techniques to compose a\nrobust multiple classifiers system (MCS) for hate speech detection. The\nexperimental study considering four hate speech classification datasets\ndemonstrates that the proposed framework is a promising methodology for\nanalyzing and designing high-performing MCS for this task. MCS system obtained\nusing the proposed framework significantly outperforms the combination of all\nmodels and the homogeneous and heterogeneous selection heuristics,\ndemonstrating the importance of having a proper selection scheme. Source code,\nfigures, and dataset splits can be found in the GitHub repository:\nhttps://github.com/Menelau/Hate-Speech-MCS.", "published": "2022-01-18 03:46:49", "link": "http://arxiv.org/abs/2201.06721v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Dilated Convolutional Neural Networks for Lightweight Diacritics\n  Restoration", "abstract": "Diacritics restoration has become a ubiquitous task in the\nLatin-alphabet-based English-dominated Internet language environment. In this\npaper, we describe a small footprint 1D dilated convolution-based approach\nwhich operates on a character-level. We find that solutions based on 1D dilated\nconvolutional neural networks are competitive alternatives to models based on\nrecursive neural networks or linguistic modeling for the task of diacritics\nrestoration. Our solution surpasses the performance of similarly sized models\nand is also competitive with larger models. A special feature of our solution\nis that it even runs locally in a web browser. We also provide a working\nexample of this browser-based implementation. Our model is evaluated on\ndifferent corpora, with emphasis on the Hungarian language. We performed\ncomparative measurements about the generalization power of the model in\nrelation to three Hungarian corpora. We also analyzed the errors to understand\nthe limitation of corpus-based self-supervised training.", "published": "2022-01-18 06:10:47", "link": "http://arxiv.org/abs/2201.06757v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Multimodal Word Discovery based on Double Articulation\n  Analysis with Co-occurrence cues", "abstract": "Human infants acquire their verbal lexicon with minimal prior knowledge of\nlanguage based on the statistical properties of phonological distributions and\nthe co-occurrence of other sensory stimuli. This study proposes a novel fully\nunsupervised learning method for discovering speech units using phonological\ninformation as a distributional cue and object information as a co-occurrence\ncue. The proposed method can acquire words and phonemes from speech signals\nusing unsupervised learning and utilize object information based on multiple\nmodalities-vision, tactile, and auditory-simultaneously. The proposed method is\nbased on the nonparametric Bayesian double articulation analyzer (NPB-DAA)\ndiscovering phonemes and words from phonological features, and multimodal\nlatent Dirichlet allocation (MLDA) categorizing multimodal information obtained\nfrom objects. In an experiment, the proposed method showed higher word\ndiscovery performance than baseline methods. Words that expressed the\ncharacteristics of objects (i.e., words corresponding to nouns and adjectives)\nwere segmented accurately. Furthermore, we examined how learning performance is\naffected by differences in the importance of linguistic information. Increasing\nthe weight of the word modality further improved performance relative to that\nof the fixed condition.", "published": "2022-01-18 07:31:59", "link": "http://arxiv.org/abs/2201.06786v2", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Human and Automatic Speech Recognition Performance on German Oral\n  History Interviews", "abstract": "Automatic speech recognition systems have accomplished remarkable\nimprovements in transcription accuracy in recent years. On some domains, models\nnow achieve near-human performance. However, transcription performance on oral\nhistory has not yet reached human accuracy. In the present work, we investigate\nhow large this gap between human and machine transcription still is. For this\npurpose, we analyze and compare transcriptions of three humans on a new oral\nhistory data set. We estimate a human word error rate of 8.7% for recent German\noral history interviews with clean acoustic conditions. For comparison with\nrecent machine transcription accuracy, we present experiments on the adaptation\nof an acoustic model achieving near-human performance on broadcast speech. We\ninvestigate the influence of different adaptation data on robustness and\ngeneralization for clean and noisy oral history interviews. We optimize our\nacoustic models by 5 to 8% relative for this task and achieve 23.9% WER on\nnoisy and 15.6% word error rate on clean oral history interviews.", "published": "2022-01-18 09:35:45", "link": "http://arxiv.org/abs/2201.06841v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Study on the Ambiguity in Human Annotation of German Oral History\n  Interviews for Perceived Emotion Recognition and Sentiment Analysis", "abstract": "For research in audiovisual interview archives often it is not only of\ninterest what is said but also how. Sentiment analysis and emotion recognition\ncan help capture, categorize and make these different facets searchable. In\nparticular, for oral history archives, such indexing technologies can be of\ngreat interest. These technologies can help understand the role of emotions in\nhistorical remembering. However, humans often perceive sentiments and emotions\nambiguously and subjectively. Moreover, oral history interviews have\nmulti-layered levels of complex, sometimes contradictory, sometimes very subtle\nfacets of emotions. Therefore, the question arises of the chance machines and\nhumans have capturing and assigning these into predefined categories. This\npaper investigates the ambiguity in human perception of emotions and sentiment\nin German oral history interviews and the impact on machine learning systems.\nOur experiments reveal substantial differences in human perception for\ndifferent emotions. Furthermore, we report from ongoing machine learning\nexperiments with different modalities. We show that the human perceptual\nambiguity and other challenges, such as class imbalance and lack of training\ndata, currently limit the opportunities of these technologies for oral history\narchives. Nonetheless, our work uncovers promising observations and\npossibilities for further research.", "published": "2022-01-18 10:53:07", "link": "http://arxiv.org/abs/2201.06868v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning grammar with a divide-and-concur neural network", "abstract": "We implement a divide-and-concur iterative projection approach to\ncontext-free grammar inference. Unlike most state-of-the-art models of natural\nlanguage processing, our method requires a relatively small number of discrete\nparameters, making the inferred grammar directly interpretable -- one can read\noff from a solution how to construct grammatically valid sentences. Another\nadvantage of our approach is the ability to infer meaningful grammatical rules\nfrom just a few sentences, compared to the hundreds of gigabytes of training\ndata many other models employ. We demonstrate several ways of applying our\napproach: classifying words and inferring a grammar from scratch, taking an\nexisting grammar and refining its categories and rules, and taking an existing\ngrammar and expanding its lexicon as it encounters new words in new data.", "published": "2022-01-18 22:42:43", "link": "http://arxiv.org/abs/2201.07341v3", "categories": ["cs.CL", "cs.LG", "nlin.CG"], "primary_category": "cs.CL"}
{"title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge\n  for Embodied Agents", "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in\ninteractive environments? In this paper, we investigate the possibility of\ngrounding high-level tasks, expressed in natural language (e.g. \"make\nbreakfast\"), to a chosen set of actionable steps (e.g. \"open fridge\"). While\nprior work focused on learning from explicit step-by-step examples of how to\nact, we surprisingly find that if pre-trained LMs are large enough and prompted\nappropriately, they can effectively decompose high-level tasks into mid-level\nplans without any further training. However, the plans produced naively by LLMs\noften cannot map precisely to admissible actions. We propose a procedure that\nconditions on existing demonstrations and semantically translates the plans to\nadmissible actions. Our evaluation in the recent VirtualHome environment shows\nthat the resulting method substantially improves executability over the LLM\nbaseline. The conducted human evaluation reveals a trade-off between\nexecutability and correctness but shows a promising sign towards extracting\nactionable knowledge from language models. Website at\nhttps://huangwl18.github.io/language-planner", "published": "2022-01-18 18:59:45", "link": "http://arxiv.org/abs/2201.07207v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.LG"}
{"title": "How Bad Are Artifacts?: Analyzing the Impact of Speech Enhancement\n  Errors on ASR", "abstract": "It is challenging to improve automatic speech recognition (ASR) performance\nin noisy conditions with single-channel speech enhancement (SE). In this paper,\nwe investigate the causes of ASR performance degradation by decomposing the SE\nerrors using orthogonal projection-based decomposition (OPD). OPD decomposes\nthe SE errors into noise and artifact components. The artifact component is\ndefined as the SE error signal that cannot be represented as a linear\ncombination of speech and noise sources. We propose manually scaling the error\ncomponents to analyze their impact on ASR. We experimentally identify the\nartifact component as the main cause of performance degradation, and we find\nthat mitigating the artifact can greatly improve ASR performance. Furthermore,\nwe demonstrate that the simple observation adding (OA) technique (i.e., adding\na scaled version of the observed signal to the enhanced speech) can\nmonotonically increase the signal-to-artifact ratio under a mild condition.\nAccordingly, we experimentally confirm that OA improves ASR performance for\nboth simulated and real recordings. The findings of this paper provide a better\nunderstanding of the influence of SE errors on ASR and open the door to future\nresearch on novel approaches for designing effective single-channel SE\nfront-ends for ASR.", "published": "2022-01-18 01:12:01", "link": "http://arxiv.org/abs/2201.06685v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
