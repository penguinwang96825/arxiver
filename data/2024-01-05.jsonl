{"title": "Unsupervised hard Negative Augmentation for contrastive learning", "abstract": "We present Unsupervised hard Negative Augmentation (UNA), a method that\ngenerates synthetic negative instances based on the term frequency-inverse\ndocument frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to\nascertain the perceived importance of terms in a sentence and then produces\nnegative samples by replacing terms with respect to that. Our experiments\ndemonstrate that models trained with UNA improve the overall performance in\nsemantic textual similarity tasks. Additional performance gains are obtained\nwhen combining UNA with the paraphrasing augmentation. Further results show\nthat our method is compatible with different backbone models. Ablation studies\nalso support the choice of having a TF-IDF-driven control on negative\naugmentation.", "published": "2024-01-05 01:31:14", "link": "http://arxiv.org/abs/2401.02594v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing Bode: A Fine-Tuned Large Language Model for Portuguese\n  Prompt-Based Task", "abstract": "Large Language Models (LLMs) are increasingly bringing advances to Natural\nLanguage Processing. However, low-resource languages, those lacking extensive\nprominence in datasets for various NLP tasks, or where existing datasets are\nnot as substantial, such as Portuguese, already obtain several benefits from\nLLMs, but not to the same extent. LLMs trained on multilingual datasets\nnormally struggle to respond to prompts in Portuguese satisfactorily,\npresenting, for example, code switching in their responses. This work proposes\na fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two\nversions: 7B and 13B. We evaluate the performance of this model in\nclassification tasks using the zero-shot approach with in-context learning, and\ncompare it with other LLMs. Our main contribution is to bring an LLM with\nsatisfactory results in the Portuguese language, as well as to provide a model\nthat is free for research or commercial purposes.", "published": "2024-01-05 17:15:01", "link": "http://arxiv.org/abs/2401.02909v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "German Text Embedding Clustering Benchmark", "abstract": "This work introduces a benchmark assessing the performance of clustering\nGerman text embeddings in different domains. This benchmark is driven by the\nincreasing use of clustering neural text embeddings in tasks that require the\ngrouping of texts (such as topic modeling) and the need for German resources in\nexisting benchmarks. We provide an initial analysis for a range of pre-trained\nmono- and multilingual models evaluated on the outcome of different clustering\nalgorithms. Results include strong performing mono- and multilingual models.\nReducing the dimensions of embeddings can further improve clustering.\nAdditionally, we conduct experiments with continued pre-training for German\nBERT models to estimate the benefits of this additional training. Our\nexperiments suggest that significant performance improvements are possible for\nshort text. All code and datasets are publicly available.", "published": "2024-01-05 08:42:45", "link": "http://arxiv.org/abs/2401.02709v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding", "abstract": "Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to\nbeam search decoding for a wide range of text generation tasks. However, MBR\nrequires a huge amount of time for inference to compute the MBR objective,\nwhich makes the method infeasible in many situations where response time is\ncritical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently\nbeen proposed to reduce the inference time in machine translation tasks.\nAlthough it is shown to significantly reduce the amount of computation, it\nrequires hyperparameter tuning using a development set to be effective. To this\nend, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a\nhyperparameter-free method to run MBR decoding approximately. AMBR is derived\nfrom the observation that the problem of computing the sample-based MBR\nobjective is the medoid identification problem. AMBR uses the Correlated\nSequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best\napproximation algorithm to date for the medoid identification problem, to\ncompute the sample-based MBR objective. We evaluate AMBR on machine\ntranslation, text summarization, and image captioning tasks. The results show\nthat AMBR achieves on par with CBP, with CBP selecting hyperparameters through\nan Oracle for each given computation budget.", "published": "2024-01-05 11:02:08", "link": "http://arxiv.org/abs/2401.02749v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with\n  Fine-Tuning of Large Language Models", "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and\nExamples), an advanced architecture enhancing the integration of Large Language\nModels (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of\nthe ReAct framework, incorporates a dual-component memory system, mirroring\nhuman short-term and long-term memory, to maintain context and continuity in\nconversations. It entails a comprehensive agent construction scenario,\nincluding phases like Conversation Selection, Scene Extraction, CoT Completion,\nand Scene Augmentation, leading to the LLMs Training phase. This approach\nappears to enhance agent controllability and adaptability in complex,\nmulti-turn dialogues. Our preliminary evaluations in a real estate sales\ncontext suggest that RAISE has some advantages over traditional agents,\nindicating its potential for broader applications. This work contributes to the\nAI field by providing a robust framework for developing more context-aware and\nversatile conversational agents.", "published": "2024-01-05 12:26:46", "link": "http://arxiv.org/abs/2401.02777v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models in Plant Biology", "abstract": "Large Language Models (LLMs), such as ChatGPT, have taken the world by storm\nand have passed certain forms of the Turing test. However, LLMs are not limited\nto human language and analyze sequential data, such as DNA, protein, and gene\nexpression. The resulting foundation models can be repurposed to identify the\ncomplex patterns within the data, resulting in powerful, multi-purpose\nprediction tools able to explain cellular systems. This review outlines the\ndifferent types of LLMs and showcases their recent uses in biology. Since LLMs\nhave not yet been embraced by the plant community, we also cover how these\nmodels can be deployed for the plant kingdom.", "published": "2024-01-05 12:59:20", "link": "http://arxiv.org/abs/2401.02789v1", "categories": ["q-bio.GN", "cs.CL"], "primary_category": "q-bio.GN"}
{"title": "PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language\n  Models for Medical Imaging", "abstract": "Multimodal large language models (MLLMs) represent an evolutionary expansion\nin the capabilities of traditional large language models, enabling them to\ntackle challenges that surpass the scope of purely text-based applications. It\nleverages the knowledge previously encoded within these language models,\nthereby enhancing their applicability and functionality in the reign of\nmultimodal contexts. Recent works investigate the adaptation of MLLMs as a\nuniversal solution to address medical multi-modal problems as a generative\ntask. In this paper, we propose a parameter efficient framework for fine-tuning\nMLLMs, specifically validated on medical visual question answering (Med-VQA)\nand medical report generation (MRG) tasks, using public benchmark datasets. We\nalso introduce an evaluation metric using the 5-point Likert scale and its\nweighted average value to measure the quality of the generated reports for MRG\ntasks, where the scale ratings are labelled by both humans manually and the\nGPT-4 model. We further assess the consistency of performance metrics across\ntraditional measures, GPT-4, and human ratings for both VQA and MRG tasks. The\nresults indicate that semantic similarity assessments using GPT-4 align closely\nwith human annotators and provide greater stability, yet they reveal a\ndiscrepancy when compared to conventional lexical similarity measurements. This\nquestions the reliability of lexical similarity metrics for evaluating the\nperformance of generative models in Med-VQA and report generation tasks.\nBesides, our fine-tuned model significantly outperforms GPT-4v. This indicates\nthat without additional fine-tuning, multi-modal models like GPT-4v do not\nperform effectively on medical imaging tasks. The code will be available here:\nhttps://github.com/jinlHe/PeFoMed.", "published": "2024-01-05 13:22:12", "link": "http://arxiv.org/abs/2401.02797v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DocGraphLM: Documental Graph Language Model for Information Extraction", "abstract": "Advances in Visually Rich Document Understanding (VrDU) have enabled\ninformation extraction and question answering over documents with complex\nlayouts. Two tropes of architectures have emerged -- transformer-based models\ninspired by LLMs, and Graph Neural Networks. In this paper, we introduce\nDocGraphLM, a novel framework that combines pre-trained language models with\ngraph semantics. To achieve this, we propose 1) a joint encoder architecture to\nrepresent documents, and 2) a novel link prediction approach to reconstruct\ndocument graphs. DocGraphLM predicts both directions and distances between\nnodes using a convergent joint loss function that prioritizes neighborhood\nrestoration and downweighs distant node detection. Our experiments on three\nSotA datasets show consistent improvement on IE and QA tasks with the adoption\nof graph features. Moreover, we report that adopting the graph features\naccelerates convergence in the learning process during training, despite being\nsolely constructed through link prediction.", "published": "2024-01-05 14:15:36", "link": "http://arxiv.org/abs/2401.02823v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Towards ASR Robust Spoken Language Understanding Through In-Context\n  Learning With Word Confusion Networks", "abstract": "In the realm of spoken language understanding (SLU), numerous natural\nlanguage understanding (NLU) methodologies have been adapted by supplying large\nlanguage models (LLMs) with transcribed speech instead of conventional written\ntext. In real-world scenarios, prior to input into an LLM, an automated speech\nrecognition (ASR) system generates an output transcript hypothesis, where\ninherent errors can degrade subsequent SLU tasks. Here we introduce a method\nthat utilizes the ASR system's lattice output instead of relying solely on the\ntop hypothesis, aiming to encapsulate speech ambiguities and enhance SLU\noutcomes. Our in-context learning experiments, covering spoken question\nanswering and intent classification, underline the LLM's resilience to noisy\nspeech transcripts with the help of word confusion networks from lattices,\nbridging the SLU performance gap between using the top ASR hypothesis and an\noracle upper bound. Additionally, we delve into the LLM's robustness to varying\nASR performance conditions and scrutinize the aspects of in-context learning\nwhich prove the most influential.", "published": "2024-01-05 17:58:10", "link": "http://arxiv.org/abs/2401.02921v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron\n  Captioning", "abstract": "Neuron labeling is an approach to visualize the behaviour and respond of a\ncertain neuron to a certain pattern that activates the neuron. Neuron labeling\nextract information about the features captured by certain neurons in a deep\nneural network, one of which uses the encoder-decoder image captioning\napproach. The encoder used can be a pretrained CNN-based model and the decoder\nis an RNN-based model for text generation. Previous work, namely MILAN (Mutual\nInformation-guided Linguistic Annotation of Neuron), has tried to visualize the\nneuron behaviour using modified Show, Attend, and Tell (SAT) model in the\nencoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show\ngreat result on short sequence neuron captioning, but it does not show great\nresult on long sequence neuron captioning, so in this work, we would like to\nimprove the performance of MILAN even more by utilizing different kind of\nattention mechanism and additionally adding several attention result into one,\nin order to combine all the advantages from several attention mechanism. Using\nour compound dataset, we obtained higher BLEU and F1-Score on our proposed\nmodel, achieving 17.742 and 0.4811 respectively. At some point where the model\nconverges at the peak, our model obtained BLEU of 21.2262 and BERTScore\nF1-Score of 0.4870.", "published": "2024-01-05 10:41:55", "link": "http://arxiv.org/abs/2401.02744v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Complex systems approach to natural language", "abstract": "The review summarizes the main methodological concepts used in studying\nnatural language from the perspective of complexity science and documents their\napplicability in identifying both universal and system-specific features of\nlanguage in its written representation. Three main complexity-related research\ntrends in quantitative linguistics are covered. The first part addresses the\nissue of word frequencies in texts and demonstrates that taking punctuation\ninto consideration restores scaling whose violation in the Zipf's law is often\nobserved for the most frequent words. The second part introduces methods\ninspired by time series analysis, used in studying various kinds of\ncorrelations in written texts. The related time series are generated on the\nbasis of text partition into sentences or into phrases between consecutive\npunctuation marks. It turns out that these series develop features often found\nin signals generated by complex systems, like long-range correlations or\n(multi)fractal structures. Moreover, it appears that the distances between\npunctuation marks comply with the discrete variant of the Weibull distribution.\nIn the third part, the application of the network formalism to natural language\nis reviewed, particularly in the context of the so-called word-adjacency\nnetworks. Parameters characterizing topology of such networks can be used for\nclassification of texts, for example, from a stylometric perspective. Network\napproach can also be applied to represent the organization of word\nassociations. Structure of word-association networks turns out to be\nsignificantly different from that observed in random networks, revealing\ngenuine properties of language. Finally, punctuation seems to have a\nsignificant impact not only on the language's information-carrying ability but\nalso on its key statistical properties, hence it is recommended to consider\npunctuation marks on a par with words.", "published": "2024-01-05 12:01:26", "link": "http://arxiv.org/abs/2401.02772v1", "categories": ["physics.soc-ph", "cs.CL", "nlin.AO", "stat.AP"], "primary_category": "physics.soc-ph"}
{"title": "Pheme: Efficient and Conversational Speech Generation", "abstract": "In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.", "published": "2024-01-05 14:47:20", "link": "http://arxiv.org/abs/2401.02839v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "AFSPP: Agent Framework for Shaping Preference and Personality with Large\n  Language Models", "abstract": "The evolution of Large Language Models (LLMs) has introduced a new paradigm\nfor investigating human behavior emulation. Recent research has employed\nLLM-based Agents to create a sociological research environment, in which agents\nexhibit behavior based on the unfiltered characteristics of large language\nmodels. However, these studies overlook the iterative development within a\nhuman-like setting - Human preferences and personalities are complex, shaped by\nvarious factors and subject to ongoing change as a result of environmental and\nsubjective influences. In light of this observation, we propose Agent Framework\nfor Shaping Preference and Personality (AFSPP), exploring the multifaceted\nimpact of social networks and subjective consciousness on LLM-based Agents'\npreference and personality formation. With AFSPP, we have, for the first time,\nsuccessfully replicated several key findings from human personality\nexperiments. And other AFSPP-based experimental results indicate that plan\nmaking, sensory perceptions and social networking with subjective information,\nwield the most pronounced influence on preference shaping. AFSPP can\nsignificantly enhance the efficiency and scope of psychological experiments,\nwhile yielding valuable insights for Trustworthy Artificial Intelligence\nresearch for strategies to prevent undesirable preference and personality\ndevelopment.", "published": "2024-01-05 15:52:59", "link": "http://arxiv.org/abs/2401.02870v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance", "abstract": "The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. This paper investigates the novel challenge of defending MLLMs against\nsuch attacks. Compared to large language models (LLMs), MLLMs include an\nadditional image modality. We discover that images act as a ``foreign language\"\nthat is not considered during safety alignment, making MLLMs more prone to\nproducing harmful responses. Unfortunately, unlike the discrete tokens\nconsidered in text-based LLMs, the continuous nature of image signals presents\nsignificant alignment challenges, which poses difficulty to thoroughly cover\nall possible scenarios. This vulnerability is exacerbated by the fact that most\nstate-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much\nfewer than the extensive text-based pretraining corpus, which makes the MLLMs\nmore prone to catastrophic forgetting of their original abilities during safety\nfine-tuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy that solves two subtasks: 1) identifying harmful\nresponses via a lightweight harm detector, and 2) transforming harmful\nresponses into harmless ones via a detoxifier. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\noriginal performance of MLLMs. Our results demonstrate that MLLM-Protector\noffers a robust solution to a previously unaddressed aspect of MLLM security.", "published": "2024-01-05 17:05:42", "link": "http://arxiv.org/abs/2401.02906v3", "categories": ["cs.CR", "cs.CL", "cs.CV"], "primary_category": "cs.CR"}
{"title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism", "abstract": "The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.", "published": "2024-01-05 18:59:13", "link": "http://arxiv.org/abs/2401.02954v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AST-T5: Structure-Aware Pretraining for Code Generation and\n  Understanding", "abstract": "Large language models (LLMs) have made significant advancements in\ncode-related tasks, yet many LLMs treat code as simple sequences, neglecting\nits structured nature. We introduce AST-T5, a novel pretraining paradigm that\nleverages the Abstract Syntax Tree (AST) for enhanced code generation,\ntranspilation, and understanding. Using dynamic programming, our AST-Aware\nSegmentation retains code structure, while our AST-Aware Span Corruption\nobjective equips the model to reconstruct various code structures. Unlike other\nmodels, AST-T5 avoids intricate program analyses or architectural changes, so\nit integrates seamlessly with any encoder-decoder Transformer. Evaluations show\nthat AST-T5 consistently outperforms similar-sized LMs across various\ncode-related tasks. Structure-awareness makes AST-T5 particularly powerful in\ncode-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the\nBugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in\nCodeXGLUE. Our code and model are publicly available at\nhttps://github.com/gonglinyuan/ast_t5.", "published": "2024-01-05 06:51:08", "link": "http://arxiv.org/abs/2401.03003v4", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Exploring Gender Biases in Language Patterns of Human-Conversational\n  Agent Conversations", "abstract": "With the rise of human-machine communication, machines are increasingly\ndesigned with humanlike characteristics, such as gender, which can\ninadvertently trigger cognitive biases. Many conversational agents (CAs), such\nas voice assistants and chatbots, default to female personas, leading to\nconcerns about perpetuating gender stereotypes and inequality. Critiques have\nemerged regarding the potential objectification of females and reinforcement of\ngender stereotypes by these technologies. This research, situated in\nconversational AI design, aims to delve deeper into the impacts of gender\nbiases in human-CA interactions. From a behavioral and communication research\nstandpoint, this program focuses not only on perceptions but also the\nlinguistic styles of users when interacting with CAs, as previous research has\nrarely explored. It aims to understand how pre-existing gender biases might be\ntriggered by CAs' gender designs. It further investigates how CAs' gender\ndesigns may reinforce gender biases and extend them to human-human\ncommunication. The findings aim to inform ethical design of conversational\nagents, addressing whether gender assignment in CAs is appropriate and how to\npromote gender equality in design.", "published": "2024-01-05 19:11:17", "link": "http://arxiv.org/abs/2401.03030v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Can Large Language Models Understand Molecules?", "abstract": "Purpose: Large Language Models (LLMs) like GPT (Generative Pre-trained\nTransformer) from OpenAI and LLaMA (Large Language Model Meta AI) from Meta AI\nare increasingly recognized for their potential in the field of\ncheminformatics, particularly in understanding Simplified Molecular Input Line\nEntry System (SMILES), a standard method for representing chemical structures.\nThese LLMs also have the ability to decode SMILES strings into vector\nrepresentations.\n  Method: We investigate the performance of GPT and LLaMA compared to\npre-trained models on SMILES in embedding SMILES strings on downstream tasks,\nfocusing on two key applications: molecular property prediction and drug-drug\ninteraction prediction.\n  Results: We find that SMILES embeddings generated using LLaMA outperform\nthose from GPT in both molecular property and DDI prediction tasks. Notably,\nLLaMA-based SMILES embeddings show results comparable to pre-trained models on\nSMILES in molecular prediction tasks and outperform the pre-trained models for\nthe DDI prediction tasks.\n  Conclusion: The performance of LLMs in generating SMILES embeddings shows\ngreat potential for further investigation of these models for molecular\nembedding. We hope our study bridges the gap between LLMs and molecular\nembedding, motivating additional research into the potential of LLMs in the\nmolecular representation field. GitHub:\nhttps://github.com/sshaghayeghs/LLaMA-VS-GPT", "published": "2024-01-05 18:31:34", "link": "http://arxiv.org/abs/2402.00024v3", "categories": ["q-bio.BM", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "q-bio.BM"}
{"title": "Towards Weakly Supervised Text-to-Audio Grounding", "abstract": "Text-to-audio grounding (TAG) task aims to predict the onsets and offsets of\nsound events described by natural language. This task can facilitate\napplications such as multimodal information retrieval. This paper focuses on\nweakly-supervised text-to-audio grounding (WSTAG), where frame-level\nannotations of sound events are unavailable, and only the caption of a whole\naudio clip can be utilized for training. WSTAG is superior to\nstrongly-supervised approaches in its scalability to large audio-text datasets.\nTwo WSTAG frameworks are studied in this paper: sentence-level and\nphrase-level. First, we analyze the limitations of mean pooling used in the\nprevious WSTAG approach and investigate the effects of different pooling\nstrategies. We then propose phrase-level WSTAG to use matching labels between\naudio clips and phrases for training. Advanced negative sampling strategies and\nself-supervision are proposed to enhance the accuracy of the weak labels and\nprovide pseudo strong labels. Experimental results show that our system\nsignificantly outperforms the previous WSTAG SOTA. Finally, we conduct\nextensive experiments to analyze the effects of several factors on phrase-level\nWSTAG. The code and model is available at\nhttps://github.com/wsntxxn/TextToAudioGrounding.", "published": "2024-01-05 00:27:32", "link": "http://arxiv.org/abs/2401.02584v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Gradient weighting for speaker verification in extremely low\n  Signal-to-Noise Ratio", "abstract": "Speaker verification is hampered by background noise, particularly at\nextremely low Signal-to-Noise Ratio (SNR) under 0 dB. It is difficult to\nsuppress noise without introducing unwanted artifacts, which adversely affects\nspeaker verification. We proposed the mechanism called Gradient Weighting\n(Grad-W), which dynamically identifies and reduces artifact noise during\nprediction. The mechanism is based on the property that the gradient indicates\nwhich parts of the input the model is paying attention to. Specifically, when\nthe speaker network focuses on a region in the denoised utterance but not on\nthe clean counterpart, we consider it artifact noise and assign higher weights\nfor this region during optimization of enhancement. We validate it by training\nan enhancement model and testing the enhanced utterance on speaker\nverification. The experimental results show that our approach effectively\nreduces artifact noise, improving speaker verification across various SNR\nlevels.", "published": "2024-01-05 04:09:27", "link": "http://arxiv.org/abs/2401.02626v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A unified multichannel far-field speech recognition system: combining\n  neural beamforming with attention based end-to-end model", "abstract": "Far-field speech recognition is a challenging task that conventionally uses\nsignal processing beamforming to attack noise and interference problem. But the\nperformance has been found usually limited due to heavy reliance on\nenvironmental assumption. In this paper, we propose a unified multichannel\nfar-field speech recognition system that combines the neural beamforming and\ntransformer-based Listen, Spell, Attend (LAS) speech recognition system, which\nextends the end-to-end speech recognition system further to include speech\nenhancement. Such framework is then jointly trained to optimize the final\nobjective of interest. Specifically, factored complex linear projection (fCLP)\nhas been adopted to form the neural beamforming. Several pooling strategies to\ncombine look directions are then compared in order to find the optimal\napproach. Moreover, information of the source direction is also integrated in\nthe beamforming to explore the usefulness of source direction as a prior, which\nis usually available especially in multi-modality scenario. Experiments on\ndifferent microphone array geometry are conducted to evaluate the robustness\nagainst spacing variance of microphone array. Large in-house databases are used\nto evaluate the effectiveness of the proposed framework and the proposed method\nachieve 19.26\\% improvement when compared with a strong baseline.", "published": "2024-01-05 07:11:13", "link": "http://arxiv.org/abs/2401.02673v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical\n  Representation of Symbolic Music", "abstract": "In addressing the challenge of interpretability and generalizability of\nartificial music intelligence, this paper introduces a novel symbolic\nrepresentation that amalgamates both explicit and implicit musical information\nacross diverse traditions and granularities. Utilizing a hierarchical and-or\ngraph representation, the model employs nodes and edges to encapsulate a broad\nspectrum of musical elements, including structures, textures, rhythms, and\nharmonies. This hierarchical approach expands the representability across\nvarious scales of music. This representation serves as the foundation for an\nenergy-based model, uniquely tailored to learn musical concepts through a\nflexible algorithm framework relying on the minimax entropy principle.\nUtilizing an adapted Metropolis-Hastings sampling technique, the model enables\nfine-grained control over music generation. A comprehensive empirical\nevaluation, contrasting this novel approach with existing methodologies,\nmanifests considerable advancements in interpretability and controllability.\nThis study marks a substantial contribution to the fields of music analysis,\ncomposition, and computational musicology.", "published": "2024-01-05 07:24:07", "link": "http://arxiv.org/abs/2401.02678v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "StreamVC: Real-Time Low-Latency Voice Conversion", "abstract": "We present StreamVC, a streaming voice conversion solution that preserves the\ncontent and prosody of any source speech while matching the voice timbre from\nany target speech. Unlike previous approaches, StreamVC produces the resulting\nwaveform at low latency from the input signal even on a mobile platform, making\nit applicable to real-time communication scenarios like calls and video\nconferencing, and addressing use cases such as voice anonymization in these\nscenarios. Our design leverages the architecture and training strategy of the\nSoundStream neural audio codec for lightweight high-quality speech synthesis.\nWe demonstrate the feasibility of learning soft speech units causally, as well\nas the effectiveness of supplying whitened fundamental frequency information to\nimprove pitch stability without leaking the source timbre information.", "published": "2024-01-05 22:37:26", "link": "http://arxiv.org/abs/2401.03078v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Using perceptive subbands analysis to perform audio scenes cartography", "abstract": "Audio scene cartography for real or simulated stereo recordings is presented.\nThis audio scene analysis is performed doing successively: a perceptive\n10-subbands analysis, calculation of temporal laws for relative delays and\ngains between both channels of each subband using a short-time cons\\-tant scene\nassumption and channels inter-correlation which permit to follow a mobile\nsource in its moves, calculation of global and subbands histograms whose peaks\ngive the incidence information for fixed sources. Audio scenes composed of 2 to\n4 fixed sources or with a fixed source and a mobile one have been already\nsuccessfully tested. Further extensions and applications will be discussed.\nAudio illustrations of audio scenes, subband analysis and demonstration of\nreal-time stereo recording simulations will be given.Paper 6340 presented at\nthe 118th Convention of the Audio Engineering Society, Barcelona, 2005", "published": "2024-01-05 10:03:47", "link": "http://arxiv.org/abs/2401.04127v1", "categories": ["eess.AS", "cs.SD", "eess.SP", "physics.class-ph"], "primary_category": "eess.AS"}
