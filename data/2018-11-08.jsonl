{"title": "Confusion2Vec: Towards Enriching Vector Space Word Representations with\n  Representational Ambiguities", "abstract": "Word vector representations are a crucial part of Natural Language Processing\n(NLP) and Human Computer Interaction. In this paper, we propose a novel word\nvector representation, Confusion2Vec, motivated from the human speech\nproduction and perception that encodes representational ambiguity. Humans\nemploy both acoustic similarity cues and contextual cues to decode information\nand we focus on a model that incorporates both sources of information. The\nrepresentational ambiguity of acoustics, which manifests itself in word\nconfusions, is often resolved by both humans and machines through contextual\ncues. A range of representational ambiguities can emerge in various domains\nfurther to acoustic perception, such as morphological transformations,\nparaphrasing for NLP tasks like machine translation etc. In this work, we\npresent a case study in application to Automatic Speech Recognition (ASR),\nwhere the word confusions are related to acoustic similarity. We present\nseveral techniques to train an acoustic perceptual similarity representation\nambiguity. We term this Confusion2Vec and learn on unsupervised-generated data\nfrom ASR confusion networks or lattice-like structures. Appropriate evaluations\nfor the Confusion2Vec are formulated for gauging acoustic similarity in\naddition to semantic-syntactic and word similarity evaluations. The\nConfusion2Vec is able to model word confusions efficiently, without\ncompromising on the semantic-syntactic word relations, thus effectively\nenriching the word vector space with extra task relevant ambiguity information.\nWe provide an intuitive exploration of the 2-dimensional Confusion2Vec space\nusing Principal Component Analysis of the embedding and relate to semantic,\nsyntactic and acoustic relationships. The potential of Confusion2Vec in the\nutilization of uncertainty present in lattices is demonstrated through small\nexamples relating to ASR error correction.", "published": "2018-11-08 00:40:25", "link": "http://arxiv.org/abs/1811.03199v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods\n  Across Different Languages", "abstract": "Modern information systems are changing the idea of \"data processing\" to the\nidea of \"concept processing\", meaning that instead of processing words, such\nsystems process semantic concepts which carry meaning and share contexts with\nother concepts. Ontology is commonly used as a structure that captures the\nknowledge about a certain area via providing concepts and relations between\nthem. Traditionally, concept hierarchies have been built manually by knowledge\nengineers or domain experts. However, the manual construction of a concept\nhierarchy suffers from several limitations such as its coverage and the\nenormous costs of its extension and maintenance. Ontology learning, usually\nreferred to the (semi-)automatic support in ontology development, is usually\ndivided into steps, going from concepts identification, passing through\nhierarchy and non-hierarchy relations detection and, seldom, axiom extraction.\nIt is reasonable to say that among these steps the current frontier is in the\nestablishment of concept hierarchies, since this is the backbone of ontologies\nand, therefore, a good concept hierarchy is already a valuable resource for\nmany ontology applications. The automatic construction of concept hierarchies\nfrom texts is a complex task and much work have been proposing approaches to\nbetter extract relations between concepts. These different proposals have never\nbeen contrasted against each other on the same set of data and across different\nlanguages. Such comparison is important to see whether they are complementary\nor incremental. Also, we can see whether they present different tendencies\ntowards recall and precision. This paper evaluates these different methods on\nthe basis of hierarchy metrics such as density and depth, and evaluation\nmetrics such as Recall and Precision. Results shed light over the comprehensive\nset of methods according to the literature in the area.", "published": "2018-11-08 03:19:59", "link": "http://arxiv.org/abs/1811.03245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Applying Distributional Compositional Categorical Models of Meaning to\n  Language Translation", "abstract": "The aim of this paper is twofold: first we will use vector space\ndistributional compositional categorical models of meaning to compare the\nmeaning of sentences in Irish and in English (and thus ascertain when a\nsentence is the translation of another sentence) using the cosine similarity\nscore. Then we shall outline a procedure which translates nouns by\nunderstanding their context, using a conceptual space model of cognition. We\nshall use metrics on the category ConvexRel to determine the distance between\nconcepts (and determine when a noun is the translation of another noun). This\npaper will focus on applications to Irish, a member of the Gaelic family of\nlanguages.", "published": "2018-11-08 05:10:51", "link": "http://arxiv.org/abs/1811.03274v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Untangling the GDPR Using ConRelMiner", "abstract": "The General Data Protection Regulation (GDPR) poses enormous challenges on\ncompanies and organizations with respect to understanding, implementing, and\nmaintaining the contained constraints. We report on how the ConRelMiner method\ncan be used for untangling the GDPR. For this, the GDPR is filtered and grouped\nalong the roles mentioned by the GDPR and the reduction of sentences to be read\nby analysts is shown. Moreover, the output of the ConRelMiner - a cluster graph\nwith relations between the sentences - is displayed and interpreted. Overall\nthe goal is to illustrate how the effort for implementing the GDPR can be\nreduced and a structured and meaningful representation of the relevant GDPR\nsentences can be found.", "published": "2018-11-08 13:28:01", "link": "http://arxiv.org/abs/1811.03399v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Representation for Easy-First Dependency Parsing", "abstract": "Easy-first parsing relies on subtree re-ranking to build the complete parse\ntree. Whereas the intermediate state of parsing processing is represented by\nvarious subtrees, whose internal structural information is the key lead for\nlater parsing action decisions, we explore a better representation for such\nsubtrees. In detail, this work introduces a bottom-up subtree encoding method\nbased on the child-sum tree-LSTM. Starting from an easy-first dependency parser\nwithout other handcraft features, we show that the effective subtree encoder\ndoes promote the parsing process, and can make a greedy search easy-first\nparser achieve promising results on benchmark treebanks compared to\nstate-of-the-art baselines. Furthermore, with the help of the current\npre-training language model, we further improve the state-of-the-art results of\nthe easy-first approach.", "published": "2018-11-08 15:59:11", "link": "http://arxiv.org/abs/1811.03511v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot learning with attention-based sequence-to-sequence models", "abstract": "End-to-end approaches have recently become popular as a means of simplifying\nthe training and deployment of speech recognition systems. However, they often\nrequire large amounts of data to perform well on large vocabulary tasks. With\nthe aim of making end-to-end approaches usable by a broader range of\nresearchers, we explore the potential to use end-to-end methods in small\nvocabulary contexts where smaller datasets may be used. A significant drawback\nof small-vocabulary systems is the difficulty of expanding the vocabulary\nbeyond the original training samples -- therefore we also study strategies to\nextend the vocabulary with only few examples per new class (few-shot learning).\n  Our results show that an attention-based encoder-decoder can be competitive\nagainst a strong baseline on a small vocabulary keyword classification task,\nreaching 97.5% of accuracy on Tensorflow's Speech Commands dataset. It also\nshows promising results on the few-shot learning problem where a simple\nstrategy achieved 68.8\\% of accuracy on new keywords with only 10 examples for\neach new class. This score goes up to 88.4\\% with a larger set of 100 examples.", "published": "2018-11-08 16:05:50", "link": "http://arxiv.org/abs/1811.03519v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Argument Prediction as Reading Comprehension", "abstract": "Implicit arguments, which cannot be detected solely through syntactic cues,\nmake it harder to extract predicate-argument tuples. We present a new model for\nimplicit argument prediction that draws on reading comprehension, casting the\npredicate-argument tuple with the missing argument as a query. We also draw on\npointer networks and multi-hop computation. Our model shows good performance on\nan argument cloze task as well as on a nominal implicit argument prediction\ntask.", "published": "2018-11-08 17:13:46", "link": "http://arxiv.org/abs/1811.03554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Federated Learning for Mobile Keyboard Prediction", "abstract": "We train a recurrent neural network language model using a distributed,\non-device learning framework called federated learning for the purpose of\nnext-word prediction in a virtual keyboard for smartphones. Server-based\ntraining using stochastic gradient descent is compared with training on client\ndevices using the Federated Averaging algorithm. The federated algorithm, which\nenables training on a higher-quality dataset for this use case, is shown to\nachieve better prediction recall. This work demonstrates the feasibility and\nbenefit of training language models on client devices without exporting\nsensitive user data to servers. The federated learning environment gives users\ngreater control over the use of their data and simplifies the task of\nincorporating privacy by default with distributed training and aggregation\nacross a population of client devices.", "published": "2018-11-08 18:37:03", "link": "http://arxiv.org/abs/1811.03604v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Flow in Pregroup Models of Natural Language", "abstract": "This paper is about pregroup models of natural languages, and how they relate\nto the explicitly categorical use of pregroups in Compositional Distributional\nSemantics and Natural Language Processing. These categorical interpretations\nmake certain assumptions about the nature of natural languages that, when\nstated formally, may be seen to impose strong restrictions on pregroup grammars\nfor natural languages.\n  We formalize this as a hypothesis about the form that pregroup models of\nnatural languages must take, and demonstrate by an artificial language example\nthat these restrictions are not imposed by the pregroup axioms themselves. We\ncompare and contrast the artificial language examples with natural languages\n(using Welsh, a language where the 'noun' type cannot be taken as primitive, as\nan illustrative example).\n  The hypothesis is simply that there must exist a causal connection, or\ninformation flow, between the words of a sentence in a language whose purpose\nis to communicate information. This is not necessarily the case with formal\nlanguages that are simply generated by a series of 'meaning-free' rules. This\nimposes restrictions on the types of pregroup grammars that we expect to find\nin natural languages; we formalize this in algebraic, categorical, and\ngraphical terms.\n  We take some preliminary steps in providing conditions that ensure pregroup\nmodels satisfy these conjectured properties, and discuss the more general forms\nthis hypothesis may take.", "published": "2018-11-08 05:10:34", "link": "http://arxiv.org/abs/1811.03273v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Quantum Semantic Correlations in Hate and Non-Hate Speeches", "abstract": "This paper aims to apply the notions of quantum geometry and correlation to\nthe typification of semantic relations between couples of keywords in different\ndocuments. In particular we analysed texts classified as hate / non hate\nspeeches, containing the keywords \"women\", \"white\", and \"black\". The paper\ncompares this approach to cosine similarity, a classical methodology, to cast\nlight on the notion of \"similar meaning\".", "published": "2018-11-08 05:11:37", "link": "http://arxiv.org/abs/1811.03275v1", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "primary_category": "cs.IR"}
{"title": "Doc2Im: document to image conversion through self-attentive embedding", "abstract": "Text classification is a fundamental task in NLP applications. Latest\nresearch in this field has largely been divided into two major sub-fields.\nLearning representations is one sub-field and learning deeper models, both\nsequential and convolutional, which again connects back to the representation\nis the other side. We posit the idea that the stronger the representation is,\nthe simpler classifier models are needed to achieve higher performance. In this\npaper we propose a completely novel direction to text classification research,\nwherein we convert text to a representation very similar to images, such that\nany deep network able to handle images is equally able to handle text. We take\na deeper look at the representation of documents as an image and subsequently\nutilize very simple convolution based models taken as is from computer vision\ndomain. This image can be cropped, re-scaled, re-sampled and augmented just\nlike any other image to work with most of the state-of-the-art large\nconvolution based models which have been designed to handle large image\ndatasets. We show impressive results with some of the latest benchmarks in the\nrelated fields. We perform transfer learning experiments, both from text to\ntext domain and also from image to text domain. We believe this is a paradigm\nshift from the way document understanding and text classification has been\ntraditionally done, and will drive numerous novel research ideas in the\ncommunity.", "published": "2018-11-08 06:51:46", "link": "http://arxiv.org/abs/1811.03291v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Marshall-Olkin Power-Law Distributions in Length-Frequency of Entities", "abstract": "Entities involve important concepts with concrete meanings and play important\nroles in numerous linguistic tasks. Entities have different forms in different\nlinguistic tasks and researchers treat those different forms as different\nconcepts. In this paper, we are curious to know whether there are some common\ncharacteristics that connect those different forms of entities. Specifically,\nwe investigate the underlying distributions of entities from different types\nand different languages, trying to figure out some common characteristics\nbehind those diverse entities. After analyzing twelve datasets about different\ntypes of entities and eighteen datasets about entities in different languages,\nwe find that while these entities are dramatically diverse from each other in\nmany aspects, their length-frequencies can be well characterized by a family of\nMarshall-Olkin power-law (MOPL) distributions. We conduct experiments on those\nthirty datasets about entities in different types and different languages, and\nexperimental results demonstrate that MOPL models characterize the\nlength-frequencies of entities much better than two state-of-the-art power-law\nmodels and an alternative log-normal model. Experimental results also\ndemonstrate that MOPL models are scalable to the length-frequency of entities\nin large-scale real-world datasets.", "published": "2018-11-08 09:16:19", "link": "http://arxiv.org/abs/1811.03325v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Neural Networks for Query Expansion using Word Embeddings", "abstract": "Query expansion is a method for alleviating the vocabulary mismatch problem\npresent in information retrieval tasks. Previous works have shown that terms\nselected for query expansion by traditional methods such as pseudo-relevance\nfeedback are not always helpful to the retrieval process. In this paper, we\nshow that this is also true for more recently proposed embedding-based query\nexpansion methods. We then introduce an artificial neural network classifier to\npredict the usefulness of query expansion terms. This classifier uses term word\nembeddings as inputs. We perform experiments on four TREC newswire and web\ncollections show that using terms selected by the classifier for expansion\nsignificantly improves retrieval performance when compared to competitive\nbaselines. The results are also shown to be more robust than the baselines.", "published": "2018-11-08 16:01:35", "link": "http://arxiv.org/abs/1811.03514v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Internal Wiring of Cartesian Verbs and Prepositions", "abstract": "Categorical compositional distributional semantics (CCDS) allows one to\ncompute the meaning of phrases and sentences from the meaning of their\nconstituent words. A type-structure carried over from the traditional\ncategorial model of grammar a la Lambek becomes a 'wire-structure' that\nmediates the interaction of word meanings. However, CCDS has a much richer\nlogical structure than plain categorical semantics in that certain words can\nalso be given an 'internal wiring' that either provides their entire meaning or\nreduces the size their meaning space. Previous examples of internal wiring\ninclude relative pronouns and intersective adjectives. Here we establish the\nsame for a large class of well-behaved transitive verbs to which we refer as\nCartesian verbs, and reduce the meaning space from a ternary tensor to a unary\none. Some experimental evidence is also provided.", "published": "2018-11-08 05:11:59", "link": "http://arxiv.org/abs/1811.05770v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Phonetic-attention scoring for deep speaker features in speaker\n  verification", "abstract": "Recent studies have shown that frame-level deep speaker features can be\nderived from a deep neural network with the training target set to discriminate\nspeakers by a short speech segment. By pooling the frame-level features,\nutterance-level representations, called d-vectors, can be derived and used in\nthe automatic speaker verification (ASV) task. This simple average pooling,\nhowever, is inherently sensitive to the phonetic content of the utterance. An\ninteresting idea borrowed from machine translation is the attention-based\nmechanism, where the contribution of an input word to the translation at a\nparticular time is weighted by an attention score. This score reflects the\nrelevance of the input word and the present translation. We can use the same\nidea to align utterances with different phonetic contents. This paper proposes\na phonetic-attention scoring approach for d-vector systems. By this approach,\nan attention score is computed for each frame pair. This score reflects the\nsimilarity of the two frames in phonetic content, and is used to weigh the\ncontribution of this frame pair in the utterance-based scoring. This new\nscoring approach emphasizes the frame pairs with similar phonetic contents,\nwhich essentially provides a soft alignment for utterances with any phonetic\ncontents. Experimental results show that compared with the naive average\npooling, this phonetic-attention scoring approach can deliver consistent\nperformance improvement in ASV tasks of both text-dependent and\ntext-independent.", "published": "2018-11-08 03:54:55", "link": "http://arxiv.org/abs/1811.03255v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gaussian-Constrained training for speaker verification", "abstract": "Neural models, in particular the d-vector and x-vector architectures, have\nproduced state-of-the-art performance on many speaker verification tasks.\nHowever, two potential problems of these neural models deserve more\ninvestigation. Firstly, both models suffer from `information leak', which means\nthat some parameters participating in model training will be discarded during\ninference, i.e, the layers that are used as the classifier. Secondly, these\nmodels do not regulate the distribution of the derived speaker vectors. This\n`unconstrained distribution' may degrade the performance of the subsequent\nscoring component, e.g., PLDA. This paper proposes a Gaussian-constrained\ntraining approach that (1) discards the parametric classifier, and (2) enforces\nthe distribution of the derived speaker vectors to be Gaussian. Our experiments\non the VoxCeleb and SITW databases demonstrated that this new training approach\nproduced more representative and regular speaker embeddings, leading to\nconsistent performance improvement.", "published": "2018-11-08 04:03:09", "link": "http://arxiv.org/abs/1811.03258v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Classical Copying versus Quantum Entanglement in Natural Language: The\n  Case of VP-ellipsis", "abstract": "This paper compares classical copying and quantum entanglement in natural\nlanguage by considering the case of verb phrase (VP) ellipsis. VP ellipsis is a\nnon-linear linguistic phenomenon that requires the reuse of resources, making\nit the ideal test case for a comparative study of different copying behaviours\nin compositional models of natural language. Following the line of research in\ncompositional distributional semantics set out by (Coecke et al., 2010) we\ndevelop an extension of the Lambek calculus which admits a controlled form of\ncontraction to deal with the copying of linguistic resources. We then develop\ntwo different compositional models of distributional meaning for this calculus.\nIn the first model, we follow the categorical approach of (Coecke et al., 2013)\nin which a functorial passage sends the proofs of the grammar to linear maps on\nvector spaces and we use Frobenius algebras to allow for copying. In the second\ncase, we follow the more traditional approach that one finds in categorial\ngrammars, whereby an intermediate step interprets proofs as non-linear lambda\nterms, using multiple variable occurrences that model classical copying. As a\ncase study, we apply the models to derive different readings of ambiguous\nelliptical phrases and compare the analyses that each model provides.", "published": "2018-11-08 05:12:46", "link": "http://arxiv.org/abs/1811.03276v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Towards Compositional Distributional Discourse Analysis", "abstract": "Categorical compositional distributional semantics provide a method to derive\nthe meaning of a sentence from the meaning of its individual words: the\ngrammatical reduction of a sentence automatically induces a linear map for\ncomposing the word vectors obtained from distributional semantics. In this\npaper, we extend this passage from word-to-sentence to sentence-to-discourse\ncomposition. To achieve this we introduce a notion of basic anaphoric\ndiscourses as a mid-level representation between natural language discourse\nformalised in terms of basic discourse representation structures (DRS); and\nknowledge base queries over the Semantic Web as described by basic graph\npatterns in the Resource Description Framework (RDF). This provides a\nhigh-level specification for compositional algorithms for question answering\nand anaphora resolution, and allows us to give a picture of natural language\nunderstanding as a process involving both statistical and logical resources.", "published": "2018-11-08 05:14:19", "link": "http://arxiv.org/abs/1811.03277v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "Translating and Evolving: Towards a Model of Language Change in DisCoCat", "abstract": "The categorical compositional distributional (DisCoCat) model of meaning\ndeveloped by Coecke et al. (2010) has been successful in modeling various\naspects of meaning. However, it fails to model the fact that language can\nchange. We give an approach to DisCoCat that allows us to represent language\nmodels and translations between them, enabling us to describe translations from\none language to another, or changes within the same language. We unify the\nproduct space representation given in (Coecke et al., 2010) and the functorial\ndescription in (Kartsaklis et al., 2013), in a way that allows us to view a\nlanguage as a catalogue of meanings. We formalize the notion of a lexicon in\nDisCoCat, and define a dictionary of meanings between two lexicons. All this is\ndone within the framework of monoidal categories. We give examples of how to\napply our methods, and give a concrete suggestion for compositional translation\nin corpora.", "published": "2018-11-08 05:11:18", "link": "http://arxiv.org/abs/1811.11041v1", "categories": ["cs.CL", "cs.AI", "math.CT"], "primary_category": "cs.CL"}
{"title": "A Comparison of Lattice-free Discriminative Training Criteria for Purely\n  Sequence-Trained Neural Network Acoustic Models", "abstract": "In this work, three lattice-free (LF) discriminative training criteria for\npurely sequence-trained neural network acoustic models are compared on LVCSR\ntasks, namely maximum mutual information (MMI), boosted maximum mutual\ninformation (bMMI) and state-level minimum Bayes risk (sMBR). We demonstrate\nthat, analogous to LF-MMI, a neural network acoustic model can also be trained\nfrom scratch using LF-bMMI or LF-sMBR criteria respectively without the need of\ncross-entropy pre-training. Furthermore, experimental results on\nSwitchboard-300hrs and Switchboard+Fisher-2100hrs datasets show that models\ntrained with LF-bMMI consistently outperform those trained with plain LF-MMI\nand achieve a relative word error rate (WER) reduction of 5% over competitive\ntemporal convolution projected LSTM (TDNN-LSTMP) LF-MMI baselines.", "published": "2018-11-08 22:37:55", "link": "http://arxiv.org/abs/1811.03700v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Disentangled Representations for Timber and Pitch in Music\n  Audio", "abstract": "Timbre and pitch are the two main perceptual properties of musical sounds.\nDepending on the target applications, we sometimes prefer to focus on one of\nthem, while reducing the effect of the other. Researchers have managed to\nhand-craft such timbre-invariant or pitch-invariant features using domain\nknowledge and signal processing techniques, but it remains difficult to\ndisentangle them in the resulting feature representations. Drawing upon\nstate-of-the-art techniques in representation learning, we propose in this\npaper two deep convolutional neural network models for learning disentangled\nrepresentation of musical timbre and pitch. Both models use encoders/decoders\nand adversarial training to learn music representations, but the second model\nadditionally uses skip connections to deal with the pitch information. As music\nis an art of time, the two models are supervised by frame-level instrument and\npitch labels using a new dataset collected from MuseScore. We compare the\nresult of the two disentangling models with a new evaluation protocol called\n\"timbre crossover\", which leads to interesting applications in audio-domain\nmusic editing. Via various objective evaluations, we show that the second model\ncan better change the instrumentation of a multi-instrument music piece without\nmuch affecting the pitch structure. By disentangling timbre and pitch, we\nenvision that the model can contribute to generating more realistic music audio\nas well.", "published": "2018-11-08 05:05:50", "link": "http://arxiv.org/abs/1811.03271v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Who Do I Sound Like? Showcasing Speaker Recognition Technology by\n  YouTube Voice Search", "abstract": "The popularization of science can often be disregarded by scientists as it\nmay be challenging to put highly sophisticated research into words that general\npublic can understand. This work aims to help presenting speaker recognition\nresearch to public by proposing a publicly appealing concept for showcasing\nrecognition systems. We leverage data from YouTube and use it in a large-scale\nvoice search web application that finds the celebrity voices that best match to\nthe user's voice. The concept was tested in a public event as well as \"in the\nwild\" and the received feedback was mostly positive. The i-vector based speaker\nidentification back end was found to be fast (665 ms per request) and had a\nhigh identification accuracy (93 %) for the YouTube target speakers. To help\nother researchers to develop the idea further, we share the source codes of the\nweb platform used for the demo at\nhttps://github.com/bilalsoomro/speech-demo-platform.", "published": "2018-11-08 07:19:33", "link": "http://arxiv.org/abs/1811.03293v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Enhancement Based on Reducing the Detail Portion of Speech\n  Spectrograms in Modulation Domain via Discrete Wavelet Transform", "abstract": "In this paper, we propose a novel speech enhancement (SE) method by\nexploiting the discrete wavelet transform (DWT). This new method reduces the\namount of fast time-varying portion, viz. the DWT-wise detail component, in the\nspectrogram of speech signals so as to highlight the speech-dominant component\nand achieves better speech quality. A particularity of this new method is that\nit is completely unsupervised and requires no prior information about the clean\nspeech and noise in the processed utterance. The presented DWT-based SE method\nwith various scaling factors for the detail part is evaluated with a subset of\nAurora-2 database, and the PESQ metric is used to indicate the quality of\nprocessed speech signals. The preliminary results show that the processed\nspeech signals reveal a higher PESQ score in comparison with the original\ncounterparts. Furthermore, we show that this method can still enhance the\nsignal by totally discarding the detail part (setting the respective scaling\nfactor to zero), revealing that the spectrogram can be down-sampled and thus\ncompressed without the cost of lowered quality. In addition, we integrate this\nnew method with conventional speech enhancement algorithms, including spectral\nsubtraction, Wiener filtering, and spectral MMSE estimation, and show that the\nresulting integration behaves better than the respective component method. As a\nresult, this new method is quite effective in improving the speech quality and\nwell additive to the other SE methods.", "published": "2018-11-08 15:16:43", "link": "http://arxiv.org/abs/1811.03486v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker-adaptive neural vocoders for parametric speech synthesis systems", "abstract": "This paper proposes speaker-adaptive neural vocoders for parametric\ntext-to-speech (TTS) systems. Recently proposed WaveNet-based neural vocoding\nsystems successfully generate a time sequence of speech signal with an\nautoregressive framework. However, it remains a challenge to synthesize\nhigh-quality speech when the amount of a target speaker's training data is\ninsufficient. To generate more natural speech signals with the constraint of\nlimited training data, we propose a speaker adaptation task with an effective\nvariation of neural vocoding models. In the proposed method, a\nspeaker-independent training method is applied to capture universal attributes\nembedded in multiple speakers, and the trained model is then optimized to\nrepresent the specific characteristics of the target speaker. Experimental\nresults verify that the proposed TTS systems with speaker-adaptive neural\nvocoders outperform those with traditional source-filter model-based vocoders\nand those with WaveNet vocoders, trained either speaker-dependently or\nspeaker-independently. In particular, our TTS system achieves 3.80 and 3.77 MOS\nfor the Korean male and Korean female speakers, respectively, even though we\nuse only ten minutes' speech corpus for training the model.", "published": "2018-11-08 08:26:03", "link": "http://arxiv.org/abs/1811.03311v5", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
