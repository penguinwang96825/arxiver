{"title": "Answering Complicated Question Intents Expressed in Decomposed Question\n  Sequences", "abstract": "Recent work in semantic parsing for question answering has focused on long\nand complicated questions, many of which would seem unnatural if asked in a\nnormal conversation between two humans. In an effort to explore a\nconversational QA setting, we present a more realistic task: answering\nsequences of simple but inter-related questions. We collect a dataset of 6,066\nquestion sequences that inquire about semi-structured tables from Wikipedia,\nwith 17,553 question-answer pairs in total. Existing QA systems face two major\nproblems when evaluated on our dataset: (1) handling questions that contain\ncoreferences to previous questions or answers, and (2) matching words or\nphrases in a question to corresponding entries in the associated table. We\nconclude by proposing strategies to handle both of these issues.", "published": "2016-11-04 01:54:03", "link": "http://arxiv.org/abs/1611.01242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "abstract": "The success of long short-term memory (LSTM) neural networks in language\nprocessing is typically attributed to their ability to capture long-distance\nstatistical regularities. Linguistic regularities are often sensitive to\nsyntactic structure; can such dependencies be captured by LSTMs, which do not\nhave explicit structural representations? We begin addressing this question\nusing number agreement in English subject-verb dependencies. We probe the\narchitecture's grammatical competence both using training objectives with an\nexplicit grammatical target (number prediction, grammaticality judgments) and\nusing language models. In the strongly supervised settings, the LSTM achieved\nvery high overall accuracy (less than 1% errors), but errors increased when\nsequential and structural information conflicted. The frequency of such errors\nrose sharply in the language-modeling setting. We conclude that LSTMs can\ncapture a non-trivial amount of grammatical structure given targeted\nsupervision, but stronger architectures may be required to further reduce\nerrors; furthermore, the language modeling signal is insufficient for capturing\nsyntax-sensitive dependencies, and should be supplemented with more direct\nsupervision if such dependencies need to be captured.", "published": "2016-11-04 13:36:32", "link": "http://arxiv.org/abs/1611.01368v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Recurrent Span Representations for Extractive Question\n  Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence\ndocument, is a central problem in natural language understanding. Recent\nformulations of this task have typically focused on answer selection from a set\nof candidates pre-defined manually or through the use of an external NLP\npipeline. However, Rajpurkar et al. (2016) recently released the SQuAD dataset\nin which the answers can be arbitrary strings from the supplied text. In this\npaper, we focus on this answer extraction task, presenting a novel model\narchitecture that efficiently builds fixed length representations of all spans\nin the evidence document with a recurrent network. We show that scoring\nexplicit span representations significantly improves performance over other\napproaches that factor the prediction into separate predictions about words or\nstart and end markers. Our approach improves upon the best published results of\nWang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.'s\nbaseline by > 50%.", "published": "2016-11-04 16:12:46", "link": "http://arxiv.org/abs/1611.01436v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Morphological Inflection Generation with Hard Monotonic Attention", "abstract": "We present a neural model for morphological inflection generation which\nemploys a hard attention mechanism, inspired by the nearly-monotonic alignment\ncommonly found between the characters in a word and the characters in its\ninflection. We evaluate the model on three previously studied morphological\ninflection generation datasets and show that it provides state of the art\nresults in various setups compared to previous neural and non-neural\napproaches. Finally we present an analysis of the continuous representations\nlearned by both the hard and soft attention \\cite{bahdanauCB14} models for the\ntask, shedding some light on the features such models extract.", "published": "2016-11-04 18:42:47", "link": "http://arxiv.org/abs/1611.01487v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Generation of Multilingual Clusters for the Evaluation of\n  Distributed Representations", "abstract": "We propose a language-agnostic way of automatically generating sets of\nsemantically similar clusters of entities along with sets of \"outlier\"\nelements, which may then be used to perform an intrinsic evaluation of word\nembeddings in the outlier detection task. We used our methodology to create a\ngold-standard dataset, which we call WikiSem500, and evaluated multiple\nstate-of-the-art embeddings. The results show a correlation between performance\non this dataset and performance on sentiment analysis.", "published": "2016-11-04 21:35:07", "link": "http://arxiv.org/abs/1611.01547v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalized Topic Modeling", "abstract": "Recently there has been significant activity in developing algorithms with\nprovable guarantees for topic modeling. In standard topic models, a topic (such\nas sports, business, or politics) is viewed as a probability distribution $\\vec\na_i$ over words, and a document is generated by first selecting a mixture $\\vec\nw$ over topics, and then generating words i.i.d. from the associated mixture\n$A{\\vec w}$. Given a large collection of such documents, the goal is to recover\nthe topic vectors and then to correctly classify new documents according to\ntheir topic mixture.\n  In this work we consider a broad generalization of this framework in which\nwords are no longer assumed to be drawn i.i.d. and instead a topic is a complex\ndistribution over sequences of paragraphs. Since one could not hope to even\nrepresent such a distribution in general (even if paragraphs are given using\nsome natural feature representation), we aim instead to directly learn a\ndocument classifier. That is, we aim to learn a predictor that given a new\ndocument, accurately predicts its topic mixture, without learning the\ndistributions explicitly. We present several natural conditions under which one\ncan do this efficiently and discuss issues such as noise tolerance and sample\ncomplexity in this model. More generally, our model can be viewed as a\ngeneralization of the multi-view or co-training setting in machine learning.", "published": "2016-11-04 03:45:03", "link": "http://arxiv.org/abs/1611.01259v1", "categories": ["cs.LG", "cs.CL", "cs.DS", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language\n  Modeling", "abstract": "Recurrent neural networks have been very successful at predicting sequences\nof words in tasks such as language modeling. However, all such models are based\non the conventional classification framework, where the model is trained\nagainst one-hot targets, and each word is represented both as an input and as\nan output in isolation. This causes inefficiencies in learning both in terms of\nutilizing all of the information and in terms of the number of parameters\nneeded to train. We introduce a novel theoretical framework that facilitates\nbetter learning in language modeling, and show that our framework leads to\ntying together the input embedding and the output projection matrices, greatly\nreducing the number of trainable variables. Our framework leads to state of the\nart performance on the Penn Treebank with a variety of network models.", "published": "2016-11-04 17:36:20", "link": "http://arxiv.org/abs/1611.01462v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning to Rank Scientific Documents from the Crowd", "abstract": "Finding related published articles is an important task in any science, but\nwith the explosion of new work in the biomedical domain it has become\nespecially challenging. Most existing methodologies use text similarity metrics\nto identify whether two articles are related or not. However biomedical\nknowledge discovery is hypothesis-driven. The most related articles may not be\nones with the highest text similarities. In this study, we first develop an\ninnovative crowd-sourcing approach to build an expert-annotated\ndocument-ranking corpus. Using this corpus as the gold standard, we then\nevaluate the approaches of using text similarity to rank the relatedness of\narticles. Finally, we develop and evaluate a new supervised model to\nautomatically rank related scientific articles. Our results show that authors'\nranking differ significantly from rankings by text-similarity-based models. By\ntraining a learning-to-rank model on a subset of the annotated corpus, we found\nthe best supervised learning-to-rank model (SVM-Rank) significantly surpassed\nstate-of-the-art baseline systems.", "published": "2016-11-04 14:43:44", "link": "http://arxiv.org/abs/1611.01400v1", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG", "cs.SI"], "primary_category": "cs.IR"}
