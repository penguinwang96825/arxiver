{"title": "Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual\n  Transfer", "abstract": "Multilingual pre-trained contextual embedding models (Devlin et al., 2019)\nhave achieved impressive performance on zero-shot cross-lingual transfer tasks.\nFinding the most effective fine-tuning strategy to fine-tune these models on\nhigh-resource languages so that it transfers well to the zero-shot languages is\na non-trivial task. In this paper, we propose a novel meta-optimizer to\nsoft-select which layers of the pre-trained model to freeze during fine-tuning.\nWe train the meta-optimizer by simulating the zero-shot transfer scenario.\nResults on cross-lingual natural language inference show that our approach\nimproves over the simple fine-tuning baseline and X-MAML (Nooralahzadeh et al.,\n2020).", "published": "2021-07-21 02:16:45", "link": "http://arxiv.org/abs/2107.09840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guided Generation of Cause and Effect", "abstract": "We present a conditional text generation framework that posits sentential\nexpressions of possible causes and effects. This framework depends on two novel\nresources we develop in the course of this work: a very large-scale collection\nof English sentences expressing causal patterns CausalBank; and a refinement\nover previous work on constructing large lexical causal knowledge graphs Cause\nEffect Graph. Further, we extend prior work in lexically-constrained decoding\nto support disjunctive positive constraints. Human assessment confirms that our\napproach gives high-quality and diverse outputs. Finally, we use CausalBank to\nperform continued training of an encoder supporting a recent state-of-the-art\nmodel for causal reasoning, leading to a 3-point improvement on the COPA\nchallenge set, with no change in model architecture.", "published": "2021-07-21 02:32:47", "link": "http://arxiv.org/abs/2107.09846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CausalBERT: Injecting Causal Knowledge Into Pre-trained Models with\n  Minimal Supervision", "abstract": "Recent work has shown success in incorporating pre-trained models like BERT\nto improve NLP systems. However, existing pre-trained models lack of causal\nknowledge which prevents today's NLP systems from thinking like humans. In this\npaper, we investigate the problem of injecting causal knowledge into\npre-trained models. There are two fundamental problems: 1) how to collect\nvarious granularities of causal pairs from unstructured texts; 2) how to\neffectively inject causal knowledge into pre-trained models. To address these\nissues, we extend the idea of CausalBERT from previous studies, and conduct\nexperiments on various datasets to evaluate its effectiveness. In addition, we\nadopt a regularization-based method to preserve the already learned knowledge\nwith an extra regularization term while injecting causal knowledge. Extensive\nexperiments on 7 datasets, including four causal pair classification tasks, two\ncausal QA tasks and a causal inference task, demonstrate that CausalBERT\ncaptures rich causal knowledge and outperforms all pre-trained models-based\nstate-of-the-art methods, achieving a new causal inference benchmark.", "published": "2021-07-21 02:49:46", "link": "http://arxiv.org/abs/2107.09852v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Do Pedophiles Tweet? Investigating the Writing Styles and Online\n  Personas of Child Cybersex Traffickers in the Philippines", "abstract": "One of the most important humanitarian responsibility of every individual is\nto protect the future of our children. This entails not only protection of\nphysical welfare but also from ill events that can potentially affect the\nmental well-being of a child such as sexual coercion and abuse which, in\nworst-case scenarios, can result to lifelong trauma. In this study, we perform\na preliminary investigation of how child sex peddlers spread illegal\npornographic content and target minors for sexual activities on Twitter in the\nPhilippines using Natural Language Processing techniques. Results of our\nstudies show frequently used and co-occurring words that traffickers use to\nspread content as well as four main roles played by these entities that\ncontribute to the proliferation of child pornography in the country.", "published": "2021-07-21 05:26:52", "link": "http://arxiv.org/abs/2107.09881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comparison of Czech Transformers on Text Classification Tasks", "abstract": "In this paper, we present our progress in pre-training monolingual\nTransformers for Czech and contribute to the research community by releasing\nour models for public. The need for such models emerged from our effort to\nemploy Transformers in our language-specific tasks, but we found the\nperformance of the published multilingual models to be very limited. Since the\nmultilingual models are usually pre-trained from 100+ languages, most of\nlow-resourced languages (including Czech) are under-represented in these\nmodels. At the same time, there is a huge amount of monolingual training data\navailable in web archives like Common Crawl. We have pre-trained and publicly\nreleased two monolingual Czech Transformers and compared them with relevant\npublic models, trained (at least partially) for Czech. The paper presents the\nTransformers pre-training procedure as well as a comparison of pre-trained\nmodels on text classification task from various domains.", "published": "2021-07-21 12:22:34", "link": "http://arxiv.org/abs/2107.10042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Text Classification via Contrastive Adversarial Training", "abstract": "We propose a simple and general method to regularize the fine-tuning of\nTransformer-based encoders for text classification tasks. Specifically, during\nfine-tuning we generate adversarial examples by perturbing the word embeddings\nof the model and perform contrastive learning on clean and adversarial examples\nin order to teach the model to learn noise-invariant representations. By\ntraining on both clean and adversarial examples along with the additional\ncontrastive objective, we observe consistent improvement over standard\nfine-tuning on clean examples. On several GLUE benchmark tasks, our fine-tuned\nBERT Large model outperforms BERT Large baseline by 1.7% on average, and our\nfine-tuned RoBERTa Large improves over RoBERTa Large baseline by 1.3%. We\nadditionally validate our method in different domains using three intent\nclassification datasets, where our fine-tuned RoBERTa Large outperforms RoBERTa\nLarge baseline by 1-2% on average.", "published": "2021-07-21 15:14:15", "link": "http://arxiv.org/abs/2107.10137v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debiasing Multilingual Word Embeddings: A Case Study of Three Indian\n  Languages", "abstract": "In this paper, we advance the current state-of-the-art method for debiasing\nmonolingual word embeddings so as to generalize well in a multilingual setting.\nWe consider different methods to quantify bias and different debiasing\napproaches for monolingual as well as multilingual settings. We demonstrate the\nsignificance of our bias-mitigation approach on downstream NLP applications.\nOur proposed methods establish the state-of-the-art performance for debiasing\nmultilingual embeddings for three Indian languages - Hindi, Bengali, and Telugu\nin addition to English. We believe that our work will open up new opportunities\nin building unbiased downstream NLP applications that are inherently dependent\non the quality of the word embeddings used.", "published": "2021-07-21 16:12:51", "link": "http://arxiv.org/abs/2107.10181v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Adversarial Debiasing to Remove Bias from Word Embeddings", "abstract": "Word Embeddings have been shown to contain the societal biases present in the\noriginal corpora. Existing methods to deal with this problem have been shown to\nonly remove superficial biases. The method of Adversarial Debiasing was\npresumed to be similarly superficial, but this is was not verified in previous\nworks. Using the experiments that demonstrated the shallow removal in other\nmethods, I show results that suggest Adversarial Debiasing is more effective at\nremoving bias and thus motivate further investigation on the utility of\nAdversarial Debiasing.", "published": "2021-07-21 17:56:55", "link": "http://arxiv.org/abs/2107.10251v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Effectiveness of Intermediate-Task Training for Code-Switched\n  Natural Language Understanding", "abstract": "While recent benchmarks have spurred a lot of new work on improving the\ngeneralization of pretrained multilingual language models on multilingual\ntasks, techniques to improve code-switched natural language understanding tasks\nhave been far less explored. In this work, we propose the use of bilingual\nintermediate pretraining as a reliable technique to derive large and consistent\nperformance gains on three different NLP tasks using code-switched text. We\nachieve substantial absolute improvements of 7.87%, 20.15%, and 10.99%, on the\nmean accuracies and F1 scores over previous state-of-the-art systems for\nHindi-English Natural Language Inference (NLI), Question Answering (QA) tasks,\nand Spanish-English Sentiment Analysis (SA) respectively. We show consistent\nperformance gains on four different code-switched language-pairs\n(Hindi-English, Spanish-English, Tamil-English and Malayalam-English) for SA.\nWe also present a code-switched masked language modelling (MLM) pretraining\ntechnique that consistently benefits SA compared to standard MLM pretraining\nusing real code-switched text.", "published": "2021-07-21 08:10:59", "link": "http://arxiv.org/abs/2107.09931v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Statistical Model of Word Rank Evolution", "abstract": "The availability of large linguistic data sets enables data-driven approaches\nto study linguistic change. The Google Books corpus unigram frequency data set\nis used to investigate the word rank dynamics in eight languages. We observed\nthe rank changes of the unigrams from 1900 to 2008 and compared it to a\nWright-Fisher inspired model that we developed for our analysis. The model\nsimulates a neutral evolutionary process with the restriction of having no\ndisappearing and added words. This work explains the mathematical framework of\nthe model - written as a Markov Chain with multinomial transition probabilities\n- to show how frequencies of words change in time. From our observations in the\ndata and our model, word rank stability shows two types of characteristics: (1)\nthe increase/decrease in ranks are monotonic, or (2) the rank stays the same.\nBased on our model, high-ranked words tend to be more stable while low-ranked\nwords tend to be more volatile. Some words change in ranks in two ways: (a) by\nan accumulation of small increasing/decreasing rank changes in time and (b) by\nshocks of increase/decrease in ranks. Most words in all of the languages we\nhave looked at are rank stable, but not as stable as a neutral model would\npredict. The stopwords and Swadesh words are observed to be rank stable across\neight languages indicating linguistic conformity in established languages.\nThese signatures suggest unigram frequencies in all languages have changed in a\nmanner inconsistent with a purely neutral evolutionary process.", "published": "2021-07-21 08:57:32", "link": "http://arxiv.org/abs/2107.09948v4", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Causality Extraction From Natural Language Requirements\n  Using Recursive Neural Tensor Networks", "abstract": "[Context:] Causal relations (e.g., If A, then B) are prevalent in functional\nrequirements. For various applications of AI4RE, e.g., the automatic derivation\nof suitable test cases from requirements, automatically extracting such causal\nstatements are a basic necessity. [Problem:] We lack an approach that is able\nto extract causal relations from natural language requirements in fine-grained\nform. Specifically, existing approaches do not consider the combinatorics\nbetween causes and effects. They also do not allow to split causes and effects\ninto more granular text fragments (e.g., variable and condition), making the\nextracted relations unsuitable for automatic test case derivation. [Objective &\nContributions:] We address this research gap and make the following\ncontributions: First, we present the Causality Treebank, which is the first\ncorpus of fully labeled binary parse trees representing the composition of\n1,571 causal requirements. Second, we propose a fine-grained causality\nextractor based on Recursive Neural Tensor Networks. Our approach is capable of\nrecovering the composition of causal statements written in natural language and\nachieves a F1 score of 74 % in the evaluation on the Causality Treebank. Third,\nwe disclose our open data sets as well as our code to foster the discourse on\nthe automatic extraction of causality in the RE community.", "published": "2021-07-21 09:52:10", "link": "http://arxiv.org/abs/2107.09980v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neuradicon: operational representation learning of neuroimaging reports", "abstract": "Radiological reports typically summarize the content and interpretation of\nimaging studies in unstructured form that precludes quantitative analysis. This\nlimits the monitoring of radiological services to throughput undifferentiated\nby content, impeding specific, targeted operational optimization. Here we\npresent Neuradicon, a natural language processing (NLP) framework for\nquantitative analysis of neuroradiological reports. Our framework is a hybrid\nof rule-based and artificial intelligence models to represent neurological\nreports in succinct, quantitative form optimally suited to operational\nguidance. We demonstrate the application of Neuradicon to operational\nphenotyping of a corpus of 336,569 reports, and report excellent\ngeneralizability across time and two independent healthcare institutions.", "published": "2021-07-21 11:31:57", "link": "http://arxiv.org/abs/2107.10021v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CATE: CAusality Tree Extractor from Natural Language Requirements", "abstract": "Causal relations (If A, then B) are prevalent in requirements artifacts.\nAutomatically extracting causal relations from requirements holds great\npotential for various RE activities (e.g., automatic derivation of suitable\ntest cases). However, we lack an approach capable of extracting causal\nrelations from natural language with reasonable performance. In this paper, we\npresent our tool CATE (CAusality Tree Extractor), which is able to parse the\ncomposition of a causal relation as a tree structure. CATE does not only\nprovide an overview of causes and effects in a sentence, but also reveals their\nsemantic coherence by translating the causal relation into a binary tree. We\nencourage fellow researchers and practitioners to use CATE at\nhttps://causalitytreeextractor.com/", "published": "2021-07-21 11:37:31", "link": "http://arxiv.org/abs/2107.10023v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Small-Text: Active Learning for Text Classification in Python", "abstract": "We introduce small-text, an easy-to-use active learning library, which offers\npool-based active learning for single- and multi-label text classification in\nPython. It features numerous pre-implemented state-of-the-art query strategies,\nincluding some that leverage the GPU. Standardized interfaces allow the\ncombination of a variety of classifiers, query strategies, and stopping\ncriteria, facilitating a quick mix and match, and enabling a rapid and\nconvenient development of both active learning experiments and applications.\nWith the objective of making various classifiers and query strategies\naccessible for active learning, small-text integrates several well-known\nmachine learning libraries, namely scikit-learn, PyTorch, and Hugging Face\ntransformers. The latter integrations are optionally installable extensions, so\nGPUs can be used but are not required. Using this new library, we investigate\nthe performance of the recently published SetFit training paradigm, which we\ncompare to vanilla transformer fine-tuning, finding that it matches the latter\nin classification accuracy while outperforming it in area under the curve. The\nlibrary is available under the MIT License at\nhttps://github.com/webis-de/small-text, in version 1.3.0 at the time of\nwriting.", "published": "2021-07-21 19:23:56", "link": "http://arxiv.org/abs/2107.10314v7", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "COfEE: A Comprehensive Ontology for Event Extraction from text", "abstract": "Data is published on the web over time in great volumes, but majority of the\ndata is unstructured, making it hard to understand and difficult to interpret.\nInformation Extraction (IE) methods obtain structured information from\nunstructured data. One of the challenging IE tasks is Event Extraction (EE)\nwhich seeks to derive information about specific incidents and their actors\nfrom the text. EE is useful in many domains such as building a knowledge base,\ninformation retrieval and summarization. In the past decades, some event\nontologies like ACE, CAMEO and ICEWS were developed to define event forms,\nactors and dimensions of events observed in the text. These event ontologies\nstill have some shortcomings such as covering only a few topics like political\nevents, having inflexible structure in defining argument roles and insufficient\ngold-standard data. To address these concerns, we propose an event ontology,\nnamely COfEE, that incorporates both expert domain knowledge and a data-driven\napproach for identifying events from text. COfEE consists of two hierarchy\nlevels (event types and event sub-types) that include new categories relating\nto environmental issues, cyberspace and criminal activity which need to be\nmonitored instantly. Also, dynamic roles according to each event sub-type are\ndefined to capture various dimensions of events. In a follow-up experiment, the\nproposed ontology is evaluated on Wikipedia events, and it is shown to be\ngeneral and comprehensive. Moreover, in order to facilitate the preparation of\ngold-standard data for event extraction, a language-independent online tool is\npresented based on COfEE. A gold-standard dataset annotated by 10 human experts\nis also prepared consisting 24K news articles in Persian language. Finally, we\npresent a supervised method based on deep learning techniques to automatically\nextract relevant events and corresponding actors.", "published": "2021-07-21 19:43:22", "link": "http://arxiv.org/abs/2107.10326v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine learning for assessing quality of service in the hospitality\n  sector based on customer reviews", "abstract": "The increasing use of online hospitality platforms provides firsthand\ninformation about clients preferences, which are essential to improve hotel\nservices and increase the quality of service perception. Customer reviews can\nbe used to automatically extract the most relevant aspects of the quality of\nservice for hospitality clientele. This paper proposes a framework for the\nassessment of the quality of service in the hospitality sector based on the\nexploitation of customer reviews through natural language processing and\nmachine learning methods. The proposed framework automatically discovers the\nquality of service aspects relevant to hotel customers. Hotel reviews from\nBogot\\'a and Madrid are automatically scrapped from Booking.com. Semantic\ninformation is inferred through Latent Dirichlet Allocation and FastText, which\nallow representing text reviews as vectors. A dimensionality reduction\ntechnique is applied to visualise and interpret large amounts of customer\nreviews. Visualisations of the most important quality of service aspects are\ngenerated, allowing to qualitatively and quantitatively assess the quality of\nservice. Results show that it is possible to automatically extract the main\nquality of service aspects perceived by customers from large customer review\ndatasets. These findings could be used by hospitality managers to understand\nclients better and to improve the quality of service.", "published": "2021-07-21 19:45:40", "link": "http://arxiv.org/abs/2107.10328v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Multi-Stream Transformers", "abstract": "Transformer-based encoder-decoder models produce a fused token-wise\nrepresentation after every encoder layer. We investigate the effects of\nallowing the encoder to preserve and explore alternative hypotheses, combined\nat the end of the encoding process. To that end, we design and examine a\n$\\textit{Multi-stream Transformer}$ architecture and find that splitting the\nTransformer encoder into multiple encoder streams and allowing the model to\nmerge multiple representational hypotheses improves performance, with further\nimprovement obtained by adding a skip connection between the first and the\nfinal encoder layer.", "published": "2021-07-21 20:16:57", "link": "http://arxiv.org/abs/2107.10342v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "JEFL: Joint Embedding of Formal Proof Libraries", "abstract": "The heterogeneous nature of the logical foundations used in different\ninteractive proof assistant libraries has rendered discovery of similar\nmathematical concepts among them difficult. In this paper, we compare a\npreviously proposed algorithm for matching concepts across libraries with our\nunsupervised embedding approach that can help us retrieve similar concepts. Our\napproach is based on the fasttext implementation of Word2Vec, on top of which a\ntree traversal module is added to adapt its algorithm to the representation\nformat of our data export pipeline. We compare the explainability,\ncustomizability, and online-servability of the approaches and argue that the\nneural embedding approach has more potential to be integrated into an\ninteractive proof assistant.", "published": "2021-07-21 16:31:33", "link": "http://arxiv.org/abs/2107.10188v1", "categories": ["cs.LO", "cs.CL", "cs.LG"], "primary_category": "cs.LO"}
{"title": "Characterizing Social Imaginaries and Self-Disclosures of Dissonance in\n  Online Conspiracy Discussion Communities", "abstract": "Online discussion platforms offer a forum to strengthen and propagate belief\nin misinformed conspiracy theories. Yet, they also offer avenues for conspiracy\ntheorists to express their doubts and experiences of cognitive dissonance. Such\nexpressions of dissonance may shed light on who abandons misguided beliefs and\nunder which circumstances. This paper characterizes self-disclosures of\ndissonance about QAnon, a conspiracy theory initiated by a mysterious leader Q\nand popularized by their followers, anons in conspiracy theory subreddits. To\nunderstand what dissonance and disbelief mean within conspiracy communities, we\nfirst characterize their social imaginaries, a broad understanding of how\npeople collectively imagine their social existence. Focusing on 2K posts from\ntwo image boards, 4chan and 8chan, and 1.2 M comments and posts from 12\nsubreddits dedicated to QAnon, we adopt a mixed methods approach to uncover the\nsymbolic language representing the movement, expectations, practices, heroes\nand foes of the QAnon community. We use these social imaginaries to create a\ncomputational framework for distinguishing belief and dissonance from general\ndiscussion about QAnon. Further, analyzing user engagement with QAnon\nconspiracy subreddits, we find that self-disclosures of dissonance correlate\nwith a significant decrease in user contributions and ultimately with their\ndeparture from the community. We contribute a computational framework for\nidentifying dissonance self-disclosures and measuring the changes in user\nengagement surrounding dissonance. Our work can provide insights into designing\ndissonance-based interventions that can potentially dissuade conspiracists from\nonline conspiracy discussion communities.", "published": "2021-07-21 16:49:21", "link": "http://arxiv.org/abs/2107.10204v1", "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.SI"}
{"title": "Digital Einstein Experience: Fast Text-to-Speech for Conversational AI", "abstract": "We describe our approach to create and deliver a custom voice for a\nconversational AI use-case. More specifically, we provide a voice for a Digital\nEinstein character, to enable human-computer interaction within the digital\nconversation experience. To create the voice which fits the context well, we\nfirst design a voice character and we produce the recordings which correspond\nto the desired speech attributes. We then model the voice. Our solution\nutilizes Fastspeech 2 for log-scaled mel-spectrogram prediction from phonemes\nand Parallel WaveGAN to generate the waveforms. The system supports a character\ninput and gives a speech waveform at the output. We use a custom dictionary for\nselected words to ensure their proper pronunciation. Our proposed cloud\narchitecture enables for fast voice delivery, making it possible to talk to the\ndigital version of Albert Einstein in real-time.", "published": "2021-07-21 12:03:27", "link": "http://arxiv.org/abs/2107.10658v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Controlling the Remixing of Separated Dialogue with a Non-Intrusive\n  Quality Estimate", "abstract": "Remixing separated audio sources trades off interferer attenuation against\nthe amount of audible deteriorations. This paper proposes a non-intrusive audio\nquality estimation method for controlling this trade-off in a signal-adaptive\nmanner. The recently proposed 2f-model is adopted as the underlying quality\nmeasure, since it has been shown to correlate strongly with basic audio quality\nin source separation. An alternative operation mode of the measure is proposed,\nmore appropriate when considering material with long inactive periods of the\ntarget source. The 2f-model requires the reference target source as an input,\nbut this is not available in many applications. Deep neural networks (DNNs) are\ntrained to estimate the 2f-model intrusively using the reference target\n(iDNN2f), non-intrusively using the input mix as reference (nDNN2f), and\nreference-free using only the separated output signal (rDNN2f). It is shown\nthat iDNN2f achieves very strong correlation with the original measure on the\ntest data (Pearson r=0.99), while performance decreases for nDNN2f (r>=0.91)\nand rDNN2f (r>=0.82). The non-intrusive estimate nDNN2f is mapped to select\nitem-dependent remixing gains with the aim of maximizing the interferer\nattenuation under a constraint on the minimum quality of the remixed output\n(e.g., audible but not annoying deteriorations). A listening test shows that\nthis is successfully achieved even with very different selected gains (up to 23\ndB difference).", "published": "2021-07-21 15:26:21", "link": "http://arxiv.org/abs/2107.10151v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "JS Fake Chorales: a Synthetic Dataset of Polyphonic Music with Human\n  Annotation", "abstract": "High-quality datasets for learning-based modelling of polyphonic symbolic\nmusic remain less readily-accessible at scale than in other domains, such as\nlanguage modelling or image classification. Deep learning algorithms show great\npotential for enabling the widespread use of interactive music generation\ntechnology in consumer applications, but the lack of large-scale datasets\nremains a bottleneck for the development of algorithms that can consistently\ngenerate high-quality outputs. We propose that models with narrow expertise can\nserve as a source of high-quality scalable synthetic data, and open-source the\nJS Fake Chorales, a dataset of 500 pieces generated by a new learning-based\nalgorithm, provided in MIDI form.\n  We take consecutive outputs from the algorithm and avoid cherry-picking in\norder to validate the potential to further scale this dataset on-demand. We\nconduct an online experiment for human evaluation, designed to be as fair to\nthe listener as possible, and find that respondents were on average only 7%\nbetter than random guessing at distinguishing JS Fake Chorales from real\nchorales composed by JS Bach. Furthermore, we make anonymised data collected\nfrom experiments available along with the MIDI samples. Finally, we conduct\nablation studies to demonstrate the effectiveness of using the synthetic pieces\nfor research in polyphonic music modelling, and find that we can improve on\nstate-of-the-art validation set loss for the canonical JSB Chorales dataset,\nusing a known algorithm, by simply augmenting the training set with the JS Fake\nChorales.", "published": "2021-07-21 23:07:22", "link": "http://arxiv.org/abs/2107.10388v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CNN Classifier for Just-in-Time Woodpeckers Detection and Deterrent", "abstract": "Woodpeckers can cause significant damage to homes, especially in suburban\nareas. There are a number of preventing and repelling methods including passive\ndecoys, though these may only provide temporary relief. Subsequently, it may be\nmore efficient to implement a woodpecker deterrent, such as motion, light,\nsound, or ultrasound that would be triggered by detection of woodpecker\nsignature drumming. To detect the typical 25 Hz drumming frequency, sampling\nperiods under 10 milliseconds with frequent FFTs are required with considerable\ncomputational costs. An in-hardware spectrum analyzer may avoid these costs by\ntrading off frequency for time resolutions. The trained model converted to TF\nLite Micro, ported to an MCU, and identifies a variety of the prerecorded\nwoodpecker drumming. The plan is to integrate the prototype with a deterrent\ndevice making it a completely autonomous solution.", "published": "2021-07-21 14:32:37", "link": "http://arxiv.org/abs/2107.10676v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Captioning Transformer", "abstract": "Audio captioning aims to automatically generate a natural language\ndescription of an audio clip. Most captioning models follow an encoder-decoder\narchitecture, where the decoder predicts words based on the audio features\nextracted by the encoder. Convolutional neural networks (CNNs) and recurrent\nneural networks (RNNs) are often used as the audio encoder. However, CNNs can\nbe limited in modelling temporal relationships among the time frames in an\naudio signal, while RNNs can be limited in modelling the long-range\ndependencies among the time frames. In this paper, we propose an Audio\nCaptioning Transformer (ACT), which is a full Transformer network based on an\nencoder-decoder architecture and is totally convolution-free. The proposed\nmethod has a better ability to model the global information within an audio\nsignal as well as capture temporal relationships between audio events. We\nevaluate our model on AudioCaps, which is the largest audio captioning dataset\npublicly available. Our model shows competitive performance compared to other\nstate-of-the-art approaches.", "published": "2021-07-21 00:31:50", "link": "http://arxiv.org/abs/2107.09817v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Melody Structure Transfer Network: Generating Music with Separable\n  Self-Attention", "abstract": "Symbolic music generation has attracted increasing attention, while most\nmethods focus on generating short piece (mostly less than 8 bars, and up to 32\nbars). Generating long music calls for effective expression of the coherent\nmusic structure. Despite their success on long sequences, self-attention\narchitectures still have challenge in dealing with long-term music as it\nrequires additional care on the subtle music structure. In this paper, we\npropose to transfer the structure of training samples for new music generation,\nand develop a novel separable self-attention based model which enable the\nlearning and transferring of the structure embedding. We show that our transfer\nmodel can generate music sequences (up to 100 bars) with interpretable\nstructures, which bears similar structures and composition techniques with the\ntemplate music from training set. Extensive experiments show its ability of\ngenerating music with target structure and well diversity. The generated 3,000\nsets of music is uploaded as supplemental material.", "published": "2021-07-21 04:38:15", "link": "http://arxiv.org/abs/2107.09877v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fine-Grained Music Plagiarism Detection: Revealing Plagiarists through\n  Bipartite Graph Matching and a Comprehensive Large-Scale Dataset", "abstract": "Music plagiarism detection is gaining more and more attention due to the\npopularity of music production and society's emphasis on intellectual property.\nWe aim to find fine-grained plagiarism in music pairs since conventional\nmethods are coarse-grained and cannot match real-life scenarios. Considering\nthat there is no sizeable dataset designed for the music plagiarism task, we\nestablish a large-scale simulated dataset, named Music Plagiarism Detection\nDataset (MPD-Set) under the guidance and expertise of renowned researchers from\nnational-level professional institutions in the field of music. MPD-Set\nconsiders diverse music plagiarism cases found in real life from the melodic,\nrhythmic, and tonal levels respectively. Further, we establish a Real-life\nDataset for evaluation, where all plagiarism pairs are real cases. To detect\nthe fine-grained plagiarism pairs effectively, we propose a graph-based method\ncalled Bipatite Melody Matching Detector (BMM-Det), which formulates the\nproblem as a max matching problem in the bipartite graph. Experimental results\non both the simulated and Real-life Datasets demonstrate that BMM-Det\noutperforms the existing plagiarism detection methods, and is robust to common\nplagiarism cases like transpositions, pitch shifts, duration variance, and\nmelody change. Datasets and source code are open-sourced at\nhttps://github.com/xuan301/BMMDet_MPDSet.", "published": "2021-07-21 06:04:47", "link": "http://arxiv.org/abs/2107.09889v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CL4AC: A Contrastive Loss for Audio Captioning", "abstract": "Automated Audio captioning (AAC) is a cross-modal translation task that aims\nto use natural language to describe the content of an audio clip. As shown in\nthe submissions received for Task 6 of the DCASE 2021 Challenges, this problem\nhas received increasing interest in the community. The existing AAC systems are\nusually based on an encoder-decoder architecture, where the audio signal is\nencoded into a latent representation, and aligned with its corresponding text\ndescriptions, then a decoder is used to generate the captions. However,\ntraining of an AAC system often encounters the problem of data scarcity, which\nmay lead to inaccurate representation and audio-text alignment. To address this\nproblem, we propose a novel encoder-decoder framework called Contrastive Loss\nfor Audio Captioning (CL4AC). In CL4AC, the self-supervision signals derived\nfrom the original audio-text paired data are used to exploit the\ncorrespondences between audio and texts by contrasting samples, which can\nimprove the quality of latent representation and the alignment between audio\nand texts, while trained with limited data. Experiments are performed on the\nClotho dataset to show the effectiveness of our proposed approach.", "published": "2021-07-21 10:13:02", "link": "http://arxiv.org/abs/2107.09990v3", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conditional Sound Generation Using Neural Discrete Time-Frequency\n  Representation Learning", "abstract": "Deep generative models have recently achieved impressive performance in\nspeech and music synthesis. However, compared to the generation of those\ndomain-specific sounds, generating general sounds (such as siren, gunshots) has\nreceived less attention, despite their wide applications. In previous work, the\nSampleRNN method was considered for sound generation in the time domain.\nHowever, SampleRNN is potentially limited in capturing long-range dependencies\nwithin sounds as it only back-propagates through a limited number of samples.\nIn this work, we propose a method for generating sounds via neural discrete\ntime-frequency representation learning, conditioned on sound classes. This\noffers an advantage in efficiently modelling long-range dependencies and\nretaining local fine-grained structures within sound clips. We evaluate our\napproach on the UrbanSound8K dataset, compared to SampleRNN, with the\nperformance metrics measuring the quality and diversity of generated sounds.\nExperimental results show that our method offers comparable performance in\nquality and significantly better performance in diversity.", "published": "2021-07-21 10:31:28", "link": "http://arxiv.org/abs/2107.09998v3", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Tandem Framework Balancing Privacy and Security for Voice User\n  Interfaces", "abstract": "Speech synthesis, voice cloning, and voice conversion techniques present\nsevere privacy and security threats to users of voice user interfaces (VUIs).\nThese techniques transform one or more elements of a speech signal, e.g.,\nidentity and emotion, while preserving linguistic information. Adversaries may\nuse advanced transformation tools to trigger a spoofing attack using fraudulent\nbiometrics for a legitimate speaker. Conversely, such techniques have been used\nto generate privacy-transformed speech by suppressing personally identifiable\nattributes in the voice signals, achieving anonymization. Prior works have\nstudied the security and privacy vectors in parallel, and thus it raises alarm\nthat if a benign user can achieve privacy by a transformation, it also means\nthat a malicious user can break security by bypassing the anti-spoofing\nmechanism. In this paper, we take a step towards balancing two seemingly\nconflicting requirements: security and privacy. It remains unclear what the\nvulnerabilities in one domain imply for the other, and what dynamic\ninteractions exist between them. A better understanding of these aspects is\ncrucial for assessing and mitigating vulnerabilities inherent with VUIs and\nbuilding effective defenses. In this paper,(i) we investigate the applicability\nof the current voice anonymization methods by deploying a tandem framework that\njointly combines anti-spoofing and authentication models, and evaluate the\nperformance of these methods;(ii) examining analytical and empirical evidence,\nwe reveal a duality between the two mechanisms as they offer different ways to\nachieve the same objective, and we show that leveraging one vector\nsignificantly amplifies the effectiveness of the other;(iii) we demonstrate\nthat to effectively defend from potential attacks against VUIs, it is necessary\nto investigate the attacks from multiple complementary perspectives(security\nand privacy).", "published": "2021-07-21 12:30:14", "link": "http://arxiv.org/abs/2107.10045v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "StarGANv2-VC: A Diverse, Unsupervised, Non-parallel Framework for\n  Natural-Sounding Voice Conversion", "abstract": "We present an unsupervised non-parallel many-to-many voice conversion (VC)\nmethod using a generative adversarial network (GAN) called StarGAN v2. Using a\ncombination of adversarial source classifier loss and perceptual loss, our\nmodel significantly outperforms previous VC models. Although our model is\ntrained only with 20 English speakers, it generalizes to a variety of voice\nconversion tasks, such as any-to-many, cross-lingual, and singing conversion.\nUsing a style encoder, our framework can also convert plain reading speech into\nstylistic speech, such as emotional and falsetto speech. Subjective and\nobjective evaluation experiments on a non-parallel many-to-many voice\nconversion task revealed that our model produces natural sounding voices, close\nto the sound quality of state-of-the-art text-to-speech (TTS) based voice\nconversion methods without the need for text labels. Moreover, our model is\ncompletely convolutional and with a faster-than-real-time vocoder such as\nParallel WaveGAN can perform real-time voice conversion.", "published": "2021-07-21 23:44:17", "link": "http://arxiv.org/abs/2107.10394v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-modal Residual Perceptron Network for Audio-Video Emotion\n  Recognition", "abstract": "Audio-Video Emotion Recognition is now attacked with Deep Neural Network\nmodeling tools. In published papers, as a rule, the authors show only cases of\nthe superiority in multi-modality over audio-only or video-only modality.\nHowever, there are cases superiority in uni-modality can be found. In our\nresearch, we hypothesize that for fuzzy categories of emotional events, the\nwithin-modal and inter-modal noisy information represented indirectly in the\nparameters of the modeling neural network impedes better performance in the\nexisting late fusion and end-to-end multi-modal network training strategies. To\ntake advantage and overcome the deficiencies in both solutions, we define a\nMulti-modal Residual Perceptron Network which performs end-to-end learning from\nmulti-modal network branches, generalizing better multi-modal feature\nrepresentation. For the proposed Multi-modal Residual Perceptron Network and\nthe novel time augmentation for streaming digital movies, the state-of-art\naverage recognition rate was improved to 91.4% for The Ryerson Audio-Visual\nDatabase of Emotional Speech and Song dataset and to 83.15% for Crowd-sourced\nEmotional multi-modal Actors dataset. Moreover, the Multi-modal Residual\nPerceptron Network concept shows its potential for multi-modal applications\ndealing with signal sources not only of optical and acoustical types.", "published": "2021-07-21 13:11:37", "link": "http://arxiv.org/abs/2107.10742v2", "categories": ["eess.SP", "cs.AI", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "eess.SP"}
