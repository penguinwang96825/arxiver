{"title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated\n  Training Corpus", "abstract": "Many efforts of research are devoted to semantic role labeling (SRL) which is\ncrucial for natural language understanding. Supervised approaches have achieved\nimpressing performances when large-scale corpora are available for\nresource-rich languages such as English. While for the low-resource languages\nwith no annotated SRL dataset, it is still challenging to obtain competitive\nperformances. Cross-lingual SRL is one promising way to address the problem,\nwhich has achieved great advances with the help of model transferring and\nannotation projection. In this paper, we propose a novel alternative based on\ncorpus translation, constructing high-quality training datasets for the target\nlanguages from the source gold-standard SRL annotations. Experimental results\non Universal Proposition Bank show that the translation-based method is highly\neffective, and the automatic pseudo datasets can improve the target-language\nSRL performances significantly.", "published": "2020-04-14 04:16:43", "link": "http://arxiv.org/abs/2004.06295v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speech Translation and the End-to-End Promise: Taking Stock of Where We\n  Are", "abstract": "Over its three decade history, speech translation has experienced several\nshifts in its primary research themes; moving from loosely coupled cascades of\nspeech recognition and machine translation, to exploring questions of tight\ncoupling, and finally to end-to-end models that have recently attracted much\nattention. This paper provides a brief survey of these developments, along with\na discussion of the main challenges of traditional approaches which stem from\ncommitting to intermediate representations from the speech recognizer, and from\ntraining cascaded models separately towards different objectives.\n  Recent end-to-end modeling techniques promise a principled way of overcoming\nthese issues by allowing joint training of all model components and removing\nthe need for explicit intermediate representations. However, a closer look\nreveals that many end-to-end models fall short of solving these issues, due to\ncompromises made to address data scarcity. This paper provides a unifying\ncategorization and nomenclature that covers both traditional and recent\napproaches and that may help researchers by highlighting both trade-offs and\nopen research questions.", "published": "2020-04-14 08:43:51", "link": "http://arxiv.org/abs/2004.06358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-Variant Advertisement Text Generation with Association Knowledge", "abstract": "Online advertising is an important revenue source for many IT companies. In\nthe search advertising scenario, advertisement text that meets the need of the\nsearch query would be more attractive to the user. However, the manual creation\nof query-variant advertisement texts for massive items is expensive.\nTraditional text generation methods tend to focus on the general searching\nneeds with high frequency while ignoring the diverse personalized searching\nneeds with low frequency. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisement texts for different web search queries with various needs based\non queries and item keywords. To solve the problem of ignoring low-frequency\nneeds, we propose a dynamic association mechanism to expand the receptive field\nbased on external knowledge, which can obtain associated words to be added to\nthe input. These associated words can serve as bridges to transfer the ability\nof the model from the familiar high-frequency words to the unfamiliar\nlow-frequency words. With association, the model can make use of various\npersonalized needs in queries and generate query-variant advertisement texts.\nBoth automatic and human evaluations show that our model can generate more\nattractive advertisement text than baselines.", "published": "2020-04-14 12:04:28", "link": "http://arxiv.org/abs/2004.06438v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What's so special about BERT's layers? A closer look at the NLP pipeline\n  in monolingual and multilingual models", "abstract": "Peeking into the inner workings of BERT has shown that its layers resemble\nthe classical NLP pipeline, with progressively more complex tasks being\nconcentrated in later layers. To investigate to what extent these results also\nhold for a language other than English, we probe a Dutch BERT-based model and\nthe multilingual BERT model for Dutch NLP tasks. In addition, through a deeper\nanalysis of part-of-speech tagging, we show that also within a given task,\ninformation is spread over different parts of the network and the pipeline\nmight not be as neat as it seems. Each layer has different specialisations, so\nthat it may be more useful to combine information from different layers,\ninstead of selecting a single one based on the best overall performance.", "published": "2020-04-14 13:41:48", "link": "http://arxiv.org/abs/2004.06499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Ontology Refined Embeddings (MORE): A Hybrid Multi-Ontology and\n  Corpus-based Semantic Representation for Biomedical Concepts", "abstract": "Objective: Currently, a major limitation for natural language processing\n(NLP) analyses in clinical applications is that a concept can be referenced in\nvarious forms across different texts. This paper introduces Multi-Ontology\nRefined Embeddings (MORE), a novel hybrid framework for incorporating domain\nknowledge from multiple ontologies into a distributional semantic model,\nlearned from a corpus of clinical text.\n  Materials and Methods: We use the RadCore and MIMIC-III free-text datasets\nfor the corpus-based component of MORE. For the ontology-based part, we use the\nMedical Subject Headings (MeSH) ontology and three state-of-the-art\nontology-based similarity measures. In our approach, we propose a new learning\nobjective, modified from the Sigmoid cross-entropy objective function.\n  Results and Discussion: We evaluate the quality of the generated word\nembeddings using two established datasets of semantic similarities among\nbiomedical concept pairs. On the first dataset with 29 concept pairs, with the\nsimilarity scores established by physicians and medical coders, MORE's\nsimilarity scores have the highest combined correlation (0.633), which is 5.0%\nhigher than that of the baseline model and 12.4% higher than that of the best\nontology-based similarity measure.On the second dataset with 449 concept pairs,\nMORE's similarity scores have a correlation of 0.481, with the average of four\nmedical residents' similarity ratings, and that outperforms the skip-gram model\nby 8.1% and the best ontology measure by 6.9%.", "published": "2020-04-14 14:38:41", "link": "http://arxiv.org/abs/2004.06555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Machine Translation: Closing the Gap between Shared and\n  Language-specific Encoder-Decoders", "abstract": "State-of-the-art multilingual machine translation relies on a universal\nencoder-decoder, which requires retraining the entire system to add new\nlanguages. In this paper, we propose an alternative approach that is based on\nlanguage-specific encoder-decoders, and can thus be more easily extended to new\nlanguages by learning their corresponding modules. So as to encourage a common\ninterlingua representation, we simultaneously train the N initial languages.\nOur experiments show that the proposed approach outperforms the universal\nencoder-decoder by 3.28 BLEU points on average, and when adding new languages,\nwithout the need to retrain the rest of the modules. All in all, our work\ncloses the gap between shared and language-specific encoder-decoders, advancing\ntoward modular multilingual machine translation systems that can be flexibly\nextended in lifelong learning settings.", "published": "2020-04-14 15:02:24", "link": "http://arxiv.org/abs/2004.06575v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Balancing Training for Multilingual Neural Machine Translation", "abstract": "When training multilingual machine translation (MT) models that can translate\nto/from multiple languages, we are faced with imbalanced training sets: some\nlanguages have much more training data than others. Standard practice is to\nup-sample less resourced languages to increase representation, and the degree\nof up-sampling has a large effect on the overall performance. In this paper, we\npropose a method that instead automatically learns how to weight training data\nthrough a data scorer that is optimized to maximize performance on all test\nlanguages. Experiments on two sets of languages under both one-to-many and\nmany-to-one MT settings show our method not only consistently outperforms\nheuristic baselines in terms of average performance, but also offers flexible\ncontrol over the performance of which languages are optimized.", "published": "2020-04-14 18:23:28", "link": "http://arxiv.org/abs/2004.06748v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Simple Yet Strong Pipeline for HotpotQA", "abstract": "State-of-the-art models for multi-hop question answering typically augment\nlarge-scale language models like BERT with additional, intuitively useful\ncapabilities such as named entity recognition, graph-based reasoning, and\nquestion decomposition. However, does their strong performance on popular\nmulti-hop datasets really justify this added design complexity? Our results\nsuggest that the answer may be no, because even our simple pipeline based on\nBERT, named Quark, performs surprisingly well. Specifically, on HotpotQA, Quark\noutperforms these models on both question answering and support identification\n(and achieves performance very close to a RoBERTa model). Our pipeline has\nthree steps: 1) use BERT to identify potentially relevant sentences\nindependently of each other; 2) feed the set of selected sentences as context\ninto a standard BERT span prediction model to choose an answer; and 3) use the\nsentence selection model, now with the chosen answer, to produce supporting\nsentences. The strong performance of Quark resurfaces the importance of\ncarefully exploring simple model designs before using popular benchmarks to\njustify the value of complex techniques.", "published": "2020-04-14 18:48:57", "link": "http://arxiv.org/abs/2004.06753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Human Evaluation of AMR-to-English Generation Systems", "abstract": "Most current state-of-the art systems for generating English text from\nAbstract Meaning Representation (AMR) have been evaluated only using automated\nmetrics, such as BLEU, which are known to be problematic for natural language\ngeneration. In this work, we present the results of a new human evaluation\nwhich collects fluency and adequacy scores, as well as categorization of error\ntypes, for several recent AMR generation systems. We discuss the relative\nquality of these systems and how our results compare to those of automatic\nmetrics, finding that while the metrics are mostly successful in ranking\nsystems overall, collecting human judgments allows for more nuanced\ncomparisons. We also analyze common errors made by these systems.", "published": "2020-04-14 21:41:30", "link": "http://arxiv.org/abs/2004.06814v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PALM: Pre-training an Autoencoding&Autoregressive Language Model for\n  Context-conditioned Generation", "abstract": "Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a\npowerful technique for natural language understanding and generation. Existing\npre-training techniques employ autoencoding and/or autoregressive objectives to\ntrain Transformer-based models by recovering original word tokens from\ncorrupted text with some masked tokens. The training goals of existing\ntechniques are often inconsistent with the goals of many language generation\ntasks, such as generative question answering and conversational response\ngeneration, for producing new text given context.\n  This work presents PALM with a novel scheme that jointly pre-trains an\nautoencoding and autoregressive language model on a large unlabeled corpus,\nspecifically designed for generating new text conditioned on context. The new\nscheme alleviates the mismatch introduced by the existing denoising scheme\nbetween pre-training and fine-tuning where generation is more than\nreconstructing original text. An extensive set of experiments show that PALM\nachieves new state-of-the-art results on a variety of language generation\nbenchmarks covering generative question answering (Rank 1 on the official MARCO\nleaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword,\nquestion generation on SQuAD, and conversational response generation on Cornell\nMovie Dialogues.", "published": "2020-04-14 06:25:36", "link": "http://arxiv.org/abs/2004.07159v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Uncertain Segmentation Information into Chinese NER for\n  Social Media Text", "abstract": "Chinese word segmentation is necessary to provide word-level information for\nChinese named entity recognition (NER) systems. However, segmentation error\npropagation is a challenge for Chinese NER while processing colloquial data\nlike social media text. In this paper, we propose a model (UIcwsNN) that\nspecializes in identifying entities from Chinese social media text, especially\nby leveraging ambiguous information of word segmentation. Such uncertain\ninformation contains all the potential segmentation states of a sentence that\nprovides a channel for the model to infer deep word-level characteristics. We\npropose a trilogy (i.e., candidate position embedding -> position selective\nattention -> adaptive word convolution) to encode uncertain word segmentation\ninformation and acquire appropriate word-level representation. Experiments\nresults on the social media corpus show that our model alleviates the\nsegmentation error cascading trouble effectively, and achieves a significant\nperformance improvement of more than 2% over previous state-of-the-art methods.", "published": "2020-04-14 09:39:35", "link": "http://arxiv.org/abs/2004.06384v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jointly Modeling Aspect and Sentiment with Dynamic Heterogeneous Graph\n  Neural Networks", "abstract": "Target-Based Sentiment Analysis aims to detect the opinion aspects (aspect\nextraction) and the sentiment polarities (sentiment detection) towards them.\nBoth the previous pipeline and integrated methods fail to precisely model the\ninnate connection between these two objectives. In this paper, we propose a\nnovel dynamic heterogeneous graph to jointly model the two objectives in an\nexplicit way. Both the ordinary words and sentiment labels are treated as nodes\nin the heterogeneous graph, so that the aspect words can interact with the\nsentiment information. The graph is initialized with multiple types of\ndependencies, and dynamically modified during real-time prediction. Experiments\non the benchmark datasets show that our model outperforms the state-of-the-art\nmodels. Further analysis demonstrates that our model obtains significant\nperformance gain on the challenging instances under multiple-opinion aspects\nand no-opinion aspect situations.", "published": "2020-04-14 11:27:30", "link": "http://arxiv.org/abs/2004.06427v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning Models for Multilingual Hate Speech Detection", "abstract": "Hate speech detection is a challenging problem with most of the datasets\navailable in only one language: English. In this paper, we conduct a large\nscale analysis of multilingual hate speech in 9 languages from 16 different\nsources. We observe that in low resource setting, simple models such as LASER\nembedding with logistic regression performs the best, while in high resource\nsetting BERT based models perform better. In case of zero-shot classification,\nlanguages such as Italian and Portuguese achieve good results. Our proposed\nframework could be used as an efficient solution for low-resource languages.\nThese models could also act as good baselines for future multilingual hate\nspeech detection tasks. We have made our code and experimental settings public\nfor other researchers at https://github.com/punyajoy/DE-LIMIT.", "published": "2020-04-14 13:14:27", "link": "http://arxiv.org/abs/2004.06465v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Multi-source Attention for Unsupervised Domain Adaptation", "abstract": "Domain adaptation considers the problem of generalising a model learnt using\ndata from a particular source domain to a different target domain. Often it is\ndifficult to find a suitable single source to adapt from, and one must consider\nmultiple sources. Using an unrelated source can result in sub-optimal\nperformance, known as the \\emph{negative transfer}. However, it is challenging\nto select the appropriate source(s) for classifying a given target instance in\nmulti-source unsupervised domain adaptation (UDA). We model source-selection as\nan attention-learning problem, where we learn attention over sources for a\ngiven target instance. For this purpose, we first independently learn\nsource-specific classification models, and a relatedness map between sources\nand target domains using pseudo-labelled target domain instances. Next, we\nlearn attention-weights over the sources for aggregating the predictions of the\nsource-specific models. Experimental results on cross-domain sentiment\nclassification benchmarks show that the proposed method outperforms prior\nproposals in multi-source UDA.", "published": "2020-04-14 15:51:02", "link": "http://arxiv.org/abs/2004.06608v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extending Text Informativeness Measures to Passage Interestingness\n  Evaluation (Language Model vs. Word Embedding)", "abstract": "Standard informativeness measures used to evaluate Automatic Text\nSummarization mostly rely on n-gram overlapping between the automatic summary\nand the reference summaries. These measures differ from the metric they use\n(cosine, ROUGE, Kullback-Leibler, Logarithm Similarity, etc.) and the bag of\nterms they consider (single words, word n-grams, entities, nuggets, etc.).\nRecent word embedding approaches offer a continuous alternative to discrete\napproaches based on the presence/absence of a text unit. Informativeness\nmeasures have been extended to Focus Information Retrieval evaluation involving\na user's information need represented by short queries. In particular for the\ntask of CLEF-INEX Tweet Contextualization, tweet contents have been considered\nas queries. In this paper we define the concept of Interestingness as a\ngeneralization of Informativeness, whereby the information need is diverse and\nformalized as an unknown set of implicit queries. We then study the ability of\nstate of the art Informativeness measures to cope with this generalization.\nLately we show that with this new framework, standard word embeddings\noutperforms discrete measures only on uni-grams, however bi-grams seems to be a\nkey point of interestingness evaluation. Lastly we prove that the CLEF-INEX\nTweet Contextualization 2012 Logarithm Similarity measure provides best\nresults.", "published": "2020-04-14 18:22:48", "link": "http://arxiv.org/abs/2004.06747v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Quantifying Community Characteristics of Maternal Mortality Using Social\n  Media", "abstract": "While most mortality rates have decreased in the US, maternal mortality has\nincreased and is among the highest of any OECD nation. Extensive public health\nresearch is ongoing to better understand the characteristics of communities\nwith relatively high or low rates. In this work, we explore the role that\nsocial media language can play in providing insights into such community\ncharacteristics. Analyzing pregnancy-related tweets generated in US counties,\nwe reveal a diverse set of latent topics including Morning Sickness, Celebrity\nPregnancies, and Abortion Rights. We find that rates of mentioning these topics\non Twitter predicts maternal mortality rates with higher accuracy than standard\nsocioeconomic and risk variables such as income, race, and access to\nhealth-care, holding even after reducing the analysis to six topics chosen for\ntheir interpretability and connections to known risk factors. We then\ninvestigate psychological dimensions of community language, finding the use of\nless trustful, more stressed, and more negative affective language is\nsignificantly associated with higher mortality rates, while trust and negative\naffect also explain a significant portion of racial disparities in maternal\nmortality. We discuss the potential for these insights to inform actionable\nhealth interventions at the community-level.", "published": "2020-04-14 04:57:51", "link": "http://arxiv.org/abs/2004.06303v1", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Transformer based Grapheme-to-Phoneme Conversion", "abstract": "Attention mechanism is one of the most successful techniques in deep learning\nbased Natural Language Processing (NLP). The transformer network architecture\nis completely based on attention mechanisms, and it outperforms\nsequence-to-sequence models in neural machine translation without recurrent and\nconvolutional layers. Grapheme-to-phoneme (G2P) conversion is a task of\nconverting letters (grapheme sequence) to their pronunciations (phoneme\nsequence). It plays a significant role in text-to-speech (TTS) and automatic\nspeech recognition (ASR) systems. In this paper, we investigate the application\nof transformer architecture to G2P conversion and compare its performance with\nrecurrent and convolutional neural network based approaches. Phoneme and word\nerror rates are evaluated on the CMUDict dataset for US English and the NetTalk\ndataset. The results show that transformer based G2P outperforms the\nconvolutional-based approach in terms of word error rate and our results\nsignificantly exceeded previous recurrent approaches (without attention)\nregarding word and phoneme error rates on both datasets. Furthermore, the size\nof the proposed model is much smaller than the size of the previous approaches.", "published": "2020-04-14 07:48:15", "link": "http://arxiv.org/abs/2004.06338v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Code Completion using Neural Attention and Byte Pair Encoding", "abstract": "In this paper, we aim to do code completion based on implementing a Neural\nNetwork from Li et. al.. Our contribution is that we use an encoding that is\nin-between character and word encoding called Byte Pair Encoding (BPE). We use\nthis on the source code files treating them as natural text without first going\nthrough the abstract syntax tree (AST). We have implemented two models: an\nattention-enhanced LSTM and a pointer network, where the pointer network was\noriginally introduced to solve out of vocabulary problems. We are interested to\nsee if BPE can replace the need for the pointer network for code completion.", "published": "2020-04-14 08:00:40", "link": "http://arxiv.org/abs/2004.06343v1", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Weight Poisoning Attacks on Pre-trained Models", "abstract": "Recently, NLP has seen a surge in the usage of large pre-trained models.\nUsers download weights of models pre-trained on large datasets, then fine-tune\nthe weights on a task of their choice. This raises the question of whether\ndownloading untrusted pre-trained weights can pose a security threat. In this\npaper, we show that it is possible to construct ``weight poisoning'' attacks\nwhere pre-trained weights are injected with vulnerabilities that expose\n``backdoors'' after fine-tuning, enabling the attacker to manipulate the model\nprediction simply by injecting an arbitrary keyword. We show that by applying a\nregularization method, which we call RIPPLe, and an initialization procedure,\nwhich we call Embedding Surgery, such attacks are possible even with limited\nknowledge of the dataset and fine-tuning procedure. Our experiments on\nsentiment classification, toxicity detection, and spam detection show that this\nattack is widely applicable and poses a serious threat. Finally, we outline\npractical defenses against such attacks. Code to reproduce our experiments is\navailable at https://github.com/neulab/RIPPLe.", "published": "2020-04-14 16:51:42", "link": "http://arxiv.org/abs/2004.06660v1", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Reasoning Visual Dialog with Sparse Graph Learning and Knowledge\n  Transfer", "abstract": "Visual dialog is a task of answering a sequence of questions grounded in an\nimage using the previous dialog history as context. In this paper, we study how\nto address two fundamental challenges for this task: (1) reasoning over\nunderlying semantic structures among dialog rounds and (2) identifying several\nappropriate answers to the given question. To address these challenges, we\npropose a Sparse Graph Learning (SGL) method to formulate visual dialog as a\ngraph structure learning task. SGL infers inherently sparse dialog structures\nby incorporating binary and score edges and leveraging a new structural loss\nfunction. Next, we introduce a Knowledge Transfer (KT) method that extracts the\nanswer predictions from the teacher model and uses them as pseudo labels. We\npropose KT to remedy the shortcomings of single ground-truth labels, which\nseverely limit the ability of a model to obtain multiple reasonable answers. As\na result, our proposed model significantly improves reasoning capability\ncompared to baseline methods and outperforms the state-of-the-art approaches on\nthe VisDial v1.0 dataset. The source code is available at\nhttps://github.com/gicheonkang/SGLKT-VisDial.", "published": "2020-04-14 17:52:41", "link": "http://arxiv.org/abs/2004.06698v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Probabilistic Model of Narratives Over Topical Trends in Social Media: A\n  Discrete Time Model", "abstract": "Online social media platforms are turning into the prime source of news and\nnarratives about worldwide events. However,a systematic summarization-based\nnarrative extraction that can facilitate communicating the main underlying\nevents is lacking. To address this issue, we propose a novel event-based\nnarrative summary extraction framework. Our proposed framework is designed as a\nprobabilistic topic model, with categorical time distribution, followed by\nextractive text summarization. Our topic model identifies topics' recurrence\nover time with a varying time resolution. This framework not only captures the\ntopic distributions from the data, but also approximates the user activity\nfluctuations over time. Furthermore, we define significance-dispersity\ntrade-off (SDT) as a comparison measure to identify the topic with the highest\nlifetime attractiveness in a timestamped corpus. We evaluate our model on a\nlarge corpus of Twitter data, including more than one million tweets in the\ndomain of the disinformation campaigns conducted against the White Helmets of\nSyria. Our results indicate that the proposed framework is effective in\nidentifying topical trends, as well as extracting narrative summaries from text\ncorpus with timestamped data.", "published": "2020-04-14 20:18:21", "link": "http://arxiv.org/abs/2004.06793v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Two-stage model and optimal SI-SNR for monaural multi-speaker speech\n  separation in noisy environment", "abstract": "In daily listening environments, speech is always distorted by background\nnoise, room reverberation and interference speakers. With the developing of\ndeep learning approaches, much progress has been performed on monaural\nmulti-speaker speech separation. Nevertheless, most studies in this area focus\non a simple problem setup of laboratory environment, which background noises\nand room reverberations are not considered. In this paper, we propose a\ntwo-stage model based on conv-TasNet to deal with the notable effects of noises\nand interference speakers separately, where enhancement and separation are\nconducted sequentially using deep dilated temporal convolutional networks\n(TCN). In addition, we develop a new objective function named optimal\nscale-invariant signal-noise ratio (OSI-SNR), which are better than original\nSI-SNR at any circumstances. By jointly training the two-stage model with\nOSI-SNR, our algorithm outperforms one-stage separation baselines\nsubstantially.", "published": "2020-04-14 07:32:19", "link": "http://arxiv.org/abs/2004.06332v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An explainability study of the constant Q cepstral coefficient spoofing\n  countermeasure for automatic speaker verification", "abstract": "Anti-spoofing for automatic speaker verification is now a well established\narea of research, with three competitive challenges having been held in the\nlast 6 years. A great deal of research effort over this time has been invested\ninto the development of front-end representations tailored to the spoofing\ndetection task. One such approach known as constant Q cepstral coefficients\n(CQCCs) have been shown to be especially effective in detecting attacks\nimplemented with a unit selection based speech synthesis algorithm. Despite\ntheir success, they largely fail in detecting other forms of spoofing attack\nwhere more traditional front-end representations give substantially better\nresults. Similar differences were also observed in the most recent, 2019\nedition of the ASVspoof challenge series. This paper reports our attempts to\nhelp explain these observations. The explanation is shown to lie in the level\nof attention paid by each front-end to different sub-band components of the\nspectrum. Thus far, surprisingly little has been learned about what artefacts\nare being detected by spoofing countermeasures. Our work hence aims to shed\nlight upon signal or spectrum level artefacts that serve to distinguish\ndifferent forms of spoofing attack from genuine, bone fide speech. With a\nbetter understanding of these artefacts we will be better positioned to design\nmore reliable countermeasures.", "published": "2020-04-14 11:16:10", "link": "http://arxiv.org/abs/2004.06422v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Hearpiece database of individual transfer functions of an openly\n  available in-the-ear earpiece for hearing device research", "abstract": "We present a database of acoustic transfer functions of the Hearpiece, an\nopenly available multi-microphone multi-driver in-the-ear earpiece for hearing\ndevice research. The database includes HRTFs for 87 incidence directions as\nwell as responses of the drivers, all measured at the four microphones of the\nHearpiece as well as the eardrum in the occluded and open ear. The transfer\nfunctions were measured in both ears of 25 human subjects and a KEMAR with\nanthropometric pinnae for five reinsertions of the device. We describe the\nmeasurements of the database and analyse derived acoustic parameters of the\ndevice. All regarded transfer functions are subject to differences between\nsubjects as well as variations due to reinsertion into the same ear. Also, the\nresults show that KEMAR measurements represent a median human ear well for all\nassessed transfer functions. The database is a rich basis for development,\nevaluation and robustness analysis of multiple hearing device algorithms and\napplications. The database is openly available at\nhttps://doi.org/10.5281/zenodo.3733191.", "published": "2020-04-14 15:04:18", "link": "http://arxiv.org/abs/2004.06579v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Alzheimer's Dementia Recognition through Spontaneous Speech: The ADReSS\n  Challenge", "abstract": "The ADReSS Challenge at INTERSPEECH 2020 defines a shared task through which\ndifferent approaches to the automated recognition of Alzheimer's dementia based\non spontaneous speech can be compared. ADReSS provides researchers with a\nbenchmark speech dataset which has been acoustically pre-processed and balanced\nin terms of age and gender, defining two cognitive assessment tasks, namely:\nthe Alzheimer's speech classification task and the neuropsychological score\nregression task. In the Alzheimer's speech classification task, ADReSS\nchallenge participants create models for classifying speech as dementia or\nhealthy control speech. In the the neuropsychological score regression task,\nparticipants create models to predict mini-mental state examination scores.\nThis paper describes the ADReSS Challenge in detail and presents a baseline for\nboth tasks, including feature extraction procedures and results for\nclassification and regression models. ADReSS aims to provide the speech and\nlanguage Alzheimer's research community with a platform for comprehensive\nmethodological comparisons. This will hopefully contribute to addressing the\nlack of standardisation that currently affects the field and shed light on\navenues for future research and clinical applicability.", "published": "2020-04-14 23:25:09", "link": "http://arxiv.org/abs/2004.06833v3", "categories": ["eess.AS", "cs.LG", "stat.ML"], "primary_category": "eess.AS"}
