{"title": "Fighting Authorship Linkability with Crowdsourcing", "abstract": "Massive amounts of contributed content -- including traditional literature,\nblogs, music, videos, reviews and tweets -- are available on the Internet\ntoday, with authors numbering in many millions. Textual information, such as\nproduct or service reviews, is an important and increasingly popular type of\ncontent that is being used as a foundation of many trendy community-based\nreviewing sites, such as TripAdvisor and Yelp. Some recent results have shown\nthat, due partly to their specialized/topical nature, sets of reviews authored\nby the same person are readily linkable based on simple stylometric features.\nIn practice, this means that individuals who author more than a few reviews\nunder different accounts (whether within one site or across multiple sites) can\nbe linked, which represents a significant loss of privacy.\n  In this paper, we start by showing that the problem is actually worse than\npreviously believed. We then explore ways to mitigate authorship linkability in\ncommunity-based reviewing. We first attempt to harness the global power of\ncrowdsourcing by engaging random strangers into the process of re-writing\nreviews. As our empirical results (obtained from Amazon Mechanical Turk)\nclearly demonstrate, crowdsourcing yields impressively sensible reviews that\nreflect sufficiently different stylometric characteristics such that prior\nstylometric linkability techniques become largely ineffective. We also consider\nusing machine translation to automatically re-write reviews. Contrary to what\nwas previously believed, our results show that translation decreases authorship\nlinkability as the number of intermediate languages grows. Finally, we explore\nthe combination of crowdsourcing and machine translation and report on the\nresults.", "published": "2014-05-19 23:26:44", "link": "http://arxiv.org/abs/1405.4918v1", "categories": ["cs.DL", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Modelling Data Dispersion Degree in Automatic Robust Estimation for\n  Multivariate Gaussian Mixture Models with an Application to Noisy Speech\n  Processing", "abstract": "The trimming scheme with a prefixed cutoff portion is known as a method of\nimproving the robustness of statistical models such as multivariate Gaussian\nmixture models (MG- MMs) in small scale tests by alleviating the impacts of\noutliers. However, when this method is applied to real- world data, such as\nnoisy speech processing, it is hard to know the optimal cut-off portion to\nremove the outliers and sometimes removes useful data samples as well. In this\npaper, we propose a new method based on measuring the dispersion degree (DD) of\nthe training data to avoid this problem, so as to realise automatic robust\nestimation for MGMMs. The DD model is studied by using two different measures.\nFor each one, we theoretically prove that the DD of the data samples in a\ncontext of MGMMs approximately obeys a specific (chi or chi-square)\ndistribution. The proposed method is evaluated on a real-world application with\na moderately-sized speaker recognition task. Experiments show that the proposed\nmethod can significantly improve the robustness of the conventional training\nmethod of GMMs for speaker recognition.", "published": "2014-05-19 04:36:38", "link": "http://arxiv.org/abs/1405.4599v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
