{"title": "Risk forecasting using Long Short-Term Memory Mixture Density Networks", "abstract": "This work aims to implement Long Short-Term Memory mixture density networks\n(LSTM-MDNs) for Value-at-Risk forecasting and compare their performance with\nestablished models (historical simulation, CMM, and GARCH) using a defined\nbacktesting procedure. The focus was on the neural network's ability to capture\nvolatility clustering and its real-world applicability. Three architectures\nwere tested: a 2-component mixture density network, a regularized 2-component\nmodel (Arimond et al., 2020), and a 3-component mixture model, the latter being\ntested for the first time in Value-at-Risk forecasting.\n  Backtesting was performed on three stock indices (FTSE 100, S&P 500, EURO\nSTOXX 50) over two distinct two-year periods (2017-2018 as a calm period,\n2021-2022 as turbulent). Model performance was assessed through unconditional\ncoverage and independence assumption tests. The neural network's ability to\nhandle volatility clustering was validated via correlation analysis and\ngraphical evaluation.\n  Results show limited success for the neural network approach. LSTM-MDNs\nperformed poorly for 2017/2018 but outperformed benchmark models in 2021/2022.\nThe LSTM mechanism allowed the neural network to capture volatility clustering\nsimilarly to GARCH models. However, several issues were identified: the need\nfor proper model initialization and reliance on large datasets for effective\nlearning. The findings suggest that while LSTM-MDNs provide adequate risk\nforecasts, further research and adjustments are necessary for stable\nperformance.", "published": "2025-01-02 14:21:28", "link": "http://arxiv.org/abs/2501.01278v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Model of an Open, Decentralized Computational Network with Incentive-Based Load Balancing", "abstract": "This paper proposes a model that enables permissionless and decentralized\nnetworks for complex computations. We explore the integration and optimize load\nbalancing in an open, decentralized computational network. Our model leverages\neconomic incentives and reputation-based mechanisms to dynamically allocate\ntasks between operators and coprocessors. This approach eliminates the need for\nspecialized hardware or software, thereby reducing operational costs and\ncomplexities. We present a mathematical model that enhances restaking processes\nin blockchain systems by enabling operators to delegate complex tasks to\ncoprocessors. The model's effectiveness is demonstrated through experimental\nsimulations, showcasing its ability to optimize reward distribution, enhance\nsecurity, and improve operational efficiency.\n  Our approach facilitates a more flexible and scalable network through the use\nof economic commitments, adaptable dynamic rating models, and a coprocessor\nload incentivization system. Supported by experimental simulations, the model\ndemonstrates its capability to optimize resource allocation, enhance system\nresilience, and reduce operational risks. This ensures significant improvements\nin both security and cost-efficiency for the blockchain ecosystem.", "published": "2025-01-02 12:05:21", "link": "http://arxiv.org/abs/2501.01219v1", "categories": ["q-fin.CP", "math.DS", "math.OC"], "primary_category": "q-fin.CP"}
{"title": "Position building in competition is a game with incomplete information", "abstract": "This paper examines strategic trading under incomplete information, where\nfirms lack full knowledge of key aspects of their competitors' trading\nstrategies such as target sizes and market impact models. We extend previous\nwork on competitive trading equilibria by incorporating uncertainty through the\nframework of Bayesian games. This allows us to analyze scenarios where firms\nhave diverse beliefs about market conditions and each other's strategies. We\nderive optimal trading strategies in this setting and demonstrate how\nuncertainty significantly impacts these strategies compared to the complete\ninformation case.\n  Furthermore, we introduce a novel approach to model the presence of\nnon-strategic traders, even when strategic firms disagree on their\ncharacteristics. Our analysis reveals the complex interplay of beliefs and\nstrategic adjustments required in such an environment. Finally, we discuss\nlimitations of the current model, including the reliance on linear market\nimpact and the lack of dynamic strategy adjustments, outlining directions for\nfuture research.", "published": "2025-01-02 13:00:14", "link": "http://arxiv.org/abs/2501.01241v2", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
{"title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model", "abstract": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.", "published": "2025-01-02 03:17:51", "link": "http://arxiv.org/abs/2501.01028v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU\n  Acceleration", "abstract": "Dataset deduplication plays a crucial role in enhancing data quality,\nultimately improving the training performance and efficiency of large language\nmodels. A commonly used method for data deduplication is the MinHash LSH\nalgorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication\nmethod, but it remains suboptimal, leaving room for further improvement in\nprocessing efficiency. This paper proposes a GPU-accelerated deduplication\nframework, FED, that optimizes MinHash LSH for GPU clusters and leverages\ncomputationally efficient, partially reusable non-cryptographic hash functions.\nFED significantly outperforms the CPU-based deduplication tool in SlimPajama\n(using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in\nNVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on\na node with four GPUs. Notably, our method dramatically accelerates the\npreviously time-consuming MinHash signature generation phase, achieving\nspeed-ups of up to 260 compared to the CPU baseline. Despite these gains in\nefficiency, FED maintains high deduplication quality, with the duplicate\ndocument sets reaching a Jaccard similarity of over 0.96 compared to those\nidentified by the standard MinHash algorithm. In large-scale experiments, the\ndeduplication of 1.2 trillion tokens is completed in just 6 hours in a\nfour-node, 16-GPU environment. The related code is publicly available on GitHub\n(\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).", "published": "2025-01-02 04:11:23", "link": "http://arxiv.org/abs/2501.01046v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attending To Syntactic Information In Biomedical Event Extraction Via\n  Graph Neural Networks", "abstract": "Many models are proposed in the literature on biomedical event\nextraction(BEE). Some of them use the shortest dependency path(SDP) information\nto represent the argument classification task. There is an issue with this\nrepresentation since even missing one word from the dependency parsing graph\nmay totally change the final prediction. To this end, the full adjacency matrix\nof the dependency graph is used to embed individual tokens using a graph\nconvolutional network(GCN). An ablation study is also done to show the effect\nof the dependency graph on the overall performance. The results show a\nsignificant improvement when dependency graph information is used. The proposed\nmodel slightly outperforms state-of-the-art models on BEE over different\ndatasets.", "published": "2025-01-02 09:25:24", "link": "http://arxiv.org/abs/2501.01158v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Refinement Strategies for LLM-based Product Attribute Value\n  Extraction", "abstract": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques (error-based prompt rewriting and self-correction)\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques fail to significantly improve the extraction performance while\nsubstantially increasing processing costs. For scenarios with development data,\nfine-tuning yields the highest performance, while the ramp-up costs of\nfine-tuning are balanced out as the amount of product descriptions increases.", "published": "2025-01-02 12:55:27", "link": "http://arxiv.org/abs/2501.01237v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Model-Enhanced Symbolic Reasoning for Knowledge Base\n  Completion", "abstract": "Integrating large language models (LLMs) with rule-based reasoning offers a\npowerful solution for improving the flexibility and reliability of Knowledge\nBase Completion (KBC). Traditional rule-based KBC methods offer verifiable\nreasoning yet lack flexibility, while LLMs provide strong semantic\nunderstanding yet suffer from hallucinations. With the aim of combining LLMs'\nunderstanding capability with the logical and rigor of rule-based approaches,\nwe propose a novel framework consisting of a Subgraph Extractor, an LLM\nProposer, and a Rule Reasoner. The Subgraph Extractor first samples subgraphs\nfrom the KB. Then, the LLM uses these subgraphs to propose diverse and\nmeaningful rules that are helpful for inferring missing facts. To effectively\navoid hallucination in LLMs' generations, these proposed rules are further\nrefined by a Rule Reasoner to pinpoint the most significant rules in the KB for\nKnowledge Base Completion. Our approach offers several key benefits: the\nutilization of LLMs to enhance the richness and diversity of the proposed rules\nand the integration with rule-based reasoning to improve reliability. Our\nmethod also demonstrates strong performance across diverse KB datasets,\nhighlighting the robustness and generalizability of the proposed framework.", "published": "2025-01-02 13:14:28", "link": "http://arxiv.org/abs/2501.01246v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings", "abstract": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 25 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies.", "published": "2025-01-02 13:49:00", "link": "http://arxiv.org/abs/2501.01257v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToolComp: A Multi-Tool Reasoning & Process Supervision Benchmark", "abstract": "Despite recent advances in AI, the development of systems capable of\nexecuting complex, multi-step reasoning tasks involving multiple tools remains\na significant challenge. Current benchmarks fall short in capturing the\nreal-world complexity of tool-use reasoning, where verifying the correctness of\nnot only the final answer but also the intermediate steps is important for\nevaluation, development, and identifying failures during inference time. To\nbridge this gap, we introduce ToolComp, a comprehensive benchmark designed to\nevaluate multi-step tool-use reasoning. ToolComp is developed through a\ncollaboration between models and human annotators, featuring\nhuman-edited/verified prompts, final answers, and process supervision labels,\nallowing for the evaluation of both final outcomes and intermediate reasoning.\nEvaluation across six different model families demonstrates the challenging\nnature of our dataset, with the majority of models achieving less than 50%\naccuracy. Additionally, we generate synthetic training data to compare the\nperformance of outcome-supervised reward models (ORMs) with process-supervised\nreward models (PRMs) to assess their ability to improve complex tool-use\nreasoning as evaluated by ToolComp. Our results show that PRMs generalize\nsignificantly better than ORMs, achieving a 19% and 11% improvement in rank@1\naccuracy for ranking base and fine-tuned model trajectories, respectively.\nThese findings highlight the critical role of process supervision in both the\nevaluation and training of AI models, paving the way for more robust and\ncapable systems in complex, multi-step tool-use tasks.", "published": "2025-01-02 15:10:52", "link": "http://arxiv.org/abs/2501.01290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Mental Health Diagnostic Assessments:\n  Exploring The Potential of Large Language Models for Assisting with Mental\n  Health Diagnostic Assessments -- The Depression and Anxiety Case", "abstract": "Large language models (LLMs) are increasingly attracting the attention of\nhealthcare professionals for their potential to assist in diagnostic\nassessments, which could alleviate the strain on the healthcare system caused\nby a high patient load and a shortage of providers. For LLMs to be effective in\nsupporting diagnostic assessments, it is essential that they closely replicate\nthe standard diagnostic procedures used by clinicians. In this paper, we\nspecifically examine the diagnostic assessment processes described in the\nPatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and\nthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized\nanxiety disorder (GAD). We investigate various prompting and fine-tuning\ntechniques to guide both proprietary and open-source LLMs in adhering to these\nprocesses, and we evaluate the agreement between LLM-generated diagnostic\noutcomes and expert-validated ground truth. For fine-tuning, we utilize the\nMentalllama and Llama models, while for prompting, we experiment with\nproprietary models like GPT-3.5 and GPT-4o, as well as open-source models such\nas llama-3.1-8b and mixtral-8x7b.", "published": "2025-01-02 15:34:02", "link": "http://arxiv.org/abs/2501.01305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking", "abstract": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches.", "published": "2025-01-02 15:36:50", "link": "http://arxiv.org/abs/2501.01306v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding Knowledge in Large Language Models: A Framework for\n  Categorization and Comprehension", "abstract": "Understanding how large language models (LLMs) acquire, retain, and apply\nknowledge remains an open challenge. This paper introduces a novel framework,\nK-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness\nand confidence. The framework defines six categories of knowledge, ranging from\nhighly confident correctness to confidently held misconceptions, enabling a\nnuanced evaluation of model comprehension beyond binary accuracy. Using this\nframework, we demonstrate how techniques like chain-of-thought prompting and\nreinforcement learning with human feedback fundamentally alter the knowledge\nstructures of internal (pre-trained) and external (context-dependent) knowledge\nin LLMs. CoT particularly enhances base model performance and shows synergistic\nbenefits when applied to aligned LLMs. Moreover, our layer-wise analysis\nreveals that higher layers in LLMs encode more high-confidence knowledge, while\nlow-confidence knowledge tends to emerge in middle-to-lower layers.", "published": "2025-01-02 16:34:10", "link": "http://arxiv.org/abs/2501.01332v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models for Faithful Integrity Against Opposing\n  Argument", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks. However, they can be easily misled by unfaithful\narguments during conversations, even when their original statements are\ncorrect. To this end, we investigate the problem of maintaining faithful\nintegrity in LLMs. This involves ensuring that LLMs adhere to their faithful\nstatements in the face of opposing arguments and are able to correct their\nincorrect statements when presented with faithful arguments. In this work, we\npropose a novel framework, named Alignment for Faithful Integrity with\nConfidence Estimation (AFICE), which aims to align the LLM responses with\nfaithful integrity. Specifically, AFICE first designs a Bilateral Confidence\nEstimation (BCE) approach for estimating the uncertainty of each response\ngenerated by the LLM given a specific context, which simultaneously estimate\nthe model's confidence to the question based on the internal states during\ndecoding as well as to the answer based on cumulative probability ratios. With\nthe BCE, we construct a conversational preference dataset composed of context,\noriginal statement, and argument, which is adopted for aligning the LLM for\nfaithful integrity using Direct Preference Optimization (DPO). Extensive\nexperimental results on a wide range of benchmarks demonstrate significant\nimprovements in the LLM's ability to maintain faithful responses when\nencountering opposing arguments, ensuring both the practical utility and\ntrustworthiness of LLMs in complex interactive settings. Code and data will be\nreleased via https://github.com/zhaoy777/AFICE.git", "published": "2025-01-02 16:38:21", "link": "http://arxiv.org/abs/2501.01336v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for\n  interpretable mental health screening through psychometric practice", "abstract": "In psychological practice, standardized questionnaires serve as essential\ntools for assessing mental constructs (e.g., attitudes, traits, and emotions)\nthrough structured questions (aka items). With the increasing prevalence of\nsocial media platforms where users share personal experiences and emotions,\nresearchers are exploring computational methods to leverage this data for rapid\nmental health screening. In this study, we propose a novel adaptive\nRetrieval-Augmented Generation (RAG) approach that completes psychological\nquestionnaires by analyzing social media posts. Our method retrieves the most\nrelevant user posts for each question in a psychological survey and uses Large\nLanguage Models (LLMs) to predict questionnaire scores in a zero-shot setting.\nOur findings are twofold. First we demonstrate that this approach can\neffectively predict users' responses to psychological questionnaires, such as\nthe Beck Depression Inventory II (BDI-II), achieving performance comparable to\nor surpassing state-of-the-art models on Reddit-based benchmark datasets\nwithout relying on training data. Second, we show how this methodology can be\ngeneralized as a scalable screening tool, as the final assessment is\nsystematically derived by completing standardized questionnaires and tracking\nhow individual item responses contribute to the diagnosis, aligning with\nestablished psychometric practices.", "published": "2025-01-02 00:01:54", "link": "http://arxiv.org/abs/2501.00982v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Information Processing in Large Language Models: Insights from\n  Information Bottleneck Theory", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\na wide range of tasks by understanding input information and predicting\ncorresponding outputs. However, the internal mechanisms by which LLMs\ncomprehend input and make effective predictions remain poorly understood. In\nthis paper, we explore the working mechanism of LLMs in information processing\nfrom the perspective of Information Bottleneck Theory. We propose a\nnon-training construction strategy to define a task space and identify the\nfollowing key findings: (1) LLMs compress input information into specific task\nspaces (e.g., sentiment space, topic space) to facilitate task understanding;\n(2) they then extract and utilize relevant information from the task space at\ncritical moments to generate accurate predictions. Based on these insights, we\nintroduce two novel approaches: an Information Compression-based Context\nLearning (IC-ICL) and a Task-Space-guided Fine-Tuning (TS-FT). IC-ICL enhances\nreasoning performance and inference efficiency by compressing retrieved example\ninformation into the task space. TS-FT employs a space-guided loss to fine-tune\nLLMs, encouraging the learning of more effective compression and selection\nmechanisms. Experiments across multiple datasets validate the effectiveness of\ntask space construction. Additionally, IC-ICL not only improves performance but\nalso accelerates inference speed by over 40\\%, while TS-FT achieves superior\nresults with a minimal strategy adjustment.", "published": "2025-01-02 01:33:58", "link": "http://arxiv.org/abs/2501.00999v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MDSF: Context-Aware Multi-Dimensional Data Storytelling Framework based\n  on Large language Model", "abstract": "The exponential growth of data and advancements in big data technologies have\ncreated a demand for more efficient and automated approaches to data analysis\nand storytelling. However, automated data analysis systems still face\nchallenges in leveraging large language models (LLMs) for data insight\ndiscovery, augmented analysis, and data storytelling. This paper introduces the\nMultidimensional Data Storytelling Framework (MDSF) based on large language\nmodels for automated insight generation and context-aware storytelling. The\nframework incorporates advanced preprocessing techniques, augmented analysis\nalgorithms, and a unique scoring mechanism to identify and prioritize\nactionable insights. The use of fine-tuned LLMs enhances contextual\nunderstanding and generates narratives with minimal manual intervention. The\narchitecture also includes an agent-based mechanism for real-time storytelling\ncontinuation control. Key findings reveal that MDSF outperforms existing\nmethods across various datasets in terms of insight ranking accuracy,\ndescriptive quality, and narrative coherence. The experimental evaluation\ndemonstrates MDSF's ability to automate complex analytical tasks, reduce\ninterpretive biases, and improve user satisfaction. User studies further\nunderscore its practical utility in enhancing content structure, conclusion\nextraction, and richness of detail.", "published": "2025-01-02 02:35:38", "link": "http://arxiv.org/abs/2501.01014v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning based on symbolic and parametric knowledge bases: a survey", "abstract": "Reasoning is fundamental to human intelligence, and critical for\nproblem-solving, decision-making, and critical thinking. Reasoning refers to\ndrawing new conclusions based on existing knowledge, which can support various\napplications like clinical diagnosis, basic education, and financial analysis.\nThough a good number of surveys have been proposed for reviewing\nreasoning-related methods, none of them has systematically investigated these\nmethods from the viewpoint of their dependent knowledge base. Both the\nscenarios to which the knowledge bases are applied and their storage formats\nare significantly different. Hence, investigating reasoning methods from the\nknowledge base perspective helps us better understand the challenges and future\ndirections. To fill this gap, this paper first classifies the knowledge base\ninto symbolic and parametric ones. The former explicitly stores information in\nhuman-readable symbols, and the latter implicitly encodes knowledge within\nparameters. Then, we provide a comprehensive overview of reasoning methods\nusing symbolic knowledge bases, parametric knowledge bases, and both of them.\nFinally, we identify the future direction toward enhancing reasoning\ncapabilities to bridge the gap between human and machine intelligence.", "published": "2025-01-02 03:21:32", "link": "http://arxiv.org/abs/2501.01030v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention", "abstract": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.", "published": "2025-01-02 03:41:32", "link": "http://arxiv.org/abs/2501.01039v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Scaling of Unit Tests for Code Reward Modeling", "abstract": "Current large language models (LLMs) often struggle to produce accurate\nresponses on the first attempt for complex reasoning tasks like code\ngeneration. Prior research tackles this challenge by generating multiple\ncandidate solutions and validating them with LLM-generated unit tests. The\nexecution results of unit tests serve as reward signals to identify correct\nsolutions. As LLMs always confidently make mistakes, these unit tests are not\nreliable, thereby diminishing the quality of reward signals. Motivated by the\nobservation that scaling the number of solutions improves LLM performance, we\nexplore the impact of scaling unit tests to enhance reward signal quality. Our\npioneer experiment reveals a positive correlation between the number of unit\ntests and reward signal quality, with greater benefits observed in more\nchallenging problems. Based on these insights, we propose CodeRM-8B, a\nlightweight yet effective unit test generator that enables efficient and\nhigh-quality unit test scaling. Additionally, we implement a dynamic scaling\nmechanism that adapts the number of unit tests based on problem difficulty,\nfurther improving efficiency. Experimental results show that our approach\nsignificantly improves performance across various models on three benchmarks\n(e.g., with gains of 18.43% for Llama3-8B and 3.42% for GPT-4o-mini on\nHumanEval Plus).", "published": "2025-01-02 04:33:31", "link": "http://arxiv.org/abs/2501.01054v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Risks of Cultural Erasure in Large Language Models", "abstract": "Large language models are increasingly being integrated into applications\nthat shape the production and discovery of societal knowledge such as search,\nonline education, and travel planning. As a result, language models will shape\nhow people learn about, perceive and interact with global cultures making it\nimportant to consider whose knowledge systems and perspectives are represented\nin models. Recognizing this importance, increasingly work in Machine Learning\nand NLP has focused on evaluating gaps in global cultural representational\ndistribution within outputs. However, more work is needed on developing\nbenchmarks for cross-cultural impacts of language models that stem from a\nnuanced sociologically-aware conceptualization of cultural impact or harm. We\njoin this line of work arguing for the need of metricizable evaluations of\nlanguage technologies that interrogate and account for historical power\ninequities and differential impacts of representation on global cultures,\nparticularly for cultures already under-represented in the digital corpora. We\nlook at two concepts of erasure: omission: where cultures are not represented\nat all and simplification i.e. when cultural complexity is erased by presenting\none-dimensional views of a rich culture. The former focuses on whether\nsomething is represented, and the latter on how it is represented. We focus our\nanalysis on two task contexts with the potential to influence global cultural\nproduction. First, we probe representations that a language model produces\nabout different places around the world when asked to describe these contexts.\nSecond, we analyze the cultures represented in the travel recommendations\nproduced by a set of language model applications. Our study shows ways in which\nthe NLP community and application developers can begin to operationalize\ncomplex socio-cultural considerations into standard evaluations and benchmarks.", "published": "2025-01-02 04:57:50", "link": "http://arxiv.org/abs/2501.01056v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Attention-Guided Context Decoding for Mitigating Context\n  Faithfulness Hallucinations in Large Language Models", "abstract": "Large language models (LLMs) often exhibit Context Faithfulness\nHallucinations, where outputs deviate from retrieved information due to\nincomplete context integration. Our analysis reveals a strong correlation\nbetween token-level uncertainty and hallucinations. We hypothesize that\nattention mechanisms inherently encode context utilization signals, supported\nby probing analysis. Based on these insights, we propose Dynamic\nAttention-Guided Context Decoding (DAGCD), a lightweight framework that\nleverages attention distributions and uncertainty signals in a single-pass\ndecoding. Experiments on open-book QA datasets demonstrate DAGCD's\neffectiveness, yielding significant improvements in faithfulness and robustness\nwhile preserving computational efficiency.", "published": "2025-01-02 05:07:06", "link": "http://arxiv.org/abs/2501.01059v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BeliN: A Novel Corpus for Bengali Religious News Headline Generation\n  using Contextual Feature Fusion", "abstract": "Automatic text summarization, particularly headline generation, remains a\ncritical yet underexplored area for Bengali religious news. Existing approaches\nto headline generation typically rely solely on the article content,\noverlooking crucial contextual features such as sentiment, category, and\naspect. This limitation significantly hinders their effectiveness and overall\nperformance. This study addresses this limitation by introducing a novel\ncorpus, BeliN (Bengali Religious News) - comprising religious news articles\nfrom prominent Bangladeshi online newspapers, and MultiGen - a contextual\nmulti-input feature fusion headline generation approach. Leveraging\ntransformer-based pre-trained language models such as BanglaT5, mBART, mT5, and\nmT0, MultiGen integrates additional contextual features - including category,\naspect, and sentiment - with the news content. This fusion enables the model to\ncapture critical contextual information often overlooked by traditional\nmethods. Experimental results demonstrate the superiority of MultiGen over the\nbaseline approach that uses only news content, achieving a BLEU score of 18.61\nand ROUGE-L score of 24.19, compared to baseline approach scores of 16.08 and\n23.08, respectively. These findings underscore the importance of incorporating\ncontextual features in headline generation for low-resource languages. By\nbridging linguistic and cultural gaps, this research advances natural language\nprocessing for Bengali and other underrepresented languages. To promote\nreproducibility and further exploration, the dataset and implementation code\nare publicly accessible at https://github.com/akabircs/BeliN.", "published": "2025-01-02 05:34:21", "link": "http://arxiv.org/abs/2501.01069v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference", "abstract": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.", "published": "2025-01-02 08:57:00", "link": "http://arxiv.org/abs/2501.01144v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Blind Men and the Elephant: Diverse Perspectives on Gender Stereotypes\n  in Benchmark Datasets", "abstract": "The multifaceted challenge of accurately measuring gender stereotypical bias\nin language models is akin to discerning different segments of a broader,\nunseen entity. This short paper primarily focuses on intrinsic bias mitigation\nand measurement strategies for language models, building on prior research that\ndemonstrates a lack of correlation between intrinsic and extrinsic approaches.\nWe delve deeper into intrinsic measurements, identifying inconsistencies and\nsuggesting that these benchmarks may reflect different facets of gender\nstereotype. Our methodology involves analyzing data distributions across\ndatasets and integrating gender stereotype components informed by social\npsychology. By adjusting the distribution of two datasets, we achieve a better\nalignment of outcomes. Our findings underscore the complexity of gender\nstereotyping in language models and point to new directions for developing more\nrefined techniques to detect and reduce bias.", "published": "2025-01-02 09:40:31", "link": "http://arxiv.org/abs/2501.01168v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Augmentation Techniques for Chinese Disease Name Normalization", "abstract": "Disease name normalization is an important task in the medical domain. It\nclassifies disease names written in various formats into standardized names,\nserving as a fundamental component in smart healthcare systems for various\ndisease-related functions. Nevertheless, the most significant obstacle to\nexisting disease name normalization systems is the severe shortage of training\ndata. Consequently, we present a novel data augmentation approach that includes\na series of data augmentation techniques and some supporting modules to help\nmitigate the problem. Through extensive experimentation, we illustrate that our\nproposed approach exhibits significant performance improvements across various\nbaseline models and training objectives, particularly in scenarios with limited\ntraining data", "published": "2025-01-02 11:12:03", "link": "http://arxiv.org/abs/2501.01195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Digital Guardians: Can GPT-4, Perspective API, and Moderation API\n  reliably detect hate speech in reader comments of German online newspapers?", "abstract": "In recent years, toxic content and hate speech have become widespread\nphenomena on the internet. Moderators of online newspapers and forums are now\nrequired, partly due to legal regulations, to carefully review and, if\nnecessary, delete reader comments. This is a labor-intensive process. Some\nproviders of large language models already offer solutions for automated hate\nspeech detection or the identification of toxic content. These include GPT-4o\nfrom OpenAI, Jigsaw's (Google) Perspective API, and OpenAI's Moderation API.\nBased on the selected German test dataset HOCON34k, which was specifically\ncreated for developing tools to detect hate speech in reader comments of online\nnewspapers, these solutions are compared with each other and against the\nHOCON34k baseline. The test dataset contains 1,592 annotated text samples. For\nGPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot,\nand Few-Shot approach. The results of the experiments demonstrate that GPT-4o\noutperforms both the Perspective API and the Moderation API, and exceeds the\nHOCON34k baseline by approximately 5 percentage points, as measured by a\ncombined metric of MCC and F2-score.", "published": "2025-01-02 13:48:56", "link": "http://arxiv.org/abs/2501.01256v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Does a Large Language Model Really Speak in Human-Like Language?", "abstract": "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.", "published": "2025-01-02 14:13:44", "link": "http://arxiv.org/abs/2501.01273v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "NeutraSum: A Language Model can help a Balanced Media Diet by\n  Neutralizing News Summaries", "abstract": "Media bias in news articles arises from the political polarisation of media\noutlets, which can reinforce societal stereotypes and beliefs. Reporting on the\nsame event often varies significantly between outlets, reflecting their\npolitical leanings through polarised language and focus. Although previous\nstudies have attempted to generate bias-free summaries from multiperspective\nnews articles, they have not effectively addressed the challenge of mitigating\ninherent media bias. To address this gap, we propose \\textbf{NeutraSum}, a\nnovel framework that integrates two neutrality losses to adjust the semantic\nspace of generated summaries, thus minimising media bias. These losses,\ndesigned to balance the semantic distances across polarised inputs and ensure\nalignment with expert-written summaries, guide the generation of neutral and\nfactually rich summaries. To evaluate media bias, we employ the political\ncompass test, which maps political leanings based on economic and social\ndimensions. Experimental results on the Allsides dataset demonstrate that\nNeutraSum not only improves summarisation performance but also achieves\nsignificant reductions in media bias, offering a promising approach for neutral\nnews summarisation.", "published": "2025-01-02 14:48:07", "link": "http://arxiv.org/abs/2501.01284v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Citations and Trust in LLM Generated Responses", "abstract": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content.", "published": "2025-01-02 15:32:50", "link": "http://arxiv.org/abs/2501.01303v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Vision-Language Model Alignment and Misalignment: A Survey Through\n  the Lens of Explainability", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in processing both visual and textual information. However, the\ncritical challenge of alignment between visual and textual representations is\nnot fully understood. This survey presents a comprehensive examination of\nalignment and misalignment in LVLMs through an explainability lens. We first\nexamine the fundamentals of alignment, exploring its representational and\nbehavioral aspects, training methodologies, and theoretical foundations. We\nthen analyze misalignment phenomena across three semantic levels: object,\nattribute, and relational misalignment. Our investigation reveals that\nmisalignment emerges from challenges at multiple levels: the data level, the\nmodel level, and the inference level. We provide a comprehensive review of\nexisting mitigation strategies, categorizing them into parameter-frozen and\nparameter-tuning approaches. Finally, we outline promising future research\ndirections, emphasizing the need for standardized evaluation protocols and\nin-depth explainability studies.", "published": "2025-01-02 16:53:50", "link": "http://arxiv.org/abs/2501.01346v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Embedding-based Approaches to Hyperpartisan News Detection", "abstract": "In this paper, we describe our systems in which the objective is to determine\nwhether a given news article could be considered as hyperpartisan.\nHyperpartisan news is news that takes an extremely polarized political\nstandpoint with an intention of creating political divide among the public. We\nattempted several approaches, including n-grams, sentiment analysis, as well as\nsentence and document representation using pre-tained ELMo. Our best system\nusing pre-trained ELMo with Bidirectional LSTM achieved an accuracy of 83%\nthrough 10-fold cross-validation without much hyperparameter tuning.", "published": "2025-01-02 17:29:53", "link": "http://arxiv.org/abs/2501.01370v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Predicting the Performance of Black-box LLMs through Self-Queries", "abstract": "As large language models (LLMs) are increasingly relied on in AI systems,\npredicting when they make mistakes is crucial. While a great deal of work in\nthe field uses internal representations to interpret model behavior, these\nrepresentations are inaccessible when given solely black-box access through an\nAPI. In this paper, we extract features of LLMs in a black-box manner by using\nfollow-up prompts and taking the probabilities of different responses as\nrepresentations to train reliable predictors of model behavior. We demonstrate\nthat training a linear model on these low-dimensional representations produces\nreliable and generalizable predictors of model performance at the instance\nlevel (e.g., if a particular generation correctly answers a question).\nRemarkably, these can often outperform white-box linear predictors that operate\nover a model's hidden state or the full distribution over its vocabulary. In\naddition, we demonstrate that these extracted features can be used to evaluate\nmore nuanced aspects of a language model's state. For instance, they can be\nused to distinguish between a clean version of GPT-4o-mini and a version that\nhas been influenced via an adversarial system prompt that answers\nquestion-answering tasks incorrectly or introduces bugs into generated code.\nFurthermore, they can reliably distinguish between different model\narchitectures and sizes, enabling the detection of misrepresented models\nprovided through an API (e.g., identifying if GPT-3.5 is supplied instead of\nGPT-4o-mini).", "published": "2025-01-02 22:26:54", "link": "http://arxiv.org/abs/2501.01558v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cross-model Transferability among Large Language Models on the Platonic\n  Representations of Concepts", "abstract": "Understanding the inner workings of Large Language Models (LLMs) is a\ncritical research frontier. Prior research has shown that a single LLM's\nconcept representations can be captured as steering vectors (SVs), enabling the\ncontrol of LLM behavior (e.g., towards generating harmful content). Our work\ntakes a novel approach by exploring the intricate relationships between concept\nrepresentations across different LLMs, drawing an intriguing parallel to\nPlato's Allegory of the Cave. In particular, we introduce a linear\ntransformation method to bridge these representations and present three key\nfindings: 1) Concept representations across different LLMs can be effectively\naligned using simple linear transformations, enabling efficient cross-model\ntransfer and behavioral control via SVs. 2) This linear transformation\ngeneralizes across concepts, facilitating alignment and control of SVs\nrepresenting different concepts across LLMs. 3) A weak-to-strong\ntransferability exists between LLM concept representations, whereby SVs\nextracted from smaller LLMs can effectively control the behavior of larger\nLLMs.", "published": "2025-01-02 11:56:59", "link": "http://arxiv.org/abs/2501.02009v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Uncertainty Modeling with Semantic Graph for Hallucination\n  Detection", "abstract": "Large Language Models (LLMs) are prone to hallucination with non-factual or\nunfaithful statements, which undermines the applications in real-world\nscenarios. Recent researches focus on uncertainty-based hallucination\ndetection, which utilizes the output probability of LLMs for uncertainty\ncalculation and does not rely on external knowledge or frequent sampling from\nLLMs. Whereas, most approaches merely consider the uncertainty of each\nindependent token, while the intricate semantic relations among tokens and\nsentences are not well studied, which limits the detection of hallucination\nthat spans over multiple tokens and sentences in the passage. In this paper, we\npropose a method to enhance uncertainty modeling with semantic graph for\nhallucination detection. Specifically, we first construct a semantic graph that\nwell captures the relations among entity tokens and sentences. Then, we\nincorporate the relations between two entities for uncertainty propagation to\nenhance sentence-level hallucination detection. Given that hallucination occurs\ndue to the conflict between sentences, we further present a graph-based\nuncertainty calibration method that integrates the contradiction probability of\nthe sentence with its neighbors in the semantic graph for uncertainty\ncalculation. Extensive experiments on two datasets show the great advantages of\nour proposed approach. In particular, we obtain substantial improvements with\n19.78% in passage-level hallucination detection.", "published": "2025-01-02 16:45:05", "link": "http://arxiv.org/abs/2501.02020v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented\n  Contextual Learning", "abstract": "Cultural values alignment in Large Language Models (LLMs) is a critical\nchallenge due to their tendency to embed Western-centric biases from training\ndata, leading to misrepresentations and fairness issues in cross-cultural\ncontexts. Recent approaches, such as role-assignment and few-shot learning,\noften struggle with reliable cultural alignment as they heavily rely on\npre-trained knowledge, lack scalability, and fail to capture nuanced cultural\nvalues effectively. To address these issues, we propose ValuesRAG, a novel and\neffective framework that applies Retrieval-Augmented Generation (RAG) with\nIn-Context Learning (ICL) to integrate cultural and demographic knowledge\ndynamically during text generation. Leveraging the World Values Survey (WVS)\ndataset, ValuesRAG first generates summaries of values for each individual.\nSubsequently, we curate several representative regional datasets to serve as\ntest datasets and retrieve relevant summaries of values based on demographic\nfeatures, followed by a reranking step to select the top-k relevant summaries.\nValuesRAG consistently outperforms baseline methods, both in the main\nexperiment and in the ablation study where only the values summary was\nprovided. Notably, ValuesRAG demonstrates an accuracy of 21% improvement over\nother baseline methods, highlighting its potential to foster culturally aligned\nAI systems and enhance the inclusivity of AI-driven applications.", "published": "2025-01-02 03:26:13", "link": "http://arxiv.org/abs/2501.01031v2", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Advancing Singlish Understanding: Bridging the Gap with Datasets and\n  Multimodal Models", "abstract": "Singlish, a Creole language rooted in English, is a key focus in linguistic\nresearch within multilingual and multicultural contexts. However, its spoken\nform remains underexplored, limiting insights into its linguistic structure and\napplications. To address this gap, we standardize and annotate the largest\nspoken Singlish corpus, introducing the Multitask National Speech Corpus\n(MNSC). These datasets support diverse tasks, including Automatic Speech\nRecognition (ASR), Spoken Question Answering (SQA), Spoken Dialogue\nSummarization (SDS), and Paralinguistic Question Answering (PQA). We release\nstandardized splits and a human-verified test set to facilitate further\nresearch. Additionally, we propose SingAudioLLM, a multi-task multimodal model\nleveraging multimodal large language models to handle these tasks concurrently.\nExperiments reveal our models adaptability to Singlish context, achieving\nstate-of-the-art performance and outperforming prior models by 10-30% in\ncomparison with other AudioLLMs and cascaded solutions.", "published": "2025-01-02 03:28:52", "link": "http://arxiv.org/abs/2501.01034v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "TED: Turn Emphasis with Dialogue Feature Attention for Emotion\n  Recognition in Conversation", "abstract": "Emotion recognition in conversation (ERC) has been attracting attention by\nmethods for modeling multi-turn contexts. The multi-turn input to a pretraining\nmodel implicitly assumes that the current turn and other turns are\ndistinguished during the training process by inserting special tokens into the\ninput sequence. This paper proposes a priority-based attention method to\ndistinguish each turn explicitly by adding dialogue features into the attention\nmechanism, called Turn Emphasis with Dialogue (TED). It has a priority for each\nturn according to turn position and speaker information as dialogue features.\nIt takes multi-head self-attention between turn-based vectors for multi-turn\ninput and adjusts attention scores with the dialogue features. We evaluate TED\non four typical benchmarks. The experimental results demonstrate that TED has\nhigh overall performance in all datasets and achieves state-of-the-art\nperformance on IEMOCAP with numerous turns.", "published": "2025-01-02 07:44:48", "link": "http://arxiv.org/abs/2501.01123v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A\n  Framework for Senior Design Projects", "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention\nfor their ability to harness collective intelligence in complex\nproblem-solving, decision-making, and planning tasks. This aligns with the\nconcept of the wisdom of crowds, where diverse agents contribute collectively\nto generating effective solutions, making it particularly suitable for\neducational settings. Senior design projects, also known as capstone or final\nyear projects, are pivotal in engineering education as they integrate\ntheoretical knowledge with practical application, fostering critical thinking,\nteamwork, and real-world problem-solving skills. In this paper, we explore the\nuse of Multi-Agent LLMs in supporting these senior design projects undertaken\nby engineering students, which often involve multidisciplinary considerations\nand conflicting objectives, such as optimizing technical performance while\naddressing ethical, social, and environmental concerns. We propose a framework\nwhere distinct LLM agents represent different expert perspectives, such as\nproblem formulation agents, system complexity agents, societal and ethical\nagents, or project managers, thus facilitating a holistic problem-solving\napproach. This implementation leverages standard multi-agent system (MAS)\nconcepts such as coordination, cooperation, and negotiation, incorporating\nprompt engineering to develop diverse personas for each agent. These agents\nengage in rich, collaborative dialogues to simulate human engineering teams,\nguided by principles from swarm AI to efficiently balance individual\ncontributions towards a unified solution. We adapt these techniques to create a\ncollaboration structure for LLM agents, encouraging interdisciplinary reasoning\nand negotiation similar to real-world senior design projects. To assess the\nefficacy of this framework, we collected six proposals of engineering and\ncomputer science of...", "published": "2025-01-02 11:25:45", "link": "http://arxiv.org/abs/2501.01205v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.MA"}
{"title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human\n  Understanding for Multi-modal Assistants", "abstract": "Faces and humans are crucial elements in social interaction and are widely\nincluded in everyday photos and videos. Therefore, a deep understanding of\nfaces and humans will enable multi-modal assistants to achieve improved\nresponse quality and broadened application scope. Currently, the multi-modal\nassistant community lacks a comprehensive and scientific evaluation of face and\nhuman understanding abilities. In this paper, we first propose a hierarchical\nability taxonomy that includes three levels of abilities. Then, based on this\ntaxonomy, we collect images and annotations from publicly available datasets in\nthe face and human community and build a semi-automatic data pipeline to\nproduce problems for the new benchmark. Finally, the obtained Face-Human-Bench\ncomprises a development set with 900 problems and a test set with 1800\nproblems, supporting both English and Chinese. We conduct evaluations over 25\nmainstream multi-modal large language models (MLLMs) with our Face-Human-Bench,\nfocusing on the correlation between abilities, the impact of the relative\nposition of targets on performance, and the impact of Chain of Thought (CoT)\nprompting on performance. Moreover, inspired by multi-modal agents, we also\nexplore which abilities of MLLMs need to be supplemented by specialist models.", "published": "2025-01-02 13:05:47", "link": "http://arxiv.org/abs/2501.01243v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ProgCo: Program Helps Self-Correction of Large Language Models", "abstract": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.", "published": "2025-01-02 13:59:20", "link": "http://arxiv.org/abs/2501.01264v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CultureVLM: Characterizing and Improving Cultural Understanding of\n  Vision-Language Models for over 100 Countries", "abstract": "Vision-language models (VLMs) have advanced human-AI interaction but struggle\nwith cultural understanding, often misinterpreting symbols, gestures, and\nartifacts due to biases in predominantly Western-centric training data. In this\npaper, we construct CultureVerse, a large-scale multimodal benchmark covering\n19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3\nquestion types, with the aim of characterizing and improving VLMs'\nmulticultural understanding capabilities. Then, we propose CultureVLM, a series\nof VLMs fine-tuned on our dataset to achieve significant performance\nimprovement in cultural understanding. Our evaluation of 16 models reveals\nsignificant disparities, with a stronger performance in Western concepts and\nweaker results in African and Asian contexts. Fine-tuning on our CultureVerse\nenhances cultural perception, demonstrating cross-cultural, cross-continent,\nand cross-dataset generalization without sacrificing performance on models'\ngeneral VLM benchmarks. We further present insights on cultural generalization\nand forgetting. We hope that this work could lay the foundation for more\nequitable and culturally aware multimodal AI systems.", "published": "2025-01-02 14:42:37", "link": "http://arxiv.org/abs/2501.01282v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for\n  Test Case Generation", "abstract": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task.", "published": "2025-01-02 16:30:05", "link": "http://arxiv.org/abs/2501.01329v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "AdaptVC: High Quality Voice Conversion with Adaptive Learning", "abstract": "The goal of voice conversion is to transform the speech of a source speaker\nto sound like that of a reference speaker while preserving the original\ncontent. A key challenge is to extract disentangled linguistic content from the\nsource and voice style from the reference. While existing approaches leverage\nvarious methods to isolate the two, a generalization still requires further\nattention, especially for robustness in zero-shot scenarios. In this paper, we\nachieve successful disentanglement of content and speaker features by tuning\nself-supervised speech features with adapters. The adapters are trained to\ndynamically encode nuanced features from rich self-supervised features, and the\ndecoder fuses them to produce speech that accurately resembles the reference\nwith minimal loss of content. Moreover, we leverage a conditional flow matching\ndecoder with cross-attention speaker conditioning to further boost the\nsynthesis quality and efficiency. Subjective and objective evaluations in a\nzero-shot scenario demonstrate that the proposed method outperforms existing\nmodels in speech quality and similarity to the reference speech.", "published": "2025-01-02 16:54:08", "link": "http://arxiv.org/abs/2501.01347v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding", "abstract": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications.", "published": "2025-01-02 17:20:41", "link": "http://arxiv.org/abs/2501.01366v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Training Medical Large Vision-Language Models with Abnormal-Aware\n  Feedback", "abstract": "Existing Medical Large Vision-Language Models (Med-LVLMs), which encapsulate\nextensive medical knowledge, demonstrate excellent capabilities in\nunderstanding medical images and responding to human queries based on these\nimages. However, there remain challenges in visual localization in medical\nimages, which is crucial for abnormality detection and interpretation. To\naddress these issues, we propose a novel UMed-LVLM designed with Unveiling\nMedical abnormalities. Specifically, we collect a Medical Abnormalities\nUnveiling (MAU) dataset and propose a two-stage training method for UMed-LVLM\ntraining. To collect MAU dataset, we propose a prompt method utilizing the\nGPT-4V to generate diagnoses based on identified abnormal areas in medical\nimages. Moreover, the two-stage training method includes Abnormal-Aware\nInstruction Tuning and Abnormal-Aware Rewarding, comprising Abnormal\nLocalization Rewarding and Vision Relevance Rewarding. Experimental results\ndemonstrate that our UMed-LVLM surpasses existing Med-LVLMs in identifying and\nunderstanding medical abnormality. In addition, this work shows that enhancing\nthe abnormality detection capabilities of Med-LVLMs significantly improves\ntheir understanding of medical images and generalization capability.", "published": "2025-01-02 17:37:20", "link": "http://arxiv.org/abs/2501.01377v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OmniChat: Enhancing Spoken Dialogue Systems with Scalable Synthetic Data\n  for Diverse Scenarios", "abstract": "With the rapid development of large language models, researchers have created\nincreasingly advanced spoken dialogue systems that can naturally converse with\nhumans. However, these systems still struggle to handle the full complexity of\nreal-world conversations, including audio events, musical contexts, and\nemotional expressions, mainly because current dialogue datasets are constrained\nin both scale and scenario diversity. In this paper, we propose leveraging\nsynthetic data to enhance the dialogue models across diverse scenarios. We\nintroduce ShareChatX, the first comprehensive, large-scale dataset for spoken\ndialogue that spans diverse scenarios. Based on this dataset, we introduce\nOmniChat, a multi-turn dialogue system with a heterogeneous feature fusion\nmodule, designed to optimize feature selection in different dialogue contexts.\nIn addition, we explored critical aspects of training dialogue systems using\nsynthetic data. Through comprehensive experimentation, we determined the ideal\nbalance between synthetic and real data, achieving state-of-the-art results on\nthe real-world dialogue dataset DailyTalk. We also highlight the crucial\nimportance of synthetic data in tackling diverse, complex dialogue scenarios,\nespecially those involving audio and music. For more details, please visit our\ndemo page at \\url{https://sharechatx.github.io/}.", "published": "2025-01-02 17:58:23", "link": "http://arxiv.org/abs/2501.01384v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unifying Specialized Visual Encoders for Video Language Models", "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding.", "published": "2025-01-02 18:59:45", "link": "http://arxiv.org/abs/2501.01426v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enhancing Reasoning through Process Supervision with Monte Carlo Tree\n  Search", "abstract": "Large language models (LLMs) have demonstrated their remarkable capacity\nacross a variety of tasks. However, reasoning remains a challenge for LLMs. To\nimprove LLMs' reasoning ability, process supervision has proven to be better\nthan outcome supervision. In this work, we study using Monte Carlo Tree Search\n(MCTS) to generate process supervision data with LLMs themselves for training\nthem. We sample reasoning steps with an LLM and assign each step a score that\ncaptures its \"relative correctness,\" and the LLM is then trained by minimizing\nweighted log-likelihood of generating the reasoning steps. This\ngenerate-then-train process is repeated iteratively until convergence.Our\nexperimental results demonstrate that the proposed methods considerably improve\nthe performance of LLMs on two mathematical reasoning datasets. Furthermore,\nmodels trained on one dataset also exhibit improved performance on the other,\nshowing the transferability of the enhanced reasoning ability.", "published": "2025-01-02 12:09:17", "link": "http://arxiv.org/abs/2501.01478v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Improving Robustness Estimates in Natural Language Explainable AI though\n  Synonymity Weighted Similarity Measures", "abstract": "Explainable AI (XAI) has seen a surge in recent interest with the\nproliferation of powerful but intractable black-box models. Moreover, XAI has\ncome under fire for techniques that may not offer reliable explanations. As\nmany of the methods in XAI are themselves models, adversarial examples have\nbeen prominent in the literature surrounding the effectiveness of XAI, with the\nobjective of these examples being to alter the explanation while maintaining\nthe output of the original model. For explanations in natural language, it is\nnatural to use measures found in the domain of information retrieval for use\nwith ranked lists to guide the adversarial XAI process. We show that the\nstandard implementation of these measures are poorly suited for the comparison\nof explanations in adversarial XAI and amend them by using information that is\ndiscarded, the synonymity of perturbed words. This synonymity weighting\nproduces more accurate estimates of the actual weakness of XAI methods to\nadversarial examples.", "published": "2025-01-02 19:49:04", "link": "http://arxiv.org/abs/2501.01516v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Metasemantic-Metapragmatic Framework for Taxonomizing Multimodal\n  Communicative Alignment", "abstract": "Drawing on contemporary pragmatist philosophy and linguistic theories on\ncognition, meaning, and communication, this paper presents a dynamic,\nmetasemantic-metapragmatic taxonomy for grounding and conceptualizing\nhuman-like multimodal communicative alignment. The framework is rooted in\ncontemporary developments of the three basic communicative capacities initially\nidentified by American logician and pragmatist philosopher Charles Sanders\nPeirce: iconic (sensory and perceptual qualities), indexical (contextual and\nsociocultural associations), and rule-like (symbolic and intuitive reasoning).\nExpanding on these developments, I introduce the concept of indexical\ncontextualization and propose the principle of \"contextualization\ndirectionality\" for characterizing the crucial metapragmatic capacity for\nmaintaining, navigating, or transitioning between semantic and pragmatic modes\nof multimodal communication. I contend that current cognitive-social\ncomputational and engineering methodologies disproportionately emphasize the\nsemantic/metasemantic domain, overlooking the pivotal role of metapragmatic\nindexicality in traversing the semantic-pragmatic spectrum of communication.\nThe framework's broader implications for intentionality, identity, affect, and\nethics in within-modal and cross-modal human-machine alignment are also\ndiscussed.", "published": "2025-01-02 21:00:19", "link": "http://arxiv.org/abs/2501.01535v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "Many of Your DPOs are Secretly One: Attempting Unification Through\n  Mutual Information", "abstract": "Post-alignment of large language models (LLMs) is critical in improving their\nutility, safety, and alignment with human intentions. Direct preference\noptimisation (DPO) has become one of the most widely used algorithms for\nachieving this alignment, given its ability to optimise models based on human\nfeedback directly. However, the vast number of DPO variants in the literature\nhas made it increasingly difficult for researchers to navigate and fully grasp\nthe connections between these approaches. This paper introduces a unifying\nframework inspired by mutual information, which proposes a new loss function\nwith flexible priors. By carefully specifying these priors, we demonstrate that\nmany existing algorithms, such as SimPO, TDPO, SparsePO, and others, can be\nderived from our framework. This unification offers a clearer and more\nstructured approach, allowing researchers to understand the relationships\nbetween different DPO variants better. We aim to simplify the landscape of DPO\nalgorithms, making it easier for the research community to gain insights and\nfoster further advancements in LLM alignment. Ultimately, we hope our framework\ncan be a foundation for developing more robust and interpretable alignment\ntechniques.", "published": "2025-01-02 21:31:38", "link": "http://arxiv.org/abs/2501.01544v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Safeguarding Large Language Models in Real-time with Tunable\n  Safety-Performance Trade-offs", "abstract": "Large Language Models (LLMs) have been shown to be susceptible to jailbreak\nattacks, or adversarial attacks used to illicit high risk behavior from a\nmodel. Jailbreaks have been exploited by cybercriminals and blackhat actors to\ncause significant harm, highlighting the critical need to safeguard\nwidely-deployed models. Safeguarding approaches, which include fine-tuning\nmodels or having LLMs \"self-reflect\", may lengthen the inference time of a\nmodel, incur a computational penalty, reduce the semantic fluency of an output,\nand restrict ``normal'' model behavior. Importantly, these Safety-Performance\nTrade-offs (SPTs) remain an understudied area. In this work, we introduce a\nnovel safeguard, called SafeNudge, that combines Controlled Text Generation\nwith \"nudging\", or using text interventions to change the behavior of a model.\nSafeNudge triggers during text-generation while a jailbreak attack is being\nexecuted, and can reduce successful jailbreak attempts by 30% by guiding the\nLLM towards a safe responses. It adds minimal latency to inference and has a\nnegligible impact on the semantic fluency of outputs. Further, we allow for\ntunable SPTs. SafeNudge is open-source and available through https://pypi.org/,\nand is compatible with models loaded with the Hugging Face \"transformers\"\nlibrary.", "published": "2025-01-02 15:15:38", "link": "http://arxiv.org/abs/2501.02018v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual\n  Vector Quantization", "abstract": "Recent years have witnessed the success of foundation models pre-trained with\nself-supervised learning (SSL) in various music informatics understanding\ntasks, including music tagging, instrument classification, key detection, and\nmore. In this paper, we propose a self-supervised music representation learning\nmodel for music understanding. Distinguished from previous studies adopting\nrandom projection or existing neural codec, the proposed model, named MuQ, is\ntrained to predict tokens generated by Mel Residual Vector Quantization\n(Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel\nspectrum quantization to enhance the stability and efficiency of target\nextraction and lead to better performance. Experiments in a large variety of\ndownstream tasks demonstrate that MuQ outperforms previous self-supervised\nmusic representation models with only 0.9K hours of open-source pre-training\ndata. Scaling up the data to over 160K hours and adopting iterative training\nconsistently improve the model performance. To further validate the strength of\nour model, we present MuQ-MuLan, a joint music-text embedding model based on\ncontrastive learning, which achieves state-of-the-art performance in the\nzero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints\nare open source in https://github.com/tencent-ailab/MuQ.", "published": "2025-01-02 07:08:29", "link": "http://arxiv.org/abs/2501.01108v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Toward Inclusive Educational AI: Auditing Frontier LLMs through a\n  Multiplexity Lens", "abstract": "As large language models (LLMs) like GPT-4 and Llama 3 become integral to\neducational contexts, concerns are mounting over the cultural biases, power\nimbalances, and ethical limitations embedded within these technologies. Though\ngenerative AI tools aim to enhance learning experiences, they often reflect\nvalues rooted in Western, Educated, Industrialized, Rich, and Democratic\n(WEIRD) cultural paradigms, potentially sidelining diverse global perspectives.\nThis paper proposes a framework to assess and mitigate cultural bias within\nLLMs through the lens of applied multiplexity. Multiplexity, inspired by\nSenturk et al. and rooted in Islamic and other wisdom traditions, emphasizes\nthe coexistence of diverse cultural viewpoints, supporting a multi-layered\nepistemology that integrates both empirical sciences and normative values. Our\nanalysis reveals that LLMs frequently exhibit cultural polarization, with\nbiases appearing in both overt responses and subtle contextual cues. To address\ninherent biases and incorporate multiplexity in LLMs, we propose two\nstrategies: \\textit{Contextually-Implemented Multiplex LLMs}, which embed\nmultiplex principles directly into the system prompt, influencing LLM outputs\nat a foundational level and independent of individual prompts, and\n\\textit{Multi-Agent System (MAS)-Implemented Multiplex LLMs}, where multiple\nLLM agents, each representing distinct cultural viewpoints, collaboratively\ngenerate a balanced, synthesized response. Our findings demonstrate that as\nmitigation strategies evolve from contextual prompting to MAS-implementation,\ncultural inclusivity markedly improves, evidenced by a significant rise in the\nPerspectives Distribution Score (PDS) and a PDS Entropy increase from 3.25\\% at\nbaseline to 98\\% with the MAS-Implemented Multiplex LLMs. Sentiment analysis\nfurther shows a shift towards positive sentiment across cultures,...", "published": "2025-01-02 11:27:08", "link": "http://arxiv.org/abs/2501.03259v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "VoiceVector: Multimodal Enrolment Vectors for Speaker Separation", "abstract": "We present a transformer-based architecture for voice separation of a target\nspeaker from multiple other speakers and ambient noise. We achieve this by\nusing two separate neural networks: (A) An enrolment network designed to craft\nspeaker-specific embeddings, exploiting various combinations of audio and\nvisual modalities; and (B) A separation network that accepts both the noisy\nsignal and enrolment vectors as inputs, outputting the clean signal of the\ntarget speaker. The novelties are: (i) the enrolment vector can be produced\nfrom: audio only, audio-visual data (using lip movements) or visual data alone\n(using lip movements from silent video); and (ii) the flexibility in\nconditioning the separation on multiple positive and negative enrolment\nvectors. We compare with previous methods and obtain superior performance.", "published": "2025-01-02 18:25:27", "link": "http://arxiv.org/abs/2501.01401v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Time Difference of Arrival Source Localization: Exact Linear Solutions\n  for the General 3D Problem", "abstract": "The time difference of arrival (TDOA) problem admits exact, purely algebraic\nsolutions for the situation in which there are 4 and 5 sensors and a single\nsource whose position is to be determined in 3 dimensions. The solutions are\nexact in the sense that there is no least squares operation (i.e., projection)\ninvolved in the solution. The solutions involve no linearization or iteration,\nand are algebraically transparent via vector algebra in Cartesian coordinates.\nThe solution with 5 sensors requires no resolution of sign ambiguities; the\nsolution with 4 sensors requires resolution of one sign ambiguity. Solutions\nare effected using only TDOA and not, e.g., frequency difference of arrival\n(FDOA) or angle of arrival (AOA).\n  We first present the 5-sensor solution and then follow with the 4-sensor\nscenario. Numerical experiments are presented showing the performance of the\ncalculations in the case of no noise, before closing with conclusions.\nPerformance of the calculations is exact within numerical error, and in the\nsmall fraction of cases in which source localization does not occur, it is\ndriven by misidentification in resolution of sign ambiguity without priors. We\ntherefore believe the calculations have substantial practical utility for their\nspeed and exactness.", "published": "2025-01-02 05:47:07", "link": "http://arxiv.org/abs/2501.01076v2", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "FAST: Fast Audio Spectrogram Transformer", "abstract": "In audio classification, developing efficient and robust models is critical\nfor real-time applications. Inspired by the design principles of MobileViT, we\npresent FAST (Fast Audio Spectrogram Transformer), a new architecture that\ncombines convolutional neural networks (CNNs) and transformers to capitalize on\nthe strengths of both. FAST integrates the local feature extraction\nefficiencies of CNNs with the global context modeling capabilities of\ntransformers, resulting in a model that is powerful yet lightweight,\nwell-suited to a real-time or mobile use case. Additionally, we incorporate\nLipschitz continuous attention mechanisms to improve training stability and\naccelerate convergence. We evaluate FAST on the ADIMA dataset, a multilingual\ncorpus towards real-time profanity and abuse detection, as well as on the more\ntraditional AudioSet. Our results show that FAST achieves state-of-the-art\nperformance on both the ADIMA and AudioSet classification tasks and in some\ncases surpasses existing benchmarks while using up to 150x fewer parameters.", "published": "2025-01-02 06:54:14", "link": "http://arxiv.org/abs/2501.01104v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sensitivity of Room Impulse Responses in Changing Acoustic Environment", "abstract": "Changes in room acoustics, such as modifications to surface absorption or the\ninsertion of a scattering object, significantly impact measured room impulse\nresponses (RIRs). These changes can affect the performance of systems used in\necho cancellation and active acoustics and support tasks such as navigation and\nobject tracking. Recognizing and quantifying such changes is, therefore,\ncritical for advancing technologies based on room acoustics. This study\nintroduces a method for analyzing acoustic environment changes by evaluating\nthe similarity of consecutively recorded RIRs. Short-time coherence is employed\nto characterize modifications, including changes in wall absorption or the\npresence of a moving person in the room. A sensitivity rating is further used\nto quantify the magnitude of these changes. The results clearly differentiate\nbetween types of modifications -- atmospheric variation, changes in absorption,\nand human presence. The methods described provide a novel approach to analyzing\nand interpreting room acoustics, emphasizing RIR similarity and extracting\ninformation from temporal and spectral signal properties.", "published": "2025-01-02 11:30:12", "link": "http://arxiv.org/abs/2501.01206v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MMVA: Multimodal Matching Based on Valence and Arousal across Images,\n  Music, and Musical Captions", "abstract": "We introduce Multimodal Matching based on Valence and Arousal (MMVA), a\ntri-modal encoder framework designed to capture emotional content across\nimages, music, and musical captions. To support this framework, we expand the\nImage-Music-Emotion-Matching-Net (IMEMNet) dataset, creating IMEMNet-C which\nincludes 24,756 images and 25,944 music clips with corresponding musical\ncaptions. We employ multimodal matching scores based on the continuous valence\n(emotional positivity) and arousal (emotional intensity) values. This\ncontinuous matching score allows for random sampling of image-music pairs\nduring training by computing similarity scores from the valence-arousal values\nacross different modalities. Consequently, the proposed approach achieves\nstate-of-the-art performance in valence-arousal prediction tasks. Furthermore,\nthe framework demonstrates its efficacy in various zeroshot tasks, highlighting\nthe potential of valence and arousal predictions in downstream applications.", "published": "2025-01-02 06:36:09", "link": "http://arxiv.org/abs/2501.01094v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Disambiguation of Chinese Polyphones in an End-to-End Framework with\n  Semantic Features Extracted by Pre-trained BERT", "abstract": "Grapheme-to-phoneme (G2P) conversion serves as an essential component in\nChinese Mandarin text-to-speech (TTS) system, where polyphone disambiguation is\nthe core issue. In this paper, we propose an end-to-end framework to predict\nthe pronunciation of a polyphonic character, which accepts sentence containing\npolyphonic character as input in the form of Chinese character sequence without\nthe necessity of any preprocessing. The proposed method consists of a\npre-trained bidirectional encoder representations from Transformers (BERT)\nmodel and a neural network (NN) based classifier. The pre-trained BERT model\nextracts semantic features from a raw Chinese character sequence and the NN\nbased classifier predicts the polyphonic character's pronunciation according to\nBERT output. In out experiments, we implemented three classifiers, a\nfully-connected network based classifier, a long short-term memory (LSTM)\nnetwork based classifier and a Transformer block based classifier. The\nexperimental results compared with the baseline approach based on LSTM\ndemonstrate that, the pre-trained model extracts effective semantic features,\nwhich greatly enhances the performance of polyphone disambiguation. In\naddition, we also explored the impact of contextual information on polyphone\ndisambiguation.", "published": "2025-01-02 06:51:52", "link": "http://arxiv.org/abs/2501.01102v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "learning discriminative features from spectrograms using center loss for\n  speech emotion recognition", "abstract": "Identifying the emotional state from speech is essential for the natural\ninteraction of the machine with the speaker. However, extracting effective\nfeatures for emotion recognition is difficult, as emotions are ambiguous. We\npropose a novel approach to learn discriminative features from variable length\nspectrograms for emotion recognition by cooperating softmax cross-entropy loss\nand center loss together. The softmax cross-entropy loss enables features from\ndifferent emotion categories separable, and center loss efficiently pulls the\nfeatures belonging to the same emotion category to their center. By combining\nthe two losses together, the discriminative power will be highly enhanced,\nwhich leads to network learning more effective features for emotion\nrecognition. As demonstrated by the experimental results, after introducing\ncenter loss, both the unweighted accuracy and weighted accuracy are improved by\nover 3\\% on Mel-spectrogram input, and more than 4\\% on Short Time Fourier\nTransform spectrogram input.", "published": "2025-01-02 06:52:28", "link": "http://arxiv.org/abs/2501.01103v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Robust COVID-19 Detection from Cough Sounds using Deep Neural Decision\n  Tree and Forest: A Comprehensive Cross-Datasets Evaluation", "abstract": "This research presents a robust approach to classifying COVID-19 cough sounds\nusing cutting-edge machine-learning techniques. Leveraging deep neural decision\ntrees and deep neural decision forests, our methodology demonstrates consistent\nperformance across diverse cough sound datasets. We begin with a comprehensive\nextraction of features to capture a wide range of audio features from\nindividuals, whether COVID-19 positive or negative. To determine the most\nimportant features, we use recursive feature elimination along with\ncross-validation. Bayesian optimization fine-tunes hyper-parameters of deep\nneural decision tree and deep neural decision forest models. Additionally, we\nintegrate the SMOTE during training to ensure a balanced representation of\npositive and negative data. Model performance refinement is achieved through\nthreshold optimization, maximizing the ROC-AUC score. Our approach undergoes a\ncomprehensive evaluation in five datasets: Cambridge, Coswara, COUGHVID,\nVirufy, and the combined Virufy with the NoCoCoDa dataset. Consistently\noutperforming state-of-the-art methods, our proposed approach yields notable\nAUC scores of 0.97, 0.98, 0.92, 0.93, 0.99, and 0.99 across the respective\ndatasets. Merging all datasets into a combined dataset, our method, using a\ndeep neural decision forest classifier, achieves an AUC of 0.97. Also, our\nstudy includes a comprehensive cross-datasets analysis, revealing demographic\nand geographic differences in the cough sounds associated with COVID-19. These\ndifferences highlight the challenges in transferring learned features across\ndiverse datasets and underscore the potential benefits of dataset integration,\nimproving generalizability and enhancing COVID-19 detection from audio signals.", "published": "2025-01-02 07:35:06", "link": "http://arxiv.org/abs/2501.01117v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RingFormer: A Neural Vocoder with Ring Attention and\n  Convolution-Augmented Transformer", "abstract": "While transformers demonstrate outstanding performance across various audio\ntasks, their application to neural vocoders remains challenging. Neural\nvocoders require the generation of long audio signals at the sample level,\nwhich demands high temporal resolution. This results in significant\ncomputational costs for attention map generation and limits their ability to\nefficiently process both global and local information. Additionally, the\nsequential nature of sample generation in neural vocoders poses difficulties\nfor real-time processing, making the direct adoption of transformers\nimpractical. To address these challenges, we propose RingFormer, a neural\nvocoder that incorporates the ring attention mechanism into a lightweight\ntransformer variant, the convolution-augmented transformer (Conformer). Ring\nattention effectively captures local details while integrating global\ninformation, making it well-suited for processing long sequences and enabling\nreal-time audio generation. RingFormer is trained using adversarial training\nwith two discriminators. The proposed model is applied to the decoder of the\ntext-to-speech model VITS and compared with state-of-the-art vocoders such as\nHiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various\nobjective and subjective metrics. Experimental results show that RingFormer\nachieves comparable or superior performance to existing models, particularly\nexcelling in real-time audio generation. Our code and audio samples are\navailable on GitHub.", "published": "2025-01-02 10:18:57", "link": "http://arxiv.org/abs/2501.01182v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Reading to Listen at the Cocktail Party: Multi-Modal Speech Separation", "abstract": "The goal of this paper is speech separation and enhancement in multi-speaker\nand noisy environments using a combination of different modalities. Previous\nworks have shown good performance when conditioning on temporal or static\nvisual evidence such as synchronised lip movements or face identity. In this\npaper, we present a unified framework for multi-modal speech separation and\nenhancement based on synchronous or asynchronous cues. To that end we make the\nfollowing contributions: (i) we design a modern Transformer-based architecture\ntailored to fuse different modalities to solve the speech separation task in\nthe raw waveform domain; (ii) we propose conditioning on the textual content of\na sentence alone or in combination with visual information; (iii) we\ndemonstrate the robustness of our model to audio-visual synchronisation\noffsets; and, (iv) we obtain state-of-the-art performance on the\nwell-established benchmark datasets LRS2 and LRS3.", "published": "2025-01-02 19:53:25", "link": "http://arxiv.org/abs/2501.01518v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "FaceSpeak: Expressive and High-Quality Speech Synthesis from Human\n  Portraits of Different Styles", "abstract": "Humans can perceive speakers' characteristics (e.g., identity, gender,\npersonality and emotion) by their appearance, which are generally aligned to\ntheir voice style. Recently, vision-driven Text-to-speech (TTS) scholars\ngrounded their investigations on real-person faces, thereby restricting\neffective speech synthesis from applying to vast potential usage scenarios with\ndiverse characters and image styles. To solve this issue, we introduce a novel\nFaceSpeak approach. It extracts salient identity characteristics and emotional\nrepresentations from a wide variety of image styles. Meanwhile, it mitigates\nthe extraneous information (e.g., background, clothing, and hair color, etc.),\nresulting in synthesized speech closely aligned with a character's persona.\nFurthermore, to overcome the scarcity of multi-modal TTS data, we have devised\nan innovative dataset, namely Expressive Multi-Modal TTS, which is diligently\ncurated and annotated to facilitate research in this domain. The experimental\nresults demonstrate our proposed FaceSpeak can generate portrait-aligned voice\nwith satisfactory naturalness and quality.", "published": "2025-01-02 02:00:15", "link": "http://arxiv.org/abs/2501.03181v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
