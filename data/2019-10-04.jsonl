{"title": "DialectGram: Detecting Dialectal Variation at Multiple Geographic\n  Resolutions", "abstract": "Several computational models have been developed to detect and analyze\ndialect variation in recent years. Most of these models assume a predefined set\nof geographical regions over which they detect and analyze dialectal variation.\nHowever, dialect variation occurs at multiple levels of geographic resolution\nranging from cities within a state, states within a country, and between\ncountries across continents. In this work, we propose a model that enables\ndetection of dialectal variation at multiple levels of geographic resolution\nobviating the need for a-priori definition of the resolution level. Our method\nDialectGram, learns dialect-sensitive word embeddings while being agnostic of\nthe geographic resolution. Specifically it only requires one-time training and\nenables analysis of dialectal variation at a chosen resolution post-hoc -- a\nsignificant departure from prior models which need to be re-trained whenever\nthe pre-defined set of regions changes. Furthermore, DialectGram explicitly\nmodels senses thus enabling one to estimate the proportion of each sense usage\nin any given region. Finally, we quantitatively evaluate our model against\nother baselines on a new evaluation dataset DialectSim (in English) and show\nthat DialectGram can effectively model linguistic variation.", "published": "2019-10-04 07:22:25", "link": "http://arxiv.org/abs/1910.01818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-level Gated Recurrent Neural Network for Dialog Act Classification", "abstract": "In this paper we focus on the problem of dialog act (DA) labelling. This\nproblem has recently attracted a lot of attention as it is an important\nsub-part of an automatic question answering system, which is currently in great\ndemand. Traditional methods tend to see this problem as a sequence labelling\ntask and deals with it by applying classifiers with rich features. Most of the\ncurrent neural network models still omit the sequential information in the\nconversation. Henceforth, we apply a novel multi-level gated recurrent neural\nnetwork (GRNN) with non-textual information to predict the DA tag. Our model\nnot only utilizes textual information, but also makes use of non-textual and\ncontextual information. In comparison, our model has shown significant\nimprovement over previous works on Switchboard Dialog Act (SWDA) task by over\n6%.", "published": "2019-10-04 07:27:23", "link": "http://arxiv.org/abs/1910.01822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Confidence in Sequence-to-Sequence Models", "abstract": "Recently, significant improvements have been achieved in various natural\nlanguage processing tasks using neural sequence-to-sequence models. While\naiming for the best generation quality is important, ultimately it is also\nnecessary to develop models that can assess the quality of their output.\n  In this work, we propose to use the similarity between training and test\nconditions as a measure for models' confidence. We investigate methods solely\nusing the similarity as well as methods combining it with the posterior\nprobability. While traditionally only target tokens are annotated with\nconfidence measures, we also investigate methods to annotate source tokens with\nconfidence. By learning an internal alignment model, we can significantly\nimprove confidence projection over using state-of-the-art external alignment\ntools. We evaluate the proposed methods on downstream confidence estimation for\nmachine translation (MT). We show improvements on segment-level confidence\nestimation as well as on confidence estimation for source tokens. In addition,\nwe show that the same methods can also be applied to other tasks using\nsequence-to-sequence models. On the automatic speech recognition (ASR) task, we\nare able to find 60% of the errors by looking at 20% of the data.", "published": "2019-10-04 10:30:36", "link": "http://arxiv.org/abs/1910.01859v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Template-free Data-to-Text Generation of Finnish Sports News", "abstract": "News articles such as sports game reports are often thought to closely follow\nthe underlying game statistics, but in practice they contain a notable amount\nof background knowledge, interpretation, insight into the game, and quotes that\nare not present in the official statistics. This poses a challenge for\nautomated data-to-text news generation with real-world news corpora as training\ndata. We report on the development of a corpus of Finnish ice hockey news,\nedited to be suitable for training of end-to-end news generation methods, as\nwell as demonstrate generation of text, which was judged by journalists to be\nrelatively close to a viable product. The new dataset and system source code\nare available for research purposes at\nhttps://github.com/scoopmatic/finnish-hockey-news-generation-paper.", "published": "2019-10-04 10:40:44", "link": "http://arxiv.org/abs/1910.01863v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tanbih: Get To Know What You Are Reading", "abstract": "We introduce Tanbih, a news aggregator with intelligent analysis tools to\nhelp readers understanding what's behind a news story. Our system displays news\ngrouped into events and generates media profiles that show the general\nfactuality of reporting, the degree of propagandistic content,\nhyper-partisanship, leading political ideology, general frame of reporting, and\nstance with respect to various claims and topics of a news outlet. In addition,\nwe automatically analyse each article to detect whether it is propagandistic\nand to determine its stance with respect to a number of controversial topics.", "published": "2019-10-04 16:43:49", "link": "http://arxiv.org/abs/1910.02028v1", "categories": ["cs.CL", "cs.IR", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can I Trust the Explainer? Verifying Post-hoc Explanatory Methods", "abstract": "For AI systems to garner widespread public acceptance, we must develop\nmethods capable of explaining the decisions of black-box models such as neural\nnetworks. In this work, we identify two issues of current explanatory methods.\nFirst, we show that two prevalent perspectives on explanations ---\nfeature-additivity and feature-selection --- lead to fundamentally different\ninstance-wise explanations. In the literature, explainers from different\nperspectives are currently being directly compared, despite their distinct\nexplanation goals. The second issue is that current post-hoc explainers are\neither validated under simplistic scenarios (on simple models such as linear\nregression, or on models trained on syntactic datasets), or, when applied to\nreal-world neural networks, explainers are commonly validated under the\nassumption that the learned models behave reasonably. However, neural networks\noften rely on unreasonable correlations, even when producing correct decisions.\nWe introduce a verification framework for explanatory methods under the\nfeature-selection perspective. Our framework is based on a non-trivial neural\nnetwork architecture trained on a real-world task, and for which we are able to\nprovide guarantees on its inner workings. We validate the efficacy of our\nevaluation by showing the failure modes of current explainers. We aim for this\nframework to provide a publicly available, off-the-shelf evaluation when the\nfeature-selection perspective on explanations is needed.", "published": "2019-10-04 17:44:36", "link": "http://arxiv.org/abs/1910.02065v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Contrastive Language Adaptation for Cross-Lingual Stance Detection", "abstract": "We study cross-lingual stance detection, which aims to leverage labeled data\nin one language to identify the relative perspective (or stance) of a given\ndocument with respect to a claim in a different target language. In particular,\nwe introduce a novel contrastive language adaptation approach applied to memory\nnetworks, which ensures accurate alignment of stances in the source and target\nlanguages, and can effectively deal with the challenge of limited labeled data\nin the target language. The evaluation results on public benchmark datasets and\ncomparison against current state-of-the-art approaches demonstrate the\neffectiveness of our approach.", "published": "2019-10-04 16:01:23", "link": "http://arxiv.org/abs/1910.02076v1", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Classification As Decoder: Trading Flexibility For Control In Neural\n  Dialogue", "abstract": "Generative seq2seq dialogue systems are trained to predict the next word in\ndialogues that have already occurred. They can learn from large unlabeled\nconversation datasets, build a deep understanding of conversational context,\nand generate a wide variety of responses. This flexibility comes at the cost of\ncontrol. Undesirable responses in the training data will be reproduced by the\nmodel at inference time, and longer generations often don't make sense. Instead\nof generating responses one word at a time, we train a classifier to choose\nfrom a predefined list of full responses. The classifier is trained on\n(conversation context, response class) pairs, where each response class is a\nnoisily labeled group of interchangeable responses. At inference, we generate\nthe exemplar response associated with the predicted response class. Experts can\nedit and improve these exemplar responses over time without retraining the\nclassifier or invalidating old training data. Human evaluation of 775 unseen\ndoctor/patient conversations shows that this tradeoff improves responses. Only\n12% of our discriminative approach's responses are worse than the doctor's\nresponse in the same conversational context, compared to 18% for the generative\nmodel. A discriminative model trained without any manual labeling of response\nclasses achieves equal performance to the generative model.", "published": "2019-10-04 21:04:20", "link": "http://arxiv.org/abs/1910.03476v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Distilling BERT into Simple Neural Networks with Unlabeled Transfer Data", "abstract": "Recent advances in pre-training huge models on large amounts of text through\nself supervision have obtained state-of-the-art results in various natural\nlanguage processing tasks. However, these huge and expensive models are\ndifficult to use in practise for downstream tasks. Some recent efforts use\nknowledge distillation to compress these models. However, we see a gap between\nthe performance of the smaller student models as compared to that of the large\nteacher. In this work, we leverage large amounts of in-domain unlabeled\ntransfer data in addition to a limited amount of labeled training instances to\nbridge this gap for distilling BERT. We show that simple RNN based student\nmodels even with hard distillation can perform at par with the huge teachers\ngiven the transfer set. The student performance can be further improved with\nsoft distillation and leveraging teacher intermediate representations. We show\nthat our student models can compress the huge teacher by up to 26x while still\nmatching or even marginally exceeding the teacher performance in low-resource\nsettings with small amount of labeled data. Additionally, for the multilingual\nextension of this work with XtremeDistil (Mukherjee and Hassan Awadallah,\n2020), we demonstrate massive distillation of multilingual BERT-like teacher\nmodels by upto 35x in terms of parameter compression and 51x in terms of\nlatency speedup for batch inference while retaining 95% of its F1-score for NER\nover 41 languages.", "published": "2019-10-04 01:01:26", "link": "http://arxiv.org/abs/1910.01769v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Detecting Deception in Political Debates Using Acoustic and Textual\n  Features", "abstract": "We present work on deception detection, where, given a spoken claim, we aim\nto predict its factuality. While previous work in the speech community has\nrelied on recordings from staged setups where people were asked to tell the\ntruth or to lie and their statements were recorded, here we use real-world\npolitical debates. Thanks to the efforts of fact-checking organizations, it is\npossible to obtain annotations for statements in the context of a political\ndiscourse as true, half-true, or false. Starting with such data from the\nCLEF-2018 CheckThat! Lab, which was limited to text, we performed alignment to\nthe corresponding videos, thus producing a multimodal dataset. We further\ndeveloped a multimodal deep-learning architecture for the task of deception\ndetection, which yielded sizable improvements over the state of the art for the\nCLEF-2018 Lab task 2. Our experiments show that the use of the acoustic signal\nconsistently helped to improve the performance compared to using textual and\nmetadata features only, based on several different evaluation measures. We\nrelease the new dataset to the research community, hoping to help advance the\noverall field of multimodal deception detection.", "published": "2019-10-04 15:28:01", "link": "http://arxiv.org/abs/1910.01990v1", "categories": ["cs.CL", "cs.AI", "eess.AS", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Predicting the Role of Political Trolls in Social Media", "abstract": "We investigate the political roles of \"Internet trolls\" in social media.\nPolitical trolls, such as the ones linked to the Russian Internet Research\nAgency (IRA), have recently gained enormous attention for their ability to sway\npublic opinion and even influence elections. Analysis of the online traces of\ntrolls has shown different behavioral patterns, which target different slices\nof the population. However, this analysis is manual and labor-intensive, thus\nmaking it impractical as a first-response tool for newly-discovered troll\nfarms. In this paper, we show how to automate this analysis by using machine\nlearning in a realistic setting. In particular, we show how to classify trolls\naccording to their political role ---left, news feed, right--- by using\nfeatures extracted from social media, i.e., Twitter, in two scenarios: (i) in a\ntraditional supervised learning scenario, where labels for trolls are\navailable, and (ii) in a distant supervision scenario, where labels for trolls\nare not available, and we rely on more-commonly-available labels for news\noutlets mentioned by the trolls. Technically, we leverage the community\nstructure and the text of the messages in the online social network of trolls\nrepresented as a graph, from which we extract several types of learned\nrepresentations, i.e.,~embeddings, for the trolls. Experiments on the \"IRA\nRussian Troll\" dataset show that our methodology improves over the\nstate-of-the-art in the first scenario, while providing a compelling case for\nthe second scenario, which has not been explored in the literature thus far.", "published": "2019-10-04 15:50:30", "link": "http://arxiv.org/abs/1910.02001v1", "categories": ["cs.CL", "cs.AI", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention\n  and Spatial Memory", "abstract": "The role of robots in society keeps expanding, bringing with it the necessity\nof interacting and communicating with humans. In order to keep such interaction\nintuitive, we provide automatic wayfinding based on verbal navigational\ninstructions. Our first contribution is the creation of a large-scale dataset\nwith verbal navigation instructions. To this end, we have developed an\ninteractive visual navigation environment based on Google Street View; we\nfurther design an annotation method to highlight mined anchor landmarks and\nlocal directions between them in order to help annotators formulate typical,\nhuman references to those. The annotation task was crowdsourced on the AMT\nplatform, to construct a new Talk2Nav dataset with $10,714$ routes. Our second\ncontribution is a new learning method. Inspired by spatial cognition research\non the mental conceptualization of navigational instructions, we introduce a\nsoft dual attention mechanism defined over the segmented language instructions\nto jointly extract two partial instructions -- one for matching the next\nupcoming visual landmark and the other for matching the local directions to the\nnext landmark. On the similar lines, we also introduce spatial memory scheme to\nencode the local directional transitions. Our work takes advantage of the\nadvance in two lines of research: mental formalization of verbal navigational\ninstructions and training neural network agents for automatic way finding.\nExtensive experiments show that our method significantly outperforms previous\nnavigation methods. For demo video, dataset and code, please refer to our\nproject page: https://www.trace.ethz.ch/publications/2019/talk2nav/index.html", "published": "2019-10-04 16:44:59", "link": "http://arxiv.org/abs/1910.02029v3", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Fine-grained Sentiment Classification using BERT", "abstract": "Sentiment classification is an important process in understanding people's\nperception towards a product, service, or topic. Many natural language\nprocessing models have been proposed to solve the sentiment classification\nproblem. However, most of them have focused on binary sentiment classification.\nIn this paper, we use a promising deep learning model called BERT to solve the\nfine-grained sentiment classification task. Experiments show that our model\noutperforms other popular models for this task without sophisticated\narchitecture. We also demonstrate the effectiveness of transfer learning in\nnatural language processing in the process.", "published": "2019-10-04 09:20:48", "link": "http://arxiv.org/abs/1910.03474v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Controlled Text Generation for Data Augmentation in Intelligent\n  Artificial Agents", "abstract": "Data availability is a bottleneck during early stages of development of new\ncapabilities for intelligent artificial agents. We investigate the use of text\ngeneration techniques to augment the training data of a popular commercial\nartificial agent across categories of functionality, with the goal of faster\ndevelopment of new functionality. We explore a variety of encoder-decoder\ngenerative models for synthetic training data generation and propose using\nconditional variational auto-encoders. Our approach requires only direct\noptimization, works well with limited data and significantly outperforms the\nprevious controlled text generation techniques. Further, the generated data are\nused as additional training samples in an extrinsic intent classification task,\nleading to improved performance by up to 5\\% absolute f-score in low-resource\ncases, validating the usefulness of our approach.", "published": "2019-10-04 20:44:21", "link": "http://arxiv.org/abs/1910.03487v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Neural Language Priors", "abstract": "The choice of sentence encoder architecture reflects assumptions about how a\nsentence's meaning is composed from its constituent words. We examine the\ncontribution of these architectures by holding them randomly initialised and\nfixed, effectively treating them as as hand-crafted language priors, and\nevaluating the resulting sentence encoders on downstream language tasks. We\nfind that even when encoders are presented with additional information that can\nbe used to solve tasks, the corresponding priors do not leverage this\ninformation, except in an isolated case. We also find that apparently\nuninformative priors are just as good as seemingly informative priors on almost\nall tasks, indicating that learning is a necessary component to leverage\ninformation provided by architecture choice.", "published": "2019-10-04 16:44:33", "link": "http://arxiv.org/abs/1910.03492v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Investigating the Effectiveness of Representations Based on\n  Word-Embeddings in Active Learning for Labelling Text Datasets", "abstract": "Manually labelling large collections of text data is a time-consuming,\nexpensive, and laborious task, but one that is necessary to support machine\nlearning based on text datasets. Active learning has been shown to be an\neffective way to alleviate some of the effort required in utilising large\ncollections of unlabelled data for machine learning tasks without needing to\nfully label them. The representation mechanism used to represent text documents\nwhen performing active learning, however, has a significant influence on how\neffective the process will be. While simple vector representations such as bag\nof words have been shown to be an effective way to represent documents during\nactive learning, the emergence of representation mechanisms based on the word\nembeddings prevalent in neural network research (e.g. word2vec and\ntransformer-based models like BERT) offer a promising, and as yet not fully\nexplored, alternative. This paper describes a large-scale evaluation of the\neffectiveness of different text representation mechanisms for active learning\nacross 8 datasets from varied domains. This evaluation shows that using\nrepresentations based on modern word embeddings---especially BERT---, which\nhave not yet been widely used in active learning, achieves a significant\nimprovement over more commonly used vector-based methods like bag of words.", "published": "2019-10-04 11:00:36", "link": "http://arxiv.org/abs/1910.03505v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "SNDCNN: Self-normalizing deep CNNs with scaled exponential linear units\n  for speech recognition", "abstract": "Very deep CNNs achieve state-of-the-art results in both computer vision and\nspeech recognition, but are difficult to train. The most popular way to train\nvery deep CNNs is to use shortcut connections (SC) together with batch\nnormalization (BN). Inspired by Self- Normalizing Neural Networks, we propose\nthe self-normalizing deep CNN (SNDCNN) based acoustic model topology, by\nremoving the SC/BN and replacing the typical RELU activations with scaled\nexponential linear unit (SELU) in ResNet-50. SELU activations make the network\nself-normalizing and remove the need for both shortcut connections and batch\nnormalization. Compared to ResNet- 50, we can achieve the same or lower (up to\n4.5% relative) word error rate (WER) while boosting both training and inference\nspeed by 60%-80%. We also explore other model inference optimization schemes to\nfurther reduce latency for production use.", "published": "2019-10-04 15:31:48", "link": "http://arxiv.org/abs/1910.01992v3", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Objective Human Affective Vocal Expression Detection and Automatic\n  Classification with Stochastic Models and Learning Systems", "abstract": "This paper presents a widespread analysis of affective vocal expression\nclassification systems. In this study, state-of-the-art acoustic features are\ncompared to two novel affective vocal prints for the detection of emotional\nstates: the Hilbert-Huang-Hurst Coefficients (HHHC) and the vector of index of\nnon-stationarity (INS). HHHC is here proposed as a nonlinear vocal source\nfeature vector that represents the affective states according to their effects\non the speech production mechanism. Emotional states are highlighted by the\nempirical mode decomposition (EMD) based method, which exploits the\nnon-stationarity of the affective acoustic variations. Hurst coefficients\n(closely related to the excitation source) are then estimated from the\ndecomposition process to compose the feature vector. Additionally, the INS\nvector is introduced as dynamic information to the HHHC feature. The proposed\nfeatures are evaluated in speech emotion classification experiments with three\ndatabases in German and English languages. Three state-of-the-art acoustic\nfeatures are adopted as baseline. The $\\alpha$-integrated Gaussian model\n($\\alpha$-GMM) is also introduced for the emotion representation and\nclassification. Its performance is compared to competing stochastic and machine\nlearning classifiers. Results demonstrate that HHHC leads to significant\nclassification improvement when compared to the baseline acoustic features.\nMoreover, results also show that $\\alpha$-GMM outperforms the competing\nclassification methods. Finally, HHHC and INS are also evaluated as\ncomplementary features for the GeMAPS and eGeMAPS feature sets", "published": "2019-10-04 14:29:34", "link": "http://arxiv.org/abs/1910.01967v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Cross lingual transfer learning for zero-resource domain adaptation", "abstract": "We propose a method for zero-resource domain adaptation of DNN acoustic\nmodels, for use in low-resource situations where the only in-language training\ndata available may be poorly matched to the intended target domain. Our method\nuses a multi-lingual model in which several DNN layers are shared between\nlanguages. This architecture enables domain adaptation transforms learned for\none well-resourced language to be applied to an entirely different low-resource\nlanguage. First, to develop the technique we use English as a well-resourced\nlanguage and take Spanish to mimic a low-resource language. Experiments in\ndomain adaptation between the conversational telephone speech (CTS) domain and\nbroadcast news (BN) domain demonstrate a 29% relative WER improvement on\nSpanish BN test data by using only English adaptation data. Second, we\ndemonstrate the effectiveness of the method for low-resource languages with a\npoor match to the well-resourced language. Even in this scenario, the proposed\nmethod achieves relative WER improvements of 18-27% by using solely English\ndata for domain adaptation. Compared to other related approaches based on\nmulti-task and multi-condition training, the proposed method is able to better\nexploit well-resource language data for improved acoustic modelling of the\nlow-resource target domain.", "published": "2019-10-04 23:21:27", "link": "http://arxiv.org/abs/1910.02168v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Modeling the Comb Filter Effect and Interaural Coherence for Binaural\n  Source Separation", "abstract": "Typical methods for binaural source separation consider only the direct sound\nas the target signal in a mixture. However, in most scenarios, this assumption\nlimits the source separation performance. It is well known that the early\nreflections interact with the direct sound, producing acoustic effects at the\nlistening position, e.g. the so-called comb filter effect. In this article, we\npropose a novel source separation model, that utilizes both the direct sound\nand the first early reflection information to model the comb filter effect.\nThis is done by observing the interaural phase difference obtained from the\ntime-frequency representation of binaural mixtures. Furthermore, a method is\nproposed to model the interaural coherence of the signals. Including\ninformation related to the sound multipath propagation, the performance of the\nproposed separation method is improved with respect to the baselines that did\nnot use such information, as illustrated by using binaural recordings made in\nfour rooms, having different sizes and reverberation times.", "published": "2019-10-04 20:04:36", "link": "http://arxiv.org/abs/1910.02127v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
