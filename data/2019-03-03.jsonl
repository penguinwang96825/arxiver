{"title": "Detecting dementia in Mandarin Chinese using transfer learning from a\n  parallel corpus", "abstract": "Machine learning has shown promise for automatic detection of Alzheimer's\ndisease (AD) through speech; however, efforts are hampered by a scarcity of\ndata, especially in languages other than English. We propose a method to learn\na correspondence between independently engineered lexicosyntactic features in\ntwo languages, using a large parallel corpus of out-of-domain movie dialogue\ndata. We apply it to dementia detection in Mandarin Chinese, and demonstrate\nthat our method outperforms both unilingual and machine translation-based\nbaselines. This appears to be the first study that transfers feature domains in\ndetecting cognitive decline.", "published": "2019-03-03 16:07:10", "link": "http://arxiv.org/abs/1903.00933v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structural Supervision Improves Learning of Non-Local Grammatical\n  Dependencies", "abstract": "State-of-the-art LSTM language models trained on large corpora learn\nsequential contingencies in impressive detail and have been shown to acquire a\nnumber of non-local grammatical dependencies with some success. Here we\ninvestigate whether supervision with hierarchical structure enhances learning\nof a range of grammatical dependencies, a question that has previously been\naddressed only for subject-verb agreement. Using controlled experimental\nmethods from psycholinguistics, we compare the performance of word-based LSTM\nmodels versus two models that represent hierarchical structure and deploy it in\nleft-to-right processing: Recurrent Neural Network Grammars (RNNGs) (Dyer et\nal., 2016) and a incrementalized version of the Parsing-as-Language-Modeling\nconfiguration from Chariak et al., (2016). Models are tested on a diverse range\nof configurations for two classes of non-local grammatical dependencies in\nEnglish---Negative Polarity licensing and Filler--Gap Dependencies. Using the\nsame training data across models, we find that structurally-supervised models\noutperform the LSTM, with the RNNG demonstrating best results on both types of\ngrammatical dependencies and even learning many of the Island Constraints on\nthe filler--gap dependency. Structural supervision thus provides data\nefficiency advantages over purely string-based training of neural language\nmodels in acquiring human-like generalizations about non-local grammatical\ndependencies.", "published": "2019-03-03 17:08:00", "link": "http://arxiv.org/abs/1903.00943v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Algorithm Classes for Programming Word Problems", "abstract": "We introduce the task of algorithm class prediction for programming word\nproblems. A programming word problem is a problem written in natural language,\nwhich can be solved using an algorithm or a program. We define classes of\nvarious programming word problems which correspond to the class of algorithms\nrequired to solve the problem. We present four new datasets for this task, two\nmulticlass datasets with 550 and 1159 problems each and two multilabel datasets\nhaving 3737 and 3960 problems each. We pose the problem as a text\nclassification problem and train neural network and non-neural network-based\nmodels on this task. Our best performing classifier gets an accuracy of 62.7\npercent for the multiclass case on the five class classification dataset,\nCodeforces Multiclass-5 (CFMC5). We also do some human-level analysis and\ncompare human performance with that of our text classification models. Our best\nclassifier has an accuracy only 9 percent lower than that of a human on this\ntask. To the best of our knowledge, these are the first reported results on\nsuch a task. We make our code and datasets publicly available.", "published": "2019-03-03 04:39:33", "link": "http://arxiv.org/abs/1903.00830v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Referring Expression Grounding with Cross-modal\n  Attention-guided Erasing", "abstract": "Referring expression grounding aims at locating certain objects or persons in\nan image with a referring expression, where the key challenge is to comprehend\nand align various types of information from visual and textual domain, such as\nvisual attributes, location and interactions with surrounding regions. Although\nthe attention mechanism has been successfully applied for cross-modal\nalignments, previous attention models focus on only the most dominant features\nof both modalities, and neglect the fact that there could be multiple\ncomprehensive textual-visual correspondences between images and referring\nexpressions. To tackle this issue, we design a novel cross-modal\nattention-guided erasing approach, where we discard the most dominant\ninformation from either textual or visual domains to generate difficult\ntraining samples online, and to drive the model to discover complementary\ntextual-visual correspondences. Extensive experiments demonstrate the\neffectiveness of our proposed method, which achieves state-of-the-art\nperformance on three referring expression grounding datasets.", "published": "2019-03-03 05:55:15", "link": "http://arxiv.org/abs/1903.00839v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Calibration of Encoder Decoder Models for Neural Machine Translation", "abstract": "We study the calibration of several state of the art neural machine\ntranslation(NMT) systems built on attention-based encoder-decoder models. For\nstructured outputs like in NMT, calibration is important not just for reliable\nconfidence with predictions, but also for proper functioning of beam-search\ninference. We show that most modern NMT models are surprisingly miscalibrated\neven when conditioned on the true previous tokens. Our investigation leads to\ntwo main reasons -- severe miscalibration of EOS (end of sequence marker) and\nsuppression of attention uncertainty. We design recalibration methods based on\nthese signals and demonstrate improved accuracy, better sequence-level\ncalibration, and more intuitive results from beam-search.", "published": "2019-03-03 01:08:47", "link": "http://arxiv.org/abs/1903.00802v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
