{"title": "Modeling Past and Future for Neural Machine Translation", "abstract": "Existing neural machine translation systems do not explicitly model what has\nbeen translated and what has not during the decoding phase. To address this\nproblem, we propose a novel mechanism that separates the source information\ninto two parts: translated Past contents and untranslated Future contents,\nwhich are modeled by two additional recurrent layers. The Past and Future\ncontents are fed to both the attention model and the decoder states, which\noffers NMT systems the knowledge of translated and untranslated contents.\nExperimental results show that the proposed approach significantly improves\ntranslation performance in Chinese-English, German-English and English-German\ntranslation tasks. Specifically, the proposed model outperforms the\nconventional coverage model in both of the translation quality and the\nalignment error rate.", "published": "2017-11-27 01:42:00", "link": "http://arxiv.org/abs/1711.09502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lexical-semantic resources: yet powerful resources for automatic\n  personality classification", "abstract": "In this paper, we aim to reveal the impact of lexical-semantic resources,\nused in particular for word sense disambiguation and sense-level semantic\ncategorization, on automatic personality classification task. While stylistic\nfeatures (e.g., part-of-speech counts) have been shown their power in this\ntask, the impact of semantics beyond targeted word lists is relatively\nunexplored. We propose and extract three types of lexical-semantic features,\nwhich capture high-level concepts and emotions, overcoming the lexical gap of\nword n-grams. Our experimental results are comparable to state-of-the-art\nmethods, while no personality-specific resources are required.", "published": "2017-11-27 16:50:15", "link": "http://arxiv.org/abs/1711.09824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Slim Embedding Layers for Recurrent Neural Language Models", "abstract": "Recurrent neural language models are the state-of-the-art models for language\nmodeling. When the vocabulary size is large, the space taken to store the model\nparameters becomes the bottleneck for the use of recurrent neural language\nmodels. In this paper, we introduce a simple space compression method that\nrandomly shares the structured parameters at both the input and output\nembedding layers of the recurrent neural language models to significantly\nreduce the size of model parameters, but still compactly represent the original\ninput and output embedding layers. The method is easy to implement and tune.\nExperiments on several data sets show that the new method can get similar\nperplexity and BLEU score results while only using a very tiny fraction of\nparameters.", "published": "2017-11-27 18:43:52", "link": "http://arxiv.org/abs/1711.09873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code Completion with Neural Attention and Pointer Networks", "abstract": "Intelligent code completion has become an essential research task to\naccelerate modern software development. To facilitate effective code completion\nfor dynamically-typed programming languages, we apply neural language models by\nlearning from large codebases, and develop a tailored attention mechanism for\ncode completion. However, standard neural language models even with attention\nmechanism cannot correctly predict the out-of-vocabulary (OoV) words that\nrestrict the code completion performance. In this paper, inspired by the\nprevalence of locally repeated terms in program source code, and the recently\nproposed pointer copy mechanism, we propose a pointer mixture network for\nbetter predicting OoV words in code completion. Based on the context, the\npointer mixture network learns to either generate a within-vocabulary word\nthrough an RNN component, or regenerate an OoV word from local context through\na pointer component. Experiments on two benchmarked datasets demonstrate the\neffectiveness of our attention mechanism and pointer mixture network on the\ncode completion task.", "published": "2017-11-27 08:17:16", "link": "http://arxiv.org/abs/1711.09573v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Production Ready Chatbots: Generate if not Retrieve", "abstract": "In this paper, we present a hybrid model that combines a neural\nconversational model and a rule-based graph dialogue system that assists users\nin scheduling reminders through a chat conversation. The graph based system has\nhigh precision and provides a grammatically accurate response but has a low\nrecall. The neural conversation model can cater to a variety of requests, as it\ngenerates the responses word by word as opposed to using canned responses. The\nhybrid system shows significant improvements over the existing baseline system\nof rule based approach and caters to complex queries with a domain-restricted\nneural model. Restricting the conversation topic and combination of graph based\nretrieval system with a neural generative model makes the final system robust\nenough for a real world application.", "published": "2017-11-27 13:40:15", "link": "http://arxiv.org/abs/1711.09684v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Table-to-text Generation by Structure-aware Seq2seq Learning", "abstract": "Table-to-text generation aims to generate a description for a factual table\nwhich can be viewed as a set of field-value records. To encode both the content\nand the structure of a table, we propose a novel structure-aware seq2seq\narchitecture which consists of field-gating encoder and description generator\nwith dual attention. In the encoding phase, we update the cell memory of the\nLSTM unit by a field gate and its corresponding field value in order to\nincorporate field information into table representation. In the decoding phase,\ndual attention mechanism which contains word level attention and field level\nattention is proposed to model the semantic relevance between the generated\ndescription and the table. We conduct experiments on the \\texttt{WIKIBIO}\ndataset which contains over 700k biographies and corresponding infoboxes from\nWikipedia. The attention visualizations and case studies show that our model is\ncapable of generating coherent and informative descriptions based on the\ncomprehensive understanding of both the content and the structure of a table.\nAutomatic evaluations also show our model outperforms the baselines by a great\nmargin. Code for this work is available on\nhttps://github.com/tyliupku/wiki2bio.", "published": "2017-11-27 14:55:17", "link": "http://arxiv.org/abs/1711.09724v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Text Generation: A Practical Guide", "abstract": "Deep learning methods have recently achieved great empirical success on\nmachine translation, dialogue response generation, summarization, and other\ntext generation tasks. At a high level, the technique has been to train\nend-to-end neural network models consisting of an encoder model to produce a\nhidden representation of the source text, followed by a decoder model to\ngenerate the target. While such models have significantly fewer pieces than\nearlier systems, significant tuning is still required to achieve good\nperformance. For text generation models in particular, the decoder can behave\nin undesired ways, such as by generating truncated or repetitive outputs,\noutputting bland and generic responses, or in some cases producing\nungrammatical gibberish. This paper is intended as a practical guide for\nresolving such undesired behavior in text generation models, with the aim of\nhelping enable real-world applications.", "published": "2017-11-27 04:50:15", "link": "http://arxiv.org/abs/1711.09534v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis", "abstract": "We consider the task of fine-grained sentiment analysis from the perspective\nof multiple instance learning (MIL). Our neural model is trained on document\nsentiment labels, and learns to predict the sentiment of text segments, i.e.\nsentences or elementary discourse units (EDUs), without segment-level\nsupervision. We introduce an attention-based polarity scoring method for\nidentifying positive and negative text snippets and a new dataset which we call\nSPOT (as shorthand for Segment-level POlariTy annotations) for evaluating\nMIL-style sentiment models like ours. Experimental results demonstrate superior\nperformance against multiple baselines, whereas a judgement elicitation study\nshows that EDU-level opinion extraction produces more informative summaries\nthan sentence-based alternatives.", "published": "2017-11-27 12:21:22", "link": "http://arxiv.org/abs/1711.09645v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Bootstrapping: Learning Word Meanings From Perception-Action\n  Association", "abstract": "We address the problem of bootstrapping language acquisition for an\nartificial system similarly to what is observed in experiments with human\ninfants. Our method works by associating meanings to words in manipulation\ntasks, as a robot interacts with objects and listens to verbal descriptions of\nthe interactions. The model is based on an affordance network, i.e., a mapping\nbetween robot actions, robot perceptions, and the perceived effects of these\nactions upon objects. We extend the affordance model to incorporate spoken\nwords, which allows us to ground the verbal symbols to the execution of actions\nand the perception of the environment. The model takes verbal descriptions of a\ntask as the input and uses temporal co-occurrence to create links between\nspeech utterances and the involved objects, actions, and effects. We show that\nthe robot is able form useful word-to-meaning associations, even without\nconsidering grammatical structure in the learning process and in the presence\nof recognition errors. These word-to-meaning associations are embedded in the\nrobot's own understanding of its actions. Thus, they can be directly used to\ninstruct the robot to perform tasks and also allow to incorporate context in\nthe speech recognition task. We believe that the encouraging results with our\napproach may afford robots with a capacity to acquire language descriptors in\ntheir operation's environment as well as to shed some light as to how this\nchallenging process develops with human infants.", "published": "2017-11-27 14:42:26", "link": "http://arxiv.org/abs/1711.09714v1", "categories": ["cs.RO", "cs.CL", "cs.HC", "stat.ML", "I.2.9; I.2.10; I.2.7; I.2.6"], "primary_category": "cs.RO"}
{"title": "Multilingual Training and Cross-lingual Adaptation on CTC-based Acoustic\n  Model", "abstract": "Multilingual models for Automatic Speech Recognition (ASR) are attractive as\nthey have been shown to benefit from more training data, and better lend\nthemselves to adaptation to under-resourced languages. However, initialisation\nfrom monolingual context-dependent models leads to an explosion of\ncontext-dependent states. Connectionist Temporal Classification (CTC) is a\npotential solution to this as it performs well with monophone labels.\n  We investigate multilingual CTC in the context of adaptation and\nregularisation techniques that have been shown to be beneficial in more\nconventional contexts. The multilingual model is trained to model a universal\nInternational Phonetic Alphabet (IPA)-based phone set using the CTC loss\nfunction. Learning Hidden Unit Contribution (LHUC) is investigated to perform\nlanguage adaptive training. In addition, dropout during cross-lingual\nadaptation is also studied and tested in order to mitigate the overfitting\nproblem.\n  Experiments show that the performance of the universal phoneme-based CTC\nsystem can be improved by applying LHUC and it is extensible to new phonemes\nduring cross-lingual adaptation. Updating all the parameters shows consistent\nimprovement on limited data. Applying dropout during adaptation can further\nimprove the system and achieve competitive performance with Deep Neural Network\n/ Hidden Markov Model (DNN/HMM) systems on limited data.", "published": "2017-11-27 22:23:52", "link": "http://arxiv.org/abs/1711.10025v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
