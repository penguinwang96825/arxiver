{"title": "SUperman: Efficient Permanent Computation on GPUs", "abstract": "The permanent is a function, defined for a square matrix, with applications\nin various domains including quantum computing, statistical physics, complexity\ntheory, combinatorics, and graph theory. Its formula is similar to that of the\ndeterminant, however unlike the determinant, its exact computation is\n#P-complete, i.e., there is no algorithm to compute the permanent in polynomial\ntime unless P=NP. For an $n \\times n$ matrix, the fastest algorithm has a time\ncomplexity of $O(2^{n-1}n)$. Although supercomputers have been employed for\npermanent computation before, there is no work and more importantly, no\npublicly available software that leverages cutting-edge, yet widely accessible,\nHigh-Performance Computing accelerators such as GPUs. In this work, we\ndesigned, developed, and investigated the performance of SUperman, a complete\nsoftware suite that can compute matrix permanents on multiple nodes/GPUs on a\ncluster while handling various matrix types, e.g., real/complex/binary and\nsparse/dense etc., with a unique treatment for each type. Compared to a\nstate-of-the-art parallel algorithm on 44 cores, SUperman can be $86\\times$\nfaster on a single Nvidia A100 GPU. Combining multiple GPUs, we also showed\nthat SUperman can compute the permanent of a $56 \\times 56$ matrix which is the\nlargest reported in the literature.", "published": "2025-02-23 13:56:07", "link": "http://arxiv.org/abs/2502.16577v2", "categories": ["cs.DC", "cs.DM", "cs.NA", "math.NA"], "primary_category": "cs.DC"}
{"title": "Worst-case Error Bounds for Online Learning of Smooth Functions", "abstract": "Online learning is a model of machine learning where the learner is trained\non sequential feedback. We investigate worst-case error for the online learning\nof real functions that have certain smoothness constraints. Suppose that\n$\\mathcal{F}_q$ is the class of all absolutely continuous functions $f: [0, 1]\n\\rightarrow \\mathbb{R}$ such that $\\|f'\\|_q \\le 1$, and\n$\\operatorname{opt}_p(\\mathcal{F}_q)$ is the best possible upper bound on the\nsum of the $p^{\\text{th}}$ powers of absolute prediction errors for any number\nof trials guaranteed by any learner. We show that for any $\\delta, \\epsilon \\in\n(0, 1)$, $\\operatorname{opt}_{1+\\delta} (\\mathcal{F}_{1+\\epsilon}) =\nO(\\min(\\delta, \\epsilon)^{-1})$. Combined with the previous results of Kimber\nand Long (1995) and Geneson and Zhou (2023), we achieve a complete\ncharacterization of the values of $p, q \\ge 1$ that result in\n$\\operatorname{opt}_p(\\mathcal{F}_q)$ being finite, a problem open for nearly\n30 years.\n  We study the learning scenarios of smooth functions that also belong to\ncertain special families of functions, such as polynomials. We prove a\nconjecture by Geneson and Zhou (2023) that it is not any easier to learn a\npolynomial in $\\mathcal{F}_q$ than it is to learn any general function in\n$\\mathcal{F}_q$. We also define a noisy model for the online learning of smooth\nfunctions, where the learner may receive incorrect feedback up to $\\eta \\ge 1$\ntimes, denoting the worst-case error bound as\n$\\operatorname{opt}^{\\text{nf}}_{p, \\eta} (\\mathcal{F}_q)$. We prove that\n$\\operatorname{opt}^{\\text{nf}}_{p, \\eta} (\\mathcal{F}_q)$ is finite if and\nonly if $\\operatorname{opt}_p(\\mathcal{F}_q)$ is. Moreover, we prove for all\n$p, q \\ge 2$ and $\\eta \\ge 1$ that $\\operatorname{opt}^{\\text{nf}}_{p, \\eta}\n(\\mathcal{F}_q) = \\Theta (\\eta)$.", "published": "2025-02-23 00:43:10", "link": "http://arxiv.org/abs/2502.16388v1", "categories": ["cs.LG", "cs.DM", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs in Trading Strategies", "abstract": "This paper presents a comprehensive study on the use of ensemble\nReinforcement Learning (RL) models in financial trading strategies, leveraging\nclassifier models to enhance performance. By combining RL algorithms such as\nA2C, PPO, and SAC with traditional classifiers like Support Vector Machines\n(SVM), Decision Trees, and Logistic Regression, we investigate how different\nclassifier groups can be integrated to improve risk-return trade-offs. The\nstudy evaluates the effectiveness of various ensemble methods, comparing them\nwith individual RL models across key financial metrics, including Cumulative\nReturns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our\nresults demonstrate that ensemble methods consistently outperform base models\nin terms of risk-adjusted returns, providing better management of drawdowns and\noverall stability. However, we identify the sensitivity of ensemble performance\nto the choice of variance threshold {\\tau}, highlighting the importance of\ndynamic {\\tau} adjustment to achieve optimal performance. This study emphasizes\nthe value of combining RL with classifiers for adaptive decision-making, with\nimplications for financial trading, robotics, and other dynamic environments.", "published": "2025-02-23 04:18:05", "link": "http://arxiv.org/abs/2502.17518v1", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "stat.ML", "68T42"], "primary_category": "cs.LG"}
{"title": "Make Literature-Based Discovery Great Again through Reproducible\n  Pipelines", "abstract": "By connecting disparate sources of scientific literature, literature\\-/based\ndiscovery (LBD) methods help to uncover new knowledge and generate new research\nhypotheses that cannot be found from domain-specific documents alone. Our work\nfocuses on bisociative LBD methods that combine bisociative reasoning with LBD\ntechniques. The paper presents LBD through the lens of reproducible science to\nensure the reproducibility of LBD experiments, overcome the inconsistent use of\nbenchmark datasets and methods, trigger collaboration, and advance the LBD\nfield toward more robust and impactful scientific discoveries. The main novelty\nof this study is a collection of Jupyter Notebooks that illustrate the steps of\nthe bisociative LBD process, including data acquisition, text preprocessing,\nhypothesis formulation, and evaluation. The contributed notebooks implement a\nselection of traditional LBD approaches, as well as our own ensemble-based,\noutlier-based, and link prediction-based approaches. The reader can benefit\nfrom hands-on experience with LBD through open access to benchmark datasets,\ncode reuse, and a ready-to-run Docker recipe that ensures reproducibility of\nthe selected LBD methods.", "published": "2025-02-23 05:38:04", "link": "http://arxiv.org/abs/2502.16450v1", "categories": ["cs.CL", "68T50 (Primary) 68-04, 68-11 (Secondary)", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning of English Language and Crystal Graphs for\n  Multimodal Representation of Materials Knowledge", "abstract": "Artificial intelligence (AI) is increasingly used for the inverse design of\nmaterials, such as crystals and molecules. Existing AI research on molecules\nhas integrated chemical structures of molecules with textual knowledge to adapt\nto complex instructions. However, this approach has been unattainable for\ncrystals due to data scarcity from the biased distribution of investigated\ncrystals and the lack of semantic supervision in peer-reviewed literature. In\nthis work, we introduce a contrastive language-crystals model (CLaC)\npre-trained on a newly synthesized dataset of 126k crystal structure-text\npairs. To demonstrate the advantage of using synthetic data to overcome data\nscarcity, we constructed a comparable dataset extracted from academic papers.\nWe evaluate CLaC's generalization ability through various zero-shot cross-modal\ntasks and downstream applications. In experiments, CLaC achieves\nstate-of-the-art zero-shot generalization performance in understanding crystal\nstructures, surpassing latest large language models.", "published": "2025-02-23 05:39:46", "link": "http://arxiv.org/abs/2502.16451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge", "abstract": "Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.", "published": "2025-02-23 06:16:23", "link": "http://arxiv.org/abs/2502.16457v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Fine-Tuning Approach for T5 Using Knowledge Graphs to Address Complex\n  Tasks", "abstract": "With the development of deep learning technology, large language models have\nachieved remarkable results in many natural language processing tasks. However,\nthese models still have certain limitations in handling complex reasoning tasks\nand understanding rich background knowledge. To solve this problem, this study\nproposed a T5 model fine-tuning method based on knowledge graphs, which\nenhances the model's reasoning ability and context understanding ability by\nintroducing external knowledge graphs. We used the SQuAD1.1 dataset for\nexperiments. The experimental results show that the T5 model based on knowledge\ngraphs is significantly better than other baseline models in reasoning\naccuracy, context understanding, and the ability to handle complex problems. At\nthe same time, we also explored the impact of knowledge graphs of different\nscales on model performance and found that as the scale of the knowledge graph\nincreases, the performance of the model gradually improves. Especially when\ndealing with complex problems, the introduction of knowledge graphs greatly\nimproves the reasoning ability of the T5 model. Ablation experiments further\nverify the importance of entity and relationship embedding in the model and\nprove that a complete knowledge graph is crucial to improving the various\ncapabilities of the T5 model. In summary, this study provides an effective\nmethod to enhance the reasoning and understanding capabilities of large\nlanguage models and provides new directions for future research.", "published": "2025-02-23 07:53:37", "link": "http://arxiv.org/abs/2502.16484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All That Glitters is Not Novel: Plagiarism in AI Generated Research", "abstract": "Automating scientific research is considered the final frontier of science.\nRecently, several papers claim autonomous research agents can generate novel\nresearch ideas. Amidst the prevailing optimism, we document a critical concern:\na considerable fraction of such research documents are smartly plagiarized.\nUnlike past efforts where experts evaluate the novelty and feasibility of\nresearch ideas, we request $13$ experts to operate under a different\nsituational logic: to identify similarities between LLM-generated research\ndocuments and existing work. Concerningly, the experts identify $24\\%$ of the\n$50$ evaluated research documents to be either paraphrased (with one-to-one\nmethodological mapping), or significantly borrowed from existing work. These\nreported instances are cross-verified by authors of the source papers.\nProblematically, these LLM-generated research documents do not acknowledge\noriginal sources, and bypass inbuilt plagiarism detectors. Lastly, through\ncontrolled experiments we show that automated plagiarism detectors are\ninadequate at catching deliberately plagiarized ideas from an LLM. We recommend\na careful assessment of LLM-generated research, and discuss the implications of\nour findings on research and academic publishing.", "published": "2025-02-23 08:00:33", "link": "http://arxiv.org/abs/2502.16487v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Intrinsic Model Weaknesses: How Priming Attacks Unveil Vulnerabilities\n  in Large Language Models", "abstract": "Large language models (LLMs) have significantly influenced various industries\nbut suffer from a critical flaw, the potential sensitivity of generating\nharmful content, which poses severe societal risks. We developed and tested\nnovel attack strategies on popular LLMs to expose their vulnerabilities in\ngenerating inappropriate content. These strategies, inspired by psychological\nphenomena such as the \"Priming Effect\", \"Safe Attention Shift\", and \"Cognitive\nDissonance\", effectively attack the models' guarding mechanisms. Our\nexperiments achieved an attack success rate (ASR) of 100% on various\nopen-source models, including Meta's Llama-3.2, Google's Gemma-2, Mistral's\nMistral-NeMo, Falcon's Falcon-mamba, Apple's DCLM, Microsoft's Phi3, and Qwen's\nQwen2.5, among others. Similarly, for closed-source models such as OpenAI's\nGPT-4o, Google's Gemini-1.5, and Claude-3.5, we observed an ASR of at least 95%\non the AdvBench dataset, which represents the current state-of-the-art. This\nstudy underscores the urgent need to reassess the use of generative models in\ncritical applications to mitigate potential adverse societal impacts.", "published": "2025-02-23 08:09:23", "link": "http://arxiv.org/abs/2502.16491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge\n  Graph-Powered Fact-Checking", "abstract": "Large language models (LLMs) are widely used, but they often generate subtle\nfactual errors, especially in long-form text. These errors are fatal in some\nspecialized domains such as medicine. Existing fact-checking with grounding\ndocuments methods face two main challenges: (1) they struggle to understand\ncomplex multihop relations in long documents, often overlooking subtle factual\nerrors; (2) most specialized methods rely on pairwise comparisons, requiring\nmultiple model calls, leading to high resource and computational costs. To\naddress these challenges, we propose \\textbf{\\textit{GraphCheck}}, a\nfact-checking framework that uses extracted knowledge graphs to enhance text\nrepresentation. Graph Neural Networks further process these graphs as a soft\nprompt, enabling LLMs to incorporate structured knowledge more effectively.\nEnhanced with graph-based reasoning, GraphCheck captures multihop reasoning\nchains which are often overlooked by existing methods, enabling precise and\nefficient fact-checking in a single inference call. Experimental results on\nseven benchmarks spanning both general and medical domains demonstrate a 6.1\\%\noverall improvement over baseline models. Notably, GraphCheck outperforms\nexisting specialized fact-checkers and achieves comparable performance with\nstate-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly\nfewer parameters.", "published": "2025-02-23 09:25:00", "link": "http://arxiv.org/abs/2502.16514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning About Persuasion: Can LLMs Enable Explainable Propaganda\n  Detection?", "abstract": "There has been significant research on propagandistic content detection\nacross different modalities and languages. However, most studies have primarily\nfocused on detection, with little attention given to explanations justifying\nthe predicted label. This is largely due to the lack of resources that provide\nexplanations alongside annotated labels. To address this issue, we propose a\nmultilingual (i.e., Arabic and English) explanation-enhanced dataset, the first\nof its kind. Additionally, we introduce an explanation-enhanced LLM for both\nlabel detection and rationale-based explanation generation. Our findings\nindicate that the model performs comparably while also generating explanations.\nWe will make the dataset and experimental resources publicly available for the\nresearch community.", "published": "2025-02-23 12:12:17", "link": "http://arxiv.org/abs/2502.16550v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization", "abstract": "Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.", "published": "2025-02-23 15:00:53", "link": "http://arxiv.org/abs/2502.16600v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CodeCriticBench: A Holistic Code Critique Benchmark for Large Language\n  Models", "abstract": "The critique capacity of Large Language Models (LLMs) is essential for\nreasoning abilities, which can provide necessary suggestions (e.g., detailed\nanalysis and constructive feedback). Therefore, how to evaluate the critique\ncapacity of LLMs has drawn great attention and several critique benchmarks have\nbeen proposed. However, existing critique benchmarks usually have the following\nlimitations: (1). Focusing on diverse reasoning tasks in general domains and\ninsufficient evaluation on code tasks (e.g., only covering code generation\ntask), where the difficulty of queries is relatively easy (e.g., the code\nqueries of CriticBench are from Humaneval and MBPP). (2). Lacking comprehensive\nevaluation from different dimensions. To address these limitations, we\nintroduce a holistic code critique benchmark for LLMs called CodeCriticBench.\nSpecifically, our CodeCriticBench includes two mainstream code tasks (i.e.,\ncode generation and code QA) with different difficulties. Besides, the\nevaluation protocols include basic critique evaluation and advanced critique\nevaluation for different characteristics, where fine-grained evaluation\nchecklists are well-designed for advanced settings. Finally, we conduct\nextensive experimental results of existing LLMs, which show the effectiveness\nof CodeCriticBench.", "published": "2025-02-23 15:36:43", "link": "http://arxiv.org/abs/2502.16614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WildLong: Synthesizing Realistic Long-Context Instruction Data at Scale", "abstract": "Large language models (LLMs) with extended context windows enable tasks\nrequiring extensive information integration but are limited by the scarcity of\nhigh-quality, diverse datasets for long-context instruction tuning. Existing\ndata synthesis methods focus narrowly on objectives like fact retrieval and\nsummarization, restricting their generalizability to complex, real-world tasks.\nWildLong extracts meta-information from real user queries, models co-occurrence\nrelationships via graph-based methods, and employs adaptive generation to\nproduce scalable data. It extends beyond single-document tasks to support\nmulti-document reasoning, such as cross-document comparison and aggregation.\nOur models, finetuned on 150K instruction-response pairs synthesized using\nWildLong, surpasses existing open-source long-context-optimized models across\nbenchmarks while maintaining strong performance on short-context tasks without\nincorporating supplementary short-context data. By generating a more diverse\nand realistic long-context instruction dataset, WildLong enhances LLMs' ability\nto generalize to complex, real-world reasoning over long contexts, establishing\na new paradigm for long-context data synthesis.", "published": "2025-02-23 18:59:09", "link": "http://arxiv.org/abs/2502.16684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Pattern Recognition: Probing Mental Representations of LMs", "abstract": "Language Models (LMs) have demonstrated impressive capabilities in solving\ncomplex reasoning tasks, particularly when prompted to generate intermediate\nexplanations. However, it remains an open question whether these intermediate\nreasoning traces represent a dynamic, evolving thought process or merely\nreflect sophisticated pattern recognition acquired during large scale pre\ntraining. Drawing inspiration from human cognition, where reasoning unfolds\nincrementally as new information is assimilated and internal models are\ncontinuously updated, we propose to delve deeper into the mental model of\nvarious LMs. We propose a new way to assess the mental modeling of LMs, where\nthey are provided with problem details gradually, allowing each new piece of\ndata to build upon and refine the model's internal representation of the task.\nWe systematically compare this step by step mental modeling strategy with\ntraditional full prompt methods across both text only and vision and text\nmodalities. Experiments on the MathWorld dataset across different model sizes\nand problem complexities confirm that both text-based LLMs and multimodal LMs\nstruggle to create mental representations, questioning how their internal\ncognitive processes work.", "published": "2025-02-23 21:20:28", "link": "http://arxiv.org/abs/2502.16717v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Detection of Research Values from Scientific Abstracts Across\n  Computer Science Subfields", "abstract": "The field of Computer science (CS) has rapidly evolved over the past few\ndecades, providing computational tools and methodologies to various fields and\nforming new interdisciplinary communities. This growth in CS has significantly\nimpacted institutional practices and relevant research communities. Therefore,\nit is crucial to explore what specific research values, known as basic and\nfundamental beliefs that guide or motivate research attitudes or actions,\nCS-related research communities promote. Prior research has manually analyzed\nresearch values from a small sample of machine learning papers. No prior work\nhas studied the automatic detection of research values in CS from large-scale\nscientific texts across different research subfields. This paper introduces a\ndetailed annotation scheme featuring ten research values that guide CS-related\nresearch. Based on the scheme, we build value classifiers to scale up the\nanalysis and present a systematic study over 226,600 paper abstracts from 32\nCS-related subfields and 86 popular publishing venues over ten years.", "published": "2025-02-23 00:44:27", "link": "http://arxiv.org/abs/2502.16390v2", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large\n  Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nadvancements in multimodal understanding; however, their fundamental visual\ncognitive abilities remain largely underexplored. To bridge this gap, we\nintroduce VisFactor, a novel benchmark derived from the Factor-Referenced\nCognitive Test (FRCT), a well-established psychometric assessment of human\ncognition. VisFactor digitalizes vision-related FRCT subtests to systematically\nevaluate MLLMs across essential visual cognitive tasks including spatial\nreasoning, perceptual speed, and pattern recognition. We present a\ncomprehensive evaluation of state-of-the-art MLLMs, such as GPT-4o, Gemini-Pro,\nand Qwen-VL, using VisFactor under diverse prompting strategies like\nChain-of-Thought and Multi-Agent Debate. Our findings reveal a concerning\ndeficiency in current MLLMs' fundamental visual cognition, with performance\nfrequently approaching random guessing and showing only marginal improvements\neven with advanced prompting techniques. These results underscore the critical\nneed for focused research to enhance the core visual reasoning capabilities of\nMLLMs. To foster further investigation in this area, we release our VisFactor\nbenchmark at https://github.com/CUHK-ARISE/VisFactor.", "published": "2025-02-23 04:21:32", "link": "http://arxiv.org/abs/2502.16435v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Compression Scaling Laws:Unifying Sparsity and Quantization", "abstract": "We investigate how different compression techniques -- such as weight and\nactivation quantization, and weight sparsity -- affect the scaling behavior of\nlarge language models (LLMs) during pretraining. Building on previous work\nshowing that weight sparsity acts as a constant multiplier on model size in\nscaling laws, we demonstrate that this \"effective parameter\" scaling pattern\nextends to quantization as well. Specifically, we establish that weight-only\nquantization achieves strong parameter efficiency multipliers, while full\nquantization of both weights and activations shows diminishing returns at lower\nbitwidths. Our results suggest that different compression techniques can be\nunified under a common scaling law framework, enabling principled comparison\nand combination of these methods.", "published": "2025-02-23 04:47:36", "link": "http://arxiv.org/abs/2502.16440v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation\n  in Machine Reading Comprehension", "abstract": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data.", "published": "2025-02-23 10:04:21", "link": "http://arxiv.org/abs/2502.16523v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual\n  Program Generation", "abstract": "Visual programming languages (VPLs) allow users to create programs through\ngraphical interfaces, which results in easier accessibility and their\nwidespread usage in various domains. To further enhance this accessibility,\nrecent research has focused on generating VPL code from user instructions using\nlarge language models (LLMs). Specifically, by employing prompting-based\nmethods, these studies have shown promising results. Nevertheless, such\napproaches can be less effective for industrial VPLs such as Ladder Diagram\n(LD). LD is a pivotal language used in industrial automation processes and\ninvolves extensive domain-specific configurations, which are difficult to\ncapture in a single prompt. In this work, we demonstrate that training-based\nmethods outperform prompting-based methods for LD generation accuracy, even\nwith smaller backbone models. Building on these findings, we propose a\ntwo-stage training strategy to further enhance VPL generation. First, we employ\nretrieval-augmented fine-tuning to leverage the repetitive use of subroutines\ncommonly seen in industrial VPLs. Second, we apply direct preference\noptimization (DPO) to further guide the model toward accurate outputs, using\nsystematically generated preference pairs through graph editing operations.\nExtensive experiments on real-world LD data demonstrate that our approach\nimproves program-level accuracy by over 10% compared to supervised fine-tuning,\nwhich highlights its potential to advance industrial automation.", "published": "2025-02-23 10:27:44", "link": "http://arxiv.org/abs/2502.16529v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning about Affordances: Causal and Compositional Reasoning in LLMs", "abstract": "With the rapid progress of Large Language Models (LLMs), it becomes\nincreasingly important to understand their abilities and limitations. In two\nexperiments, we investigate the causal and compositional reasoning abilities of\nLLMs and humans in the domain of object affordances, an area traditionally\nlinked to embodied cognition. The tasks, designed from scratch to avoid data\ncontamination, require decision-makers to select unconventional objects to\nreplace a typical tool for a particular purpose, such as using a table tennis\nracket to dig a hole. In Experiment 1, we evaluated GPT-3.5 and GPT-4o, finding\nthat GPT-4o, when given chain-of-thought prompting, performed on par with human\nparticipants, while GPT-3.5 lagged significantly. In Experiment 2, we\nintroduced two new conditions, Distractor (more object choices, increasing\ndifficulty) and Image (object options presented visually), and evaluated Claude\n3 Sonnet and Claude 3.5 Sonnet in addition to the GPT models. The Distractor\ncondition significantly impaired performance across humans and models, although\nGPT-4o and Claude 3.5 still performed well above chance. Surprisingly, the\nImage condition had little impact on humans or GPT-4o, but significantly\nlowered Claude 3.5's accuracy. Qualitative analysis showed that GPT-4o and\nClaude 3.5 have a stronger ability than their predecessors to identify and\nflexibly apply causally relevant object properties. The improvement from\nGPT-3.5 and Claude 3 to GPT-4o and Claude 3.5 suggests that models are\nincreasingly capable of causal and compositional reasoning in some domains,\nalthough further mechanistic research is necessary to understand how LLMs\nreason.", "published": "2025-02-23 15:21:47", "link": "http://arxiv.org/abs/2502.16606v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MemeIntel: Explainable Detection of Propagandistic and Hateful Memes", "abstract": "The proliferation of multimodal content on social media presents significant\nchallenges in understanding and moderating complex, context-dependent issues\nsuch as misinformation, hate speech, and propaganda. While efforts have been\nmade to develop resources and propose new methods for automatic detection,\nlimited attention has been given to label detection and the generation of\nexplanation-based rationales for predicted labels. To address this challenge,\nwe introduce MemeIntel, an explanation-enhanced dataset for propaganda memes in\nArabic and hateful memes in English, making it the first large-scale resource\nfor these tasks. To solve these tasks, we propose a multi-stage optimization\napproach and train Vision-Language Models (VLMs). Our results demonstrate that\nthis approach significantly improves performance over the base model for both\n\\textbf{label detection} and explanation generation, outperforming the current\nstate-of-the-art with an absolute improvement of ~3% on ArMeme and ~7% on\nHateful Memes. For reproducibility and future research, we aim to make the\nMemeIntel dataset and experimental resources publicly available.", "published": "2025-02-23 15:35:48", "link": "http://arxiv.org/abs/2502.16612v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation\n  for Visual Knowledge Intensive Queries", "abstract": "Retrieval-Augmented Generation (RAG) is a popular approach for enhancing\nLarge Language Models (LLMs) by addressing their limitations in verifying facts\nand answering knowledge-intensive questions. As the research in LLM extends\ntheir capability to handle input modality other than text, e.g. image, several\nmultimodal RAG benchmarks are proposed. Nonetheless, they mainly use textual\nknowledge bases as the primary source of evidences for augmentation. There\nstill lack benchmarks designed to evaluate images as augmentation in RAG\nsystems and how they leverage visual knowledge. We propose Visual-RAG, a novel\nQuestion Answering benchmark that emphasizes visual knowledge intensive\nquestions. Unlike prior works relying on text-based evidence, Visual-RAG\nnecessitates text-to-image retrieval and integration of relevant clue images to\nextract visual knowledge as evidence. With Visual-RAG, we evaluate 5\nopen-sourced and 3 proprietary Multimodal LLMs (MLLMs), revealing that images\ncan serve as good evidence in RAG; however, even the SoTA models struggle with\neffectively extracting and utilizing visual knowledge", "published": "2025-02-23 16:23:50", "link": "http://arxiv.org/abs/2502.16636v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Automatic Input Rewriting Improves Translation with Large Language\n  Models", "abstract": "Can we improve machine translation (MT) with LLMs by rewriting their inputs\nautomatically? Users commonly rely on the intuition that well-written text is\neasier to translate when using off-the-shelf MT systems. LLMs can rewrite text\nin many ways but in the context of MT, these capabilities have been primarily\nexploited to rewrite outputs via post-editing. We present an empirical study of\n21 input rewriting methods with 3 open-weight LLMs for translating from English\ninto 6 target languages. We show that text simplification is the most effective\nMT-agnostic rewrite strategy and that it can be improved further when using\nquality estimation to assess translatability. Human evaluation further confirms\nthat simplified rewrites and their MT outputs both largely preserve the\noriginal meaning of the source and MT. These results suggest LLM-assisted input\nrewriting as a promising direction for improving translations.", "published": "2025-02-23 18:56:56", "link": "http://arxiv.org/abs/2502.16682v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncovering the Hidden Threat of Text Watermarking from Users with\n  Cross-Lingual Knowledge", "abstract": "In this study, we delve into the hidden threats posed to text watermarking by\nusers with cross-lingual knowledge. While most research focuses on watermarking\nmethods for English, there is a significant gap in evaluating these methods in\ncross-lingual contexts. This oversight neglects critical adversary scenarios\ninvolving cross-lingual users, creating uncertainty regarding the effectiveness\nof cross-lingual watermarking. We assess four watermarking techniques across\nfour linguistically rich languages, examining watermark resilience and text\nquality across various parameters and attacks. Our focus is on a realistic\nscenario featuring adversaries with cross-lingual expertise, evaluating the\nadequacy of current watermarking methods against such challenges.", "published": "2025-02-23 19:49:01", "link": "http://arxiv.org/abs/2502.16699v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Code Summarization Beyond Function Level", "abstract": "Code summarization is a critical task in natural language processing and\nsoftware engineering, which aims to generate concise descriptions of source\ncode. Recent advancements have improved the quality of these summaries,\nenhancing code readability and maintainability. However, the content of a\nrepository or a class has not been considered in function code summarization.\nThis study investigated the effectiveness of code summarization models beyond\nthe function level, exploring the impact of class and repository contexts on\nthe summary quality. The study involved revising benchmarks for evaluating\nmodels at class and repository levels, assessing baseline models, and\nevaluating LLMs with in-context learning to determine the enhancement of\nsummary quality with additional context. The findings revealed that the\nfine-tuned state-of-the-art CodeT5+ base model excelled in code summarization,\nwhile incorporating few-shot learning and retrieved code chunks from RAG\nsignificantly enhanced the performance of LLMs in this task. Notably, the\nDeepseek Coder 1.3B and Starcoder2 15B models demonstrated substantial\nimprovements in metrics such as BLEURT, METEOR, and BLEU-4 at both class and\nrepository levels. Repository-level summarization exhibited promising potential\nbut necessitates significant computational resources and gains from the\ninclusion of structured context. Lastly, we employed the recent SIDE code\nsummarization metric in our evaluation. This study contributes to refining\nstrategies for prompt engineering, few-shot learning, and RAG, addressing gaps\nin benchmarks for code summarization at various levels. Finally, we publish all\nstudy details, code, datasets, and results of evaluation in the GitHub\nrepository available at\nhttps://github.com/kilimanj4r0/code-summarization-beyond-function-level.", "published": "2025-02-23 20:31:21", "link": "http://arxiv.org/abs/2502.16704v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT Learn to Count Letters?", "abstract": "Large language models (LLMs) struggle on simple tasks such as counting the\nnumber of occurrences of a letter in a word. In this paper, we investigate if\nChatGPT can learn to count letters and propose an efficient solution.", "published": "2025-02-23 20:31:22", "link": "http://arxiv.org/abs/2502.16705v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speed and Conversational Large Language Models: Not All Is About Tokens\n  per Second", "abstract": "The speed of open-weights large language models (LLMs) and its dependency on\nthe task at hand, when run on GPUs, is studied to present a comparative\nanalysis of the speed of the most popular open LLMs.", "published": "2025-02-23 21:28:55", "link": "http://arxiv.org/abs/2502.16721v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Layer-Wise Evolution of Representations in Fine-Tuned Transformers:\n  Insights from Sparse AutoEncoders", "abstract": "Fine-tuning pre-trained transformers is a powerful technique for enhancing\nthe performance of base models on specific tasks. From early applications in\nmodels like BERT to fine-tuning Large Language Models (LLMs), this approach has\nbeen instrumental in adapting general-purpose architectures for specialized\ndownstream tasks. Understanding the fine-tuning process is crucial for\nuncovering how transformers adapt to specific objectives, retain general\nrepresentations, and acquire task-specific features. This paper explores the\nunderlying mechanisms of fine-tuning, specifically in the BERT transformer, by\nanalyzing activation similarity, training Sparse AutoEncoders (SAEs), and\nvisualizing token-level activations across different layers. Based on\nexperiments conducted across multiple datasets and BERT layers, we observe a\nsteady progression in how features adapt to the task at hand: early layers\nprimarily retain general representations, middle layers act as a transition\nbetween general and task-specific features, and later layers fully specialize\nin task adaptation. These findings provide key insights into the inner workings\nof fine-tuning and its impact on representation learning within transformer\narchitectures.", "published": "2025-02-23 21:29:50", "link": "http://arxiv.org/abs/2502.16722v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in\n  Data Science", "abstract": "Large Language Models (LLMs) have demonstrated potential for data science\ntasks via code generation. However, the exploratory nature of data science,\nalongside the stochastic and opaque outputs of LLMs, raise concerns about their\nreliability. While prior work focuses on benchmarking LLM accuracy,\nreproducibility remains underexplored, despite being critical to establishing\ntrust in LLM-driven analysis.\n  We propose a novel analyst-inspector framework to automatically evaluate and\nenforce the reproducibility of LLM-generated data science workflows - the first\nrigorous approach to the best of our knowledge. Defining reproducibility as the\nsufficiency and completeness of workflows for reproducing functionally\nequivalent code, this framework enforces computational reproducibility\nprinciples, ensuring transparent, well-documented LLM workflows while\nminimizing reliance on implicit model assumptions.\n  Using this framework, we systematically evaluate five state-of-the-art LLMs\non 1,032 data analysis tasks across three diverse benchmark datasets. We also\nintroduce two novel reproducibility-enhancing prompting strategies. Our results\nshow that higher reproducibility strongly correlates with improved accuracy and\nreproducibility-enhancing prompts are effective, demonstrating structured\nprompting's potential to enhance automated data science workflows and enable\ntransparent, robust AI-driven analysis. Our code is publicly available.", "published": "2025-02-23 01:15:50", "link": "http://arxiv.org/abs/2502.16395v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Ensemble ToT of LLMs and Its Application to Automatic Grading System for\n  Supporting Self-Learning", "abstract": "Providing students with detailed and timely grading feedback is essential for\nself-learning. While existing LLM-based grading systems are promising, most of\nthem rely on one single model, which limits their performance. To address this,\nwe propose Ensemble Tree-of-Thought (ToT), a framework that enhances LLM\noutputs by integrating multiple models. Using this framework, we develop a\ngrading system. Ensemble ToT follows three steps: (1) analyzing LLM\nperformance, (2) generating candidate answers, and (3) refining them into a\nfinal result. Based on this, our grading system first evaluates the grading\ntendencies of LLMs, then generates multiple results, and finally integrates\nthem via a simulated debate. Experimental results demonstrate our approach's\nability to provide accurate and explainable grading by effectively coordinating\nmultiple LLMs.", "published": "2025-02-23 01:17:46", "link": "http://arxiv.org/abs/2502.16399v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; K.3.1; K.3.2"], "primary_category": "cs.IR"}
{"title": "Sequence-level Large Language Model Training with Contrastive Preference\n  Optimization", "abstract": "The next token prediction loss is the dominant self-supervised training\nobjective for large language models and has achieved promising results in a\nvariety of downstream tasks. However, upon closer investigation of this\nobjective, we find that it lacks an understanding of sequence-level signals,\nleading to a mismatch between training and inference processes. To bridge this\ngap, we introduce a contrastive preference optimization (CPO) procedure that\ncan inject sequence-level information into the language model at any training\nstage without expensive human labeled data. Our experiments show that the\nproposed objective surpasses the next token prediction in terms of win rate in\nthe instruction-following and text generation tasks.", "published": "2025-02-23 04:13:27", "link": "http://arxiv.org/abs/2502.16433v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FanChuan: A Multilingual and Graph-Structured Benchmark For Parody\n  Detection and Analysis", "abstract": "Parody is an emerging phenomenon on social media, where individuals imitate a\nrole or position opposite to their own, often for humor, provocation, or\ncontroversy. Detecting and analyzing parody can be challenging and is often\nreliant on context, yet it plays a crucial role in understanding cultural\nvalues, promoting subcultures, and enhancing self-expression. However, the\nstudy of parody is hindered by limited available data and deficient diversity\nin current datasets. To bridge this gap, we built seven parody datasets from\nboth English and Chinese corpora, with 14,755 annotated users and 21,210\nannotated comments in total. To provide sufficient context information, we also\ncollect replies and construct user-interaction graphs to provide richer\ncontextual information, which is lacking in existing datasets. With these\ndatasets, we test traditional methods and Large Language Models (LLMs) on three\nkey tasks: (1) parody detection, (2) comment sentiment analysis with parody,\nand (3) user sentiment analysis with parody. Our extensive experiments reveal\nthat parody-related tasks still remain challenging for all models, and\ncontextual information plays a critical role. Interestingly, we find that, in\ncertain scenarios, traditional sentence embedding methods combined with simple\nclassifiers can outperform advanced LLMs, i.e. DeepSeek-R1 and GPT-o3,\nhighlighting parody as a significant challenge for LLMs.", "published": "2025-02-23 08:52:46", "link": "http://arxiv.org/abs/2502.16503v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual\n  Capabilities and Cultural Alignment in LLMs", "abstract": "Large Language Models (LLMs) are becoming increasingly capable across global\nlanguages. However, the ability to communicate across languages does not\nnecessarily translate to appropriate cultural representations. A key concern is\nUS-centric bias, where LLMs reflect US rather than local cultural values. We\npropose a novel methodology that compares LLM-generated response distributions\nagainst population-level opinion data from the World Value Survey across four\nlanguages (Danish, Dutch, English, and Portuguese). Using a rigorous linear\nmixed-effects regression framework, we compare two families of models: Google's\nGemma models (2B--27B parameters) and successive iterations of OpenAI's\nturbo-series. Across the families of models, we find no consistent\nrelationships between language capabilities and cultural alignment. While the\nGemma models have a positive correlation between language capability and\ncultural alignment across languages, the OpenAI models do not. Importantly, we\nfind that self-consistency is a stronger predictor of multicultural alignment\nthan multilingual capabilities. Our results demonstrate that achieving\nmeaningful cultural alignment requires dedicated effort beyond improving\ngeneral language capabilities.", "published": "2025-02-23 11:02:41", "link": "http://arxiv.org/abs/2502.16534v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Beyond Words: How Large Language Models Perform in Quantitative\n  Management Problem-Solving", "abstract": "This study examines how Large Language Models (LLMs) perform when tackling\nquantitative management decision problems in a zero-shot setting. Drawing on\n900 responses generated by five leading models across 20 diverse managerial\nscenarios, our analysis explores whether these base models can deliver accurate\nnumerical decisions under varying presentation formats, scenario complexities,\nand repeated attempts. Contrary to prior findings, we observed no significant\neffects of text presentation format (direct, narrative, or tabular) or text\nlength on accuracy. However, scenario complexity -- particularly in terms of\nconstraints and irrelevant parameters -- strongly influenced performance, often\ndegrading accuracy. Surprisingly, the models handled tasks requiring multiple\nsolution steps more effectively than expected. Notably, only 28.8\\% of\nresponses were exactly correct, highlighting limitations in precision. We\nfurther found no significant ``learning effect'' across iterations: performance\nremained stable across repeated queries. Nonetheless, significant variations\nemerged among the five tested LLMs, with some showing superior binary accuracy.\nOverall, these findings underscore both the promise and the pitfalls of\nharnessing LLMs for complex quantitative decision-making, informing managers\nand researchers about optimal deployment strategies.", "published": "2025-02-23 12:39:39", "link": "http://arxiv.org/abs/2502.16556v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Analysis of Emotion in Rumour Threads on Social Media", "abstract": "Rumours in online social media pose significant risks to modern society,\nmotivating the need for better understanding of how they develop. We focus\nspecifically on the interface between emotion and rumours in threaded\ndiscourses, building on the surprisingly sparse literature on the topic which\nhas largely focused on emotions within the original rumour posts themselves,\nand largely overlooked the comparative differences between rumours and\nnon-rumours. In this work, we provide a comprehensive analytical emotion\nframework, contrasting rumour and non-rumour cases using existing NLP datasets\nto further understand the emotion dynamics within rumours. Our framework\nreveals several findings: rumours exhibit more negative sentiment and emotions,\nincluding anger, fear and pessimism, while non-rumours evoke more positive\nemotions; emotions are contagious in online interactions, with rumours\nfacilitate negative emotions and non-rumours foster positive emotions; and\nbased on causal analysis, surprise acts as a bridge between rumours and other\nemotions, pessimism is driven by sadness and fear, optimism by joy and love.", "published": "2025-02-23 12:57:40", "link": "http://arxiv.org/abs/2502.16560v1", "categories": ["cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.AI"}
{"title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity\n  Tradeoff in Adaptive Multi-Agent Systems", "abstract": "Consensus formation is pivotal in multi-agent systems (MAS), balancing\ncollective coherence with individual diversity. Conventional LLM-based MAS\nprimarily rely on explicit coordination, e.g., prompts or voting, risking\npremature homogenization. We argue that implicit consensus, where agents\nexchange information yet independently form decisions via in-context learning,\ncan be more effective in dynamic environments that require long-horizon\nadaptability. By retaining partial diversity, systems can better explore novel\nstrategies and cope with external shocks. We formalize a consensus-diversity\ntradeoff, showing conditions where implicit methods outperform explicit ones.\nExperiments on three scenarios -- Dynamic Disaster Response, Information Spread\nand Manipulation, and Dynamic Public-Goods Provision -- confirm partial\ndeviation from group norms boosts exploration, robustness, and performance. We\nhighlight emergent coordination via in-context learning, underscoring the value\nof preserving diversity for resilient decision-making.", "published": "2025-02-23 13:12:53", "link": "http://arxiv.org/abs/2502.16565v1", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.MA"}
{"title": "Can Large Vision-Language Models Detect Images Copyright Infringement\n  from GenAI?", "abstract": "Generative AI models, renowned for their ability to synthesize high-quality\ncontent, have sparked growing concerns over the improper generation of\ncopyright-protected material. While recent studies have proposed various\napproaches to address copyright issues, the capability of large vision-language\nmodels (LVLMs) to detect copyright infringements remains largely unexplored. In\nthis work, we focus on evaluating the copyright detection abilities of\nstate-of-the-art LVLMs using a various set of image samples. Recognizing the\nabsence of a comprehensive dataset that includes both IP-infringement samples\nand ambiguous non-infringement negative samples, we construct a benchmark\ndataset comprising positive samples that violate the copyright protection of\nwell-known IP figures, as well as negative samples that resemble these figures\nbut do not raise copyright concerns. This dataset is created using advanced\nprompt engineering techniques. We then evaluate leading LVLMs using our\nbenchmark dataset. Our experimental results reveal that LVLMs are prone to\noverfitting, leading to the misclassification of some negative samples as\nIP-infringement cases. In the final section, we analyze these failure cases and\npropose potential solutions to mitigate the overfitting problem.", "published": "2025-02-23 15:41:12", "link": "http://arxiv.org/abs/2502.16618v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Retrieval-Augmented Visual Question Answering via Built-in\n  Autoregressive Search Engines", "abstract": "Retrieval-augmented generation (RAG) has emerged to address the\nknowledge-intensive visual question answering (VQA) task. Current methods\nmainly employ separate retrieval and generation modules to acquire external\nknowledge and generate answers, respectively. We propose ReAuSE, an alternative\nto the previous RAG model for the knowledge-based VQA task, which seamlessly\nintegrates knowledge retriever into the generative multi-modal large language\nmodel, serving as a built-in search engine. Specifically, our model functions\nboth as a generative retriever and an accurate answer generator. It not only\nhelps retrieve documents from the knowledge base by producing identifiers for\neach document, but it also answers visual questions based on the retrieved\ndocuments. Furthermore, we propose a reinforced retrieval calibration module\nfrom relevance feedback to improve retrieval performance and align with the\npreferences for accurate answer generation. Extensive experiments on two\nrepresentative OKVQA and A-OKVQA datasets demonstrate significant improvements\nranging from 2.9\\% to 9.6\\% across all evaluation metrics when compared to\nstrong baselines.", "published": "2025-02-23 16:39:39", "link": "http://arxiv.org/abs/2502.16641v1", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "CODESYNC: Synchronizing Large Language Models with Dynamic Code\n  Evolution at Scale", "abstract": "Large Language Models (LLMs) have exhibited exceptional performance in\nsoftware engineering yet face challenges in adapting to continually evolving\ncode knowledge, particularly regarding the frequent updates of third-party\nlibrary APIs. This limitation, stemming from static pre-training datasets,\noften results in non-executable code or implementations with suboptimal safety\nand efficiency. To this end, this paper introduces CODESYNC, a data engine for\nidentifying outdated code patterns and collecting real-time code knowledge\nupdates from Python third-party libraries. Building upon CODESYNC, we develop\nCODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay\nsynchronized with code evolution, which covers real-world updates for 220 APIs\nfrom six Python libraries. Our benchmark offers 3,300 test cases across three\nevaluation tasks and an update-aware instruction tuning dataset consisting of\n2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs\nreveal that they struggle with dynamic code evolution, even with the support of\nadvanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe\nthat our benchmark can offer a strong foundation for the development of more\neffective methods for real-time code knowledge updating in the future. The\nexperimental code and dataset are publicly available at:\nhttps://github.com/Lucky-voyage/Code-Sync.", "published": "2025-02-23 16:46:18", "link": "http://arxiv.org/abs/2502.16645v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Few-shot Continual Relation Extraction via Open Information Extraction", "abstract": "Typically, Few-shot Continual Relation Extraction (FCRE) models must balance\nretaining prior knowledge while adapting to new tasks with extremely limited\ndata. However, real-world scenarios may also involve unseen or undetermined\nrelations that existing methods still struggle to handle. To address these\nchallenges, we propose a novel approach that leverages the Open Information\nExtraction concept of Knowledge Graph Construction (KGC). Our method not only\nexposes models to all possible pairs of relations, including determined and\nundetermined labels not available in the training set, but also enriches model\nknowledge with diverse relation descriptions, thereby enhancing knowledge\nretention and adaptability in this challenging scenario. In the perspective of\nKGC, this is the first work explored in the setting of Continual Learning,\nallowing efficient expansion of the graph as the data evolves. Experimental\nresults demonstrate our superior performance compared to other state-of-the-art\nFCRE baselines, as well as the efficiency in handling dynamic graph\nconstruction in this setting.", "published": "2025-02-23 16:52:59", "link": "http://arxiv.org/abs/2502.16648v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "SBSC: Step-By-Step Coding for Improving Mathematical Olympiad\n  Performance", "abstract": "We propose Step-by-Step Coding (SBSC): a multi-turn math reasoning framework\nthat enables Large Language Models (LLMs) to generate sequence of programs for\nsolving Olympiad level math problems. At each step/turn, by leveraging the code\nexecution outputs and programs of previous steps, the model generates the next\nsub-task and the corresponding program to solve it. This way, SBSC,\nsequentially navigates to reach the final answer. SBSC allows more granular,\nflexible and precise approach to problem-solving compared to existing methods.\nExtensive experiments highlight the effectiveness of SBSC in tackling\ncompetition and Olympiad-level math problems. For Claude-3.5-Sonnet, we observe\nSBSC (greedy decoding) surpasses existing state-of-the-art (SOTA) program\ngeneration based reasoning strategies by absolute 10.7% on AMC12, 8% on AIME\nand 12.6% on MathOdyssey. Given SBSC is multi-turn in nature, we also benchmark\nSBSC's greedy decoding against self-consistency decoding results of existing\nSOTA math reasoning strategies and observe performance gain by absolute 6.2% on\nAMC, 6.7% on AIME and 7.4% on MathOdyssey.", "published": "2025-02-23 17:51:26", "link": "http://arxiv.org/abs/2502.16666v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models", "abstract": "Socially intelligent AI that can understand and interact seamlessly with\nhumans in daily lives is increasingly important as AI becomes more closely\nintegrated with peoples' daily activities. However, current works in artificial\nsocial reasoning all rely on language-only, or language-dominant approaches to\nbenchmark and training models, resulting in systems that are improving in\nverbal communication but struggle with nonverbal social understanding. To\naddress this limitation, we tap into a novel source of data rich in nonverbal\nand social interactions -- mime videos. Mimes refer to the art of expression\nthrough gesture and movement without spoken words, which presents unique\nchallenges and opportunities in interpreting non-verbal social communication.\nWe contribute a new dataset called MimeQA, obtained by sourcing 221 videos from\nYouTube, through rigorous annotation and verification, resulting in a benchmark\nwith 101 videos and 806 question-answer pairs. Using MimeQA, we evaluate\nstate-of-the-art video large language models (vLLMs) and find that their\noverall accuracy ranges from 15-30%. Our analysis reveals that vLLMs often fail\nto ground imagined objects and over-rely on the text prompt while ignoring\nsubtle nonverbal interactions. Our data resources are released at\nhttps://github.com/MIT-MI/MimeQA to inspire future work in foundation models\nthat embody true social intelligence capable of interpreting non-verbal human\ninteractions.", "published": "2025-02-23 18:05:49", "link": "http://arxiv.org/abs/2502.16671v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Toward Responsible Federated Large Language Models: Leveraging a Safety\n  Filter and Constitutional AI", "abstract": "Recent research has increasingly focused on training large language models\n(LLMs) using federated learning, known as FedLLM. However, responsible AI\n(RAI), which aims to ensure safe responses, remains underexplored in the\ncontext of FedLLM. In FedLLM, client data used for training may contain harmful\ncontent, leading to unsafe LLMs that generate harmful responses. Aggregating\nsuch unsafe LLMs into the global model and distributing them to clients may\nresult in the widespread deployment of unsafe LLMs. To address this issue, we\nincorporate two well-known RAI methods into FedLLM: the safety filter and\nconstitutional AI. Our experiments demonstrate that these methods significantly\nenhance the safety of the LLM, achieving over a 20% improvement on AdvBench, a\nbenchmark for evaluating safety performance.", "published": "2025-02-23 19:12:10", "link": "http://arxiv.org/abs/2502.16691v1", "categories": ["cs.CL", "cs.DC", "cs.MA"], "primary_category": "cs.CL"}
{"title": "DISC: Dynamic Decomposition Improves LLM Inference Scaling", "abstract": "Many inference scaling methods work by breaking a problem into smaller steps\n(or groups of tokens), then sampling and choosing the best next step. However,\nthese steps and their sizes are usually predetermined based on human intuition\nor domain knowledge. This paper introduces dynamic decomposition, a method that\nautomatically and adaptively splits solution and reasoning traces into steps\nduring inference. This approach improves computational efficiency by focusing\nmore resources on difficult steps, breaking them down further and prioritizing\ntheir sampling. Experiments on coding and math benchmarks (APPS, MATH, and\nLiveCodeBench) show that dynamic decomposition performs better than static\nmethods, which rely on fixed steps like token-level, sentence-level, or\nsingle-step decompositions. These results suggest that dynamic decomposition\ncan enhance many inference scaling techniques.", "published": "2025-02-23 20:37:32", "link": "http://arxiv.org/abs/2502.16706v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SE", "I.2.6; I.2.7; I.2.8; D.2.3; F.2.2"], "primary_category": "cs.LG"}
{"title": "SQLong: Enhanced NL2SQL for Longer Contexts with LLMs", "abstract": "Open-weight large language models (LLMs) have significantly advanced\nperformance in the Natural Language to SQL (NL2SQL) task. However, their\neffectiveness diminishes when dealing with large database schemas, as the\ncontext length increases. To address this limitation, we present SQLong, a\nnovel and efficient data augmentation framework designed to enhance LLM\nperformance in long-context scenarios for the NL2SQL task. SQLong generates\naugmented datasets by extending existing database schemas with additional\nsynthetic CREATE TABLE commands and corresponding data rows, sampled from\ndiverse schemas in the training data. This approach effectively simulates\nlong-context scenarios during finetuning and evaluation. Through experiments on\nthe Spider and BIRD datasets, we demonstrate that LLMs finetuned with\nSQLong-augmented data significantly outperform those trained on standard\ndatasets. These imply SQLong's practical implementation and its impact on\nimproving NL2SQL capabilities in real-world settings with complex database\nschemas.", "published": "2025-02-23 23:23:51", "link": "http://arxiv.org/abs/2502.16747v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Large Langauge Model Benchmarks against Data\n  Contamination: From Static to Dynamic Evaluation", "abstract": "Data contamination has received increasing attention in the era of large\nlanguage models (LLMs) due to their reliance on vast Internet-derived training\ncorpora. To mitigate the risk of potential data contamination, LLM benchmarking\nhas undergone a transformation from static to dynamic benchmarking. In this\nwork, we conduct an in-depth analysis of existing static to dynamic\nbenchmarking methods aimed at reducing data contamination risks. We first\nexamine methods that enhance static benchmarks and identify their inherent\nlimitations. We then highlight a critical gap-the lack of standardized criteria\nfor evaluating dynamic benchmarks. Based on this observation, we propose a\nseries of optimal design principles for dynamic benchmarking and analyze the\nlimitations of existing dynamic benchmarks. This survey provides a concise yet\ncomprehensive overview of recent advancements in data contamination research,\noffering valuable insights and a clear guide for future research efforts. We\nmaintain a GitHub repository to continuously collect both static and dynamic\nbenchmarking methods for LLMs. The repository can be found at this link.", "published": "2025-02-23 08:18:37", "link": "http://arxiv.org/abs/2502.17521v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI\n  Model Safety", "abstract": "Novel research aimed at text-to-image (T2I) generative AI safety often relies\non publicly available datasets for training and evaluation, making the quality\nand composition of these datasets crucial. This paper presents a comprehensive\nreview of the key datasets used in the T2I research, detailing their collection\nmethods, compositions, semantic and syntactic diversity of prompts and the\nquality, coverage, and distribution of harm types in the datasets. By\nhighlighting the strengths and limitations of the datasets, this study enables\nresearchers to find the most relevant datasets for a use case, critically\nassess the downstream impacts of their work given the dataset distribution,\nparticularly regarding model safety and ethical considerations, and also\nidentify the gaps in dataset coverage and quality that future research may\naddress.", "published": "2025-02-23 00:59:04", "link": "http://arxiv.org/abs/2503.00020v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Optimizing Retrieval-Augmented Generation of Medical Content for Spaced\n  Repetition Learning", "abstract": "Advances in Large Language Models revolutionized medical education by\nenabling scalable and efficient learning solutions. This paper presents a\npipeline employing Retrieval-Augmented Generation (RAG) system to prepare\ncomments generation for Poland's State Specialization Examination (PES) based\non verified resources. The system integrates these generated comments and\nsource documents with a spaced repetition learning algorithm to enhance\nknowledge retention while minimizing cognitive overload. By employing a refined\nretrieval system, query rephraser, and an advanced reranker, our modified RAG\nsolution promotes accuracy more than efficiency. Rigorous evaluation by medical\nannotators demonstrates improvements in key metrics such as document relevance,\ncredibility, and logical coherence of generated content, proven by a series of\nexperiments presented in the paper. This study highlights the potential of RAG\nsystems to provide scalable, high-quality, and individualized educational\nresources, addressing non-English speaking users.", "published": "2025-02-23 20:56:31", "link": "http://arxiv.org/abs/2503.01859v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MV-CLAM: Multi-View Molecular Interpretation with Cross-Modal Projection\n  via Language Model", "abstract": "Human expertise in chemistry and biomedicine relies on contextual molecular\nunderstanding, a capability that large language models (LLMs) can extend\nthrough fine-grained alignment between molecular structures and text. Recent\nmultimodal learning advances focus on cross-modal alignment, but existing\nmolecule-text models ignore complementary information in different molecular\nviews and rely on single-view representations, limiting molecular\nunderstanding. Moreover, na\\\"ive multi-view alignment strategies face two\nchallenges: (1) separate aligned spaces with inconsistent mappings between\nmolecule and text embeddings, and that (2) existing loss objectives fail to\npreserve complementary information for fine-grained alignment. This can limit\nthe LLM's ability to fully understand the molecular properties. To address\nthese issues, we propose MV-CLAM, a novel framework that aligns multi-view\nmolecular representations into a unified textual space using a multi-query\ntransformer (MQ-Former). Our approach ensures cross-view consistency while a\ntoken-level contrastive loss preserves diverse molecular features across\ntextual queries. MV-CLAM enhances molecular reasoning, improving retrieval and\ncaptioning accuracy. The source code of MV-CLAM is available in\nhttps://github.com/sumin124/mv-clam.git.", "published": "2025-02-23 14:38:29", "link": "http://arxiv.org/abs/2503.04780v1", "categories": ["cs.CL", "cs.AI", "physics.atom-ph"], "primary_category": "cs.CL"}
{"title": "Advanced Chain-of-Thought Reasoning for Parameter Extraction from\n  Documents Using Large Language Models", "abstract": "Extracting parameters from technical documentation is crucial for ensuring\ndesign precision and simulation reliability in electronic design. However,\ncurrent methods struggle to handle high-dimensional design data and meet the\ndemands of real-time processing. In electronic design automation (EDA),\nengineers often manually search through extensive documents to retrieve\ncomponent parameters required for constructing PySpice models, a process that\nis both labor-intensive and time-consuming. To address this challenge, we\npropose an innovative framework that leverages large language models (LLMs) to\nautomate the extraction of parameters and the generation of PySpice models\ndirectly from datasheets. Our framework introduces three Chain-of-Thought (CoT)\nbased techniques: (1) Targeted Document Retrieval (TDR), which enables the\nrapid identification of relevant technical sections; (2) Iterative Retrieval\nOptimization (IRO), which refines the parameter search through iterative\nimprovements; and (3) Preference Optimization (PO), which dynamically\nprioritizes key document sections based on relevance. Experimental results show\nthat applying all three methods together improves retrieval precision by 47.69%\nand reduces processing latency by 37.84%. Furthermore, effect size analysis\nusing Cohen's d reveals that PO significantly reduces latency, while IRO\ncontributes most to precision enhancement. These findings underscore the\npotential of our framework to streamline EDA processes, enhance design\naccuracy, and shorten development timelines. Additionally, our algorithm has\nmodel-agnostic generalization, meaning it can improve parameter search\nperformance across different LLMs.", "published": "2025-02-23 11:19:44", "link": "http://arxiv.org/abs/2502.16540v1", "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio-FLAN: A Preliminary Release", "abstract": "Recent advancements in audio tokenization have significantly enhanced the\nintegration of audio capabilities into large language models (LLMs). However,\naudio understanding and generation are often treated as distinct tasks,\nhindering the development of truly unified audio-language models. While\ninstruction tuning has demonstrated remarkable success in improving\ngeneralization and zero-shot learning across text and vision, its application\nto audio remains largely unexplored. A major obstacle is the lack of\ncomprehensive datasets that unify audio understanding and generation. To\naddress this, we introduce Audio-FLAN, a large-scale instruction-tuning dataset\ncovering 80 diverse tasks across speech, music, and sound domains, with over\n100 million instances. Audio-FLAN lays the foundation for unified\naudio-language models that can seamlessly handle both understanding (e.g.,\ntranscription, comprehension) and generation (e.g., speech, music, sound) tasks\nacross a wide range of audio domains in a zero-shot manner. The Audio-FLAN\ndataset is available on HuggingFace and GitHub and will be continuously\nupdated.", "published": "2025-02-23 14:24:15", "link": "http://arxiv.org/abs/2502.16584v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Target Speaker Extraction through Comparing Noisy Positive and Negative\n  Audio Enrollments", "abstract": "Target speaker extraction focuses on isolating a specific speaker's voice\nfrom an audio mixture containing multiple speakers. To provide information\nabout the target speaker's identity, prior works have utilized clean audio\nexamples as conditioning inputs. However, such clean audio examples are not\nalways readily available (e.g. It is impractical to obtain a clean audio\nexample of a stranger's voice at a cocktail party without stepping away from\nthe noisy environment). Limited prior research has explored extracting the\ntarget speaker's characteristics from noisy audio examples, which may include\noverlapping speech from disturbing speakers. In this work, we focus on target\nspeaker extraction when multiple speakers are present during the enrollment\nstage, through leveraging differences between audio segments where the target\nspeakers are speaking (Positive Enrollments) and segments where they are not\n(Negative Enrollments). Experiments show the effectiveness of our model\narchitecture and the dedicated pretraining method for the proposed task. Our\nmethod achieves state-of-the-art performance in the proposed application\nsettings and demonstrates strong generalizability across challenging and\nrealistic scenarios.", "published": "2025-02-23 15:33:44", "link": "http://arxiv.org/abs/2502.16611v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
