{"title": "SUperman: Efficient Permanent Computation on GPUs", "abstract": "The permanent is a function, defined for a square matrix, with applications\nin various domains including quantum computing, statistical physics, complexity\ntheory, combinatorics, and graph theory. Its formula is similar to that of the\ndeterminant, however unlike the determinant, its exact computation is\n#P-complete, i.e., there is no algorithm to compute the permanent in polynomial\ntime unless P=NP. For an $n \\times n$ matrix, the fastest algorithm has a time\ncomplexity of $O(2^{n-1}n)$. Although supercomputers have been employed for\npermanent computation before, there is no work and more importantly, no\npublicly available software that leverages cutting-edge, yet widely accessible,\nHigh-Performance Computing accelerators such as GPUs. In this work, we\ndesigned, developed, and investigated the performance of SUperman, a complete\nsoftware suite that can compute matrix permanents on multiple nodes/GPUs on a\ncluster while handling various matrix types, e.g., real/complex/binary and\nsparse/dense etc., with a unique treatment for each type. Compared to a\nstate-of-the-art parallel algorithm on 44 cores, SUperman can be $86\\times$\nfaster on a single Nvidia A100 GPU. Combining multiple GPUs, we also showed\nthat SUperman can compute the permanent of a $56 \\times 56$ matrix which is the\nlargest reported in the literature.", "published": "2025-02-23 13:56:07", "link": "http://arxiv.org/abs/2502.16577v2", "categories": ["cs.DC", "cs.DM", "cs.NA", "math.NA"], "primary_category": "cs.DC"}
{"title": "Worst-case Error Bounds for Online Learning of Smooth Functions", "abstract": "Online learning is a model of machine learning where the learner is trained\non sequential feedback. We investigate worst-case error for the online learning\nof real functions that have certain smoothness constraints. Suppose that\n$\\mathcal{F}_q$ is the class of all absolutely continuous functions $f: [0, 1]\n\\rightarrow \\mathbb{R}$ such that $\\|f'\\|_q \\le 1$, and\n$\\operatorname{opt}_p(\\mathcal{F}_q)$ is the best possible upper bound on the\nsum of the $p^{\\text{th}}$ powers of absolute prediction errors for any number\nof trials guaranteed by any learner. We show that for any $\\delta, \\epsilon \\in\n(0, 1)$, $\\operatorname{opt}_{1+\\delta} (\\mathcal{F}_{1+\\epsilon}) =\nO(\\min(\\delta, \\epsilon)^{-1})$. Combined with the previous results of Kimber\nand Long (1995) and Geneson and Zhou (2023), we achieve a complete\ncharacterization of the values of $p, q \\ge 1$ that result in\n$\\operatorname{opt}_p(\\mathcal{F}_q)$ being finite, a problem open for nearly\n30 years.\n  We study the learning scenarios of smooth functions that also belong to\ncertain special families of functions, such as polynomials. We prove a\nconjecture by Geneson and Zhou (2023) that it is not any easier to learn a\npolynomial in $\\mathcal{F}_q$ than it is to learn any general function in\n$\\mathcal{F}_q$. We also define a noisy model for the online learning of smooth\nfunctions, where the learner may receive incorrect feedback up to $\\eta \\ge 1$\ntimes, denoting the worst-case error bound as\n$\\operatorname{opt}^{\\text{nf}}_{p, \\eta} (\\mathcal{F}_q)$. We prove that\n$\\operatorname{opt}^{\\text{nf}}_{p, \\eta} (\\mathcal{F}_q)$ is finite if and\nonly if $\\operatorname{opt}_p(\\mathcal{F}_q)$ is. Moreover, we prove for all\n$p, q \\ge 2$ and $\\eta \\ge 1$ that $\\operatorname{opt}^{\\text{nf}}_{p, \\eta}\n(\\mathcal{F}_q) = \\Theta (\\eta)$.", "published": "2025-02-23 00:43:10", "link": "http://arxiv.org/abs/2502.16388v1", "categories": ["cs.LG", "cs.DM", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs in Trading Strategies", "abstract": "This paper presents a comprehensive study on the use of ensemble\nReinforcement Learning (RL) models in financial trading strategies, leveraging\nclassifier models to enhance performance. By combining RL algorithms such as\nA2C, PPO, and SAC with traditional classifiers like Support Vector Machines\n(SVM), Decision Trees, and Logistic Regression, we investigate how different\nclassifier groups can be integrated to improve risk-return trade-offs. The\nstudy evaluates the effectiveness of various ensemble methods, comparing them\nwith individual RL models across key financial metrics, including Cumulative\nReturns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our\nresults demonstrate that ensemble methods consistently outperform base models\nin terms of risk-adjusted returns, providing better management of drawdowns and\noverall stability. However, we identify the sensitivity of ensemble performance\nto the choice of variance threshold {\\tau}, highlighting the importance of\ndynamic {\\tau} adjustment to achieve optimal performance. This study emphasizes\nthe value of combining RL with classifiers for adaptive decision-making, with\nimplications for financial trading, robotics, and other dynamic environments.", "published": "2025-02-23 04:18:05", "link": "http://arxiv.org/abs/2502.17518v1", "categories": ["cs.LG", "cs.AI", "q-fin.CP", "stat.ML", "68T42"], "primary_category": "cs.LG"}
