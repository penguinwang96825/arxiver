{"title": "Big Data Small Data, In Domain Out-of Domain, Known Word Unknown Word:\n  The Impact of Word Representation on Sequence Labelling Tasks", "abstract": "Word embeddings -- distributed word representations that can be learned from\nunlabelled data -- have been shown to have high utility in many natural\nlanguage processing applications. In this paper, we perform an extrinsic\nevaluation of five popular word embedding methods in the context of four\nsequence labelling tasks: POS-tagging, syntactic chunking, NER and MWE\nidentification. A particular focus of the paper is analysing the effects of\ntask-based updating of word representations. We show that when using word\nembeddings as features, as few as several hundred training instances are\nsufficient to achieve competitive results, and that word embeddings lead to\nimprovements over OOV words and out of domain. Perhaps more surprisingly, our\nresults indicate there is little difference between the different word\nembedding methods, and that simple Brown clusters are often competitive with\nword embeddings across all tasks we consider.", "published": "2015-04-21 06:58:26", "link": "http://arxiv.org/abs/1504.05319v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
