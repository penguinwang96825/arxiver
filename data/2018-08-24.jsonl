{"title": "Features of word similarity", "abstract": "In this theoretical note we compare different types of computational models\nof word similarity and association in their ability to predict a set of about\n900 rating data. Using regression and predictive modeling tools (neural net,\ndecision tree) the performance of a total of 28 models using different\ncombinations of both surface and semantic word features is evaluated. The\nresults present evidence for the hypothesis that word similarity ratings are\nbased on more than only semantic relatedness. The limited cross-validated\nperformance of the models asks for the development of psychological process\nmodels of the word similarity rating task.", "published": "2018-08-24 03:46:45", "link": "http://arxiv.org/abs/1808.07999v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Role Semantics for Better Models of Implicit Discourse Relations", "abstract": "Predicting the structure of a discourse is challenging because relations\nbetween discourse segments are often implicit and thus hard to distinguish\ncomputationally. I extend previous work to classify implicit discourse\nrelations by introducing a novel set of features on the level of semantic\nroles. My results demonstrate that such features are helpful, yielding results\ncompetitive with other feature-rich approaches on the PDTB. My main\ncontribution is an analysis of improvements that can be traced back to\nrole-based features, providing insights into why and when role semantics is\nhelpful.", "published": "2018-08-24 08:32:38", "link": "http://arxiv.org/abs/1808.08047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring LDA Topic Stability from Clusters of Replicated Runs", "abstract": "Background: Unstructured and textual data is increasing rapidly and Latent\nDirichlet Allocation (LDA) topic modeling is a popular data analysis methods\nfor it. Past work suggests that instability of LDA topics may lead to\nsystematic errors. Aim: We propose a method that relies on replicated LDA runs,\nclustering, and providing a stability metric for the topics. Method: We\ngenerate k LDA topics and replicate this process n times resulting in n*k\ntopics. Then we use K-medioids to cluster the n*k topics to k clusters. The k\nclusters now represent the original LDA topics and we present them like normal\nLDA topics showing the ten most probable words. For the clusters, we try\nmultiple stability metrics, out of which we recommend Rank-Biased Overlap,\nshowing the stability of the topics inside the clusters. Results: We provide an\ninitial validation where our method is used for 270,000 Mozilla Firefox commit\nmessages with k=20 and n=20. We show how our topic stability metrics are\nrelated to the contents of the topics. Conclusions: Advances in text mining\nenable us to analyze large masses of text in software engineering but\nnon-deterministic algorithms, such as LDA, may lead to unreplicable\nconclusions. Our approach makes LDA stability transparent and is also\ncomplementary rather than alternative to many prior works that focus on LDA\nparameter tuning.", "published": "2018-08-24 11:37:40", "link": "http://arxiv.org/abs/1808.08098v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Visual Attention Grounding Neural Model for Multimodal Machine\n  Translation", "abstract": "We introduce a novel multimodal machine translation model that utilizes\nparallel visual and textual information. Our model jointly optimizes the\nlearning of a shared visual-language embedding and a translator. The model\nleverages a visual attention grounding mechanism that links the visual\nsemantics with the corresponding textual semantics. Our approach achieves\ncompetitive state-of-the-art results on the Multi30K and the Ambiguous COCO\ndatasets. We also collected a new multilingual multimodal product description\ndataset to simulate a real-world international online shopping scenario. On\nthis dataset, our visual attention grounding model outperforms other methods by\na large margin.", "published": "2018-08-24 18:47:29", "link": "http://arxiv.org/abs/1808.08266v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Approximate Distribution Matching for Sequence-to-Sequence Learning", "abstract": "Sequence-to-Sequence models were introduced to tackle many real-life problems\nlike machine translation, summarization, image captioning, etc. The standard\noptimization algorithms are mainly based on example-to-example matching like\nmaximum likelihood estimation, which is known to suffer from data sparsity\nproblem. Here we present an alternate view to explain sequence-to-sequence\nlearning as a distribution matching problem, where each source or target\nexample is viewed to represent a local latent distribution in the source or\ntarget domain. Then, we interpret sequence-to-sequence learning as learning a\ntransductive model to transform the source local latent distributions to match\ntheir corresponding target distributions. In our framework, we approximate both\nthe source and target latent distributions with recurrent neural networks\n(augmenter). During training, the parallel augmenters learn to better\napproximate the local latent distributions, while the sequence prediction model\nlearns to minimize the KL-divergence of the transformed source distributions\nand the approximated target distributions. This algorithm can alleviate the\ndata sparsity issues in sequence learning by locally augmenting more unseen\ndata pairs and increasing the model's robustness. Experiments conducted on\nmachine translation and image captioning consistently demonstrate the\nsuperiority of our proposed algorithm over the other competing algorithms.", "published": "2018-08-24 05:00:17", "link": "http://arxiv.org/abs/1808.08003v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Under the Hood: Using Diagnostic Classifiers to Investigate and Improve\n  how Language Models Track Agreement Information", "abstract": "How do neural language models keep track of number agreement between subject\nand verb? We show that `diagnostic classifiers', trained to predict number from\nthe internal states of a language model, provide a detailed understanding of\nhow, when, and where this information is represented. Moreover, they give us\ninsight into when and where number information is corrupted in cases where the\nlanguage model ends up making agreement errors. To demonstrate the causal role\nplayed by the representations we find, we then use agreement information to\ninfluence the course of the LSTM during the processing of difficult sentences.\nResults from such an intervention reveal a large increase in the language\nmodel's accuracy. Together, these results show that diagnostic classifiers give\nus an unrivalled detailed look into the representation of linguistic\ninformation in neural models, and demonstrate that this knowledge can be used\nto improve their performance.", "published": "2018-08-24 10:29:45", "link": "http://arxiv.org/abs/1808.08079v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning End-to-End Goal-Oriented Dialog with Multiple Answers", "abstract": "In a dialog, there can be multiple valid next utterances at any point. The\npresent end-to-end neural methods for dialog do not take this into account.\nThey learn with the assumption that at any time there is only one correct next\nutterance. In this work, we focus on this problem in the goal-oriented dialog\nsetting where there are different paths to reach a goal. We propose a new\nmethod, that uses a combination of supervised learning and reinforcement\nlearning approaches to address this issue. We also propose a new and more\neffective testbed, permuted-bAbI dialog tasks, by introducing multiple valid\nnext utterances to the original-bAbI dialog tasks, which allows evaluation of\ngoal-oriented dialog systems in a more realistic setting. We show that there is\na significant drop in performance of existing end-to-end neural methods from\n81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on\npermuted-bAbI dialog tasks. We also show that our proposed method improves the\nperformance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog\ntasks.", "published": "2018-08-24 19:24:58", "link": "http://arxiv.org/abs/1808.09996v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Proximal Policy Optimization and its Dynamic Version for Sequence\n  Generation", "abstract": "In sequence generation task, many works use policy gradient for model\noptimization to tackle the intractable backpropagation issue when maximizing\nthe non-differentiable evaluation metrics or fooling the discriminator in\nadversarial learning. In this paper, we replace policy gradient with proximal\npolicy optimization (PPO), which is a proved more efficient reinforcement\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\nperformance.", "published": "2018-08-24 02:14:43", "link": "http://arxiv.org/abs/1808.07982v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "From Random to Supervised: A Novel Dropout Mechanism Integrated with\n  Global Information", "abstract": "Dropout is used to avoid overfitting by randomly dropping units from the\nneural networks during training. Inspired by dropout, this paper presents\nGI-Dropout, a novel dropout method integrating with global information to\nimprove neural networks for text classification. Unlike the traditional dropout\nmethod in which the units are dropped randomly according to the same\nprobability, we aim to use explicit instructions based on global information of\nthe dataset to guide the training process. With GI-Dropout, the model is\nsupposed to pay more attention to inapparent features or patterns. Experiments\ndemonstrate the effectiveness of the dropout with global information on seven\ntext classification tasks, including sentiment analysis and topic\nclassification.", "published": "2018-08-24 14:17:01", "link": "http://arxiv.org/abs/1808.08149v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Robust Text Classifier on Test-Time Budgets", "abstract": "We propose a generic and interpretable learning framework for building robust\ntext classification model that achieves accuracy comparable to full models\nunder test-time budget constraints. Our approach learns a selector to identify\nwords that are relevant to the prediction tasks and passes them to the\nclassifier for processing. The selector is trained jointly with the classifier\nand directly learns to incorporate with the classifier. We further propose a\ndata aggregation scheme to improve the robustness of the classifier. Our\nlearning framework is general and can be incorporated with any type of text\nclassification model. On real-world data, we show that the proposed approach\nimproves the performance of a given classifier and speeds up the model with a\nmere loss in accuracy performance.", "published": "2018-08-24 19:22:12", "link": "http://arxiv.org/abs/1808.08270v5", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Trio Neural Model for Dynamic Entity Relatedness Ranking", "abstract": "Measuring entity relatedness is a fundamental task for many natural language\nprocessing and information retrieval applications. Prior work often studies\nentity relatedness in static settings and an unsupervised manner. However,\nentities in real-world are often involved in many different relationships,\nconsequently entity-relations are very dynamic over time. In this work, we\npropose a neural networkbased approach for dynamic entity relatedness,\nleveraging the collective attention as supervision. Our model is capable of\nlearning rich and different entity representations in a joint framework.\nThrough extensive experiments on large-scale datasets, we demonstrate that our\nmethod achieves better results than competitive baselines.", "published": "2018-08-24 21:29:53", "link": "http://arxiv.org/abs/1808.08316v4", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Independent Low-Rank Matrix Analysis Based on Time-Variant Sub-Gaussian\n  Source Model", "abstract": "Independent low-rank matrix analysis (ILRMA) is a fast and stable method for\nblind audio source separation. Conventional ILRMAs assume time-variant\n(super-)Gaussian source models, which can only represent signals that follow a\nsuper-Gaussian distribution. In this paper, we focus on ILRMA based on a\ngeneralized Gaussian distribution (GGD-ILRMA) and propose a new type of\nGGD-ILRMA that adopts a time-variant sub-Gaussian distribution for the source\nmodel. By using a new update scheme called generalized iterative projection for\nhomogeneous source models, we obtain a convergence-guaranteed update rule for\ndemixing spatial parameters. In the experimental evaluation, we show the\nversatility of the proposed method, i.e., the proposed time-variant\nsub-Gaussian source model can be applied to various types of source signal.", "published": "2018-08-24 09:19:06", "link": "http://arxiv.org/abs/1808.08056v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Harnessing Infant Cry for swift, cost-effective Diagnosis of Perinatal\n  Asphyxia in low-resource settings", "abstract": "Perinatal Asphyxia is one of the top three causes of infant mortality in\ndeveloping countries, resulting to the death of about 1.2 million newborns\nevery year. At its early stages, the presence of asphyxia cannot be\nconclusively determined visually or via physical examination, but by medical\ndiagnosis. In resource-poor settings, where skilled attendance at birth is a\nluxury, most cases only get detected when the damaging consequences begin to\nmanifest or worse still, after death of the affected infant. In this project,\nwe explored the approach of machine learning in developing a low-cost\ndiagnostic solution. We designed a support vector machine-based pattern\nrecognition system that models patterns in the cries of known asphyxiating\ninfants (and normal infants) and then uses the developed model for\nclassification of `new' infants as having asphyxia or not. Our prototype has\nbeen tested in a laboratory setting to give prediction accuracy of up to\n88.85%. If higher accuracies can be obtained, this research may be a key\ncontributor to the 4th Millennium Development Goal (MDG) of reducing mortality\nin under-five children.", "published": "2018-08-24 20:25:01", "link": "http://arxiv.org/abs/1808.08299v1", "categories": ["stat.AP", "cs.SD", "eess.AS"], "primary_category": "stat.AP"}
{"title": "Voice Conversion with Conditional SampleRNN", "abstract": "Here we present a novel approach to conditioning the SampleRNN generative\nmodel for voice conversion (VC). Conventional methods for VC modify the\nperceived speaker identity by converting between source and target acoustic\nfeatures. Our approach focuses on preserving voice content and depends on the\ngenerative network to learn voice style. We first train a multi-speaker\nSampleRNN model conditioned on linguistic features, pitch contour, and speaker\nidentity using a multi-speaker speech corpus. Voice-converted speech is\ngenerated using linguistic features and pitch contour extracted from the source\nspeaker, and the target speaker identity. We demonstrate that our system is\ncapable of many-to-many voice conversion without requiring parallel data,\nenabling broad applications. Subjective evaluation demonstrates that our\napproach outperforms conventional VC methods.", "published": "2018-08-24 21:14:40", "link": "http://arxiv.org/abs/1808.08311v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
