{"title": "Path weighting sensitivities", "abstract": "In this paper, we study the computation of sensitivities with respect to spot\nof path dependent financial derivatives by means of path weighting. We propose\nexplicit path weighting formula and variance reduction adjustment in order to\naddress the large variance happening when the first simulation time step is\nsmall. We also propose a covariance inflation technique to addresses the\ndegenerator case when the covariance matrix is singular. The stock dynamics we\nconsider is given in a general functional form, which includes the classical\nBlack-Scholes model, the implied distribution model, and the local volatility\nmodel.", "published": "2024-11-20 15:43:47", "link": "http://arxiv.org/abs/2411.13403v1", "categories": ["math.PR", "q-fin.ST"], "primary_category": "math.PR"}
{"title": "A Deep Learning Approach to Predict the Fall [of Price] of Cryptocurrency Long Before its Actual Fall", "abstract": "In modern times, the cryptocurrency market is one of the world's most rapidly\nrising financial markets. The cryptocurrency market is regarded to be more\nvolatile and illiquid than traditional markets such as equities, foreign\nexchange, and commodities. The risk of this market creates an uncertain\ncondition among the investors. The purpose of this research is to predict the\nmagnitude of the risk factor of the cryptocurrency market. Risk factor is also\ncalled volatility. Our approach will assist people who invest in the\ncryptocurrency market by overcoming the problems and difficulties they\nexperience. Our approach starts with calculating the risk factor of the\ncryptocurrency market from the existing parameters. In twenty elements of the\ncryptocurrency market, the risk factor has been predicted using different\nmachine learning algorithms such as CNN, LSTM, BiLSTM, and GRU. All of the\nmodels have been applied to the calculated risk factor parameter. A new model\nhas been developed to predict better than the existing models. Our proposed\nmodel gives the highest RMSE value of 1.3229 and the lowest RMSE value of\n0.0089. Following our model, it will be easier for investors to trade in\ncomplicated and challenging financial assets like bitcoin, Ethereum, dogecoin,\netc. Where the other existing models, the highest RMSE was 14.5092, and the\nlower was 0.02769. So, the proposed model performs much better than models with\nproper generalization. Using our approach, it will be easier for investors to\ntrade in complicated and challenging financial assets like Bitcoin, Ethereum,\nand Dogecoin.", "published": "2024-11-20 08:09:35", "link": "http://arxiv.org/abs/2411.13615v3", "categories": ["q-fin.ST", "cs.CV", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Simulating Liquidity: Agent-Based Modeling of Illiquid Markets for Fractional Ownership", "abstract": "This research investigates liquidity dynamics in fractional ownership\nmarkets, focusing on illiquid alternative investments traded on a FinTech\nplatform. By leveraging empirical data and employing agent-based modeling\n(ABM), the study simulates trading behaviors in sell offer-driven systems,\nproviding a foundation for generating insights into how different market\nstructures influence liquidity. The ABM-based simulation model provides a data\naugmentation environment which allows for the exploration of diverse trading\narchitectures and rules, offering an alternative to direct experimentation.\nThis approach bridges academic theory and practical application, supported by\ncollaboration with industry and Swiss federal funding. The paper lays the\nfoundation for planned extensions, including the identification of a\nliquidity-maximizing trading environment and the design of a market maker, by\nsimulating the current functioning of the investment platform using an ABM\nspecified with empirical data.", "published": "2024-11-20 15:07:53", "link": "http://arxiv.org/abs/2411.13381v2", "categories": ["q-fin.TR", "econ.GN", "q-fin.EC"], "primary_category": "q-fin.TR"}
{"title": "MemoryFormer: Minimize Transformer Computation by Removing\n  Fully-Connected Layers", "abstract": "In order to reduce the computational complexity of large language models,\ngreat efforts have been made to to improve the efficiency of transformer models\nsuch as linear attention and flash-attention. However, the model size and\ncorresponding computational complexity are constantly scaled up in pursuit of\nhigher performance. In this work, we present MemoryFormer, a novel transformer\narchitecture which significantly reduces the computational complexity (FLOPs)\nfrom a new perspective. We eliminate nearly all the computations of the\ntransformer model except for the necessary computation required by the\nmulti-head attention operation. This is made possible by utilizing an\nalternative method for feature transformation to replace the linear projection\nof fully-connected layers. Specifically, we first construct a group of\nin-memory lookup tables that store a large amount of discrete vectors to\nreplace the weight matrix used in linear projection. We then use a hash\nalgorithm to retrieve a correlated subset of vectors dynamically based on the\ninput embedding. The retrieved vectors combined together will form the output\nembedding, which provides an estimation of the result of matrix multiplication\noperation in a fully-connected layer. Compared to conducting matrix\nmultiplication, retrieving data blocks from memory is a much cheaper operation\nwhich requires little computations. We train MemoryFormer from scratch and\nconduct extensive experiments on various benchmarks to demonstrate the\neffectiveness of the proposed model.", "published": "2024-11-20 02:41:53", "link": "http://arxiv.org/abs/2411.12992v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Patience Is The Key to Large Language Model Reasoning", "abstract": "Recent advancements in the field of large language models, particularly\nthrough the Chain of Thought (CoT) approach, have demonstrated significant\nimprovements in solving complex problems. However, existing models either tend\nto sacrifice detailed reasoning for brevity due to user preferences, or require\nextensive and expensive training data to learn complicated reasoning ability,\nlimiting their potential in solving complex tasks. To bridge this gap,\nfollowing the concept of scaling test-time, we propose a simple method by\nencouraging models to adopt a more patient reasoning style without the need of\nintroducing new knowledge or skills. To employ a preference optimization\napproach, we generate detailed reasoning processes as positive examples and\nsimple answers as negative examples, thereby training the model to favor\nthoroughness in its responses. Our results demonstrate a performance increase\nof up to 2.1% on GSM8k with training just on a lightweight dataset.", "published": "2024-11-20 07:20:48", "link": "http://arxiv.org/abs/2411.13082v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AIDBench: A benchmark for evaluating the authorship identification\n  capability of large language models", "abstract": "As large language models (LLMs) rapidly advance and integrate into daily\nlife, the privacy risks they pose are attracting increasing attention. We focus\non a specific privacy risk where LLMs may help identify the authorship of\nanonymous texts, which challenges the effectiveness of anonymity in real-world\nsystems such as anonymous peer review systems. To investigate these risks, we\npresent AIDBench, a new benchmark that incorporates several author\nidentification datasets, including emails, blogs, reviews, articles, and\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\nauthorship identification, which determines whether two texts are from the same\nauthor; and one-to-many authorship identification, which, given a query text\nand a list of candidate texts, identifies the candidate most likely written by\nthe same author as the query text. We also introduce a Retrieval-Augmented\nGeneration (RAG)-based method to enhance the large-scale authorship\nidentification capabilities of LLMs, particularly when input lengths exceed the\nmodels' context windows, thereby establishing a new baseline for authorship\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\ncan correctly guess authorship at rates well above random chance, revealing new\nprivacy risks posed by these powerful models. The source code and data will be\nmade publicly available after acceptance.", "published": "2024-11-20 11:41:08", "link": "http://arxiv.org/abs/2411.13226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BIPro: Zero-shot Chinese Poem Generation via Block Inverse Prompting\n  Constrained Generation Framework", "abstract": "Recently, generative pre-trained models have made significant strides,\nparticularly highlighted by the release of ChatGPT and GPT-4, which exhibit\nsuperior cross-domain capabilities. However, these models still face challenges\non constrained writing tasks like poem generation under open-domain titles. In\nresponse to this challenge, we introduce Block Inverse Prompting (BIPro)\nconstrained generation framework. BIPro leverages two block inverse prompting\nmethods, revise and rewrite, that mimic the process of human text writing using\nblock generative models. It significantly improves the zero-shot generation\nquality on the formidable constrained generation task of open-domain\ntraditional-form Chinese poem generation. Based on a less powerful block\ngenerative model GLM-10B-Chinese, poems composed via BIPro without priming or\nadditional training outperform both most advanced direct generative systems\nlike GPT-4 or GLM-4 and best domain-specific systems such as Yusheng,\nShisanbai, or Baidu Poetry Helper in human evaluation by proficient poets.\nFinally, BIPro considerably narrows the gap between AI-generated works and\nshort-listed human literary arts in another human evaluation, unveiling the\npromising potential of block generative models in improving the quality of\nconstrained generation.", "published": "2024-11-20 11:56:56", "link": "http://arxiv.org/abs/2411.13237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for\n  Text-to-SQL", "abstract": "Large Language Models (LLMs) exhibit impressive problem-solving skills across\nmany tasks, but they still underperform compared to humans in various\ndownstream applications, such as text-to-SQL. On the BIRD benchmark\nleaderboard, human performance achieves an accuracy of 92.96\\%, whereas the\ntop-performing method reaches only 72.39\\%. Notably, these state-of-the-art\n(SoTA) methods predominantly rely on in-context learning to simulate human-like\nreasoning. However, they overlook a critical human skill: continual learning.\nInspired by the educational practice of maintaining mistake notebooks during\nour formative years, we propose LPE-SQL (Leveraging Prior Experience: An\nExpandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework\ndesigned to augment LLMs by enabling continual learning without requiring\nparameter fine-tuning. LPE-SQL consists of four modules that \\textbf{i)}\nretrieve relevant entries, \\textbf{ii)} efficient sql generation, \\textbf{iii)}\ngenerate the final result through a cross-consistency mechanism and\n\\textbf{iv)} log successful and failed tasks along with their reasoning\nprocesses or reflection-generated tips. Importantly, the core module of LPE-SQL\nis the fourth one, while the other modules employ foundational methods,\nallowing LPE-SQL to be easily integrated with SoTA technologies to further\nenhance performance. Our experimental results demonstrate that this continual\nlearning approach yields substantial performance gains, with the smaller\nLlama-3.1-70B model with surpassing the performance of the larger\nLlama-3.1-405B model using SoTA methods.", "published": "2024-11-20 12:03:17", "link": "http://arxiv.org/abs/2411.13244v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Autoregressive and Autoencoder Language Models for Text\n  Classification", "abstract": "This paper presents CAALM-TC (Combining Autoregressive and Autoencoder\nLanguage Models for Text Classification), a novel method that enhances text\nclassification by integrating autoregressive and autoencoder language models.\nAutoregressive large language models such as Open AI's GPT, Meta's Llama or\nMicrosoft's Phi offer promising prospects for content analysis practitioners,\nbut they generally underperform supervised BERT based models for text\nclassification. CAALM leverages autoregressive models to generate contextual\ninformation based on input texts, which is then combined with the original text\nand fed into an autoencoder model for classification. This hybrid approach\ncapitalizes on the extensive contextual knowledge of autoregressive models and\nthe efficient classification capabilities of autoencoders. Experimental results\non four benchmark datasets demonstrate that CAALM consistently outperforms\nexisting methods, particularly in tasks with smaller datasets and more abstract\nclassification objectives. The findings indicate that CAALM offers a scalable\nand effective solution for automated content analysis in social science\nresearch that minimizes sample size requirements.", "published": "2024-11-20 12:49:42", "link": "http://arxiv.org/abs/2411.13282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformer-Based Contextualized Language Models Joint with Neural\n  Networks for Natural Language Inference in Vietnamese", "abstract": "Natural Language Inference (NLI) is a task within Natural Language Processing\n(NLP) that holds value for various AI applications. However, there have been\nlimited studies on Natural Language Inference in Vietnamese that explore the\nconcept of joint models. Therefore, we conducted experiments using various\ncombinations of contextualized language models (CLM) and neural networks. We\nuse CLM to create contextualized work presentations and use Neural Networks for\nclassification. Furthermore, we have evaluated the strengths and weaknesses of\neach joint model and identified the model failure points in the Vietnamese\ncontext. The highest F1 score in this experiment, up to 82.78% in the benchmark\ndataset (ViNLI). By conducting experiments with various models, the most\nconsiderable size of the CLM is XLM-R (355M). That combination has consistently\ndemonstrated superior performance compared to fine-tuning strong pre-trained\nlanguage models like PhoBERT (+6.58%), mBERT (+19.08%), and XLM-R (+0.94%) in\nterms of F1-score. This article aims to introduce a novel approach or model\nthat attains improved performance for Vietnamese NLI. Overall, we find that the\njoint approach of CLM and neural networks is simple yet capable of achieving\nhigh-quality performance, which makes it suitable for applications that require\nefficient resource utilization.", "published": "2024-11-20 15:46:48", "link": "http://arxiv.org/abs/2411.13407v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training", "abstract": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext.", "published": "2024-11-20 17:22:31", "link": "http://arxiv.org/abs/2411.13476v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Memory and Reasoning Ability in Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.", "published": "2024-11-20 17:55:38", "link": "http://arxiv.org/abs/2411.13504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictive Insights into LGBTQ+ Minority Stress: A Transductive\n  Exploration of Social Media Discourse", "abstract": "Individuals who identify as sexual and gender minorities, including lesbian,\ngay, bisexual, transgender, queer, and others (LGBTQ+) are more likely to\nexperience poorer health than their heterosexual and cisgender counterparts.\nOne primary source that drives these health disparities is minority stress\n(i.e., chronic and social stressors unique to LGBTQ+ communities' experiences\nadapting to the dominant culture). This stress is frequently expressed in\nLGBTQ+ users' posts on social media platforms. However, these expressions are\nnot just straightforward manifestations of minority stress. They involve\nlinguistic complexity (e.g., idiom or lexical diversity), rendering them\nchallenging for many traditional natural language processing methods to detect.\nIn this work, we designed a hybrid model using Graph Neural Networks (GNN) and\nBidirectional Encoder Representations from Transformers (BERT), a pre-trained\ndeep language model to improve the classification performance of minority\nstress detection. We experimented with our model on a benchmark social media\ndataset for minority stress detection (LGBTQ+ MiSSoM+). The dataset is\ncomprised of 5,789 human-annotated Reddit posts from LGBTQ+ subreddits. Our\napproach enables the extraction of hidden linguistic nuances through\npretraining on a vast amount of raw data, while also engaging in transductive\nlearning to jointly develop representations for both labeled training data and\nunlabeled test data. The RoBERTa-GCN model achieved an accuracy of 0.86 and an\nF1 score of 0.86, surpassing the performance of other baseline models in\npredicting LGBTQ+ minority stress. Improved prediction of minority stress\nexpressions on social media could lead to digital health interventions to\nimprove the wellbeing of LGBTQ+ people-a community with high rates of\nstress-sensitive health problems.", "published": "2024-11-20 18:35:41", "link": "http://arxiv.org/abs/2411.13534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Text Classification (HTC) vs. eXtreme Multilabel\n  Classification (XML): Two Sides of the Same Medal", "abstract": "Assigning a subset of labels from a fixed pool of labels to a given input\ntext is a text classification problem with many real-world applications, such\nas in recommender systems. Two separate research streams address this issue.\nHierarchical Text Classification (HTC) focuses on datasets with smaller label\npools of hundreds of entries, accompanied by a semantic label hierarchy. In\ncontrast, eXtreme Multi-Label Text Classification (XML) considers very large\nlabel pools with up to millions of entries, in which the labels are not\narranged in any particular manner. However, in XML, a common approach is to\nconstruct an artificial hierarchy without any semantic information before or\nduring the training process. Here, we investigate how state-of-the-art models\nfrom one domain perform when trained and tested on datasets from the other\ndomain. The HBGL and HGLCR models from the HTC domain are trained and tested on\nthe datasets Wiki10-31K, AmazonCat-13K, and Amazon-670K from the XML domain. On\nthe other side, the XML models CascadeXML and XR-Transformer are trained and\ntested on the datasets Web of Science, The New York Times Annotated Corpus, and\nRCV1-V2 from the HTC domain. HTC models, on the other hand, are not equipped to\nhandle the size of XML datasets and achieve poor transfer results. The code and\nnumerous files that are needed to reproduce our results can be obtained from\nhttps://github.com/FloHauss/XMC_HTC", "published": "2024-11-20 20:07:25", "link": "http://arxiv.org/abs/2411.13687v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection", "abstract": "Large Language Models (LLMs) are prone to off-topic misuse, where users may\nprompt these models to perform tasks beyond their intended scope. Current\nguardrails, which often rely on curated examples or custom classifiers, suffer\nfrom high false-positive rates, limited adaptability, and the impracticality of\nrequiring real-world data that is not available in pre-production. In this\npaper, we introduce a flexible, data-free guardrail development methodology\nthat addresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.", "published": "2024-11-20 00:31:23", "link": "http://arxiv.org/abs/2411.12946v2", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning", "abstract": "Contemporary embodied agents powered by large language models (LLMs), such as\nVoyager, have shown promising capabilities in individual learning within\nopen-ended environments like Minecraft. However, when powered by open LLMs,\nthey struggle with basic tasks even after domain-specific fine-tuning. We\npresent MindForge, a generative-agent framework for collaborative lifelong\nlearning through explicit perspective taking. We introduce three key\ninnovations: (1) a structured theory of mind representation linking percepts,\nbeliefs, desires, and actions; (2) natural interagent communication; and (3) a\nmulticomponent memory system. In Minecraft experiments, MindForge agents\npowered by open-weight LLMs significantly outperform their Voyager counterparts\nin basic tasks where traditional Voyager fails without GPT-4, collecting\n$2.3\\times$ more unique items and achieving $3\\times$ more tech-tree\nmilestones, advancing from basic wood tools to advanced iron equipment.\nMindForge agents demonstrate sophisticated behaviors, including expert-novice\nknowledge transfer, collaborative problem solving, and adaptation to\nout-of-distribution tasks through accumulated collaborative experiences.\nMindForge advances the democratization of embodied AI development through\nopen-ended social learning, enabling peer-to-peer knowledge sharing.", "published": "2024-11-20 02:10:44", "link": "http://arxiv.org/abs/2411.12977v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Training Bilingual LMs with Data Constraints in the Targeted Language", "abstract": "Large language models are trained on massive scrapes of the web, as required\nby current scaling laws. Most progress is made for English, given its abundance\nof high-quality pretraining data. For most other languages, however, such high\nquality pretraining data is unavailable. In this work, we study how to boost\npretrained model performance in a target language with insufficient pretraining\ndata for training a high performing language model, by enlisting data from an\nauxiliary language for which high quality data is available. We study this by\nquantifying the performance gap between training with data in a data-rich\nauxiliary language compared with training in the target language, exploring the\nbenefits of translation systems, studying the limitations of model scaling when\ndata is limited in the target languages, and proposing new methods for\nupsampling data from the auxiliary language. Our results show that stronger\nauxiliary datasets result in performance gains without modification to the\nmodel or training objective for close languages, and, in particular, that\nperformance gains due to the development of more information-rich English\npretraining datasets can extend to targeted language settings with limited\ndata.", "published": "2024-11-20 02:27:40", "link": "http://arxiv.org/abs/2411.12986v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts", "abstract": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.", "published": "2024-11-20 03:17:51", "link": "http://arxiv.org/abs/2411.13009v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Breaking the Cycle of Recurring Failures: Applying Generative AI to Root\n  Cause Analysis in Legacy Banking Systems", "abstract": "Traditional banks face significant challenges in digital transformation,\nprimarily due to legacy system constraints and fragmented ownership. Recent\nincidents show that such fragmentation often results in superficial incident\nresolutions, leaving root causes unaddressed and causing recurring failures. We\nintroduce a novel approach to post-incident analysis, integrating\nknowledge-based GenAI agents with the \"Five Whys\" technique to examine problem\ndescriptions and change request data. This method uncovered that approximately\n70% of the incidents previously attributed to management or vendor failures\nwere due to underlying internal code issues. We present a case study to show\nthe impact of our method. By scanning over 5,000 projects, we identified over\n400 files with a similar root cause. Overall, we leverage the knowledge-based\nagents to automate and elevate root cause analysis, transforming it into a more\nproactive process. These agents can be applied across other phases of the\nsoftware development lifecycle, further improving development processes.", "published": "2024-11-20 03:43:03", "link": "http://arxiv.org/abs/2411.13017v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level\n  Granularity Syllable Count Control", "abstract": "Lyrics generation presents unique challenges, particularly in achieving\nprecise syllable control while adhering to song form structures such as verses\nand choruses. Conventional line-by-line approaches often lead to unnatural\nphrasing, underscoring the need for more granular syllable management. We\npropose a framework for lyrics generation that enables multi-level syllable\ncontrol at the word, phrase, line, and paragraph levels, aware of song form.\nOur approach generates complete lyrics conditioned on input text and song form,\nensuring alignment with specified syllable constraints. Generated lyrics\nsamples are available at: https://tinyurl.com/lyrics9999", "published": "2024-11-20 07:57:58", "link": "http://arxiv.org/abs/2411.13100v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fact-Level Confidence Calibration and Self-Correction", "abstract": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems.", "published": "2024-11-20 14:15:18", "link": "http://arxiv.org/abs/2411.13343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Way to LLM Personalization: Learning to Remember User\n  Conversations", "abstract": "Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations.", "published": "2024-11-20 15:45:08", "link": "http://arxiv.org/abs/2411.13405v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LIMBA: An Open-Source Framework for the Preservation and Valorization of\n  Low-Resource Languages using Generative Models", "abstract": "Minority languages are vital to preserving cultural heritage, yet they face\ngrowing risks of extinction due to limited digital resources and the dominance\nof artificial intelligence models trained on high-resource languages. This\nwhite paper proposes a framework to generate linguistic tools for low-resource\nlanguages, focusing on data creation to support the development of language\nmodels that can aid in preservation efforts. Sardinian, an endangered language,\nserves as the case study to demonstrate the framework's effectiveness. By\naddressing the data scarcity that hinders intelligent applications for such\nlanguages, we contribute to promoting linguistic diversity and support ongoing\nefforts in language standardization and revitalization through modern\ntechnologies.", "published": "2024-11-20 16:59:41", "link": "http://arxiv.org/abs/2411.13453v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Advancing Complex Medical Communication in Arabic with Sporo AraSum:\n  Surpassing Existing Large Language Models", "abstract": "The increasing demand for multilingual capabilities in healthcare underscores\nthe need for AI models adept at processing diverse languages, particularly in\nclinical documentation and decision-making. Arabic, with its complex\nmorphology, syntax, and diglossia, poses unique challenges for natural language\nprocessing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a\nlanguage model tailored for Arabic clinical documentation, against JAIS, the\nleading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics\nmodified ourselves for the purposes of assessing model performances in a\ndifferent language. The study assessed the models' performance in summarizing\npatient-physician interactions, focusing on accuracy, comprehensiveness,\nclinical utility, and linguistic-cultural competence.\n  Results indicate that Sporo AraSum significantly outperforms JAIS in\nAI-centric quantitative metrics and all qualitative attributes measured in our\nmodified version of the PDQI-9. AraSum's architecture enables precise and\nculturally sensitive documentation, addressing the linguistic nuances of Arabic\nwhile mitigating risks of AI hallucinations. These findings suggest that Sporo\nAraSum is better suited to meet the demands of Arabic-speaking healthcare\nenvironments, offering a transformative solution for multilingual clinical\nworkflows. Future research should incorporate real-world data to further\nvalidate these findings and explore broader integration into healthcare\nsystems.", "published": "2024-11-20 18:10:19", "link": "http://arxiv.org/abs/2411.13518v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Generation for Domain-Specific Question Answering: A\n  Case Study on Pittsburgh and CMU", "abstract": "We designed a Retrieval-Augmented Generation (RAG) system to provide large\nlanguage models with relevant documents for answering domain-specific questions\nabout Pittsburgh and Carnegie Mellon University (CMU). We extracted over 1,800\nsubpages using a greedy scraping strategy and employed a hybrid annotation\nprocess, combining manual and Mistral-generated question-answer pairs,\nachieving an inter-annotator agreement (IAA) score of 0.7625. Our RAG framework\nintegrates BM25 and FAISS retrievers, enhanced with a reranker for improved\ndocument retrieval accuracy. Experimental results show that the RAG system\nsignificantly outperforms a non-RAG baseline, particularly in time-sensitive\nand complex queries, with an F1 score improvement from 5.45% to 42.21% and\nrecall of 56.18%. This study demonstrates the potential of RAG systems in\nenhancing answer precision and relevance, while identifying areas for further\noptimization in document retrieval and model training.", "published": "2024-11-20 20:10:43", "link": "http://arxiv.org/abs/2411.13691v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Assessing Gender Bias in LLMs: Comparing LLM Outputs with Human\n  Perceptions and Official Statistics", "abstract": "This study investigates gender bias in large language models (LLMs) by\ncomparing their gender perception to that of human respondents, U.S. Bureau of\nLabor Statistics data, and a 50% no-bias benchmark. We created a new evaluation\nset using occupational data and role-specific sentences. Unlike common\nbenchmarks included in LLM training data, our set is newly developed,\npreventing data leakage and test set contamination. Five LLMs were tested to\npredict the gender for each role using single-word answers. We used\nKullback-Leibler (KL) divergence to compare model outputs with human\nperceptions, statistical data, and the 50% neutrality benchmark. All LLMs\nshowed significant deviation from gender neutrality and aligned more with\nstatistical data, still reflecting inherent biases.", "published": "2024-11-20 22:43:18", "link": "http://arxiv.org/abs/2411.13738v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Planning with Compound LLM Architectures: An LLM-Modulo Approach", "abstract": "Previous work has attempted to boost Large Language Model (LLM) performance\non planning and scheduling tasks through a variety of prompt engineering\ntechniques. While these methods can work within the distributions tested, they\nare neither robust nor predictable. This limitation can be addressed through\ncompound LLM architectures where LLMs work in conjunction with other components\nto ensure reliability. In this paper, we present a technical evaluation of a\ncompound LLM architecture--the LLM-Modulo framework. In this framework, an LLM\nis paired with a complete set of sound verifiers that validate its output,\nre-prompting it if it fails. This approach ensures that the system can never\noutput any fallacious output, and therefore that every output generated is\nguaranteed correct--something previous techniques have not been able to claim.\nOur results, evaluated across four scheduling domains, demonstrate significant\nperformance gains with the LLM-Modulo framework using various models.\nAdditionally, we explore modifications to the base configuration of the\nframework and assess their impact on overall system performance.", "published": "2024-11-20 02:04:09", "link": "http://arxiv.org/abs/2411.14484v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Impossible Test: A 2024 Unsolvable Dataset and A Chance for an AGI\n  Quiz", "abstract": "This research introduces a novel evaluation framework designed to assess\nlarge language models' (LLMs) ability to acknowledge uncertainty on 675\nfundamentally unsolvable problems. Using a curated dataset of graduate-level\ngrand challenge questions with intentionally unknowable answers, we evaluated\ntwelve state-of-the-art LLMs, including both open and closed-source models, on\ntheir propensity to admit ignorance rather than generate plausible but\nincorrect responses. The best models scored in 62-68% accuracy ranges for\nadmitting the problem solution was unknown in fields ranging from biology to\nphilosophy and mathematics. We observed an inverse relationship between problem\ndifficulty and model accuracy, with GPT-4 demonstrating higher rates of\nuncertainty acknowledgment on more challenging problems (35.8%) compared to\nsimpler ones (20.0%). This pattern indicates that models may be more prone to\ngenerate speculative answers when problems appear more tractable. The study\nalso revealed significant variations across problem categories, with models\nshowing difficulty in acknowledging uncertainty in invention and NP-hard\nproblems while performing relatively better on philosophical and psychological\nchallenges. These results contribute to the growing body of research on\nartificial general intelligence (AGI) assessment by highlighting the importance\nof uncertainty recognition as a critical component of future machine\nintelligence evaluation. This impossibility test thus extends previous\ntheoretical frameworks for universal intelligence testing by providing\nempirical evidence of current limitations in LLMs' ability to recognize their\nown knowledge boundaries, suggesting new directions for improving model\ntraining architectures and evaluation approaches.", "published": "2024-11-20 04:12:29", "link": "http://arxiv.org/abs/2411.14486v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Human-Centric LLMs", "abstract": "The rapid evolution of large language models (LLMs) and their capacity to\nsimulate human cognition and behavior has given rise to LLM-based frameworks\nand tools that are evaluated and applied based on their ability to perform\ntasks traditionally performed by humans, namely those involving cognition,\ndecision-making, and social interaction. This survey provides a comprehensive\nexamination of such human-centric LLM capabilities, focusing on their\nperformance in both individual tasks (where an LLM acts as a stand-in for a\nsingle human) and collective tasks (where multiple LLMs coordinate to mimic\ngroup dynamics). We first evaluate LLM competencies across key areas including\nreasoning, perception, and social cognition, comparing their abilities to\nhuman-like skills. Then, we explore real-world applications of LLMs in\nhuman-centric domains such as behavioral science, political science, and\nsociology, assessing their effectiveness in replicating human behaviors and\ninteractions. Finally, we identify challenges and future research directions,\nsuch as improving LLM adaptability, emotional intelligence, and cultural\nsensitivity, while addressing inherent biases and enhancing frameworks for\nhuman-AI collaboration. This survey aims to provide a foundational\nunderstanding of LLMs from a human-centric perspective, offering insights into\ntheir current capabilities and potential for future development.", "published": "2024-11-20 12:34:44", "link": "http://arxiv.org/abs/2411.14491v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning", "abstract": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.", "published": "2024-11-20 05:30:15", "link": "http://arxiv.org/abs/2411.13045v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding", "abstract": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.", "published": "2024-11-20 09:46:30", "link": "http://arxiv.org/abs/2411.13157v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot\n  TTS and LLM", "abstract": "Text-to-speech (TTS) models have been widely adopted to enhance automatic\nspeech recognition (ASR) systems using text-only corpora, thereby reducing the\ncost of labeling real speech data. Existing research primarily utilizes\nadditional text data and predefined speech styles supported by TTS models. In\nthis paper, we propose Hard-Synth, a novel ASR data augmentation method that\nleverages large language models (LLMs) and advanced zero-shot TTS. Our approach\nemploys LLMs to generate diverse in-domain text through rewriting, without\nrelying on additional text data. Rather than using predefined speech styles, we\nintroduce a hard prompt selection method with zero-shot TTS to clone speech\nstyles that the ASR model finds challenging to recognize. Experiments\ndemonstrate that Hard-Synth significantly enhances the Conformer model,\nachieving relative word error rate (WER) reductions of 6.5\\%/4.4\\% on\nLibriSpeech dev/test-other subsets. Additionally, we show that Hard-Synth is\ndata-efficient and capable of reducing bias in ASR.", "published": "2024-11-20 09:49:37", "link": "http://arxiv.org/abs/2411.13159v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal\n  Models in Video Analysis through User Simulation", "abstract": "Large multimodal models (LMMs) with advanced video analysis capabilities have\nrecently garnered significant attention. However, most evaluations rely on\ntraditional methods like multiple-choice questions in benchmarks such as\nVideoMME and LongVideoBench, which are prone to lack the depth needed to\ncapture the complex demands of real-world users. To address this limitation-and\ndue to the prohibitive cost and slow pace of human annotation for video\ntasks-we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS\nChatbot Arena's framework, designed to automatically assess LMMs' video\nanalysis abilities. VideoAutoArena utilizes user simulation to generate\nopen-ended, adaptive questions that rigorously assess model performance in\nvideo understanding. The benchmark features an automated, scalable evaluation\nframework, incorporating a modified ELO Rating System for fair and continuous\ncomparisons across multiple LMMs. To validate our automated judging system, we\nconstruct a 'gold standard' using a carefully curated subset of human\nannotations, demonstrating that our arena strongly aligns with human judgment\nwhile maintaining scalability. Additionally, we introduce a fault-driven\nevolution strategy, progressively increasing question complexity to push models\ntoward handling more challenging video analysis scenarios. Experimental results\ndemonstrate that VideoAutoArena effectively differentiates among\nstate-of-the-art LMMs, providing insights into model strengths and areas for\nimprovement. To further streamline our evaluation, we introduce VideoAutoBench\nas an auxiliary benchmark, where human annotators label winners in a subset of\nVideoAutoArena battles. We use GPT-4o as a judge to compare responses against\nthese human-validated answers. Together, VideoAutoArena and VideoAutoBench\noffer a cost-effective, and scalable framework for evaluating LMMs in\nuser-centric video analysis.", "published": "2024-11-20 12:48:34", "link": "http://arxiv.org/abs/2411.13281v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Executable QR codes with Machine Learning for Industrial Applications", "abstract": "Executable QR codes, also known as eQR codes or just sQRy, are a special kind\nof QR codes that embed programs conceived to run on mobile devices like\nsmartphones. Since the program is directly encoded in binary form within the QR\ncode, it can be executed even when the reading device is not provided with\nInternet access. The applications of this technology are manifold, and range\nfrom smart user guides to advisory systems. The first programming language made\navailable for eQR is QRtree, which enables the implementation of decision trees\naimed, for example, at guiding the user in operating/maintaining a complex\nmachinery or for reaching a specific location.\n  In this work, an additional language is proposed, we term QRind, which was\nspecifically devised for Industry. It permits to integrate distinct\ncomputational blocks into the QR code, e.g., machine learning models to enable\npredictive maintenance and algorithms to ease machinery usage. QRind permits\nthe Industry 4.0/5.0 paradigms to be implemented, in part, also in those cases\nwhere Internet is unavailable.", "published": "2024-11-20 15:38:33", "link": "http://arxiv.org/abs/2411.13400v1", "categories": ["cs.NI", "cs.CL", "cs.FL"], "primary_category": "cs.NI"}
{"title": "Unification of Balti and trans-border sister dialects in the essence of\n  LLMs and AI Technology", "abstract": "The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far.", "published": "2024-11-20 15:48:21", "link": "http://arxiv.org/abs/2411.13409v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CAFE A Novel Code switching Dataset for Algerian Dialect French and\n  English", "abstract": "The paper introduces and publicly releases (Data download link available\nafter acceptance) CAFE -- the first Code-switching dataset between Algerian\ndialect, French, and english languages. The CAFE speech data is unique for (a)\nits spontaneous speaking style in vivo human-human conversation capturing\nphenomena like code-switching and overlapping speech, (b) addresses distinct\nlinguistic challenges in North African Arabic dialect; (c) the CAFE captures\ndialectal variations from various parts of Algeria within different\nsociolinguistic contexts. CAFE data contains approximately 37 hours of speech,\nwith a subset, CAFE-small, of 2 hours and 36 minutes released with manual human\nannotation including speech segmentation, transcription, explicit annotation of\ncode-switching points, overlapping speech, and other events such as noises, and\nlaughter among others. The rest approximately 34.58 hours contain pseudo label\ntranscriptions. In addition to the data release, the paper also highlighted the\nchallenges of using state-of-the-art Automatic Speech Recognition (ASR) models\nsuch as Whisper large-v2,3 and PromptingWhisper to handle such content.\nFollowing, we benchmark CAFE data with the aforementioned Whisper models and\nshow how well-designed data processing pipelines and advanced decoding\ntechniques can improve the ASR performance in terms of Mixed Error Rate (MER)\nof 0.310, Character Error Rate (CER) of 0.329 and Word Error Rate (WER) of\n0.538.", "published": "2024-11-20 16:09:16", "link": "http://arxiv.org/abs/2411.13424v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "WaterPark: A Robustness Assessment of Language Model Watermarking", "abstract": "Various watermarking methods (``watermarkers'') have been proposed to\nidentify LLM-generated texts; yet, due to the lack of unified evaluation\nplatforms, many critical questions remain under-explored: i) What are the\nstrengths/limitations of various watermarkers, especially their attack\nrobustness? ii) How do various design choices impact their robustness? iii) How\nto optimally operate watermarkers in adversarial environments? To fill this\ngap, we systematize existing LLM watermarkers and watermark removal attacks,\nmapping out their design spaces. We then develop WaterPark, a unified platform\nthat integrates 10 state-of-the-art watermarkers and 12 representative attacks.\nMore importantly, by leveraging WaterPark, we conduct a comprehensive\nassessment of existing watermarkers, unveiling the impact of various design\nchoices on their attack robustness. We further explore the best practices to\noperate watermarkers in adversarial environments. We believe our study sheds\nlight on current LLM watermarking techniques while WaterPark serves as a\nvaluable testbed to facilitate future research.", "published": "2024-11-20 16:09:22", "link": "http://arxiv.org/abs/2411.13425v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from\n  Human Demonstrations", "abstract": "State-of-the-art multimodal web agents, powered by Multimodal Large Language\nModels (MLLMs), can autonomously execute many web tasks by processing user\ninstructions and interacting with graphical user interfaces (GUIs). Current\nstrategies for building web agents rely on (i) the generalizability of\nunderlying MLLMs and their steerability via prompting, and (ii) large-scale\nfine-tuning of MLLMs on web-related tasks. However, web agents still struggle\nto automate tasks on unseen websites and domains, limiting their applicability\nto enterprise-specific and proprietary platforms. Beyond generalization from\nlarge-scale pre-training and fine-tuning, we propose building agents for\nfew-shot adaptability using human demonstrations. We introduce the AdaptAgent\nframework that enables both proprietary and open-weights multimodal web agents\nto adapt to new websites and domains using few human demonstrations (up to 2).\nOur experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show\nthat using in-context demonstrations (for proprietary models) or\nmeta-adaptation demonstrations (for meta-learned open-weights models) boosts\ntask success rate by 3.36% to 7.21% over non-adapted state-of-the-art models,\ncorresponding to a relative increase of 21.03% to 65.75%. Furthermore, our\nadditional analyses (a) show the effectiveness of multimodal demonstrations\nover text-only ones, (b) shed light on the influence of different data\nselection strategies during meta-learning on the generalization of the agent,\nand (c) demonstrate the effect of number of few-shot examples on the web\nagent's success rate. Overall, our results unlock a complementary axis for\ndeveloping widely applicable multimodal web agents beyond large-scale\npre-training and fine-tuning, emphasizing few-shot adaptability.", "published": "2024-11-20 16:54:15", "link": "http://arxiv.org/abs/2411.13451v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "PatentEdits: Framing Patent Novelty as Textual Entailment", "abstract": "A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art.", "published": "2024-11-20 17:23:40", "link": "http://arxiv.org/abs/2411.13477v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets", "abstract": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.", "published": "2024-11-20 17:35:21", "link": "http://arxiv.org/abs/2411.13485v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3; I.2.6; H.5.2"], "primary_category": "cs.CL"}
{"title": "Hymba: A Hybrid-head Architecture for Small Language Models", "abstract": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.", "published": "2024-11-20 19:51:25", "link": "http://arxiv.org/abs/2411.13676v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Test Security in Remote Testing Age: Perspectives from Process Data\n  Analytics and AI", "abstract": "The COVID-19 pandemic has accelerated the implementation and acceptance of\nremotely proctored high-stake assessments. While the flexible administration of\nthe tests brings forth many values, it raises test security-related concerns.\nMeanwhile, artificial intelligence (AI) has witnessed tremendous advances in\nthe last five years. Many AI tools (such as the very recent ChatGPT) can\ngenerate high-quality responses to test items. These new developments require\ntest security research beyond the statistical analysis of scores and response\ntime. Data analytics and AI methods based on clickstream process data can get\nus deeper insight into the test-taking process and hold great promise for\nsecuring remotely administered high-stakes tests. This chapter uses real-world\nexamples to show that this is indeed the case.", "published": "2024-11-20 20:38:34", "link": "http://arxiv.org/abs/2411.13699v2", "categories": ["cs.CR", "cs.CL", "cs.HC"], "primary_category": "cs.CR"}
{"title": "Mediating Modes of Thought: LLM's for design scripting", "abstract": "Architects adopt visual scripting and parametric design tools to explore more\nexpansive design spaces (Coates, 2010), refine their thinking about the\ngeometric logic of their design (Woodbury, 2010), and overcome conventional\nsoftware limitations (Burry, 2011). Despite two decades of effort to make\ndesign scripting more accessible, a disconnect between a designer's free ways\nof thinking and the rigidity of algorithms remains (Burry, 2011). Recent\ndevelopments in Large Language Models (LLMs) suggest this might soon change, as\nLLMs encode a general understanding of human context and exhibit the capacity\nto produce geometric logic. This project speculates that if LLMs can\neffectively mediate between user intent and algorithms, they become a powerful\ntool to make scripting in design more widespread and fun. We explore if such\nsystems can interpret natural language prompts to assemble geometric operations\nrelevant to computational design scripting. In the system, multiple layers of\nLLM agents are configured with specific context to infer the user intent and\nconstruct a sequential logic. Given a user's high-level text prompt, a\ngeometric description is created, distilled into a sequence of logic\noperations, and mapped to software-specific commands. The completed script is\nconstructed in the user's visual programming interface. The system succeeds in\ngenerating complete visual scripts up to a certain complexity but fails beyond\nthis complexity threshold. It shows how LLMs can make design scripting much\nmore aligned with human creativity and thought. Future research should explore\nconversational interactions, expand to multimodal inputs and outputs, and\nassess the performance of these tools.", "published": "2024-11-20 02:49:18", "link": "http://arxiv.org/abs/2411.14485v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Ensuring Safety and Trust: Analyzing the Risks of Large Language Models\n  in Medicine", "abstract": "The remarkable capabilities of Large Language Models (LLMs) make them\nincreasingly compelling for adoption in real-world healthcare applications.\nHowever, the risks associated with using LLMs in medical applications have not\nbeen systematically characterized. We propose using five key principles for\nsafe and trustworthy medical AI: Truthfulness, Resilience, Fairness,\nRobustness, and Privacy, along with ten specific aspects. Under this\ncomprehensive framework, we introduce a novel MedGuard benchmark with 1,000\nexpert-verified questions. Our evaluation of 11 commonly used LLMs shows that\nthe current language models, regardless of their safety alignment mechanisms,\ngenerally perform poorly on most of our benchmarks, particularly when compared\nto the high performance of human physicians. Despite recent reports indicate\nthat advanced LLMs like ChatGPT can match or even exceed human performance in\nvarious medical tasks, this study underscores a significant safety gap,\nhighlighting the crucial need for human oversight and the implementation of AI\nsafety guardrails.", "published": "2024-11-20 06:34:32", "link": "http://arxiv.org/abs/2411.14487v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "GhostRNN: Reducing State Redundancy in RNN with Cheap Operations", "abstract": "Recurrent neural network (RNNs) that are capable of modeling long-distance\ndependencies are widely used in various speech tasks, eg., keyword spotting\n(KWS) and speech enhancement (SE). Due to the limitation of power and memory in\nlow-resource devices, efficient RNN models are urgently required for real-world\napplications. In this paper, we propose an efficient RNN architecture,\nGhostRNN, which reduces hidden state redundancy with cheap operations. In\nparticular, we observe that partial dimensions of hidden states are similar to\nthe others in trained RNN models, suggesting that redundancy exists in specific\nRNNs. To reduce the redundancy and hence computational cost, we propose to\nfirst generate a few intrinsic states, and then apply cheap operations to\nproduce ghost states based on the intrinsic states. Experiments on KWS and SE\ntasks demonstrate that the proposed GhostRNN significantly reduces the memory\nusage (~40%) and computation cost while keeping performance similar.", "published": "2024-11-20 11:37:14", "link": "http://arxiv.org/abs/2411.14489v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "From Statistical Methods to Pre-Trained Models; A Survey on Automatic\n  Speech Recognition for Resource Scarce Urdu Language", "abstract": "Automatic Speech Recognition (ASR) technology has witnessed significant\nadvancements in recent years, revolutionizing human-computer interactions.\nWhile major languages have benefited from these developments, lesser-resourced\nlanguages like Urdu face unique challenges. This paper provides an extensive\nexploration of the dynamic landscape of ASR research, focusing particularly on\nthe resource-constrained Urdu language, which is widely spoken across South\nAsian nations. It outlines current research trends, technological advancements,\nand potential directions for future studies in Urdu ASR, aiming to pave the way\nfor forthcoming researchers interested in this domain. By leveraging\ncontemporary technologies, analyzing existing datasets, and evaluating\neffective algorithms and tools, the paper seeks to shed light on the unique\nchallenges and opportunities associated with Urdu language processing and its\nintegration into the broader field of speech research.", "published": "2024-11-20 17:39:56", "link": "http://arxiv.org/abs/2411.14493v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multimodal large language model for wheat breeding: a new exploration of\n  smart breeding", "abstract": "UAV remote sensing technology has become a key technology in crop breeding,\nwhich can achieve high-throughput and non-destructive collection of crop\nphenotyping data. However, the multidisciplinary nature of breeding has brought\ntechnical barriers and efficiency challenges to knowledge mining. Therefore, it\nis important to develop a smart breeding goal tool to mine cross-domain\nmultimodal data. Based on different pre-trained open-source multimodal large\nlanguage models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used\nsupervised fine-tuning (SFT), retrieval-augmented generation (RAG), and\nreinforcement learning from human feedback (RLHF) technologies to inject\ncross-domain knowledge into MLLMs, thereby constructing multiple multimodal\nlarge language models for wheat breeding (WBLMs). The above WBLMs were\nevaluated using the newly created evaluation benchmark in this study. The\nresults showed that the WBLM constructed using SFT, RAG and RLHF technologies\nand InternVL2-8B has leading performance. Then, subsequent experiments were\nconducted using the WBLM. Ablation experiments indicated that the combination\nof SFT, RAG, and RLHF technologies can improve the overall generation\nperformance, enhance the generated quality, balance the timeliness and\nadaptability of the generated answer, and reduce hallucinations and biases. The\nWBLM performed best in wheat yield prediction using cross-domain data (remote\nsensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of\n0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate\nprofessional decision support answers for phenotyping estimation, environmental\nstress assessment, target germplasm screening, cultivation technique\nrecommendation, and seed price query tasks.", "published": "2024-11-20 04:47:42", "link": "http://arxiv.org/abs/2411.15203v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Uni-Mlip: Unified Self-supervision for Medical Vision Language\n  Pre-training", "abstract": "Recent advancements in vision-language pre-training via contrastive learning\nhave significantly improved performance across computer vision tasks. However,\nin the medical domain, obtaining multimodal data is often costly and\nchallenging due to privacy, sensitivity, and annotation complexity. To mitigate\ndata scarcity while boosting model performance, we introduce \\textbf{Uni-Mlip},\na unified self-supervision framework specifically designed to enhance medical\nvision-language pre-training. Uni-Mlip seamlessly integrates cross-modality,\nuni-modality, and fused-modality self-supervision techniques at the data-level\nand the feature-level. Additionally, Uni-Mlip tailors uni-modal image\nself-supervision to accommodate the unique characteristics of medical images.\nOur experiments across datasets of varying scales demonstrate that Uni-Mlip\nsignificantly surpasses current state-of-the-art methods in three key\ndownstream tasks: image-text retrieval, image classification, and visual\nquestion answering (VQA).", "published": "2024-11-20 09:43:26", "link": "http://arxiv.org/abs/2411.15207v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "SlideSpawn: An Automatic Slides Generation System for Research\n  Publications", "abstract": "Research papers are well structured documents. They have text, figures,\nequations, tables etc., to covey their ideas and findings. They are divided\ninto sections like Introduction, Model, Experiments etc., which deal with\ndifferent aspects of research. Characteristics like these set research papers\napart from ordinary documents and allows us to significantly improve their\nsummarization. In this paper, we propose a novel system, SlideSpwan, that takes\nPDF of a research document as an input and generates a quality presentation\nproviding it's summary in a visual and concise fashion. The system first\nconverts the PDF of the paper to an XML document that has the structural\ninformation about various elements. Then a machine learning model, trained on\nPS5K dataset and Aminer 9.5K Insights dataset (that we introduce), is used to\npredict salience of each sentence in the paper. Sentences for slides are\nselected using ILP and clustered based on their similarity with each cluster\nbeing given a suitable title. Finally a slide is generated by placing any\ngraphical element referenced in the selected sentences next to them.\nExperiments on a test set of 650 pairs of papers and slides demonstrate that\nour system generates presentations with better quality.", "published": "2024-11-20 18:16:16", "link": "http://arxiv.org/abs/2411.17719v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "H.3"], "primary_category": "cs.CL"}
{"title": "Towards Advanced Speech Signal Processing: A Statistical Perspective on\n  Convolution-Based Architectures and its Applications", "abstract": "This article surveys convolution-based models including convolutional neural\nnetworks (CNNs), Conformers, ResNets, and CRNNs-as speech signal processing\nmodels and provide their statistical backgrounds and speech recognition,\nspeaker identification, emotion recognition, and speech enhancement\napplications. Through comparative training cost assessment, model size,\naccuracy and speed assessment, we compare the strengths and weaknesses of each\nmodel, identify potential errors and propose avenues for further research,\nemphasizing the central role it plays in advancing applications of speech\ntechnologies.", "published": "2024-11-20 13:01:30", "link": "http://arxiv.org/abs/2411.18636v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NLP Cluster Analysis of Common Core State Standards and NAEP Item\n  Specifications", "abstract": "Camilli (2024) proposed a methodology using natural language processing (NLP)\nto map the relationship of a set of content standards to item specifications.\nThis study provided evidence that NLP can be used to improve the mapping\nprocess. As part of this investigation, the nominal classifications of\nstandards and items specifications were used to examine construct equivalence.\nIn the current paper, we determine the strength of empirical support for the\nsemantic distinctiveness of these classifications, which are known as \"domains\"\nfor Common Core standards, and \"strands\" for National Assessment of Educational\nProgress (NAEP) item specifications. This is accomplished by separate k-means\nclustering for standards and specifications of their corresponding embedding\nvectors. We then briefly illustrate an application of these findings.", "published": "2024-11-20 15:44:58", "link": "http://arxiv.org/abs/2412.04482v2", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "I2TTS: Image-indicated Immersive Text-to-speech Synthesis with Spatial\n  Perception", "abstract": "Controlling the style and characteristics of speech synthesis is crucial for\nadapting the output to specific contexts and user requirements. Previous\nText-to-speech (TTS) works have focused primarily on the technical aspects of\nproducing natural-sounding speech, such as intonation, rhythm, and clarity.\nHowever, they overlook the fact that there is a growing emphasis on spatial\nperception of synthesized speech, which may provide immersive experience in\ngaming and virtual reality. To solve this issue, in this paper, we present a\nnovel multi-modal TTS approach, namely Image-indicated Immersive Text-to-speech\nSynthesis (I2TTS). Specifically, we introduce a scene prompt encoder that\nintegrates visual scene prompts directly into the synthesis pipeline to control\nthe speech generation process. Additionally, we propose a reverberation\nclassification and refinement technique that adjusts the synthesized\nmel-spectrogram to enhance the immersive experience, ensuring that the involved\nreverberation condition matches the scene accurately. Experimental results\ndemonstrate that our model achieves high-quality scene and spatial matching\nwithout compromising speech naturalness, marking a significant advancement in\nthe field of context-aware speech synthesis. Project demo page:\nhttps://spatialTTS.github.io/ Index Terms-Speech synthesis, scene prompt,\nspatial perception", "published": "2024-11-20 13:28:42", "link": "http://arxiv.org/abs/2411.13314v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ESARM: 3D Emotional Speech-to-Animation via Reward Model from\n  Automatically-Ranked Demonstrations", "abstract": "This paper proposes a novel 3D speech-to-animation (STA) generation framework\ndesigned to address the shortcomings of existing models in producing diverse\nand emotionally resonant animations. Current STA models often generate\nanimations that lack emotional depth and variety, failing to align with human\nexpectations. To overcome these limitations, we introduce a novel STA model\ncoupled with a reward model. This combination enables the decoupling of emotion\nand content under audio conditions through a cross-coupling training approach.\nAdditionally, we develop a training methodology that leverages automatic\nquality evaluation of generated facial animations to guide the reinforcement\nlearning process. This methodology encourages the STA model to explore a\nbroader range of possibilities, resulting in the generation of diverse and\nemotionally expressive facial animations of superior quality. We conduct\nextensive empirical experiments on a benchmark dataset, and the results\nvalidate the effectiveness of our proposed framework in generating\nhigh-quality, emotionally rich 3D animations that are better aligned with human\npreferences.", "published": "2024-11-20 07:37:37", "link": "http://arxiv.org/abs/2411.13089v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "SONNET: Enhancing Time Delay Estimation by Leveraging Simulated Audio", "abstract": "Time delay estimation or Time-Difference-Of-Arrival estimates is a critical\ncomponent for multiple localization applications such as multilateration,\ndirection of arrival, and self-calibration. The task is to estimate the time\ndifference between a signal arriving at two different sensors. For the audio\nsensor modality, most current systems are based on classical methods such as\nthe Generalized Cross-Correlation Phase Transform (GCC-PHAT) method. In this\npaper we demonstrate that learning based methods can, even based on synthetic\ndata, significantly outperform GCC-PHAT on novel real world data. To overcome\nthe lack of data with ground truth for the task, we train our model on a\nsimulated dataset which is sufficiently large and varied, and that captures the\nrelevant characteristics of the real world problem. We provide our trained\nmodel, SONNET (Simulation Optimized Neural Network Estimator of Timeshifts),\nwhich is runnable in real-time and works on novel data out of the box for many\nreal data applications, i.e. without re-training. We further demonstrate\ngreatly improved performance on the downstream task of self-calibration when\nusing our model compared to classical methods.", "published": "2024-11-20 10:23:21", "link": "http://arxiv.org/abs/2411.13179v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comparative Analysis of Audio Feature Extraction for Real-Time Talking\n  Portrait Synthesis", "abstract": "This paper examines the integration of real-time talking-head generation for\ninterviewer training, focusing on overcoming challenges in Audio Feature\nExtraction (AFE), which often introduces latency and limits responsiveness in\nreal-time applications. To address these issues, we propose and implement a\nfully integrated system that replaces conventional AFE models with Open AI's\nWhisper, leveraging its encoder to optimize processing and improve overall\nsystem efficiency. Our evaluation of two open-source real-time models across\nthree different datasets shows that Whisper not only accelerates processing but\nalso improves specific aspects of rendering quality, resulting in more\nrealistic and responsive talking-head interactions. These advancements make the\nsystem a more effective tool for immersive, interactive training applications,\nexpanding the potential of AI-driven avatars in interviewer training.", "published": "2024-11-20 11:18:05", "link": "http://arxiv.org/abs/2411.13209v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS", "68T45, 68T07, 68T01"], "primary_category": "cs.SD"}
{"title": "Energy-based features and bi-LSTM neural network for EEG-based music and\n  voice classification", "abstract": "The human brain receives stimuli in multiple ways; among them, audio\nconstitutes an important source of relevant stimuli for the brain regarding\ncommunication, amusement, warning, etc. In this context, the aim of this\nmanuscript is to advance in the classification of brain responses to music of\ndiverse genres and to sounds of different nature: speech and music. For this\npurpose, two different experiments have been designed to acquiere EEG signals\nfrom subjects listening to songs of different musical genres and sentences in\nvarious languages. With this, a novel scheme is proposed to characterize brain\nsignals for their classification; this scheme is based on the construction of a\nfeature matrix built on relations between energy measured at the different EEG\nchannels and the usage of a bi-LSTM neural network. With the data obtained,\nevaluations regarding EEG-based classification between speech and music,\ndifferent musical genres, and whether the subject likes the song listened to or\nnot are carried out. The experiments unveil satisfactory performance to the\nproposed scheme. The results obtained for binary audio type classification\nattain 98.66% of success. In multi-class classification between 4 musical\ngenres, the accuracy attained is 61.59%, and results for binary classification\nof musical taste rise to 96.96%.", "published": "2024-11-20 11:23:13", "link": "http://arxiv.org/abs/2411.13217v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "68", "G.3; I.5"], "primary_category": "eess.SP"}
{"title": "Building music with Lego bricks and Raspberry Pi", "abstract": "In this paper, a system to build music in an intuitive and accessible way,\nwith Lego bricks, is presented. The system makes use of the new powerful and\ncheap possibilities that technology offers for making old things in a new way.\nThe Raspberry Pi is used to control the system and run the necessary\nalgorithms, customized Lego bricks are used for building melodies, custom\nelectronic designs, software pieces and 3D printed parts complete the items\nemployed. The system designed is modular, it allows creating melodies with\nchords and percussion or just melodies or perform as a beatbox or a melody box.\nThe main interaction with the system is made using Lego-type building blocks.\nTests have demonstrated its versatility and ease of use, as well as its\nusefulness in music learning for both children and adults.", "published": "2024-11-20 11:37:05", "link": "http://arxiv.org/abs/2411.13224v1", "categories": ["cs.HC", "cs.SD", "eess.AS", "68", "B.m; J.m"], "primary_category": "cs.HC"}
{"title": "Efficient Streaming Voice Steganalysis in Challenging Detection\n  Scenarios", "abstract": "In recent years, there has been an increasing number of information hiding\ntechniques based on network streaming media, focusing on how to covertly and\nefficiently embed secret information into real-time transmitted network media\nsignals to achieve concealed communication. The misuse of these techniques can\nlead to significant security risks, such as the spread of malicious code,\ncommands, and viruses. Current steganalysis methods for network voice streams\nface two major challenges: efficient detection under low embedding rates and\nshort duration conditions. These challenges arise because, with low embedding\nrates (e.g., as low as 10%) and short transmission durations (e.g., only 0.1\nsecond), detection models struggle to acquire sufficiently rich sample\nfeatures, making effective steganalysis difficult. To address these challenges,\nthis paper introduces a Dual-View VoIP Steganalysis Framework (DVSF). The\nframework first randomly obfuscates parts of the native steganographic\ndescriptors in VoIP stream segments, making the steganographic features of\nhard-to-detect samples more pronounced and easier to learn. It then captures\nfine-grained local features related to steganography, building on the global\nfeatures of VoIP. Specially constructed VoIP segment triplets further adjust\nthe feature distances within the model. Ultimately, this method effectively\naddress the detection difficulty in VoIP. Extensive experiments demonstrate\nthat our method significantly improves the accuracy of streaming voice\nsteganalysis in these challenging detection scenarios, surpassing existing\nstate-of-the-art methods and offering superior near-real-time performance.", "published": "2024-11-20 02:22:58", "link": "http://arxiv.org/abs/2411.13612v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "FabuLight-ASD: Unveiling Speech Activity via Body Language", "abstract": "Active speaker detection (ASD) in multimodal environments is crucial for\nvarious applications, from video conferencing to human-robot interaction. This\npaper introduces FabuLight-ASD, an advanced ASD model that integrates facial,\naudio, and body pose information to enhance detection accuracy and robustness.\nOur model builds upon the existing Light-ASD framework by incorporating human\npose data, represented through skeleton graphs, which minimises computational\noverhead. Using the Wilder Active Speaker Detection (WASD) dataset, renowned\nfor reliable face and body bounding box annotations, we demonstrate\nFabuLight-ASD's effectiveness in real-world scenarios. Achieving an overall\nmean average precision (mAP) of 94.3%, FabuLight-ASD outperforms Light-ASD,\nwhich has an overall mAP of 93.7% across various challenging scenarios. The\nincorporation of body pose information shows a particularly advantageous\nimpact, with notable improvements in mAP observed in scenarios with speech\nimpairment, face occlusion, and human voice background noise. Furthermore,\nefficiency analysis indicates only a modest increase in parameter count (27.3%)\nand multiply-accumulate operations (up to 2.4%), underscoring the model's\nefficiency and feasibility. These findings validate the efficacy of\nFabuLight-ASD in enhancing ASD performance through the integration of body pose\ndata. FabuLight-ASD's code and model weights are available at\nhttps://github.com/knowledgetechnologyuhh/FabuLight-ASD.", "published": "2024-11-20 19:45:54", "link": "http://arxiv.org/abs/2411.13674v2", "categories": ["cs.CV", "cs.LG", "cs.NE", "cs.SD", "eess.AS", "68T20", "I.2.0"], "primary_category": "cs.CV"}
