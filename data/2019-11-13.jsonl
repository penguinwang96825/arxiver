{"title": "Adapting and evaluating a deep learning language model for clinical\n  why-question answering", "abstract": "Objectives: To adapt and evaluate a deep learning language model for\nanswering why-questions based on patient-specific clinical text. Materials and\nMethods: Bidirectional encoder representations from transformers (BERT) models\nwere trained with varying data sources to perform SQuAD 2.0 style why-question\nanswering (why-QA) on clinical notes. The evaluation focused on: 1) comparing\nthe merits from different training data, 2) error analysis. Results: The best\nmodel achieved an accuracy of 0.707 (or 0.760 by partial match). Training\ntoward customization for the clinical language helped increase 6% in accuracy.\nDiscussion: The error analysis suggested that the model did not really perform\ndeep reasoning and that clinical why-QA might warrant more sophisticated\nsolutions. Conclusion: The BERT model achieved moderate accuracy in clinical\nwhy-QA and should benefit from the rapidly evolving technology. Despite the\nidentified limitations, it could serve as a competent proxy for question-driven\nclinical information extraction.", "published": "2019-11-13 16:43:22", "link": "http://arxiv.org/abs/1911.05604v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prevalence of code mixing in semi-formal patient communication in low\n  resource languages of South Africa", "abstract": "In this paper we address the problem of code-mixing in resource-poor language\nsettings. We examine data consisting of 182k unique questions generated by\nusers of the MomConnect helpdesk, part of a national scale public health\nplatform in South Africa. We show evidence of code-switching at the level of\napproximately 10% within this dataset -- a level that is likely to pose\nchallenges for future services. We use a natural language processing library\n(Polyglot) that supports detection of 196 languages and attempt to evaluate its\nperformance at identifying English, isiZulu and code-mixed questions.", "published": "2019-11-13 17:12:40", "link": "http://arxiv.org/abs/1911.05636v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text", "abstract": "Modeling semantic plausibility requires commonsense knowledge about the world\nand has been used as a testbed for exploring various knowledge representations.\nPrevious work has focused specifically on modeling physical plausibility and\nshown that distributional methods fail when tested in a supervised setting. At\nthe same time, distributional models, namely large pretrained language models,\nhave led to improved results for many natural language understanding tasks. In\nthis work, we show that these pretrained language models are in fact effective\nat modeling physical plausibility in the supervised setting. We therefore\npresent the more difficult problem of learning to model physical plausibility\ndirectly from text. We create a training set by extracting attested events from\na large corpus, and we provide a baseline for training on these attested events\nin a self-supervised manner and testing on a physical plausibility task. We\nbelieve results could be further improved by injecting explicit commonsense\nknowledge into a distributional model.", "published": "2019-11-13 18:13:07", "link": "http://arxiv.org/abs/1911.05689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mark my Word: A Sequence-to-Sequence Approach to Definition Modeling", "abstract": "Defining words in a textual context is a useful task both for practical\npurposes and for gaining insight into distributed word representations.\nBuilding on the distributional hypothesis, we argue here that the most natural\nformalization of definition modeling is to treat it as a sequence-to-sequence\ntask, rather than a word-to-sequence task: given an input sequence with a\nhighlighted word, generate a contextually appropriate definition for it. We\nimplement this approach in a Transformer-based sequence-to-sequence model. Our\nproposal allows to train contextualization and definition generation in an\nend-to-end fashion, which is a conceptual improvement over earlier works. We\nachieve state-of-the-art results both in contextual and non-contextual\ndefinition modeling.", "published": "2019-11-13 18:44:15", "link": "http://arxiv.org/abs/1911.05715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do you mean, BERT? Assessing BERT as a Distributional Semantics\n  Model", "abstract": "Contextualized word embeddings, i.e. vector representations for words in\ncontext, are naturally seen as an extension of previous noncontextual\ndistributional semantic models. In this work, we focus on BERT, a deep neural\nnetwork that produces contextualized embeddings and has set the\nstate-of-the-art in several semantic tasks, and study the semantic coherence of\nits embedding space. While showing a tendency towards coherence, BERT does not\nfully live up to the natural expectations for a semantic vector space. In\nparticular, we find that the position of the sentence in which a word occurs,\nwhile having no meaning correlates, leaves a noticeable trace on the word\nembeddings and disturbs similarity relationships.", "published": "2019-11-13 19:04:42", "link": "http://arxiv.org/abs/1911.05758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Supervised Extractive Text Summarization via RNN-based Sequence\n  Classification", "abstract": "This article briefly explains our submitted approach to the DocEng'19\ncompetition on extractive summarization. We implemented a recurrent neural\nnetwork based model that learns to classify whether an article's sentence\nbelongs to the corresponding extractive summary or not. We bypass the lack of\nlarge annotated news corpora for extractive summarization by generating\nextractive summaries from abstractive ones, which are available from the CNN\ncorpus.", "published": "2019-11-13 15:51:23", "link": "http://arxiv.org/abs/1911.06121v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language\n  Representation", "abstract": "Pre-trained language representation models (PLMs) cannot well capture factual\nknowledge from text. In contrast, knowledge embedding (KE) methods can\neffectively represent the relational facts in knowledge graphs (KGs) with\ninformative entity embeddings, but conventional KE models cannot take full\nadvantage of the abundant textual information. In this paper, we propose a\nunified model for Knowledge Embedding and Pre-trained LanguagE Representation\n(KEPLER), which can not only better integrate factual knowledge into PLMs but\nalso produce effective text-enhanced KE with the strong PLMs. In KEPLER, we\nencode textual entity descriptions with a PLM as their embeddings, and then\njointly optimize the KE and language modeling objectives. Experimental results\nshow that KEPLER achieves state-of-the-art performances on various NLP tasks,\nand also works remarkably well as an inductive KE model on KG link prediction.\nFurthermore, for pre-training and evaluating KEPLER, we construct Wikidata5M, a\nlarge-scale KG dataset with aligned entity descriptions, and benchmark\nstate-of-the-art KE methods on it. It shall serve as a new KE benchmark and\nfacilitate the research on large KG, inductive KE, and KG with text. The source\ncode can be obtained from https://github.com/THU-KEG/KEPLER.", "published": "2019-11-13 05:21:45", "link": "http://arxiv.org/abs/1911.06136v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Pre-training for Natural Language Generation: A Literature\n  Review", "abstract": "Recently, unsupervised pre-training is gaining increasing popularity in the\nrealm of computational linguistics, thanks to its surprising success in\nadvancing natural language understanding (NLU) and the potential to effectively\nexploit large-scale unlabelled corpus. However, regardless of the success in\nNLU, the power of unsupervised pre-training is only partially excavated when it\ncomes to natural language generation (NLG). The major obstacle stems from an\nidiosyncratic nature of NLG: Texts are usually generated based on certain\ncontext, which may vary with the target applications. As a result, it is\nintractable to design a universal architecture for pre-training as in NLU\nscenarios. Moreover, retaining the knowledge learned from pre-training when\nlearning on the target task is also a non-trivial problem. This review\nsummarizes the recent efforts to enhance NLG systems with unsupervised\npre-training, with a special focus on the methods to catalyse the integration\nof pre-trained models into downstream tasks. They are classified into\narchitecture-based methods and strategy-based methods, based on their way of\nhandling the above obstacle. Discussions are also provided to give further\ninsights into the relationship between these two lines of work, some\ninformative empirical phenomenons, as well as some possible directions where\nfuture work can be devoted to.", "published": "2019-11-13 16:09:59", "link": "http://arxiv.org/abs/1911.06171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word-level Lexical Normalisation using Context-Dependent Embeddings", "abstract": "Lexical normalisation (LN) is the process of correcting each word in a\ndataset to its canonical form so that it may be more easily and more accurately\nanalysed. Most lexical normalisation systems operate at the character-level,\nwhile word-level models are seldom used. Recent language models offer solutions\nto the drawbacks of word-level LN models, yet, to the best of our knowledge, no\nresearch has investigated their effectiveness on LN. In this paper we introduce\na word-level GRU-based LN model and investigate the effectiveness of recent\nembedding techniques on word-level LN. Our results show that our GRU-based\nword-level model produces greater results than character-level models, and\noutperforms existing deep-learning based LN techniques on Twitter data. We also\nfind that randomly-initialised embeddings are capable of outperforming\npre-trained embedding models in certain scenarios. Finally, we release a\nsubstantial lexical normalisation dataset to the community.", "published": "2019-11-13 14:42:55", "link": "http://arxiv.org/abs/1911.06172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robustness to Capitalization Errors in Named Entity Recognition", "abstract": "Robustness to capitalization errors is a highly desirable characteristic of\nnamed entity recognizers, yet we find standard models for the task are\nsurprisingly brittle to such noise. Existing methods to improve robustness to\nthe noise completely discard given orthographic information, mwhich\nsignificantly degrades their performance on well-formed text. We propose a\nsimple alternative approach based on data augmentation, which allows the model\nto \\emph{learn} to utilize or ignore orthographic information depending on its\nusefulness in the context. It achieves competitive robustness to capitalization\nerrors while making negligible compromise to its performance on well-formed\ntext and significantly improving generalization power on noisy user-generated\ntext. Our experiments clearly and consistently validate our claim across\ndifferent types of machine learning models, languages, and dataset sizes.", "published": "2019-11-13 01:52:27", "link": "http://arxiv.org/abs/1911.05241v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LexiPers: An ontology based sentiment lexicon for Persian", "abstract": "Sentiment analysis refers to the use of natural language processing to\nidentify and extract subjective information from textual resources. One\napproach for sentiment extraction is using a sentiment lexicon. A sentiment\nlexicon is a set of words associated with the sentiment orientation that they\nexpress. In this paper, we describe the process of generating a general purpose\nsentiment lexicon for Persian. A new graph-based method is introduced for seed\nselection and expansion based on an ontology. Sentiment lexicon generation is\nthen mapped to a document classification problem. We used the K-nearest\nneighbors and nearest centroid methods for classification. These classifiers\nhave been evaluated based on a set of hand labeled synsets. The final sentiment\nlexicon has been generated by the best classifier. The results show an\nacceptable performance in terms of accuracy and F-measure in the generated\nsentiment lexicon.", "published": "2019-11-13 02:48:33", "link": "http://arxiv.org/abs/1911.05263v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Stable Variational Autoencoder for Text Modelling", "abstract": "Variational Autoencoder (VAE) is a powerful method for learning\nrepresentations of high-dimensional data. However, VAEs can suffer from an\nissue known as latent variable collapse (or KL loss vanishing), where the\nposterior collapses to the prior and the model will ignore the latent codes in\ngenerative tasks. Such an issue is particularly prevalent when employing\nVAE-RNN architectures for text modelling (Bowman et al., 2016). In this paper,\nwe present a simple architecture called holistic regularisation VAE (HR-VAE),\nwhich can effectively avoid latent variable collapse. Compared to existing\nVAE-RNN architectures, we show that our model can achieve much more stable\ntraining process and can generate text with significantly better quality.", "published": "2019-11-13 08:11:42", "link": "http://arxiv.org/abs/1911.05343v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Duplicate Question Detection without Labeled Training Data", "abstract": "Supervised training of neural models to duplicate question detection in\ncommunity Question Answering (cQA) requires large amounts of labeled question\npairs, which are costly to obtain. To minimize this cost, recent works thus\noften used alternative methods, e.g., adversarial domain adaptation. In this\nwork, we propose two novel methods: (1) the automatic generation of duplicate\nquestions, and (2) weak supervision using the title and body of a question. We\nshow that both can achieve improved performances even though they do not\nrequire any labeled data. We provide comprehensive comparisons of popular\ntraining strategies, which provides important insights on how to best train\nmodels in different scenarios. We show that our proposed approaches are more\neffective in many cases because they can utilize larger amounts of unlabeled\ndata from cQA forums. Finally, we also show that our proposed approach for weak\nsupervision with question title and body information is also an effective\nmethod to train cQA answer selection models without direct answer supervision.", "published": "2019-11-13 16:38:30", "link": "http://arxiv.org/abs/1911.05594v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Domain Adaptation on Reading Comprehension", "abstract": "Reading comprehension (RC) has been studied in a variety of datasets with the\nboosted performance brought by deep neural networks. However, the\ngeneralization capability of these models across different domains remains\nunclear. To alleviate this issue, we are going to investigate unsupervised\ndomain adaptation on RC, wherein a model is trained on labeled source domain\nand to be applied to the target domain with only unlabeled samples. We first\nshow that even with the powerful BERT contextual representation, the\nperformance is still unsatisfactory when the model trained on one dataset is\ndirectly applied to another target dataset. To solve this, we provide a novel\nconditional adversarial self-training method (CASe). Specifically, our approach\nleverages a BERT model fine-tuned on the source dataset along with the\nconfidence filtering to generate reliable pseudo-labeled samples in the target\ndomain for self-training. On the other hand, it further reduces domain\ndistribution discrepancy through conditional adversarial learning across\ndomains. Extensive experiments show our approach achieves comparable accuracy\nto supervised models on multiple large-scale benchmark datasets.", "published": "2019-11-13 00:54:39", "link": "http://arxiv.org/abs/1911.06137v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structured Sparsification of Gated Recurrent Neural Networks", "abstract": "Recently, a lot of techniques were developed to sparsify the weights of\nneural networks and to remove networks' structure units, e.g. neurons. We\nadjust the existing sparsification approaches to the gated recurrent\narchitectures. Specifically, in addition to the sparsification of weights and\nneurons, we propose sparsifying the preactivations of gates. This makes some\ngates constant and simplifies LSTM structure. We test our approach on the text\nclassification and language modeling tasks. We observe that the resulting\nstructure of gate sparsity depends on the task and connect the learned\nstructure to the specifics of the particular tasks. Our method also improves\nneuron-wise compression of the model in most of the tasks.", "published": "2019-11-13 16:26:22", "link": "http://arxiv.org/abs/1911.05585v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "The phonetic bases of vocal expressed emotion: natural versus acted", "abstract": "Can vocal emotions be emulated? This question has been a recurrent concern of\nthe speech community, and has also been vigorously investigated. It has been\nfueled further by its link to the issue of validity of acted emotion databases.\nMuch of the speech and vocal emotion research has relied on acted emotion\ndatabases as valid proxies for studying natural emotions. To create models that\ngeneralize to natural settings, it is crucial to work with valid prototypes --\nones that can be assumed to reliably represent natural emotions. More\nconcretely, it is important to study emulated emotions against natural emotions\nin terms of their physiological, and psychological concomitants. In this paper,\nwe present an on-scale systematic study of the differences between natural and\nacted vocal emotions. We use a self-attention based emotion classification\nmodel to understand the phonetic bases of emotions by discovering the most\n'attended' phonemes for each class of emotions. We then compare these\nattended-phonemes in their importance and distribution across acted and natural\nclasses. Our tests show significant differences in the manner and choice of\nphonemes in acted and natural speech, concluding moderate to low validity and\nvalue in using acted speech databases for emotion classification tasks.", "published": "2019-11-13 03:44:08", "link": "http://arxiv.org/abs/1911.05733v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhanced Voice Post Processing Using Voice Decoder Guidance Indicators", "abstract": "Voice enhancement and voice coding are imperative and important functions in\na voice-communication system. However, both functions are commonly treated\nindependently, even though both utilize similar features of the underlying\nsignals. Our proposal is to leverage information from one function to the\nbenefit of the other. Specifically, our proposed changes are focused on changes\nto the voice enhancement at the downlink side and utilizing information of the\nvoice decoding. Preliminary results show that such an approach results in\nimproved quality. Additionally, suggestions are provided on future extensions\nof the proposed concept.", "published": "2019-11-13 15:48:27", "link": "http://arxiv.org/abs/1911.05560v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "3-D Feature and Acoustic Modeling for Far-Field Speech Recognition", "abstract": "Automatic speech recognition in multi-channel reverberant conditions is a\nchallenging task. The conventional way of suppressing the reverberation\nartifacts involves a beamforming based enhancement of the multi-channel speech\nsignal, which is used to extract spectrogram based features for a neural\nnetwork acoustic model. In this paper, we propose to extract features directly\nfrom the multi-channel speech signal using a multi variate autoregressive (MAR)\nmodeling approach, where the correlations among all the three dimensions of\ntime, frequency and channel are exploited. The MAR features are fed to a\nconvolutional neural network (CNN) architecture which performs the joint\nacoustic modeling on the three dimensions. The 3-D CNN architecture allows the\ncombination of multi-channel features that optimize the speech recognition cost\ncompared to the traditional beamforming models that focus on the enhancement\ntask. Experiments are conducted on the CHiME-3 and REVERB Challenge dataset\nusing multi-channel reverberant speech. In these experiments, the proposed 3-D\nfeature and acoustic modeling approach provides significant improvements over\nan ASR system trained with beamformed audio (average relative improvements of\n10 % and 9 % in word error rates for CHiME-3 and REVERB Challenge datasets\nrespectively.", "published": "2019-11-13 14:26:54", "link": "http://arxiv.org/abs/1911.05504v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
