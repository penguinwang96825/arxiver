{"title": "On the Analysis of Cross-Lingual Prompt Tuning for Decoder-based\n  Multilingual Model", "abstract": "An exciting advancement in the field of multilingual models is the emergence\nof autoregressive models with zero- and few-shot capabilities, a phenomenon\nwidely reported in large-scale language models. To further improve model\nadaptation to cross-lingual tasks, another trend is to further fine-tune the\nlanguage models with either full fine-tuning or parameter-efficient tuning.\nHowever, the interaction between parameter-efficient fine-tuning (PEFT) and\ncross-lingual tasks in multilingual autoregressive models has yet to be\nstudied. Specifically, we lack an understanding of the role of linguistic\ndistributions in multilingual models in the effectiveness of token-based prompt\ntuning. To address this question, we conduct experiments comparing prompt\ntuning and fine-tuning on the decoder-based multilingual model, XGLM, with four\ncross-lingual tasks (XNLI, PAWS-X, POS, NER). According to our study, prompt\ntuning achieves on par or better performance over fine-tuning across all\nlanguages while updating at most 0.13\\% of the model parameters. Moreover, we\nempirically show that prompt tuning is more effective in enhancing the\nperformance of low-resource languages than fine-tuning. Our further analysis\nshows that the phenomenon is related to the tokenization scheme of the\nmultilingual model.", "published": "2023-11-14 00:43:33", "link": "http://arxiv.org/abs/2311.07820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fair Abstractive Summarization of Diverse Perspectives", "abstract": "People from different social and demographic groups express diverse\nperspectives and conflicting opinions on a broad set of topics such as product\nreviews, healthcare, law, and politics. A fair summary should provide a\ncomprehensive coverage of diverse perspectives without underrepresenting\ncertain groups. However, current work in summarization metrics and Large\nLanguage Models (LLMs) evaluation has not explored fair abstractive\nsummarization. In this paper, we systematically investigate fair abstractive\nsummarization for user-generated data. We first formally define fairness in\nabstractive summarization as not underrepresenting perspectives of any groups\nof people, and we propose four reference-free automatic metrics by measuring\nthe differences between target and source perspectives. We evaluate nine LLMs,\nincluding three GPT models, four LLaMA models, PaLM 2, and Claude, on six\ndatasets collected from social media, online reviews, and recorded transcripts.\nExperiments show that both the model-generated and the human-written reference\nsummaries suffer from low fairness. We conduct a comprehensive analysis of the\ncommon factors influencing fairness and propose three simple but effective\nmethods to alleviate unfair summarization. Our dataset and code are available\nat https://github.com/psunlpgroup/FairSumm.", "published": "2023-11-14 03:38:55", "link": "http://arxiv.org/abs/2311.07884v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CPopQA: Ranking Cultural Concept Popularity by LLMs", "abstract": "Prior work has demonstrated large language models' (LLMs) potential to\ndiscern statistical tendencies within their pre-training corpora. Despite that,\nmany examinations of LLMs' knowledge capacity focus on knowledge explicitly\nappearing in the training data or implicitly inferable from similar contexts.\nHow well an LLM captures the corpus-level statistical trends of concepts for\nreasoning, especially long-tail ones, is still underexplored. In this study, we\nintroduce a novel few-shot question-answering task (CPopQA) that examines LLMs'\nstatistical ranking abilities for long-tail cultural concepts (e.g., holidays),\nwith a specific focus on these concepts' popularity in the United States and\nthe United Kingdom, respectively. We curate a dataset containing 459 holidays\nacross 58 countries, generating a total of 6,000 QA testing pairs. Experiments\non four strong LLMs show that large models are capable of ranking long-tail\ncultural concepts regarding their statistical tendency. Notably, GPT-3.5\ndisplayed superior performance and exhibited its potential to identify\ngeo-cultural proximity across continents.", "published": "2023-11-14 04:10:40", "link": "http://arxiv.org/abs/2311.07897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated title and abstract screening for scoping reviews using the\n  GPT-4 Large Language Model", "abstract": "Scoping reviews, a type of literature review, require intensive human effort\nto screen large numbers of scholarly sources for their relevance to the review\nobjectives. This manuscript introduces GPTscreenR, a package for the R\nstatistical programming language that uses the GPT-4 Large Language Model (LLM)\nto automatically screen sources. The package makes use of the chain-of-thought\ntechnique with the goal of maximising performance on complex screening tasks.\nIn validation against consensus human reviewer decisions, GPTscreenR performed\nsimilarly to an alternative zero-shot technique, with a sensitivity of 71%,\nspecificity of 89%, and overall accuracy of 84%. Neither method achieved\nperfect accuracy nor human levels of intraobserver agreement. GPTscreenR\ndemonstrates the potential for LLMs to support scholarly work and provides a\nuser-friendly software framework that can be integrated into existing review\nprocesses.", "published": "2023-11-14 05:30:43", "link": "http://arxiv.org/abs/2311.07918v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's All Relative! -- A Synthetic Query Generation Approach for\n  Improving Zero-Shot Relevance Prediction", "abstract": "Recent developments in large language models (LLMs) have shown promise in\ntheir ability to generate synthetic query-document pairs by prompting with as\nfew as 8 demonstrations. This has enabled building better IR models, especially\nfor tasks with no training data readily available. Typically, such synthetic\nquery generation (QGen) approaches condition on an input context (e.g. a text\ndocument) and generate a query relevant to that context, or condition the QGen\nmodel additionally on the relevance label (e.g. relevant vs irrelevant) to\ngenerate queries across relevance buckets. However, we find that such QGen\napproaches are sub-optimal as they require the model to reason about the\ndesired label and the input from a handful of examples. In this work, we\npropose to reduce this burden of LLMs by generating queries simultaneously for\ndifferent labels. We hypothesize that instead of asking the model to generate,\nsay, an irrelevant query given an input context, asking the model to generate\nan irrelevant query relative to a relevant query is a much simpler task setup\nfor the model to reason about. Extensive experimentation across seven IR\ndatasets shows that synthetic queries generated in such a fashion translates to\na better downstream performance, suggesting that the generated queries are\nindeed of higher quality.", "published": "2023-11-14 06:16:49", "link": "http://arxiv.org/abs/2311.07930v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "First-Step Advantage: Importance of Starting Right in Multi-Step Math\n  Reasoning", "abstract": "Language models can solve complex reasoning tasks better by learning to\ngenerate rationales for their predictions. Often these models know how to solve\na task but their auto-regressive decoding nature leads to incorrect results if\nthey start incorrectly. We observe that smaller models in particular when\ncorrected, can solve a task that they would have otherwise struggled with. We\ndemonstrate this phenomenon by using a larger model to guide smaller models,\nwhich leads to significantly improved performance (up to +24 points on the\nGSM8K dataset by 7B models). To assist smaller models in initiating the\nstarting step, we propose QuestCoT, where a smaller model first asks itself how\nto start, before proceeding with a chain of reasoning. On various multistep\nmathematical reasoning datasets over multiple smaller models, we show that\ngetting the right start can lead to significant performance gains across all\nmodels (gains of up to +6 points on GSM8K, +9 on SVAMP, +5 on ASDiv, and +7 on\nMultiArith).", "published": "2023-11-14 06:45:31", "link": "http://arxiv.org/abs/2311.07945v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The ART of LLM Refinement: Ask, Refine, and Trust", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ngenerative abilities, but can they judge the quality of their own generations?\nA popular concept, referred to as self-refinement, postulates that LLMs can\ndetect and correct the errors in their generations when asked to do so.\nHowever, recent empirical evidence points in the opposite direction, suggesting\nthat LLMs often struggle to accurately identify errors when reasoning is\ninvolved. To address this, we propose a reasoning with refinement objective\ncalled ART: Ask, Refine, and Trust, which asks necessary questions to decide\nwhen an LLM should refine its output, and either affirm or withhold trust in\nits refinement by ranking the refinement and the initial prediction. On two\nmultistep reasoning tasks of mathematical word problems (GSM8K) and question\nanswering (StrategyQA), ART achieves a performance gain of +5 points over\nself-refinement baselines, while using a much smaller model as the decision\nmaker. We also demonstrate the benefit of using smaller models to make\nrefinement decisions as a cost-effective alternative to fine-tuning a larger\nmodel.", "published": "2023-11-14 07:26:32", "link": "http://arxiv.org/abs/2311.07961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Do Text Embedding Models Understand Syntax?", "abstract": "Text embedding models have significantly contributed to advancements in\nnatural language processing by adeptly capturing semantic properties of textual\ndata. However, the ability of these models to generalize across a wide range of\nsyntactic contexts remains under-explored. In this paper, we first develop an\nevaluation set, named \\textbf{SR}, to scrutinize the capability for syntax\nunderstanding of text embedding models from two crucial syntactic aspects:\nStructural heuristics, and Relational understanding among concepts, as revealed\nby the performance gaps in previous studies. Our findings reveal that existing\ntext embedding models have not sufficiently addressed these syntactic\nunderstanding challenges, and such ineffectiveness becomes even more apparent\nwhen evaluated against existing benchmark datasets. Furthermore, we conduct\nrigorous analysis to unearth factors that lead to such limitations and examine\nwhy previous evaluations fail to detect such ineffectiveness. Lastly, we\npropose strategies to augment the generalization ability of text embedding\nmodels in diverse syntactic scenarios. This study serves to highlight the\nhurdles associated with syntactic generalization and provides pragmatic\nguidance for boosting model performance across varied syntactic contexts.", "published": "2023-11-14 08:51:00", "link": "http://arxiv.org/abs/2311.07996v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Forgetting before Learning: Utilizing Parametric Arithmetic for\n  Knowledge Updating in Large Language Models", "abstract": "Recent advancements in Large Language Models (LLMs) have showcased their\nremarkable capabilities in text understanding and generation. However, even\nstronger LLMs are susceptible to acquiring erroneous or obsolete information\nfrom the training corpus. Direct secondary fine-tuning with data containing new\nknowledge may be ineffective in updating knowledge due to the conflict between\nold and new knowledge. In this paper, we propose a new paradigm for fine-tuning\ncalled F-Learning (Forgetting before Learning), which employs parametric\narithmetic to facilitate the forgetting of old knowledge and learning of new\nknowledge. Experimental results on two publicly available datasets demonstrate\nthat our proposed F-Learning can obviously improve the knowledge updating\nperformance of both full fine-tuning and LoRA fine-tuning, simultaneously\noutperforming the existing baselines in most cases. Moreover, we have also\ndiscovered that forgetting old knowledge by subtracting the parameters of LoRA\ncan yield a similar effect to subtracting the parameters of full fine-tuning,\nand occasionally even surpass it significantly.", "published": "2023-11-14 09:12:40", "link": "http://arxiv.org/abs/2311.08011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving In-context Learning of Multilingual Generative Language Models\n  with Cross-lingual Alignment", "abstract": "Multilingual generative models obtain remarkable cross-lingual in-context\nlearning capabilities through pre-training on large-scale corpora. However,\nthey still exhibit a performance bias toward high-resource languages and learn\nisolated distributions of multilingual sentence representations, which may\nhinder knowledge transfer across languages. To bridge this gap, we propose a\nsimple yet effective cross-lingual alignment framework exploiting pairs of\ntranslation sentences. It aligns the internal sentence representations across\ndifferent languages via multilingual contrastive learning and aligns outputs by\nfollowing cross-lingual instructions in the target language. Experimental\nresults show that even with less than 0.1 {\\textperthousand} of pre-training\ntokens, our alignment framework significantly boosts the cross-lingual\nabilities of generative language models and mitigates the performance gap.\nFurther analyses reveal that it results in a better internal multilingual\nrepresentation distribution of multilingual models.", "published": "2023-11-14 11:24:08", "link": "http://arxiv.org/abs/2311.08089v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language\n  Models", "abstract": "The dynamic nature of knowledge in an ever-changing world presents challenges\nfor language models trained on static data; the model in the real world often\nrequires not only acquiring new knowledge but also overwriting outdated\ninformation into updated ones. To study the ability of language models for\nthese time-dependent dynamics in human language, we introduce a novel task,\nEvolvingQA, a temporally evolving question-answering benchmark designed for\ntraining and evaluating LMs on an evolving Wikipedia database. The construction\nof EvolvingQA is automated with our pipeline using large language models. We\nuncover that existing continual learning baselines suffer from updating and\nremoving outdated knowledge. Our analysis suggests that models fail to rectify\nknowledge due to small weight gradients. In addition, we elucidate that\nlanguage models particularly struggle to reflect the change of numerical or\ntemporal information. Our work aims to model the dynamic nature of real-world\ninformation, suggesting faithful evaluations of the evolution-adaptability of\nlanguage models.", "published": "2023-11-14 12:12:02", "link": "http://arxiv.org/abs/2311.08106v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training\n  with Adversarial Remarks", "abstract": "Large Language Models (LLMs) can justify or critique their predictions\nthrough discussions with other models or humans, thereby enriching their\nintrinsic understanding of instances. While proactive discussions in the\ninference phase have been shown to boost performance, such interactions have\nnot been extensively explored during the training phase. We hypothesize that\nincorporating interactive discussions into the training process can enhance the\nmodels' understanding and improve their reasoning and verbal expression\nabilities during inference. This work introduces the SAIE framework, which\nfacilitates supportive and adversarial discussions between learner and partner\nmodels. The learner model receives responses from the partner, and its\nparameters are then updated based on this discussion. This dynamic adjustment\nprocess continues throughout the training phase, responding to the evolving\noutputs of the learner model. Our empirical evaluation across various tasks,\nincluding math problems, commonsense reasoning, and multi-domain knowledge,\ndemonstrates that models fine-tuned with the SAIE framework outperform those\ntrained with conventional fine-tuning approaches. Furthermore, our method\nenhances the models' reasoning capabilities, improving both individual and\nmulti-agent inference performance.", "published": "2023-11-14 12:12:25", "link": "http://arxiv.org/abs/2311.08107v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Insights into Classifying and Mitigating LLMs' Hallucinations", "abstract": "The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.", "published": "2023-11-14 12:30:28", "link": "http://arxiv.org/abs/2311.08117v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sinkhorn Transformations for Single-Query Postprocessing in Text-Video\n  Retrieval", "abstract": "A recent trend in multimodal retrieval is related to postprocessing test set\nresults via the dual-softmax loss (DSL). While this approach can bring\nsignificant improvements, it usually presumes that an entire matrix of test\nsamples is available as DSL input. This work introduces a new postprocessing\napproach based on Sinkhorn transformations that outperforms DSL. Further, we\npropose a new postprocessing setting that does not require access to multiple\ntest queries. We show that our approach can significantly improve the results\nof state of the art models such as CLIP4Clip, BLIP, X-CLIP, and DRL, thus\nachieving a new state-of-the-art on several standard text-video retrieval\ndatasets both with access to the entire test set and in the single-query\nsetting.", "published": "2023-11-14 13:20:23", "link": "http://arxiv.org/abs/2311.08143v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review\n  Collaboration", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in general\nnatural language processing tasks but often fall short in complex reasoning\ntasks. Recent studies have explored human-like problem-solving strategies, such\nas self-correct, to push further the boundary of single-model reasoning\nability. In this work, we let a single model \"step outside the box\" by engaging\nmultiple models to correct each other. We introduce a multi-agent collaboration\nstrategy that emulates the academic peer review process. Each agent\nindependently constructs its own solution, provides reviews on the solutions of\nothers, and assigns confidence levels to its reviews. Upon receiving peer\nreviews, agents revise their initial solutions. Extensive experiments on three\ndifferent types of reasoning tasks show that our collaboration approach\ndelivers superior accuracy across all ten datasets compared to existing\nmethods. Further study underscores the effectiveness of integrating confidence\nin reviews, demonstrates the superiority of feedback exchange over mere\nsolution sharing, and highlights the role of capability and diversity in\nfostering successful collaboration.", "published": "2023-11-14 13:27:07", "link": "http://arxiv.org/abs/2311.08152v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "All Data on the Table: Novel Dataset and Benchmark for Cross-Modality\n  Scientific Information Extraction", "abstract": "Extracting key information from scientific papers has the potential to help\nresearchers work more efficiently and accelerate the pace of scientific\nprogress. Over the last few years, research on Scientific Information\nExtraction (SciIE) witnessed the release of several new systems and benchmarks.\nHowever, existing paper-focused datasets mostly focus only on specific parts of\na manuscript (e.g., abstracts) and are single-modality (i.e., text- or\ntable-only), due to complex processing and expensive annotations. Moreover,\ncore information can be present in either text or tables or across both. To\nclose this gap in data availability and enable cross-modality IE, while\nalleviating labeling costs, we propose a semi-supervised pipeline for\nannotating entities in text, as well as entities and relations in tables, in an\niterative procedure. Based on this pipeline, we release novel resources for the\nscientific community, including a high-quality benchmark, a large-scale corpus,\nand a semi-supervised annotation pipeline. We further report the performance of\nstate-of-the-art IE models on the proposed benchmark dataset, as a baseline.\nLastly, we explore the potential capability of large language models such as\nChatGPT for the current task. Our new dataset, results, and analysis validate\nthe effectiveness and efficiency of our semi-supervised pipeline, and we\ndiscuss its remaining limitations.", "published": "2023-11-14 14:22:47", "link": "http://arxiv.org/abs/2311.08189v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GEC-DePenD: Non-Autoregressive Grammatical Error Correction with\n  Decoupled Permutation and Decoding", "abstract": "Grammatical error correction (GEC) is an important NLP task that is currently\nusually solved with autoregressive sequence-to-sequence models. However,\napproaches of this class are inherently slow due to one-by-one token\ngeneration, so non-autoregressive alternatives are needed. In this work, we\npropose a novel non-autoregressive approach to GEC that decouples the\narchitecture into a permutation network that outputs a self-attention weight\nmatrix that can be used in beam search to find the best permutation of input\ntokens (with auxiliary {ins} tokens) and a decoder network based on a\nstep-unrolled denoising autoencoder that fills in specific tokens. This allows\nus to find the token permutation after only one forward pass of the permutation\nnetwork, avoiding autoregressive constructions. We show that the resulting\nnetwork improves over previously known non-autoregressive methods for GEC and\nreaches the level of autoregressive methods that do not use language-specific\nsynthetic data generation methods. Our results are supported by a comprehensive\nexperimental validation on the ConLL-2014 and Write&Improve+LOCNESS datasets\nand an extensive ablation study that supports our architectural and algorithmic\nchoices.", "published": "2023-11-14 14:24:36", "link": "http://arxiv.org/abs/2311.08191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Using Distribution-Based Compositionality Assessment to Evaluate\n  Compositional Generalisation in Machine Translation", "abstract": "Compositional generalisation (CG), in NLP and in machine learning more\ngenerally, has been assessed mostly using artificial datasets. It is important\nto develop benchmarks to assess CG also in real-world natural language tasks in\norder to understand the abilities and limitations of systems deployed in the\nwild. To this end, our GenBench Collaborative Benchmarking Task submission\nutilises the distribution-based compositionality assessment (DBCA) framework to\nsplit the Europarl translation corpus into a training and a test set in such a\nway that the test set requires compositional generalisation capacity.\nSpecifically, the training and test sets have divergent distributions of\ndependency relations, testing NMT systems' capability of translating\ndependencies that they have not been trained on. This is a fully-automated\nprocedure to create natural language compositionality benchmarks, making it\nsimple and inexpensive to apply it further to other datasets and languages. The\ncode and data for the experiments is available at\nhttps://github.com/aalto-speech/dbca.", "published": "2023-11-14 15:37:19", "link": "http://arxiv.org/abs/2311.08249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads\n  to Answers Faster", "abstract": "In this work, we propose FastCoT, a model-agnostic framework based on\nparallel decoding without any further training of an auxiliary model or\nmodification to the LLM itself. FastCoT uses a size-varying context window\nwhose size changes with position to conduct parallel decoding and\nauto-regressive decoding simultaneously, thus fully utilizing GPU computation\nresources. In FastCoT, the parallel decoding part provides the LLM with a quick\nglance of the future composed of approximate tokens, which could lead to faster\nanswers compared to regular autoregressive decoding used by causal\ntransformers. We also provide an implementation of parallel decoding within\nLLM, which supports KV-cache generation and batch processing. Through extensive\nexperiments, we demonstrate that FastCoT saves inference time by nearly 20%\nwith only a negligible performance drop compared to the regular approach.\nAdditionally, we show that the context window size exhibits considerable\nrobustness for different tasks.", "published": "2023-11-14 15:56:18", "link": "http://arxiv.org/abs/2311.08263v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily", "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially\nharmful content. Exploring jailbreak prompts can help to better reveal the\nweaknesses of LLMs and further steer us to secure them. Unfortunately, existing\njailbreak methods either suffer from intricate manual design or require\noptimization on other white-box models, which compromises either generalization\nor efficiency. In this paper, we generalize jailbreak prompt attacks into two\naspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we\npropose ReNeLLM, an automatic framework that leverages LLMs themselves to\ngenerate effective jailbreak prompts. Extensive experiments demonstrate that\nReNeLLM significantly improves the attack success rate while greatly reducing\nthe time cost compared to existing baselines. Our study also reveals the\ninadequacy of current defense methods in safeguarding LLMs. Finally, we analyze\nthe failure of LLMs defense from the perspective of prompt execution priority,\nand propose corresponding defense strategies. We hope that our research can\ncatalyze both the academic community and LLMs developers towards the provision\nof safer and more regulated LLMs. The code is available at\nhttps://github.com/NJUNLP/ReNeLLM.", "published": "2023-11-14 16:02:16", "link": "http://arxiv.org/abs/2311.08268v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Examining Modularity in Multilingual LMs via Language-Specialized\n  Subnetworks", "abstract": "Recent work has proposed explicitly inducing language-wise modularity in\nmultilingual LMs via sparse fine-tuning (SFT) on per-language subnetworks as a\nmeans of better guiding cross-lingual sharing. In this work, we investigate (1)\nthe degree to which language-wise modularity naturally arises within models\nwith no special modularity interventions, and (2) how cross-lingual sharing and\ninterference differ between such models and those with explicit SFT-guided\nsubnetwork modularity. To quantify language specialization and cross-lingual\ninteraction, we use a Training Data Attribution method that estimates the\ndegree to which a model's predictions are influenced by in-language or\ncross-language training examples. Our results show that language-specialized\nsubnetworks do naturally arise, and that SFT, rather than always increasing\nmodularity, can decrease language specialization of subnetworks in favor of\nmore cross-lingual sharing.", "published": "2023-11-14 16:11:23", "link": "http://arxiv.org/abs/2311.08273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Do Large Language Models Understand Syntax? An Evaluation by\n  Asking Natural Language Questions", "abstract": "While recent advancements in large language models (LLMs) bring us closer to\nachieving artificial general intelligence, the question persists: Do LLMs truly\nunderstand language, or do they merely mimic comprehension through pattern\nrecognition? This study seeks to explore this question through the lens of\nsyntax, a crucial component of sentence comprehension. Adopting a natural\nlanguage question-answering (Q&A) scheme, we craft questions targeting nine\nsyntactic knowledge points that are most closely related to sentence\ncomprehension. Experiments conducted on 24 LLMs suggest that most have a\nlimited grasp of syntactic knowledge, exhibiting notable discrepancies across\ndifferent syntactic knowledge points. In particular, questions involving\nprepositional phrase attachment pose the greatest challenge, whereas those\nconcerning adjectival modifier and indirect object are relatively easier for\nLLMs to handle. Furthermore, a case study on the training dynamics of the LLMs\nreveals that the majority of syntactic knowledge is learned during the initial\nstages of training, hinting that simply increasing the number of training\ntokens may not be the `silver bullet' for improving the comprehension ability\nof LLMs.", "published": "2023-11-14 16:30:36", "link": "http://arxiv.org/abs/2311.08287v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On-the-Fly Fusion of Large Language Models and Machine Translation", "abstract": "We propose the on-the-fly ensembling of a machine translation model with an\nLLM, prompted on the same task and input. We perform experiments on 4 language\npairs (both directions) with varying data amounts. We find that a slightly\nweaker-at-translation LLM can improve translations of a NMT model, and\nensembling with an LLM can produce better translations than ensembling two\nstronger MT models. We combine our method with various techniques from LLM\nprompting, such as in context learning and translation context.", "published": "2023-11-14 16:49:33", "link": "http://arxiv.org/abs/2311.08306v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KTRL+F: Knowledge-Augmented In-Document Search", "abstract": "We introduce a new problem KTRL+F, a knowledge-augmented in-document search\ntask that necessitates real-time identification of all semantic targets within\na document with the awareness of external sources through a single natural\nquery. KTRL+F addresses following unique challenges for in-document search:\n1)utilizing knowledge outside the document for extended use of additional\ninformation about targets, and 2) balancing between real-time applicability\nwith the performance. We analyze various baselines in KTRL+F and find\nlimitations of existing models, such as hallucinations, high latency, or\ndifficulties in leveraging external knowledge. Therefore, we propose a\nKnowledge-Augmented Phrase Retrieval model that shows a promising balance\nbetween speed and performance by simply augmenting external knowledge in phrase\nembedding. We also conduct a user study to verify whether solving KTRL+F can\nenhance search experience for users. It demonstrates that even with our simple\nmodel, users can reduce the time for searching with less queries and reduced\nextra visits to other sources for collecting evidence. We encourage the\nresearch community to work on KTRL+F to enhance more efficient in-document\ninformation access.", "published": "2023-11-14 17:18:08", "link": "http://arxiv.org/abs/2311.08329v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority\n  Languages in China", "abstract": "Current large language models demonstrate deficiencies in understanding\nlow-resource languages, particularly the minority languages in China. This\nlimitation stems from the scarcity of available pre-training data. To address\nthis accessibility challenge, we present MC$^2$, a Multilingual Corpus of\nMinority Languages in China, which is the largest open-source corpus of its\nkind so far. MC$^2$ includes four underrepresented languages: Tibetan, Uyghur,\nKazakh, and Mongolian. Notably, we focus on the less common writing systems of\nKazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian\nscript, respectively, which have been long neglected in previous corpus\nconstruction efforts. Recognizing the prevalence of language contamination\nwithin existing corpora, we adopt a quality-centric solution for collecting\nMC$^2$, prioritizing accuracy while enhancing diversity. Furthermore, we\nunderscore the importance of attending to the multiplicity of writing systems,\nwhich is closely related to the cultural awareness of the resulting models. The\nMC$^2$ corpus and related models are made public to the community.", "published": "2023-11-14 17:45:50", "link": "http://arxiv.org/abs/2311.08348v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AI-generated text boundary detection with RoFT", "abstract": "Due to the rapid development of large language models, people increasingly\noften encounter texts that may start as written by a human but continue as\nmachine-generated. Detecting the boundary between human-written and\nmachine-generated parts of such texts is a challenging problem that has not\nreceived much attention in literature. We attempt to bridge this gap and\nexamine several ways to adapt state of the art artificial text detection\nclassifiers to the boundary detection setting. We push all detectors to their\nlimits, using the Real or Fake text benchmark that contains short texts on\nseveral topics and includes generations of various language models. We use this\ndiversity to deeply examine the robustness of all detectors in cross-domain and\ncross-model settings to provide baselines and insights for future research. In\nparticular, we find that perplexity-based approaches to boundary detection tend\nto be more robust to peculiarities of domain-specific data than supervised\nfine-tuning of the RoBERTa model; we also find which features of the text\nconfuse boundary detection algorithms and negatively influence their\nperformance in cross-domain settings.", "published": "2023-11-14 17:48:19", "link": "http://arxiv.org/abs/2311.08349v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How You Prompt Matters! Even Task-Oriented Constraints in Instructions\n  Affect LLM-Generated Text Detection", "abstract": "To combat the misuse of Large Language Models (LLMs), many recent studies\nhave presented LLM-generated-text detectors with promising performance. When\nusers instruct LLMs to generate texts, the instruction can include different\nconstraints depending on the user's need. However, most recent studies do not\ncover such diverse instruction patterns when creating datasets for LLM\ndetection. In this paper, we reveal that even task-oriented constraints --\nconstraints that would naturally be included in an instruction and are not\nrelated to detection-evasion -- cause existing powerful detectors to have a\nlarge variance in detection performance. We focus on student essay writing as a\nrealistic domain and manually create task-oriented constraints based on several\nfactors for essay quality. Our experiments show that the standard deviation\n(SD) of current detector performance on texts generated by an instruction with\nsuch a constraint is significantly larger (up to an SD of 14.4 F1-score) than\nthat by generating texts multiple times or paraphrasing the instruction. We\nalso observe an overall trend where the constraints can make LLM detection more\nchallenging than without them. Finally, our analysis indicates that the high\ninstruction-following ability of LLMs fosters the large impact of such\nconstraints on detection performance.", "published": "2023-11-14 18:32:52", "link": "http://arxiv.org/abs/2311.08369v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in\n  Large Language Models", "abstract": "The past year has seen rapid acceleration in the development of large\nlanguage models (LLMs). However, without proper steering and safeguards, LLMs\nwill readily follow malicious instructions, provide unsafe advice, and generate\ntoxic content. We introduce SimpleSafetyTests (SST) as a new test suite for\nrapidly and systematically identifying such critical safety risks. The test\nsuite comprises 100 test prompts across five harm areas that LLMs, for the vast\nmajority of applications, should refuse to comply with. We test 11 open-access\nand open-source LLMs and four closed-source LLMs, and find critical safety\nweaknesses. While some of the models do not give a single unsafe response, most\ngive unsafe responses to more than 20% of the prompts, with over 50% unsafe\nresponses in the extreme. Prepending a safety-emphasising system prompt\nsubstantially reduces the occurrence of unsafe responses, but does not\ncompletely stop them from happening. Trained annotators labelled every model\nresponse to SST (n = 3,000). We use these annotations to evaluate five AI\nsafety filters (which assess whether a models' response is unsafe given a\nprompt) as a way of automatically evaluating models' performance on SST. The\nfilters' performance varies considerably. There are also differences across the\nfive harm areas, and on the unsafe versus safe responses. The widely-used\nPerspective API has 72% accuracy and a newly-created zero-shot prompt to\nOpenAI's GPT-4 performs best with 89% accuracy. Content Warning: This paper\ncontains prompts and responses that relate to child abuse, suicide, self-harm\nand eating disorders, scams and fraud, illegal items, and physical harm.", "published": "2023-11-14 18:33:43", "link": "http://arxiv.org/abs/2311.08370v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts", "abstract": "In the realm of text manipulation and linguistic transformation, the question\nof authorship has been a subject of fascination and philosophical inquiry. Much\nlike the Ship of Theseus paradox, which ponders whether a ship remains the same\nwhen each of its original planks is replaced, our research delves into an\nintriguing question: Does a text retain its original authorship when it\nundergoes numerous paraphrasing iterations? Specifically, since Large Language\nModels (LLMs) have demonstrated remarkable proficiency in both the generation\nof original content and the modification of human-authored texts, a pivotal\nquestion emerges concerning the determination of authorship in instances where\nLLMs or similar paraphrasing tools are employed to rephrase the text--i.e.,\nwhether authorship should be attributed to the original human author or the\nAI-powered tool. Therefore, we embark on a philosophical voyage through the\nseas of language and authorship to unravel this intricate puzzle. Using a\ncomputational approach, we discover that the diminishing performance in text\nclassification models, with each successive paraphrasing iteration, is closely\nassociated with the extent of deviation from the original author's style, thus\nprovoking a reconsideration of the current notion of authorship.", "published": "2023-11-14 18:40:42", "link": "http://arxiv.org/abs/2311.08374v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Direct Preference Optimization for Neural Machine Translation with\n  Minimum Bayes Risk Decoding", "abstract": "Minimum Bayes Risk (MBR) decoding can significantly improve translation\nperformance of Multilingual Large Language Models (MLLMs). However, MBR\ndecoding is computationally expensive. We show how the recently developed\nReinforcement Learning technique, Direct Preference Optimization (DPO), can\nfine-tune MLLMs to get the gains of MBR without any additional computation in\ninference. Our method uses only a small monolingual fine-tuning set and yields\nsignificantly improved performance on multiple NMT test sets compared to MLLMs\nwithout DPO.", "published": "2023-11-14 18:43:51", "link": "http://arxiv.org/abs/2311.08380v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aligning Large Language Models with Human Opinions through Persona\n  Selection and Value--Belief--Norm Reasoning", "abstract": "Reasoning and predicting human opinions with large language models (LLMs) is\nessential yet challenging. Current methods employ role-playing with personae\nbut face two major issues: LLMs are sensitive to even a single irrelevant\npersona, skewing predictions by up to 30%, and LLMs fail to reason\nstrategically over personae. We propose Chain-of-Opinion (COO), a simple\nfour-step solution modeling which and how to reason with personae, inspired by\nthe Value--Belief--Norm (VBN) theory. COO differentiates between explicit\npersonae (demographics and ideology) and implicit personae (historical\nopinions), involves: (1) filtering irrelevant attributes from explicit\npersonae, (2) ranking implicit personae into a preferential list for selecting\ntop-k, (3) applying novel VBN reasoning to extract user environmental and\npersonal value, belief, and norm variables for accurate and reliable\npredictions, and (4) iterating VBN reasoning with progressively larger lists of\nimplicit personae to handle potential persona insufficiency. COO efficiently\nachieves new state-of-the-art opinion prediction via prompting with only 5\ninference calls, improving prior techniques by up to 4%. Notably, fine-tuning\nLMs with COO data results in significantly better opinion-aligned models, by up\nto 23%.", "published": "2023-11-14 18:48:27", "link": "http://arxiv.org/abs/2311.08385v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting Text Preference Via Structured Comparative Reasoning", "abstract": "Comparative reasoning plays a crucial role in text preference prediction;\nhowever, large language models (LLMs) often demonstrate inconsistencies in\ntheir reasoning. While approaches like Chain-of-Thought improve accuracy in\nmany other settings, they struggle to consistently distinguish the similarities\nand differences of complex texts. We introduce SC, a prompting approach that\npredicts text preferences by generating structured intermediate comparisons. SC\nbegins by proposing aspects of comparison, followed by generating textual\ncomparisons under each aspect. We select consistent comparisons with a pairwise\nconsistency comparator that ensures each aspect's comparisons clearly\ndistinguish differences between texts, significantly reducing hallucination and\nimproving consistency. Our comprehensive evaluations across various NLP tasks,\nincluding summarization, retrieval, and automatic rating, demonstrate that SC\nequips LLMs to achieve state-of-the-art performance in text preference\nprediction.", "published": "2023-11-14 18:51:38", "link": "http://arxiv.org/abs/2311.08390v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Material Lens on Coloniality in NLP", "abstract": "Coloniality, the continuation of colonial harms beyond \"official\"\ncolonization, has pervasive effects across society and scientific fields.\nNatural Language Processing (NLP) is no exception to this broad phenomenon. In\nthis work, we argue that coloniality is implicitly embedded in and amplified by\nNLP data, algorithms, and software. We formalize this analysis using\nActor-Network Theory (ANT): an approach to understanding social phenomena\nthrough the network of relationships between human stakeholders and technology.\nWe use our Actor-Network to guide a quantitative survey of the geography of\ndifferent phases of NLP research, providing evidence that inequality along\ncolonial boundaries increases as NLP builds on itself. Based on this, we argue\nthat combating coloniality in NLP requires not only changing current values but\nalso active work to remove the accumulation of colonial ideals in our\nfoundational data and algorithms.", "published": "2023-11-14 18:52:09", "link": "http://arxiv.org/abs/2311.08391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations", "abstract": "Language technologies that accurately model the dynamics of events must\nperform commonsense reasoning. Existing work evaluating commonsense reasoning\nfocuses on making inferences about common, everyday situations. To instead\ninvestigate the ability to model unusual, unexpected, and unlikely situations,\nwe explore the task of uncommonsense abductive reasoning. Given a piece of\ncontext with an unexpected outcome, this task requires reasoning abductively to\ngenerate an explanation that makes the unexpected outcome more likely in the\ncontext. To this end, we curate and release a new English language corpus\ncalled UNcommonsense. We characterize the performance differences between human\nexplainers and the best-performing large language models, finding that\nmodel-enhanced human-written explanations achieve the highest quality by\ntrading off between specificity and diversity. Finally, we experiment with\nseveral imitation learning algorithms to train open and accessible language\nmodels on this task. When compared with the vanilla supervised fine-tuning\napproach, these methods consistently reduce lose rates on both common and\nuncommonsense abductive reasoning judged by human evaluators.", "published": "2023-11-14 19:00:55", "link": "http://arxiv.org/abs/2311.08469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Selecting Shots for Demographic Fairness in Few-Shot Learning with Large\n  Language Models", "abstract": "Recently, work in NLP has shifted to few-shot (in-context) learning, with\nlarge language models (LLMs) performing well across a range of tasks. However,\nwhile fairness evaluations have become a standard for supervised methods,\nlittle is known about the fairness of LLMs as prediction systems. Further,\ncommon standard methods for fairness involve access to models weights or are\napplied during finetuning, which are not applicable in few-shot learning. Do\nLLMs exhibit prediction biases when used for standard NLP tasks? In this work,\nwe explore the effect of shots, which directly affect the performance of\nmodels, on the fairness of LLMs as NLP classification systems. We consider how\ndifferent shot selection strategies, both existing and new demographically\nsensitive methods, affect model fairness across three standard fairness\ndatasets. We discuss how future work can include LLM fairness evaluations.", "published": "2023-11-14 19:02:03", "link": "http://arxiv.org/abs/2311.08472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Functionality learning through specification instructions", "abstract": "Test suites assess natural language processing models' performance on\nspecific functionalities: cases of interest involving model robustness,\nfairness, or particular linguistic capabilities. This paper introduces\nspecification instructions: text descriptions specifying fine-grained\ntask-specific behaviors. For each functionality in a suite, we generate an\ninstruction that describes it. We combine the specification instructions to\ncreate specification-augmented prompts, which we feed to language models\npre-trained on natural instruction data.\n  We conduct experiments to measure how optimizing for some functionalities may\nnegatively impact functionalities that are not covered by the specification\nset. Our analyses across four tasks and models of diverse sizes and families\nshow that smaller models struggle to follow specification instructions.\nHowever, larger models (>~3B params.) can benefit from specifications and --\nsurprisingly -- even generalize certain desirable behaviors across\nfunctionalities.", "published": "2023-11-14 19:15:55", "link": "http://arxiv.org/abs/2311.08481v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Structured Chain-of-Thought: Integrating Multiple Sources of\n  Knowledge for Improved Language Model Reasoning", "abstract": "An important open question in the use of large language models for\nknowledge-intensive tasks is how to effectively integrate knowledge from three\nsources: the model's parametric memory, external structured knowledge, and\nexternal unstructured knowledge. Most existing prompting methods either rely on\none or two of these sources, or require repeatedly invoking large language\nmodels to generate similar or identical content. In this work, we overcome\nthese limitations by introducing a novel semi-structured prompting approach\nthat seamlessly integrates the model's parametric memory with unstructured\nknowledge from text documents and structured knowledge from knowledge graphs.\nExperimental results on open-domain multi-hop question answering datasets\ndemonstrate that our prompting method significantly surpasses existing\ntechniques, even exceeding those that require fine-tuning.", "published": "2023-11-14 19:53:53", "link": "http://arxiv.org/abs/2311.08505v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoRE-CoG: Conversational Recommendation of Entities using Constrained\n  Generation", "abstract": "End-to-end conversational recommendation systems (CRS) generate responses by\nleveraging both dialog history and a knowledge base (KB). A CRS mainly faces\nthree key challenges: (1) at each turn, it must decide if recommending a KB\nentity is appropriate; if so, it must identify the most relevant KB entity to\nrecommend; and finally, it must recommend the entity in a fluent utterance that\nis consistent with the conversation history. Recent CRSs do not pay sufficient\nattention to these desiderata, often generating unfluent responses or not\nrecommending (relevant) entities at the right turn. We introduce a new CRS we\ncall CoRE-CoG. CoRE-CoG addresses the limitations in prior systems by\nimplementing (1) a recommendation trigger that decides if the system utterance\nshould include an entity, (2) a type pruning module that improves the relevance\nof recommended entities, and (3) a novel constrained response generator to make\nrecommendations while maintaining fluency. Together, these modules ensure\nsimultaneous accurate recommendation decisions and fluent system utterances.\nExperiments with recent benchmarks show the superiority particularly on\nconditional generation sub-tasks with close to 10 F1 and 4 Recall@1 percent\npoints gain over baselines.", "published": "2023-11-14 20:07:34", "link": "http://arxiv.org/abs/2311.08511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending Multilingual Machine Translation through Imitation Learning", "abstract": "Despite the growing variety of languages supported by existing multilingual\nneural machine translation (MNMT) models, most of the world's languages are\nstill being left behind. We aim to extend large-scale MNMT models to a new\nlanguage, allowing for translation between the newly added and all of the\nalready supported languages in a challenging scenario: using only a parallel\ncorpus between the new language and English. Previous approaches, such as\ncontinued training on parallel data including the new language, suffer from\ncatastrophic forgetting (i.e., performance on other languages is reduced). Our\nnovel approach Imit-MNMT treats the task as an imitation learning process,\nwhich mimicks the behavior of an expert, a technique widely used in the\ncomputer vision area, but not well explored in NLP. More specifically, we\nconstruct a pseudo multi-parallel corpus of the new and the original languages\nby pivoting through English, and imitate the output distribution of the\noriginal MNMT model. Extensive experiments show that our approach significantly\nimproves the translation performance between the new and the original\nlanguages, without severe catastrophic forgetting. We also demonstrate that our\napproach is capable of solving copy and off-target problems, which are two\ncommon issues existence in current large-scale MNMT models.", "published": "2023-11-14 21:04:03", "link": "http://arxiv.org/abs/2311.08538v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Continual Pre-training for Building Domain Specific Large\n  Language Models", "abstract": "Large language models (LLMs) have demonstrated remarkable open-domain\ncapabilities. Traditionally, LLMs tailored for a domain are trained from\nscratch to excel at handling domain-specific tasks. In this work, we explore an\nalternative strategy of continual pre-training as a means to develop\ndomain-specific LLMs. We introduce FinPythia-6.9B, developed through\ndomain-adaptive continual pre-training on the financial domain. Continual\npre-trained FinPythia showcases consistent improvements on financial tasks over\nthe original foundational model. We further explore simple but effective data\nselection strategies for continual pre-training. Our data selection strategies\noutperforms vanilla continual pre-training's performance with just 10% of\ncorpus size and cost, without any degradation on open-domain standard tasks.\nOur work proposes an alternative solution to building domain-specific LLMs from\nscratch in a cost-effective manner.", "published": "2023-11-14 21:19:14", "link": "http://arxiv.org/abs/2311.08545v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UT5: Pretraining Non autoregressive T5 with unrolled denoising", "abstract": "Recent advances in Transformer-based Large Language Models have made great\nstrides in natural language generation. However, to decode K tokens, an\nautoregressive model needs K sequential forward passes, which may be a\nperformance bottleneck for large language models. Many non-autoregressive (NAR)\nresearch are aiming to address this sequentiality bottleneck, albeit many have\nfocused on a dedicated architecture in supervised benchmarks. In this work, we\nstudied unsupervised pretraining for non auto-regressive T5 models via unrolled\ndenoising and shown its SoTA results in downstream generation tasks such as\nSQuAD question generation and XSum.", "published": "2023-11-14 21:28:10", "link": "http://arxiv.org/abs/2311.08552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration", "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating exceptional reasoning, tool usage, and memory\ncapabilities. As their applications expand into multi-agent environments, there\narises a need for a comprehensive evaluation framework that captures LLMs'\nreasoning, planning, collaboration, and other social abilities. This work\nintroduces a novel competition-based benchmark framework specifically designed\nto assess LLMs within multi-agent settings, providing quantitative metrics to\nevaluate their judgment, reasoning, deception, self-awareness, cooperation,\ncoordination, and rationality. We utilize two social deduction games alongside\nthree game-theory scenarios to create diverse environments. Our frame is\nfortified with the probabilistic graphic modeling (PGM) method, enhancing the\nLLMs' capabilities in navigating complex social and cognitive dimensions. We\nevaluate seven LLMs, quantitatively highlighting a significant capability gap\nof over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.\nIt also confirms that our PGM enhancement boosts the abilities of all selected\nmodels by an average of 37%. Our data and code can be found here\nhttps://github.com/cathyxl/MAgIC.", "published": "2023-11-14 21:46:27", "link": "http://arxiv.org/abs/2311.08562v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational\n  AutoEncoders", "abstract": "The injection of syntactic information in Variational AutoEncoders (VAEs) has\nbeen shown to result in an overall improvement of performances and\ngeneralisation. An effective strategy to achieve such a goal is to separate the\nencoding of distributional semantic features and syntactic structures into\nheterogeneous latent spaces via multi-task learning or dual encoder\narchitectures. However, existing works employing such techniques are limited to\nLSTM-based VAEs. In this paper, we investigate latent space separation methods\nfor structural syntactic injection in Transformer-based VAE architectures\n(i.e., Optimus). Specifically, we explore how syntactic structures can be\nleveraged in the encoding stage through the integration of graph-based and\nsequential models, and how multiple, specialised latent representations can be\ninjected into the decoder's attention mechanism via low-rank operators. Our\nempirical evaluation, carried out on natural language sentences and\nmathematical expressions, reveals that the proposed end-to-end VAE architecture\ncan result in a better overall organisation of the latent space, alleviating\nthe information loss occurring in standard VAE setups, resulting in enhanced\nperformances on language modelling and downstream generation tasks.", "published": "2023-11-14 22:47:23", "link": "http://arxiv.org/abs/2311.08579v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asking More Informative Questions for Grounded Retrieval", "abstract": "When a model is trying to gather information in an interactive setting, it\nbenefits from asking informative questions. However, in the case of a grounded\nmulti-turn image identification task, previous studies have been constrained to\npolar yes/no questions, limiting how much information the model can gain in a\nsingle turn. We present an approach that formulates more informative,\nopen-ended questions. In doing so, we discover that off-the-shelf visual\nquestion answering (VQA) models often make presupposition errors, which\nstandard information gain question selection methods fail to account for. To\naddress this issue, we propose a method that can incorporate presupposition\nhandling into both question selection and belief updates. Specifically, we use\na two-stage process, where the model first filters out images which are\nirrelevant to a given question, then updates its beliefs about which image the\nuser intends. Through self-play and human evaluations, we show that our method\nis successful in asking informative open-ended questions, increasing accuracy\nover the past state-of-the-art by 14%, while resulting in 48% more efficient\ngames in human evaluations.", "published": "2023-11-14 23:13:27", "link": "http://arxiv.org/abs/2311.08584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language\n  Models", "abstract": "Pre-trained language models (PLMs) show impressive performance in various\ndownstream NLP tasks. However, pre-training large language models demands\nsubstantial memory and training compute. Furthermore, due to the substantial\nresources required, many PLM weights are confidential. Consequently, users are\ncompelled to share their data with model owners for fine-tuning specific tasks.\nTo overcome the limitations, we introduce Plug-in External Memory Adaptation\n(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM\nfine-tuning without requiring access to all the weights. PEMA integrates with\ncontext representations from test data during inference to perform downstream\ntasks. It uses external memory to store PLM-generated context representations\nmapped with target tokens. Our method utilizes weight matrices of LoRA-like\nbottlenecked adapter in the PLM's final layer to enhance efficiency. Our\napproach also includes Gradual Unrolling, a novel interpolation strategy to\nimprove generation quality. We validate PEMA's effectiveness through\nexperiments on syntactic and real datasets for machine translation and style\ntransfer. Our findings show that PEMA outperforms other PEFT approaches in\nmemory and latency efficiency for training, and also excels in maintaining\nsentence meaning and generating appropriate language and styles.", "published": "2023-11-14 23:20:51", "link": "http://arxiv.org/abs/2311.08590v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are You Sure? Challenging LLMs Leads to Performance Drops in The\n  FlipFlop Experiment", "abstract": "The interactive nature of Large Language Models (LLMs) theoretically allows\nmodels to refine and improve their answers, yet systematic analysis of the\nmulti-turn behavior of LLMs remains limited. In this paper, we propose the\nFlipFlop experiment: in the first round of the conversation, an LLM completes a\nclassification task. In a second round, the LLM is challenged with a follow-up\nphrase like \"Are you sure?\", offering an opportunity for the model to reflect\non its initial answer, and decide whether to confirm or flip its answer. A\nsystematic study of ten LLMs on seven classification tasks reveals that models\nflip their answers on average 46% of the time and that all models see a\ndeterioration of accuracy between their first and final prediction, with an\naverage drop of 17% (the FlipFlop effect). We conduct finetuning experiments on\nan open-source LLM and find that finetuning on synthetically created data can\nmitigate - reducing performance deterioration by 60% - but not resolve\nsycophantic behavior entirely. The FlipFlop experiment illustrates the\nuniversality of sycophantic behavior in LLMs and provides a robust framework to\nanalyze model behavior and evaluate future models.", "published": "2023-11-14 23:40:22", "link": "http://arxiv.org/abs/2311.08596v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$DA^3$: A Distribution-Aware Adversarial Attack against Language Models", "abstract": "Language models can be manipulated by adversarial attacks, which introduce\nsubtle perturbations to input data. While recent attack methods can achieve a\nrelatively high attack success rate (ASR), we've observed that the generated\nadversarial examples have a different data distribution compared with the\noriginal examples. Specifically, these adversarial examples exhibit reduced\nconfidence levels and greater divergence from the training data distribution.\nConsequently, they are easy to detect using straightforward detection methods,\ndiminishing the efficacy of such attacks. To address this issue, we propose a\nDistribution-Aware Adversarial Attack ($DA^3$) method. $DA^3$ considers the\ndistribution shifts of adversarial examples to improve attacks' effectiveness\nunder detection methods. We further design a novel evaluation metric, the\nNon-detectable Attack Success Rate (NASR), which integrates both ASR and\ndetectability for the attack task. We conduct experiments on four widely used\ndatasets to validate the attack effectiveness and transferability of\nadversarial examples generated by $DA^3$ against both the white-box BERT-base\nand RoBERTa-base models and the black-box LLaMA2-7b model.", "published": "2023-11-14 23:43:47", "link": "http://arxiv.org/abs/2311.08598v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Parametric Memory Guidance for Multi-Document Summarization", "abstract": "Multi-document summarization (MDS) is a difficult task in Natural Language\nProcessing, aiming to summarize information from several documents. However,\nthe source documents are often insufficient to obtain a qualitative summary. We\npropose a retriever-guided model combined with non-parametric memory for\nsummary generation. This model retrieves relevant candidates from a database\nand then generates the summary considering the candidates with a copy mechanism\nand the source documents. The retriever is implemented with Approximate Nearest\nNeighbor Search (ANN) to search large databases. Our method is evaluated on the\nMultiXScience dataset which includes scientific articles. Finally, we discuss\nour results and possible directions for future work.", "published": "2023-11-14 07:41:48", "link": "http://arxiv.org/abs/2311.10760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Mutually Informed Representations for Characters and Subwords", "abstract": "Most pretrained language models rely on subword tokenization, which processes\ntext as a sequence of subword tokens. However, different granularities of text,\nsuch as characters, subwords, and words, can contain different kinds of\ninformation. Previous studies have shown that incorporating multiple input\ngranularities improves model generalization, yet very few of them outputs\nuseful representations for each granularity. In this paper, we introduce the\nentanglement model, aiming to combine character and subword language models.\nInspired by vision-language models, our model treats characters and subwords as\nseparate modalities, and it generates mutually informed representations for\nboth granularities as output. We evaluate our model on text classification,\nnamed entity recognition, POS-tagging, and character-level sequence labeling\n(intraword code-switching). Notably, the entanglement model outperforms its\nbackbone language models, particularly in the presence of noisy texts and\nlow-resource languages. Furthermore, the entanglement model even outperforms\nlarger pre-trained models on all English sequence labeling tasks and\nclassification tasks. We make our code publically available.", "published": "2023-11-14 02:09:10", "link": "http://arxiv.org/abs/2311.07853v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting\n  Volunteer Content Moderators", "abstract": "Extensive efforts in automated approaches for content moderation have been\nfocused on developing models to identify toxic, offensive, and hateful content\nwith the aim of lightening the load for moderators. Yet, it remains uncertain\nwhether improvements on those tasks have truly addressed moderators' needs in\naccomplishing their work. In this paper, we surface gaps between past research\nefforts that have aimed to provide automation for aspects of content moderation\nand the needs of volunteer content moderators, regarding identifying violations\nof various moderation rules. To do so, we conduct a model review on Hugging\nFace to reveal the availability of models to cover various moderation rules and\nguidelines from three exemplar forums. We further put state-of-the-art LLMs to\nthe test, evaluating how well these models perform in flagging violations of\nplatform rules from one particular forum. Finally, we conduct a user survey\nstudy with volunteer moderators to gain insight into their perspectives on\nuseful moderation models. Overall, we observe a non-trivial gap, as missing\ndeveloped models and LLMs exhibit moderate to low performance on a significant\nportion of the rules. Moderators' reports provide guides for future work on\ndeveloping moderation assistant models.", "published": "2023-11-14 03:18:28", "link": "http://arxiv.org/abs/2311.07879v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey", "abstract": "The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\ncomprehensively review these knowledge-graph-based augmentation techniques in\nLLMs, focusing on their efficacy in mitigating hallucinations. We\nsystematically categorize these methods into three overarching groups, offering\nmethodological comparisons and performance evaluations. Lastly, this survey\nexplores the current trends and challenges associated with these techniques and\noutlines potential avenues for future research in this emerging field.", "published": "2023-11-14 05:21:57", "link": "http://arxiv.org/abs/2311.07914v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Brain-Driven Representation Learning Based on Diffusion Model", "abstract": "Interpreting EEG signals linked to spoken language presents a complex\nchallenge, given the data's intricate temporal and spatial attributes, as well\nas the various noise factors. Denoising diffusion probabilistic models (DDPMs),\nwhich have recently gained prominence in diverse areas for their capabilities\nin representation learning, are explored in our research as a means to address\nthis issue. Using DDPMs in conjunction with a conditional autoencoder, our new\napproach considerably outperforms traditional machine learning algorithms and\nestablished baseline models in accuracy. Our results highlight the potential of\nDDPMs as a sophisticated computational method for the analysis of\nspeech-related EEG signals. This could lead to significant advances in\nbrain-computer interfaces tailored for spoken communication.", "published": "2023-11-14 05:59:58", "link": "http://arxiv.org/abs/2311.07925v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive Machine Translation with Probabilistic Context-free\n  Grammar", "abstract": "Non-autoregressive Transformer(NAT) significantly accelerates the inference\nof neural machine translation. However, conventional NAT models suffer from\nlimited expression power and performance degradation compared to autoregressive\n(AT) models due to the assumption of conditional independence among target\ntokens. To address these limitations, we propose a novel approach called\nPCFG-NAT, which leverages a specially designed Probabilistic Context-Free\nGrammar (PCFG) to enhance the ability of NAT models to capture complex\ndependencies among output tokens. Experimental results on major machine\ntranslation benchmarks demonstrate that PCFG-NAT further narrows the gap in\ntranslation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a\ndeeper understanding of the generated sentences, addressing the lack of\nsatisfactory explainability in neural machine translation.Code is publicly\navailable at https://github.com/ictnlp/PCFG-NAT.", "published": "2023-11-14 06:39:04", "link": "http://arxiv.org/abs/2311.07941v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Closer Look at the Self-Verification Abilities of Large Language\n  Models in Logical Reasoning", "abstract": "Logical reasoning has been an ongoing pursuit in the field of AI. Despite\nsignificant advancements made by large language models (LLMs), they still\nstruggle with complex logical reasoning problems. To enhance reasoning\nperformance, one promising direction is scalable oversight, which requires LLMs\nto identify their own errors and then improve by themselves. Various\nself-verification methods have been proposed in pursuit of this goal.\nNevertheless, whether existing models understand their own errors well is still\nunder investigation. In this paper, we take a closer look at the\nself-verification abilities of LLMs in the context of logical reasoning,\nfocusing on their ability to identify logical fallacies accurately. We\nintroduce a dataset, FALLACIES, containing 232 types of reasoning fallacies\ncategorized in a hierarchical taxonomy. By conducting exhaustive experiments on\nFALLACIES, we obtain comprehensive and detailed analyses of a series of models\non their verification abilities. Our main findings suggest that existing LLMs\ncould struggle to identify fallacious reasoning steps accurately and may fall\nshort of guaranteeing the validity of self-verification methods. Drawing from\nthese observations, we offer suggestions for future research and practical\napplications of self-verification methods.", "published": "2023-11-14 07:13:10", "link": "http://arxiv.org/abs/2311.07954v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Improving the Robustness of Distantly-Supervised Named Entity\n  Recognition via Uncertainty-Aware Teacher Learning and Student-Student\n  Collaborative Learning", "abstract": "Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in\nreal-world scenarios. It can effectively alleviate the burden of annotation by\nmatching entities in existing knowledge bases with snippets in the text but\nsuffer from the label noise. Recent works attempt to adopt the teacher-student\nframework to gradually refine the training labels and improve the overall\nrobustness. However, these teacher-student methods achieve limited performance\nbecause the poor calibration of the teacher network produces incorrectly\npseudo-labeled samples, leading to error propagation. Therefore, we propose:\n(1) Uncertainty-Aware Teacher Learning that leverages the prediction\nuncertainty to reduce the number of incorrect pseudo labels in the\nself-training stage; (2) Student-Student Collaborative Learning that allows the\ntransfer of reliable labels between two student networks instead of\nindiscriminately relying on all pseudo labels from its teacher, and further\nenables a full exploration of mislabeled samples rather than simply filtering\nunreliable pseudo-labeled samples. We evaluate our proposed method on five\nDS-NER datasets, demonstrating that our method is superior to the\nstate-of-the-art DS-NER methods.", "published": "2023-11-14 09:09:58", "link": "http://arxiv.org/abs/2311.08010v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data and models for stance and premise detection in COVID-19 tweets:\n  insights from the Social Media Mining for Health (SMM4H) 2022 shared task", "abstract": "The COVID-19 pandemic has sparked numerous discussions on social media\nplatforms, with users sharing their views on topics such as mask-wearing and\nvaccination. To facilitate the evaluation of neural models for stance detection\nand premise classification, we organized the Social Media Mining for Health\n(SMM4H) 2022 Shared Task 2. This competition utilized manually annotated posts\non three COVID-19-related topics: school closures, stay-at-home orders, and\nwearing masks. In this paper, we extend the previous work and present newly\ncollected data on vaccination from Twitter to assess the performance of models\non a different topic. To enhance the accuracy and effectiveness of our\nevaluation, we employed various strategies to aggregate tweet texts with\nclaims, including models with feature-level (early) fusion and dual-view\narchitectures from SMM4H 2022 leaderboard. Our primary objective was to create\na valuable dataset and perform an extensive experimental evaluation to support\nfuture research in argument mining in the health domain.", "published": "2023-11-14 10:30:49", "link": "http://arxiv.org/abs/2311.08057v1", "categories": ["cs.CL", "cs.SI", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Spot: A Natural Language Interface for Geospatial Searches in OSM", "abstract": "Investigative journalists and fact-checkers have found OpenStreetMap (OSM) to\nbe an invaluable resource for their work due to its extensive coverage and\nintricate details of various locations, which play a crucial role in\ninvestigating news scenes. Despite its value, OSM's complexity presents\nconsiderable accessibility and usability challenges, especially for those\nwithout a technical background. To address this, we introduce 'Spot', a\nuser-friendly natural language interface for querying OSM data. Spot utilizes a\nsemantic mapping from natural language to OSM tags, leveraging artificially\ngenerated sentence queries and a T5 transformer. This approach enables Spot to\nextract relevant information from user-input sentences and display candidate\nlocations matching the descriptions on a map. To foster collaboration and\nfuture advancement, all code and generated data is available as an open-source\nrepository.", "published": "2023-11-14 11:35:09", "link": "http://arxiv.org/abs/2311.08093v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts", "abstract": "Reasoning methods, best exemplified by the well-known Chain-of-Thought (CoT),\nempower the reasoning abilities of Large Language Models (LLMs) by eliciting\nthem to solve complex tasks in a step-by-step manner. Although they are\nachieving significant success, the ability to deliver multi-step reasoning\nremains limited to English because of the imbalance in the distribution of\npre-training data, which makes other languages a barrier. In this paper, we\npropose Cross-lingual Tree-of-Thoughts (Cross-ToT), a method for aligning\nCross-lingual CoT reasoning across languages. The proposed method, through a\nself-consistent cross-lingual prompting mechanism inspired by the\nTree-of-Thoughts approach, provides multi-step reasoning paths in different\nlanguages that, during the steps, lead to the final solution. Experimental\nevaluations show that our method significantly outperforms existing prompting\nmethods by reducing the number of interactions and achieving state-of-the-art\nperformance.", "published": "2023-11-14 11:49:43", "link": "http://arxiv.org/abs/2311.08097v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DiLoCo: Distributed Low-Communication Training of Language Models", "abstract": "Large language models (LLM) have become a critical component in many\napplications of machine learning. However, standard approaches to training LLM\nrequire a large number of tightly interconnected accelerators, with devices\nexchanging gradients and other intermediate states at each optimization step.\nWhile it is difficult to build and maintain a single computing cluster hosting\nmany accelerators, it might be easier to find several computing clusters each\nhosting a smaller number of devices. In this work, we propose a distributed\noptimization algorithm, Distributed Low-Communication (DiLoCo), that enables\ntraining of language models on islands of devices that are poorly connected.\nThe approach is a variant of federated averaging, where the number of inner\nsteps is large, the inner optimizer is AdamW, and the outer optimizer is\nNesterov momentum. On the widely used C4 dataset, we show that DiLoCo on 8\nworkers performs as well as fully synchronous optimization while communicating\n500 times less. DiLoCo exhibits great robustness to the data distribution of\neach worker. It is also robust to resources becoming unavailable over time, and\nvice versa, it can seamlessly leverage resources that become available during\ntraining.", "published": "2023-11-14 12:05:45", "link": "http://arxiv.org/abs/2311.08105v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Hateful Meme Detection through Retrieval-Guided Contrastive\n  Learning", "abstract": "Hateful memes have emerged as a significant concern on the Internet.\nDetecting hateful memes requires the system to jointly understand the visual\nand textual modalities. Our investigation reveals that the embedding space of\nexisting CLIP-based systems lacks sensitivity to subtle differences in memes\nthat are vital for correct hatefulness classification. We propose constructing\na hatefulness-aware embedding space through retrieval-guided contrastive\ntraining. Our approach achieves state-of-the-art performance on the\nHatefulMemes dataset with an AUROC of 87.0, outperforming much larger\nfine-tuned large multimodal models. We demonstrate a retrieval-based hateful\nmemes detection system, which is capable of identifying hatefulness based on\ndata unseen in training. This allows developers to update the hateful memes\ndetection system by simply adding new examples without retraining, a desirable\nfeature for real services in the constantly evolving landscape of hateful memes\non the Internet.", "published": "2023-11-14 12:14:54", "link": "http://arxiv.org/abs/2311.08110v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Memory-efficient Stochastic methods for Memory-based Transformers", "abstract": "Training Memory-based transformers can require a large amount of memory and\ncan be quite inefficient. We propose a novel two-phase training mechanism and a\nnovel regularization technique to improve the training efficiency of\nmemory-based transformers, which are often used for long-range context\nproblems. For our experiments, we consider transformer-XL as our baseline model\nwhich is one of memorybased transformer models. We show that our resultant\nmodel, Skip Cross-head TransformerXL, outperforms the baseline on character\nlevel language modeling task with similar parameters and outperforms the\nbaseline on word level language modelling task with almost 20% fewer\nparameters. Our proposed methods do not require any additional memory. We also\ndemonstrate the effectiveness of our regularization mechanism on BERT which\nshows similar performance with reduction in standard deviation of scores of\naround 30% on multiple GLUE tasks.", "published": "2023-11-14 12:37:25", "link": "http://arxiv.org/abs/2311.08123v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "RECALL: A Benchmark for LLMs Robustness against External Counterfactual\n  Knowledge", "abstract": "LLMs and AI chatbots have improved people's efficiency in various fields.\nHowever, the necessary knowledge for answering the question may be beyond the\nmodels' knowledge boundaries. To mitigate this issue, many researchers try to\nintroduce external knowledge, such as knowledge graphs and Internet contents,\ninto LLMs for up-to-date information. However, the external information from\nthe Internet may include counterfactual information that will confuse the model\nand lead to an incorrect response. Thus there is a pressing need for LLMs to\npossess the ability to distinguish reliable information from external\nknowledge. Therefore, to evaluate the ability of LLMs to discern the\nreliability of external knowledge, we create a benchmark from existing\nknowledge bases. Our benchmark consists of two tasks, Question Answering and\nText Generation, and for each task, we provide models with a context containing\ncounterfactual information. Evaluation results show that existing LLMs are\nsusceptible to interference from unreliable external knowledge with\ncounterfactual information, and simple intervention methods make limited\ncontributions to the alleviation of this issue.", "published": "2023-11-14 13:24:19", "link": "http://arxiv.org/abs/2311.08147v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Just Ask One More Time! Self-Agreement Improves Reasoning of Language\n  Models in (Almost) All Scenarios", "abstract": "Although chain-of-thought (CoT) prompting combined with language models has\nachieved encouraging results on complex reasoning tasks, the naive greedy\ndecoding used in CoT prompting usually causes the repetitiveness and local\noptimality. To address this shortcoming, ensemble-optimization tries to obtain\nmultiple reasoning paths to get the final answer assembly. However, current\nensemble-optimization methods either simply employ rule-based post-processing\nsuch as \\textit{self-consistency}, or train an additional model based on\nseveral task-related human annotations to select the best one among multiple\nreasoning paths, yet fail to generalize to realistic settings where the type of\ninput questions is unknown or the answer format of reasoning paths is unknown.\nTo avoid their limitations, we propose \\textbf{Self-Agreement}, a generalizable\nensemble-optimization method applying in almost all scenarios where the type of\ninput questions and the answer format of reasoning paths may be known or\nunknown. Self-agreement firstly samples from language model's decoder to\ngenerate a \\textit{diverse} set of reasoning paths, and subsequently prompts\nthe language model \\textit{one more time} to determine the optimal answer by\nselecting the most \\textit{agreed} answer among the sampled reasoning paths.\nSelf-agreement simultaneously achieves remarkable performance on six public\nreasoning benchmarks and superior generalization capabilities.", "published": "2023-11-14 13:30:54", "link": "http://arxiv.org/abs/2311.08154v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning", "abstract": "Enhancing the instruction-following ability of Large Language Models (LLMs)\nprimarily demands substantial instruction-tuning datasets. However, the sheer\nvolume of these imposes a considerable computational burden and annotation\ncost. To investigate a label-efficient instruction tuning method that allows\nthe model itself to actively sample subsets that are equally or even more\neffective, we introduce a self-evolving mechanism DiverseEvol. In this process,\na model iteratively augments its training subset to refine its own performance,\nwithout requiring any intervention from humans or more advanced LLMs. The key\nto our data sampling technique lies in the enhancement of diversity in the\nchosen subsets, as the model selects new data points most distinct from any\nexisting ones according to its current embedding space. Extensive experiments\nacross three datasets and benchmarks demonstrate the effectiveness of\nDiverseEvol. Our models, trained on less than 8% of the original dataset,\nmaintain or improve performance compared with finetuning on full data. We also\nprovide empirical evidence to analyze the importance of diversity in\ninstruction data and the iterative scheme as opposed to one-time sampling. Our\ncode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.", "published": "2023-11-14 14:10:40", "link": "http://arxiv.org/abs/2311.08182v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Fact-Checking in Dialogue: Are Specialized Models Needed?", "abstract": "Prior research has shown that typical fact-checking models for stand-alone\nclaims struggle with claims made in dialogues. As a solution, fine-tuning these\nmodels on labelled dialogue data has been proposed. However, creating separate\nmodels for each use case is impractical, and we show that fine-tuning models\nfor dialogue results in poor performance on typical fact-checking. To overcome\nthis challenge, we present techniques that allow us to use the same models for\nboth dialogue and typical fact-checking. These mainly focus on retrieval\nadaptation and transforming conversational inputs so that they can be\naccurately predicted by models trained on stand-alone claims. We demonstrate\nthat a typical fact-checking model incorporating these techniques is\ncompetitive with state-of-the-art models fine-tuned for dialogue, while\nmaintaining its accuracy on stand-alone claims.", "published": "2023-11-14 14:29:00", "link": "http://arxiv.org/abs/2311.08195v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unlock the Power: Competitive Distillation for Multi-Modal Large\n  Language Models", "abstract": "Recently, multi-modal content generation has attracted lots of attention from\nresearchers by investigating the utilization of visual instruction tuning based\non large language models (LLMs). To enhance the performance and generalization\nability of such LLMs, the practice of distilling knowledge from pretrained\nmulti-modal models (a.k.a. teachers) to more compact multi-modal LLMs\n(students) has gained considerable interest. However, the prevailing paradigm\nof instructiontuning in multi-modal LLMs knowledge distillation is\nresource-intensive and unidirectional, neglecting the potential for mutual\nfeedback between the student and teacher models. Thus, we propose an innovative\nCompetitive Multi-modal Distillation framework (CoMD), which captures\nbidirectional feedback between teacher and student models and continually\nupdates the multi-modal capabilities that the student model has learned. It\ncomprises two stages: multi-modal pre-training and multi-modal competitive\ndistillation. The first stage pre-trains the student model on a large number of\nfiltered multi-modal datasets. The second stage facilitates a bidirectional\nknowledge transfer between the student and teacher models. Our experimental\nanalysis of diverse datasets shows that our knowledge transfer method\nconsistently improves the capabilities of the student model. Finally, the\n7B-sized student model after four distillations surpassed the current\nstate-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also\noutperforms other strong baselines in the zero-shot setting.", "published": "2023-11-14 14:49:46", "link": "http://arxiv.org/abs/2311.08213v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese\n  Spelling Correction", "abstract": "ChatGPT has demonstrated impressive performance in various downstream tasks.\nHowever, in the Chinese Spelling Correction (CSC) task, we observe a\ndiscrepancy: while ChatGPT performs well under human evaluation, it scores\npoorly according to traditional metrics. We believe this inconsistency arises\nbecause the traditional metrics are not well-suited for evaluating generative\nmodels. Their overly strict length and phonics constraints may lead to\nunderestimating ChatGPT's correction capabilities. To better evaluate\ngenerative models in the CSC task, this paper proposes a new evaluation metric:\nEval-GCSC. By incorporating word-level and semantic similarity judgments, it\nrelaxes the stringent length and phonics constraints. Experimental results show\nthat Eval-GCSC closely aligns with human evaluations. Under this metric,\nChatGPT's performance is comparable to traditional token-level classification\nmodels (TCM), demonstrating its potential as a CSC tool. The source code and\nscripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.", "published": "2023-11-14 14:56:33", "link": "http://arxiv.org/abs/2311.08219v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Investigating the Encoding of Words in BERT's Neurons using Feature\n  Textualization", "abstract": "Pretrained language models (PLMs) form the basis of most state-of-the-art NLP\ntechnologies. Nevertheless, they are essentially black boxes: Humans do not\nhave a clear understanding of what knowledge is encoded in different parts of\nthe models, especially in individual neurons. The situation is different in\ncomputer vision, where feature visualization provides a decompositional\ninterpretability technique for neurons of vision models. Activation\nmaximization is used to synthesize inherently interpretable visual\nrepresentations of the information encoded in individual neurons. Our work is\ninspired by this but presents a cautionary tale on the interpretability of\nsingle neurons, based on the first large-scale attempt to adapt activation\nmaximization to NLP, and, more specifically, large PLMs. We propose feature\ntextualization, a technique to produce dense representations of neurons in the\nPLM word embedding space. We apply feature textualization to the BERT model\n(Devlin et al., 2019) to investigate whether the knowledge encoded in\nindividual neurons can be interpreted and symbolized. We find that the produced\nrepresentations can provide insights about the knowledge encoded in individual\nneurons, but that individual neurons do not represent clearcut symbolic units\nof language such as words. Additionally, we use feature textualization to\ninvestigate how many neurons are needed to encode words in BERT.", "published": "2023-11-14 15:21:49", "link": "http://arxiv.org/abs/2311.08240v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Confidence Estimation and Calibration in Large Language\n  Models", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks in various domains. Despite their impressive performance,\nthey can be unreliable due to factual errors in their generations. Assessing\ntheir confidence and calibrating them across different tasks can help mitigate\nrisks and enable LLMs to produce better generations. There has been a lot of\nrecent research aiming to address this, but there has been no comprehensive\noverview to organize it and outline the main lessons learned. The present\nsurvey aims to bridge this gap. In particular, we outline the challenges and we\nsummarize recent technical advancements for LLM confidence estimation and\ncalibration. We further discuss their applications and suggest promising\ndirections for future work.", "published": "2023-11-14 16:43:29", "link": "http://arxiv.org/abs/2311.08298v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing", "abstract": "Reflective listening is a fundamental skill that counselors must acquire to\nachieve proficiency in motivational interviewing (MI). It involves responding\nin a manner that acknowledges and explores the meaning of what the client has\nexpressed in the conversation. In this work, we introduce the task of\ncounseling response rewriting, which transforms non-reflective statements into\nreflective responses. We introduce VERVE, a template-based rewriting system\nwith paraphrase-augmented training and adaptive template updating. VERVE first\ncreates a template by identifying and filtering out tokens that are not\nrelevant to reflections and constructs a reflective response using the\ntemplate. Paraphrase-augmented training allows the model to learn less-strict\nfillings of masked spans, and adaptive template updating helps discover\neffective templates for rewriting without significantly removing the original\ncontent. Using both automatic and human evaluations, we compare our method\nagainst text rewriting baselines and show that our framework is effective in\nturning non-reflective statements into more reflective responses while\nachieving a good content preservation-reflection style trade-off.", "published": "2023-11-14 16:44:16", "link": "http://arxiv.org/abs/2311.08299v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Workflow-Guided Response Generation for Task-Oriented Dialogue", "abstract": "Task-oriented dialogue (TOD) systems aim to achieve specific goals through\ninteractive dialogue. Such tasks usually involve following specific workflows,\ni.e. executing a sequence of actions in a particular order. While prior work\nhas focused on supervised learning methods to condition on past actions, they\ndo not explicitly optimize for compliance to a desired workflow. In this paper,\nwe propose a novel framework based on reinforcement learning (RL) to generate\ndialogue responses that are aligned with a given workflow. Our framework\nconsists of ComplianceScorer, a metric designed to evaluate how well a\ngenerated response executes the specified action, combined with an RL\nopimization process that utilizes an interactive sampling technique. We\nevaluate our approach on two TOD datasets, Action-Based Conversations Dataset\n(ABCD) (Chen et al., 2021a) and MultiWOZ 2.2 (Zang et al., 2020) on a range of\nautomated and human evaluation metrics. Our findings indicate that our RL-based\nframework outperforms baselines and is effective at enerating responses that\nboth comply with the intended workflows while being expressed in a natural and\nfluent manner.", "published": "2023-11-14 16:44:33", "link": "http://arxiv.org/abs/2311.08300v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extrinsically-Focused Evaluation of Omissions in Medical Summarization", "abstract": "Large language models (LLMs) have shown promise in safety-critical\napplications such as healthcare, yet the ability to quantify performance has\nlagged. An example of this challenge is in evaluating a summary of the\npatient's medical record. A resulting summary can enable the provider to get a\nhigh-level overview of the patient's health status quickly. Yet, a summary that\nomits important facts about the patient's record can produce a misleading\npicture. This can lead to negative consequences on medical decision-making. We\npropose MED-OMIT as a metric to explore this challenge. We focus on using\nprovider-patient history conversations to generate a subjective (a summary of\nthe patient's history) as a case study. We begin by discretizing facts from the\ndialogue and identifying which are omitted from the subjective. To determine\nwhich facts are clinically relevant, we measure the importance of each fact to\na simulated differential diagnosis. We compare MED-OMIT's performance to that\nof clinical experts and find broad agreement We use MED-OMIT to evaluate LLM\nperformance on subjective generation and find some LLMs (gpt-4 and\nllama-3.1-405b) work well with little effort, while others (e.g. Llama 2)\nperform worse.", "published": "2023-11-14 16:46:15", "link": "http://arxiv.org/abs/2311.08303v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Anti-LM Decoding for Zero-shot In-context Machine Translation", "abstract": "Zero-shot In-context learning is the phenomenon where models can perform the\ntask simply given the instructions. However, pre-trained large language models\nare known to be poorly calibrated for this task. One of the most effective\napproaches to handling this bias is to adopt a contrastive decoding objective,\nwhich accounts for the prior probability of generating the next token by\nconditioning on some context. This work introduces an Anti-Language Model\nobjective with a decay factor designed to address the weaknesses of In-context\nMachine Translation. We conduct our experiments across 3 model types and sizes,\n3 language directions, and for both greedy decoding and beam search ($B=5$).\nThe proposed method outperforms other state-of-art decoding objectives, with up\nto $20$ BLEU point improvement from the default objective observed in some\nsettings.", "published": "2023-11-14 17:09:43", "link": "http://arxiv.org/abs/2311.08324v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Filter Context for Retrieval-Augmented Generation", "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of\nreliable systems for tasks such as open-domain question answering and fact\nverification. However, because retrieval systems are not perfect, generation\nmodels are required to generate outputs given partially or entirely irrelevant\npassages. This can cause over- or under-reliance on context, and result in\nproblems in the generated output such as hallucinations. To alleviate these\nproblems, we propose FILCO, a method that improves the quality of the context\nprovided to the generator by (1) identifying useful context based on lexical\nand information-theoretic approaches, and (2) training context filtering models\nthat can filter retrieved contexts at test time. We experiment on six\nknowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our\nmethod outperforms existing approaches on extractive question answering (QA),\ncomplex multi-hop and long-form QA, fact verification, and dialog generation\ntasks. FILCO effectively improves the quality of context, whether or not it\nsupports the canonical output.", "published": "2023-11-14 18:41:54", "link": "http://arxiv.org/abs/2311.08377v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PSST: A Benchmark for Evaluation-driven Text Public-Speaking Style\n  Transfer", "abstract": "Language style is necessary for AI systems to understand and generate diverse\nhuman language accurately. However, previous text style transfer primarily\nfocused on sentence-level data-driven approaches, limiting exploration of\npotential problems in large language models (LLMs) and the ability to meet\ncomplex application needs. To overcome these limitations, we introduce a novel\ntask called Public-Speaking Style Transfer (PSST), which aims to simulate\nhumans to transform passage-level, official texts into a public-speaking style.\nGrounded in the analysis of real-world data from a linguistic perspective, we\ndecompose public-speaking style into key sub-styles to pose challenges and\nquantify the style modeling capability of LLMs. For such intricate text style\ntransfer, we further propose a fine-grained evaluation framework to analyze the\ncharacteristics and identify the problems of stylized texts. Comprehensive\nexperiments suggest that current LLMs struggle to generate public speaking\ntexts that align with human preferences, primarily due to excessive stylization\nand loss of semantic information.", "published": "2023-11-14 18:50:51", "link": "http://arxiv.org/abs/2311.08389v3", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Temporally Grounded?", "abstract": "Are Large language models (LLMs) temporally grounded? Since LLMs cannot\nperceive and interact with the environment, it is impossible to answer this\nquestion directly. Instead, we provide LLMs with textual narratives and probe\nthem with respect to their common-sense knowledge of the structure and duration\nof events, their ability to order events along a timeline, and self-consistency\nwithin their temporal model (e.g., temporal relations such as after and before\nare mutually exclusive for any pair of events). We evaluate state-of-the-art\nLLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.\nGenerally, we find that LLMs lag significantly behind both human performance as\nwell as small-scale, specialised LMs. In-context learning, instruction tuning,\nand chain-of-thought prompting reduce this gap only to a limited degree.\nCrucially, LLMs struggle the most with self-consistency, displaying incoherent\nbehaviour in at least 27.23% of their predictions. Contrary to expectations, we\nalso find that scaling the model size does not guarantee positive gains in\nperformance. To explain these results, we study the sources from which LLMs may\ngather temporal information: we find that sentence ordering in unlabelled\ntexts, available during pre-training, is only weakly correlated with event\nordering. Moreover, public instruction tuning mixtures contain few temporal\ntasks. Hence, we conclude that current LLMs lack a consistent temporal model of\ntextual narratives. Code, datasets, and LLM outputs are available at\nhttps://github.com/yfqiu-nlp/temporal-llms.", "published": "2023-11-14 18:57:15", "link": "http://arxiv.org/abs/2311.08398v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Alignment is not sufficient to prevent large language models from\n  generating harmful information: A psychoanalytic perspective", "abstract": "Large Language Models (LLMs) are central to a multitude of applications but\nstruggle with significant risks, notably in generating harmful content and\nbiases. Drawing an analogy to the human psyche's conflict between evolutionary\nsurvival instincts and societal norm adherence elucidated in Freud's\npsychoanalysis theory, we argue that LLMs suffer a similar fundamental\nconflict, arising between their inherent desire for syntactic and semantic\ncontinuity, established during the pre-training phase, and the post-training\nalignment with human values. This conflict renders LLMs vulnerable to\nadversarial attacks, wherein intensifying the models' desire for continuity can\ncircumvent alignment efforts, resulting in the generation of harmful\ninformation. Through a series of experiments, we first validated the existence\nof the desire for continuity in LLMs, and further devised a straightforward yet\npowerful technique, such as incomplete sentences, negative priming, and\ncognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle\nto prevent the generation of harmful information. In summary, our study\nuncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby\nquestioning the efficacy of solely relying on sophisticated alignment methods,\nand further advocates for a new training idea that integrates modal concepts\nalongside traditional amodal concepts, aiming to endow LLMs with a more nuanced\nunderstanding of real-world contexts and ethical considerations.", "published": "2023-11-14 19:28:51", "link": "http://arxiv.org/abs/2311.08487v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Summarization-Based Document IDs for Generative Retrieval with Language\n  Models", "abstract": "Generative retrieval (Wang et al., 2022; Tay et al., 2022) is a popular\napproach for end-to-end document retrieval that directly generates document\nidentifiers given an input query. We introduce summarization-based document\nIDs, in which each document's ID is composed of an extractive summary or\nabstractive keyphrases generated by a language model, rather than an integer ID\nsequence or bags of n-grams as proposed in past work. We find that abstractive,\ncontent-based IDs (ACID) and an ID based on the first 30 tokens are very\neffective in direct comparisons with previous approaches to ID creation. We\nshow that using ACID improves top-10 and top-20 recall by 15.6% and 14.4%\n(relative) respectively versus the cluster-based integer ID baseline on the\nMSMARCO 100k retrieval task, and 9.8% and 9.9% respectively on the\nWikipedia-based NQ 100k retrieval task. Our results demonstrate the\neffectiveness of human-readable, natural-language IDs created through\nsummarization for generative retrieval. We also observed that extractive IDs\noutperformed abstractive IDs on Wikipedia articles in NQ but not the snippets\nin MSMARCO, which suggests that document characteristics affect generative\nretrieval performance.", "published": "2023-11-14 23:28:36", "link": "http://arxiv.org/abs/2311.08593v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation", "abstract": "Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.", "published": "2023-11-14 01:38:02", "link": "http://arxiv.org/abs/2311.07838v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA", "abstract": "We present BYOKG, a universal question-answering (QA) system that can operate\non any knowledge graph (KG), requires no human-annotated training data, and can\nbe ready to use within a day -- attributes that are out-of-scope for current\nKGQA systems. BYOKG draws inspiration from the remarkable ability of humans to\ncomprehend information present in an unseen KG through exploration -- starting\nat random nodes, inspecting the labels of adjacent nodes and edges, and\ncombining them with their prior world knowledge. In BYOKG, exploration\nleverages an LLM-backed symbolic agent that generates a diverse set of\nquery-program exemplars, which are then used to ground a retrieval-augmented\nreasoning procedure to predict programs for arbitrary questions. BYOKG is\neffective over both small- and large-scale graphs, showing dramatic gains in QA\naccuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,\nrespectively. On GrailQA, we further show that our unsupervised BYOKG\noutperforms a supervised in-context learning method, demonstrating the\neffectiveness of exploration. Lastly, we find that performance of BYOKG\nreliably improves with continued exploration as well as improvements in the\nbase LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1\non a sub-sampled zero-shot split of GrailQA.", "published": "2023-11-14 02:05:29", "link": "http://arxiv.org/abs/2311.07850v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Instruction-Following Evaluation for Large Language Models", "abstract": "One core capability of Large Language Models (LLMs) is to follow natural\nlanguage instructions. However, the evaluation of such abilities is not\nstandardized: Human evaluations are expensive, slow, and not objectively\nreproducible, while LLM-based auto-evaluation is potentially biased or limited\nby the ability of the evaluator LLM. To overcome these issues, we introduce\nInstruction-Following Eval (IFEval) for large language models. IFEval is a\nstraightforward and easy-to-reproduce evaluation benchmark. It focuses on a set\nof \"verifiable instructions\" such as \"write in more than 400 words\" and\n\"mention the keyword of AI at least 3 times\". We identified 25 types of those\nverifiable instructions and constructed around 500 prompts, with each prompt\ncontaining one or more verifiable instructions. We show evaluation results of\ntwo widely available LLMs on the market. Our code and data can be found at\nhttps://github.com/google-research/google-research/tree/master/instruction_following_eval", "published": "2023-11-14 05:13:55", "link": "http://arxiv.org/abs/2311.07911v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T99 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified\n  Large-Scale Audio-Language Models", "abstract": "Recently, instruction-following audio-language models have received broad\nattention for audio interaction with humans. However, the absence of\npre-trained audio models capable of handling diverse audio types and tasks has\nhindered progress in this field. Consequently, most existing works have only\nbeen able to support a limited range of interaction capabilities. In this\npaper, we develop the Qwen-Audio model and address this limitation by scaling\nup audio-language pre-training to cover over 30 tasks and various audio types,\nsuch as human speech, natural sounds, music, and songs, to facilitate universal\naudio understanding abilities. However, directly co-training all tasks and\ndatasets can lead to interference issues, as the textual labels associated with\ndifferent datasets exhibit considerable variations due to differences in task\nfocus, language, granularity of annotation, and text structure. To overcome the\none-to-many interference, we carefully design a multi-task training framework\nby conditioning on a sequence of hierarchical tags to the decoder for\nencouraging knowledge sharing and avoiding interference through shared and\nspecified tags respectively. Remarkably, Qwen-Audio achieves impressive\nperformance across diverse benchmark tasks without requiring any task-specific\nfine-tuning, surpassing its counterparts. Building upon the capabilities of\nQwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from\nvarious audios and text inputs, enabling multi-turn dialogues and supporting\nvarious audio-central scenarios.", "published": "2023-11-14 05:34:50", "link": "http://arxiv.org/abs/2311.07919v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "AfroBench: How Good are Large Language Models on African Languages?", "abstract": "Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/", "published": "2023-11-14 08:10:14", "link": "http://arxiv.org/abs/2311.07978v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on\n  Language Models for Code", "abstract": "In this work we systematically review the recent advancements in software\nengineering with language models, covering 70+ models, 40+ evaluation tasks,\n180+ datasets, and 900 related works. Unlike previous works, we integrate\nsoftware engineering (SE) with natural language processing (NLP) by discussing\nthe perspectives of both sides: SE applies language models for development\nautomation, while NLP adopts SE tasks for language model evaluation. We break\ndown code processing models into general language models represented by the GPT\nfamily and specialized models that are specifically pretrained on code, often\nwith tailored objectives. We discuss the relations and differences between\nthese models, and highlight the historical transition of code modeling from\nstatistical models and RNNs to pretrained Transformers and LLMs, which is\nexactly the same course that had been taken by NLP. We also go beyond\nprogramming and review LLMs' application in other software engineering\nactivities including requirement engineering, testing, deployment, and\noperations in an endeavor to provide a global view of NLP in SE, and identify\nkey challenges and potential future directions in this domain. We keep the\nsurvey open and updated on GitHub at\nhttps://github.com/codefuse-ai/Awesome-Code-LLM.", "published": "2023-11-14 08:34:26", "link": "http://arxiv.org/abs/2311.07989v7", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Comparative Analysis of the COVID-19 Infodemic in English and Chinese:\n  Insights from Social Media Textual Data", "abstract": "The COVID-19 infodemic, characterized by the rapid spread of misinformation\nand unverified claims related to the pandemic, presents a significant\nchallenge. This paper presents a comparative analysis of the COVID-19 infodemic\nin the English and Chinese languages, utilizing textual data extracted from\nsocial media platforms. To ensure a balanced representation, two infodemic\ndatasets were created by augmenting previously collected social media textual\ndata. Through word frequency analysis, the thirty-five most frequently\noccurring infodemic words are identified, shedding light on prevalent\ndiscussions surrounding the infodemic. Moreover, topic clustering analysis\nuncovers thematic structures and provides a deeper understanding of primary\ntopics within each language context. Additionally, sentiment analysis enables\ncomprehension of the emotional tone associated with COVID-19 information on\nsocial media platforms in English and Chinese. This research contributes to a\nbetter understanding of the COVID-19 infodemic phenomenon and can guide the\ndevelopment of strategies to combat misinformation during public health crises\nacross different languages.", "published": "2023-11-14 08:55:11", "link": "http://arxiv.org/abs/2311.08001v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "TempTabQA: Temporal Question Answering for Semi-Structured Tables", "abstract": "Semi-structured data, such as Infobox tables, often include temporal\ninformation about entities, either implicitly or explicitly. Can current NLP\nsystems reason about such information in semi-structured tables? To tackle this\nquestion, we introduce the task of temporal question answering on\nsemi-structured tables. We present a dataset, TempTabQA, which comprises 11,454\nquestion-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning\nmore than 90 distinct domains. Using this dataset, we evaluate several\nstate-of-the-art models for temporal reasoning. We observe that even the\ntop-performing LLMs lag behind human performance by more than 13.5 F1 points.\nGiven these results, our dataset has the potential to serve as a challenging\nbenchmark to improve the temporal reasoning capabilities of NLP models.", "published": "2023-11-14 08:57:01", "link": "http://arxiv.org/abs/2311.08002v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM\n  Game", "abstract": "Human preference alignment is essential to improve the interaction quality of\nlarge language models (LLMs). Existing alignment methods depend on manually\nannotated preference data to guide the LLM optimization directions. However,\ncontinuously updating LLMs for alignment raises a distribution gap between\nmodel-generated samples and human-annotated responses, hindering training\neffectiveness. To mitigate this issue, previous methods require additional\npreference annotation on newly generated samples to adapt to the shifted\ndistribution, which consumes a large amount of annotation resources. Targeting\nmore efficient human preference optimization, we propose an Adversarial\nPreference Optimization (APO) framework, in which the LLM and the reward model\nupdate alternatively via a min-max game. Through adversarial training, the\nreward model can adapt to the shifted generation distribution of the LLM\nwithout any additional annotation. With comprehensive experiments, we find the\nproposed adversarial training framework further enhances existing alignment\nbaselines in terms of LLM helpfulness and harmlessness. The code is at\nhttps://github.com/Linear95/APO.", "published": "2023-11-14 10:10:31", "link": "http://arxiv.org/abs/2311.08045v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Semi-supervised Hierarchical Stacked Encoder for Legal\n  Judgement Prediction", "abstract": "Predicting the judgment of a legal case from its unannotated case facts is a\nchallenging task. The lengthy and non-uniform document structure poses an even\ngreater challenge in extracting information for decision prediction. In this\nwork, we explore and propose a two-level classification mechanism; both\nsupervised and unsupervised; by using domain-specific pre-trained BERT to\nextract information from long documents in terms of sentence embeddings further\nprocessing with transformer encoder layer and use unsupervised clustering to\nextract hidden labels from these embeddings to better predict a judgment of a\nlegal case. We conduct several experiments with this mechanism and see higher\nperformance gains than the previously proposed methods on the ILDC dataset. Our\nexperimental results also show the importance of domain-specific pre-training\nof Transformer Encoders in legal information processing.", "published": "2023-11-14 12:03:26", "link": "http://arxiv.org/abs/2311.08103v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Human-Centric Autonomous Systems With LLMs for User Command Reasoning", "abstract": "The evolution of autonomous driving has made remarkable advancements in\nrecent years, evolving into a tangible reality. However, a human-centric\nlarge-scale adoption hinges on meeting a variety of multifaceted requirements.\nTo ensure that the autonomous system meets the user's intent, it is essential\nto accurately discern and interpret user commands, especially in complex or\nemergency situations. To this end, we propose to leverage the reasoning\ncapabilities of Large Language Models (LLMs) to infer system requirements from\nin-cabin users' commands. Through a series of experiments that include\ndifferent LLM models and prompt designs, we explore the few-shot multivariate\nbinary classification accuracy of system requirements from natural language\ntextual commands. We confirm the general ability of LLMs to understand and\nreason about prompts but underline that their effectiveness is conditioned on\nthe quality of both the LLM model and the design of appropriate sequential\nprompts. Code and models are public with the link\n\\url{https://github.com/KTH-RPL/DriveCmd_LLM}.", "published": "2023-11-14 14:42:28", "link": "http://arxiv.org/abs/2311.08206v2", "categories": ["cs.CL", "cs.AI", "cs.RO"], "primary_category": "cs.CL"}
{"title": "REST: Retrieval-Based Speculative Decoding", "abstract": "We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm\ndesigned to speed up language model generation. The key insight driving the\ndevelopment of REST is the observation that the process of text generation\noften includes certain common phases and patterns. Unlike previous methods that\nrely on a draft language model for speculative decoding, REST harnesses the\npower of retrieval to generate draft tokens. This method draws from the\nreservoir of existing knowledge, retrieving and employing relevant tokens based\non the current context. Its plug-and-play nature allows for seamless\nintegration and acceleration of any language models, all without necessitating\nadditional training. When benchmarked on 7B and 13B language models in a\nsingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on\ncode or text generation. The code of REST is available at\nhttps://github.com/FasterDecoding/REST.", "published": "2023-11-14 15:43:47", "link": "http://arxiv.org/abs/2311.08252v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The taste of IPA: Towards open-vocabulary keyword spotting and forced\n  alignment in any language", "abstract": "In this project, we demonstrate that phoneme-based models for speech\nprocessing can achieve strong crosslinguistic generalizability to unseen\nlanguages. We curated the IPAPACK, a massively multilingual speech corpora with\nphonemic transcriptions, encompassing more than 115 languages from diverse\nlanguage families, selectively checked by linguists. Based on the IPAPACK, we\npropose CLAP-IPA, a multi-lingual phoneme-speech contrastive embedding model\ncapable of open-vocabulary matching between arbitrary speech signals and\nphonemic sequences. The proposed model was tested on 95 unseen languages,\nshowing strong generalizability across languages. Temporal alignments between\nphonemes and speech signals also emerged from contrastive training, enabling\nzeroshot forced alignment in unseen languages. We further introduced a neural\nforced aligner IPA-ALIGNER by finetuning CLAP-IPA with the Forward-Sum loss to\nlearn better phone-to-audio alignment. Evaluation results suggest that\nIPA-ALIGNER can generalize to unseen languages without adaptation.", "published": "2023-11-14 17:09:07", "link": "http://arxiv.org/abs/2311.08323v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Transient Nature of Emergent In-Context Learning in Transformers", "abstract": "Transformer neural networks can exhibit a surprising capacity for in-context\nlearning (ICL) despite not being explicitly trained for it. Prior work has\nprovided a deeper understanding of how ICL emerges in transformers, e.g.\nthrough the lens of mechanistic interpretability, Bayesian inference, or by\nexamining the distributional properties of training data. However, in each of\nthese cases, ICL is treated largely as a persistent phenomenon; namely, once\nICL emerges, it is assumed to persist asymptotically. Here, we show that the\nemergence of ICL during transformer training is, in fact, often transient. We\ntrain transformers on synthetic data designed so that both ICL and in-weights\nlearning (IWL) strategies can lead to correct predictions. We find that ICL\nfirst emerges, then disappears and gives way to IWL, all while the training\nloss decreases, indicating an asymptotic preference for IWL. The transient\nnature of ICL is observed in transformers across a range of model sizes and\ndatasets, raising the question of how much to \"overtrain\" transformers when\nseeking compact, cheaper-to-run models. We find that L2 regularization may\noffer a path to more persistent ICL that removes the need for early stopping\nbased on ICL-style validation tasks. Finally, we present initial evidence that\nICL transience may be caused by competition between ICL and IWL circuits.", "published": "2023-11-14 18:03:20", "link": "http://arxiv.org/abs/2311.08360v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-shot audio captioning with audio-language model guidance and audio\n  context keywords", "abstract": "Zero-shot audio captioning aims at automatically generating descriptive\ntextual captions for audio content without prior training for this task.\nDifferent from speech recognition which translates audio content that contains\nspoken language into text, audio captioning is commonly concerned with ambient\nsounds, or sounds produced by a human performing an action. Inspired by\nzero-shot image captioning methods, we propose ZerAuCap, a novel framework for\nsummarising such general audio signals in a text caption without requiring\ntask-specific training. In particular, our framework exploits a pre-trained\nlarge language model (LLM) for generating the text which is guided by a\npre-trained audio-language model to produce captions that describe the audio\ncontent. Additionally, we use audio context keywords that prompt the language\nmodel to generate text that is broadly relevant to sounds. Our proposed\nframework achieves state-of-the-art results in zero-shot audio captioning on\nthe AudioCaps and Clotho datasets. Our code is available at\nhttps://github.com/ExplainableML/ZerAuCap.", "published": "2023-11-14 18:55:48", "link": "http://arxiv.org/abs/2311.08396v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fine-tuning Language Models for Factuality", "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.", "published": "2023-11-14 18:59:15", "link": "http://arxiv.org/abs/2311.08401v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Retrieve and Copy: Scaling ASR Personalization to Large Catalogs", "abstract": "Personalization of automatic speech recognition (ASR) models is a widely\nstudied topic because of its many practical applications. Most recently,\nattention-based contextual biasing techniques are used to improve the\nrecognition of rare words and domain specific entities. However, due to\nperformance constraints, the biasing is often limited to a few thousand\nentities, restricting real-world usability. To address this, we first propose a\n\"Retrieve and Copy\" mechanism to improve latency while retaining the accuracy\neven when scaled to a large catalog. We also propose a training strategy to\novercome the degradation in recall at such scale due to an increased number of\nconfusing entities. Overall, our approach achieves up to 6% more Word Error\nRate reduction (WERR) and 3.6% absolute improvement in F1 when compared to a\nstrong baseline. Our method also allows for large catalog sizes of up to 20K\nwithout significantly affecting WER and F1-scores, while achieving at least 20%\ninference speedup per acoustic frame.", "published": "2023-11-14 18:59:24", "link": "http://arxiv.org/abs/2311.08402v1", "categories": ["cs.CL", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LLMs cannot find reasoning errors, but can correct them given the error\n  location", "abstract": "While self-correction has shown promise in improving LLM outputs in terms of\nstyle and quality (e.g. Chen et al., 2023b; Madaan et al., 2023), recent\nattempts to self-correct logical or reasoning errors often cause correct\nanswers to become incorrect, resulting in worse performances overall (Huang et\nal., 2023). In this paper, we show that poor self-correction performance stems\nfrom LLMs' inability to find logical mistakes, rather than their ability to\ncorrect a known mistake. Firstly, we benchmark several state-of-the-art LLMs on\ntheir mistake-finding ability and demonstrate that they generally struggle with\nthe task, even in highly objective, unambiguous cases. Secondly, we test the\ncorrection abilities of LLMs -- separately from mistake finding -- using a\nbacktracking setup that feeds ground truth mistake location information to the\nmodel. We show that this boosts downstream task performance across our 5\nreasoning tasks, indicating that LLMs' correction abilities are robust.\nFinally, we show that it is possible to obtain mistake location information\nwithout ground truth labels or in-domain training data. We train a small\nclassifier with out-of-domain data, which exhibits stronger mistake-finding\nperformance than prompting a large model. We release our dataset of\nLLM-generated logical mistakes, BIG-Bench Mistake, to enable further research\ninto locating LLM reasoning mistakes.", "published": "2023-11-14 20:12:38", "link": "http://arxiv.org/abs/2311.08516v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "GLiNER: Generalist Model for Named Entity Recognition using\n  Bidirectional Transformer", "abstract": "Named Entity Recognition (NER) is essential in various Natural Language\nProcessing (NLP) applications. Traditional NER models are effective but limited\nto a set of predefined entity types. In contrast, Large Language Models (LLMs)\ncan extract arbitrary entities through natural language instructions, offering\ngreater flexibility. However, their size and cost, particularly for those\naccessed via APIs like ChatGPT, make them impractical in resource-limited\nscenarios. In this paper, we introduce a compact NER model trained to identify\nany type of entity. Leveraging a bidirectional transformer encoder, our model,\nGLiNER, facilitates parallel entity extraction, an advantage over the slow\nsequential token generation of LLMs. Through comprehensive testing, GLiNER\ndemonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs\nin zero-shot evaluations on various NER benchmarks.", "published": "2023-11-14 20:39:12", "link": "http://arxiv.org/abs/2311.08526v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Financial Regulation", "abstract": "This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.", "published": "2023-11-14 20:58:21", "link": "http://arxiv.org/abs/2311.08533v1", "categories": ["cs.CL", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Low-Rank Adaptation for Multilingual Summarization: An Empirical Study", "abstract": "Although the advancements of pre-trained Large Language Models have\nsignificantly accelerated recent progress in NLP, their ever-increasing size\nposes significant challenges for conventional fine-tuning, especially in\nmemory-intensive tasks. We investigate the potential of Parameter-Efficient\nFine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of\nmultilingual summarization, a task that is both challenging (due to typically\nlong inputs), and relatively unexplored. We conduct an extensive study across\ndifferent data availability scenarios, including high- and low-data settings,\nand cross-lingual transfer, leveraging models of different sizes. Our findings\nreveal that LoRA is competitive with full fine-tuning when trained with high\nquantities of data, and excels in low-data scenarios and cross-lingual\ntransfer. We also study different strategies for few-shot cross-lingual\ntransfer, finding that continued LoRA tuning outperforms full fine-tuning and\nthe dynamic composition of language-specific LoRA modules.", "published": "2023-11-14 22:32:39", "link": "http://arxiv.org/abs/2311.08572v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Evaluating AI Systems for Moral Status Using Self-Reports", "abstract": "As AI systems become more advanced and widely deployed, there will likely be\nincreasing debate over whether AI systems could have conscious experiences,\ndesires, or other states of potential moral significance. It is important to\ninform these discussions with empirical evidence to the extent possible. We\nargue that under the right circumstances, self-reports, or an AI system's\nstatements about its own internal states, could provide an avenue for\ninvestigating whether AI systems have states of moral significance.\nSelf-reports are the main way such states are assessed in humans (\"Are you in\npain?\"), but self-reports from current systems like large language models are\nspurious for many reasons (e.g. often just reflecting what humans would say).\nTo make self-reports more appropriate for this purpose, we propose to train\nmodels to answer many kinds of questions about themselves with known answers,\nwhile avoiding or limiting training incentives that bias self-reports. The hope\nof this approach is that models will develop introspection-like capabilities,\nand that these capabilities will generalize to questions about states of moral\nsignificance. We then propose methods for assessing the extent to which these\ntechniques have succeeded: evaluating self-report consistency across contexts\nand between similar models, measuring the confidence and resilience of models'\nself-reports, and using interpretability to corroborate self-reports. We also\ndiscuss challenges for our approach, from philosophical difficulties in\ninterpreting self-reports to technical reasons why our proposal might fail. We\nhope our discussion inspires philosophers and AI researchers to criticize and\nimprove our proposed methodology, as well as to run experiments to test whether\nself-reports can be made reliable enough to provide information about states of\nmoral significance.", "published": "2023-11-14 22:45:44", "link": "http://arxiv.org/abs/2311.08576v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional\n  Benchmark for Evaluating LLMs on Code Understanding and Generation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on\nassisting humans in programming and facilitating programming automation.\nHowever, existing benchmarks for evaluating the code understanding and\ngeneration capacities of LLMs suffer from severe limitations. First, most\nbenchmarks are insufficient as they focus on a narrow range of popular\nprogramming languages and specific tasks, whereas real-world software\ndevelopment scenarios show a critical need to implement systems with\nmultilingual and multitask programming environments to satisfy diverse\nrequirements. Second, most benchmarks fail to consider the actual executability\nand the consistency of execution results of the generated code. To bridge these\ngaps between existing benchmarks and expectations from practical applications,\nwe introduce CodeScope, an execution-based, multilingual, multitask,\nmultidimensional evaluation benchmark for comprehensively measuring LLM\ncapabilities on coding tasks. CodeScope covers 43 programming languages and\neight coding tasks. It evaluates the coding performance of LLMs from three\ndimensions (perspectives): length, difficulty, and efficiency. To facilitate\nexecution-based evaluations of code generation, we develop MultiCodeEngine, an\nautomated code execution engine that supports 14 programming languages.\nFinally, we systematically evaluate and analyze eight mainstream LLMs and\ndemonstrate the superior breadth and challenges of CodeScope for evaluating\nLLMs on code understanding and generation tasks compared to other benchmarks.\nThe CodeScope benchmark and code are publicly available at\nhttps://github.com/WeixiangYAN/CodeScope.", "published": "2023-11-14 23:18:52", "link": "http://arxiv.org/abs/2311.08588v3", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "AART: AI-Assisted Red-Teaming with Diverse Data Generation for New\n  LLM-powered Applications", "abstract": "Adversarial testing of large language models (LLMs) is crucial for their safe\nand responsible deployment. We introduce a novel approach for automated\ngeneration of adversarial evaluation datasets to test the safety of LLM\ngenerations on new downstream applications. We call it AI-assisted Red-Teaming\n(AART) - an automated alternative to current manual red-teaming efforts. AART\noffers a data generation and augmentation pipeline of reusable and customizable\nrecipes that reduce human effort significantly and enable integration of\nadversarial testing earlier in new product development. AART generates\nevaluation datasets with high diversity of content characteristics critical for\neffective adversarial testing (e.g. sensitive and harmful concepts, specific to\na wide range of cultural and geographic regions and application scenarios). The\ndata generation is steered by AI-assisted recipes to define, scope and\nprioritize diversity within the application context. This feeds into a\nstructured LLM-generation process that scales up evaluation priorities.\nCompared to some state-of-the-art tools, AART shows promising results in terms\nof concept coverage and data quality.", "published": "2023-11-14 23:28:23", "link": "http://arxiv.org/abs/2311.08592v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "In the Red(dit): Social Media and Stock Prices", "abstract": "Spearheaded by retail traders on the website reddit, the GameStop short\nsqueeze of early 2021 shows that social media embeds information that\ncorrelates with market movements. This paper seeks to examine this relationship\nby using daily frequencies of classified comments and buzzwords as additional\nfactors in a Fama-French three factor model. Comments are classified using an\nunsupervised clustering method, while past studies have used pretrained models\nthat are not specific to the domains being studied.", "published": "2023-11-14 16:58:41", "link": "http://arxiv.org/abs/2311.09252v1", "categories": ["cs.SI", "cs.CE", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Large Language Model-Driven Classroom Flipping: Empowering\n  Student-Centric Peer Questioning with Flipped Interaction", "abstract": "Reciprocal questioning is essential for effective teaching and learning,\nfostering active engagement and deeper understanding through collaborative\ninteractions, especially in large classrooms. Can large language model (LLM),\nsuch as OpenAI's GPT (Generative Pre-trained Transformer) series, assist in\nthis? This paper investigates a pedagogical approach of classroom flipping\nbased on flipped interaction in LLMs. Flipped interaction involves using\nlanguage models to prioritize generating questions instead of answers to\nprompts. We demonstrate how traditional classroom flipping techniques,\nincluding Peer Instruction and Just-in-Time Teaching (JiTT), can be enhanced\nthrough flipped interaction techniques, creating student-centric questions for\nhybrid teaching. In particular, we propose a workflow to integrate prompt\nengineering with clicker and JiTT quizzes by a poll-prompt-quiz routine and a\nquiz-prompt-discuss routine to empower students to self-regulate their learning\ncapacity and enable teachers to swiftly personalize training pathways. We\ndevelop an LLM-driven chatbot software that digitizes various elements of\nclassroom flipping and facilitates the assessment of students using these\nroutines to deliver peer-generated questions. We have applied our LLM-driven\nchatbot software for teaching both undergraduate and graduate students from\n2020 to 2022, effectively useful for bridging the gap between teachers and\nstudents in remote teaching during the COVID-19 pandemic years. In particular,\nLLM-driven classroom flipping can be particularly beneficial in large class\nsettings to optimize teaching pace and enable engaging classroom experiences.", "published": "2023-11-14 15:48:19", "link": "http://arxiv.org/abs/2311.14708v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "MechAgents: Large language model multi-agent collaborations can solve\n  mechanics problems, generate new data, and integrate knowledge", "abstract": "Solving mechanics problems using numerical methods requires comprehensive\nintelligent capability of retrieving relevant knowledge and theory,\nconstructing and executing codes, analyzing the results, a task that has thus\nfar mainly been reserved for humans. While emerging AI methods can provide\neffective approaches to solve end-to-end problems, for instance via the use of\ndeep surrogate models or various data analytics strategies, they often lack\nphysical intuition since knowledge is baked into the parametric complement\nthrough training, offering less flexibility when it comes to incorporating\nmathematical or physical insights. By leveraging diverse capabilities of\nmultiple dynamically interacting large language models (LLMs), we can overcome\nthe limitations of conventional approaches and develop a new class of\nphysics-inspired generative machine learning platform, here referred to as\nMechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for\nelasticity problems, via autonomous collaborations. A two-agent team can\neffectively write, execute and self-correct code, in order to apply finite\nelement methods to solve classical elasticity problems in various flavors\n(different boundary conditions, domain geometries, meshes, small/finite\ndeformation and linear/hyper-elastic constitutive laws, and others). For more\ncomplex tasks, we construct a larger group of agents with enhanced division of\nlabor among planning, formulating, coding, executing and criticizing the\nprocess and results. The agents mutually correct each other to improve the\noverall team-work performance in understanding, formulating and validating the\nsolution. Our framework shows the potential of synergizing the intelligence of\nlanguage models, the reliability of physics-based modeling, and the dynamic\ncollaborations among diverse agents, opening novel avenues for automation of\nsolving engineering problems.", "published": "2023-11-14 13:49:03", "link": "http://arxiv.org/abs/2311.08166v1", "categories": ["cs.AI", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Mustango: Toward Controllable Text-to-Music Generation", "abstract": "The quality of the text-to-music models has reached new heights due to recent\nadvancements in diffusion models. The controllability of various musical\naspects, however, has barely been explored. In this paper, we propose Mustango:\na music-domain-knowledge-inspired text-to-music system based on diffusion.\nMustango aims to control the generated music, not only with general text\ncaptions, but with more rich captions that can include specific instructions\nrelated to chords, beats, tempo, and key. At the core of Mustango is MuNet, a\nMusic-Domain-Knowledge-Informed UNet guidance module that steers the generated\nmusic to include the music-specific conditions, which we predict from the text\nprompt, as well as the general text embedding, during the reverse diffusion\nprocess. To overcome the limited availability of open datasets of music with\ntext captions, we propose a novel data augmentation method that includes\naltering the harmonic, rhythmic, and dynamic aspects of music audio and using\nstate-of-the-art Music Information Retrieval methods to extract the music\nfeatures which will then be appended to the existing descriptions in text\nformat. We release the resulting MusicBench dataset which contains over 52K\ninstances and includes music-theory-based descriptions in the caption text.\nThrough extensive experiments, we show that the quality of the music generated\nby Mustango is state-of-the-art, and the controllability through music-specific\ntext prompts greatly outperforms other models such as MusicGen and AudioLDM2.", "published": "2023-11-14 17:54:38", "link": "http://arxiv.org/abs/2311.08355v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized\n  Representation", "abstract": "Most existing neural-based text-to-speech methods rely on extensive datasets\nand face challenges under low-resource condition. In this paper, we introduce a\nnovel semi-supervised text-to-speech synthesis model that learns from both\npaired and unpaired data to address this challenge. The key component of the\nproposed model is a dynamic quantized representation module, which is\nintegrated into a sequential autoencoder. When given paired data, the module\nincorporates a trainable codebook that learns quantized representations under\nthe supervision of the paired data. However, due to the limited paired data in\nlow-resource scenario, these paired data are difficult to cover all phonemes.\nThen unpaired data is fed to expand the dynamic codebook by adding quantized\nrepresentation vectors that are sufficiently distant from the existing ones\nduring training. Experiments show that with less than 120 minutes of paired\ndata, the proposed method outperforms existing methods in both subjective and\nobjective metrics.", "published": "2023-11-14 07:40:08", "link": "http://arxiv.org/abs/2311.07965v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion", "abstract": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn\ncompact yet expressive features and a powerful decoder in a single network. A\nchallenging problem as such results in unwelcome complexity increase and\ninferior speech quality. In this paper, we propose to separate the\nrepresentation learning and information reconstruction tasks. We leverage an\nend-to-end codec for learning low-dimensional discrete tokens and employ a\nlatent diffusion model to de-quantize coded features into a high-dimensional\ncontinuous space, relieving the decoder's burden of de-quantizing and\nupsampling. To mitigate the issue of over-smooth generation, we introduce\nmidway-infilling with less noise reduction and stronger conditioning. In\nablation studies, we investigate the hyperparameters for midway-infilling and\nlatent diffusion space with different dimensions. Subjective listening tests\nshow that our model outperforms the state-of-the-art at two low bitrates, 1.5\nand 3 kbps. Codes and samples of this work are available on our webpage.", "published": "2023-11-14 17:19:40", "link": "http://arxiv.org/abs/2311.08330v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhanced Generative Adversarial Networks for Unseen Word Generation from\n  EEG Signals", "abstract": "Recent advances in brain-computer interface (BCI) technology, particularly\nbased on generative adversarial networks (GAN), have shown great promise for\nimproving decoding performance for BCI. Within the realm of Brain-Computer\nInterfaces (BCI), GANs find application in addressing many areas. They serve as\na valuable tool for data augmentation, which can solve the challenge of limited\ndata availability, and synthesis, effectively expanding the dataset and\ncreating novel data formats, thus enhancing the robustness and adaptability of\nBCI systems. Research in speech-related paradigms has significantly expanded,\nwith a critical impact on the advancement of assistive technologies and\ncommunication support for individuals with speech impairments. In this study,\nGANs were investigated, particularly for the BCI field, and applied to generate\ntext from EEG signals. The GANs could generalize all subjects and decode unseen\nwords, indicating its ability to capture underlying speech patterns consistent\nacross different individuals. The method has practical applications in neural\nsignal-based speech recognition systems and communication aids for individuals\nwith speech difficulties.", "published": "2023-11-14 00:20:25", "link": "http://arxiv.org/abs/2311.17923v1", "categories": ["eess.AS", "cs.HC"], "primary_category": "eess.AS"}
{"title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice\n  Conversion", "abstract": "Research on deep learning-powered voice conversion (VC) in speech-to-speech\nscenarios is getting increasingly popular. Although many of the works in the\nfield of voice conversion share a common global pipeline, there is a\nconsiderable diversity in the underlying structures, methods, and neural\nsub-blocks used across research efforts. Thus, obtaining a comprehensive\nunderstanding of the reasons behind the choice of the different methods in the\nvoice conversion pipeline can be challenging, and the actual hurdles in the\nproposed solutions are often unclear. To shed light on these aspects, this\npaper presents a scoping review that explores the use of deep learning in\nspeech analysis, synthesis, and disentangled speech representation learning\nwithin modern voice conversion systems. We screened 621 publications from more\nthan 38 different venues between the years 2017 and 2023, followed by an\nin-depth review of a final database consisting of 123 eligible studies. Based\non the review, we summarise the most frequently used approaches to voice\nconversion based on deep learning and highlight common pitfalls within the\ncommunity. Lastly, we condense the knowledge gathered, identify main challenges\nand provide recommendations for future research directions.", "published": "2023-11-14 12:03:46", "link": "http://arxiv.org/abs/2311.08104v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Variational Auto-Encoder Architectures, Configurations, and\n  Datasets for Generative Music Explainable AI", "abstract": "Generative AI models for music and the arts in general are increasingly\ncomplex and hard to understand. The field of eXplainable AI (XAI) seeks to make\ncomplex and opaque AI models such as neural networks more understandable to\npeople. One approach to making generative AI models more understandable is to\nimpose a small number of semantically meaningful attributes on generative AI\nmodels. This paper contributes a systematic examination of the impact that\ndifferent combinations of Variational Auto-Encoder models (MeasureVAE and\nAdversarialVAE), configurations of latent space in the AI model (from 4 to 256\nlatent dimensions), and training datasets (Irish folk, Turkish folk, Classical,\nand pop) have on music generation performance when 2 or 4 meaningful musical\nattributes are imposed on the generative model. To date there have been no\nsystematic comparisons of such models at this level of combinatorial detail.\nOur findings show that MeasureVAE has better reconstruction performance than\nAdversarialVAE which has better musical attribute independence. Results\ndemonstrate that MeasureVAE was able to generate music across music genres with\ninterpretable musical dimensions of control, and performs best with low\ncomplexity music such a pop and rock. We recommend that a 32 or 64 latent\ndimensional space is optimal for 4 regularised dimensions when using MeasureVAE\nto generate music across genres. Our results are the first detailed comparisons\nof configurations of state-of-the-art generative AI models for music and can be\nused to help select and configure AI models, musical features, and datasets for\nmore understandable generation of music.", "published": "2023-11-14 17:27:30", "link": "http://arxiv.org/abs/2311.08336v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ChoralSynth: Synthetic Dataset of Choral Singing", "abstract": "Choral singing, a widely practiced form of ensemble singing, lacks\ncomprehensive datasets in the realm of Music Information Retrieval (MIR)\nresearch, due to challenges arising from the requirement to curate multitrack\nrecordings. To address this, we devised a novel methodology, leveraging\nstate-of-the-art synthesizers to create and curate quality renditions. The\nscores were sourced from Choral Public Domain Library(CPDL). This work is done\nin collaboration with a diverse team of musicians, software engineers and\nresearchers. The resulting dataset, complete with its associated metadata, and\nmethodology is released as part of this work, opening up new avenues for\nexploration and advancement in the field of singing voice research.", "published": "2023-11-14 17:48:27", "link": "http://arxiv.org/abs/2311.08350v2", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
