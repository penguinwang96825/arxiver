{"title": "Strategy of the Negative Sampling for Training Retrieval-Based Dialogue\n  Systems", "abstract": "The article describes the new approach for quality improvement of automated\ndialogue systems for customer support service. Analysis produced in the paper\ndemonstrates the dependency of the quality of the retrieval-based dialogue\nsystem quality on the choice of negative responses. The proposed approach\nimplies choosing the negative samples according to the distribution of\nresponses in the train set. In this implementation the negative samples are\nrandomly chosen from the original response distribution and from the\n\"artificial\" distribution of negative responses, such as uniform distribution\nor the distribution obtained by transformation of the original one. The results\nobtained for the implemented systems and reported in this paper confirm the\nsignificant improvement of automated dialogue systems quality in case of using\nthe negative responses from transformed distribution.", "published": "2018-11-24 08:04:42", "link": "http://arxiv.org/abs/1811.09785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimation of Inter-Sentiment Correlations Employing Deep Neural Network\n  Models", "abstract": "This paper focuses on sentiment mining and sentiment correlation analysis of\nweb events. Although neural network models have contributed a lot to mining\ntext information, little attention is paid to analysis of the inter-sentiment\ncorrelations. This paper fills the gap between sentiment calculation and\ninter-sentiment correlations. In this paper, the social emotion is divided into\nsix categories: love, joy, anger, sadness, fear, and surprise. Two deep neural\nnetwork models are presented for sentiment calculation. Three datasets - the\ntitles, the bodies, the comments of news articles - are collected, covering\nboth objective and subjective texts in varying lengths (long and short). From\neach dataset, three kinds of features are extracted: explicit expression,\nimplicit expression, and alphabet characters. The performance of the two models\nare analyzed, with respect to each of the three kinds of the features. There is\ncontroversial phenomenon on the interpretation of anger (fn) and love (gd). In\nsubjective text, other emotions are easily to be considered as anger. By\ncontrast, in objective news bodies and titles, it is easy to regard text as\ncaused love (gd). It means, journalist may want to arouse emotion love by\nwriting news, but cause anger after the news is published. This result reflects\nthe sentiment complexity and unpredictability.", "published": "2018-11-24 03:47:04", "link": "http://arxiv.org/abs/1811.09755v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HCqa: Hybrid and Complex Question Answering on Textual Corpus and\n  Knowledge Graph", "abstract": "Question Answering (QA) systems provide easy access to the vast amount of\nknowledge without having to know the underlying complex structure of the\nknowledge. The research community has provided ad hoc solutions to the key QA\ntasks, including named entity recognition and disambiguation, relation\nextraction and query building. Furthermore, some have integrated and composed\nthese components to implement many tasks automatically and efficiently.\nHowever, in general, the existing solutions are limited to simple and short\nquestions and still do not address complex questions composed of several\nsub-questions. Exploiting the answer to complex questions is further challenged\nif it requires integrating knowledge from unstructured data sources, i.e.,\ntextual corpus, as well as structured data sources, i.e., knowledge graphs. In\nthis paper, an approach (HCqa) is introduced for dealing with complex questions\nrequiring federating knowledge from a hybrid of heterogeneous data sources\n(structured and unstructured). We contribute in developing (i) a decomposition\nmechanism which extracts sub-questions from potentially long and complex input\nquestions, (ii) a novel comprehensive schema, first of its kind, for extracting\nand annotating relations, and (iii) an approach for executing and aggregating\nthe answers of sub-questions. The evaluation of HCqa showed a superior accuracy\nin the fundamental tasks, such as relation extraction, as well as the\nfederation task.", "published": "2018-11-24 07:03:53", "link": "http://arxiv.org/abs/1811.10986v5", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Connecting the Dots Between MLE and RL for Sequence Prediction", "abstract": "Sequence prediction models can be learned from example sequences with a\nvariety of training algorithms. Maximum likelihood learning is simple and\nefficient, yet can suffer from compounding error at test time. Reinforcement\nlearning such as policy gradient addresses the issue but can have prohibitively\npoor exploration efficiency. A rich set of other algorithms such as RAML, SPG,\nand data noising, have also been developed from different perspectives. This\npaper establishes a formal connection between these algorithms. We present a\ngeneralized entropy regularized policy optimization formulation, and show that\nthe apparently distinct algorithms can all be reformulated as special instances\nof the framework, with the only difference being the configurations of a reward\nfunction and a couple of hyperparameters. The unified interpretation offers a\nsystematic view of the varying properties of exploration and learning\nefficiency. Besides, inspired from the framework, we present a new algorithm\nthat dynamically interpolates among the family of algorithms for scheduled\nsequence model learning. Experiments on machine translation, text\nsummarization, and game imitation learning demonstrate the superiority of the\nproposed algorithm.", "published": "2018-11-24 01:33:39", "link": "http://arxiv.org/abs/1811.09740v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Recurrently Controlled Recurrent Networks", "abstract": "Recurrent neural networks (RNNs) such as long short-term memory and gated\nrecurrent units are pivotal building blocks across a broad spectrum of sequence\nmodeling problems. This paper proposes a recurrently controlled recurrent\nnetwork (RCRN) for expressive and powerful sequence encoding. More concretely,\nthe key idea behind our approach is to learn the recurrent gating functions\nusing recurrent networks. Our architecture is split into two components - a\ncontroller cell and a listener cell whereby the recurrent controller actively\ninfluences the compositionality of the listener cell. We conduct extensive\nexperiments on a myriad of tasks in the NLP domain such as sentiment analysis\n(SST, IMDb, Amazon reviews, etc.), question classification (TREC), entailment\nclassification (SNLI, SciTail), answer selection (WikiQA, TrecQA) and reading\ncomprehension (NarrativeQA). Across all 26 datasets, our results demonstrate\nthat RCRN not only consistently outperforms BiLSTMs but also stacked BiLSTMs,\nsuggesting that our controller architecture might be a suitable replacement for\nthe widely adopted stacked architecture.", "published": "2018-11-24 08:15:50", "link": "http://arxiv.org/abs/1811.09786v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Latent Dirichlet Allocation with Residual Convolutional Neural Network\n  Applied in Evaluating Credibility of Chinese Listed Companies", "abstract": "This project demonstrated a methodology to estimating cooperate credibility\nwith a Natural Language Processing approach. As cooperate transparency impacts\nboth the credibility and possible future earnings of the firm, it is an\nimportant factor to be considered by banks and investors on risk assessments of\nlisted firms. This approach of estimating cooperate credibility can bypass\nhuman bias and inconsistency in the risk assessment, the use of large\nquantitative data and neural network models provides more accurate estimation\nin a more efficient manner compare to manual assessment. At the beginning, the\nmodel will employs Latent Dirichlet Allocation and THU Open Chinese Lexicon\nfrom Tsinghua University to classify topics in articles which are potentially\nrelated to corporate credibility. Then with the keywords related to each\ntopics, we trained a residual convolutional neural network with data labeled\naccording to surveys of fund manager and accountant's opinion on corporate\ncredibility. After the training, we run the model with preprocessed news\nreports regarding to all of the 3065 listed companies, the model is supposed to\ngive back companies ranking based on the level of their transparency.", "published": "2018-11-24 17:50:41", "link": "http://arxiv.org/abs/1811.11017v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
