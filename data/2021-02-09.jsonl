{"title": "Efficient Retrieval Augmented Generation from Unstructured Knowledge for\n  Task-Oriented Dialog", "abstract": "This paper summarizes our work on the first track of the ninth Dialog System\nTechnology Challenge (DSTC 9), \"Beyond Domain APIs: Task-oriented\nConversational Modeling with Unstructured Knowledge Access\". The goal of the\ntask is to generate responses to user turns in a task-oriented dialog that\nrequire knowledge from unstructured documents. The task is divided into three\nsubtasks: detection, selection and generation. In order to be compute\nefficient, we formulate the selection problem in terms of hierarchical\nclassification steps. We achieve our best results with this model.\nAlternatively, we employ siamese sequence embedding models, referred to as\nDense Knowledge Retrieval, to retrieve relevant documents. This method further\nreduces the computation time by a factor of more than 100x at the cost of\ndegradation in R@1 of 5-6% compared to the first model. Then for either\napproach, we use Retrieval Augmented Generation to generate responses based on\nmultiple selected snippets and we show how the method can be used to fine-tune\ntrained embeddings.", "published": "2021-02-09 04:50:35", "link": "http://arxiv.org/abs/2102.04643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Query Rewriting with Self-supervised Learning", "abstract": "Context modeling plays a critical role in building multi-turn dialogue\nsystems. Conversational Query Rewriting (CQR) aims to simplify the multi-turn\ndialogue modeling into a single-turn problem by explicitly rewriting the\nconversational query into a self-contained utterance. However, existing\napproaches rely on massive supervised training data, which is labor-intensive\nto annotate. And the detection of the omitted important information from\ncontext can be further improved. Besides, intent consistency constraint between\ncontextual query and rewritten query is also ignored. To tackle these issues,\nwe first propose to construct a large-scale CQR dataset automatically via\nself-supervised learning, which does not need human annotation. Then we\nintroduce a novel CQR model Teresa based on Transformer, which is enhanced by\nself-attentive keywords detection and intent consistency constraint. Finally,\nwe conduct extensive experiments on two public datasets. Experimental results\ndemonstrate that our proposed model outperforms existing CQR baselines\nsignificantly, and also prove the effectiveness of self-supervised learning on\nimproving the CQR performance.", "published": "2021-02-09 08:57:53", "link": "http://arxiv.org/abs/2102.04708v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bayesian Transformer Language Models for Speech Recognition", "abstract": "State-of-the-art neural language models (LMs) represented by Transformers are\nhighly complex. Their use of fixed, deterministic parameter estimates fail to\naccount for model uncertainty and lead to over-fitting and poor generalization\nwhen given limited training data. In order to address these issues, this paper\nproposes a full Bayesian learning framework for Transformer LM estimation.\nEfficient variational inference based approaches are used to estimate the\nlatent parameter posterior distributions associated with different parts of the\nTransformer model architecture including multi-head self-attention, feed\nforward and embedding layers. Statistically significant word error rate (WER)\nreductions up to 0.5\\% absolute (3.18\\% relative) and consistent perplexity\ngains were obtained over the baseline Transformer LMs on state-of-the-art\nSwitchboard corpus trained LF-MMI factored TDNN systems with i-Vector speaker\nadaptation. Performance improvements were also obtained on a cross domain LM\nadaptation task requiring porting a Transformer LM trained on the Switchboard\nand Fisher data to a low-resource DementiaBank elderly speech corpus.", "published": "2021-02-09 10:55:27", "link": "http://arxiv.org/abs/2102.04754v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Broader terms curriculum mapping: Using natural language processing and\n  visual-supported communication to create representative program planning\n  experiences", "abstract": "Accreditation bodies call for curriculum development processes open to all\nstakeholders, reflecting viewpoints of students, industry, university faculty\nand society. However, communication difficulties between faculty and\nnon-faculty groups leave unexplored an immense collaboration potential. Using\nclassification of learning objectives, natural language processing, and data\nvisualization, this paper presents a method to deliver program plan\nrepresentations that are universal, self-explanatory, and empowering. A simple\nexample shows how the method contributes to representative program planning\nexperiences and a case study is used to confirm the method's accuracy and\nutility.", "published": "2021-02-09 13:27:04", "link": "http://arxiv.org/abs/2102.04811v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Modality-Specific Representations with Self-Supervised\n  Multi-Task Learning for Multimodal Sentiment Analysis", "abstract": "Representation Learning is a significant and challenging task in multimodal\nlearning. Effective modality representations should contain two parts of\ncharacteristics: the consistency and the difference. Due to the unified\nmultimodal annotation, existing methods are restricted in capturing\ndifferentiated information. However, additional uni-modal annotations are high\ntime- and labor-cost. In this paper, we design a label generation module based\non the self-supervised learning strategy to acquire independent unimodal\nsupervisions. Then, joint training the multi-modal and uni-modal tasks to learn\nthe consistency and difference, respectively. Moreover, during the training\nstage, we design a weight-adjustment strategy to balance the learning progress\namong different subtasks. That is to guide the subtasks to focus on samples\nwith a larger difference between modality supervisions. Last, we conduct\nextensive experiments on three public multimodal baseline datasets. The\nexperimental results validate the reliability and stability of auto-generated\nunimodal supervisions. On MOSI and MOSEI datasets, our method surpasses the\ncurrent state-of-the-art methods. On the SIMS dataset, our method achieves\ncomparable performance than human-annotated unimodal labels. The full codes are\navailable at https://github.com/thuiar/Self-MM.", "published": "2021-02-09 14:05:02", "link": "http://arxiv.org/abs/2102.04830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NewsBERT: Distilling Pre-trained Language Model for Intelligent News\n  Application", "abstract": "Pre-trained language models (PLMs) like BERT have made great progress in NLP.\nNews articles usually contain rich textual information, and PLMs have the\npotentials to enhance news text modeling for various intelligent news\napplications like news recommendation and retrieval. However, most existing\nPLMs are in huge size with hundreds of millions of parameters. Many online news\napplications need to serve millions of users with low latency tolerance, which\nposes huge challenges to incorporating PLMs in these scenarios. Knowledge\ndistillation techniques can compress a large PLM into a much smaller one and\nmeanwhile keeps good performance. However, existing language models are\npre-trained and distilled on general corpus like Wikipedia, which has some gaps\nwith the news domain and may be suboptimal for news intelligence. In this\npaper, we propose NewsBERT, which can distill PLMs for efficient and effective\nnews intelligence. In our approach, we design a teacher-student joint learning\nand distillation framework to collaboratively learn both teacher and student\nmodels, where the student model can learn from the learning experience of the\nteacher model. In addition, we propose a momentum distillation method by\nincorporating the gradients of teacher model into the update of student model\nto better transfer useful knowledge learned by the teacher model. Extensive\nexperiments on two real-world datasets with three tasks show that NewsBERT can\neffectively improve the model performance in various intelligent news\napplications with much smaller models.", "published": "2021-02-09 15:41:12", "link": "http://arxiv.org/abs/2102.04887v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BembaSpeech: A Speech Recognition Corpus for the Bemba Language", "abstract": "We present a preprocessed, ready-to-use automatic speech recognition corpus,\nBembaSpeech, consisting over 24 hours of read speech in the Bemba language, a\nwritten but low-resourced language spoken by over 30% of the population in\nZambia. To assess its usefulness for training and testing ASR systems for\nBemba, we train an end-to-end Bemba ASR system by fine-tuning a pre-trained\nDeepSpeech English model on the training portion of the BembaSpeech corpus. Our\nbest model achieves a word error rate (WER) of 54.78%. The results show that\nthe corpus can be used for building ASR systems for Bemba. The corpus and\nmodels are publicly released at https://github.com/csikasote/BembaSpeech.", "published": "2021-02-09 15:42:00", "link": "http://arxiv.org/abs/2102.04889v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging cross-platform data to improve automated hate speech\n  detection", "abstract": "Hate speech is increasingly prevalent online, and its negative outcomes\ninclude increased prejudice, extremism, and even offline hate crime. Automatic\ndetection of online hate speech can help us to better understand these impacts.\nHowever, while the field has recently progressed through advances in natural\nlanguage processing, challenges still remain. In particular, most existing\napproaches for hate speech detection focus on a single social media platform in\nisolation. This limits both the use of these models and their validity, as the\nnature of language varies from platform to platform. Here we propose a new\ncross-platform approach to detect hate speech which leverages multiple datasets\nand classification models from different platforms and trains a superlearner\nthat can combine existing and novel training data to improve detection and\nincrease model applicability. We demonstrate how this approach outperforms\nexisting models, and achieves good performance when tested on messages from\nnovel social media platforms not included in the original training data.", "published": "2021-02-09 15:52:34", "link": "http://arxiv.org/abs/2102.04895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping Relation Extractors using Syntactic Search by Examples", "abstract": "The advent of neural-networks in NLP brought with it substantial improvements\nin supervised relation extraction. However, obtaining a sufficient quantity of\ntraining data remains a key challenge. In this work we propose a process for\nbootstrapping training datasets which can be performed quickly by\nnon-NLP-experts. We take advantage of search engines over syntactic-graphs\n(Such as Shlain et al. (2020)) which expose a friendly by-example syntax. We\nuse these to obtain positive examples by searching for sentences that are\nsyntactically similar to user input examples. We apply this technique to\nrelations from TACRED and DocRED and show that the resulting models are\ncompetitive with models trained on manually annotated data and on data obtained\nfrom distant supervision. The models also outperform models trained using NLG\ndata augmentation techniques. Extending the search-based approach with the NLG\nmethod further improves the results.", "published": "2021-02-09 18:17:59", "link": "http://arxiv.org/abs/2102.05007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transfer Learning Approach for Arabic Offensive Language Detection\n  System -- BERT-Based Model", "abstract": "Developing a system to detect online offensive language is very important to\nthe health and the security of online users. Studies have shown that cyberhate,\nonline harassment and other misuses of technology are on the rise, particularly\nduring the global Coronavirus pandemic in 2020. According to the latest report\nby the Anti-Defamation League (ADL), 35% of online users reported online\nharassment related to their identity-based characteristics, which is a 3%\nincrease over 2019. Applying advanced techniques from the Natural Language\nProcessing (NLP) field to support the development of an online hate-free\ncommunity is a critical task for social justice. Transfer learning enhances the\nperformance of the classifier by allowing the transfer of knowledge from one\ndomain or one dataset to others that have not been seen before, thus,\nsupporting the classifier to be more generalizable. In our study, we apply the\nprinciples of transfer learning cross multiple Arabic offensive language\ndatasets to compare the effects on system performance. This study aims at\ninvestigating the effects of fine-tuning and training Bidirectional Encoder\nRepresentations from Transformers (BERT) model on multiple Arabic offensive\nlanguage datasets individually and testing it using other datasets\nindividually. Our experiment starts with a comparison among multiple BERT\nmodels to guide the selection of the main model that is used for our study. The\nstudy also investigates the effects of concatenating all datasets to be used\nfor fine-tuning and training BERT model. Our results demonstrate the limited\neffects of transfer learning on the performance of the classifiers,\nparticularly for highly dialectic comments.", "published": "2021-02-09 04:58:18", "link": "http://arxiv.org/abs/2102.05708v1", "categories": ["cs.CL", "cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Intent Detection and Slot Filling with Wheel-Graph Attention\n  Networks", "abstract": "Intent detection and slot filling are two fundamental tasks for building a\nspoken language understanding (SLU) system. Multiple deep learning-based joint\nmodels have demonstrated excellent results on the two tasks. In this paper, we\npropose a new joint model with a wheel-graph attention network (Wheel-GAT)\nwhich is able to model interrelated connections directly for intent detection\nand slot filling. To construct a graph structure for utterances, we create\nintent nodes, slot nodes, and directed edges. Intent nodes can provide\nutterance-level semantic information for slot filling, while slot nodes can\nalso provide local keyword information for intent. Experiments show that our\nmodel outperforms multiple baselines on two public datasets. Besides, we also\ndemonstrate that using Bidirectional Encoder Representation from Transformer\n(BERT) model further boosts the performance in the SLU task.", "published": "2021-02-09 02:37:56", "link": "http://arxiv.org/abs/2102.04610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Statistically Profiling Biases in Natural Language Reasoning Datasets\n  and Models", "abstract": "Recent work has indicated that many natural language understanding and\nreasoning datasets contain statistical cues that may be taken advantaged of by\nNLP models whose capability may thus be grossly overestimated. To discover the\npotential weakness in the models, some human-designed stress tests have been\nproposed but they are expensive to create and do not generalize to arbitrary\nmodels. We propose a light-weight and general statistical profiling framework,\nICQ (I-See-Cue), which automatically identifies possible biases in any\nmultiple-choice NLU datasets without the need to create any additional test\ncases, and further evaluates through blackbox testing the extent to which\nmodels may exploit these biases.", "published": "2021-02-09 03:51:53", "link": "http://arxiv.org/abs/2102.04632v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding\n  and Generation", "abstract": "Benchmark datasets have a significant impact on accelerating research in\nprogramming language tasks. In this paper, we introduce CodeXGLUE, a benchmark\ndataset to foster machine learning research for program understanding and\ngeneration. CodeXGLUE includes a collection of 10 tasks across 14 datasets and\na platform for model evaluation and comparison. CodeXGLUE also features three\nbaseline systems, including the BERT-style, GPT-style, and Encoder-Decoder\nmodels, to make it easy for researchers to use the platform. The availability\nof such data and baselines can help the development and validation of new\nmethods that can be applied to various program understanding and generation\nproblems.", "published": "2021-02-09 06:16:25", "link": "http://arxiv.org/abs/2102.04664v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Telling the What while Pointing to the Where: Multimodal Queries for\n  Image Retrieval", "abstract": "Most existing image retrieval systems use text queries as a way for the user\nto express what they are looking for. However, fine-grained image retrieval\noften requires the ability to also express where in the image the content they\nare looking for is. The text modality can only cumbersomely express such\nlocalization preferences, whereas pointing is a more natural fit. In this\npaper, we propose an image retrieval setup with a new form of multimodal\nqueries, where the user simultaneously uses both spoken natural language (the\nwhat) and mouse traces over an empty canvas (the where) to express the\ncharacteristics of the desired target image. We then describe simple\nmodifications to an existing image retrieval model, enabling it to operate in\nthis setup. Qualitative and quantitative experiments show that our model\neffectively takes this spatial guidance into account, and provides\nsignificantly more accurate retrieval results compared to text-only equivalent\nsystems.", "published": "2021-02-09 17:54:34", "link": "http://arxiv.org/abs/2102.04980v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "In Defense of Scene Graphs for Image Captioning", "abstract": "The mainstream image captioning models rely on Convolutional Neural Network\n(CNN) image features to generate captions via recurrent models. Recently, image\nscene graphs have been used to augment captioning models so as to leverage\ntheir structural semantics, such as object entities, relationships and\nattributes. Several studies have noted that the naive use of scene graphs from\na black-box scene graph generator harms image captioning performance and that\nscene graph-based captioning models have to incur the overhead of explicit use\nof image features to generate decent captions. Addressing these challenges, we\npropose \\textbf{SG2Caps}, a framework that utilizes only the scene graph labels\nfor competitive image captioning performance. The basic idea is to close the\nsemantic gap between the two scene graphs - one derived from the input image\nand the other from its caption. In order to achieve this, we leverage the\nspatial location of objects and the Human-Object-Interaction (HOI) labels as an\nadditional HOI graph. SG2Caps outperforms existing scene graph-only captioning\nmodels by a large margin, indicating scene graphs as a promising representation\nfor image captioning. Direct utilization of scene graph labels avoids expensive\ngraph convolutions over high-dimensional CNN features resulting in 49% fewer\ntrainable parameters. Our code is available at:\nhttps://github.com/Kien085/SG2Caps", "published": "2021-02-09 18:00:53", "link": "http://arxiv.org/abs/2102.04990v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Decontextualization: Making Sentences Stand-Alone", "abstract": "Models for question answering, dialogue agents, and summarization often\ninterpret the meaning of a sentence in a rich context and use that meaning in a\nnew context. Taking excerpts of text can be problematic, as key pieces may not\nbe explicit in a local window. We isolate and define the problem of sentence\ndecontextualization: taking a sentence together with its context and rewriting\nit to be interpretable out of context, while preserving its meaning. We\ndescribe an annotation procedure, collect data on the Wikipedia corpus, and use\nthe data to train models to automatically decontextualize sentences. We present\npreliminary studies that show the value of sentence decontextualization in a\nuser facing task, and as preprocessing for systems that perform document\nunderstanding. We argue that decontextualization is an important subtask in\nmany downstream applications, and that the definitions and resources provided\ncan benefit tasks that operate on sentences that occur in a richer context.", "published": "2021-02-09 22:52:37", "link": "http://arxiv.org/abs/2102.05169v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A comparative study of two-dimensional vocal tract acoustic modeling\n  based on Finite-Difference Time-Domain methods", "abstract": "The two-dimensional (2D) numerical approaches for vocal tract (VT) modelling\ncan afford a better balance between the low computational cost and accurate\nrendering of acoustic wave propagation. However, they require a high\nspatio-temporal resolution in the numerical scheme for a precise estimation of\nacoustic formants at the simulation run-time expense. We have recently proposed\na new VT acoustic modelling technique, known as the 2.5D Finite-Difference\nTime-Domain (2.5D FDTD), which extends the existing 2D FDTD approach by adding\ntube depth to its acoustic wave solver. In this work, first, the simulated\nacoustic outputs of our new model are shown to be comparable with the 2D FDTD\nand a realistic 3D FEM VT model at a low spatio-temporal resolution. Next, a\nradiation model is developed by including a circular baffle around the VT as\nhead geometry. The transfer functions of the radiation model are analyzed using\nfive different vocal tract shapes for vowel sounds /a/, /e/, /i/, /o/ and /u/.", "published": "2021-02-09 00:40:52", "link": "http://arxiv.org/abs/2102.04588v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Role of the Input in Natural Language Video Description", "abstract": "Natural Language Video Description (NLVD) has recently received strong\ninterest in the Computer Vision, Natural Language Processing (NLP), Multimedia,\nand Autonomous Robotics communities. The State-of-the-Art (SotA) approaches\nobtained remarkable results when tested on the benchmark datasets. However,\nthose approaches poorly generalize to new datasets. In addition, none of the\nexisting works focus on the processing of the input to the NLVD systems, which\nis both visual and textual. In this work, it is presented an extensive study\ndealing with the role of the visual input, evaluated with respect to the\noverall NLP performance. This is achieved performing data augmentation of the\nvisual component, applying common transformations to model camera distortions,\nnoise, lighting, and camera positioning, that are typical in real-world\noperative scenarios. A t-SNE based analysis is proposed to evaluate the effects\nof the considered transformations on the overall visual data distribution. For\nthis study, it is considered the English subset of Microsoft Research Video\nDescription (MSVD) dataset, which is used commonly for NLVD. It was observed\nthat this dataset contains a relevant amount of syntactic and semantic errors.\nThese errors have been amended manually, and the new version of the dataset\n(called MSVD-v2) is used in the experimentation. The MSVD-v2 dataset is\nreleased to help to gain insight into the NLVD problem.", "published": "2021-02-09 19:00:35", "link": "http://arxiv.org/abs/2102.05067v1", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue\n  with Pre-Trained Language Models", "abstract": "Attention-based pre-trained language models such as GPT-2 brought\nconsiderable progress to end-to-end dialogue modelling. However, they also\npresent considerable risks for task-oriented dialogue, such as lack of\nknowledge grounding or diversity. To address these issues, we introduce\nmodified training objectives for language model finetuning, and we employ\nmassive data augmentation via back-translation to increase the diversity of the\ntraining data. We further examine the possibilities of combining data from\nmultiples sources to improve performance on the target dataset. We carefully\nevaluate our contributions with both human and automatic methods. Our model\nsubstantially outperforms the baseline on the MultiWOZ data and shows\ncompetitive performance with state of the art in both automatic and human\nevaluation.", "published": "2021-02-09 20:53:34", "link": "http://arxiv.org/abs/2102.05126v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sparsification via Compressed Sensing for Automatic Speech Recognition", "abstract": "In order to achieve high accuracy for machine learning (ML) applications, it\nis essential to employ models with a large number of parameters. Certain\napplications, such as Automatic Speech Recognition (ASR), however, require\nreal-time interactions with users, hence compelling the model to have as low\nlatency as possible. Deploying large scale ML applications thus necessitates\nmodel quantization and compression, especially when running ML models on\nresource constrained devices. For example, by forcing some of the model weight\nvalues into zero, it is possible to apply zero-weight compression, which\nreduces both the model size and model reading time from the memory. In the\nliterature, such methods are referred to as sparse pruning. The fundamental\nquestions are when and which weights should be forced to zero, i.e. be pruned.\nIn this work, we propose a compressed sensing based pruning (CSP) approach to\neffectively address those questions. By reformulating sparse pruning as a\nsparsity inducing and compression-error reduction dual problem, we introduce\nthe classic compressed sensing process into the ML model training process.\nUsing ASR task as an example, we show that CSP consistently outperforms\nexisting approaches in the literature.", "published": "2021-02-09 16:41:31", "link": "http://arxiv.org/abs/2102.04932v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Hallmarks of Human-Machine Collaboration: A framework for assessment in\n  the DARPA Communicating with Computers Program", "abstract": "There is a growing desire to create computer systems that can communicate\neffectively to collaborate with humans on complex, open-ended activities.\nAssessing these systems presents significant challenges. We describe a\nframework for evaluating systems engaged in open-ended complex scenarios where\nevaluators do not have the luxury of comparing performance to a single right\nanswer. This framework has been used to evaluate human-machine creative\ncollaborations across story and music generation, interactive block building,\nand exploration of molecular mechanisms in cancer. These activities are\nfundamentally different from the more constrained tasks performed by most\ncontemporary personal assistants as they are generally open-ended, with no\nsingle correct solution, and often no obvious completion criteria.\n  We identified the Key Properties that must be exhibited by successful\nsystems. From there we identified \"Hallmarks\" of success -- capabilities and\nfeatures that evaluators can observe that would be indicative of progress\ntoward achieving a Key Property. In addition to being a framework for\nassessment, the Key Properties and Hallmarks are intended to serve as goals in\nguiding research direction.", "published": "2021-02-09 17:13:53", "link": "http://arxiv.org/abs/2102.04958v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA", "cs.MM", "I.2.11; I.2.7; H.5.2"], "primary_category": "cs.HC"}
{"title": "Real-time Monaural Speech Enhancement With Short-time Discrete Cosine\n  Transform", "abstract": "Speech enhancement algorithms based on deep learning have been improved in\nterms of speech intelligibility and perceptual quality greatly. Many methods\nfocus on enhancing the amplitude spectrum while reconstructing speech using the\nmixture phase. Since the clean phase is very important and difficult to\npredict, the performance of these methods will be limited. Some researchers\nattempted to estimate the phase spectrum directly or indirectly, but the effect\nis not ideal. Recently, some studies proposed the complex-valued model and\nachieved state-of-the-art performance, such as deep complex convolution\nrecurrent network (DCCRN). However, the computation of the model is huge. To\nreduce the complexity and further improve the performance, we propose a novel\nmethod using discrete cosine transform as the input in this paper, called deep\ncosine transform convolutional recurrent network (DCTCRN). Experimental results\nshow that DCTCRN achieves state-of-the-art performance both on objective and\nsubjective metrics. Compared with noisy mixtures, the mean opinion score (MOS)\nincreased by 0.46 (2.86 to 3.32) absolute processed by the proposed model with\nonly 2.86M parameters.", "published": "2021-02-09 03:34:58", "link": "http://arxiv.org/abs/2102.04629v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Tr\u00e4umerAI: Dreaming Music with StyleGAN", "abstract": "The goal of this paper to generate a visually appealing video that responds\nto music with a neural network so that each frame of the video reflects the\nmusical characteristics of the corresponding audio clip. To achieve the goal,\nwe propose a neural music visualizer directly mapping deep music embeddings to\nstyle embeddings of StyleGAN, named Tr\\\"aumerAI, which consists of a music\nauto-tagging model using short-chunk CNN and StyleGAN2 pre-trained on WikiArt\ndataset. Rather than establishing an objective metric between musical and\nvisual semantics, we manually labeled the pairs in a subjective manner. An\nannotator listened to 100 music clips of 10 seconds long and selected an image\nthat suits the music among the 200 StyleGAN-generated examples. Based on the\ncollected data, we trained a simple transfer function that converts an audio\nembedding to a style embedding. The generated examples show that the mapping\nbetween audio and video makes a certain level of intra-segment similarity and\ninter-segment dissimilarity.", "published": "2021-02-09 07:04:22", "link": "http://arxiv.org/abs/2102.04680v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Independent Vector Extraction for Fast Joint Blind Source Separation and\n  Dereverberation", "abstract": "We address a blind source separation (BSS) problem in a noisy reverberant\nenvironment in which the number of microphones $M$ is greater than the number\nof sources of interest, and the other noise components can be approximated as\nstationary and Gaussian distributed. Conventional BSS algorithms for the\noptimization of a multi-input multi-output convolutional beamformer have\nsuffered from a huge computational cost when $M$ is large. We here propose a\ncomputationally efficient method that integrates a weighted prediction error\n(WPE) dereverberation method and a fast BSS method called independent vector\nextraction (IVE), which has been developed for less reverberant environments.\nWe show that, given the power spectrum for each source, the optimization\nproblem of the new method can be reduced to that of IVE by exploiting the\nstationary condition, which makes the optimization easy to handle and\ncomputationally efficient. An experiment of speech signal separation shows\nthat, compared to a conventional method that integrates WPE and independent\nvector analysis, our proposed method achieves much faster convergence while\nmaintaining its separation performance.", "published": "2021-02-09 08:18:10", "link": "http://arxiv.org/abs/2102.04696v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Train your classifier first: Cascade Neural Networks Training from upper\n  layers to lower layers", "abstract": "Although the lower layers of a deep neural network learn features which are\ntransferable across datasets, these layers are not transferable within the same\ndataset. That is, in general, freezing the trained feature extractor (the lower\nlayers) and retraining the classifier (the upper layers) on the same dataset\nleads to worse performance. In this paper, for the first time, we show that the\nfrozen classifier is transferable within the same dataset. We develop a novel\ntop-down training method which can be viewed as an algorithm for searching for\nhigh-quality classifiers. We tested this method on automatic speech recognition\n(ASR) tasks and language modelling tasks. The proposed method consistently\nimproves recurrent neural network ASR models on Wall Street Journal,\nself-attention ASR models on Switchboard, and AWD-LSTM language models on\nWikiText-2.", "published": "2021-02-09 08:19:49", "link": "http://arxiv.org/abs/2102.04697v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Principal components variable importance reconstruction (PC-VIR):\n  Exploring predictive importance in multicollinear acoustic speech data", "abstract": "This paper presents a method of exploring the relative predictive importance\nof individual variables in multicollinear data sets at three levels of\nsignificance: strong importance, moderate importance, and no importance.\nImplementation of Bonferroni adjustment to control for Type I error in the\nmethod is described, and results with and without the correction are compared.\nAn example of the method in binary logistic modeling is demonstrated by using a\nset of 20 acoustic features to discriminate vocalic nasality in the speech of\nsix speakers of the Mixean variety of Low Navarrese Basque. Validation of the\nmethod is presented by comparing the direction of significant effects to those\nobserved in separate logistic mixed effects models, as well as goodness of fit\nand prediction accuracy compared to partial least squares logistic regression.\nThe results show that the proposed method yields: (1) similar, but more\nconservative estimates in comparison to separate logistic regression models,\n(2) models that fit data as well as partial least squares methods, and (3)\npredictions for new data that are as accurate as partial least squares methods.", "published": "2021-02-09 10:06:43", "link": "http://arxiv.org/abs/2102.04740v1", "categories": ["stat.ME", "cs.SD", "eess.AS"], "primary_category": "stat.ME"}
{"title": "Fast and Accurate Amplitude Demodulation of Wideband Signals", "abstract": "Amplitude demodulation is a classical operation used in signal processing.\nFor a long time, its effective applications in practice have been limited to\nnarrowband signals. In this work, we generalize amplitude demodulation to\nwideband signals. We pose demodulation as a recovery problem of an oversampled\ncorrupted signal and introduce special iterative schemes belonging to the\nfamily of alternating projection algorithms to solve it. Sensibly chosen\nstructural assumptions on the demodulation outputs allow us to reveal the high\ninferential accuracy of the method over a rich set of relevant signals. This\nnew approach surpasses current state-of-the-art demodulation techniques apt to\nwideband signals in computational efficiency by up to many orders of magnitude\nwith no sacrifice in quality. Such performance opens the door for applications\nof the amplitude demodulation procedure in new contexts. In particular, the new\nmethod makes online and large-scale offline data processing feasible, including\nthe calculation of modulator-carrier pairs in higher dimensions and poor\nsampling conditions, independent of the signal bandwidth. We illustrate the\nutility and specifics of applications of the new method in practice by using\nnatural speech and synthetic signals.", "published": "2021-02-09 14:06:29", "link": "http://arxiv.org/abs/2102.04832v2", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "On permutation invariant training for speech source separation", "abstract": "We study permutation invariant training (PIT), which targets at the\npermutation ambiguity problem for speaker independent source separation models.\nWe extend two state-of-the-art PIT strategies. First, we look at the two-stage\nspeaker separation and tracking algorithm based on frame level PIT (tPIT) and\nclustering, which was originally proposed for the STFT domain, and we adapt it\nto work with waveforms and over a learned latent space. Further, we propose an\nefficient clustering loss scalable to waveform models. Second, we extend a\nrecently proposed auxiliary speaker-ID loss with a deep feature loss based on\n\"problem agnostic speech features\", to reduce the local permutation errors made\nby the utterance level PIT (uPIT). Our results show that the proposed\nextensions help reducing permutation ambiguity. However, we also note that the\nstudied STFT-based models are more effective at reducing permutation errors\nthan waveform-based models, a perspective overlooked in recent studies.", "published": "2021-02-09 16:57:32", "link": "http://arxiv.org/abs/2102.04945v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Neural Network based Cough Detection using Bed-mounted\n  Accelerometer Measurements", "abstract": "We have performed cough detection based on measurements from an accelerometer\nattached to the patient's bed. This form of monitoring is less intrusive than\nbody-attached accelerometer sensors, and sidesteps privacy concerns encountered\nwhen using audio for cough detection. For our experiments, we have compiled a\nmanually-annotated dataset containing the acceleration signals of approximately\n6000 cough and 68000 non-cough events from 14 adult male patients in a\ntuberculosis clinic. As classifiers, we have considered convolutional neural\nnetworks (CNN), long-short-term-memory (LSTM) networks, and a residual neural\nnetwork (Resnet50). We find that all classifiers are able to distinguish\nbetween the acceleration signals due to coughing and those due to other\nactivities including sneezing, throat-clearing and movement in the bed with\nhigh accuracy. The Resnet50 performs the best, achieving an area under the ROC\ncurve (AUC) exceeding 0.98 in cross-validation experiments. We conclude that\nhigh-accuracy cough monitoring based only on measurements from the\naccelerometer in a consumer smartphone is possible. Since the need to gather\naudio is avoided and therefore privacy is inherently protected, and since the\naccelerometer is attached to the bed and not worn, this form of monitoring may\nrepresent a more convenient and readily accepted method of long-term patient\ncough monitoring.", "published": "2021-02-09 18:04:35", "link": "http://arxiv.org/abs/2102.04997v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CDPAM: Contrastive learning for perceptual audio similarity", "abstract": "Many speech processing methods based on deep learning require an automatic\nand differentiable audio metric for the loss function. The DPAM approach of\nManocha et al. learns a full-reference metric trained directly on human\njudgments, and thus correlates well with human perception. However, it requires\na large number of human annotations and does not generalize well outside the\nrange of perturbations on which it was trained. This paper introduces CDPAM, a\nmetric that builds on and advances DPAM. The primary improvement is to combine\ncontrastive learning and multi-dimensional representations to build robust\nmodels from limited data. In addition, we collect human judgments on triplet\ncomparisons to improve generalization to a broader range of audio\nperturbations. CDPAM correlates well with human responses across nine varied\ndatasets. We also show that adding this metric to existing speech synthesis and\nenhancement methods yields significant improvement, as measured by objective\nand subjective tests.", "published": "2021-02-09 20:15:29", "link": "http://arxiv.org/abs/2102.05109v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Audio Augmentation Methods with Consistency Learning", "abstract": "Data augmentation is an inexpensive way to increase training data diversity\nand is commonly achieved via transformations of existing data. For tasks such\nas classification, there is a good case for learning representations of the\ndata that are invariant to such transformations, yet this is not explicitly\nenforced by classification losses such as the cross-entropy loss. This paper\ninvestigates the use of training objectives that explicitly impose this\nconsistency constraint and how it can impact downstream audio classification\ntasks. In the context of deep convolutional neural networks in the supervised\nsetting, we show empirically that certain measures of consistency are not\nimplicitly captured by the cross-entropy loss and that incorporating such\nmeasures into the loss function can improve the performance of audio\nclassification systems. Put another way, we demonstrate how existing\naugmentation methods can further improve learning by enforcing consistency.", "published": "2021-02-09 22:01:58", "link": "http://arxiv.org/abs/2102.05151v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
