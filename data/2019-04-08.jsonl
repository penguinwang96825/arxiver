{"title": "Enriching Rare Word Representations in Neural Language Models by\n  Embedding Matrix Augmentation", "abstract": "The neural language models (NLM) achieve strong generalization capability by\nlearning the dense representation of words and using them to estimate\nprobability distribution function. However, learning the representation of rare\nwords is a challenging problem causing the NLM to produce unreliable\nprobability estimates. To address this problem, we propose a method to enrich\nrepresentations of rare words in pre-trained NLM and consequently improve its\nprobability estimation performance. The proposed method augments the word\nembedding matrices of pre-trained NLM while keeping other parameters unchanged.\nSpecifically, our method updates the embedding vectors of rare words using\nembedding vectors of other semantically and syntactically similar words. To\nevaluate the proposed method, we enrich the rare street names in the\npre-trained NLM and use it to rescore 100-best hypotheses output from the\nSingapore English speech recognition system. The enriched NLM reduces the word\nerror rate by 6% relative and improves the recognition accuracy of the rare\nwords by 16% absolute as compared to the baseline NLM.", "published": "2019-04-08 01:50:45", "link": "http://arxiv.org/abs/1904.03799v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Speaker-Dependent Separation for CHiME-5 Challenge", "abstract": "This paper summarizes several follow-up contributions for improving our\nsubmitted NWPU speaker-dependent system for CHiME-5 challenge, which aims to\nsolve the problem of multi-channel, highly-overlapped conversational speech\nrecognition in a dinner party scenario with reverberations and non-stationary\nnoises. We adopt a speaker-aware training method by using i-vector as the\ntarget speaker information for multi-talker speech separation. With only one\nunified separation model for all speakers, we achieve a 10\\% absolute\nimprovement in terms of word error rate (WER) over the previous baseline of\n80.28\\% on the development set by leveraging our newly proposed data processing\ntechniques and beamforming approach. With our improved back-end acoustic model,\nwe further reduce WER to 60.15\\% which surpasses the result of our submitted\nCHiME-5 challenge system without applying any fusion techniques.", "published": "2019-04-08 01:21:38", "link": "http://arxiv.org/abs/1904.03792v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Direct Modelling of Speech Emotion from Raw Speech", "abstract": "Speech emotion recognition is a challenging task and heavily depends on\nhand-engineered acoustic features, which are typically crafted to echo human\nperception of speech signals. However, a filter bank that is designed from\nperceptual evidence is not always guaranteed to be the best in a statistical\nmodelling framework where the end goal is for example emotion classification.\nThis has fuelled the emerging trend of learning representations from raw speech\nespecially using deep learning neural networks. In particular, a combination of\nConvolution Neural Networks (CNNs) and Long Short Term Memory (LSTM) have\ngained great traction for the intrinsic property of LSTM in learning contextual\ninformation crucial for emotion recognition; and CNNs been used for its ability\nto overcome the scalability problem of regular neural networks. In this paper,\nwe show that there are still opportunities to improve the performance of\nemotion recognition from the raw speech by exploiting the properties of CNN in\nmodelling contextual information. We propose the use of parallel convolutional\nlayers to harness multiple temporal resolutions in the feature extraction block\nthat is jointly trained with the LSTM based classification network for the\nemotion recognition task. Our results suggest that the proposed model can reach\nthe performance of CNN trained with hand-engineered features from both IEMOCAP\nand MSP-IMPROV datasets.", "published": "2019-04-08 04:29:29", "link": "http://arxiv.org/abs/1904.03833v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Duration robust weakly supervised sound event detection", "abstract": "Task 4 of the DCASE2018 challenge demonstrated that substantially more\nresearch is needed for a real-world application of sound event detection.\nAnalyzing the challenge results it can be seen that most successful models are\nbiased towards predicting long (e.g., over 5s) clips. This work aims to\ninvestigate the performance impact of fixed-sized window median filter\npost-processing and advocate the use of double thresholding as a more robust\nand predictable post-processing method. Further, four different temporal\nsubsampling methods within the CRNN framework are proposed: mean-max,\nalpha-mean-max, Lp-norm and convolutional. We show that for this task\nsubsampling the temporal resolution by a neural network enhances the F1 score\nas well as its robustness towards short, sporadic sound events. Our best single\nmodel achieves 30.1% F1 on the evaluation set and the best fusion model 32.5%,\nwhile being robust to event length variations.", "published": "2019-04-08 05:07:14", "link": "http://arxiv.org/abs/1904.03841v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Parrotron: An End-to-End Speech-to-Speech Conversion Model and its\n  Applications to Hearing-Impaired Speech and Speech Separation", "abstract": "We describe Parrotron, an end-to-end-trained speech-to-speech conversion\nmodel that maps an input spectrogram directly to another spectrogram, without\nutilizing any intermediate discrete representation. The network is composed of\nan encoder, spectrogram and phoneme decoders, followed by a vocoder to\nsynthesize a time-domain waveform. We demonstrate that this model can be\ntrained to normalize speech from any speaker regardless of accent, prosody, and\nbackground noise, into the voice of a single canonical target speaker with a\nfixed accent and consistent articulation and prosody. We further show that this\nnormalization model can be adapted to normalize highly atypical speech from a\ndeaf speaker, resulting in significant improvements in intelligibility and\nnaturalness, measured via a speech recognizer and listening tests. Finally,\ndemonstrating the utility of this model on other speech tasks, we show that the\nsame model architecture can be trained to perform a speech separation task", "published": "2019-04-08 16:25:45", "link": "http://arxiv.org/abs/1904.04169v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bayesian Non-Parametric Multi-Source Modelling Based Determined Blind\n  Source Separation", "abstract": "This paper proposes a determined blind source separation method using\nBayesian non-parametric modelling of sources. Conventionally source signals are\nseparated from a given set of mixture signals by modelling them using\nnon-negative matrix factorization (NMF). However in NMF, a latent variable\nsignifying model complexity must be appropriately specified to avoid\nover-fitting or under-fitting. As real-world sources can be of varying and\nunknown complexities, we propose a Bayesian non-parametric framework which is\ninvariant to such latent variables. We show that our proposed method adapts to\ndifferent source complexities, while conventional methods require parameter\ntuning for optimal separation.", "published": "2019-04-08 00:39:30", "link": "http://arxiv.org/abs/1904.03787v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal Convolution for Real-time Keyword Spotting on Mobile Devices", "abstract": "Keyword spotting (KWS) plays a critical role in enabling speech-based user\ninteractions on smart devices. Recent developments in the field of deep\nlearning have led to wide adoption of convolutional neural networks (CNNs) in\nKWS systems due to their exceptional accuracy and robustness. The main\nchallenge faced by KWS systems is the trade-off between high accuracy and low\nlatency. Unfortunately, there has been little quantitative analysis of the\nactual latency of KWS models on mobile devices. This is especially concerning\nsince conventional convolution-based KWS approaches are known to require a\nlarge number of operations to attain an adequate level of performance. In this\npaper, we propose a temporal convolution for real-time KWS on mobile devices.\nUnlike most of the 2D convolution-based KWS approaches that require a deep\narchitecture to fully capture both low- and high-frequency domains, we exploit\ntemporal convolutions with a compact ResNet architecture. In Google Speech\nCommand Dataset, we achieve more than \\textbf{385x} speedup on Google Pixel 1\nand surpass the accuracy compared to the state-of-the-art model. In addition,\nwe release the implementation of the proposed and the baseline models including\nan end-to-end pipeline for training models and evaluating them on mobile\ndevices.", "published": "2019-04-08 03:21:11", "link": "http://arxiv.org/abs/1904.03814v2", "categories": ["cs.SD", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Audio: A New Information Hiding Method and Backdoor for\n  DNN-based Speech Recognition Models", "abstract": "Audio is an important medium in people's daily life, hidden information can\nbe embedded into audio for covert communication. Current audio information\nhiding techniques can be roughly classed into time domain-based and transform\ndomain-based techniques. Time domain-based techniques have large hiding\ncapacity but low imperceptibility. Transform domain-based techniques have\nbetter imperceptibility, but the hiding capacity is poor. This paper proposes a\nnew audio information hiding technique which shows high hiding capacity and\ngood imperceptibility. The proposed audio information hiding method takes the\noriginal audio signal as input and obtains the audio signal embedded with\nhidden information (called stego audio) through the training of our private\nautomatic speech recognition (ASR) model. Without knowing the internal\nparameters and structure of the private model, the hidden information can be\nextracted by the private model but cannot be extracted by public models. We use\nfour other ASR models to extract the hidden information on the stego audios to\nevaluate the security of the private model. The experimental results show that\nthe proposed audio information hiding technique has a high hiding capacity of\n48 cps with good imperceptibility and high security. In addition, our proposed\nadversarial audio can be used to activate an intrinsic backdoor of DNN-based\nASR models, which brings a serious threat to intelligent speakers.", "published": "2019-04-08 04:16:40", "link": "http://arxiv.org/abs/1904.03829v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "A Statistical Investigation of Long Memory in Language and Music", "abstract": "Representation and learning of long-range dependencies is a central challenge\nconfronted in modern applications of machine learning to sequence data. Yet\ndespite the prominence of this issue, the basic problem of measuring long-range\ndependence, either in a given data source or as represented in a trained deep\nmodel, remains largely limited to heuristic tools. We contribute a statistical\nframework for investigating long-range dependence in current applications of\ndeep sequence modeling, drawing on the well-developed theory of long memory\nstochastic processes. This framework yields testable implications concerning\nthe relationship between long memory in real-world data and its learned\nrepresentation in a deep learning architecture, which are explored through a\nsemiparametric framework adapted to the high-dimensional setting.", "published": "2019-04-08 04:36:14", "link": "http://arxiv.org/abs/1904.03834v2", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "Bayesian Subspace Hidden Markov Model for Acoustic Unit Discovery", "abstract": "This work tackles the problem of learning a set of language specific acoustic\nunits from unlabeled speech recordings given a set of labeled recordings from\nother languages. Our approach may be described by the following two steps\nprocedure: first the model learns the notion of acoustic units from the\nlabelled data and then the model uses its knowledge to find new acoustic units\non the target language. We implement this process with the Bayesian Subspace\nHidden Markov Model (SHMM), a model akin to the Subspace Gaussian Mixture Model\n(SGMM) where each low dimensional embedding represents an acoustic unit rather\nthan just a HMM's state. The subspace is trained on 3 languages from the\nGlobalPhone corpus (German, Polish and Spanish) and the AUs are discovered on\nthe TIMIT corpus. Results, measured in equivalent Phone Error Rate, show that\nthis approach significantly outperforms previous HMM based acoustic units\ndiscovery systems and compares favorably with the Variational Auto Encoder-HMM.", "published": "2019-04-08 07:48:36", "link": "http://arxiv.org/abs/1904.03876v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "GELP: GAN-Excited Linear Prediction for Speech Synthesis from\n  Mel-spectrogram", "abstract": "Recent advances in neural network -based text-to-speech have reached human\nlevel naturalness in synthetic speech. The present sequence-to-sequence models\ncan directly map text to mel-spectrogram acoustic features, which are\nconvenient for modeling, but present additional challenges for vocoding (i.e.,\nwaveform generation from the acoustic features). High-quality synthesis can be\nachieved with neural vocoders, such as WaveNet, but such autoregressive models\nsuffer from slow sequential inference. Meanwhile, their existing parallel\ninference counterparts are difficult to train and require increasingly large\nmodel sizes. In this paper, we propose an alternative training strategy for a\nparallel neural vocoder utilizing generative adversarial networks, and\nintegrate a linear predictive synthesis filter into the model. Results show\nthat the proposed model achieves significant improvement in inference speed,\nwhile outperforming a WaveNet in copy-synthesis quality.", "published": "2019-04-08 11:58:00", "link": "http://arxiv.org/abs/1904.03976v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Completely Unsupervised Speech Recognition By A Generative Adversarial\n  Network Harmonized With Iteratively Refined Hidden Markov Models", "abstract": "Producing a large annotated speech corpus for training ASR systems remains\ndifficult for more than 95% of languages all over the world which are\nlow-resourced, but collecting a relatively big unlabeled data set for such\nlanguages is more achievable. This is why some initial effort have been\nreported on completely unsupervised speech recognition learned from unlabeled\ndata only, although with relatively high error rates. In this paper, we develop\na Generative Adversarial Network (GAN) to achieve this purpose, in which a\nGenerator and a Discriminator learn from each other iteratively to improve the\nperformance. We further use a set of Hidden Markov Models (HMMs) iteratively\nrefined from the machine generated labels to work in harmony with the GAN. The\ninitial experiments on TIMIT data set achieve an phone error rate of 33.1%,\nwhich is 8.5% lower than the previous state-of-the-art.", "published": "2019-04-08 14:47:45", "link": "http://arxiv.org/abs/1904.04100v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio Source Separation via Multi-Scale Learning with Dilated Dense\n  U-Nets", "abstract": "Modern audio source separation techniques rely on optimizing sequence model\narchitectures such as, 1D-CNNs, on mixture recordings to generalize well to\nunseen mixtures. Specifically, recent focus is on time-domain based\narchitectures such as Wave-U-Net which exploit temporal context by extracting\nmulti-scale features. However, the optimality of the feature extraction process\nin these architectures has not been well investigated. In this paper, we\nexamine and recommend critical architectural changes that forge an optimal\nmulti-scale feature extraction process. To this end, we replace regular $1-$D\nconvolutions with adaptive dilated convolutions that have innate capability of\ncapturing increased context by using large temporal receptive fields. We also\ninvestigate the impact of dense connections on the extraction process that\nencourage feature reuse and better gradient flow. The dense connections between\nthe downsampling and upsampling paths of a U-Net architecture capture\nmulti-resolution information leading to improved temporal modelling. We\nevaluate the proposed approaches on the MUSDB test dataset. In addition to\nproviding an improved performance over the state-of-the-art, we also provide\ninsights on the impact of different architectural choices on complex\ndata-driven solutions for source separation.", "published": "2019-04-08 16:13:16", "link": "http://arxiv.org/abs/1904.04161v1", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unsupervised Feature Learning for Environmental Sound Classification\n  Using Weighted Cycle-Consistent Generative Adversarial Network", "abstract": "In this paper we propose a novel environmental sound classification approach\nincorporating unsupervised feature learning from codebook via spherical\n$K$-Means++ algorithm and a new architecture for high-level data augmentation.\nThe audio signal is transformed into a 2D representation using a discrete\nwavelet transform (DWT). The DWT spectrograms are then augmented by a novel\narchitecture for cycle-consistent generative adversarial network. This\nhigh-level augmentation bootstraps generated spectrograms in both intra and\ninter class manners by translating structural features from sample to sample. A\ncodebook is built by coding the DWT spectrograms with the speeded-up robust\nfeature detector (SURF) and the K-Means++ algorithm. The Random Forest is our\nfinal learning algorithm which learns the environmental sound classification\ntask from the clustered codewords in the codebook. Experimental results in four\nbenchmarking environmental sound datasets (ESC-10, ESC-50, UrbanSound8k, and\nDCASE-2017) have shown that the proposed classification approach outperforms\nthe state-of-the-art classifiers in the scope, including advanced and dense\nconvolutional neural networks such as AlexNet and GoogLeNet, improving the\nclassification rate between 3.51% and 14.34%, depending on the dataset.", "published": "2019-04-08 17:44:14", "link": "http://arxiv.org/abs/1904.04221v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exploring Methods for the Automatic Detection of Errors in Manual\n  Transcription", "abstract": "Quality of data plays an important role in most deep learning tasks. In the\nspeech community, transcription of speech recording is indispensable. Since the\ntranscription is usually generated artificially, automatically finding errors\nin manual transcriptions not only saves time and labors but benefits the\nperformance of tasks that need the training process. Inspired by the success of\nhybrid automatic speech recognition using both language model and acoustic\nmodel, two approaches of automatic error detection in the transcriptions have\nbeen explored in this work. Previous study using a biased language model\napproach, relying on a strong transcription-dependent language model, has been\nreviewed. In this work, we propose a novel acoustic model based approach,\nfocusing on the phonetic sequence of speech. Both methods have been evaluated\non a completely real dataset, which was originally transcribed with errors and\nstrictly corrected manually afterwards.", "published": "2019-04-08 18:48:45", "link": "http://arxiv.org/abs/1904.04294v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Audio Classification of Bit-Representation Waveform", "abstract": "This study investigated the waveform representation for audio signal\nclassification. Recently, many studies on audio waveform classification such as\nacoustic event detection and music genre classification have been published.\nMost studies on audio waveform classification have proposed the use of a deep\nlearning (neural network) framework. Generally, a frequency analysis method\nsuch as Fourier transform is applied to extract the frequency or spectral\ninformation from the input audio waveform before inputting the raw audio\nwaveform into the neural network. In contrast to these previous studies, in\nthis paper, we propose a novel waveform representation method, in which audio\nwaveforms are represented as a bit sequence, for audio classification. In our\nexperiment, we compare the proposed bit representation waveform, which is\ndirectly given to a neural network, to other representations of audio waveforms\nsuch as a raw audio waveform and a power spectrum with two classification\ntasks: one is an acoustic event classification task and the other is a\nsound/music classification task. The experimental results showed that the bit\nrepresentation waveform achieved the best classification performance for both\nthe tasks.", "published": "2019-04-08 21:24:31", "link": "http://arxiv.org/abs/1904.04364v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning the EEG Manifold for Phonological Categorization from\n  Active Thoughts", "abstract": "Speech-related Brain Computer Interfaces (BCI) aim primarily at finding an\nalternative vocal communication pathway for people with speaking disabilities.\nAs a step towards full decoding of imagined speech from active thoughts, we\npresent a BCI system for subject-independent classification of phonological\ncategories exploiting a novel deep learning based hierarchical feature\nextraction scheme. To better capture the complex representation of\nhigh-dimensional electroencephalography (EEG) data, we compute the joint\nvariability of EEG electrodes into a channel cross-covariance matrix. We then\nextract the spatio-temporal information encoded within the matrix using a mixed\ndeep neural network strategy. Our model framework is composed of a\nconvolutional neural network (CNN), a long-short term network (LSTM), and a\ndeep autoencoder. We train the individual networks hierarchically, feeding\ntheir combined outputs in a final gradient boosting classification step. Our\nbest models achieve an average accuracy of 77.9% across five different binary\nclassification tasks, providing a significant 22.5% improvement over previous\nmethods. As we also show visually, our work demonstrates that the speech\nimagery EEG possesses significant discriminative information about the intended\narticulatory movements responsible for natural speech synthesis.", "published": "2019-04-08 21:11:40", "link": "http://arxiv.org/abs/1904.04358v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SPEAK YOUR MIND! Towards Imagined Speech Recognition With Hierarchical\n  Deep Learning", "abstract": "Speech-related Brain Computer Interface (BCI) technologies provide effective\nvocal communication strategies for controlling devices through speech commands\ninterpreted from brain signals. In order to infer imagined speech from active\nthoughts, we propose a novel hierarchical deep learning BCI system for\nsubject-independent classification of 11 speech tokens including phonemes and\nwords. Our novel approach exploits predicted articulatory information of six\nphonological categories (e.g., nasal, bilabial) as an intermediate step for\nclassifying the phonemes and words, thereby finding discriminative signal\nresponsible for natural speech synthesis. The proposed network is composed of\nhierarchical combination of spatial and temporal CNN cascaded with a deep\nautoencoder. Our best models on the KARA database achieve an average accuracy\nof 83.42% across the six different binary phonological classification tasks,\nand 53.36% for the individual token identification task, significantly\noutperforming our baselines. Ultimately, our work suggests the possible\nexistence of a brain imagery footprint for the underlying articulatory movement\nrelated to different sounds that can be used to aid imagined speech decoding.", "published": "2019-04-08 21:41:54", "link": "http://arxiv.org/abs/1904.05746v1", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
