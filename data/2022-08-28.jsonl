{"title": "CJaFr-v3 : A Freely Available Filtered Japanese-French Aligned Corpus", "abstract": "We present a free Japanese-French parallel corpus. It includes 15M aligned\nsegments and is obtained by compiling and filtering several existing resources.\nIn this paper, we describe the existing resources, their quantity and quality,\nthe filtering we applied to improve the quality of the corpus, and the content\nof the ready-to-use corpus. We also evaluate the usefulness of this corpus and\nthe quality of our filtering by training and evaluating some standard MT\nsystems with it.", "published": "2022-08-28 08:33:18", "link": "http://arxiv.org/abs/2208.13170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Podcast Summary Assessment: A Resource for Evaluating Summary Assessment\n  Methods", "abstract": "Automatic summary assessment is useful for both machine-generated and\nhuman-produced summaries. Automatically evaluating the summary text given the\ndocument enables, for example, summary generation system development and\ndetection of inappropriate summaries. Summary assessment can be run in a number\nof modes: ranking summary generation systems; ranking summaries of a particular\ndocument; and estimating the quality of a document-summary pair on an absolute\nscale. Existing datasets with annotation for summary assessment are usually\nbased on news summarization datasets such as CNN/DailyMail or XSum. In this\nwork, we describe a new dataset, the podcast summary assessment corpus, a\ncollection of podcast summaries that were evaluated by human experts at\nTREC2020. Compared to existing summary assessment data, this dataset has two\nunique aspects: (i) long-input, speech podcast based, documents; and (ii) an\nopportunity to detect inappropriate reference summaries in podcast corpus.\nFirst, we examine existing assessment methods, including model-free and\nmodel-based methods, and provide benchmark results for this long-input summary\nassessment dataset. Second, with the aim of filtering reference\nsummary-document pairings for training, we apply summary assessment for data\nselection. The experimental results on these two aspects provide interesting\ninsights on the summary assessment and generation tasks. The podcast summary\nassessment data is available.", "published": "2022-08-28 18:24:41", "link": "http://arxiv.org/abs/2208.13265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bayesian Neural Network Language Modeling for Speech Recognition", "abstract": "State-of-the-art neural network language models (NNLMs) represented by long\nshort term memory recurrent neural networks (LSTM-RNNs) and Transformers are\nbecoming highly complex. They are prone to overfitting and poor generalization\nwhen given limited training data. To this end, an overarching full Bayesian\nlearning framework encompassing three methods is proposed in this paper to\naccount for the underlying uncertainty in LSTM-RNN and Transformer LMs. The\nuncertainty over their model parameters, choice of neural activations and\nhidden output representations are modeled using Bayesian, Gaussian Process and\nvariational LSTM-RNN or Transformer LMs respectively. Efficient inference\napproaches were used to automatically select the optimal network internal\ncomponents to be Bayesian learned using neural architecture search. A minimal\nnumber of Monte Carlo parameter samples as low as one was also used. These\nallow the computational costs incurred in Bayesian NNLM training and evaluation\nto be minimized. Experiments are conducted on two tasks: AMI meeting\ntranscription and Oxford-BBC LipReading Sentences 2 (LRS2) overlapped speech\nrecognition using state-of-the-art LF-MMI trained factored TDNN systems\nfeaturing data augmentation, speaker adaptation and audio-visual multi-channel\nbeamforming for overlapped speech. Consistent performance improvements over the\nbaseline LSTM-RNN and Transformer LMs with point estimated model parameters and\ndrop-out regularization were obtained across both tasks in terms of perplexity\nand word error rate (WER). In particular, on the LRS2 data, statistically\nsignificant WER reductions up to 1.3% and 1.2% absolute (12.1% and 11.3%\nrelative) were obtained over the baseline LSTM-RNN and Transformer LMs\nrespectively after model combination between Bayesian NNLMs and their\nrespective baselines.", "published": "2022-08-28 17:50:19", "link": "http://arxiv.org/abs/2208.13259v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for\n  Conversational Embodied Agents", "abstract": "Building a conversational embodied agent to execute real-life tasks has been\na long-standing yet quite challenging research goal, as it requires effective\nhuman-agent communication, multi-modal understanding, long-range sequential\ndecision making, etc. Traditional symbolic methods have scaling and\ngeneralization issues, while end-to-end deep learning models suffer from data\nscarcity and high task complexity, and are often hard to explain. To benefit\nfrom both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning\nframework for modular, generalizable, and interpretable conversational embodied\nagents. First, it acquires symbolic representations by prompting large language\nmodels (LLMs) for language understanding and sub-goal planning, and by\nconstructing semantic maps from visual observations. Then the symbolic module\nreasons for sub-goal planning and action generation based on task- and\naction-level common sense. Extensive experiments on the TEACh dataset validate\nthe efficacy and efficiency of our JARVIS framework, which achieves\nstate-of-the-art (SOTA) results on all three dialog-based embodied tasks,\nincluding Execution from Dialog History (EDH), Trajectory from Dialog (TfD),\nand Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen\nSuccess Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze\nthe essential factors that affect the task performance and also demonstrate the\nsuperiority of our method in few-shot settings. Our JARVIS model ranks first in\nthe Alexa Prize SimBot Public Benchmark Challenge.", "published": "2022-08-28 18:30:46", "link": "http://arxiv.org/abs/2208.13266v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Adapting the LodView RDF Browser for Navigation over the Multilingual\n  Linguistic Linked Open Data Cloud", "abstract": "The paper is dedicated to the use of LodView for navigation over the\nmultilingual Linguistic Linked Open Data cloud. First, we define the class of\nPubby-like tools, that LodView belongs to, and clarify the relation of this\nclass to the classes of URI dereferenciation tools, RDF browsers and LOD\nvisualization tools. Second, we reveal several limitations of LodView that\nimpede its use for the designated purpose, and propose improvements to be made\nfor fixing these limitations. These improvements are: 1) resolution of Cyrillic\nURIs; 2) decoding Cyrillic URIs in Turtle representations of resources; 3)\nsupport of Cyrillic literals; 4) user-friendly URLs for RDF representations of\nresources; 5) support of hash URIs; 6) expanding nested resources; 7) support\nof RDF collections; 8) pagination of resource property values; and 9) support\nof $\\LaTeX$ math notation. Third, we partially implement several of the\nproposed improvements.", "published": "2022-08-28 21:47:59", "link": "http://arxiv.org/abs/2208.13295v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "68T30 (Primary) 68T50 (Secondary)", "I.2.4; H.5.4"], "primary_category": "cs.CL"}
{"title": "Training Text-To-Speech Systems From Synthetic Data: A Practical\n  Approach For Accent Transfer Tasks", "abstract": "Transfer tasks in text-to-speech (TTS) synthesis - where one or more aspects\nof the speech of one set of speakers is transferred to another set of speakers\nthat do not feature these aspects originally - remains a challenging task. One\nof the challenges is that models that have high-quality transfer capabilities\ncan have issues in stability, making them impractical for user-facing critical\ntasks. This paper demonstrates that transfer can be obtained by training a\nrobust TTS system on data generated by a less robust TTS system designed for a\nhigh-quality transfer task; in particular, a CHiVE-BERT monolingual TTS system\nis trained on the output of a Tacotron model designed for accent transfer.\nWhile some quality loss is inevitable with this approach, experimental results\nshow that the models trained on synthetic data this way can produce high\nquality audio displaying accent transfer, while preserving speaker\ncharacteristics such as speaking style.", "published": "2022-08-28 09:24:45", "link": "http://arxiv.org/abs/2208.13183v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Disentangled Speech Representations", "abstract": "The careful construction of audio representations has become a dominant\nfeature in the design of approaches to many speech tasks. Increasingly, such\napproaches have emphasized \"disentanglement\", where a representation contains\nonly parts of the speech signal relevant to transcription while discarding\nirrelevant information. In this paper, we construct a representation learning\ntask based on joint modeling of ASR and TTS, and seek to learn a representation\nof audio that disentangles that part of the speech signal that is relevant to\ntranscription from that part which is not. We present empirical evidence that\nsuccessfully finding such a representation is tied to the randomness inherent\nin training. We then make the observation that these desired, disentangled\nsolutions to the optimization problem possess unique statistical properties.\nFinally, we show that enforcing these properties during training improves WER\nby 24.5% relative on average for our joint modeling task. These observations\nmotivate a novel approach to learning effective audio representations.", "published": "2022-08-28 10:03:55", "link": "http://arxiv.org/abs/2208.13191v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computing with Hypervectors for Efficient Speaker Identification", "abstract": "We introduce a method to identify speakers by computing with high-dimensional\nrandom vectors. Its strengths are simplicity and speed. With only 1.02k active\nparameters and a 128-minute pass through the training data we achieve Top-1 and\nTop-5 scores of 31% and 52% on the VoxCeleb1 dataset of 1,251 speakers. This is\nin contrast to CNN models requiring several million parameters and orders of\nmagnitude higher computational complexity for only a 2$\\times$ gain in\ndiscriminative power as measured in mutual information. An additional 92\nseconds of training with Generalized Learning Vector Quantization (GLVQ) raises\nthe scores to 48% and 67%. A trained classifier classifies 1 second of speech\nin 5.7 ms. All processing was done on standard CPU-based machines.", "published": "2022-08-28 20:32:24", "link": "http://arxiv.org/abs/2208.13285v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
