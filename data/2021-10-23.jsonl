{"title": "PhoMT: A High-Quality and Large-Scale Benchmark Dataset for\n  Vietnamese-English Machine Translation", "abstract": "We introduce a high-quality and large-scale Vietnamese-English parallel\ndataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark\nVietnamese-English machine translation corpus IWSLT15. We conduct experiments\ncomparing strong neural baselines and well-known automatic translation engines\non our dataset and find that in both automatic and human evaluations: the best\nperformance is obtained by fine-tuning the pre-trained sequence-to-sequence\ndenoising auto-encoder mBART. To our best knowledge, this is the first\nlarge-scale Vietnamese-English machine translation study. We hope our publicly\navailable dataset and study can serve as a starting point for future research\nand applications on Vietnamese-English machine translation.", "published": "2021-10-23 11:42:01", "link": "http://arxiv.org/abs/2110.12199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PASTRIE: A Corpus of Prepositions Annotated with Supersense Tags in\n  Reddit International English", "abstract": "We present the Prepositions Annotated with Supersense Tags in Reddit\nInternational English (\"PASTRIE\") corpus, a new dataset containing manually\nannotated preposition supersenses of English data from presumed speakers of\nfour L1s: English, French, German, and Spanish. The annotations are\ncomprehensive, covering all preposition types and tokens in the sample. Along\nwith the corpus, we provide analysis of distributional patterns across the\nincluded L1s and a discussion of the influence of L1s on L2 preposition choice.", "published": "2021-10-23 15:22:45", "link": "http://arxiv.org/abs/2110.12243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate and Offensive Speech Detection in Hindi and Marathi", "abstract": "Sentiment analysis is the most basic NLP task to determine the polarity of\ntext data. There has been a significant amount of work in the area of\nmultilingual text as well. Still hate and offensive speech detection faces a\nchallenge due to inadequate availability of data, especially for Indian\nlanguages like Hindi and Marathi. In this work, we consider hate and offensive\nspeech detection in Hindi and Marathi texts. The problem is formulated as a\ntext classification task using the state of the art deep learning approaches.\nWe explore different deep learning architectures like CNN, LSTM, and variations\nof BERT like multilingual BERT, IndicBERT, and monolingual RoBERTa. The basic\nmodels based on CNN and LSTM are augmented with fast text word embeddings. We\nuse the HASOC 2021 Hindi and Marathi hate speech datasets to compare these\nalgorithms. The Marathi dataset consists of binary labels and the Hindi dataset\nconsists of binary as well as more-fine grained labels. We show that the\ntransformer-based models perform the best and even the basic models along with\nFastText embeddings give a competitive performance. Moreover, with normal\nhyper-parameter tuning, the basic models perform better than BERT-based models\non the fine-grained Hindi dataset.", "published": "2021-10-23 11:57:36", "link": "http://arxiv.org/abs/2110.12200v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spanish Legalese Language Model and Corpora", "abstract": "There are many Language Models for the English language according to its\nworldwide relevance. However, for the Spanish language, even if it is a widely\nspoken language, there are very few Spanish Language Models which result to be\nsmall and too general. Legal slang could be think of a Spanish variant on its\nown as it is very complicated in vocabulary, semantics and phrase\nunderstanding. For this work we gathered legal-domain corpora from different\nsources, generated a model and evaluated against Spanish general domain tasks.\nThe model provides reasonable results in those tasks.", "published": "2021-10-23 12:06:51", "link": "http://arxiv.org/abs/2110.12201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Guided Policy Search for Parameterized Skills using Adverbs", "abstract": "We present a method for using adverb phrases to adjust skill parameters via\nlearned adverb-skill groundings. These groundings allow an agent to use adverb\nfeedback provided by a human to directly update a skill policy, in a manner\nsimilar to traditional local policy search methods. We show that our method can\nbe used as a drop-in replacement for these policy search methods when dense\nreward from the environment is not available but human language feedback is. We\ndemonstrate improved sample efficiency over modern policy search methods in two\nexperiments.", "published": "2021-10-23 21:34:09", "link": "http://arxiv.org/abs/2110.15799v1", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.RO", "68T20", "I.2.6; I.2.7"], "primary_category": "cs.AI"}
{"title": "Optimizing Alignment of Speech and Language Latent Spaces for End-to-End\n  Speech Recognition and Understanding", "abstract": "The advances in attention-based encoder-decoder (AED) networks have brought\ngreat progress to end-to-end (E2E) automatic speech recognition (ASR). One way\nto further improve the performance of AED-based E2E ASR is to introduce an\nextra text encoder for leveraging extensive text data and thus capture more\ncontext-aware linguistic information. However, this approach brings a mismatch\nproblem between the speech encoder and the text encoder due to the different\nunits used for modeling. In this paper, we propose an embedding aligner and\nmodality switch training to better align the speech and text latent spaces. The\nembedding aligner is a shared linear projection between text encoder and speech\nencoder trained by masked language modeling (MLM) loss and connectionist\ntemporal classification (CTC), respectively. The modality switch training\nrandomly swaps speech and text embeddings based on the forced alignment result\nto learn a joint representation space. Experimental results show that our\nproposed approach achieves a relative 14% to 19% word error rate (WER)\nreduction on Librispeech ASR task. We further verify its effectiveness on\nspoken language understanding (SLU), i.e., an absolute 2.5% to 2.8% F1 score\nimprovement on SNIPS slot filling task.", "published": "2021-10-23 04:45:22", "link": "http://arxiv.org/abs/2110.12138v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study of Acoustic Features in Arabic Speaker Identification under\n  Noisy Environmental Conditions", "abstract": "One of the major parts of the voice recognition field is the choice of\nacoustic features which have to be robust against the variability of the speech\nsignal, mismatched conditions, and noisy environments. Thus, different speech\nfeature extraction techniques have been developed. In this paper, we\ninvestigate the robustness of several front-end techniques in Arabic speaker\nidentification. We evaluate five different features in babble, factory and\nsubway conditions at the various signal to noise ratios (SNR). The obtained\nresults showed that two of the auditory feature i.e. gammatone frequency\ncepstral coefficient (GFCC) and power normalization cepstral coefficients\n(PNCC), unlike their combination performs substantially better than a\nconventional speaker features i.e. Mel-frequency cepstral coefficients (MFCC).", "published": "2021-10-23 21:50:50", "link": "http://arxiv.org/abs/2110.12304v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Study of Multimodal Person Verification Using Audio-Visual-Thermal\n  Data", "abstract": "In this paper, we study an approach to multimodal person verification using\naudio, visual, and thermal modalities. The combination of audio and visual\nmodalities has already been shown to be effective for robust person\nverification. From this perspective, we investigate the impact of further\nincreasing the number of modalities by adding thermal images. In particular, we\nimplemented unimodal, bimodal, and trimodal verification systems using\nstate-of-the-art deep learning architectures and compared their performance\nunder clean and noisy conditions. We also compared two popular fusion\napproaches based on simple score averaging and the soft attention mechanism.\nThe experiment conducted on the SpeakingFaces dataset demonstrates the superior\nperformance of the trimodal verification system. Specifically, on the easy test\nset, the trimodal system outperforms the best unimodal and bimodal systems by\nover 50% and 18% relative equal error rates, respectively, under both the clean\nand noisy conditions. On the hard test set, the trimodal system outperforms the\nbest unimodal and bimodal systems by over 40% and 13% relative equal error\nrates, respectively, under both the clean and noisy conditions. To enable\nreproducibility of the experiment and facilitate research into multimodal\nperson verification, we made our code, pretrained models, and preprocessed\ndataset freely available in our GitHub repository.", "published": "2021-10-23 04:41:03", "link": "http://arxiv.org/abs/2110.12136v2", "categories": ["cs.CV", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
