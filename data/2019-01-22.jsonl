{"title": "An Adversarial Approach to High-Quality, Sentiment-Controlled Neural\n  Dialogue Generation", "abstract": "In this work, we propose a method for neural dialogue response generation\nthat allows not only generating semantically reasonable responses according to\nthe dialogue history, but also explicitly controlling the sentiment of the\nresponse via sentiment labels. Our proposed model is based on the paradigm of\nconditional adversarial learning; the training of a sentiment-controlled\ndialogue generator is assisted by an adversarial discriminator which assesses\nthe fluency and feasibility of the response generating from the dialogue\nhistory and a given sentiment label. Because of the flexibility of our\nframework, the generator could be a standard sequence-to-sequence (SEQ2SEQ)\nmodel or a more complicated one such as a conditional variational\nautoencoder-based SEQ2SEQ model. Experimental results using automatic and human\nevaluation both demonstrate that our proposed framework is able to generate\nboth semantically reasonable and sentiment-controlled dialogue responses.", "published": "2019-01-22 00:29:27", "link": "http://arxiv.org/abs/1901.07129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Language Model Pretraining", "abstract": "Recent studies have demonstrated the efficiency of generative pretraining for\nEnglish natural language understanding. In this work, we extend this approach\nto multiple languages and show the effectiveness of cross-lingual pretraining.\nWe propose two methods to learn cross-lingual language models (XLMs): one\nunsupervised that only relies on monolingual data, and one supervised that\nleverages parallel data with a new cross-lingual language model objective. We\nobtain state-of-the-art results on cross-lingual classification, unsupervised\nand supervised machine translation. On XNLI, our approach pushes the state of\nthe art by an absolute gain of 4.9% accuracy. On unsupervised machine\ntranslation, we obtain 34.3 BLEU on WMT'16 German-English, improving the\nprevious state of the art by more than 9 BLEU. On supervised machine\ntranslation, we obtain a new state of the art of 38.5 BLEU on WMT'16\nRomanian-English, outperforming the previous best approach by more than 4 BLEU.\nOur code and pretrained models will be made publicly available.", "published": "2019-01-22 13:22:34", "link": "http://arxiv.org/abs/1901.07291v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep learning and sub-word-unit approach in written art generation", "abstract": "Automatic poetry generation is novel and interesting application of natural\nlanguage processing research. It became more popular during the last few years\ndue to the rapid development of technology and neural computing power. This\nline of research can be applied to the study of linguistics and literature, for\nsocial science experiments, or simply for entertainment. The most effective\nknown method of artificial poem generation uses recurrent neural networks\n(RNN). We also used RNNs to generate poems in the style of Adam Mickiewicz. Our\nnetwork was trained on the Sir Thaddeus poem. For data pre-processing, we used\na specialized stemming tool, which is one of the major innovations and\ncontributions of this work. Our experiment was conducted on the source text,\ndivided into sub-word units (at a level of resolution close to syllables). This\napproach is novel and is not often employed in the published literature. The\nsubwords units seem to be a natural choice for analysis of the Polish language,\nas the language is morphologically rich due to cases, gender forms and a large\nvocabulary. Moreover, Sir Thaddeus contains rhymes, so the analysis of\nsyllables can be meaningful. We verified our model with different settings for\nthe temperature parameter, which controls the randomness of the generated text.\nWe also compared our results with similar models trained on the same text but\ndivided into characters (which is the most common approach alongside the use of\nfull word units). The differences were tremendous. Our solution generated much\nbetter poems that were able to follow the metre and vocabulary of the source\ndata text.", "published": "2019-01-22 15:42:51", "link": "http://arxiv.org/abs/1901.07426v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debugging Frame Semantic Role Labeling", "abstract": "We propose a quantitative and qualitative analysis of the performances of\nstatistical models for frame semantic structure extraction. We report on a\nreplication study on FrameNet 1.7 data and show that preprocessing toolkits\nplay a major role in argument identification performances, observing gains\nsimilar in their order of magnitude to those reported by recent models for\nframe semantic parsing. We report on the robustness of a recent statistical\nclassifier for frame semantic parsing to lexical configurations of\npredicate-argument structures, relying on an artificially augmented dataset\ngenerated using a rule-based algorithm combining valence pattern matching and\nlexical substitution. We prove that syntactic pre-processing plays a major role\nin the performances of statistical classifiers to argument identification, and\ndiscuss the core reasons of syntactic mismatch between dependency parsers\noutput and FrameNet syntactic formalism. Finally, we suggest new leads for\nimproving statistical models for frame semantic parsing, including joint\nsyntax-semantic parsing relying on FrameNet syntactic formalism, latent classes\ninference via split-and-merge algorithms and neural network architectures\nrelying on rich input representations of words.", "published": "2019-01-22 17:17:02", "link": "http://arxiv.org/abs/1901.07475v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Delta-training: Simple Semi-Supervised Text Classification using\n  Pretrained Word Embeddings", "abstract": "We propose a novel and simple method for semi-supervised text classification.\nThe method stems from the hypothesis that a classifier with pretrained word\nembeddings always outperforms the same classifier with randomly initialized\nword embeddings, as empirically observed in NLP tasks. Our method first builds\ntwo sets of classifiers as a form of model ensemble, and then initializes their\nword embeddings differently: one using random, the other using pretrained word\nembeddings. We focus on different predictions between the two classifiers on\nunlabeled data while following the self-training framework. We also use\nearly-stopping in meta-epoch to improve the performance of our method. Our\nmethod, Delta-training, outperforms the self-training and the co-training\nframework in 4 different text classification datasets, showing robustness\nagainst error accumulation.", "published": "2019-01-22 23:55:49", "link": "http://arxiv.org/abs/1901.07651v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Rules for Fooling Deep Neural Networks based Text\n  Classification", "abstract": "Recently, deep learning based natural language processing techniques are\nbeing extensively used to deal with spam mail, censorship evaluation in social\nnetworks, among others. However, there is only a couple of works evaluating the\nvulnerabilities of such deep neural networks. Here, we go beyond attacks to\ninvestigate, for the first time, universal rules, i.e., rules that are sample\nagnostic and therefore could turn any text sample in an adversarial one. In\nfact, the universal rules do not use any information from the method itself (no\ninformation from the method, gradient information or training dataset\ninformation is used), making them black-box universal attacks. In other words,\nthe universal rules are sample and method agnostic. By proposing a\ncoevolutionary optimization algorithm we show that it is possible to create\nuniversal rules that can automatically craft imperceptible adversarial samples\n(only less than five perturbations which are close to misspelling are inserted\nin the text sample). A comparison with a random search algorithm further\njustifies the strength of the method. Thus, universal rules for fooling\nnetworks are here shown to exist. Hopefully, the results from this work will\nimpact the development of yet more sample and model agnostic attacks as well as\ntheir defenses, culminating in perhaps a new age for artificial intelligence.", "published": "2019-01-22 00:54:30", "link": "http://arxiv.org/abs/1901.07132v2", "categories": ["cs.LG", "cs.CL", "cs.CR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Self-Attention Networks for Connectionist Temporal Classification in\n  Speech Recognition", "abstract": "The success of self-attention in NLP has led to recent applications in\nend-to-end encoder-decoder architectures for speech recognition. Separately,\nconnectionist temporal classification (CTC) has matured as an alignment-free,\nnon-autoregressive approach to sequence transduction, either by itself or in\nvarious multitask and decoding frameworks. We propose SAN-CTC, a deep, fully\nself-attentional network for CTC, and show it is tractable and competitive for\nend-to-end speech recognition. SAN-CTC trains quickly and outperforms existing\nCTC models and most encoder-decoder models, with character error rates (CERs)\nof 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean,\nwith a fixed architecture and one GPU. Similar improvements hold for WERs after\nLM decoding. We motivate the architecture for speech, evaluate position and\ndownsampling approaches, and explore how label alphabets (character, phoneme,\nsubword) affect attention heads and performance.", "published": "2019-01-22 21:37:07", "link": "http://arxiv.org/abs/1901.10055v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Non linear time compression of clear and normal speech at high rates", "abstract": "We compare a series of time compression methods applied to normal and clear\nspeech. First we evaluate a linear (uniform) method applied to these styles as\nwell as to naturally-produced fast speech. We found, in line with the\nliterature, that unprocessed fast speech was less intelligible than linearly\ncompressed normal speech. Fast speech was also less intelligible than\ncompressed clear speech but at the highest rate (three times faster than\nnormal) the advantage of clear over fast speech was lost. To test whether this\nwas due to shorter speech duration we evaluate, in our second experiments, a\nrange of methods that compress speech and silence at different rates. We found\nthat even when the overall duration of speech and silence is kept the same\nacross styles, compressed normal speech is still more intelligible than\ncompressed clear speech. Compressing silence twice as much as speech improved\nresults further for normal speech with very little additional computational\ncosts.", "published": "2019-01-22 10:16:58", "link": "http://arxiv.org/abs/1901.07239v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Separation Using Gain-Adapted Factorial Hidden Markov Models", "abstract": "We present a new probabilistic graphical model which generalizes factorial\nhidden Markov models (FHMM) for the problem of single-channel speech separation\n(SCSS) in which we wish to separate the two speech signals $X(t)$ and $V(t)$\nfrom a single recording of their mixture $Y(t)=X(t)+V(t)$ using the trained\nmodels of the speakers' speech signals. Current techniques assume the data used\nin the training and test phases of the separation model have the same loudness.\nIn this paper, we introduce GFHMM, gain adapted FHMM, to extend SCSS to the\ngeneral case in which $Y(t)=g_xX(t)+g_vV(t)$, where $g_x$ and $g_v$ are unknown\ngain factors. GFHMM consists of two independent-state HMMs and a hidden node\nwhich model spectral patterns and gain difference, respectively. A novel\ninference method is presented using the Viterbi algorithm and quadratic\noptimization with minimal computational overhead. Experimental results,\nconducted on 180 mixtures with gain differences from 0 to 15~dB, show that the\nproposed technique significantly outperforms FHMM and its memoryless\ncounterpart, i.e., vector quantization (VQ)-based SCSS.", "published": "2019-01-22 20:17:07", "link": "http://arxiv.org/abs/1901.07604v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
