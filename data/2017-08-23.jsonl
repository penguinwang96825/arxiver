{"title": "Automatic Detection of Fake News", "abstract": "The proliferation of misleading information in everyday access media outlets\nsuch as social media feeds, news blogs, and online newspapers have made it\nchallenging to identify trustworthy news sources, thus increasing the need for\ncomputational tools able to provide insights into the reliability of online\ncontent. In this paper, we focus on the automatic identification of fake\ncontent in online news. Our contribution is twofold. First, we introduce two\nnovel datasets for the task of fake news detection, covering seven different\nnews domains. We describe the collection, annotation, and validation process in\ndetail and present several exploratory analysis on the identification of\nlinguistic differences in fake and legitimate news content. Second, we conduct\na set of learning experiments to build accurate fake news detectors. In\naddition, we provide comparative analyses of the automatic and manual\nidentification of fake news.", "published": "2017-08-23 17:12:03", "link": "http://arxiv.org/abs/1708.07104v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Network Approach for Mixing Language Models", "abstract": "The performance of Neural Network (NN)-based language models is steadily\nimproving due to the emergence of new architectures, which are able to learn\ndifferent natural language characteristics. This paper presents a novel\nframework, which shows that a significant improvement can be achieved by\ncombining different existing heterogeneous models in a single architecture.\nThis is done through 1) a feature layer, which separately learns different\nNN-based models and 2) a mixture layer, which merges the resulting model\nfeatures. In doing so, this architecture benefits from the learning\ncapabilities of each model with no noticeable increase in the number of model\nparameters or the training time. Extensive experiments conducted on the Penn\nTreebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a\nsignificant reduction of the perplexity when compared to state-of-the-art\nfeedforward as well as recurrent neural network architectures.", "published": "2017-08-23 13:27:16", "link": "http://arxiv.org/abs/1708.06989v1", "categories": ["cs.CL", "cs.AI", "97K50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Discovering Political Topics in Facebook Discussion threads with Graph\n  Contextualization", "abstract": "We propose a graph contextualization method, pairGraphText, to study\npolitical engagement on Facebook during the 2012 French presidential election.\nIt is a spectral algorithm that contextualizes graph data with text data for\nonline discussion thread. In particular, we examine the Facebook posts of the\neight leading candidates and the comments beneath these posts. We find evidence\nof both (i) candidate-centered structure, where citizens primarily comment on\nthe wall of one candidate and (ii) issue-centered structure (i.e. on political\ntopics), where citizens' attention and expression is primarily directed towards\na specific set of issues (e.g. economics, immigration, etc). To identify\nissue-centered structure, we develop pairGraphText, to analyze a network with\nhigh-dimensional features on the interactions (i.e. text). This technique\nscales to hundreds of thousands of nodes and thousands of unique words. In the\nFacebook data, spectral clustering without the contextualizing text information\nfinds a mixture of (i) candidate and (ii) issue clusters. The contextualized\ninformation with text data helps to separate these two structures. We conclude\nby showing that the novel methodology is consistent under a statistical model.", "published": "2017-08-23 02:42:59", "link": "http://arxiv.org/abs/1708.06872v3", "categories": ["stat.AP", "cs.CL", "physics.soc-ph"], "primary_category": "stat.AP"}
{"title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue\n  Responses", "abstract": "Automatically evaluating the quality of dialogue responses for unstructured\ndomains is a challenging problem. Unfortunately, existing automatic evaluation\nmetrics are biased and correlate very poorly with human judgements of response\nquality. Yet having an accurate automatic evaluation procedure is crucial for\ndialogue research, as it allows rapid prototyping and testing of new models\nwith fewer expensive human evaluations. In response to this challenge, we\nformulate automatic dialogue evaluation as a learning problem. We present an\nevaluation model (ADEM) that learns to predict human-like scores to input\nresponses, using a new dataset of human response scores. We show that the ADEM\nmodel's predictions correlate significantly, and at a level much higher than\nword-overlap metrics such as BLEU, with human judgements at both the utterance\nand system-level. We also show that ADEM can generalize to evaluating dialogue\nmodels unseen during training, an important step for automatic dialogue\nevaluation.", "published": "2017-08-23 18:56:00", "link": "http://arxiv.org/abs/1708.07149v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
