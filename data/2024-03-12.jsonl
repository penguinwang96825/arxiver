{"title": "LaERC-S: Improving LLM-based Emotion Recognition in Conversation with\n  Speaker Characteristics", "abstract": "Emotion recognition in conversation (ERC), the task of discerning human\nemotions for each utterance within a conversation, has garnered significant\nattention in human-computer interaction systems. Previous ERC studies focus on\nspeaker-specific information that predominantly stems from relationships among\nutterances, which lacks sufficient information around conversations. Recent\nresearch in ERC has sought to exploit pre-trained large language models (LLMs)\nwith speaker modelling to comprehend emotional states. Although these methods\nhave achieved encouraging results, the extracted speaker-specific information\nstruggles to indicate emotional dynamics. In this paper, motivated by the fact\nthat speaker characteristics play a crucial role and LLMs have rich world\nknowledge, we present LaERC-S, a novel framework that stimulates LLMs to\nexplore speaker characteristics involving the mental state and behavior of\ninterlocutors, for accurate emotion predictions. To endow LLMs with this\nknowledge information, we adopt the two-stage learning to make the models\nreason speaker characteristics and track the emotion of the speaker in complex\nconversation scenarios. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of LaERC-S, reaching the new state-of-the-art.", "published": "2024-03-12 02:37:11", "link": "http://arxiv.org/abs/2403.07260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Explainable Knowledge Tracing", "abstract": "With the long term accumulation of high quality educational data, artificial\nintelligence has shown excellent performance in knowledge tracing. However, due\nto the lack of interpretability and transparency of some algorithms, this\napproach will result in reduced stakeholder trust and a decreased acceptance of\nintelligent decisions. Therefore, algorithms need to achieve high accuracy, and\nusers need to understand the internal operating mechanism and provide reliable\nexplanations for decisions. This paper thoroughly analyzes the interpretability\nof KT algorithms. First, the concepts and common methods of explainable\nartificial intelligence and knowledge tracing are introduced. Next, explainable\nknowledge tracing models are classified into two categories: transparent models\nand black box models. Then, the interpretable methods used are reviewed from\nthree stages: ante hoc interpretable methods, post hoc interpretable methods,\nand other dimensions. It is worth noting that current evaluation methods for\nexplainable knowledge tracing are lacking. Hence, contrast and deletion\nexperiments are conducted to explain the prediction results of the deep\nknowledge tracing model on the ASSISTment2009 by using three XAI methods.\nMoreover, this paper offers some insights into evaluation methods from the\nperspective of educational stakeholders. This paper provides a detailed and\ncomprehensive review of the research on explainable knowledge tracing, aiming\nto offer some basis and inspiration for researchers interested in the\ninterpretability of knowledge tracing.", "published": "2024-03-12 03:17:59", "link": "http://arxiv.org/abs/2403.07279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-generated Text Detection: Benchmark Dataset and Tensor-based\n  Detection Method", "abstract": "As natural language models like ChatGPT become increasingly prevalent in\napplications and services, the need for robust and accurate methods to detect\ntheir output is of paramount importance. In this paper, we present GPT Reddit\nDataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text\ndetection dataset designed to assess the performance of detection models in\nidentifying generated responses from ChatGPT. The dataset consists of a diverse\ncollection of context-prompt pairs based on Reddit, with human-generated and\nChatGPT-generated responses. We provide an analysis of the dataset's\ncharacteristics, including linguistic diversity, context complexity, and\nresponse quality. To showcase the dataset's utility, we benchmark several\ndetection methods on it, demonstrating their efficacy in distinguishing between\nhuman and ChatGPT-generated responses. This dataset serves as a resource for\nevaluating and advancing detection techniques in the context of ChatGPT and\ncontributes to the ongoing efforts to ensure responsible and trustworthy\nAI-driven communication on the internet. Finally, we propose GpTen, a novel\ntensor-based GPT text detection method that is semi-supervised in nature since\nit only has access to human-generated text and performs on par with\nfully-supervised baselines.", "published": "2024-03-12 05:15:21", "link": "http://arxiv.org/abs/2403.07321v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki", "abstract": "NLP in the age of monolithic large language models is approaching its limits\nin terms of size and information that can be handled. The trend goes to\nmodularization, a necessary step into the direction of designing smaller\nsub-networks and components with specialized functionality. In this paper, we\npresent the MAMMOTH toolkit: a framework designed for training massively\nmultilingual modular machine translation systems at scale, initially derived\nfrom OpenNMT-py and then adapted to ensure efficient training across\ncomputation clusters. We showcase its efficiency across clusters of A100 and\nV100 NVIDIA GPUs, and discuss our design philosophy and plans for future\ninformation. The toolkit is publicly available online.", "published": "2024-03-12 11:32:30", "link": "http://arxiv.org/abs/2403.07544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Truth-Aware Context Selection: Mitigating Hallucinations of Large\n  Language Models Being Misled by Untruthful Contexts", "abstract": "Although Large Language Models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by untruthful contexts provided\nby users or knowledge augmentation tools, leading to hallucinations. To\nalleviate LLMs from being misled by untruthful context and take advantage of\nknowledge augmentation, we propose Truth-Aware Context Selection (TACS), a\nlightweight method to adaptively recognize and mask untruthful context from the\ninputs. TACS begins by performing truth detection on the input context,\nleveraging the parameterized knowledge within the LLM. Subsequently, it\nconstructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results indicate that TACS can effectively\nfilter untruthful context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.", "published": "2024-03-12 11:40:44", "link": "http://arxiv.org/abs/2403.07556v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource\n  Agglutinative Data-to-Text Generation", "abstract": "Most data-to-text datasets are for English, so the difficulties of modelling\ndata-to-text for low-resource languages are largely unexplored. In this paper\nwe tackle data-to-text for isiXhosa, which is low-resource and agglutinative.\nWe introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of\nWebNLG, which presents a new linguistic context that shifts modelling demands\nto subword-driven techniques. We also develop an evaluation framework for T2X\nthat measures how accurately generated text describes the data. This enables\nfuture users of T2X to go beyond surface-level metrics in evaluation. On the\nmodelling side we explore two classes of methods - dedicated data-to-text\nmodels trained from scratch and pretrained language models (PLMs). We propose a\nnew dedicated architecture aimed at agglutinative data-to-text, the Subword\nSegmental Pointer Generator (SSPG). It jointly learns to segment words and copy\nentities, and outperforms existing dedicated models for 2 agglutinative\nlanguages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,\nwhich reveals that standard PLMs come up short. Fine-tuning machine translation\nmodels emerges as the best method overall. These findings underscore the\ndistinct challenge presented by T2X: neither well-established data-to-text\narchitectures nor customary pretrained methodologies prove optimal. We conclude\nwith a qualitative analysis of generation errors and an ablation study.", "published": "2024-03-12 11:53:27", "link": "http://arxiv.org/abs/2403.07567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced\n  Personality Detection Model", "abstract": "Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.", "published": "2024-03-12 12:10:18", "link": "http://arxiv.org/abs/2403.07581v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models", "abstract": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.", "published": "2024-03-12 14:57:40", "link": "http://arxiv.org/abs/2403.07714v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and\n  Related Observable Overgeneration Mistakes", "abstract": "This paper presents the results of the SHROOM, a shared task focused on\ndetecting hallucinations: outputs from natural language generation (NLG)\nsystems that are fluent, yet inaccurate. Such cases of overgeneration put in\njeopardy many NLG applications, where correctness is often mission-critical.\nThe shared task was conducted with a newly constructed dataset of 4000 model\noutputs labeled by 5 annotators each, spanning 3 NLP tasks: machine\ntranslation, paraphrase generation and definition modeling.\n  The shared task was tackled by a total of 58 different users grouped in 42\nteams, out of which 27 elected to write a system description paper;\ncollectively, they submitted over 300 prediction sets on both tracks of the\nshared task. We observe a number of key trends in how this approach was tackled\n-- many participants rely on a handful of model, and often rely either on\nsynthetic data for fine-tuning or zero-shot prompting strategies. While a\nmajority of the teams did outperform our proposed baseline system, the\nperformances of top-scoring systems are still consistent with a random handling\nof the more challenging items.", "published": "2024-03-12 15:06:22", "link": "http://arxiv.org/abs/2403.07726v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Large Language Models with Sequential Instructions", "abstract": "Despite the success of existing instruction-tuned models, we find that they\nusually struggle to respond to queries with multiple instructions. This impairs\ntheir performance in complex problems whose solution consists of multiple\nintermediate tasks. Thus, we contend that part of the fine-tuning data mixture\nshould be sequential--containing a chain of interrelated tasks. We first\napproach sequential instruction tuning from a task-driven perspective, manually\ncreating interpretable intermediate tasks for multilingual and visual question\nanswering: namely \"translate then predict\" and \"caption then answer\". Next, we\nautomate this process by turning instructions in existing datasets (e.g.,\nAlpaca and FlanCoT) into diverse and complex sequential instructions, making\nour method general-purpose. Models that underwent our sequential instruction\ntuning show improved results in coding, maths, and open-ended generation.\nMoreover, we put forward a new benchmark named SeqEval to evaluate a model's\nability to follow all the instructions in a sequence, which further\ncorroborates the benefits of our fine-tuning method. We hope that our\nendeavours will open new research avenues on instruction tuning for complex\ntasks.", "published": "2024-03-12 16:33:30", "link": "http://arxiv.org/abs/2403.07794v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficiently Quantifying and Mitigating Ripple Effects in Model Editing", "abstract": "Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy. However, editing these models, crucial for rectifying\noutdated or erroneous information, often leads to a complex issue known as the\nripple effect in the hidden space. While difficult to detect, this effect can\nsignificantly impede the efficacy of model editing tasks and deteriorate model\nperformance. This paper addresses this scientific challenge by proposing a\nnovel evaluation methodology, Graphical Impact Evaluation(GIE), which\nquantitatively evaluates the adaptations of the model and the subsequent impact\nof editing. Furthermore, we introduce the Selective Impact Revision(SIR), a\nmodel editing method designed to mitigate this ripple effect. Our comprehensive\nevaluations reveal that the ripple effect in the hidden space is a significant\nissue in all current model editing methods. However, our proposed methods, GIE\nand SIR, effectively identify and alleviate this issue, contributing to the\nadvancement of LLM editing techniques.", "published": "2024-03-12 17:04:28", "link": "http://arxiv.org/abs/2403.07825v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Generative Large Language Model Evaluation for Semantic\n  Comprehension", "abstract": "Despite their sophisticated capabilities, large language models (LLMs)\nencounter a major hurdle in effective assessment. This paper first revisits the\nprevalent evaluation method-multiple choice question answering (MCQA), which\nallows for straightforward accuracy measurement. Through a comprehensive\nevaluation of 24 models across 11 benchmarks, we highlight several potential\ndrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation\nand the generation of open-ended responses in practical scenarios. In response,\nwe introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,\nGoogle-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with\nGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This\nsystem is designed to mirror real-world usage, and for this purpose, we have\ncompiled a new benchmark called ``Real-world questions'' (RWQ), comprising\n20,772 authentic user inquiries. Additionally, we thoroughly analyze the\ncharacteristics of our system and compare it with prior leaderboards like\nAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo\nsystem, the feasibility of registering new models, and its potential to reshape\nLLM leaderboards.", "published": "2024-03-12 17:59:48", "link": "http://arxiv.org/abs/2403.07872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debatrix: Multi-dimensional Debate Judge with Iterative Chronological\n  Analysis Based on LLM", "abstract": "How can we construct an automated debate judge to evaluate an extensive,\nvibrant, multi-turn debate? This task is challenging, as judging a debate\ninvolves grappling with lengthy texts, intricate argument relationships, and\nmulti-dimensional assessments. At the same time, current research mainly\nfocuses on short dialogues, rarely touching upon the evaluation of an entire\ndebate. In this paper, by leveraging Large Language Models (LLMs), we propose\nDebatrix, which makes the analysis and assessment of multi-turn debates more\naligned with majority preferences. Specifically, Debatrix features a vertical,\niterative chronological analysis and a horizontal, multi-dimensional evaluation\ncollaboration. To align with real-world debate scenarios, we introduced the\nPanelBench benchmark, comparing our system's performance to actual debate\noutcomes. The findings indicate a notable enhancement over directly using LLMs\nfor debate evaluation. Source code and benchmark data are available online at\nhttps://github.com/ljcleo/debatrix .", "published": "2024-03-12 18:19:47", "link": "http://arxiv.org/abs/2403.08010v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Authorship Style Transfer with Policy Optimization", "abstract": "Authorship style transfer aims to rewrite a given text into a specified\ntarget while preserving the original meaning in the source. Existing approaches\nrely on the availability of a large number of target style exemplars for model\ntraining. However, these overlook cases where a limited number of target style\nexamples are available. The development of parameter-efficient transfer\nlearning techniques and policy optimization (PO) approaches suggest lightweight\nPO is a feasible approach to low-resource style transfer. In this work, we\npropose a simple two-stage tune-and-optimize technique for low-resource textual\nstyle transfer. We apply our technique to authorship transfer as well as a\nlarger-data native language style task and in both cases find it outperforms\nstate-of-the-art baseline models.", "published": "2024-03-12 19:34:54", "link": "http://arxiv.org/abs/2403.08043v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Big City Bias: Evaluating the Impact of Metropolitan Size on\n  Computational Job Market Abilities of Language Models", "abstract": "Large language models (LLMs) have emerged as a useful technology for job\nmatching, for both candidates and employers. Job matching is often based on a\nparticular geographic location, such as a city or region. However, LLMs have\nknown biases, commonly derived from their training data. In this work, we aim\nto quantify the metropolitan size bias encoded within large language models,\nevaluating zero-shot salary, employer presence, and commute duration\npredictions in 384 of the United States' metropolitan regions. Across all\nbenchmarks, we observe negative correlations between the metropolitan size and\nthe performance of the LLMS, indicating that smaller regions are indeed\nunderrepresented. More concretely, the smallest 10 metropolitan regions show\nupwards of 300% worse benchmark performance than the largest 10.", "published": "2024-03-12 19:40:18", "link": "http://arxiv.org/abs/2403.08046v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Generating Clarification Questions for Disambiguating Contracts", "abstract": "Enterprises frequently enter into commercial contracts that can serve as\nvital sources of project-specific requirements. Contractual clauses are\nobligatory, and the requirements derived from contracts can detail the\ndownstream implementation activities that non-legal stakeholders, including\nrequirement analysts, engineers, and delivery personnel, need to conduct.\nHowever, comprehending contracts is cognitively demanding and error-prone for\nsuch stakeholders due to the extensive use of Legalese and the inherent\ncomplexity of contract language. Furthermore, contracts often contain\nambiguously worded clauses to ensure comprehensive coverage. In contrast,\nnon-legal stakeholders require a detailed and unambiguous comprehension of\ncontractual clauses to craft actionable requirements. In this work, we\nintroduce a novel legal NLP task that involves generating clarification\nquestions for contracts. These questions aim to identify contract ambiguities\non a document level, thereby assisting non-legal stakeholders in obtaining the\nnecessary details for eliciting requirements. This task is challenged by three\ncore issues: (1) data availability, (2) the length and unstructured nature of\ncontracts, and (3) the complexity of legal text. To address these issues, we\npropose ConRAP, a retrieval-augmented prompting framework for generating\nclarification questions to disambiguate contractual text. Experiments conducted\non contracts sourced from the publicly available CUAD dataset show that ConRAP\nwith ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the\ngenerated clarification questions are deemed useful by human evaluators.", "published": "2024-03-12 19:57:39", "link": "http://arxiv.org/abs/2403.08053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAGEL: Bootstrapping Agents by Guiding Exploration with Language", "abstract": "Following natural language instructions by executing actions in digital\nenvironments (e.g. web-browsers and REST APIs) is a challenging task for\nlanguage model (LM) agents. Unfortunately, LM agents often fail to generalize\nto new environments without human demonstrations. This work presents BAGEL, a\nmethod for bootstrapping LM agents without human supervision. BAGEL converts a\nseed set of randomly explored trajectories or synthetic instructions, into\ndemonstrations, via round-trips between two noisy LM components: an LM labeler\nwhich converts a trajectory into a synthetic instruction, and a zero-shot LM\nagent which maps the synthetic instruction into a refined trajectory. By\nperforming these round-trips iteratively, BAGEL quickly converts the initial\ndistribution of trajectories towards those that are well-described by natural\nlanguage. We use BAGEL demonstrations to adapt a zero shot LM agent at test\ntime via in-context learning over retrieved demonstrations, and find\nimprovements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x\nreduction in execution failures.", "published": "2024-03-12 23:59:15", "link": "http://arxiv.org/abs/2403.08140v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CALF: Aligning LLMs for Time Series Forecasting via Cross-modal\n  Fine-Tuning", "abstract": "Deep learning (e.g., Transformer) has been widely and successfully used in\nmultivariate time series forecasting (MTSF). Unlike existing methods that focus\non training models from a single modal of time series input, large language\nmodels (LLMs) based MTSF methods with cross-modal text and time series input\nhave recently shown great superiority, especially with limited temporal data.\nHowever, current LLM-based MTSF methods usually focus on adapting and\nfine-tuning LLMs, while neglecting the distribution discrepancy between textual\nand temporal input tokens, thus leading to sub-optimal performance. To address\nthis issue, we propose a novel Cross-Modal LLM Fine-Tuning (CALF) framework for\nMTSF by reducing the distribution discrepancy between textual and temporal\ndata, which mainly consists of the temporal target branch with temporal input\nand the textual source branch with aligned textual input. To reduce the\ndistribution discrepancy, we develop the cross-modal match module to first\nalign cross-modal input distributions. Additionally, to minimize the modality\ndistribution gap in both feature and output spaces, feature regularization loss\nis developed to align the intermediate features between the two branches for\nbetter weight updates, while output consistency loss is introduced to allow the\noutput representations of both branches to correspond effectively. Thanks to\nthe modality alignment, CALF establishes state-of-the-art performance for both\nlong-term and short-term forecasting tasks with low computational complexity,\nand exhibiting favorable few-shot and zero-shot abilities similar to that in\nLLMs. Code is available at https://github.com/Hank0626/LLaTA.", "published": "2024-03-12 04:04:38", "link": "http://arxiv.org/abs/2403.07300v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Knowledge Graph Large Language Model (KG-LLM) for Link Prediction", "abstract": "The task of multi-hop link prediction within knowledge graphs (KGs) stands as\na challenge in the field of knowledge graph analysis, as it requires the model\nto reason through and understand all intermediate connections before making a\nprediction. In this paper, we introduce the Knowledge Graph Large Language\nModel (KG-LLM), a novel framework that leverages large language models (LLMs)\nfor knowledge graph tasks. We first convert structured knowledge graph data\ninto natural language and then use these natural language prompts to fine-tune\nLLMs to enhance multi-hop link prediction in KGs. By converting the KG to\nnatural language prompts, our framework is designed to learn the latent\nrepresentations of entities and their interrelations. To show the efficacy of\nthe KG-LLM Framework, we fine-tune three leading LLMs within this framework,\nincluding Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's\npotential to provide LLMs with zero-shot capabilities for handling previously\nunseen prompts. Experimental results show that KG-LLM significantly improves\nthe models' generalization capabilities, leading to more accurate predictions\nin unfamiliar scenarios.", "published": "2024-03-12 04:47:29", "link": "http://arxiv.org/abs/2403.07311v8", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive\n  Learning", "abstract": "Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of\nfine-grained sentiment analysis, aiming to extract structured sentiment\ntriplets from unstructured textual data. Existing approaches to ASTE often\ncomplicate the task with additional structures or external data. In this\nresearch, we propose a novel tagging scheme and employ a contrastive learning\napproach to mitigate these challenges. The proposed approach demonstrates\ncomparable or superior performance in comparison to state-of-the-art\ntechniques, while featuring a more compact design and reduced computational\noverhead. Notably, even in the era of Large Language Models (LLMs), our method\nexhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning\nscenarios. This study also provides valuable insights for the advancement of\nASTE techniques within the paradigm of large language models.", "published": "2024-03-12 06:01:04", "link": "http://arxiv.org/abs/2403.07342v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large\n  Language Model Compression", "abstract": "The advancements in Large Language Models (LLMs) have been hindered by their\nsubstantial sizes, which necessitates LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression methods\nhave two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the compressed weights after SVD\ntruncation. In this work, we propose SVD-LLM, a SVD-based post-training LLM\ncompression method that addresses the limitations of existing methods. SVD-LLM\nincorporates a truncation-aware data whitening technique to ensure a direct\nmapping between singular values and compression loss. Moreover, SVD-LLM adopts\na parameter update with sequential low-rank approximation to compensate for the\naccuracy degradation after SVD compression. We evaluate SVD-LLM on 10 datasets\nand seven models from three different LLM families at three different scales.\nOur results demonstrate the superiority of SVD-LLM over state-of-the-arts,\nespecially at high model compression ratios. Our code is available at\nhttps://github.com/AIoT-MLSys-Lab/SVD-LLM", "published": "2024-03-12 07:31:18", "link": "http://arxiv.org/abs/2403.07378v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs", "abstract": "Event commonsense reasoning requires the ability to reason about the\nrelationship between events, as well as infer implicit context underlying that\nrelationship. However, data scarcity makes it challenging for language models\nto learn to generate commonsense inferences for contexts and questions\ninvolving interactions between complex events. To address this demand, we\npresent COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop\nlogical queries (e.g., the joint effect or cause of both event A and B, or the\neffect of the effect of event C) from an existing commonsense knowledge graph\n(CSKG), and verbalizing them using handcrafted rules and large language models\ninto multiple-choice and text generation questions. Our experiments show that\nlanguage models trained on COM2 exhibit significant improvements in complex\nreasoning ability, resulting in enhanced zero-shot performance in both\nin-domain and out-of-domain tasks for question answering and generative\ncommonsense reasoning, without expensive human annotations. Code and data are\navailable at https://github.com/tqfang/complex-commonsense-reasoning.", "published": "2024-03-12 08:13:52", "link": "http://arxiv.org/abs/2403.07398v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\n  Brain-Inspired Method for Parameter-Efficient Fine-Tuning", "abstract": "Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\nbeen proven to significantly enhance model performance on a variety of\ndownstream tasks and effectively control the output behaviors of LPLMs. Recent\nstudies have proposed numerous methods for fine-tuning a small number of\nparameters based on open-source LPLMs, reducing the demand for computational\nand storage resources. Among these, reparameterization fine-tuning methods\nrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\nalthough these methods perform well in many aspects, there is still\nconsiderable room for improvement in terms of complex task adaptability,\nperformance, stability, and algorithm complexity. In response to this, inspired\nby the idea that the functions of the brain are shaped by its geometric\nstructure, this paper integrates this idea into LoRA technology and proposes a\nnew matrix transformation-based reparameterization method for efficient\nfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\nMTLoRA aims to dynamically alter its spatial geometric structure by applying a\ntransformation-matrix T to perform linear transformations, such as rotation,\nscaling, and translation, on the task-specific parameter matrix, generating new\nmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\ncomplex geometric structure feature patterns in the brain on functions, thereby\nenhancing the model's performance in downstream tasks. In Natural Language\nUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\nthe results reveal that MTLoRA achieves an overall performance increase of\nabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\nMTLoRA improves performance by an average of 0.95% and 0.56% in the DART and\nWebNLG tasks, respectively.", "published": "2024-03-12 09:32:25", "link": "http://arxiv.org/abs/2403.07440v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SIFiD: Reassess Summary Factual Inconsistency Detection with LLM", "abstract": "Ensuring factual consistency between the summary and the original document is\nparamount in summarization tasks. Consequently, considerable effort has been\ndedicated to detecting inconsistencies. With the advent of Large Language\nModels (LLMs), recent studies have begun to leverage their advanced language\nunderstanding capabilities for inconsistency detection. However, early attempts\nhave shown that LLMs underperform traditional models due to their limited\nability to follow instructions and the absence of an effective detection\nmethodology. In this study, we reassess summary inconsistency detection with\nLLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in\nLLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency\nDetection with Filtered Document) that identify key sentences within documents\nby either employing natural language inference or measuring semantic similarity\nbetween summaries and documents.", "published": "2024-03-12 11:41:51", "link": "http://arxiv.org/abs/2403.07557v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Harder Tasks Need More Experts: Dynamic Routing in MoE Models", "abstract": "In this paper, we introduce a novel dynamic expert selection framework for\nMixture of Experts (MoE) models, aiming to enhance computational efficiency and\nmodel performance by adjusting the number of activated experts based on input\ndifficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,\nwhich activates a predetermined number of experts regardless of the input's\ncomplexity, our method dynamically selects experts based on the confidence\nlevel in expert selection for each input. This allows for a more efficient\nutilization of computational resources, activating more experts for complex\ntasks requiring advanced reasoning and fewer for simpler tasks. Through\nextensive evaluations, our dynamic routing method demonstrates substantial\nimprovements over conventional Top-2 routing across various benchmarks,\nachieving an average improvement of 0.7% with less than 90% activated\nparameters. Further analysis shows our model dispatches more experts to tasks\nrequiring complex reasoning skills, like BBH, confirming its ability to\ndynamically allocate computational resources in alignment with the input's\ncomplexity. Our findings also highlight a variation in the number of experts\nneeded across different layers of the transformer model, offering insights into\nthe potential for designing heterogeneous MoE frameworks. The code and models\nare available at https://github.com/ZhenweiAn/Dynamic_MoE.", "published": "2024-03-12 13:41:15", "link": "http://arxiv.org/abs/2403.07652v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MoralBERT: A Fine-Tuned Language Model for Capturing Moral Values in\n  Social Discussions", "abstract": "Moral values play a fundamental role in how we evaluate information, make\ndecisions, and form judgements around important social issues. Controversial\ntopics, including vaccination, abortion, racism, and sexual orientation, often\nelicit opinions and attitudes that are not solely based on evidence but rather\nreflect moral worldviews. Recent advances in Natural Language Processing (NLP)\nshow that moral values can be gauged in human-generated textual content.\nBuilding on the Moral Foundations Theory (MFT), this paper introduces\nMoralBERT, a range of language representation models fine-tuned to capture\nmoral sentiment in social discourse. We describe a framework for both\naggregated and domain-adversarial training on multiple heterogeneous MFT\nhuman-annotated datasets sourced from Twitter (now X), Reddit, and Facebook\nthat broaden textual content diversity in terms of social media audience\ninterests, content presentation and style, and spreading patterns. We show that\nthe proposed framework achieves an average F1 score that is between 11% and 32%\nhigher than lexicon-based approaches, Word2Vec embeddings, and zero-shot\nclassification with large language models such as GPT-4 for in-domain\ninference. Domain-adversarial training yields better out-of domain predictions\nthan aggregate training while achieving comparable performance to zero-shot\nlearning. Our approach contributes to annotation-free and effective morality\nlearning, and provides useful insights towards a more comprehensive\nunderstanding of moral narratives in controversial social debates using NLP.", "published": "2024-03-12 14:12:59", "link": "http://arxiv.org/abs/2403.07678v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted\n  Technical Debt", "abstract": "Self-admitted technical debt (SATD) refers to a form of technical debt in\nwhich developers explicitly acknowledge and document the existence of technical\nshortcuts, workarounds, or temporary solutions within the codebase. Over recent\nyears, researchers have manually labeled datasets derived from various software\ndevelopment artifacts: source code comments, messages from the issue tracker\nand pull request sections, and commit messages. These datasets are designed for\ntraining, evaluation, performance validation, and improvement of machine\nlearning and deep learning models to accurately identify SATD instances.\nHowever, class imbalance poses a serious challenge across all the existing\ndatasets, particularly when researchers are interested in categorizing the\nspecific types of SATD. In order to address the scarcity of labeled data for\nSATD \\textit{identification} (i.e., whether an instance is SATD or not) and\n\\textit{categorization} (i.e., which type of SATD is being classified) in\nexisting datasets, we share the \\textit{SATDAUG} dataset, an augmented version\nof existing SATD datasets, including source code comments, issue tracker, pull\nrequests, and commit messages. These augmented datasets have been balanced in\nrelation to the available artifacts and provide a much richer source of labeled\ndata for training machine learning or deep learning models.", "published": "2024-03-12 14:33:53", "link": "http://arxiv.org/abs/2403.07690v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "ORPO: Monolithic Preference Optimization without Reference Model", "abstract": "While recent preference alignment algorithms for language models have\ndemonstrated promising results, supervised fine-tuning (SFT) remains imperative\nfor achieving successful convergence. In this paper, we study the crucial role\nof SFT within the context of preference alignment, emphasizing that a minor\npenalty for the disfavored generation style is sufficient for\npreference-aligned SFT. Building on this foundation, we introduce a\nstraightforward and innovative reference model-free monolithic odds ratio\npreference optimization algorithm, ORPO, eliminating the necessity for an\nadditional preference alignment phase. We demonstrate, both empirically and\ntheoretically, that the odds ratio is a sensible choice for contrasting favored\nand disfavored styles during SFT across the diverse sizes from 125M to 7B.\nSpecifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with\nORPO on the UltraFeedback alone surpasses the performance of state-of-the-art\nlanguage models with more than 7B and 13B parameters: achieving up to 12.20% on\n$\\text{AlpacaEval}_{2.0}$ (Figure 1), 66.19% on IFEval (instruction-level\nloose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model\ncheckpoints for Mistral-ORPO-$\\alpha$ (7B) and Mistral-ORPO-$\\beta$ (7B).", "published": "2024-03-12 14:34:08", "link": "http://arxiv.org/abs/2403.07691v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large, Small or Both: A Novel Data Augmentation Framework Based on\n  Language Models for Debiasing Opinion Summarization", "abstract": "As more than 70$\\%$ of reviews in the existing opinion summary data set are\npositive, current opinion summarization approaches are reluctant to generate\nnegative summaries given the input of negative texts. To address such sentiment\nbias, a direct approach without the over-reliance on a specific framework is to\ngenerate additional data based on large language models to balance the\nemotional distribution of the dataset. However, data augmentation based on\nlarge language models faces two disadvantages: 1) the potential issues or\ntoxicity in the augmented data; 2) the expensive costs. Therefore, in this\npaper, we propose a novel data augmentation framework based on both large and\nsmall language models for debiasing opinion summarization. In specific, a small\nsize of synthesized negative reviews is obtained by rewriting the positive text\nvia a large language model. Then, a disentangle reconstruction model is trained\nbased on the generated data. After training, a large amount of synthetic data\ncan be obtained by decoding the new representation obtained from the\ncombination of different sample representations and filtering based on\nconfusion degree and sentiment classification. Experiments have proved that our\nframework can effectively alleviate emotional bias same as using only large\nmodels, but more economically.", "published": "2024-03-12 14:37:03", "link": "http://arxiv.org/abs/2403.07693v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Reinforcement Learning from Human Feedback Using Contrastive\n  Rewards", "abstract": "Reinforcement learning from human feedback (RLHF) is the mainstream paradigm\nused to align large language models (LLMs) with human preferences. Yet existing\nRLHF heavily relies on accurate and informative reward models, which are\nvulnerable and sensitive to noise from various sources, e.g. human labeling\nerrors, making the pipeline fragile. In this work, we improve the effectiveness\nof the reward model by introducing a penalty term on the reward, named as\n\\textit{contrastive rewards}. %Contrastive rewards Our approach involves two\nsteps: (1) an offline sampling step to obtain responses to prompts that serve\nas baseline calculation and (2) a contrastive reward calculated using the\nbaseline responses and used in the Proximal Policy Optimization (PPO) step. We\nshow that contrastive rewards enable the LLM to penalize reward uncertainty,\nimprove robustness, encourage improvement over baselines, calibrate according\nto task difficulty, and reduce variance in PPO. We show empirically contrastive\nrewards can improve RLHF substantially, evaluated by both GPTs and humans, and\nour method consistently outperforms strong baselines.", "published": "2024-03-12 14:51:57", "link": "http://arxiv.org/abs/2403.07708v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese\n  Large Language Models", "abstract": "To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon.", "published": "2024-03-12 15:32:39", "link": "http://arxiv.org/abs/2403.07747v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond Memorization: The Challenge of Random Memory Access in Language\n  Models", "abstract": "Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.", "published": "2024-03-12 16:42:44", "link": "http://arxiv.org/abs/2403.07805v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "pyvene: A Library for Understanding and Improving PyTorch Models via\n  Interventions", "abstract": "Interventions on model-internal states are fundamental operations in many\nareas of AI, including model editing, steering, robustness, and\ninterpretability. To facilitate such research, we introduce $\\textbf{pyvene}$,\nan open-source Python library that supports customizable interventions on a\nrange of different PyTorch modules. $\\textbf{pyvene}$ supports complex\nintervention schemes with an intuitive configuration format, and its\ninterventions can be static or include trainable parameters. We show how\n$\\textbf{pyvene}$ provides a unified and extensible framework for performing\ninterventions on neural models and sharing the intervened upon models with\nothers. We illustrate the power of the library via interpretability analyses\nusing causal abstraction and knowledge localization. We publish our library\nthrough Python Package Index (PyPI) and provide code, documentation, and\ntutorials at https://github.com/stanfordnlp/pyvene.", "published": "2024-03-12 16:46:54", "link": "http://arxiv.org/abs/2403.07809v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM", "abstract": "We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff.", "published": "2024-03-12 16:54:58", "link": "http://arxiv.org/abs/2403.07816v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards a clinically accessible radiology foundation model: open-access\n  and lightweight, with automated evaluation", "abstract": "The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world clinics. Frontier general-domain models such as GPT-4V still\nhave significant performance gaps in multimodal biomedical applications. More\nimportantly, less-acknowledged pragmatic issues, including accessibility, model\ncost, and tedious manual evaluation make it hard for clinicians to use\nstate-of-the-art large models directly on private patient data. Here, we\nexplore training open-source small multimodal models (SMMs) to bridge\ncompetency gaps for unmet clinical needs in radiology. To maximize data\nefficiency, we adopt a modular approach by incorporating state-of-the-art\npre-trained models for image and text modalities, and focusing on training a\nlightweight adapter to ground each modality to the text embedding space, as\nexemplified by LLaVA-Med. For training, we assemble a large dataset of over 697\nthousand radiology image-text pairs. For evaluation, we propose CheXprompt, a\nGPT-4-based metric for factuality evaluation, and demonstrate its parity with\nexpert evaluation. For best practice, we conduct a systematic ablation study on\nvarious choices in data engineering and multimodal training. The resulting\nLlaVA-Rad (7B) model attains state-of-the-art results on standard radiology\ntasks such as report generation and cross-modal retrieval, even outperforming\nmuch larger models such as GPT-4V and Med-PaLM M (84B). The inference of\nLlaVA-Rad is fast and can be performed on a single V100 GPU in private\nsettings, offering a promising state-of-the-art tool for real-world clinical\napplications.", "published": "2024-03-12 18:12:02", "link": "http://arxiv.org/abs/2403.08002v5", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the\n  Challenges and Opportunities of Large Language Models in Hate Speech\n  Detection", "abstract": "Large language models (LLMs) excel in many diverse applications beyond\nlanguage generation, e.g., translation, summarization, and sentiment analysis.\nOne intriguing application is in text classification. This becomes pertinent in\nthe realm of identifying hateful or toxic speech -- a domain fraught with\nchallenges and ethical dilemmas. In our study, we have two objectives: firstly,\nto offer a literature review revolving around LLMs as classifiers, emphasizing\ntheir role in detecting and classifying hateful or toxic content. Subsequently,\nwe explore the efficacy of several LLMs in classifying hate speech: identifying\nwhich LLMs excel in this task as well as their underlying attributes and\ntraining. Providing insight into the factors that contribute to an LLM\nproficiency (or lack thereof) in discerning hateful content. By combining a\ncomprehensive literature review with an empirical analysis, our paper strives\nto shed light on the capabilities and constraints of LLMs in the crucial domain\nof hate speech detection.", "published": "2024-03-12 19:12:28", "link": "http://arxiv.org/abs/2403.08035v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CHAI: Clustered Head Attention for Efficient LLM Inference", "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have\ntransformed the field of machine learning. However, serving these models at\ninference time is both compute and memory intensive, where a single request can\nrequire multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is\none of the key components of LLMs, which can account for over 50% of LLMs\nmemory and compute requirement. We observe that there is a high amount of\nredundancy across heads on which tokens they pay attention to. Based on this\ninsight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a\nhigh amount of correlation for self-attention at runtime, thus reducing both\nmemory and compute. In our experiments, we show that CHAI is able to reduce the\nmemory requirements for storing K,V cache by up to 21.4% and inference time\nlatency by up to 1.73x without any fine-tuning required. CHAI achieves this\nwith a maximum 3.2% deviation in accuracy across 3 different models (i.e.\nOPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.", "published": "2024-03-12 20:10:04", "link": "http://arxiv.org/abs/2403.08058v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Contextual Clarity: Generating Sentences with Transformer Models using\n  Context-Reverso Data", "abstract": "In the age of information abundance, the ability to provide users with\ncontextually relevant and concise information is crucial. Keyword in Context\n(KIC) generation is a task that plays a vital role in and generation\napplications, such as search engines, personal assistants, and content\nsummarization. In this paper, we present a novel approach to generating\nunambiguous and brief sentence-contexts for given keywords using the T5\ntransformer model, leveraging data obtained from the Context-Reverso API. The\ncode is available at https://github.com/Rusamus/word2context/tree/main .", "published": "2024-03-12 22:23:08", "link": "http://arxiv.org/abs/2403.08103v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Comprehensive Implementation of TextCNN for Enhanced Collaboration\n  between Natural Language Processing and System Recommendation", "abstract": "Natural Language Processing (NLP) is an important branch of artificial\nintelligence that studies how to enable computers to understand, process, and\ngenerate human language. Text classification is a fundamental task in NLP,\nwhich aims to classify text into different predefined categories. Text\nclassification is the most basic and classic task in natural language\nprocessing, and most of the tasks in natural language processing can be\nregarded as classification tasks. In recent years, deep learning has achieved\ngreat success in many research fields, and today, it has also become a standard\ntechnology in the field of NLP, which is widely integrated into text\nclassification tasks. Unlike numbers and images, text processing emphasizes\nfine-grained processing ability. Traditional text classification methods\ngenerally require preprocessing the input model's text data. Additionally, they\nalso need to obtain good sample features through manual annotation and then use\nclassical machine learning algorithms for classification. Therefore, this paper\nanalyzes the application status of deep learning in the three core tasks of NLP\n(including text representation, word order modeling, and knowledge\nrepresentation). This content explores the improvement and synergy achieved\nthrough natural language processing in the context of text classification,\nwhile also taking into account the challenges posed by adversarial techniques\nin text generation, text classification, and semantic parsing. An empirical\nstudy on text classification tasks demonstrates the effectiveness of\ninteractive integration training, particularly in conjunction with TextCNN,\nhighlighting the significance of these advancements in text classification\naugmentation and enhancement.", "published": "2024-03-12 07:25:53", "link": "http://arxiv.org/abs/2403.09718v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew\n  Language", "abstract": "In this paper, we introduce summarization MevakerSumm and conclusion\nextraction MevakerConc datasets for the Hebrew language based on the State\nComptroller and Ombudsman of Israel reports, along with two auxiliary datasets.\nWe accompany these datasets with models for conclusion extraction (HeConE,\nHeConEspc) and conclusion allocation (HeCross). All of the code, datasets, and\nmodel checkpoints used in this work are publicly available.", "published": "2024-03-12 08:40:44", "link": "http://arxiv.org/abs/2403.09719v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-tuning vs Prompting, Can Language Models Understand Human Values?", "abstract": "Accurately handling the underlying support values in sentences is crucial for\nunderstanding the speaker's tendencies, yet it poses a challenging task in\nnatural language understanding (NLU). In this article, we explore the potential\nof fine-tuning and prompt tuning in this downstream task, using the Human Value\nDetection 2023. Additionally, we attempt to validate whether models can\neffectively solve the problem based on the knowledge acquired during the\npre-training stage. Simultaneously, our interest lies in the capabilities of\nlarge language models (LLMs) aligned with RLHF in this task, and some\npreliminary attempts are presented.", "published": "2024-03-12 08:49:31", "link": "http://arxiv.org/abs/2403.09720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Semantic Mention Graph Augmented Model for Document-Level Event\n  Argument Extraction", "abstract": "Document-level Event Argument Extraction (DEAE) aims to identify arguments\nand their specific roles from an unstructured document. The advanced approaches\non DEAE utilize prompt-based methods to guide pre-trained language models\n(PLMs) in extracting arguments from input documents. They mainly concentrate on\nestablishing relations between triggers and entity mentions within documents,\nleaving two unresolved problems: a) independent modeling of entity mentions; b)\ndocument-prompt isolation. To this end, we propose a semantic mention Graph\nAugmented Model (GAM) to address these two problems in this paper. Firstly, GAM\nconstructs a semantic mention graph that captures relations within and between\ndocuments and prompts, encompassing co-existence, co-reference and co-type\nrelations. Furthermore, we introduce an ensembled graph transformer module to\naddress mentions and their three semantic relations effectively. Later, the\ngraph-augmented encoder-decoder module incorporates the relation-specific graph\ninto the input embedding of PLMs and optimizes the encoder section with\ntopology information, enhancing the relations comprehensively. Extensive\nexperiments on the RAMS and WikiEvents datasets demonstrate the effectiveness\nof our approach, surpassing baseline methods and achieving a new\nstate-of-the-art performance.", "published": "2024-03-12 08:58:07", "link": "http://arxiv.org/abs/2403.09721v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Readmission Prediction with Deep Learning: Extracting\n  Biomedical Concepts from Clinical Texts", "abstract": "Hospital readmission, defined as patients being re-hospitalized shortly after\ndischarge, is a critical concern as it impacts patient outcomes and healthcare\ncosts. Identifying patients at risk of readmission allows for timely\ninterventions, reducing re-hospitalization rates and overall treatment costs.\nThis study focuses on predicting patient readmission within less than 30 days\nusing text mining techniques applied to discharge report texts from electronic\nhealth records (EHR). Various machine learning and deep learning methods were\nemployed to develop a classification model for this purpose. A novel aspect of\nthis research involves leveraging the Bio-Discharge Summary Bert (BDSS) model\nalong with principal component analysis (PCA) feature extraction to preprocess\ndata for deep learning model input. Our analysis of the MIMIC-III dataset\nindicates that our approach, which combines the BDSS model with a multilayer\nperceptron (MLP), outperforms state-of-the-art methods. This model achieved a\nrecall of 94% and an area under the curve (AUC) of 75%, showcasing its\neffectiveness in predicting patient readmissions. This study contributes to the\nadvancement of predictive modeling in healthcare by integrating text mining\ntechniques with deep learning algorithms to improve patient outcomes and\noptimize resource allocation.", "published": "2024-03-12 09:03:44", "link": "http://arxiv.org/abs/2403.09722v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAD-PHI2: Instruction Tuning PHI-2 for Radiology", "abstract": "Small Language Models (SLMs) have shown remarkable performance in general\ndomain language understanding, reasoning and coding tasks, but their\ncapabilities in the medical domain, particularly concerning radiology text, is\nless explored. In this study, we investigate the application of SLMs for\ngeneral radiology knowledge specifically question answering related to\nunderstanding of symptoms, radiological appearances of findings, differential\ndiagnosis, assessing prognosis, and suggesting treatments w.r.t diseases\npertaining to different organ systems. Additionally, we explore the utility of\nSLMs in handling text-related tasks with respect to radiology reports within\nAI-driven radiology workflows. We fine-tune Phi-2, a SLM with 2.7 billion\nparameters using high-quality educational content from Radiopaedia, a\ncollaborative online radiology resource. The resulting language model,\nRadPhi-2-Base, exhibits the ability to address general radiology queries across\nvarious systems (e.g., chest, cardiac). Furthermore, we investigate Phi-2 for\ninstruction tuning, enabling it to perform specific tasks. By fine-tuning Phi-2\non both general domain tasks and radiology-specific tasks related to chest\nX-ray reports, we create Rad-Phi2. Our empirical results reveal that Rad-Phi2\nBase and Rad-Phi2 perform comparably or even outperform larger models such as\nMistral-7B-Instruct-v0.2 and GPT-4 providing concise and precise answers. In\nsummary, our work demonstrates the feasibility and effectiveness of utilizing\nSLMs in radiology workflows both for knowledge related queries as well as for\nperforming specific tasks related to radiology reports thereby opening up new\navenues for enhancing the quality and efficiency of radiology practice.", "published": "2024-03-12 17:27:22", "link": "http://arxiv.org/abs/2403.09725v1", "categories": ["cs.CL", "cs.AI", "J.3"], "primary_category": "cs.CL"}
{"title": "TMU at TREC Clinical Trials Track 2023", "abstract": "This paper describes Toronto Metropolitan University's participation in the\nTREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced\nnatural language processing techniques and neural language models in our\nexperiments to retrieve the most relevant clinical trials. We illustrate the\noverall methodology, experimental settings, and results of our implementation\nfor the run submission as part of Team - V-TorontoMU.", "published": "2024-03-12 00:45:49", "link": "http://arxiv.org/abs/2403.12088v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A novel interface for adversarial trivia question-writing", "abstract": "A critical component when developing question-answering AIs is an adversarial\ndataset that challenges models to adapt to the complex syntax and reasoning\nunderlying our natural language. Present techniques for procedurally generating\nadversarial texts are not robust enough for training on complex tasks such as\nanswering multi-sentence trivia questions. We instead turn to human-generated\ndata by introducing an interface for collecting adversarial human-written\ntrivia questions. Our interface is aimed towards question writers and players\nof Quiz Bowl, a buzzer-based trivia competition where paragraph-long questions\nconsist of a sequence of clues of decreasing difficulty. To incentivize usage,\na suite of machine learning-based tools in our interface assist humans in\nwriting questions that are more challenging to answer for Quiz Bowl players and\ncomputers alike. Not only does our interface gather training data for the\ngroundbreaking Quiz Bowl AI project QANTA, but it is also a proof-of-concept of\nfuture adversarial data collection for question-answering systems. The results\nof performance-testing our interface with ten originally-composed questions\nindicate that, despite some flaws, our interface's novel question-writing\nfeatures as well as its real-time exposure of useful responses from our machine\nmodels could facilitate and enhance the collection of adversarial questions.", "published": "2024-03-12 02:37:24", "link": "http://arxiv.org/abs/2404.00011v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked\n  Preferences", "abstract": "Direct Preference Optimization (DPO) is an effective technique that leverages\npairwise preference data (usually one chosen and rejected response pair per\nuser prompt) to align LLMs to human preferences. In practice, multiple\nresponses can exist for a given prompt with varying quality relative to each\nother. With availability of such quality ratings for multiple responses, we\npropose utilizing these responses to create multiple preference pairs for a\ngiven prompt. Our work focuses on systematically using the constructed multiple\npreference pair in DPO training via curriculum learning methodology. In\nparticular, we order these multiple pairs of preference data from easy to hard\n(emulating curriculum training) according to various criteria. We show detailed\ncomparisons of our proposed approach to the standard single-pair DPO setting.\nOur method, which we call Curry-DPO consistently shows increased performance\ngains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,\nhighlighting its effectiveness. More specifically, Curry-DPO achieves a score\nof 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs\nwith similar parameter size. Curry-DPO also achieves the highest adjusted win\nrates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and\n87.9% respectively) in our experiments, with notable gains of upto 7.5% when\ncompared to standard DPO technique. We release the preference pairs used in\nalignment at:\nhttps://huggingface.co/datasets/ServiceNow-AI/Curriculum_DPO_preferences", "published": "2024-03-12 00:58:19", "link": "http://arxiv.org/abs/2403.07230v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Framework for Cost-Effective and Self-Adaptive LLM Shaking and\n  Recovery Mechanism", "abstract": "As Large Language Models (LLMs) gain great success in real-world\napplications, an increasing number of users are seeking to develop and deploy\ntheir customized LLMs through cloud services. Nonetheless, in some specific\ndomains, there are still concerns regarding cost and trade-offs between privacy\nissues and accuracy. In this study, we introduce a cost-effective and\nself-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With\ncarefully designed horizontal and vertical shaking operators, we can achieve\ncomparable accuracy results with SOTA privacy-preserving LLM schemes using\nCryptography-based or Differential Privacy-based methods. Experiments also show\nthat with the CypherTalk framework, users can achieve reliable accuracy when\nusing optimized shaking operator settings. To our best knowledge, this is the\nfirst work that considers cost, and trade-off between model utility and privacy\nin LLM scenarios.", "published": "2024-03-12 03:30:04", "link": "http://arxiv.org/abs/2403.07283v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision\n  Integers", "abstract": "GEneral Matrix Multiply (GEMM) is a central operation in deep learning and\ncorresponds to the largest chunk of the compute footprint. Therefore, improving\nits efficiency is an active topic of ongoing research. A popular strategy is\nthe use of low bit-width integers to approximate the original entries in a\nmatrix. This allows efficiency gains, but often requires sophisticated\ntechniques to control the rounding error incurred. In this work, we first\nverify/check that when the low bit-width restriction is removed, for a variety\nof Transformer-based models, whether integers are sufficient for all GEMMs need\n-- for {\\em both} training and inference stages, and can achieve parity with\nfloating point counterparts. No sophisticated techniques are needed. We find\nthat while a large majority of entries in matrices (encountered in such models)\ncan be easily represented by {\\em low} bit-width integers, the existence of a\nfew heavy hitter entries make it difficult to achieve efficiency gains via the\nexclusive use of low bit-width GEMMs alone. To address this issue, we develop a\nsimple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\em unpack} a\nmatrix with large integer entries into a larger matrix whose entries all lie\nwithin the representable range of arbitrarily low bit-width integers. This\nallows {\\em equivalence} with the original GEMM, i.e., the exact result can be\nobtained using purely low bit-width integer GEMMs. This comes at the cost of\nadditional operations -- we show that for many popular models, this overhead is\nquite small.", "published": "2024-03-12 05:44:27", "link": "http://arxiv.org/abs/2403.07339v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark", "abstract": "Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\nhttps://github.com/VLKEB/VLKEB.", "published": "2024-03-12 06:16:33", "link": "http://arxiv.org/abs/2403.07350v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning\n  Disentangled Reasoning", "abstract": "Vision-and-Language Navigation (VLN), as a crucial research problem of\nEmbodied AI, requires an embodied agent to navigate through complex 3D\nenvironments following natural language instructions. Recent research has\nhighlighted the promising capacity of large language models (LLMs) in VLN by\nimproving navigational reasoning accuracy and interpretability. However, their\npredominant use in an offline manner usually suffers from substantial domain\ngap between the VLN task and the LLM training corpus. This paper introduces a\nnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\nparameter-efficient in-domain training to enable self-guided navigational\ndecision, leading to a significant mitigation of the domain gap in a\ncost-effective manner. Specifically, at each timestep, the LLM is prompted to\nforecast the navigational chain-of-thought by: 1) acting as a world model to\nimagine the next observation according to the instruction, 2) selecting the\ncandidate observation that best aligns with the imagination, and 3) determining\nthe action based on the reasoning from the prior steps. Through constructing\nformalized labels for training, the LLM can learn to generate desired and\nreasonable chain-of-thought outputs for improving the action decision.\nExperimental results across various training settings and popular VLN\nbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\n(R4R)) show the significant superiority of NavCoT over the direct action\nprediction variants. Through simple parameter-efficient finetuning, our NavCoT\noutperforms a recent GPT4-based approach with ~7% relative improvement on the\nR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\nscalable LLM-based embodied agents, which are helpful for developing real-world\nrobotics applications. Code is available at\nhttps://github.com/expectorlin/NavCoT.", "published": "2024-03-12 07:27:02", "link": "http://arxiv.org/abs/2403.07376v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Hallmarks of Optimization Trajectories in Neural Networks: Directional\n  Exploration and Redundancy", "abstract": "We propose a fresh take on understanding the mechanisms of neural networks by\nanalyzing the rich directional structure of optimization trajectories,\nrepresented by their pointwise parameters. Towards this end, we introduce some\nnatural notions of the complexity of optimization trajectories, both\nqualitative and quantitative, which hallmark the directional nature of\noptimization in neural networks: when is there redundancy, and when\nexploration. We use them to reveal the inherent nuance and interplay involved\nbetween various optimization choices, such as momentum and weight decay.\nFurther, the trajectory perspective helps us see the effect of scale on\nregularizing the directional nature of trajectories, and as a by-product, we\nalso observe an intriguing heterogeneity of Q,K,V dynamics in the middle\nattention layers in LLMs and which is homogenized by scale. Importantly, we put\nthe significant directional redundancy observed to the test by demonstrating\nthat training only scalar batchnorm parameters some while into training matches\nthe performance of training the entire network, which thus exhibits the\npotential of hybrid optimization schemes that are geared towards efficiency.", "published": "2024-03-12 07:32:47", "link": "http://arxiv.org/abs/2403.07379v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large\n  Language Models by Summarizing Training Trajectories of Small Models", "abstract": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.", "published": "2024-03-12 07:45:33", "link": "http://arxiv.org/abs/2403.07384v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost", "abstract": "Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most of this data\ncomes from Western countries, leading to poor results for underrepresented\ncountries. To address this issue, more data needs to be collected from these\ncountries, but the cost of annotation can be a significant bottleneck. In this\npaper, we propose methods to identify the data to be annotated to balance model\nperformance and annotation costs. Our approach first involves finding the\ncountries with images of topics (objects and actions) most visually distinct\nfrom those already in the training datasets used by current large\nvision-language foundation models. Next, we identify countries with higher\nvisual similarity for these topics and show that using data from these\ncountries to supplement the training data improves model performance and\nreduces annotation costs. The resulting lists of countries and corresponding\ntopics are made available at\nhttps://github.com/MichiganNLP/visual_diversity_budget.", "published": "2024-03-12 14:27:17", "link": "http://arxiv.org/abs/2403.07687v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Transforming Competition into Collaboration: The Revolutionary Role of\n  Multi-Agent Systems and Language Models in Modern Organizations", "abstract": "This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.", "published": "2024-03-12 15:56:10", "link": "http://arxiv.org/abs/2403.07769v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.MA"], "primary_category": "cs.AI"}
{"title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large\n  Language Models for Code", "abstract": "Large Language Models (LLMs) applied to code-related applications have\nemerged as a prominent field, attracting significant interest from both\nacademia and industry. However, as new and improved LLMs are developed,\nexisting evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient\nfor assessing their capabilities. In this work, we propose LiveCodeBench, a\ncomprehensive and contamination-free evaluation of LLMs for code, which\ncontinuously collects new problems over time from contests across three\ncompetition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our\nbenchmark also focuses on a broader range of code related capabilities, such as\nself-repair, code execution, and test output prediction, beyond just code\ngeneration. Currently, LiveCodeBench hosts four hundred high-quality coding\nproblems that were published between May 2023 and May 2024. We have evaluated\n18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present\nempirical findings on contamination, holistic performance comparisons,\npotential overfitting in existing benchmarks as well as individual model\ncomparisons. We will release all prompts and model completions for further\ncommunity analysis, along with a general toolkit for adding new scenarios and\nmodel", "published": "2024-03-12 17:58:04", "link": "http://arxiv.org/abs/2403.07974v2", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Leveraging LLMs for On-the-Fly Instruction Guided Image Editing", "abstract": "The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset.", "published": "2024-03-12 18:12:50", "link": "http://arxiv.org/abs/2403.08004v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Gujarati-English Code-Switching Speech Recognition using ensemble\n  prediction of spoken language", "abstract": "An important and difficult task in code-switched speech recognition is to\nrecognize the language, as lots of words in two languages can sound similar,\nespecially in some accents. We focus on improving performance of end-to-end\nAutomatic Speech Recognition models by conditioning transformer layers on\nlanguage ID of words and character in the output in an per layer supervised\nmanner. To this end, we propose two methods of introducing language specific\nparameters and explainability in the multi-head attention mechanism, and\nimplement a Temporal Loss that helps maintain continuity in input alignment.\nDespite being unable to reduce WER significantly, our method shows promise in\npredicting the correct language from just spoken data. We introduce\nregularization in the language prediction by dropping LID in the sequence,\nwhich helps align long repeated output sequences.", "published": "2024-03-12 18:21:20", "link": "http://arxiv.org/abs/2403.08011v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FluoroSAM: A Language-aligned Foundation Model for X-ray Image\n  Segmentation", "abstract": "Automated X-ray image segmentation would accelerate research and development\nin diagnostic and interventional precision medicine. Prior efforts have\ncontributed task-specific models capable of solving specific image analysis\nproblems, but the utility of these models is restricted to their particular\ntask domain, and expanding to broader use requires additional data, labels, and\nretraining efforts. Recently, foundation models (FMs) -- machine learning\nmodels trained on large amounts of highly variable data thus enabling broad\napplicability -- have emerged as promising tools for automated image analysis.\nExisting FMs for medical image analysis focus on scenarios and modalities where\nobjects are clearly defined by visually apparent boundaries, such as surgical\ntool segmentation in endoscopy. X-ray imaging, by contrast, does not generally\noffer such clearly delineated boundaries or structure priors. During X-ray\nimage formation, complex 3D structures are projected in transmission onto the\nimaging plane, resulting in overlapping features of varying opacity and shape.\nTo pave the way toward an FM for comprehensive and automated analysis of\narbitrary medical X-ray images, we develop FluoroSAM, a language-aligned\nvariant of the Segment-Anything Model, trained from scratch on 1.6M synthetic\nX-ray images. FluoroSAM is trained on data including masks for 128 organ types\nand 464 non-anatomical objects, such as tools and implants. In real X-ray\nimages of cadaveric specimens, FluoroSAM is able to segment bony anatomical\nstructures based on text-only prompting with 0.51 and 0.79 DICE with\npoint-based refinement, outperforming competing SAM variants for all\nstructures. FluoroSAM is also capable of zero-shot generalization to segmenting\nclasses beyond the training set thanks to its language alignment, which we\ndemonstrate for full lung segmentation on real chest X-rays.", "published": "2024-03-12 20:11:38", "link": "http://arxiv.org/abs/2403.08059v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Mechanics of Next Token Prediction with Self-Attention", "abstract": "Transformer-based language models are trained on large datasets to predict\nthe next token given an input sequence. Despite this simple training objective,\nthey have led to revolutionary advances in natural language processing.\nUnderlying this success is the self-attention mechanism. In this work, we ask:\n$\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$\n$\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$\n$\\textit{next-token}$ $\\textit{prediction?}$ We show that training\nself-attention with gradient descent learns an automaton which generates the\nnext token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$\n$\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects\nthe $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with\nthe last input token. $\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It\nthen creates a convex combination of the high-priority tokens from which the\nnext token can be sampled. Under suitable conditions, we rigorously\ncharacterize these mechanics through a directed graph over tokens extracted\nfrom the training data. We prove that gradient descent implicitly discovers the\nstrongly-connected components (SCC) of this graph and self-attention learns to\nretrieve the tokens that belong to the highest-priority SCC available in the\ncontext window. Our theory relies on decomposing the model weights into a\ndirectional component and a finite component that correspond to hard retrieval\nand soft composition steps respectively. This also formalizes a related\nimplicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that\nthese findings shed light on how self-attention processes sequential data and\npave the path toward demystifying more complex architectures.", "published": "2024-03-12 21:15:38", "link": "http://arxiv.org/abs/2403.08081v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "AI-Assisted Causal Pathway Diagram for Human-Centered Design", "abstract": "This paper explores the integration of causal pathway diagrams (CPD) into\nhuman-centered design (HCD), investigating how these diagrams can enhance the\nearly stages of the design process. A dedicated CPD plugin for the online\ncollaborative whiteboard platform Miro was developed to streamline diagram\ncreation and offer real-time AI-driven guidance. Through a user study with\ndesigners (N=20), we found that CPD's branching and its emphasis on causal\nconnections supported both divergent and convergent processes during design.\nCPD can also facilitate communication among stakeholders. Additionally, we\nfound our plugin significantly reduces designers' cognitive workload and\nincreases their creativity during brainstorming, highlighting the implications\nof AI-assisted tools in supporting creative work and evidence-based designs.", "published": "2024-03-12 22:36:27", "link": "http://arxiv.org/abs/2403.08111v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Legally Binding but Unfair? Towards Assessing Fairness of Privacy\n  Policies", "abstract": "Privacy policies are expected to inform data subjects about their data\nprotection rights and should explain the data controller's data management\npractices. Privacy policies only fulfill their purpose, if they are correctly\ninterpreted, understood, and trusted by the data subject. This implies that a\nprivacy policy is written in a fair way, e.g., it does not use polarizing\nterms, does not require a certain education, or does not assume a particular\nsocial background. We outline our approach to assessing fairness in privacy\npolicies. We identify from fundamental legal sources and fairness research, how\nthe dimensions informational fairness, representational fairness and ethics /\nmorality are related to privacy policies. We propose options to automatically\nassess policies in these fairness dimensions, based on text statistics,\nlinguistic methods and artificial intelligence. We conduct initial experiments\nwith German privacy policies to provide evidence that our approach is\napplicable. Our experiments indicate that there are issues in all three\ndimensions of fairness. This is important, as future privacy policies may be\nused in a corpus for legal artificial intelligence models.", "published": "2024-03-12 22:53:32", "link": "http://arxiv.org/abs/2403.08115v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "K.4.m"], "primary_category": "cs.CY"}
{"title": "From Paper to Card: Transforming Design Implications with Generative AI", "abstract": "Communicating design implications is common within the HCI community when\npublishing academic papers, yet these papers are rarely read and used by\ndesigners. One solution is to use design cards as a form of translational\nresource that communicates valuable insights from papers in a more digestible\nand accessible format to assist in design processes. However, creating design\ncards can be time-consuming, and authors may lack the resources/know-how to\nproduce cards. Through an iterative design process, we built a system that\nhelps create design cards from academic papers using an LLM and text-to-image\nmodel. Our evaluation with designers (N=21) and authors of selected papers\n(N=12) revealed that designers perceived the design implications from our\ndesign cards as more inspiring and generative, compared to reading original\npaper texts, and the authors viewed our system as an effective way of\ncommunicating their design implications. We also propose future enhancements\nfor AI-generated design cards.", "published": "2024-03-12 23:47:28", "link": "http://arxiv.org/abs/2403.08137v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.2; I.2.7"], "primary_category": "cs.HC"}
{"title": "Enhancing Depression-Diagnosis-Oriented Chat with Psychological State\n  Tracking", "abstract": "Depression-diagnosis-oriented chat aims to guide patients in self-expression\nto collect key symptoms for depression detection. Recent work focuses on\ncombining task-oriented dialogue and chitchat to simulate the interview-based\ndepression diagnosis. Whereas, these methods can not well capture the changing\ninformation, feelings, or symptoms of the patient during dialogues. Moreover,\nno explicit framework has been explored to guide the dialogue, which results in\nsome useless communications that affect the experience. In this paper, we\npropose to integrate Psychological State Tracking (POST) within the large\nlanguage model (LLM) to explicitly guide depression-diagnosis-oriented chat.\nSpecifically, the state is adapted from a psychological theoretical model,\nwhich consists of four components, namely Stage, Information, Summary and Next.\nWe fine-tune an LLM model to generate the dynamic psychological state, which is\nfurther used to assist response generation at each turn to simulate the\npsychiatrist. Experimental results on the existing benchmark show that our\nproposed method boosts the performance of all subtasks in\ndepression-diagnosis-oriented chat.", "published": "2024-03-12 07:17:01", "link": "http://arxiv.org/abs/2403.09717v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
{"title": "ClaimVer: Explainable Claim-Level Verification and Evidence Attribution\n  of Text Through Knowledge Graphs", "abstract": "In the midst of widespread misinformation and disinformation through social\nmedia and the proliferation of AI-generated texts, it has become increasingly\ndifficult for people to validate and trust information they encounter. Many\nfact-checking approaches and tools have been developed, but they often lack\nappropriate explainability or granularity to be useful in various contexts. A\ntext validation method that is easy to use, accessible, and can perform\nfine-grained evidence attribution has become crucial. More importantly,\nbuilding user trust in such a method requires presenting the rationale behind\neach prediction, as research shows this significantly influences people's\nbelief in automated systems. Localizing and bringing users' attention to the\nspecific problematic content is also paramount, instead of providing simple\nblanket labels. In this paper, we present ClaimVer, a human-centric framework\ntailored to meet users' informational and verification needs by generating rich\nannotations and thereby reducing cognitive load. Designed to deliver\ncomprehensive evaluations of texts, it highlights each claim, verifies it\nagainst a trusted knowledge graph (KG), presents the evidence, and provides\nsuccinct, clear explanations for each claim prediction. Finally, our framework\nintroduces an attribution score, enhancing applicability across a wide range of\ndownstream tasks.", "published": "2024-03-12 17:07:53", "link": "http://arxiv.org/abs/2403.09724v4", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the performance of Retrieval-Augmented Generation and\n  fine-tuning for the development of AI-driven knowledge-based systems", "abstract": "The development of generative large language models (G-LLM) opened up new\nopportunities for the development of new types of knowledge-based systems\nsimilar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented\nGeneration (RAG) are the techniques that can be used to implement domain\nadaptation for the development of G-LLM-based knowledge systems. In our study,\nusing ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine\nthe performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2\nlanguage models. Based on measurements shown on different datasets, we\ndemonstrate that RAG-based constructions are more efficient than models\nproduced with FN. We point out that connecting RAG and FN is not trivial,\nbecause connecting FN models with RAG can cause a decrease in performance.\nFurthermore, we outline a simple RAG-based architecture which, on average,\noutperforms the FN models by 16% in terms of the ROGUE score, 15% in the case\nof the BLEU score, and 53% based on the cosine similarity. This shows the\nsignificant advantage of RAG over FN in terms of hallucination, which is not\noffset by the fact that the average 8% better METEOR score of FN models\nindicates greater creativity compared to RAG.", "published": "2024-03-12 21:06:31", "link": "http://arxiv.org/abs/2403.09727v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simulating Weighted Automata over Sequences and Trees with Transformers", "abstract": "Transformers are ubiquitous models in the natural language processing (NLP)\ncommunity and have shown impressive empirical successes in the past few years.\nHowever, little is understood about how they reason and the limits of their\ncomputational capabilities. These models do not process data sequentially, and\nyet outperform sequential neural models such as RNNs. Recent work has shown\nthat these models can compactly simulate the sequential reasoning abilities of\ndeterministic finite automata (DFAs). This leads to the following question: can\ntransformers simulate the reasoning of more complex finite state machines? In\nthis work, we show that transformers can simulate weighted finite automata\n(WFAs), a class of models which subsumes DFAs, as well as weighted tree\nautomata (WTA), a generalization of weighted automata to tree structured\ninputs. We prove these claims formally and provide upper bounds on the sizes of\nthe transformer models needed as a function of the number of states the target\nautomata. Empirically, we perform synthetic experiments showing that\ntransformers are able to learn these compact solutions via standard\ngradient-based training.", "published": "2024-03-12 21:54:34", "link": "http://arxiv.org/abs/2403.09728v1", "categories": ["cs.CL", "cs.AI", "cs.CC"], "primary_category": "cs.CL"}
{"title": "Duwak: Dual Watermarks in Large Language Models", "abstract": "As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.", "published": "2024-03-12 16:25:38", "link": "http://arxiv.org/abs/2403.13000v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Stress index strategy enhanced with financial news sentiment analysis\n  for the equity markets", "abstract": "This paper introduces a new risk-on risk-off strategy for the stock market,\nwhich combines a financial stress indicator with a sentiment analysis done by\nChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of\nmarket stress derived from volatility and credit spreads are enhanced when\ncombined with the financial news sentiment derived from GPT-4. As a result, the\nstrategy shows improved performance, evidenced by higher Sharpe ratio and\nreduced maximum drawdowns. The improved performance is consistent across the\nNASDAQ, the S&P 500 and the six major equity markets, indicating that the\nmethod generalises across equities markets.", "published": "2024-03-12 08:23:30", "link": "http://arxiv.org/abs/2404.00012v1", "categories": ["q-fin.ST", "cs.AI", "cs.CL", "q-fin.RM"], "primary_category": "q-fin.ST"}
{"title": "CodeAttack: Revealing Safety Generalization Challenges of Large Language\n  Models via Code Completion", "abstract": "The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable generative capabilities but also raised concerns about their\npotential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\nnew and universal safety vulnerability of these models against code input:\nCodeAttack bypasses the safety guardrails of all models more than 80\\% of the\ntime. We find that a larger distribution gap between CodeAttack and natural\nlanguage leads to weaker safety generalization, such as encoding natural\nlanguage input with data structures. Furthermore, we give our hypotheses about\nthe success of CodeAttack: the misaligned bias acquired by LLMs during code\ntraining, prioritizing code completion over avoiding the potential safety risk.\nFinally, we analyze potential mitigation measures. These findings highlight new\nsafety risks in the code domain and the need for more robust safety alignment\nalgorithms to match the code capabilities of LLMs.", "published": "2024-03-12 17:55:38", "link": "http://arxiv.org/abs/2403.07865v5", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "On HRTF Notch Frequency Prediction Using Anthropometric Features and\n  Neural Networks", "abstract": "High fidelity spatial audio often performs better when produced using a\npersonalized head-related transfer function (HRTF). However, the direct\nacquisition of HRTFs is cumbersome and requires specialized equipment. Thus,\nmany personalization methods estimate HRTF features from easily obtained\nanthropometric features of the pinna, head, and torso. The first HRTF notch\nfrequency (N1) is known to be a dominant feature in elevation localization, and\nthus a useful feature for HRTF personalization. This paper describes the\nprediction of N1 frequency from pinna anthropometry using a neural model.\nPrediction is performed separately on three databases, both simulated and\nmeasured, and then by domain mixing in-between the databases. The model\nsuccessfully predicts N1 frequency for individual databases and by domain\nmixing between some databases. Prediction errors are better or comparable to\nthose previously reported, showing significant improvement when acquired over a\nlarge database and with a larger output range.", "published": "2024-03-12 12:07:56", "link": "http://arxiv.org/abs/2403.07579v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Gender-ambiguous voice generation through feminine speaking style\n  transfer in male voices", "abstract": "Recently, and under the umbrella of Responsible AI, efforts have been made to\ndevelop gender-ambiguous synthetic speech to represent with a single voice all\nindividuals in the gender spectrum. However, research efforts have completely\noverlooked the speaking style despite differences found among binary and\nnon-binary populations. In this work, we synthesise gender-ambiguous speech by\ncombining the timbre of a male speaker with the manner of speech of a female\nspeaker using voice morphing and pitch shifting towards the male-female\nboundary. Subjective evaluations indicate that the ambiguity of the morphed\nsamples that convey the female speech style is higher than those that undergo\nplain pitch transformations suggesting that the speaking style can be a\ncontributing factor in creating gender-ambiguous speech. To our knowledge, this\nis the first study that explicitly uses the transfer of the speaking style to\ncreate gender-ambiguous voices.", "published": "2024-03-12 13:50:58", "link": "http://arxiv.org/abs/2403.07661v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multichannel Long-Term Streaming Neural Speech Enhancement for Static\n  and Moving Speakers", "abstract": "In this work, we extend our previously proposed offline SpatialNet for\nlong-term streaming multichannel speech enhancement in both static and moving\nspeaker scenarios. SpatialNet exploits spatial information, such as the\nspatial/steering direction of speech, for discriminating between target speech\nand interferences, and achieved outstanding performance. The core of SpatialNet\nis a narrow-band self-attention module used for learning the temporal dynamic\nof spatial vectors. Towards long-term streaming speech enhancement, we propose\nto replace the offline self-attention network with online networks that have\nlinear inference complexity w.r.t signal length and meanwhile maintain the\ncapability of learning long-term information. Three variants are developed\nbased on (i) masked self-attention, (ii) Retention, a self-attention variant\nwith linear inference complexity, and (iii) Mamba, a\nstructured-state-space-based RNN-like network. Moreover, we investigate the\nlength extrapolation ability of different networks, namely test on signals that\nare much longer than training signals, and propose a short-signal training plus\nlong-signal fine-tuning strategy, which largely improves the length\nextrapolation ability of the networks within limited training time. Overall,\nthe proposed online SpatialNet achieves outstanding speech enhancement\nperformance for long audio streams, and for both static and moving speakers.\nThe proposed method is open-sourced in https://github.com/Audio-WestlakeU/NBSS.", "published": "2024-03-12 14:11:29", "link": "http://arxiv.org/abs/2403.07675v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Two-sided Acoustic Metascreen for Broadband and Individual Reflection\n  and Transmission Control", "abstract": "Acoustic wave modulation plays a pivotal role in various applications,\nincluding sound-field reconstruction, wireless communication, and particle\nmanipulation, among others. However, current acoustic metamaterial and\nmetasurface designs typically focus on controlling either reflection or\ntransmission waves, often overlooking the coupling between amplitude and phase\nof acoustic waves. To fulfill this gap, we propose and experimentally validate\na design enabling complete control of reflected and transmitted acoustic waves\nindividually across a frequency range of 4 kHz to 8 kHz, allowing arbitrary\ncombinations of amplitude and phase for reflected and transmitted sound in a\nbroadband manner. Additionally, we demonstrate the significance of our approach\nfor sound manipulation by achieving acoustic diffusion, reflection, focusing,\nand generating a two-sided 3D hologram at three distinct frequencies. These\nfindings open an alternative avenue for extensively engineering sound waves,\npromising applications in acoustics and related fields.", "published": "2024-03-12 16:46:57", "link": "http://arxiv.org/abs/2403.10548v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech\n  Recognition Datasets", "abstract": "Paralinguistic traits like cognitive load and emotion are increasingly\nrecognized as pivotal areas in speech recognition research, often examined\nthrough specialized datasets like CLSE and IEMOCAP. However, the integrity of\nthese datasets is seldom scrutinized for text-dependency. This paper critically\nevaluates the prevalent assumption that machine learning models trained on such\ndatasets genuinely learn to identify paralinguistic traits, rather than merely\ncapturing lexical features. By examining the lexical overlap in these datasets\nand testing the performance of machine learning models, we expose significant\ntext-dependency in trait-labeling. Our results suggest that some machine\nlearning models, especially large pre-trained models like HuBERT, might\ninadvertently focus on lexical characteristics rather than the intended\nparalinguistic features. The study serves as a call to action for the research\ncommunity to reevaluate the reliability of existing datasets and methodologies,\nensuring that machine learning models genuinely learn what they are designed to\nrecognize.", "published": "2024-03-12 15:54:32", "link": "http://arxiv.org/abs/2403.07767v2", "categories": ["eess.AS", "cs.LG", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Boosting keyword spotting through on-device learnable user speech\n  characteristics", "abstract": "Keyword spotting systems for always-on TinyML-constrained applications\nrequire on-site tuning to boost the accuracy of offline trained classifiers\nwhen deployed in unseen inference conditions. Adapting to the speech\npeculiarities of target users requires many in-domain samples, often\nunavailable in real-world scenarios. Furthermore, current on-device learning\ntechniques rely on computationally intensive and memory-hungry backbone update\nschemes, unfit for always-on, battery-powered devices. In this work, we propose\na novel on-device learning architecture, composed of a pretrained backbone and\na user-aware embedding learning the user's speech characteristics. The\nso-generated features are fused and used to classify the input utterance. For\ndomain shifts generated by unseen speakers, we measure error rate reductions of\nup to 19% from 30.1% to 24.3% based on the 35-class problem of the Google\nSpeech Commands dataset, through the inexpensive update of the user\nprojections. We moreover demonstrate the few-shot learning capabilities of our\nproposed architecture in sample- and class-scarce learning conditions. With\n23.7 kparameters and 1 MFLOP per epoch required for on-device training, our\nsystem is feasible for TinyML applications aimed at battery-powered\nmicrocontrollers.", "published": "2024-03-12 16:41:31", "link": "http://arxiv.org/abs/2403.07802v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic\n  Music Generation", "abstract": "Modelling musical structure is vital yet challenging for artificial\nintelligence systems that generate symbolic music compositions. This literature\nreview dissects the evolution of techniques for incorporating coherent\nstructure, from symbolic approaches to foundational and transformative deep\nlearning methods that harness the power of computation and data across a wide\nvariety of training paradigms. In the later stages, we review an emerging\ntechnique which we refer to as \"sub-task decomposition\" that involves\ndecomposing music generation into separate high-level structural planning and\ncontent creation stages. Such systems incorporate some form of musical\nknowledge or neuro-symbolic methods by extracting melodic skeletons or\nstructural templates to guide the generation. Progress is evident in capturing\nmotifs and repetitions across all three eras reviewed, yet modelling the\nnuanced development of themes across extended compositions in the style of\nhuman composers remains difficult. We outline several key future directions to\nrealize the synergistic benefits of combining approaches from all eras\nexamined.", "published": "2024-03-12 18:03:08", "link": "http://arxiv.org/abs/2403.07995v1", "categories": ["cs.SD", "cs.LG", "cs.SC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On-Device Domain Learning for Keyword Spotting on Low-Power Extreme Edge\n  Embedded Systems", "abstract": "Keyword spotting accuracy degrades when neural networks are exposed to noisy\nenvironments. On-site adaptation to previously unseen noise is crucial to\nrecovering accuracy loss, and on-device learning is required to ensure that the\nadaptation process happens entirely on the edge device. In this work, we\npropose a fully on-device domain adaptation system achieving up to 14% accuracy\ngains over already-robust keyword spotting models. We enable on-device learning\nwith less than 10 kB of memory, using only 100 labeled utterances to recover 5%\naccuracy after adapting to the complex speech noise. We demonstrate that domain\nadaptation can be achieved on ultra-low-power microcontrollers with as little\nas 806 mJ in only 14 s on always-on, battery-operated devices.", "published": "2024-03-12 19:54:35", "link": "http://arxiv.org/abs/2403.10549v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
