{"title": "Contrastive Learning with Adversarial Perturbations for Conditional Text\n  Generation", "abstract": "Recently, sequence-to-sequence (seq2seq) models with the Transformer\narchitecture have achieved remarkable performance on various conditional text\ngeneration tasks, such as machine translation. However, most of them are\ntrained with teacher forcing with the ground truth label given at each time\nstep, without being exposed to incorrectly generated tokens during training,\nwhich hurts its generalization to unseen inputs, that is known as the \"exposure\nbias\" problem. In this work, we propose to mitigate the conditional text\ngeneration problem by contrasting positive pairs with negative pairs, such that\nthe model is exposed to various valid or incorrect perturbations of the inputs,\nfor improved generalization. However, training the model with naive contrastive\nlearning framework using random non-target sequences as negative examples is\nsuboptimal, since they are easily distinguishable from the correct output,\nespecially so with models pretrained with large text corpora. Also, generating\npositive examples requires domain-specific augmentation heuristics which may\nnot generalize over diverse domains. To tackle this problem, we propose a\nprincipled method to generate positive and negative samples for contrastive\nlearning of seq2seq models. Specifically, we generate negative examples by\nadding small perturbations to the input sequence to minimize its conditional\nlikelihood, and positive examples by adding large perturbations while enforcing\nit to have a high conditional likelihood. Such \"hard\" positive and negative\npairs generated using our method guides the model to better distinguish correct\noutputs from incorrect ones. We empirically show that our proposed method\nsignificantly improves the generalization of the seq2seq on three text\ngeneration tasks - machine translation, text summarization, and question\ngeneration.", "published": "2020-12-14 06:20:27", "link": "http://arxiv.org/abs/2012.07280v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Summarization for Chat Logs with Topic-Oriented Ranking and\n  Context-Aware Auto-Encoders", "abstract": "Automatic chat summarization can help people quickly grasp important\ninformation from numerous chat messages. Unlike conventional documents, chat\nlogs usually have fragmented and evolving topics. In addition, these logs\ncontain a quantity of elliptical and interrogative sentences, which make the\nchat summarization highly context dependent. In this work, we propose a novel\nunsupervised framework called RankAE to perform chat summarization without\nemploying manually labeled data. RankAE consists of a topic-oriented ranking\nstrategy that selects topic utterances according to centrality and diversity\nsimultaneously, as well as a denoising auto-encoder that is carefully designed\nto generate succinct but context-informative summaries based on the selected\nutterances. To evaluate the proposed method, we collect a large-scale dataset\nof chat logs from a customer service environment and build an annotated set\nonly for model evaluation. Experimental results show that RankAE significantly\noutperforms other unsupervised methods and is able to generate high-quality\nsummaries in terms of relevance and topic coverage.", "published": "2020-12-14 07:31:17", "link": "http://arxiv.org/abs/2012.07300v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic-Oriented Spoken Dialogue Summarization for Customer Service with\n  Saliency-Aware Topic Modeling", "abstract": "In a customer service system, dialogue summarization can boost service\nefficiency by automatically creating summaries for long spoken dialogues in\nwhich customers and agents try to address issues about specific topics. In this\nwork, we focus on topic-oriented dialogue summarization, which generates highly\nabstractive summaries that preserve the main ideas from dialogues. In spoken\ndialogues, abundant dialogue noise and common semantics could obscure the\nunderlying informative content, making the general topic modeling approaches\ndifficult to apply. In addition, for customer service, role-specific\ninformation matters and is an indispensable part of a summary. To effectively\nperform topic modeling on dialogues and capture multi-role information, in this\nwork we propose a novel topic-augmented two-stage dialogue summarizer (TDS)\njointly with a saliency-aware neural topic model (SATM) for topic-oriented\nsummarization of customer service dialogues. Comprehensive studies on a\nreal-world Chinese customer service dataset demonstrated the superiority of our\nmethod against several strong baselines.", "published": "2020-12-14 07:50:25", "link": "http://arxiv.org/abs/2012.07311v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Math Word Problems from Equations with Topic Controlling and\n  Commonsense Enforcement", "abstract": "Recent years have seen significant advancement in text generation tasks with\nthe help of neural language models. However, there exists a challenging task:\ngenerating math problem text based on mathematical equations, which has made\nlittle progress so far. In this paper, we present a novel equation-to-problem\ntext generation model. In our model, 1) we propose a flexible scheme to\neffectively encode math equations, we then enhance the equation encoder by a\nVaritional Autoen-coder (VAE) 2) given a math equation, we perform topic\nselection, followed by which a dynamic topic memory mechanism is introduced to\nrestrict the topic distribution of the generator 3) to avoid commonsense\nviolation in traditional generation model, we pretrain word embedding with\nbackground knowledge graph (KG), and we link decoded words to related words in\nKG, targeted at injecting background knowledge into our model. We evaluate our\nmodel through both automatic metrices and human evaluation, experiments\ndemonstrate our model outperforms baseline and previous models in both accuracy\nand richness of generated problem text.", "published": "2020-12-14 10:02:11", "link": "http://arxiv.org/abs/2012.07379v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning in Dialog: Improving Response Generation by Context Reading\n  Comprehension", "abstract": "In multi-turn dialog, utterances do not always take the full form of\nsentences \\cite{Carbonell1983DiscoursePA}, which naturally makes understanding\nthe dialog context more difficult. However, it is essential to fully grasp the\ndialog context to generate a reasonable response. Hence, in this paper, we\npropose to improve the response generation performance by examining the model's\nability to answer a reading comprehension question, where the question is\nfocused on the omitted information in the dialog. Enlightened by the multi-task\nlearning scheme, we propose a joint framework that unifies these two tasks,\nsharing the same encoder to extract the common and task-invariant features with\ndifferent decoders to learn task-specific features. To better fusing\ninformation from the question and the dialog history in the encoding part, we\npropose to augment the Transformer architecture with a memory updater, which is\ndesigned to selectively store and update the history dialog information so as\nto support downstream tasks. For the experiment, we employ human annotators to\nwrite and examine a large-scale dialog reading comprehension dataset. Extensive\nexperiments are conducted on this dataset, and the results show that the\nproposed model brings substantial improvements over several strong baselines on\nboth tasks. In this way, we demonstrate that reasoning can indeed help better\nresponse generation and vice versa. We release our large-scale dataset for\nfurther research.", "published": "2020-12-14 10:58:01", "link": "http://arxiv.org/abs/2012.07410v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Style-Content Duality of Attractiveness: Learning to Write\n  Eye-Catching Headlines via Disentanglement", "abstract": "Eye-catching headlines function as the first device to trigger more clicks,\nbringing reciprocal effect between producers and viewers. Producers can obtain\nmore traffic and profits, and readers can have access to outstanding articles.\nWhen generating attractive headlines, it is important to not only capture the\nattractive content but also follow an eye-catching written style. In this\npaper, we propose a Disentanglement-based Attractive Headline Generator (DAHG)\nthat generates headline which captures the attractive content following the\nattractive style. Concretely, we first devise a disentanglement module to\ndivide the style and content of an attractive prototype headline into latent\nspaces, with two auxiliary constraints to ensure the two spaces are indeed\ndisentangled. The latent content information is then used to further polish the\ndocument representation and help capture the salient part. Finally, the\ngenerator takes the polished document as input to generate headline under the\nguidance of the attractive style. Extensive experiments on the public Kuaibao\ndataset show that DAHG achieves state-of-the-art performance. Human evaluation\nalso demonstrates that DAHG triggers 22% more clicks than existing models.", "published": "2020-12-14 11:11:43", "link": "http://arxiv.org/abs/2012.07419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Opinion Summarization with Content Planning", "abstract": "The recent success of deep learning techniques for abstractive summarization\nis predicated on the availability of large-scale datasets. When summarizing\nreviews (e.g., for products or movies), such training data is neither available\nnor can be easily sourced, motivating the development of methods which rely on\nsynthetic datasets for supervised training. We show that explicitly\nincorporating content planning in a summarization model not only yields output\nof higher quality, but also allows the creation of synthetic datasets which are\nmore natural, resembling real world document-summary pairs. Our content plans\ntake the form of aspect and sentiment distributions which we induce from data\nwithout access to expensive annotations. Synthetic datasets are created by\nsampling pseudo-reviews from a Dirichlet distribution parametrized by our\ncontent planner, while our model generates summaries based on input reviews and\ninduced content plans. Experimental results on three domains show that our\napproach outperforms competitive models in generating informative, coherent,\nand fluent summaries that capture opinion consensus.", "published": "2020-12-14 18:41:58", "link": "http://arxiv.org/abs/2012.07808v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Choices Influence Attributive Word Associations: A Semi-supervised\n  Analysis of Static Word Embeddings", "abstract": "Static word embeddings encode word associations, extensively utilized in\ndownstream NLP tasks. Although prior studies have discussed the nature of such\nword associations in terms of biases and lexical regularities captured, the\nvariation in word associations based on the embedding training procedure\nremains in obscurity. This work aims to address this gap by assessing\nattributive word associations across five different static word embedding\narchitectures, analyzing the impact of the choice of the model architecture,\ncontext learning flavor and training corpora. Our approach utilizes a\nsemi-supervised clustering method to cluster annotated proper nouns and\nadjectives, based on their word embedding features, revealing underlying\nattributive word associations formed in the embedding space, without\nintroducing any confirmation bias. Our results reveal that the choice of the\ncontext learning flavor during embedding training (CBOW vs skip-gram) impacts\nthe word association distinguishability and word embeddings' sensitivity to\ndeviations in the training corpora. Moreover, it is empirically shown that even\nwhen trained over the same corpora, there is significant inter-model disparity\nand intra-model similarity in the encoded word associations across different\nword embedding models, portraying specific patterns in the way the embedding\nspace is created for each embedding architecture.", "published": "2020-12-14 22:27:18", "link": "http://arxiv.org/abs/2012.07978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Rationalize for Nonmonotonic Reasoning with Distant\n  Supervision", "abstract": "The black-box nature of neural models has motivated a line of research that\naims to generate natural language rationales to explain why a model made\ncertain predictions. Such rationale generation models, to date, have been\ntrained on dataset-specific crowdsourced rationales, but this approach is\ncostly and is not generalizable to new tasks and domains. In this paper, we\ninvestigate the extent to which neural models can reason about natural language\nrationales that explain model predictions, relying only on distant supervision\nwith no additional annotation cost for human-written rationales. We investigate\nmultiple ways to automatically generate rationales using pre-trained language\nmodels, neural knowledge models, and distant supervision from related tasks,\nand train generative models capable of composing explanatory rationales for\nunseen instances. We demonstrate our approach on the defeasible inference task,\na nonmonotonic reasoning task in which an inference may be strengthened or\nweakened when new information (an update) is introduced. Our model shows\npromises at generating post-hoc rationales explaining why an inference is more\nor less likely given the additional information, however, it mostly generates\ntrivial rationales reflecting the fundamental limitations of neural language\nmodels. Conversely, the more realistic setup of jointly predicting the update\nor its type and generating rationale is more challenging, suggesting an\nimportant future direction.", "published": "2020-12-14 23:50:20", "link": "http://arxiv.org/abs/2012.08012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LRC-BERT: Latent-representation Contrastive Knowledge Distillation for\n  Natural Language Understanding", "abstract": "The pre-training models such as BERT have achieved great results in various\nnatural language processing problems. However, a large number of parameters\nneed significant amounts of memory and the consumption of inference time, which\nmakes it difficult to deploy them on edge devices. In this work, we propose a\nknowledge distillation method LRC-BERT based on contrastive learning to fit the\noutput of the intermediate layer from the angular distance aspect, which is not\nconsidered by the existing distillation methods. Furthermore, we introduce a\ngradient perturbation-based training architecture in the training phase to\nincrease the robustness of LRC-BERT, which is the first attempt in knowledge\ndistillation. Additionally, in order to better capture the distribution\ncharacteristics of the intermediate layer, we design a two-stage training\nmethod for the total distillation loss. Finally, by verifying 8 datasets on the\nGeneral Language Understanding Evaluation (GLUE) benchmark, the performance of\nthe proposed LRC-BERT exceeds the existing state-of-the-art methods, which\nproves the effectiveness of our method.", "published": "2020-12-14 08:39:38", "link": "http://arxiv.org/abs/2012.07335v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A comparison of self-supervised speech representations as input features\n  for unsupervised acoustic word embeddings", "abstract": "Many speech processing tasks involve measuring the acoustic similarity\nbetween speech segments. Acoustic word embeddings (AWE) allow for efficient\ncomparisons by mapping speech segments of arbitrary duration to\nfixed-dimensional vectors. For zero-resource speech processing, where\nunlabelled speech is the only available resource, some of the best AWE\napproaches rely on weak top-down constraints in the form of automatically\ndiscovered word-like segments. Rather than learning embeddings at the segment\nlevel, another line of zero-resource research has looked at representation\nlearning at the short-time frame level. Recent approaches include\nself-supervised predictive coding and correspondence autoencoder (CAE) models.\nIn this paper we consider whether these frame-level features are beneficial\nwhen used as inputs for training to an unsupervised AWE model. We compare\nframe-level features from contrastive predictive coding (CPC), autoregressive\npredictive coding and a CAE to conventional MFCCs. These are used as inputs to\na recurrent CAE-based AWE model. In a word discrimination task on English and\nXitsonga data, all three representation learning approaches outperform MFCCs,\nwith CPC consistently showing the biggest improvement. In cross-lingual\nexperiments we find that CPC features trained on English can also be\ntransferred to Xitsonga.", "published": "2020-12-14 10:17:25", "link": "http://arxiv.org/abs/2012.07387v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Towards localisation of keywords in speech using weak supervision", "abstract": "Developments in weakly supervised and self-supervised models could enable\nspeech technology in low-resource settings where full transcriptions are not\navailable. We consider whether keyword localisation is possible using two forms\nof weak supervision where location information is not provided explicitly. In\nthe first, only the presence or absence of a word is indicated, i.e. a\nbag-of-words (BoW) labelling. In the second, visual context is provided in the\nform of an image paired with an unlabelled utterance; a model then needs to be\ntrained in a self-supervised fashion using the paired data. For keyword\nlocalisation, we adapt a saliency-based method typically used in the vision\ndomain. We compare this to an existing technique that performs localisation as\na part of the network architecture. While the saliency-based method is more\nflexible (it can be applied without architectural restrictions), we identify a\ncritical limitation when using it for keyword localisation. Of the two forms of\nsupervision, the visually trained model performs worse than the BoW-trained\nmodel. We show qualitatively that the visually trained model sometimes locate\nsemantically related words, but this is not consistent. While our results show\nthat there is some signal allowing for localisation, it also calls for other\nlocalisation methods better matched to these forms of weak supervision.", "published": "2020-12-14 10:30:51", "link": "http://arxiv.org/abs/2012.07396v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Transfer Learning with Diff Pruning", "abstract": "While task-specific finetuning of pretrained networks has led to significant\nempirical advances in NLP, the large size of networks makes finetuning\ndifficult to deploy in multi-task, memory-constrained settings. We propose diff\npruning as a simple approach to enable parameter-efficient transfer learning\nwithin the pretrain-finetune framework. This approach views finetuning as\nlearning a task-specific diff vector that is applied on top of the pretrained\nparameter vector, which remains fixed and is shared across different tasks. The\ndiff vector is adaptively pruned during training with a differentiable\napproximation to the L0-norm penalty to encourage sparsity. Diff pruning\nbecomes parameter-efficient as the number of tasks increases, as it requires\nstoring only the nonzero positions and weights of the diff vector for each\ntask, while the cost of storing the shared pretrained model remains constant.\nIt further does not require access to all tasks during training, which makes it\nattractive in settings where tasks arrive in stream or the set of tasks is\nunknown. We find that models finetuned with diff pruning can match the\nperformance of fully finetuned baselines on the GLUE benchmark while only\nmodifying 0.5% of the pretrained model's parameters per task.", "published": "2020-12-14 12:34:01", "link": "http://arxiv.org/abs/2012.07463v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Movie Summarization via Sparse Graph Construction", "abstract": "We summarize full-length movies by creating shorter videos containing their\nmost informative scenes. We explore the hypothesis that a summary can be\ncreated by assembling scenes which are turning points (TPs), i.e., key events\nin a movie that describe its storyline. We propose a model that identifies TP\nscenes by building a sparse movie graph that represents relations between\nscenes and is constructed using multimodal information. According to human\njudges, the summaries created by our approach are more informative and\ncomplete, and receive higher ratings, than the outputs of sequence-based models\nand general-purpose summarization algorithms. The induced graphs are\ninterpretable, displaying different topology for different movie genres.", "published": "2020-12-14 13:54:34", "link": "http://arxiv.org/abs/2012.07536v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Towards unsupervised phone and word segmentation using self-supervised\n  vector-quantized neural networks", "abstract": "We investigate segmenting and clustering speech into low-bitrate phone-like\nsequences without supervision. We specifically constrain pretrained\nself-supervised vector-quantized (VQ) neural networks so that blocks of\ncontiguous feature vectors are assigned to the same code, thereby giving a\nvariable-rate segmentation of the speech into discrete units. Two segmentation\nmethods are considered. In the first, features are greedily merged until a\nprespecified number of segments are reached. The second uses dynamic\nprogramming to optimize a squared error with a penalty term to encourage fewer\nbut longer segments. We show that these VQ segmentation methods can be used\nwithout alteration across a wide range of tasks: unsupervised phone\nsegmentation, ABX phone discrimination, same-different word discrimination, and\nas inputs to a symbolic word segmentation algorithm. The penalized dynamic\nprogramming method generally performs best. While performance on individual\ntasks is only comparable to the state-of-the-art in some cases, in all tasks a\nreasonable competing approach is outperformed at a substantially lower bitrate.", "published": "2020-12-14 14:17:33", "link": "http://arxiv.org/abs/2012.07551v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Clickbait in Hindi News Media : A Preliminary Study", "abstract": "A corpus of Hindi news headlines shared on Twitter was created by collecting\ntweets of 5 mainstream Hindi news sources for a period of 4 months. 7\nindependent annotators were recruited to mark the 20 most retweeted news posts\nby each of the 5 news sources on its clickbait nature. The clickbait score\nhence generated was assessed for its correlation with interactions on the\nplatform (retweets, favorites, reader replies), tweet word count, and\nnormalized POS (part-of-speech) tag counts in tweets. A positive correlation\nwas observed between readers' interactions with tweets and tweets' clickbait\nscore. Significant correlations were also observed for POS tag counts and\nclickbait score. The prevalence of clickbait in mainstream Hindi news media was\nfound to be similar to its prevalence in English news media. We hope that our\nobservations would provide a platform for discussions on clickbait in\nmainstream Hindi news media.", "published": "2020-12-14 14:59:08", "link": "http://arxiv.org/abs/2012.07609v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Time to Transfer: Predicting and Evaluating Machine-Human Chatting\n  Handoff", "abstract": "Is chatbot able to completely replace the human agent? The short answer could\nbe - \"it depends...\". For some challenging cases, e.g., dialogue's topical\nspectrum spreads beyond the training corpus coverage, the chatbot may\nmalfunction and return unsatisfied utterances. This problem can be addressed by\nintroducing the Machine-Human Chatting Handoff (MHCH), which enables\nhuman-algorithm collaboration. To detect the normal/transferable utterances, we\npropose a Difficulty-Assisted Matching Inference (DAMI) network, utilizing\ndifficulty-assisted encoding to enhance the representations of utterances.\nMoreover, a matching inference mechanism is introduced to capture the\ncontextual matching features. A new evaluation metric, Golden Transfer within\nTolerance (GT-T), is proposed to assess the performance by considering the\ntolerance property of the MHCH. To provide insights into the task and validate\nthe proposed model, we collect two new datasets. Extensive experimental results\nare presented and contrasted against a series of baseline models to demonstrate\nthe efficacy of our model on MHCH.", "published": "2020-12-14 15:02:08", "link": "http://arxiv.org/abs/2012.07610v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A review of on-device fully neural end-to-end automatic speech\n  recognition algorithms", "abstract": "In this paper, we review various end-to-end automatic speech recognition\nalgorithms and their optimization techniques for on-device applications.\nConventional speech recognition systems comprise a large number of discrete\ncomponents such as an acoustic model, a language model, a pronunciation model,\na text-normalizer, an inverse-text normalizer, a decoder based on a Weighted\nFinite State Transducer (WFST), and so on. To obtain sufficiently high speech\nrecognition accuracy with such conventional speech recognition systems, a very\nlarge language model (up to 100 GB) is usually needed. Hence, the corresponding\nWFST size becomes enormous, which prohibits their on-device implementation.\nRecently, fully neural network end-to-end speech recognition algorithms have\nbeen proposed. Examples include speech recognition systems based on\nConnectionist Temporal Classification (CTC), Recurrent Neural Network\nTransducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic\nChunk-wise Attention (MoChA), transformer-based speech recognition systems, and\nso on. These fully neural network-based systems require much smaller memory\nfootprints compared to conventional algorithms, therefore their on-device\nimplementation has become feasible. In this paper, we review such end-to-end\nspeech recognition models. We extensively discuss their structures,\nperformance, and advantages compared to conventional algorithms.", "published": "2020-12-14 22:18:08", "link": "http://arxiv.org/abs/2012.07974v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Primer AI's Systems for Acronym Identification and Disambiguation", "abstract": "The prevalence of ambiguous acronyms make scientific documents harder to\nunderstand for humans and machines alike, presenting a need for models that can\nautomatically identify acronyms in text and disambiguate their meaning. We\nintroduce new methods for acronym identification and disambiguation: our\nacronym identification model projects learned token embeddings onto tag\npredictions, and our acronym disambiguation model finds training examples with\nsimilar sentence embeddings as test examples. Both of our systems achieve\nsignificant performance gains over previously suggested methods, and perform\ncompetitively on the SDU@AAAI-21 shared task leaderboard. Our models were\ntrained in part on new distantly-supervised datasets for these tasks which we\ncall AuxAI and AuxAD. We also identified a duplication conflict issue in the\nSciAD dataset, and formed a deduplicated version of SciAD that we call\nSciAD-dedupe. We publicly released all three of these datasets, and hope that\nthey help the community make further strides in scientific document\nunderstanding.", "published": "2020-12-14 23:59:05", "link": "http://arxiv.org/abs/2012.08013v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Audio Captioning using Pre-Trained Large-Scale Language Model Guided by\n  Audio-based Similar Caption Retrieval", "abstract": "The goal of audio captioning is to translate input audio into its description\nusing natural language. One of the problems in audio captioning is the lack of\ntraining data due to the difficulty in collecting audio-caption pairs by\ncrawling the web. In this study, to overcome this problem, we propose to use a\npre-trained large-scale language model. Since an audio input cannot be directly\ninputted into such a language model, we utilize guidance captions retrieved\nfrom a training dataset based on similarities that may exist in different\naudio. Then, the caption of the audio input is generated by using a pre-trained\nlanguage model while referring to the guidance captions. Experimental results\nshow that (i) the proposed method has succeeded to use a pre-trained language\nmodel for audio captioning, and (ii) the oracle performance of the pre-trained\nmodel-based caption generator was clearly better than that of the conventional\nmethod trained from scratch.", "published": "2020-12-14 08:27:36", "link": "http://arxiv.org/abs/2012.07331v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Classification of ALS patients based on acoustic analysis of sustained\n  vowel phonations", "abstract": "Amyotrophic lateral sclerosis (ALS) is incurable neurological disorder with\nrapidly progressive course. Common early symptoms of ALS are difficulty in\nswallowing and speech. However, early acoustic manifestation of speech and\nvoice symptoms is very variable, that making their detection very challenging,\nboth by human specialists and automatic systems. This study presents an\napproach to voice assessment for automatic system that separates healthy people\nfrom patients with ALS. In particular, this work focus on analysing of sustain\nphonation of vowels /a/ and /i/ to perform automatic classification of ALS\npatients. A wide range of acoustic features such as MFCC, formants, jitter,\nshimmer, vibrato, PPE, GNE, HNR, etc. were analysed. We also proposed a new set\nof acoustic features for characterizing harmonic structure of the vowels.\nCalculation of these features is based on pitch synchronized voice analysis. A\nlinear discriminant analysis (LDA) was used to classify the phonation produced\nby patients with ALS and those by healthy individuals. Several algorithms of\nfeature selection were tested to find optimal feature subset for LDA model. The\nstudy's experiments show that the most successful LDA model based on 32\nfeatures picked out by LASSO feature selection algorithm attains 99.7% accuracy\nwith 99.3% sensitivity and 99.9% specificity. Among the classifiers with a\nsmall number of features, we can highlight LDA model with 5 features, which has\n89.0% accuracy (87.5% sensitivity and 90.4% specificity).", "published": "2020-12-14 08:56:53", "link": "http://arxiv.org/abs/2012.07347v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fork or Fail: Cycle-Consistent Training with Many-to-One Mappings", "abstract": "Cycle-consistent training is widely used for jointly learning a forward and\ninverse mapping between two domains of interest without the cumbersome\nrequirement of collecting matched pairs within each domain. In this regard, the\nimplicit assumption is that there exists (at least approximately) a\nground-truth bijection such that a given input from either domain can be\naccurately reconstructed from successive application of the respective\nmappings. But in many applications no such bijection can be expected to exist\nand large reconstruction errors can compromise the success of cycle-consistent\ntraining. As one important instance of this limitation, we consider\npractically-relevant situations where there exists a many-to-one or surjective\nmapping between domains. To address this regime, we develop a conditional\nvariational autoencoder (CVAE) approach that can be viewed as converting\nsurjective mappings to implicit bijections whereby reconstruction errors in\nboth directions can be minimized, and as a natural byproduct, realistic output\ndiversity can be obtained in the one-to-many direction. As theoretical\nmotivation, we analyze a simplified scenario whereby minima of the proposed\nCVAE-based energy function align with the recovery of ground-truth surjective\nmappings. On the empirical side, we consider a synthetic image dataset with\nknown ground-truth, as well as a real-world application involving natural\nlanguage generation from knowledge graphs and vice versa, a prototypical\nsurjective case. For the latter, our CVAE pipeline can capture such many-to-one\nmappings during cycle training while promoting textural diversity for\ngraph-to-text tasks. Our code is available at github.com/QipengGuo/CycleGT\n  *A condensed version of this paper has been accepted to AISTATS 2021. This\nversion contains additional content and updates.", "published": "2020-12-14 10:59:59", "link": "http://arxiv.org/abs/2012.07412v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A learning perspective on the emergence of abstractions: the curious\n  case of phonemes", "abstract": "In the present paper we use a range of modeling techniques to investigate\nwhether an abstract phone could emerge from exposure to speech sounds. In\neffect, the study represents an attempt for operationalize a theoretical device\nof Usage-based Linguistics of emergence of an abstraction from language use.\nOur quest focuses on the simplest of such hypothesized abstractions. We test\ntwo opposing principles regarding the development of language knowledge in\nlinguistically untrained language users: Memory-Based Learning (MBL) and\nError-Correction Learning (ECL). A process of generalization underlies the\nabstractions linguists operate with, and we probed whether MBL and ECL could\ngive rise to a type of language knowledge that resembles linguistic\nabstractions. Each model was presented with a significant amount of\npre-processed speech produced by one speaker. We assessed the consistency or\nstability of what these simple models have learned and their ability to give\nrise to abstract categories. Both types of models fare differently with regard\nto these tests. We show that ECL models can learn abstractions and that at\nleast part of the phone inventory and grouping into traditional types can be\nreliably identified from the input.", "published": "2020-12-14 13:33:34", "link": "http://arxiv.org/abs/2012.07499v4", "categories": ["cs.CL", "cs.AI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "What Makes a Good and Useful Summary? Incorporating Users in Automatic\n  Summarization Research", "abstract": "Automatic text summarization has enjoyed great progress over the years and is\nused in numerous applications, impacting the lives of many. Despite this\ndevelopment, there is little research that meaningfully investigates how the\ncurrent research focus in automatic summarization aligns with users' needs. To\nbridge this gap, we propose a survey methodology that can be used to\ninvestigate the needs of users of automatically generated summaries.\nImportantly, these needs are dependent on the target group. Hence, we design\nour survey in such a way that it can be easily adjusted to investigate\ndifferent user groups. In this work we focus on university students, who make\nextensive use of summaries during their studies. We find that the current\nresearch directions of the automatic summarization community do not fully align\nwith students' needs. Motivated by our findings, we present ways to mitigate\nthis mismatch in future research on automatic summarization: we propose\nresearch directions that impact the design, the development and the evaluation\nof automatically generated summaries.", "published": "2020-12-14 15:12:35", "link": "http://arxiv.org/abs/2012.07619v3", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Vartani Spellcheck -- Automatic Context-Sensitive Spelling Correction of\n  OCR-generated Hindi Text Using BERT and Levenshtein Distance", "abstract": "Traditional Optical Character Recognition (OCR) systems that generate text of\nhighly inflectional Indic languages like Hindi tend to suffer from poor\naccuracy due to a wide alphabet set, compound characters and difficulty in\nsegmenting characters in a word. Automatic spelling error detection and\ncontext-sensitive error correction can be used to improve accuracy by\npost-processing the text generated by these OCR systems. A majority of\npreviously developed language models for error correction of Hindi spelling\nhave been context-free. In this paper, we present Vartani Spellcheck - a\ncontext-sensitive approach for spelling correction of Hindi text using a\nstate-of-the-art transformer - BERT in conjunction with the Levenshtein\ndistance algorithm, popularly known as Edit Distance. We use a lookup\ndictionary and context-based named entity recognition (NER) for detection of\npossible spelling errors in the text. Our proposed technique has been tested on\na large corpus of text generated by the widely used Tesseract OCR on the Hindi\nepic Ramayana. With an accuracy of 81%, the results show a significant\nimprovement over some of the previously established context-sensitive error\ncorrection mechanisms for Hindi. We also explain how Vartani Spellcheck may be\nused for on-the-fly autocorrect suggestion during continuous typing in a text\neditor environment.", "published": "2020-12-14 15:49:54", "link": "http://arxiv.org/abs/2012.07652v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful Memes", "abstract": "This work presents Vilio, an implementation of state-of-the-art\nvisio-linguistic models and their application to the Hateful Memes Dataset. The\nimplemented models have been fitted into a uniform code-base and altered to\nyield better performance. The goal of Vilio is to provide a user-friendly\nstarting point for any visio-linguistic problem. An ensemble of 5 different V+L\nmodels implemented in Vilio achieves 2nd place in the Hateful Memes Challenge\nout of 3,300 participants. The code is available at\nhttps://github.com/Muennighoff/vilio.", "published": "2020-12-14 18:25:03", "link": "http://arxiv.org/abs/2012.07788v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Extracting Training Data from Large Language Models", "abstract": "It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.", "published": "2020-12-14 18:39:09", "link": "http://arxiv.org/abs/2012.07805v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Multi-SpectroGAN: High-Diversity and High-Fidelity Spectrogram\n  Generation with Adversarial Style Combination for Speech Synthesis", "abstract": "While generative adversarial networks (GANs) based neural text-to-speech\n(TTS) systems have shown significant improvement in neural speech synthesis,\nthere is no TTS system to learn to synthesize speech from text sequences with\nonly adversarial feedback. Because adversarial feedback alone is not sufficient\nto train the generator, current models still require the reconstruction loss\ncompared with the ground-truth and the generated mel-spectrogram directly. In\nthis paper, we present Multi-SpectroGAN (MSG), which can train the\nmulti-speaker model with only the adversarial feedback by conditioning a\nself-supervised hidden representation of the generator to a conditional\ndiscriminator. This leads to better guidance for generator training. Moreover,\nwe also propose adversarial style combination (ASC) for better generalization\nin the unseen speaking style and transcript, which can learn latent\nrepresentations of the combined style embedding from multiple mel-spectrograms.\nTrained with ASC and feature matching, the MSG synthesizes a high-diversity\nmel-spectrogram by controlling and mixing the individual speaking styles (e.g.,\nduration, pitch, and energy). The result shows that the MSG synthesizes a\nhigh-fidelity mel-spectrogram, which has almost the same naturalness MOS score\nas the ground-truth mel-spectrogram.", "published": "2020-12-14 05:33:26", "link": "http://arxiv.org/abs/2012.07267v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Few Shot Adaptive Normalization Driven Multi-Speaker Speech Synthesis", "abstract": "The style of the speech varies from person to person and every person\nexhibits his or her own style of speaking that is determined by the language,\ngeography, culture and other factors. Style is best captured by prosody of a\nsignal. High quality multi-speaker speech synthesis while considering prosody\nand in a few shot manner is an area of active research with many real-world\napplications. While multiple efforts have been made in this direction, it\nremains an interesting and challenging problem. In this paper, we present a\nnovel few shot multi-speaker speech synthesis approach (FSM-SS) that leverages\nadaptive normalization architecture with a non-autoregressive multi-head\nattention model. Given an input text and a reference speech sample of an unseen\nperson, FSM-SS can generate speech in that person's style in a few shot manner.\nAdditionally, we demonstrate how the affine parameters of normalization help in\ncapturing the prosodic features such as energy and fundamental frequency in a\ndisentangled fashion and can be used to generate morphed speech output. We\ndemonstrate the efficacy of our proposed architecture on multi-speaker VCTK and\nLibriTTS datasets, using multiple quantitative metrics that measure generated\nspeech distortion and MoS, along with speaker embedding analysis of the\ngenerated speech vs the actual speech samples.", "published": "2020-12-14 04:37:07", "link": "http://arxiv.org/abs/2012.07252v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "AV Taris: Online Audio-Visual Speech Recognition", "abstract": "In recent years, Automatic Speech Recognition (ASR) technology has approached\nhuman-level performance on conversational speech under relatively clean\nlistening conditions. In more demanding situations involving distant\nmicrophones, overlapped speech, background noise, or natural dialogue\nstructures, the ASR error rate is at least an order of magnitude higher. The\nvisual modality of speech carries the potential to partially overcome these\nchallenges and contribute to the sub-tasks of speaker diarisation, voice\nactivity detection, and the recovery of the place of articulation, and can\ncompensate for up to 15dB of noise on average. This article develops AV Taris,\na fully differentiable neural network model capable of decoding audio-visual\nspeech in real time. We achieve this by connecting two recently proposed models\nfor audio-visual speech integration and online speech recognition, namely AV\nAlign and Taris. We evaluate AV Taris under the same conditions as AV Align and\nTaris on one of the largest publicly available audio-visual speech datasets,\nLRS2. Our results show that AV Taris is superior to the audio-only variant of\nTaris, demonstrating the utility of the visual modality to speech recognition\nwithin the real time decoding framework defined by Taris. Compared to an\nequivalent Transformer-based AV Align model that takes advantage of full\nsentences without meeting the real-time requirement, we report an absolute\ndegradation of approximately 3% with AV Taris. As opposed to the more popular\nalternative for online speech recognition, namely the RNN Transducer, Taris\noffers a greatly simplified fully differentiable training pipeline. As a\nconsequence, AV Taris has the potential to popularise the adoption of\nAudio-Visual Speech Recognition (AVSR) technology and overcome the inherent\nlimitations of the audio modality in less optimal listening conditions.", "published": "2020-12-14 12:39:02", "link": "http://arxiv.org/abs/2012.07467v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Group Communication with Context Codec for Lightweight Source Separation", "abstract": "Despite the recent progress on neural network architectures for speech\nseparation, the balance between the model size, model complexity and model\nperformance is still an important and challenging problem for the deployment of\nsuch models to low-resource platforms. In this paper, we propose two simple\nmodules, group communication and context codec, that can be easily applied to a\nwide range of architectures to jointly decrease the model size and complexity\nwithout sacrificing the performance. A group communication module splits a\nhigh-dimensional feature into groups of low-dimensional features and captures\nthe inter-group dependency. A separation module with a significantly smaller\nmodel size can then be shared by all the groups. A context codec module,\ncontaining a context encoder and a context decoder, is designed as a learnable\ndownsampling and upsampling module to decrease the length of a sequential\nfeature processed by the separation module. The combination of the group\ncommunication and the context codec modules is referred to as the GC3 design.\nExperimental results show that applying GC3 on multiple network architectures\nfor speech separation can achieve on-par or better performance with as small as\n2.5% model size and 17.6% model complexity, respectively.", "published": "2020-12-14 06:57:58", "link": "http://arxiv.org/abs/2012.07291v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "REDAT: Accent-Invariant Representation for End-to-End ASR by Domain\n  Adversarial Training with Relabeling", "abstract": "Accents mismatching is a critical problem for end-to-end ASR. This paper aims\nto address this problem by building an accent-robust RNN-T system with domain\nadversarial training (DAT). We unveil the magic behind DAT and provide, for the\nfirst time, a theoretical guarantee that DAT learns accent-invariant\nrepresentations. We also prove that performing the gradient reversal in DAT is\nequivalent to minimizing the Jensen-Shannon divergence between domain output\ndistributions. Motivated by the proof of equivalence, we introduce reDAT, a\nnovel technique based on DAT, which relabels data using either unsupervised\nclustering or soft labels. Experiments on 23K hours of multi-accent data show\nthat DAT achieves competitive results over accent-specific baselines on both\nnative and non-native English accents but up to 13% relative WER reduction on\nunseen accents; our reDAT yields further improvements over DAT by 3% and 8%\nrelatively on non-native accents of American and British English.", "published": "2020-12-14 09:09:08", "link": "http://arxiv.org/abs/2012.07353v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Bayesian Learning for Deep Neural Network Adaptation", "abstract": "A key task for speech recognition systems is to reduce the mismatch between\ntraining and evaluation data that is often attributable to speaker differences.\nSpeaker adaptation techniques play a vital role to reduce the mismatch.\nModel-based speaker adaptation approaches often require sufficient amounts of\ntarget speaker data to ensure robustness. When the amount of speaker level data\nis limited, speaker adaptation is prone to overfitting and poor generalization.\nTo address the issue, this paper proposes a full Bayesian learning based DNN\nspeaker adaptation framework to model speaker-dependent (SD) parameter\nuncertainty given limited speaker specific adaptation data. This framework is\ninvestigated in three forms of model based DNN adaptation techniques: Bayesian\nlearning of hidden unit contributions (BLHUC), Bayesian parameterized\nactivation functions (BPAct), and Bayesian hidden unit bias vectors (BHUB). In\nthe three methods, deterministic SD parameters are replaced by latent variable\nposterior distributions for each speaker, whose parameters are efficiently\nestimated using a variational inference based approach. Experiments conducted\non 300-hour speed perturbed Switchboard corpus trained LF-MMI TDNN/CNN-TDNN\nsystems suggest the proposed Bayesian adaptation approaches consistently\noutperform the deterministic adaptation on the NIST Hub5'00 and RT03 evaluation\nsets. When using only the first five utterances from each speaker as adaptation\ndata, significant word error rate reductions up to 1.4% absolute (7.2%\nrelative) were obtained on the CallHome subset. The efficacy of the proposed\nBayesian adaptation techniques is further demonstrated in a comparison against\nthe state-of-the-art performance obtained on the same task using the most\nrecent systems reported in the literature.", "published": "2020-12-14 12:30:41", "link": "http://arxiv.org/abs/2012.07460v4", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
