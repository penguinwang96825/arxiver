{"title": "Large Language Model as an Assignment Evaluator: Insights, Feedback, and\n  Challenges in a 1000+ Student Course", "abstract": "Using large language models (LLMs) for automatic evaluation has become an\nimportant evaluation method in NLP research. However, it is unclear whether\nthese LLM-based evaluators can be applied in real-world classrooms to assess\nstudent assignments. This empirical report shares how we use GPT-4 as an\nautomatic assignment evaluator in a university course with 1,028 students.\nBased on student responses, we find that LLM-based assignment evaluators are\ngenerally acceptable to students when students have free access to these\nLLM-based evaluators. However, students also noted that the LLM sometimes fails\nto adhere to the evaluation instructions. Additionally, we observe that\nstudents can easily manipulate the LLM-based evaluator to output specific\nstrings, allowing them to achieve high scores without meeting the assignment\nrubric. Based on student feedback and our experience, we provide several\nrecommendations for integrating LLM-based evaluators into future classrooms.\nOur observation also highlights potential directions for improving LLM-based\nevaluators, including their instruction-following ability and vulnerability to\nprompt hacking.", "published": "2024-07-07 00:17:24", "link": "http://arxiv.org/abs/2407.05216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Flood of Techniques and Drought of Theories: Emotion Mining in Disasters", "abstract": "Emotion mining has become a crucial tool for understanding human emotions\nduring disasters, leveraging the extensive data generated on social media\nplatforms. This paper aims to summarize existing research on emotion mining\nwithin disaster contexts, highlighting both significant discoveries and\npersistent issues. On the one hand, emotion mining techniques have achieved\nacceptable accuracy enabling applications such as rapid damage assessment and\nmental health surveillance. On the other hand, with many studies adopting\ndata-driven approaches, several methodological issues remain. These include\narbitrary emotion classification, ignoring biases inherent in data collection\nfrom social media, such as the overrepresentation of individuals from higher\nsocioeconomic status on Twitter, and the lack of application of theoretical\nframeworks like cross-cultural comparisons. These problems can be summarized as\na notable lack of theory-driven research and ignoring insights from social and\nbehavioral sciences. This paper underscores the need for interdisciplinary\ncollaboration between computer scientists and social scientists to develop more\nrobust and theoretically grounded approaches in emotion mining. By addressing\nthese gaps, we aim to enhance the effectiveness and reliability of emotion\nmining methodologies, ultimately contributing to improved disaster\npreparedness, response, and recovery.\n  Keywords: emotion mining, sentiment analysis, natural disasters, psychology,\ntechnological disasters", "published": "2024-07-07 00:43:05", "link": "http://arxiv.org/abs/2407.05219v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLIMB: A Benchmark of Clinical Bias in Large Language Models", "abstract": "Large language models (LLMs) are increasingly applied to clinical\ndecision-making. However, their potential to exhibit bias poses significant\nrisks to clinical equity. Currently, there is a lack of benchmarks that\nsystematically evaluate such clinical bias in LLMs. While in downstream tasks,\nsome biases of LLMs can be avoided such as by instructing the model to answer\n\"I'm not sure...\", the internal bias hidden within the model still lacks deep\nstudies. We introduce CLIMB (shorthand for A Benchmark of Clinical Bias in\nLarge Language Models), a pioneering comprehensive benchmark to evaluate both\nintrinsic (within LLMs) and extrinsic (on downstream tasks) bias in LLMs for\nclinical decision tasks. Notably, for intrinsic bias, we introduce a novel\nmetric, AssocMAD, to assess the disparities of LLMs across multiple demographic\ngroups. Additionally, we leverage counterfactual intervention to evaluate\nextrinsic bias in a task of clinical diagnosis prediction. Our experiments\nacross popular and medically adapted LLMs, particularly from the Mistral and\nLLaMA families, unveil prevalent behaviors with both intrinsic and extrinsic\nbias. This work underscores the critical need to mitigate clinical bias and\nsets a new standard for future evaluations of LLMs' clinical bias.", "published": "2024-07-07 03:41:51", "link": "http://arxiv.org/abs/2407.05250v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Binary Gender Labels: Revealing Gender Biases in LLMs through\n  Gender-Neutral Name Predictions", "abstract": "Name-based gender prediction has traditionally categorized individuals as\neither female or male based on their names, using a binary classification\nsystem. That binary approach can be problematic in the cases of gender-neutral\nnames that do not align with any one gender, among other reasons. Relying\nsolely on binary gender categories without recognizing gender-neutral names can\nreduce the inclusiveness of gender prediction tasks. We introduce an additional\ngender category, i.e., \"neutral\", to study and address potential gender biases\nin Large Language Models (LLMs). We evaluate the performance of several\nfoundational and large language models in predicting gender based on first\nnames only. Additionally, we investigate the impact of adding birth years to\nenhance the accuracy of gender prediction, accounting for shifting associations\nbetween names and genders over time. Our findings indicate that most LLMs\nidentify male and female names with high accuracy (over 80%) but struggle with\ngender-neutral names (under 40%), and the accuracy of gender prediction is\nhigher for English-based first names than non-English names. The experimental\nresults show that incorporating the birth year does not improve the overall\naccuracy of gender prediction, especially for names with evolving gender\nassociations. We recommend using caution when applying LLMs for gender\nidentification in downstream tasks, particularly when dealing with non-binary\ngender labels.", "published": "2024-07-07 05:59:09", "link": "http://arxiv.org/abs/2407.05271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Targeted Adversarial Attacks For Neural Machine Translation", "abstract": "Targeted adversarial attacks are widely used to evaluate the robustness of\nneural machine translation systems. Unfortunately, this paper first identifies\na critical issue in the existing settings of NMT targeted adversarial attacks,\nwhere their attacking results are largely overestimated. To this end, this\npaper presents a new setting for NMT targeted adversarial attacks that could\nlead to reliable attacking results. Under the new setting, it then proposes a\nTargeted Word Gradient adversarial Attack (TWGA) method to craft adversarial\nexamples. Experimental results demonstrate that our proposed setting could\nprovide faithful attacking results for targeted adversarial attacks on NMT\nsystems, and the proposed TWGA method can effectively attack such victim NMT\nsystems. In-depth analyses on a large-scale dataset further illustrate some\nvaluable findings. 1 Our code and data are available at\nhttps://github.com/wujunjie1998/TWGA.", "published": "2024-07-07 10:16:06", "link": "http://arxiv.org/abs/2407.05319v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Model Uncertainty Function as a Proxy for Multiple-Choice Question\n  Item Difficulty?", "abstract": "Estimating the difficulty of multiple-choice questions would be great help\nfor educators who must spend substantial time creating and piloting stimuli for\ntheir tests, and for learners who want to practice. Supervised approaches to\ndifficulty estimation have yielded to date mixed results. In this contribution\nwe leverage an aspect of generative large models which might be seen as a\nweakness when answering questions, namely their uncertainty, and exploit it\ntowards exploring correlations between two different metrics of uncertainty,\nand the actual student response distribution. While we observe some present but\nweak correlations, we also discover that the models' behaviour is different in\nthe case of correct vs wrong answers, and that correlations differ\nsubstantially according to the different question types which are included in\nour fine-grained, previously unused dataset of 451 questions from a\nBiopsychology course. In discussing our findings, we also suggest potential\navenues to further leverage model uncertainty as an additional proxy for item\ndifficulty.", "published": "2024-07-07 10:48:04", "link": "http://arxiv.org/abs/2407.05327v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Task Experts through Retrieval Based Distillation", "abstract": "One of the most reliable ways to create deployable models for specialized\ntasks is to obtain an adequate amount of high-quality task-specific data.\nHowever, for specialized tasks, often such datasets do not exist. Existing\nmethods address this by creating such data from large language models (LLMs)\nand then distilling such knowledge into smaller models. However, these methods\nare limited by the quality of the LLMs output, and tend to generate repetitive\nor incorrect data. In this work, we present Retrieval Based Distillation\n(ReBase), a method that first retrieves data from rich online sources and then\ntransforms them into domain-specific data. This method greatly enhances data\ndiversity. Moreover, ReBase generates Chain-of-Thought reasoning and distills\nthe reasoning capacity of LLMs. We test our method on 4 benchmarks and results\nshow that our method significantly improves performance by up to 7.8% on SQuAD,\n1.37% on MNLI, and 1.94% on BigBench-Hard.", "published": "2024-07-07 18:27:59", "link": "http://arxiv.org/abs/2407.05463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Biomedical Nested NER with Large Language Model and UMLS Heuristics", "abstract": "In this paper, we present our system for the BioNNE English track, which aims\nto extract 8 types of biomedical nested named entities from biomedical text. We\nuse a large language model (Mixtral 8x7B instruct) and ScispaCy NER model to\nidentify entities in an article and build custom heuristics based on unified\nmedical language system (UMLS) semantic types to categorize the entities. We\ndiscuss the results and limitations of our system and propose future\nimprovements. Our system achieved an F1 score of 0.39 on the BioNNE validation\nset and 0.348 on the test set.", "published": "2024-07-07 19:37:40", "link": "http://arxiv.org/abs/2407.05480v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Effective are State Space Models for Machine Translation?", "abstract": "Transformers are the current architecture of choice for NLP, but their\nattention layers do not scale well to long contexts. Recent works propose to\nreplace attention with linear recurrent layers -- this is the case for state\nspace models, which enjoy efficient training and inference. However, it remains\nunclear whether these models are competitive with transformers in machine\ntranslation (MT). In this paper, we provide a rigorous and comprehensive\nexperimental comparison between transformers and linear recurrent models for\nMT. Concretely, we experiment with RetNet, Mamba, and hybrid versions of Mamba\nwhich incorporate attention mechanisms. Our findings demonstrate that Mamba is\nhighly competitive with transformers on sentence and paragraph-level datasets,\nwhere in the latter both models benefit from shifting the training distribution\ntowards longer sequences. Further analysis show that integrating attention into\nMamba improves translation quality, robustness to sequence length\nextrapolation, and the ability to recall named entities.", "published": "2024-07-07 20:21:49", "link": "http://arxiv.org/abs/2407.05489v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Advancing Prompt Recovery in NLP: A Deep Dive into the Integration of\n  Gemma-2b-it and Phi2 Models", "abstract": "Prompt recovery, a crucial task in natural language processing, entails the\nreconstruction of prompts or instructions that language models use to convert\ninput text into a specific output. Although pivotal, the design and\neffectiveness of prompts represent a challenging and relatively untapped field\nwithin NLP research. This paper delves into an exhaustive investigation of\nprompt recovery methodologies, employing a spectrum of pre-trained language\nmodels and strategies. Our study is a comparative analysis aimed at gauging the\nefficacy of various models on a benchmark dataset, with the goal of pinpointing\nthe most proficient approach for prompt recovery. Through meticulous\nexperimentation and detailed analysis, we elucidate the outstanding performance\nof the Gemma-2b-it + Phi2 model + Pretrain. This model surpasses its\ncounterparts, showcasing its exceptional capability in accurately\nreconstructing prompts for text transformation tasks. Our findings offer a\nsignificant contribution to the existing knowledge on prompt recovery, shedding\nlight on the intricacies of prompt design and offering insightful perspectives\nfor future innovations in text rewriting and the broader field of natural\nlanguage processing.", "published": "2024-07-07 02:15:26", "link": "http://arxiv.org/abs/2407.05233v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Some Issues in Predictive Ethics Modeling: An Annotated Contrast Set of\n  \"Moral Stories\"", "abstract": "Models like Delphi have been able to label ethical dilemmas as moral or\nimmoral with astonishing accuracy. This paper challenges accuracy as a holistic\nmetric for ethics modeling by identifying issues with translating moral\ndilemmas into text-based input. It demonstrates these issues with contrast sets\nthat substantially reduce the performance of classifiers trained on the dataset\nMoral Stories. Ultimately, we obtain concrete estimates for how much specific\nforms of data misrepresentation harm classifier accuracy. Specifically,\nlabel-changing tweaks to the descriptive content of a situation (as small as\n3-5 words) can reduce classifier accuracy to as low as 51%, almost half the\ninitial accuracy of 99.8%. Associating situations with a misleading social norm\nlowers accuracy to 98.8%, while adding textual bias (i.e. an implication that a\nsituation already fits a certain label) lowers accuracy to 77%.\n  These results suggest not only that many ethics models have substantially\noverfit, but that several precautions are required to ensure that input\naccurately captures a moral dilemma. This paper recommends re-examining the\nstructure of a social norm, training models to ask for context with defeasible\nreasoning, and filtering input for textual bias. Doing so not only gives us the\nfirst concrete estimates of the average cost to accuracy of misrepresenting\nethics data, but gives researchers practical tips for considering these\nestimates in research.", "published": "2024-07-07 03:22:49", "link": "http://arxiv.org/abs/2407.05244v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool", "abstract": "Multimodal large language models (MLLMs) are flourishing, but mainly focus on\nimages with less attention than videos, especially in sub-fields such as prompt\nengineering, video chain-of-thought (CoT), and instruction tuning on videos.\nTherefore, we try to explore the collection of CoT datasets in videos to lead\nto video OpenQA and improve the reasoning ability of MLLMs. Unfortunately,\nmaking such video CoT datasets is not an easy task. Given that human annotation\nis too cumbersome and expensive, while machine-generated is not reliable due to\nthe hallucination issue, we develop an automatic annotation tool that combines\nmachine and human experts, under the active learning paradigm. Active learning\nis an interactive strategy between the model and human experts, in this way,\nthe workload of human labeling can be reduced and the quality of the dataset\ncan be guaranteed. With the help of the automatic annotation tool, we strive to\ncontribute three datasets, namely VideoCoT, TopicQA, TopicCoT. Furthermore, we\npropose a simple but effective benchmark based on the collected datasets, which\nexploits CoT to maximize the complex reasoning capabilities of MLLMs. Extensive\nexperiments demonstrate the effectiveness our solution.", "published": "2024-07-07 13:10:23", "link": "http://arxiv.org/abs/2407.05355v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for\n  Large-Scale Speech Generation", "abstract": "Recent advancements in speech generation models have been significantly\ndriven by the use of large-scale training data. However, producing highly\nspontaneous, human-like speech remains a challenge due to the scarcity of\nlarge, diverse, and spontaneous speech datasets. In response, we introduce\nEmilia, the first large-scale, multilingual, and diverse speech generation\ndataset. Emilia starts with over 101k hours of speech across six languages,\ncovering a wide range of speaking styles to enable more natural and spontaneous\nspeech generation. To facilitate the scale-up of Emilia, we also present\nEmilia-Pipe, the first open-source preprocessing pipeline designed to\nefficiently transform raw, in-the-wild speech data into high-quality training\ndata with speech annotations. Experimental results demonstrate the\neffectiveness of both Emilia and Emilia-Pipe. Demos are available at:\nhttps://emilia-dataset.github.io/Emilia-Demo-Page/.", "published": "2024-07-07 13:24:54", "link": "http://arxiv.org/abs/2407.05361v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Multimodal Prompt Learning with Missing Modalities for Sentiment\n  Analysis and Emotion Recognition", "abstract": "The development of multimodal models has significantly advanced multimodal\nsentiment analysis and emotion recognition. However, in real-world\napplications, the presence of various missing modality cases often leads to a\ndegradation in the model's performance. In this work, we propose a novel\nmultimodal Transformer framework using prompt learning to address the issue of\nmissing modalities. Our method introduces three types of prompts: generative\nprompts, missing-signal prompts, and missing-type prompts. These prompts enable\nthe generation of missing modality features and facilitate the learning of\nintra- and inter-modality information. Through prompt learning, we achieve a\nsubstantial reduction in the number of trainable parameters. Our proposed\nmethod outperforms other methods significantly across all evaluation metrics.\nExtensive experiments and ablation studies are conducted to demonstrate the\neffectiveness and robustness of our method, showcasing its ability to\neffectively handle missing modalities.", "published": "2024-07-07 13:55:56", "link": "http://arxiv.org/abs/2407.05374v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in\n  Large Language Models", "abstract": "Temporal reasoning (TR) is a critical component of artificial intelligence,\nencompassing understanding and processing temporal information and\nrelationships between events. To discover and study the TR ability in Large\nLanguage Models (LLMs), various datasets have been constructed in different\nways for evaluating various aspects of TR ability. Our work proposes a novel\napproach to design and develop a pipeline for constructing datasets to evaluate\nthe TR ability of LLMs by leveraging random directed graph generation, LTL\nformula, and the NuSMV model checker. Based on the pipeline, we have also\nconstructed a dataset as a benchmark, namely LTLBench, consisting of 2,000 TR\nchallenges and evaluated six LLMs with it. Furthermore, we have conducted\nadditional experiments to discover the impact of increasing the number of\nevents and formula operators on the complexity of TR problems and the\nperformance of LLMs. We have demonstrated that although LLMs exhibit some\npromise in handling TR challenges, they still struggle with complex TR. We\nexpect this work can offer insights into TR ability in LLMs while also\nproviding a valuable tool for future TR evaluations.", "published": "2024-07-07 16:37:06", "link": "http://arxiv.org/abs/2407.05434v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SmurfCat at PAN 2024 TextDetox: Alignment of Multilingual Transformers\n  for Text Detoxification", "abstract": "This paper presents a solution for the Multilingual Text Detoxification task\nin the PAN-2024 competition of the SmurfCat team. Using data augmentation\nthrough machine translation and a special filtering procedure, we collected an\nadditional multilingual parallel dataset for text detoxification. Using the\nobtained data, we fine-tuned several multilingual sequence-to-sequence models,\nsuch as mT0 and Aya, on a text detoxification task. We applied the ORPO\nalignment technique to the final model. Our final model has only 3.7 billion\nparameters and achieves state-of-the-art results for the Ukrainian language and\nnear state-of-the-art results for other languages. In the competition, our team\nachieved first place in the automated evaluation with a score of 0.52 and\nsecond place in the final human evaluation with a score of 0.74.", "published": "2024-07-07 17:19:34", "link": "http://arxiv.org/abs/2407.05449v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Experiments with truth using Machine Learning: Spectral analysis and\n  explainable classification of synthetic, false, and genuine information", "abstract": "Misinformation is still a major societal problem and the arrival of Large\nLanguage Models (LLMs) only added to it. This paper analyzes synthetic, false,\nand genuine information in the form of text from spectral analysis,\nvisualization, and explainability perspectives to find the answer to why the\nproblem is still unsolved despite multiple years of research and a plethora of\nsolutions in the literature. Various embedding techniques on multiple datasets\nare used to represent information for the purpose. The diverse spectral and\nnon-spectral methods used on these embeddings include t-distributed Stochastic\nNeighbor Embedding (t-SNE), Principal Component Analysis (PCA), and Variational\nAutoencoders (VAEs). Classification is done using multiple machine learning\nalgorithms. Local Interpretable Model-Agnostic Explanations (LIME), SHapley\nAdditive exPlanations (SHAP), and Integrated Gradients are used for the\nexplanation of the classification. The analysis and the explanations generated\nshow that misinformation is quite closely intertwined with genuine information\nand the machine learning algorithms are not as effective in separating the two\ndespite the claims in the literature.", "published": "2024-07-07 18:31:09", "link": "http://arxiv.org/abs/2407.05464v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Enhancing Hallucination Detection through Perturbation-Based Synthetic\n  Data Generation in System Responses", "abstract": "Detecting hallucinations in large language model (LLM) outputs is pivotal,\nyet traditional fine-tuning for this classification task is impeded by the\nexpensive and quickly outdated annotation process, especially across numerous\nvertical domains and in the face of rapid LLM advancements. In this study, we\nintroduce an approach that automatically generates both faithful and\nhallucinated outputs by rewriting system responses. Experimental findings\ndemonstrate that a T5-base model, fine-tuned on our generated dataset,\nsurpasses state-of-the-art zero-shot detectors and existing synthetic\ngeneration methods in both accuracy and latency, indicating efficacy of our\napproach.", "published": "2024-07-07 19:19:32", "link": "http://arxiv.org/abs/2407.05474v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Just read twice: closing the recall gap for recurrent language models", "abstract": "Recurrent large language models that compete with Transformers in language\nmodeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV).\nExcitingly, these architectures use a constant amount of memory during\ninference. However, due to the limited memory, recurrent LMs cannot recall and\nuse all the information in long contexts leading to brittle in-context learning\n(ICL) quality. A key challenge for efficient LMs is selecting what information\nto store versus discard. In this work, we observe the order in which\ninformation is shown to the LM impacts the selection difficulty. To formalize\nthis, we show that the hardness of information recall reduces to the hardness\nof a problem called set disjointness (SD), a quintessential problem in\ncommunication complexity that requires a streaming algorithm (e.g., recurrent\nmodel) to decide whether inputted sets are disjoint. We empirically and\ntheoretically show that the recurrent memory required to solve SD changes with\nset order, i.e., whether the smaller set appears first in-context. Our analysis\nsuggests, to mitigate the reliance on data order, we can put information in the\nright order in-context or process prompts non-causally. Towards that end, we\npropose: (1) JRT-Prompt, where context gets repeated multiple times in the\nprompt, effectively showing the model all data orders. This gives $11.0 \\pm\n1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL\ntasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation\nprefill (length $32$k, batch size $16$, NVidia H100). We then propose (2)\nJRT-RNN, which uses non-causal prefix-linear-attention to process prompts and\nprovides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and\n$96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with\n$19.2\\times$ higher throughput for prefill than FA2.", "published": "2024-07-07 19:55:09", "link": "http://arxiv.org/abs/2407.05483v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting Word Similarity in Context with Referential Translation\n  Machines", "abstract": "We identify the similarity between two words in English by casting the task\nas machine translation performance prediction (MTPP) between the words given\nthe context and the distance between their similarities. We use referential\ntranslation machines (RTMs), which allows a common representation for training\nand test sets and stacked machine learning models. RTMs can achieve the top\nresults in Graded Word Similarity in Context (GWSC) task.", "published": "2024-07-07 09:36:41", "link": "http://arxiv.org/abs/2407.06230v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Survey on biomarkers in human vocalizations", "abstract": "Recent years has witnessed an increase in technologies that use speech for\nthe sensing of the health of the talker. This survey paper proposes a general\ntaxonomy of the technologies and a broad overview of current progress and\nchallenges. Vocal biomarkers are often secondary measures that are\napproximating a signal of another sensor or identifying an underlying mental,\ncognitive, or physiological state. Their measurement involve disturbances and\nuncertainties that may be considered as noise sources and the biomarkers are\ncoarsely qualified in terms of the various sources of noise involved in their\ndetermination. While in some proposed biomarkers the error levels seem high,\nthere are vocal biomarkers where the errors are expected to be low and thus are\nmore likely to qualify as candidates for adoption in healthcare applications.", "published": "2024-07-07 08:09:28", "link": "http://arxiv.org/abs/2407.17505v2", "categories": ["q-bio.NC", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning", "abstract": "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems.", "published": "2024-07-07 14:55:04", "link": "http://arxiv.org/abs/2407.05399v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "iSign: A Benchmark for Indian Sign Language Processing", "abstract": "Indian Sign Language has limited resources for developing machine learning\nand data-driven approaches for automated language processing. Though\ntext/audio-based language processing techniques have shown colossal research\ninterest and tremendous improvements in the last few years, Sign Languages\nstill need to catch up due to the need for more resources. To bridge this gap,\nin this work, we propose iSign: a benchmark for Indian Sign Language (ISL)\nProcessing. We make three primary contributions to this work. First, we release\none of the largest ISL-English datasets with more than 118K\nvideo-sentence/phrase pairs. To the best of our knowledge, it is the largest\nsign language dataset available for ISL. Second, we propose multiple\nNLP-specific tasks (including SignVideo2Text, SignPose2Text, Text2Pose, Word\nPrediction, and Sign Semantics) and benchmark them with the baseline models for\neasier access to the research community. Third, we provide detailed insights\ninto the proposed benchmarks with a few linguistic insights into the workings\nof ISL. We streamline the evaluation of Sign Language processing, addressing\nthe gaps in the NLP research community for Sign Languages. We release the\ndataset, tasks, and models via the following website:\nhttps://exploration-lab.github.io/iSign/", "published": "2024-07-07 15:07:35", "link": "http://arxiv.org/abs/2407.05404v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SBoRA: Low-Rank Adaptation with Regional Weight Updates", "abstract": "This paper introduces Standard Basis LoRA (SBoRA), a novel\nparameter-efficient fine-tuning approach for Large Language Models that builds\nupon the pioneering works of Low-Rank Adaptation (LoRA) and Orthogonal\nAdaptation. SBoRA reduces the number of trainable parameters by half or doubles\nthe rank with the similar number of trainable parameters as LoRA, while\nimproving learning performance. By utilizing orthogonal standard basis vectors\nto initialize one of the low-rank matrices (either $\\mathbf{A}$ or\n$\\mathbf{B}$), SBoRA facilitates regional weight updates and memory-efficient\nfine-tuning. This results in two variants, SBoRA-FA and SBoRA-FB, where only\none of the matrices is updated, leading to a sparse update matrix\n$\\mathrm{\\Delta} \\mathbf{W}$ with predominantly zero rows or columns.\nConsequently, most of the fine-tuned model's weights\n$(\\mathbf{W}_0+\\mathrm{\\Delta} \\mathbf{W})$ remain unchanged from the\npre-trained weights, akin to the modular organization of the human brain, which\nefficiently adapts to new tasks. Our empirical results demonstrate the\nsuperiority of SBoRA-FA over LoRA in various fine-tuning tasks, including\ncommonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the\neffectiveness of QSBoRA on quantized LLaMA models of varying scales,\nhighlighting its potential for efficient adaptation to new tasks. Code is\navailable at https://github.com/cityuhkai/SBoRA", "published": "2024-07-07 15:37:13", "link": "http://arxiv.org/abs/2407.05413v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models", "abstract": "Although the multilingual capability of LLMs offers new opportunities to\novercome the language barrier, do these capabilities translate into real-life\nscenarios where linguistic divide and knowledge conflicts between multilingual\nsources are known occurrences? In this paper, we studied LLM's linguistic\npreference in a cross-language RAG-based information search setting. We found\nthat LLMs displayed systemic bias towards information in the same language as\nthe query language in both document retrieval and answer generation.\nFurthermore, in scenarios where no information is in the language of the query,\nLLMs prefer documents in high-resource languages during generation, potentially\nreinforcing the dominant views. Such bias exists for both factual and\nopinion-based queries. Our results highlight the linguistic divide within\nmultilingual LLMs in information search systems. The seemingly beneficial\nmultilingual capability of LLMs may backfire on information parity by\nreinforcing language-specific information cocoons or filter bubbles further\nmarginalizing low-resource views.", "published": "2024-07-07 21:26:36", "link": "http://arxiv.org/abs/2407.05502v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Morse Code-Enabled Speech Recognition for Individuals with Visual and\n  Hearing Impairments", "abstract": "The proposed model aims to develop a speech recognition technology for\nhearing, speech, or cognitively disabled people. All the available technology\nin the field of speech recognition doesn't come with an interface for\ncommunication for people with hearing, speech, or cognitive disabilities. The\nproposed model proposes the speech from the user, is transmitted to the speech\nrecognition layer where it is converted into text and then that text is then\ntransmitted to the morse code conversion layer where the morse code of the\ncorresponding speech is given as the output. The accuracy of the model is\ncompletely dependent on speech recognition, as the morse code conversion is a\nprocess. The model is tested with recorded audio files with different\nparameters. The proposed model's WER and accuracy are both determined to be\n10.18% and 89.82%, respectively.", "published": "2024-07-07 09:54:29", "link": "http://arxiv.org/abs/2407.14525v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.SD"}
{"title": "ASRRL-TTS: Agile Speaker Representation Reinforcement Learning for\n  Text-to-Speech Speaker Adaptation", "abstract": "Speaker adaptation, which involves cloning voices from unseen speakers in the\nText-to-Speech task, has garnered significant interest due to its numerous\napplications in multi-media fields. Despite recent advancements, existing\nmethods often struggle with inadequate speaker representation accuracy and\noverfitting, particularly in limited reference speeches scenarios. To address\nthese challenges, we propose an Agile Speaker Representation Reinforcement\nLearning strategy to enhance speaker similarity in speaker adaptation tasks.\nASRRL is the first work to apply reinforcement learning to improve the modeling\naccuracy of speaker embeddings in speaker adaptation, addressing the challenge\nof decoupling voice content and timbre. Our approach introduces two action\nstrategies tailored to different reference speeches scenarios. In the\nsingle-sentence scenario, a knowledge-oriented optimal routine searching RL\nmethod is employed to expedite the exploration and retrieval of refinement\ninformation on the fringe of speaker representations. In the few-sentence\nscenario, we utilize a dynamic RL method to adaptively fuse reference speeches,\nenhancing the robustness and accuracy of speaker modeling. To achieve optimal\nresults in the target domain, a multi-scale fusion scoring mechanism based\nreward model that evaluates speaker similarity, speech quality, and\nintelligibility across three dimensions is proposed, ensuring that improvements\nin speaker similarity do not compromise speech quality or intelligibility. The\nexperimental results on the LibriTTS and VCTK datasets within mainstream TTS\nframeworks demonstrate the extensibility and generalization capabilities of the\nproposed ASRRL method. The results indicate that the ASRRL method significantly\noutperforms traditional fine-tuning approaches, achieving higher speaker\nsimilarity and better overall speech quality with limited reference speeches.", "published": "2024-07-07 15:58:11", "link": "http://arxiv.org/abs/2407.05421v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fine-Grained and Interpretable Neural Speech Editing", "abstract": "Fine-grained editing of speech attributes$\\unicode{x2014}$such as prosody\n(i.e., the pitch, loudness, and phoneme durations), pronunciation, speaker\nidentity, and formants$\\unicode{x2014}$is useful for fine-tuning and fixing\nimperfections in human and AI-generated speech recordings for creation of\npodcasts, film dialogue, and video game dialogue. Existing speech synthesis\nsystems use representations that entangle two or more of these attributes,\nprohibiting their use in fine-grained, disentangled editing. In this paper, we\ndemonstrate the first disentangled and interpretable representation of speech\nwith comparable subjective and objective vocoding reconstruction accuracy to\nMel spectrograms. Our interpretable representation, combined with our proposed\ndata augmentation method, enables training an existing neural vocoder to\nperform fast, accurate, and high-quality editing of pitch, duration, volume,\ntimbral correlates of volume, pronunciation, speaker identity, and spectral\nbalance.", "published": "2024-07-07 19:05:52", "link": "http://arxiv.org/abs/2407.05471v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Ternary Spike-based Neuromorphic Signal Processing System", "abstract": "Deep Neural Networks (DNNs) have been successfully implemented across various\nsignal processing fields, resulting in significant enhancements in performance.\nHowever, DNNs generally require substantial computational resources, leading to\nsignificant economic costs and posing challenges for their deployment on\nresource-constrained edge devices. In this study, we take advantage of spiking\nneural networks (SNNs) and quantization technologies to develop an\nenergy-efficient and lightweight neuromorphic signal processing system. Our\nsystem is characterized by two principal innovations: a threshold-adaptive\nencoding (TAE) method and a quantized ternary SNN (QT-SNN). The TAE method can\nefficiently encode time-varying analog signals into sparse ternary spike\ntrains, thereby reducing energy and memory demands for signal processing.\nQT-SNN, compatible with ternary spike trains from the TAE method, quantifies\nboth membrane potentials and synaptic weights to reduce memory requirements\nwhile maintaining performance. Extensive experiments are conducted on two\ntypical signal-processing tasks: speech and electroencephalogram recognition.\nThe results demonstrate that our neuromorphic signal processing system achieves\nstate-of-the-art (SOTA) performance with a 94% reduced memory requirement.\nFurthermore, through theoretical energy consumption analysis, our system shows\n7.5x energy saving compared to other SNN works. The efficiency and efficacy of\nthe proposed system highlight its potential as a promising avenue for\nenergy-efficient signal processing.", "published": "2024-07-07 09:32:19", "link": "http://arxiv.org/abs/2407.05310v1", "categories": ["eess.SP", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Music Era Recognition Using Supervised Contrastive Learning and Artist\n  Information", "abstract": "Does popular music from the 60s sound different than that of the 90s? Prior\nstudy has shown that there would exist some variations of patterns and\nregularities related to instrumentation changes and growing loudness across\nmulti-decadal trends. This indicates that perceiving the era of a song from\nmusical features such as audio and artist information is possible. Music era\ninformation can be an important feature for playlist generation and\nrecommendation. However, the release year of a song can be inaccessible in many\ncircumstances. This paper addresses a novel task of music era recognition. We\nformulate the task as a music classification problem and propose solutions\nbased on supervised contrastive learning. An audio-based model is developed to\npredict the era from audio. For the case where the artist information is\navailable, we extend the audio-based model to take multimodal inputs and\ndevelop a framework, called MultiModal Contrastive (MMC) learning, to enhance\nthe training. Experimental result on Million Song Dataset demonstrates that the\naudio-based model achieves 54% in accuracy with a tolerance of 3-years range;\nincorporating the artist information with the MMC framework for training leads\nto 9% improvement further.", "published": "2024-07-07 13:43:55", "link": "http://arxiv.org/abs/2407.05368v1", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Research on the Acoustic Emission Source Localization Methodology in\n  Composite Materials based on Artificial Intelligence", "abstract": "In this study, methodology of acoustic emission source localization in\ncomposite materials based on artificial intelligence was presented. Carbon\nfiber reinforced plastic was selected for specimen, and acoustic emission\nsignal were measured using piezoelectric devices. The measured signal was\nwavelet-transformed to obtain scalograms, which were used as training data for\nthe artificial intelligence model. AESLNet(acoustic emission source\nlocalization network), proposed in this study, was constructed convolutional\nlayers in parallel due to anisotropy of the composited materials. It is\nregression model to detect the coordinates of acoustic emission source\nlocation. Hyper-parameter of network has been optimized by Bayesian\noptimization. It has been confirmed that network can detect location of\nacoustic emission source with an average error of 3.02mm and a resolution of\n20mm.", "published": "2024-07-07 15:12:04", "link": "http://arxiv.org/abs/2407.05405v1", "categories": ["cs.SD", "eess.AS", "physics.data-an"], "primary_category": "cs.SD"}
{"title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer\n  based on Supervised Semantic Tokens", "abstract": "Recent years have witnessed a trend that large language model (LLM) based\ntext-to-speech (TTS) emerges into the mainstream due to their high naturalness\nand zero-shot capacity. In this paradigm, speech signals are discretized into\ntoken sequences, which are modeled by an LLM with text as prompts and\nreconstructed by a token-based vocoder to waveforms. Obviously, speech tokens\nplay a critical role in LLM-based TTS models. Current speech tokens are learned\nin an unsupervised manner, which lacks explicit semantic information and\nalignment to the text. In this paper, we propose to represent speech with\nsupervised semantic tokens, which are derived from a multilingual speech\nrecognition model by inserting vector quantization into the encoder. Based on\nthe tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice,\nwhich consists of an LLM for text-to-token generation and a conditional flow\nmatching model for token-to-speech synthesis. Experimental results show that\nsupervised semantic tokens significantly outperform existing unsupervised\ntokens in terms of content consistency and speaker similarity for zero-shot\nvoice cloning. Moreover, we find that utilizing large-scale data further\nimproves the synthesis performance, indicating the scalable capacity of\nCosyVoice. To the best of our knowledge, this is the first attempt to involve\nsupervised speech tokens into TTS models.", "published": "2024-07-07 15:16:19", "link": "http://arxiv.org/abs/2407.05407v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Differentiable Modal Synthesis for Physical Modeling of Planar String\n  Sound and Motion Simulation", "abstract": "While significant advancements have been made in music generation and\ndifferentiable sound synthesis within machine learning and computer audition,\nthe simulation of instrument vibration guided by physical laws has been\nunderexplored. To address this gap, we introduce a novel model for simulating\nthe spatio-temporal motion of nonlinear strings, integrating modal synthesis\nand spectral modeling within a neural network framework. Our model leverages\nphysical properties and fundamental frequencies as inputs, outputting string\nstates across time and space that solve the partial differential equation\ncharacterizing the nonlinear string. Empirical evaluations demonstrate that the\nproposed architecture achieves superior accuracy in string motion simulation\ncompared to existing baseline architectures. The code and demo are available\nonline.", "published": "2024-07-07 23:36:51", "link": "http://arxiv.org/abs/2407.05516v2", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
