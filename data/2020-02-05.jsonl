{"title": "Discontinuous Constituent Parsing with Pointer Networks", "abstract": "One of the most complex syntactic representations used in computational\nlinguistics and NLP are discontinuous constituent trees, crucial for\nrepresenting all grammatical phenomena of languages such as German. Recent\nadvances in dependency parsing have shown that Pointer Networks excel in\nefficiently parsing syntactic relations between words in a sentence. This kind\nof sequence-to-sequence models achieve outstanding accuracies in building\nnon-projective dependency trees, but its potential has not been proved yet on a\nmore difficult task. We propose a novel neural network architecture that, by\nmeans of Pointer Networks, is able to generate the most accurate discontinuous\nconstituent representations to date, even without the need of Part-of-Speech\ntagging information. To do so, we internally model discontinuous constituent\nstructures as augmented non-projective dependency structures. The proposed\napproach achieves state-of-the-art results on the two widely-used NEGRA and\nTIGER benchmarks, outperforming previous work by a wide margin.", "published": "2020-02-05 15:12:03", "link": "http://arxiv.org/abs/2002.01824v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Parsing as Pretraining", "abstract": "Recent analyses suggest that encoders pretrained for language modeling\ncapture certain morpho-syntactic structure. However, probing frameworks for\nword vectors still do not report results on standard setups such as constituent\nand dependency parsing. This paper addresses this problem and does full parsing\n(on English) relying only on pretraining architectures -- and no decoding. We\nfirst cast constituent and dependency parsing as sequence tagging. We then use\na single feed-forward layer to directly map word vectors to labels that encode\na linearized tree. This is used to: (i) see how far we can reach on syntax\nmodelling with just pretrained encoders, and (ii) shed some light about the\nsyntax-sensitivity of different word vectors (by freezing the weights of the\npretraining network during training). For evaluation, we use bracketing\nF1-score and LAS, and analyze in-depth differences across representations for\nspan lengths and dependency displacements. The overall results surpass existing\nsequence tagging parsers on the PTB (93.5%) and end-to-end EN-EWT UD (78.8%).", "published": "2020-02-05 08:43:02", "link": "http://arxiv.org/abs/2002.01685v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Fusion Chinese WordNet (MCW) : Compound of Machine Learning and\n  Manual Correction", "abstract": "Princeton WordNet (PWN) is a lexicon-semantic network based on cognitive\nlinguistics, which promotes the development of natural language processing.\nBased on PWN, five Chinese wordnets have been developed to solve the problems\nof syntax and semantics. They include: Northeastern University Chinese WordNet\n(NEW), Sinica Bilingual Ontological WordNet (BOW), Southeast University Chinese\nWordNet (SEW), Taiwan University Chinese WordNet (CWN), Chinese Open WordNet\n(COW). By using them, we found that these word networks have low accuracy and\ncoverage, and cannot completely portray the semantic network of PWN. So we\ndecided to make a new Chinese wordnet called Multi-Fusion Chinese Wordnet (MCW)\nto make up those shortcomings. The key idea is to extend the SEW with the help\nof Oxford bilingual dictionary and Xinhua bilingual dictionary, and then\ncorrect it. More specifically, we used machine learning and manual adjustment\nin our corrections. Two standards were formulated to help our work. We\nconducted experiments on three tasks including relatedness calculation, word\nsimilarity and word sense disambiguation for the comparison of lemma's\naccuracy, at the same time, coverage also was compared. The results indicate\nthat MCW can benefit from coverage and accuracy via our method. However, it\nstill has room for improvement, especially with lemmas. In the future, we will\ncontinue to enhance the accuracy of MCW and expand the concepts in it.", "published": "2020-02-05 12:44:01", "link": "http://arxiv.org/abs/2002.01761v1", "categories": ["cs.CL", "cs.AI", "I.7.0"], "primary_category": "cs.CL"}
{"title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters", "abstract": "We study the problem of injecting knowledge into large pre-trained models\nlike BERT and RoBERTa. Existing methods typically update the original\nparameters of pre-trained models when injecting knowledge. However, when\nmultiple kinds of knowledge are injected, the historically injected knowledge\nwould be flushed away. To address this, we propose K-Adapter, a framework that\nretains the original parameters of the pre-trained model fixed and supports the\ndevelopment of versatile knowledge-infused model. Taking RoBERTa as the\nbackbone model, K-Adapter has a neural adapter for each kind of infused\nknowledge, like a plug-in connected to RoBERTa. There is no information flow\nbetween different adapters, thus multiple adapters can be efficiently trained\nin a distributed way. As a case study, we inject two kinds of knowledge in this\nwork, including (1) factual knowledge obtained from automatically aligned\ntext-triplets on Wikipedia and Wikidata and (2) linguistic knowledge obtained\nvia dependency parsing. Results on three knowledge-driven tasks, including\nrelation classification, entity typing, and question answering, demonstrate\nthat each adapter improves the performance and the combination of both adapters\nbrings further improvements. Further analysis indicates that K-Adapter captures\nversatile knowledge than RoBERTa.", "published": "2020-02-05 14:30:49", "link": "http://arxiv.org/abs/2002.01808v5", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B,\n  Phase-B", "abstract": "In this paper, we detail our submission to the 2019, 7th year, BioASQ\ncompetition. We present our approach for Task-7b, Phase B, Exact Answering\nTask. These Question Answering (QA) tasks include Factoid, Yes/No, List Type\nQuestion answering. Our system is based on a contextual word embedding model.\nWe have used a Bidirectional Encoder Representations from Transformers(BERT)\nbased system, fined tuned for biomedical question answering task using BioBERT.\nIn the third test batch set, our system achieved the highest MRR score for\nFactoid Question Answering task. Also, for List type question answering task\nour system achieved the highest recall score in the fourth test batch set.\nAlong with our detailed approach, we present the results for our submissions,\nand also highlight identified downsides for our current approach and ways to\nimprove them in our future experiments.", "published": "2020-02-05 20:43:14", "link": "http://arxiv.org/abs/2002.01984v1", "categories": ["cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Aligning the Pretraining and Finetuning Objectives of Language Models", "abstract": "We demonstrate that explicitly aligning the pretraining objectives to the\nfinetuning objectives in language model training significantly improves the\nfinetuning task performance and reduces the minimum amount of finetuning\nexamples required. The performance margin gained from objective alignment\nallows us to build language models with smaller sizes for tasks with less\navailable training data. We provide empirical evidence of these claims by\napplying objective alignment to concept-of-interest tagging and acronym\ndetection tasks. We found that, with objective alignment, our 768 by 3 and 512\nby 3 transformer language models can reach accuracy of 83.9%/82.5% for\nconcept-of-interest tagging and 73.8%/70.2% for acronym detection using only\n200 finetuning examples per task, outperforming the 768 by 3 model pretrained\nwithout objective alignment by +4.8%/+3.4% and +9.9%/+6.3%. We name finetuning\nsmall language models in the presence of hundreds of training examples or less\n\"Few Example learning\". In practice, Few Example Learning enabled by objective\nalignment not only saves human labeling costs, but also makes it possible to\nleverage language models in more real-time applications.", "published": "2020-02-05 21:40:50", "link": "http://arxiv.org/abs/2002.02000v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stimulating Creativity with FunLines: A Case Study of Humor Generation\n  in Headlines", "abstract": "Building datasets of creative text, such as humor, is quite challenging. We\nintroduce FunLines, a competitive game where players edit news headlines to\nmake them funny, and where they rate the funniness of headlines edited by\nothers. FunLines makes the humor generation process fun, interactive,\ncollaborative, rewarding and educational, keeping players engaged and providing\nhumor data at a very low cost compared to traditional crowdsourcing approaches.\nFunLines offers useful performance feedback, assisting players in getting\nbetter over time at generating and assessing humor, as our analysis shows. This\nhelps to further increase the quality of the generated dataset. We show the\neffectiveness of this data by training humor classification models that\noutperform a previous benchmark, and we release this dataset to the public.", "published": "2020-02-05 22:56:11", "link": "http://arxiv.org/abs/2002.02031v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Identification of Indian Languages using Ghost-VLAD pooling", "abstract": "In this work, we propose a new pooling strategy for language identification\nby considering Indian languages. The idea is to obtain utterance level features\nfor any variable length audio for robust language recognition. We use the\nGhostVLAD approach to generate an utterance level feature vector for any\nvariable length input audio by aggregating the local frame level features\nacross time. The generated feature vector is shown to have very good language\ndiscriminative features and helps in getting state of the art results for\nlanguage identification task. We conduct our experiments on 635Hrs of audio\ndata for 7 Indian languages. Our method outperforms the previous state of the\nart x-vector [11] method by an absolute improvement of 1.88% in F1-score and\nachieves 98.43% F1-score on the held-out test data. We compare our system with\nvarious pooling approaches and show that GhostVLAD is the best pooling approach\nfor this task. We also provide visualization of the utterance level embeddings\ngenerated using Ghost-VLAD pooling and show that this method creates embeddings\nwhich has very good language discriminative features.", "published": "2020-02-05 07:07:15", "link": "http://arxiv.org/abs/2002.01664v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Geosocial Location Classification: Associating Type to Places Based on\n  Geotagged Social-Media Posts", "abstract": "Associating type to locations can be used to enrich maps and can serve a\nplethora of geospatial applications. An automatic method to do so could make\nthe process less expensive in terms of human labor, and faster to react to\nchanges. In this paper we study the problem of Geosocial Location\nClassification, where the type of a site, e.g., a building, is discovered based\non social-media posts. Our goal is to correctly associate a set of messages\nposted in a small radius around a given location with the corresponding\nlocation type, e.g., school, church, restaurant or museum. We explore two\napproaches to the problem: (a) a pipeline approach, where each message is first\nclassified, and then the location associated with the message set is inferred\nfrom the individual message labels; and (b) a joint approach where the\nindividual messages are simultaneously processed to yield the desired location\ntype. We tested the two approaches over a dataset of geotagged tweets. Our\nresults demonstrate the superiority of the joint approach. Moreover, we show\nthat due to the unique structure of the problem, where weakly-related messages\nare jointly processed to yield a single final label, linear classifiers\noutperform deep neural network alternatives.", "published": "2020-02-05 16:09:52", "link": "http://arxiv.org/abs/2002.01846v2", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific\n  Business Documents", "abstract": "Techniques for automatically extracting important content elements from\nbusiness documents such as contracts, statements, and filings have the\npotential to make business operations more efficient. This problem can be\nformulated as a sequence labeling task, and we demonstrate the adaption of BERT\nto two types of business documents: regulatory filings and property lease\nagreements. There are aspects of this problem that make it easier than\n\"standard\" information extraction tasks and other aspects that make it more\ndifficult, but on balance we find that modest amounts of annotated data (less\nthan 100 documents) are sufficient to achieve reasonable accuracy. We integrate\nour models into an end-to-end cloud platform that provides both an easy-to-use\nannotation interface as well as an inference interface that allows users to\nupload documents and inspect model outputs.", "published": "2020-02-05 16:45:44", "link": "http://arxiv.org/abs/2002.01861v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "If I Hear You Correctly: Building and Evaluating Interview Chatbots with\n  Active Listening Skills", "abstract": "Interview chatbots engage users in a text-based conversation to draw out\ntheir views and opinions. It is, however, challenging to build effective\ninterview chatbots that can handle user free-text responses to open-ended\nquestions and deliver engaging user experience. As the first step, we are\ninvestigating the feasibility and effectiveness of using publicly available,\npractical AI technologies to build effective interview chatbots. To demonstrate\nfeasibility, we built a prototype scoped to enable interview chatbots with a\nsubset of active listening skills - the abilities to comprehend a user's input\nand respond properly. To evaluate the effectiveness of our prototype, we\ncompared the performance of interview chatbots with or without active listening\nskills on four common interview topics in a live evaluation with 206 users. Our\nwork presents practical design implications for building effective interview\nchatbots, hybrid chatbot platforms, and empathetic chatbots beyond interview\ntasks.", "published": "2020-02-05 16:52:52", "link": "http://arxiv.org/abs/2002.01862v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Goal-Oriented Multi-Task BERT-Based Dialogue State Tracker", "abstract": "Dialogue State Tracking (DST) is a core component of virtual assistants such\nas Alexa or Siri. To accomplish various tasks, these assistants need to support\nan increasing number of services and APIs. The Schema-Guided State Tracking\ntrack of the 8th Dialogue System Technology Challenge highlighted the DST\nproblem for unseen services. The organizers introduced the Schema-Guided\nDialogue (SGD) dataset with multi-domain conversations and released a zero-shot\ndialogue state tracking model. In this work, we propose a GOaL-Oriented\nMulti-task BERT-based dialogue state tracker (GOLOMB) inspired by architectures\nfor reading comprehension question answering systems. The model \"queries\"\ndialogue history with descriptions of slots and services as well as possible\nvalues of slots. This allows to transfer slot values in multi-domain dialogues\nand have a capability to scale to unseen slot types. Our model achieves a joint\ngoal accuracy of 53.97% on the SGD dataset, outperforming the baseline model.", "published": "2020-02-05 22:56:12", "link": "http://arxiv.org/abs/2002.02450v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Fake News Detection on News-Oriented Heterogeneous Information Networks\n  through Hierarchical Graph Attention", "abstract": "The viral spread of fake news has caused great social harm, making fake news\ndetection an urgent task. Current fake news detection methods rely heavily on\ntext information by learning the extracted news content or writing style of\ninternal knowledge. However, deliberate rumors can mask writing style,\nbypassing language models and invalidating simple text-based models. In fact,\nnews articles and other related components (such as news creators and news\ntopics) can be modeled as a heterogeneous information network (HIN for short).\nIn this paper, we propose a novel fake news detection framework, namely\nHierarchical Graph Attention Network(HGAT), which uses a novel hierarchical\nattention mechanism to perform node representation learning in HIN, and then\ndetects fake news by classifying news article nodes. Experiments on two\nreal-world fake news datasets show that HGAT can outperform text-based models\nand other network-based models. In addition, the experiment proved the\nexpandability and generalizability of our for graph representation learning and\nother node classification related applications in heterogeneous graphs.", "published": "2020-02-05 19:09:13", "link": "http://arxiv.org/abs/2002.04397v2", "categories": ["cs.SI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Prediction of head motion from speech waveforms with a\n  canonical-correlation-constrained autoencoder", "abstract": "This study investigates the direct use of speech waveforms to predict head\nmotion for speech-driven head-motion synthesis, whereas the use of spectral\nfeatures such as MFCC as basic input features together with additional features\nsuch as energy and F0 is common in the literature. We show that, rather than\ncombining different features that originate from waveforms, it is more\neffective to use waveforms directly predicting corresponding head motion. The\nchallenge with the waveform-based approach is that waveforms contain a large\namount of information irrelevant to predict head motion, which hinders the\ntraining of neural networks. To overcome the problem, we propose a\ncanonical-correlation-constrained autoencoder (CCCAE), where hidden layers are\ntrained to not only minimise the error but also maximise the canonical\ncorrelation with head motion. Compared with an MFCC-based system, the proposed\nsystem shows comparable performance in objective evaluation, and better\nperformance in subject evaluation.", "published": "2020-02-05 17:08:58", "link": "http://arxiv.org/abs/2002.01869v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Spatial and spectral deep attention fusion for multi-channel speech\n  separation using deep embedding features", "abstract": "Multi-channel deep clustering (MDC) has acquired a good performance for\nspeech separation. However, MDC only applies the spatial features as the\nadditional information. So it is difficult to learn mutual relationship between\nspatial and spectral features. Besides, the training objective of MDC is\ndefined at embedding vectors, rather than real separated sources, which may\ndamage the separation performance. In this work, we propose a deep attention\nfusion method to dynamically control the weights of the spectral and spatial\nfeatures and combine them deeply. In addition, to solve the training objective\nproblem of MDC, the real separated sources are used as the training objectives.\nSpecifically, we apply the deep clustering network to extract deep embedding\nfeatures. Instead of using the unsupervised K-means clustering to estimate\nbinary masks, another supervised network is utilized to learn soft masks from\nthese deep embedding features. Our experiments are conducted on a spatialized\nreverberant version of WSJ0-2mix dataset. Experimental results show that the\nproposed method outperforms MDC baseline and even better than the oracle ideal\nbinary mask (IBM).", "published": "2020-02-05 03:49:39", "link": "http://arxiv.org/abs/2002.01626v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Limitations of weak labels for embedding and tagging", "abstract": "Many datasets and approaches in ambient sound analysis use weakly labeled\ndata.Weak labels are employed because annotating every data sample with a\nstrong label is too expensive.Yet, their impact on the performance in\ncomparison to strong labels remains unclear.Indeed, weak labels must often be\ndealt with at the same time as other challenges, namely multiple labels per\nsample, unbalanced classes and/or overlapping events.In this paper, we\nformulate a supervised learning problem which involves weak labels.We create a\ndataset that focuses on the difference between strong and weak labels as\nopposed to other challenges. We investigate the impact of weak labels when\ntraining an embedding or an end-to-end classifier.Different experimental\nscenarios are discussed to provide insights into which applications are most\nsensitive to weakly labeled data.", "published": "2020-02-05 08:54:08", "link": "http://arxiv.org/abs/2002.01687v4", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Speaker Verification Backend for Improved Calibration Performance\n  across Varying Conditions", "abstract": "In a recent work, we presented a discriminative backend for speaker\nverification that achieved good out-of-the-box calibration performance on most\ntested conditions containing varying levels of mismatch to the training\nconditions. This backend mimics the standard PLDA-based backend process used in\nmost current speaker verification systems, including the calibration stage. All\nparameters of the backend are jointly trained to optimize the binary\ncross-entropy for the speaker verification task. Calibration robustness is\nachieved by making the parameters of the calibration stage a function of\nvectors representing the conditions of the signal, which are extracted using a\nmodel trained to predict condition labels. In this work, we propose a\nsimplified version of this backend where the vectors used to compute the\ncalibration parameters are estimated within the backend, without the need for a\ncondition prediction model. We show that this simplified method provides\nsimilar performance to the previously proposed method while being simpler to\nimplement, and having less requirements on the training data. Further, we\nprovide an analysis of different aspects of the method including the effect of\ninitialization, the nature of the vectors used to compute the calibration\nparameters, and the effect that the random seed and the number of training\nepochs has on performance. We also compare the proposed method with the\ntrial-based calibration (TBC) method that, to our knowledge, was the\nstate-of-the-art for achieving good calibration across varying conditions. We\nshow that the proposed method outperforms TBC while also being several orders\nof magnitude faster to run, comparable to the standard PLDA baseline.", "published": "2020-02-05 15:37:46", "link": "http://arxiv.org/abs/2002.03802v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Vocoder-free End-to-End Voice Conversion with Transformer Network", "abstract": "Mel-frequency filter bank (MFB) based approaches have the advantage of\nlearning speech compared to raw spectrum since MFB has less feature size.\nHowever, speech generator with MFB approaches require additional vocoder that\nneeds a huge amount of computation expense for training process. The additional\npre/post processing such as MFB and vocoder is not essential to convert real\nhuman speech to others. It is possible to only use the raw spectrum along with\nthe phase to generate different style of voices with clear pronunciation. In\nthis regard, we propose a fast and effective approach to convert realistic\nvoices using raw spectrum in a parallel manner. Our transformer-based model\narchitecture which does not have any CNN or RNN layers has shown the advantage\nof learning fast and solved the limitation of sequential computation of\nconventional RNN. In this paper, we introduce a vocoder-free end-to-end voice\nconversion method using transformer network. The presented conversion model can\nalso be used in speaker adaptation for speech recognition. Our approach can\nconvert the source voice to a target voice without using MFB and vocoder. We\ncan get an adapted MFB for speech recognition by multiplying the converted\nmagnitude with phase. We perform our voice conversion experiments on TIDIGITS\ndataset using the metrics such as naturalness, similarity, and clarity with\nmean opinion score, respectively.", "published": "2020-02-05 06:19:24", "link": "http://arxiv.org/abs/2002.03808v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
