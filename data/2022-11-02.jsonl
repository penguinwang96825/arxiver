{"title": "Title2Event: Benchmarking Open Event Extraction with a Large-scale\n  Chinese Title Dataset", "abstract": "Event extraction (EE) is crucial to downstream tasks such as new aggregation\nand event knowledge graph construction. Most existing EE datasets manually\ndefine fixed event types and design specific schema for each of them, failing\nto cover diverse events emerging from the online text. Moreover, news titles,\nan important source of event mentions, have not gained enough attention in\ncurrent EE research. In this paper, We present Title2Event, a large-scale\nsentence-level dataset benchmarking Open Event Extraction without restricting\nevent types. Title2Event contains more than 42,000 news titles in 34 topics\ncollected from Chinese web pages. To the best of our knowledge, it is currently\nthe largest manually-annotated Chinese dataset for open event extraction. We\nfurther conduct experiments on Title2Event with different models and show that\nthe characteristics of titles make it challenging for event extraction,\naddressing the significance of advanced study on this problem. The dataset and\nbaseline codes are available at https://open-event-hub.github.io/title2event.", "published": "2022-11-02 04:39:36", "link": "http://arxiv.org/abs/2211.00869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Syntactically Controlled Paraphrase Generation with\n  Abstract Meaning Representations", "abstract": "Syntactically controlled paraphrase generation has become an emerging\nresearch direction in recent years. Most existing approaches require annotated\nparaphrase pairs for training and are thus costly to extend to new domains.\nUnsupervised approaches, on the other hand, do not need paraphrase pairs but\nsuffer from relatively poor performance in terms of syntactic control and\nquality of generated paraphrases. In this paper, we demonstrate that leveraging\nAbstract Meaning Representations (AMR) can greatly improve the performance of\nunsupervised syntactically controlled paraphrase generation. Our proposed\nmodel, AMR-enhanced Paraphrase Generator (AMRPG), separately encodes the AMR\ngraph and the constituency parse of the input sentence into two disentangled\nsemantic and syntactic embeddings. A decoder is then learned to reconstruct the\ninput sentence from the semantic and syntactic embeddings. Our experiments show\nthat AMRPG generates more accurate syntactically controlled paraphrases, both\nquantitatively and qualitatively, compared to the existing unsupervised\napproaches. We also demonstrate that the paraphrases generated by AMRPG can be\nused for data augmentation to improve the robustness of NLP models.", "published": "2022-11-02 04:58:38", "link": "http://arxiv.org/abs/2211.00881v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PLATO-K: Internal and External Knowledge Enhanced Dialogue Generation", "abstract": "Recently, the practical deployment of open-domain dialogue systems has been\nplagued by the knowledge issue of information deficiency and factual\ninaccuracy. To this end, we introduce PLATO-K based on two-stage dialogic\nlearning to strengthen internal knowledge memorization and external knowledge\nexploitation. In the first stage, PLATO-K learns through massive dialogue\ncorpora and memorizes essential knowledge into model parameters. In the second\nstage, PLATO-K mimics human beings to search for external information and to\nleverage the knowledge in response generation. Extensive experiments reveal\nthat the knowledge issue is alleviated significantly in PLATO-K with such\ncomprehensive internal and external knowledge enhancement. Compared to the\nexisting state-of-the-art Chinese dialogue model, the overall engagingness of\nPLATO-K is improved remarkably by 36.2% and 49.2% on chit-chat and\nknowledge-intensive conversations.", "published": "2022-11-02 06:23:16", "link": "http://arxiv.org/abs/2211.00910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialect-robust Evaluation of Generated Text", "abstract": "Evaluation metrics that are not robust to dialect variation make it\nimpossible to tell how well systems perform for many groups of users, and can\neven penalize systems for producing text in lower-resource dialects. However,\ncurrently, there exists no way to quantify how metrics respond to change in the\ndialect of a generated utterance. We thus formalize dialect robustness and\ndialect awareness as goals for NLG evaluation metrics. We introduce a suite of\nmethods and corresponding statistical tests one can use to assess metrics in\nlight of the two goals. Applying the suite to current state-of-the-art metrics,\nwe demonstrate that they are not dialect-robust and that semantic perturbations\nfrequently lead to smaller decreases in a metric than the introduction of\ndialect features. As a first step to overcome this limitation, we propose a\ntraining schema, NANO, which introduces regional and language information to\nthe pretraining process of a metric. We demonstrate that NANO provides a\nsize-efficient way for models to improve the dialect robustness while\nsimultaneously improving their performance on the standard metric benchmark.", "published": "2022-11-02 07:12:23", "link": "http://arxiv.org/abs/2211.00922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Processing Long Legal Documents with Pre-trained Transformers: Modding\n  LegalBERT and Longformer", "abstract": "Pre-trained Transformers currently dominate most NLP tasks. They impose,\nhowever, limits on the maximum input length (512 sub-words in BERT), which are\ntoo restrictive in the legal domain. Even sparse-attention models, such as\nLongformer and BigBird, which increase the maximum input length to 4,096\nsub-words, severely truncate texts in three of the six datasets of LexGLUE.\nSimpler linear classifiers with TF-IDF features can handle texts of any length,\nrequire far less resources to train and deploy, but are usually outperformed by\npre-trained Transformers. We explore two directions to cope with long legal\ntexts: (i) modifying a Longformer warm-started from LegalBERT to handle even\nlonger texts (up to 8,192 sub-words), and (ii) modifying LegalBERT to use\nTF-IDF representations. The first approach is the best in terms of performance,\nsurpassing a hierarchical version of LegalBERT, which was the previous state of\nthe art in LexGLUE. The second approach leads to computationally more efficient\nmodels at the expense of lower performance, but the resulting models still\noutperform overall a linear SVM with TF-IDF features in long legal document\nclassification.", "published": "2022-11-02 09:27:01", "link": "http://arxiv.org/abs/2211.00974v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-level Distillation of Semantic Knowledge for Pre-training\n  Multilingual Language Model", "abstract": "Pre-trained multilingual language models play an important role in\ncross-lingual natural language understanding tasks. However, existing methods\ndid not focus on learning the semantic structure of representation, and thus\ncould not optimize their performance. In this paper, we propose Multi-level\nMultilingual Knowledge Distillation (MMKD), a novel method for improving\nmultilingual language models. Specifically, we employ a teacher-student\nframework to adopt rich semantic representation knowledge in English BERT. We\npropose token-, word-, sentence-, and structure-level alignment objectives to\nencourage multiple levels of consistency between source-target pairs and\ncorrelation similarity between teacher and student models. We conduct\nexperiments on cross-lingual evaluation benchmarks including XNLI, PAWS-X, and\nXQuAD. Experimental results show that MMKD outperforms other baseline models of\nsimilar size on XNLI and XQuAD and obtains comparable performance on PAWS-X.\nEspecially, MMKD obtains significant performance gains on low-resource\nlanguages.", "published": "2022-11-02 15:23:13", "link": "http://arxiv.org/abs/2211.01200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Intrinsic Compositionality in Transformers with Tree\n  Projections", "abstract": "When trained on language data, do transformers learn some arbitrary\ncomputation that utilizes the full capacity of the architecture or do they\nlearn a simpler, tree-like computation, hypothesized to underlie compositional\nmeaning systems like human languages? There is an apparent tension between\ncompositional accounts of human language understanding, which are based on a\nrestricted bottom-up computational process, and the enormous success of neural\nmodels like transformers, which can route information arbitrarily between\ndifferent parts of their input. One possibility is that these models, while\nextremely flexible in principle, in practice learn to interpret language\nhierarchically, ultimately building sentence representations close to those\npredictable by a bottom-up, tree-structured model. To evaluate this\npossibility, we describe an unsupervised and parameter-free method to\n\\emph{functionally project} the behavior of any transformer into the space of\ntree-structured networks. Given an input sentence, we produce a binary tree\nthat approximates the transformer's representation-building process and a score\nthat captures how \"tree-like\" the transformer's behavior is on the input. While\ncalculation of this score does not require training any additional models, it\nprovably upper-bounds the fit between a transformer and any tree-structured\napproximation. Using this method, we show that transformers for three different\ntasks become more tree-like over the course of training, in some cases\nunsupervisedly recovering the same trees as supervised parsers. These trees, in\nturn, are predictive of model behavior, with more tree-like models generalizing\nbetter on tests of compositional generalization.", "published": "2022-11-02 17:10:07", "link": "http://arxiv.org/abs/2211.01288v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MT-GenEval: A Counterfactual and Contextual Dataset for Evaluating\n  Gender Accuracy in Machine Translation", "abstract": "As generic machine translation (MT) quality has improved, the need for\ntargeted benchmarks that explore fine-grained aspects of quality has increased.\nIn particular, gender accuracy in translation can have implications in terms of\noutput fluency, translation accuracy, and ethics. In this paper, we introduce\nMT-GenEval, a benchmark for evaluating gender accuracy in translation from\nEnglish into eight widely-spoken languages. MT-GenEval complements existing\nbenchmarks by providing realistic, gender-balanced, counterfactual data in\neight language pairs where the gender of individuals is unambiguous in the\ninput segment, including multi-sentence segments requiring inter-sentential\ngender agreement. Our data and code is publicly available under a CC BY SA 3.0\nlicense.", "published": "2022-11-02 17:55:43", "link": "http://arxiv.org/abs/2211.01355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchies over Vector Space: Orienting Word and Graph Embeddings", "abstract": "Word and graph embeddings are widely used in deep learning applications. We\npresent a data structure that captures inherent hierarchical properties from an\nunordered flat embedding space, particularly a sense of direction between pairs\nof entities. Inspired by the notion of \\textit{distributional generality}, our\nalgorithm constructs an arborescence (a directed rooted tree) by inserting\nnodes in descending order of entity power (e.g., word frequency), pointing each\nentity to the closest more powerful node as its parent.\n  We evaluate the performance of the resulting tree structures on three tasks:\nhypernym relation discovery, least-common-ancestor (LCA) discovery among words,\nand Wikipedia page link recovery. We achieve average 8.98\\% and 2.70\\% for\nhypernym and LCA discovery across five languages and 62.76\\% accuracy on\ndirected Wiki-page link recovery, with both substantially above baselines.\nFinally, we investigate the effect of insertion order, the power/similarity\ntrade-off and various power sources to optimize parent selection.", "published": "2022-11-02 18:54:44", "link": "http://arxiv.org/abs/2211.01430v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Entity-to-Entity Stance Detection with Knowledge Graph\n  Augmentation", "abstract": "Stance detection is typically framed as predicting the sentiment in a given\ntext towards a target entity. However, this setup overlooks the importance of\nthe source entity, i.e., who is expressing the opinion. In this paper, we\nemphasize the need for studying interactions among entities when inferring\nstances. We first introduce a new task, entity-to-entity (E2E) stance\ndetection, which primes models to identify entities in their canonical names\nand discern stances jointly. To support this study, we curate a new dataset\nwith 10,619 annotations labeled at the sentence-level from news articles of\ndifferent ideological leanings. We present a novel generative framework to\nallow the generation of canonical names for entities as well as stances among\nthem. We further enhance the model with a graph encoder to summarize entity\nactivities and external knowledge surrounding the entities. Experiments show\nthat our model outperforms strong comparisons by large margins. Further\nanalyses demonstrate the usefulness of E2E stance detection for understanding\nmedia quotation and stance landscape, as well as inferring entity ideology.", "published": "2022-11-02 20:16:42", "link": "http://arxiv.org/abs/2211.01467v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradient Knowledge Distillation for Pre-trained Language Models", "abstract": "Knowledge distillation (KD) is an effective framework to transfer knowledge\nfrom a large-scale teacher to a compact yet well-performing student. Previous\nKD practices for pre-trained language models mainly transfer knowledge by\naligning instance-wise outputs between the teacher and student, while\nneglecting an important knowledge source, i.e., the gradient of the teacher.\nThe gradient characterizes how the teacher responds to changes in inputs, which\nwe assume is beneficial for the student to better approximate the underlying\nmapping function of the teacher. Therefore, we propose Gradient Knowledge\nDistillation (GKD) to incorporate the gradient alignment objective into the\ndistillation process. Experimental results show that GKD outperforms previous\nKD methods regarding student performance. Further analysis shows that\nincorporating gradient knowledge makes the student behave more consistently\nwith the teacher, improving the interpretability greatly.", "published": "2022-11-02 12:07:16", "link": "http://arxiv.org/abs/2211.01071v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Vector Retrieval as Sparse Alignment", "abstract": "Multi-vector retrieval models improve over single-vector dual encoders on\nmany information retrieval tasks. In this paper, we cast the multi-vector\nretrieval problem as sparse alignment between query and document tokens. We\npropose AligneR, a novel multi-vector retrieval model that learns sparsified\npairwise alignments between query and document tokens (e.g. `dog' vs. `puppy')\nand per-token unary saliences reflecting their relative importance for\nretrieval. We show that controlling the sparsity of pairwise token alignments\noften brings significant performance gains. While most factoid questions\nfocusing on a specific part of a document require a smaller number of\nalignments, others requiring a broader understanding of a document favor a\nlarger number of alignments. Unary saliences, on the other hand, decide whether\na token ever needs to be aligned with others for retrieval (e.g. `kind' from\n`kind of currency is used in new zealand}'). With sparsified unary saliences,\nwe are able to prune a large number of query and document token vectors and\nimprove the efficiency of multi-vector retrieval. We learn the sparse unary\nsaliences with entropy-regularized linear programming, which outperforms other\nmethods to achieve sparsity. In a zero-shot setting, AligneR scores 51.1 points\nnDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the\nBEIR benchmark. In addition, adapting pairwise alignments with a few examples\n(<= 8) further improves the performance up to 15.7 points nDCG@10 for argument\nretrieval tasks. The unary saliences of AligneR helps us to keep only 20% of\nthe document token representations with minimal performance loss. We further\nshow that our model often produces interpretable alignments and significantly\nimproves its performance when initialized from larger language models.", "published": "2022-11-02 16:49:58", "link": "http://arxiv.org/abs/2211.01267v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Boosting word frequencies in authorship attribution", "abstract": "In this paper, I introduce a simple method of computing relative word\nfrequencies for authorship attribution and similar stylometric tasks. Rather\nthan computing relative frequencies as the number of occurrences of a given\nword divided by the total number of tokens in a text, I argue that a more\nefficient normalization factor is the total number of relevant tokens only. The\nnotion of relevant words includes synonyms and, usually, a few dozen other\nwords in some ways semantically similar to a word in question. To determine\nsuch a semantic background, one of word embedding models can be used. The\nproposed method outperforms classical most-frequent-word approaches\nsubstantially, usually by a few percentage points depending on the input\nsettings.", "published": "2022-11-02 17:11:35", "link": "http://arxiv.org/abs/2211.01289v1", "categories": ["cs.CL", "cs.LG", "62H30", "I.7"], "primary_category": "cs.CL"}
{"title": "Learning an Artificial Language for Knowledge-Sharing in Multilingual\n  Translation", "abstract": "The cornerstone of multilingual neural translation is shared representations\nacross languages. Given the theoretically infinite representation power of\nneural networks, semantically identical sentences are likely represented\ndifferently. While representing sentences in the continuous latent space\nensures expressiveness, it introduces the risk of capturing of irrelevant\nfeatures which hinders the learning of a common representation. In this work,\nwe discretize the encoder output latent space of multilingual models by\nassigning encoder states to entries in a codebook, which in effect represents\nsource sentences in a new artificial language. This discretization process not\nonly offers a new way to interpret the otherwise black-box model\nrepresentations, but, more importantly, gives potential for increasing\nrobustness in unseen testing conditions. We validate our approach on\nlarge-scale experiments with realistic data volumes and domains. When tested in\nzero-shot conditions, our approach is competitive with two strong alternatives\nfrom the literature. We also use the learned artificial language to analyze\nmodel behavior, and discover that using a similar bridge language increases\nknowledge-sharing among the remaining languages.", "published": "2022-11-02 17:14:42", "link": "http://arxiv.org/abs/2211.01292v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese", "abstract": "The tremendous success of CLIP (Radford et al., 2021) has promoted the\nresearch and application of contrastive learning for vision-language\npretraining. In this work, we construct a large-scale dataset of image-text\npairs in Chinese, where most data are retrieved from publicly available\ndatasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5\nChinese CLIP models of multiple sizes, spanning from 77 to 958 million\nparameters. Furthermore, we propose a two-stage pretraining method, where the\nmodel is first trained with the image encoder frozen and then trained with all\nparameters being optimized, to achieve enhanced model performance. Our\ncomprehensive experiments demonstrate that Chinese CLIP can achieve the\nstate-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups\nof zero-shot learning and finetuning, and it is able to achieve competitive\nperformance in zero-shot image classification based on the evaluation on the\nELEVATER benchmark (Li et al., 2022). We have released our codes, models, and\ndemos in https://github.com/OFA-Sys/Chinese-CLIP", "published": "2022-11-02 17:47:23", "link": "http://arxiv.org/abs/2211.01335v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cross-stitching Text and Knowledge Graph Encoders for Distantly\n  Supervised Relation Extraction", "abstract": "Bi-encoder architectures for distantly-supervised relation extraction are\ndesigned to make use of the complementary information found in text and\nknowledge graphs (KG). However, current architectures suffer from two\ndrawbacks. They either do not allow any sharing between the text encoder and\nthe KG encoder at all, or, in case of models with KG-to-text attention, only\nshare information in one direction. Here, we introduce cross-stitch\nbi-encoders, which allow full interaction between the text encoder and the KG\nencoder via a cross-stitch mechanism. The cross-stitch mechanism allows sharing\nand updating representations between the two encoders at any layer, with the\namount of sharing being dynamically controlled via cross-attention-based gates.\nExperimental results on two relation extraction benchmarks from two different\ndomains show that enabling full interaction between the two encoders yields\nstrong improvements.", "published": "2022-11-02 19:01:26", "link": "http://arxiv.org/abs/2211.01432v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Assessing Resource-Performance Trade-off of Natural Language Models\n  using Data Envelopment Analysis", "abstract": "Natural language models are often summarized through a high-dimensional set\nof descriptive metrics including training corpus size, training time, the\nnumber of trainable parameters, inference times, and evaluation statistics that\nassess performance across tasks. The high dimensional nature of these metrics\nyields challenges with regard to objectively comparing models; in particular it\nis challenging to assess the trade-off models make between performance and\nresources (compute time, memory, etc.).\n  We apply Data Envelopment Analysis (DEA) to this problem of assessing the\nresource-performance trade-off. DEA is a nonparametric method that measures\nproductive efficiency of abstract units that consume one or more inputs and\nyield at least one output. We recast natural language models as units suitable\nfor DEA, and we show that DEA can be used to create an effective framework for\nquantifying model performance and efficiency. A central feature of DEA is that\nit identifies a subset of models that live on an efficient frontier of\nperformance. DEA is also scalable, having been applied to problems with\nthousands of units. We report empirical results of DEA applied to 14 different\nlanguage models that have a variety of architectures, and we show that DEA can\nbe used to identify a subset of models that effectively balance resource\ndemands against performance.", "published": "2022-11-02 21:17:00", "link": "http://arxiv.org/abs/2211.01486v1", "categories": ["cs.CL", "math.OC"], "primary_category": "cs.CL"}
{"title": "Numerical Optimizations for Weighted Low-rank Estimation on Language\n  Model", "abstract": "Singular value decomposition (SVD) is one of the most popular compression\nmethods that approximate a target matrix with smaller matrices. However,\nstandard SVD treats the parameters within the matrix with equal importance,\nwhich is a simple but unrealistic assumption. The parameters of a trained\nneural network model may affect task performance unevenly, which suggests\nnon-equal importance among the parameters. Compared to SVD, the decomposition\nmethod aware of parameter importance is the more practical choice in real\ncases. Unlike standard SVD, weighted value decomposition is a non-convex\noptimization problem that lacks a closed-form solution. We systematically\ninvestigated multiple optimization strategies to tackle the problem and\nexamined our method by compressing Transformer-based language models. Further,\nwe designed a metric to predict when the SVD may introduce a significant\nperformance drop, for which our method can be a rescue strategy. The extensive\nevaluations demonstrate that our method can perform better than current SOTA\nmethods in compressing Transformer-based language models.", "published": "2022-11-02 00:58:02", "link": "http://arxiv.org/abs/2211.09718v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Adversarial Training Can Improve Neural Language Models", "abstract": "While deep learning in the form of recurrent neural networks (RNNs) has\ncaused a significant improvement in neural language modeling, the fact that\nthey are extremely prone to overfitting is still a mainly unresolved issue. In\nthis paper we propose a regularization method based on generative adversarial\nnetworks (GANs) and adversarial training (AT), that can prevent overfitting in\nneural language models. Unlike common adversarial training methods such as the\nfast gradient sign method (FGSM) that require a second back-propagation through\ntime, and therefore effectively require at least twice the amount of time for\nregular training, the overhead of our method does not exceed more than 20% of\nthe training of the baselines.", "published": "2022-11-02 17:56:33", "link": "http://arxiv.org/abs/2211.09728v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder", "abstract": "We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech\nrecognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced\nencoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR\nhas been actively studied, aiming to utilize versatile linguistic knowledge for\ngenerating accurate text. One crucial factor that makes this integration\nchallenging lies in the vocabulary mismatch; the vocabulary constructed for a\npre-trained LM is generally too large for E2E-ASR training and is likely to\nhave a mismatch against a target ASR domain. To overcome such an issue, we\npropose BECTRA, an extended version of our previous BERT-CTC, that realizes\nBERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based\nmodel, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder\nusing a vocabulary suitable for a target task. With the combination of the\ntransducer and BERT-CTC, we also propose a novel inference algorithm for taking\nadvantage of both autoregressive and non-autoregressive decoding. Experimental\nresults on several ASR tasks, varying in amounts of data, speaking styles, and\nlanguages, demonstrate that BECTRA outperforms BERT-CTC by effectively dealing\nwith the vocabulary mismatch while exploiting BERT knowledge.", "published": "2022-11-02 00:10:43", "link": "http://arxiv.org/abs/2211.00792v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss", "abstract": "This paper presents InterMPL, a semi-supervised learning method of end-to-end\nautomatic speech recognition (ASR) that performs pseudo-labeling (PL) with\nintermediate supervision. Momentum PL (MPL) trains a connectionist temporal\nclassification (CTC)-based model on unlabeled data by continuously generating\npseudo-labels on the fly and improving their quality. In contrast to\nautoregressive formulations, such as the attention-based encoder-decoder and\ntransducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in\ngeneral, owing to its simple/fast inference algorithm and robustness against\ngenerating collapsed labels. However, CTC generally yields inferior performance\nthan the autoregressive models due to the conditional independence assumption,\nthereby limiting the performance of MPL. We propose to enhance MPL by\nintroducing intermediate loss, inspired by the recent advances in CTC-based\nmodeling. Specifically, we focus on self-conditional and hierarchical\nconditional CTC, that apply auxiliary CTC losses to intermediate layers such\nthat the conditional independence assumption is explicitly relaxed. We also\nexplore how pseudo-labels should be generated and used as supervision for\nintermediate losses. Experimental results in different semi-supervised settings\ndemonstrate that the proposed approach outperforms MPL and improves an ASR\nmodel by up to a 12.1% absolute performance gain. In addition, our detailed\nanalysis validates the importance of the intermediate loss.", "published": "2022-11-02 00:18:25", "link": "http://arxiv.org/abs/2211.00795v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SIMD-size aware weight regularization for fast neural vocoding on CPU", "abstract": "This paper proposes weight regularization for a faster neural vocoder.\nPruning time-consuming DNN modules is a promising way to realize a real-time\nvocoder on a CPU (e.g. WaveRNN, LPCNet). Regularization that encourages\nsparsity is also effective in avoiding the quality degradation created by\npruning. However, the orders of weight matrices must be contiguous in SIMD size\nfor fast vocoding. To ensure this order, we propose explicit SIMD size aware\nregularization. Our proposed method reshapes a weight matrix into a tensor so\nthat the weights are aligned by group size in advance, and then computes the\ngroup Lasso-like regularization loss. Experiments on 70% sparse subband WaveRNN\nshow that pruning in conventional Lasso and column-wise group Lasso degrades\nthe synthetic speech's naturalness. The vocoder with proposed regularization 1)\nachieves comparable naturalness to that without pruning and 2) performs\nmeaningfully faster than other conventional vocoders using regularization.", "published": "2022-11-02 05:43:53", "link": "http://arxiv.org/abs/2211.00898v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Passage-Mask: A Learnable Regularization Strategy for Retriever-Reader\n  Models", "abstract": "Retriever-reader models achieve competitive performance across many different\nNLP tasks such as open question answering and dialogue conversations. In this\nwork, we notice these models easily overfit the top-rank retrieval passages and\nstandard training fails to reason over the entire retrieval passages. We\nintroduce a learnable passage mask mechanism which desensitizes the impact from\nthe top-rank retrieval passages and prevents the model from overfitting.\nControlling the gradient variance with fewer mask candidates and selecting the\nmask candidates with one-shot bi-level optimization, our learnable\nregularization strategy enforces the answer generation to focus on the entire\nretrieval passages. Experiments on different tasks across open question\nanswering, dialogue conversation, and fact verification show that our method\nconsistently outperforms its baselines. Extensive experiments and ablation\nstudies demonstrate that our method can be general, effective, and beneficial\nfor many NLP tasks.", "published": "2022-11-02 06:39:46", "link": "http://arxiv.org/abs/2211.00915v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SpeechBlender: Speech Augmentation Framework for Mispronunciation Data\n  Generation", "abstract": "The lack of labeled second language (L2) speech data is a major challenge in\ndesigning mispronunciation detection models. We introduce SpeechBlender - a\nfine-grained data augmentation pipeline for generating mispronunciation errors\nto overcome such data scarcity. The SpeechBlender utilizes varieties of masks\nto target different regions of phonetic units, and use the mixing factors to\nlinearly interpolate raw speech signals while augmenting pronunciation. The\nmasks facilitate smooth blending of the signals, generating more effective\nsamples than the `Cut/Paste' method. Our proposed technique achieves\nstate-of-the-art results, with Speechocean762, on ASR dependent\nmispronunciation detection models at phoneme level, with a 2.0% gain in Pearson\nCorrelation Coefficient (PCC) compared to the previous state-of-the-art [1].\nAdditionally, we demonstrate a 5.0% improvement at the phoneme level compared\nto our baseline. We also observed a 4.6% increase in F1-score with Arabic\nAraVoiceL2 testset.", "published": "2022-11-02 07:13:30", "link": "http://arxiv.org/abs/2211.00923v3", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Internal Language Model Estimation based Adaptive Language Model Fusion\n  for Domain Adaptation", "abstract": "ASR model deployment environment is ever-changing, and the incoming speech\ncan be switched across different domains during a session. This brings a\nchallenge for effective domain adaptation when only target domain text data is\navailable, and our objective is to obtain obviously improved performance on the\ntarget domain while the performance on the general domain is less undermined.\nIn this paper, we propose an adaptive LM fusion approach called internal\nlanguage model estimation based adaptive domain adaptation (ILME-ADA). To\nrealize such an ILME-ADA, an interpolated log-likelihood score is calculated\nbased on the maximum of the scores from the internal LM and the external LM\n(ELM) respectively. We demonstrate the efficacy of the proposed ILME-ADA method\nwith both RNN-T and LAS modeling frameworks employing neural network and n-gram\nLMs as ELMs respectively on two domain specific (target) test sets. The\nproposed method can achieve significantly better performance on the target test\nsets while it gets minimal performance degradation on the general test set,\ncompared with both shallow and ILME-based LM fusion methods.", "published": "2022-11-02 09:15:20", "link": "http://arxiv.org/abs/2211.00968v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Monolingual Recognizers Fusion for Code-switching Speech Recognition", "abstract": "The bi-encoder structure has been intensively investigated in code-switching\n(CS) automatic speech recognition (ASR). However, most existing methods require\nthe structures of two monolingual ASR models (MAMs) should be the same and only\nuse the encoder of MAMs. This leads to the problem that pre-trained MAMs cannot\nbe timely and fully used for CS ASR. In this paper, we propose a monolingual\nrecognizers fusion method for CS ASR. It has two stages: the speech awareness\n(SA) stage and the language fusion (LF) stage. In the SA stage, acoustic\nfeatures are mapped to two language-specific predictions by two independent\nMAMs. To keep the MAMs focused on their own language, we further extend the\nlanguage-aware training strategy for the MAMs. In the LF stage, the BELM fuses\ntwo language-specific predictions to get the final prediction. Moreover, we\npropose a text simulation strategy to simplify the training process of the BELM\nand reduce reliance on CS data. Experiments on a Mandarin-English corpus show\nthe efficiency of the proposed method. The mix error rate is significantly\nreduced on the test set after using open-source pre-trained MAMs.", "published": "2022-11-02 11:24:26", "link": "http://arxiv.org/abs/2211.01046v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transformer-based encoder-encoder architecture for Spoken Term Detection", "abstract": "The paper presents a method for spoken term detection based on the\nTransformer architecture. We propose the encoder-encoder architecture employing\ntwo BERT-like encoders with additional modifications, including convolutional\nand upsampling layers, attention masking, and shared parameters. The encoders\nproject a recognized hypothesis and a searched term into a shared embedding\nspace, where the score of the putative hit is computed using the calibrated dot\nproduct. In the experiments, we used the Wav2Vec 2.0 speech recognizer, and the\nproposed system outperformed a baseline method based on deep LSTMs on the\nEnglish and Czech STD datasets based on USC Shoah Foundation Visual History\nArchive (MALACH).", "published": "2022-11-02 13:03:15", "link": "http://arxiv.org/abs/2211.01089v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "M-SpeechCLIP: Leveraging Large-Scale, Pre-Trained Models for\n  Multilingual Speech to Image Retrieval", "abstract": "This work investigates the use of large-scale, English-only pre-trained\nmodels (CLIP and HuBERT) for multilingual image-speech retrieval. For\nnon-English image-speech retrieval, we outperform the current state-of-the-art\nperformance by a wide margin both when training separate models for each\nlanguage, and with a single model which processes speech in all three\nlanguages. We identify key differences in model behavior and performance\nbetween English and non-English settings, attributable to the English-only\npre-training of CLIP and HuBERT, and investigate how fine-tuning the\npre-trained models impacts these differences. Finally, we show that our models\ncan be used for mono- and cross-lingual speech-text retrieval and cross-lingual\nspeech-speech retrieval, despite never having seen any parallel speech-text or\nspeech-speech data during training.", "published": "2022-11-02 14:54:45", "link": "http://arxiv.org/abs/2211.01180v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "data2vec-aqc: Search for the right Teaching Assistant in the\n  Teacher-Student training setup", "abstract": "In this paper, we propose a new Self-Supervised Learning (SSL) algorithm\ncalled data2vec-aqc, for speech representation learning from unlabeled speech\ndata. Our goal is to improve SSL for speech in domains where both unlabeled and\nlabeled data are limited. Building on the recently introduced data2vec, we\nintroduce additional modules to the data2vec framework that leverage the\nbenefit of data augmentations, quantized representations, and clustering. The\ninteraction between these modules helps solve the cross-contrastive loss as an\nadditional self-supervised objective. data2vec-aqc achieves up to 14.1% and\n20.9% relative WER improvement over the existing state-of-the-art data2vec\nsystem over the test-clean and test-other sets, respectively of LibriSpeech,\nwithout the use of any language model (LM). Our proposed model also achieves up\nto 17.8\\% relative WER gains over the baseline data2vec when fine-tuned on a\nsubset of the Switchboard dataset. Code:\nhttps://github.com/Speech-Lab-IITM/data2vec-aqc.", "published": "2022-11-02 16:29:59", "link": "http://arxiv.org/abs/2211.01246v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Late Audio-Visual Fusion for In-The-Wild Speaker Diarization", "abstract": "Speaker diarization is well studied for constrained audios but little\nexplored for challenging in-the-wild videos, which have more speakers, shorter\nutterances, and inconsistent on-screen speakers. We address this gap by\nproposing an audio-visual diarization model which combines audio-only and\nvisual-centric sub-systems via late fusion. For audio, we show that an\nattractor-based end-to-end system (EEND-EDA) performs remarkably well when\ntrained with our proposed recipe of a simulated proxy dataset, and propose an\nimproved version, EEND-EDA++, that uses attention in decoding and a speaker\nrecognition loss during training to better handle the larger number of\nspeakers. The visual-centric sub-system leverages facial attributes and\nlip-audio synchrony for identity and speech activity estimation of on-screen\nspeakers. Both sub-systems surpass the state of the art (SOTA) by a large\nmargin, with the fused audio-visual system achieving a new SOTA on the AVA-AVD\nbenchmark.", "published": "2022-11-02 17:20:42", "link": "http://arxiv.org/abs/2211.01299v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Predicting phoneme-level prosody latents using AR and flow-based Prior\n  Networks for expressive speech synthesis", "abstract": "A large part of the expressive speech synthesis literature focuses on\nlearning prosodic representations of the speech signal which are then modeled\nby a prior distribution during inference. In this paper, we compare different\nprior architectures at the task of predicting phoneme level prosodic\nrepresentations extracted with an unsupervised FVAE model. We use both\nsubjective and objective metrics to show that normalizing flow based prior\nnetworks can result in more expressive speech at the cost of a slight drop in\nquality. Furthermore, we show that the synthesized speech has higher\nvariability, for a given text, due to the nature of normalizing flows. We also\npropose a Dynamical VAE model, that can generate higher quality speech although\nwith decreased expressiveness and variability compared to the flow based\nmodels.", "published": "2022-11-02 17:45:01", "link": "http://arxiv.org/abs/2211.01327v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Named Entity Recognition in Telephone Conversations via\n  Effective Active Learning with Human in the Loop", "abstract": "Telephone transcription data can be very noisy due to speech recognition\nerrors, disfluencies, etc. Not only that annotating such data is very\nchallenging for the annotators, but also such data may have lots of annotation\nerrors even after the annotation job is completed, resulting in a very poor\nmodel performance. In this paper, we present an active learning framework that\nleverages human in the loop learning to identify data samples from the\nannotated dataset for re-annotation that are more likely to contain annotation\nerrors. In this way, we largely reduce the need for data re-annotation for the\nwhole dataset. We conduct extensive experiments with our proposed approach for\nNamed Entity Recognition and observe that by re-annotating only about 6%\ntraining instances out of the whole dataset, the F1 score for a certain entity\ntype can be significantly improved by about 25%.", "published": "2022-11-02 17:55:04", "link": "http://arxiv.org/abs/2211.01354v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variable Attention Masking for Configurable Transformer Transducer\n  Speech Recognition", "abstract": "This work studies the use of attention masking in transformer transducer\nbased speech recognition for building a single configurable model for different\ndeployment scenarios. We present a comprehensive set of experiments comparing\nfixed masking, where the same attention mask is applied at every frame, with\nchunked masking, where the attention mask for each frame is determined by chunk\nboundaries, in terms of recognition accuracy and latency. We then explore the\nuse of variable masking, where the attention masks are sampled from a target\ndistribution at training time, to build models that can work in different\nconfigurations. Finally, we investigate how a single configurable model can be\nused to perform both first pass streaming recognition and second pass acoustic\nrescoring. Experiments show that chunked masking achieves a better accuracy vs\nlatency trade-off compared to fixed masking, both with and without FastEmit. We\nalso show that variable masking improves the accuracy by up to 8% relative in\nthe acoustic re-scoring scenario.", "published": "2022-11-02 19:14:02", "link": "http://arxiv.org/abs/2211.01438v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Zero-Shot Code-Switched Speech Recognition", "abstract": "In this work, we seek to build effective code-switched (CS) automatic speech\nrecognition systems (ASR) under the zero-shot setting where no transcribed CS\nspeech data is available for training. Previously proposed frameworks which\nconditionally factorize the bilingual task into its constituent monolingual\nparts are a promising starting point for leveraging monolingual data\nefficiently. However, these methods require the monolingual modules to perform\nlanguage segmentation. That is, each monolingual module has to simultaneously\ndetect CS points and transcribe speech segments of one language while ignoring\nthose of other languages -- not a trivial task. We propose to simplify each\nmonolingual module by allowing them to transcribe all speech segments\nindiscriminately with a monolingual script (i.e. transliteration). This simple\nmodification passes the responsibility of CS point detection to subsequent\nbilingual modules which determine the final output by considering multiple\nmonolingual transliterations along with external language model information. We\napply this transliteration-based approach in an end-to-end differentiable\nneural network and demonstrate its efficacy for zero-shot CS ASR on\nMandarin-English SEAME test sets.", "published": "2022-11-02 19:52:54", "link": "http://arxiv.org/abs/2211.01458v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Phoneme Segmentation Using Self-Supervised Speech Models", "abstract": "We apply transfer learning to the task of phoneme segmentation and\ndemonstrate the utility of representations learned in self-supervised\npre-training for the task. Our model extends transformer-style encoders with\nstrategically placed convolutions that manipulate features learned in\npre-training. Using the TIMIT and Buckeye corpora we train and test the model\nin the supervised and unsupervised settings. The latter case is accomplished by\nfurnishing a noisy label-set with the predictions of a separate model, it\nhaving been trained in an unsupervised fashion. Results indicate our model\neclipses previous state-of-the-art performance in both settings and on both\ndatasets. Finally, following observations during published code review and\nattempts to reproduce past segmentation results, we find a need to disambiguate\nthe definition and implementation of widely-used evaluation metrics. We resolve\nthis ambiguity by delineating two distinct evaluation schemes and describing\ntheir nuances.", "published": "2022-11-02 19:57:31", "link": "http://arxiv.org/abs/2211.01461v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Over-communicate no more: Situated RL agents learn concise communication\n  protocols", "abstract": "While it is known that communication facilitates cooperation in multi-agent\nsettings, it is unclear how to design artificial agents that can learn to\neffectively and efficiently communicate with each other. Much research on\ncommunication emergence uses reinforcement learning (RL) and explores\nunsituated communication in one-step referential tasks -- the tasks are not\ntemporally interactive and lack time pressures typically present in natural\ncommunication. In these settings, agents may successfully learn to communicate,\nbut they do not learn to exchange information concisely -- they tend towards\nover-communication and an inefficient encoding. Here, we explore situated\ncommunication in a multi-step task, where the acting agent has to forgo an\nenvironmental action to communicate. Thus, we impose an opportunity cost on\ncommunication and mimic the real-world pressure of passing time. We compare\ncommunication emergence under this pressure against learning to communicate\nwith a cost on articulation effort, implemented as a per-message penalty (fixed\nand progressively increasing). We find that while all tested pressures can\ndisincentivise over-communication, situated communication does it most\neffectively and, unlike the cost on effort, does not negatively impact\nemergence. Implementing an opportunity cost on communication in a temporally\nextended environment is a step towards embodiment, and might be a pre-condition\nfor incentivising efficient, human-like communication.", "published": "2022-11-02 21:08:14", "link": "http://arxiv.org/abs/2211.01480v1", "categories": ["cs.MA", "cs.CL", "cs.HC"], "primary_category": "cs.MA"}
{"title": "RQUGE: Reference-Free Metric for Evaluating Question Generation by\n  Answering the Question", "abstract": "Existing metrics for evaluating the quality of automatically generated\nquestions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and\npredicted questions, providing a high score when there is a considerable\nlexical overlap or semantic similarity between the candidate and the reference\nquestions. This approach has two major shortcomings. First, we need expensive\nhuman-provided reference questions. Second, it penalises valid questions that\nmay not have high lexical or semantic similarity to the reference questions. In\nthis paper, we propose a new metric, RQUGE, based on the answerability of the\ncandidate question given the context. The metric consists of a\nquestion-answering and a span scorer modules, using pre-trained models from\nexisting literature, thus it can be used without any further training. We\ndemonstrate that RQUGE has a higher correlation with human judgment without\nrelying on the reference question. Additionally, RQUGE is shown to be more\nrobust to several adversarial corruptions. Furthermore, we illustrate that we\ncan significantly improve the performance of QA models on out-of-domain\ndatasets by fine-tuning on synthetic data generated by a question generation\nmodel and re-ranked by RQUGE.", "published": "2022-11-02 21:10:09", "link": "http://arxiv.org/abs/2211.01482v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MAST: Multiscale Audio Spectrogram Transformers", "abstract": "We present Multiscale Audio Spectrogram Transformer (MAST) for audio\nclassification, which brings the concept of multiscale feature hierarchies to\nthe Audio Spectrogram Transformer (AST). Given an input audio spectrogram, we\nfirst patchify and project it into an initial temporal resolution and embedding\ndimension, post which the multiple stages in MAST progressively expand the\nembedding dimension while reducing the temporal resolution of the input. We use\na pyramid structure that allows early layers of MAST operating at a high\ntemporal resolution but low embedding space to model simple low-level acoustic\ninformation and deeper temporally coarse layers to model high-level acoustic\ninformation with high-dimensional embeddings. We also extend our approach to\npresent a new Self-Supervised Learning (SSL) method called SS-MAST, which\ncalculates a symmetric contrastive loss between latent representations from a\nstudent and a teacher encoder, leveraging patch-drop, a novel audio\naugmentation approach that we introduce. In practice, MAST significantly\noutperforms AST by an average accuracy of 3.4% across 8 speech and non-speech\ntasks from the LAPE Benchmark, achieving state-of-the-art results on keyword\nspotting in Speech Commands. Additionally, our proposed SS-MAST achieves an\nabsolute average improvement of 2.6% over the previously proposed SSAST.", "published": "2022-11-02 23:34:12", "link": "http://arxiv.org/abs/2211.01515v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SLICER: Learning universal audio representations using low-resource\n  self-supervised pre-training", "abstract": "We present a new Self-Supervised Learning (SSL) approach to pre-train\nencoders on unlabeled audio data that reduces the need for large amounts of\nlabeled data for audio and speech classification. Our primary aim is to learn\naudio representations that can generalize across a large variety of speech and\nnon-speech tasks in a low-resource un-labeled audio pre-training setting.\nInspired by the recent success of clustering and contrasting learning paradigms\nfor SSL-based speech representation learning, we propose SLICER (Symmetrical\nLearning of Instance and Cluster-level Efficient Representations), which brings\ntogether the best of both clustering and contrasting learning paradigms. We use\na symmetric loss between latent representations from student and teacher\nencoders and simultaneously solve instance and cluster-level contrastive\nlearning tasks. We obtain cluster representations online by just projecting the\ninput spectrogram into an output subspace with dimensions equal to the number\nof clusters. In addition, we propose a novel mel-spectrogram augmentation\nprocedure, k-mix, based on mixup, which does not require labels and aids\nunsupervised representation learning for audio. Overall, SLICER achieves\nstate-of-the-art results on the LAPE Benchmark \\cite{9868132}, significantly\noutperforming DeLoRes-M and other prior approaches, which are pre-trained on\n$10\\times$ larger of unsupervised data. We will make all our codes available on\nGitHub.", "published": "2022-11-02 23:45:33", "link": "http://arxiv.org/abs/2211.01519v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Build a SRE Challenge System: Lessons from VoxSRC 2022 and CNSRC 2022", "abstract": "Many speaker recognition challenges have been held to assess the speaker\nverification system in the wild and probe the performance limit. Voxceleb\nSpeaker Recognition Challenge (VoxSRC), based on the voxceleb, is the most\npopular. Besides, another challenge called CN-Celeb Speaker Recognition\nChallenge (CNSRC) is also held this year, which is based on the Chinese\ncelebrity multi-genre dataset CN-Celeb. This year, our team participated in\nboth speaker verification closed tracks in CNSRC 2022 and VoxSRC 2022, and\nachieved the 1st place and 3rd place respectively. In most system reports, the\nauthors usually only provide a description of their systems but lack an\neffective analysis of their methods. In this paper, we will outline how to\nbuild a strong speaker verification challenge system and give a detailed\nanalysis of each method compared with some other popular technical means.", "published": "2022-11-02 01:33:23", "link": "http://arxiv.org/abs/2211.00815v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conversation-oriented ASR with multi-look-ahead CBS architecture", "abstract": "During conversations, humans are capable of inferring the intention of the\nspeaker at any point of the speech to prepare the following action promptly.\nSuch ability is also the key for conversational systems to achieve rhythmic and\nnatural conversation. To perform this, the automatic speech recognition (ASR)\nused for transcribing the speech in real-time must achieve high accuracy\nwithout delay. In streaming ASR, high accuracy is assured by attending to\nlook-ahead frames, which leads to delay increments. To tackle this trade-off\nissue, we propose a multiple latency streaming ASR to achieve high accuracy\nwith zero look-ahead. The proposed system contains two encoders that operate in\nparallel, where a primary encoder generates accurate outputs utilizing\nlook-ahead frames, and the auxiliary encoder recognizes the look-ahead portion\nof the primary encoder without look-ahead. The proposed system is constructed\nbased on contextual block streaming (CBS) architecture, which leverages block\nprocessing and has a high affinity for the multiple latency architecture.\nVarious methods are also studied for architecting the system, including\nshifting the network to perform as different encoders; as well as generating\nboth encoders' outputs in one encoding pass.", "published": "2022-11-02 03:58:56", "link": "http://arxiv.org/abs/2211.00858v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Factorized Blank Thresholding for Improved Runtime Efficiency of Neural\n  Transducers", "abstract": "We show how factoring the RNN-T's output distribution can significantly\nreduce the computation cost and power consumption for on-device ASR inference\nwith no loss in accuracy. With the rise in popularity of neural-transducer type\nmodels like the RNN-T for on-device ASR, optimizing RNN-T's runtime efficiency\nis of great interest. While previous work has primarily focused on the\noptimization of RNN-T's acoustic encoder and predictor, this paper focuses the\nattention on the joiner. We show that despite being only a small part of RNN-T,\nthe joiner has a large impact on the overall model's runtime efficiency. We\npropose to utilize HAT-style joiner factorization for the purpose of skipping\nthe more expensive non-blank computation when the blank probability exceeds a\ncertain threshold. Since the blank probability can be computed very efficiently\nand the RNN-T output is dominated by blanks, our proposed method leads to a\n26-30% decoding speed-up and 43-53% reduction in on-device power consumption,\nall the while incurring no accuracy degradation and being relatively simple to\nimplement.", "published": "2022-11-02 05:42:53", "link": "http://arxiv.org/abs/2211.00896v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast-U2++: Fast and Accurate End-to-End Speech Recognition in Joint\n  CTC/Attention Frames", "abstract": "Recently, the unified streaming and non-streaming two-pass (U2/U2++)\nend-to-end model for speech recognition has shown great performance in terms of\nstreaming capability, accuracy and latency. In this paper, we present\nfast-U2++, an enhanced version of U2++ to further reduce partial latency. The\ncore idea of fast-U2++ is to output partial results of the bottom layers in its\nencoder with a small chunk, while using a large chunk in the top layers of its\nencoder to compensate the performance degradation caused by the small chunk.\nMoreover, we use knowledge distillation method to reduce the token emission\nlatency. We present extensive experiments on Aishell-1 dataset. Experiments and\nablation studies show that compared to U2++, fast-U2++ reduces model latency\nfrom 320ms to 80ms, and achieves a character error rate (CER) of 5.06% with a\nstreaming setup.", "published": "2022-11-02 08:01:52", "link": "http://arxiv.org/abs/2211.00941v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarial Guitar Amplifier Modelling With Unpaired Data", "abstract": "We propose an audio effects processing framework that learns to emulate a\ntarget electric guitar tone from a recording. We train a deep neural network\nusing an adversarial approach, with the goal of transforming the timbre of a\nguitar, into the timbre of another guitar after audio effects processing has\nbeen applied, for example, by a guitar amplifier. The model training requires\nno paired data, and the resulting model emulates the target timbre well whilst\nbeing capable of real-time processing on a modern personal computer. To verify\nour approach we present two experiments, one which carries out unpaired\ntraining using paired data, allowing us to monitor training via objective\nmetrics, and another that uses fully unpaired data, corresponding to a\nrealistic scenario where a user wants to emulate a guitar timbre only using\naudio data from a recording. Our listening test results confirm that the models\nare perceptually convincing.", "published": "2022-11-02 08:06:14", "link": "http://arxiv.org/abs/2211.00943v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multi-Speaker Multi-Style Speech Synthesis with Timbre and Style\n  Disentanglement", "abstract": "Disentanglement of a speaker's timbre and style is very important for style\ntransfer in multi-speaker multi-style text-to-speech (TTS) scenarios. With the\ndisentanglement of timbres and styles, TTS systems could synthesize expressive\nspeech for a given speaker with any style which has been seen in the training\ncorpus. However, there are still some shortcomings with the current research on\ntimbre and style disentanglement. The current method either requires\nsingle-speaker multi-style recordings, which are difficult and expensive to\ncollect, or uses a complex network and complicated training method, which is\ndifficult to reproduce and control the style transfer behavior. To improve the\ndisentanglement effectiveness of timbres and styles, and to remove the reliance\non single-speaker multi-style corpus, a simple but effective timbre and style\ndisentanglement method is proposed in this paper. The FastSpeech2 network is\nemployed as the backbone network, with explicit duration, pitch, and energy\ntrajectory to represent the style. Each speaker's data is considered as a\nseparate and isolated style, then a speaker embedding and a style embedding are\nadded to the FastSpeech2 network to learn disentangled representations.\nUtterance level pitch and energy normalization are utilized to improve the\ndecoupling effect. Experimental results demonstrate that the proposed model\ncould synthesize speech with any style seen during training with high style\nsimilarity while maintaining very high speaker similarity.", "published": "2022-11-02 09:13:49", "link": "http://arxiv.org/abs/2211.00967v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpectroMap: Peak detection algorithm for audio fingerprinting", "abstract": "Audio fingerprinting is a technique used to identify and match audio\nrecordings based on their unique characteristics. It involves creating a\ncondensed representation of an audio signal that can be used to quickly compare\nand match against other audio recordings. The fingerprinting process involves\nanalyzing the audio signal to extract certain features, such as spectral\ncontent, tempo, and rhythm, among other things. In this paper, we present\nSpectroMap, an open-source GitHub repository for audio fingerprinting written\nin Python programming language. It is composed of a peak search algorithm that\nextracts topological prominences from a spectrogram via time-frequency bands.\nIn this paper, we introduce the algorithm functioning with two experimental\napplications in a high-quality urban sound dataset and environmental audio\nrecordings to describe how it works and how effective it is in handling the\ninput data. Finally, we have posed two Python scripts that would reproduce the\nproposed case studies in order to ease the reproducibility of our audio\nfingerprinting system.", "published": "2022-11-02 09:40:22", "link": "http://arxiv.org/abs/2211.00982v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Singing Voice Synthesis with Vibrato Modeling and Latent Energy\n  Representation", "abstract": "This paper proposes an expressive singing voice synthesis system by\nintroducing explicit vibrato modeling and latent energy representation. Vibrato\nis essential to the naturalness of synthesized sound, due to the inherent\ncharacteristics of human singing. Hence, a deep learning-based vibrato model is\nintroduced in this paper to control the vibrato's likeliness, rate, depth and\nphase in singing, where the vibrato likeliness represents the existence\nprobability of vibrato and it would help improve the singing voice's\nnaturalness. Actually, there is no annotated label about vibrato likeliness in\nexisting singing corpus. We adopt a novel vibrato likeliness labeling method to\nlabel the vibrato likeliness automatically. Meanwhile, the power spectrogram of\naudio contains rich information that can improve the expressiveness of singing.\nAn autoencoder-based latent energy bottleneck feature is proposed for\nexpressive singing voice synthesis. Experimental results on the open dataset\nNUS48E show that both the vibrato modeling and the latent energy representation\ncould significantly improve the expressiveness of singing voice. The audio\nsamples are shown in the demo website.", "published": "2022-11-02 09:58:25", "link": "http://arxiv.org/abs/2211.00996v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Intermediate Fine-Tuning Using Imperfect Synthetic Speech for Improving\n  Electrolaryngeal Speech Recognition", "abstract": "Research on automatic speech recognition (ASR) systems for electrolaryngeal\nspeakers has been relatively unexplored due to small datasets. When training\ndata is lacking in ASR, a large-scale pretraining and fine tuning framework is\noften sufficient to achieve high recognition rates; however, in\nelectrolaryngeal speech, the domain shift between the pretraining and\nfine-tuning data is too large to overcome, limiting the maximum improvement of\nrecognition rates. To resolve this, we propose an intermediate fine-tuning step\nthat uses imperfect synthetic speech to close the domain shift gap between the\npretraining and target data. Despite the imperfect synthetic data, we show the\neffectiveness of this on electrolaryngeal speech datasets, with improvements of\n6.1% over the baseline that did not use imperfect synthetic speech. Results\nshow how the intermediate fine-tuning stage focuses on learning the high-level\ninherent features of the imperfect synthetic data rather than the low-level\nfeatures such as intelligibility.", "published": "2022-11-02 12:32:26", "link": "http://arxiv.org/abs/2211.01079v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DSPGAN: a GAN-based universal vocoder for high-fidelity TTS by\n  time-frequency domain supervision from DSP", "abstract": "Recent development of neural vocoders based on the generative adversarial\nneural network (GAN) has shown obvious advantages of generating raw waveform\nconditioned on mel-spectrogram with fast inference speed and lightweight\nnetworks. Whereas, it is still challenging to train a universal neural vocoder\nthat can synthesize high-fidelity speech from various scenarios with unseen\nspeakers, languages, and speaking styles. In this paper, we propose DSPGAN, a\nGAN-based universal vocoder for high-fidelity speech synthesis by applying the\ntime-frequency domain supervision from digital signal processing (DSP). To\neliminate the mismatch problem caused by the ground-truth spectrograms in the\ntraining phase and the predicted spectrograms in the inference phase, we\nleverage the mel-spectrogram extracted from the waveform generated by a DSP\nmodule, rather than the predicted mel-spectrogram from the Text-to-Speech (TTS)\nacoustic model, as the time-frequency domain supervision to the GAN-based\nvocoder. We also utilize sine excitation as the time-domain supervision to\nimprove the harmonic modeling and eliminate various artifacts of the GAN-based\nvocoder. Experiments show that DSPGAN significantly outperforms the compared\napproaches and it can generate high-fidelity speech for various TTS models\ntrained using diverse data.", "published": "2022-11-02 12:56:56", "link": "http://arxiv.org/abs/2211.01087v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Analysis of Noisy-target Training for DNN-based speech enhancement", "abstract": "Deep neural network (DNN)-based speech enhancement usually uses a clean\nspeech as a training target. However, it is hard to collect large amounts of\nclean speech because the recording is very costly. In other words, the\nperformance of current speech enhancement has been limited by the amount of\ntraining data. To relax this limitation, Noisy-target Training (NyTT) that\nutilizes noisy speech as a training target has been proposed. Although it has\nbeen experimentally shown that NyTT can train a DNN without clean speech, a\ndetailed analysis has not been conducted and its behavior has not been\nunderstood well. In this paper, we conduct various analyses to deepen our\nunderstanding of NyTT. In addition, based on the property of NyTT, we propose a\nrefined method that is comparable to the method using clean speech.\nFurthermore, we show that we can improve the performance by using a huge amount\nof noisy speech with clean speech.", "published": "2022-11-02 15:21:28", "link": "http://arxiv.org/abs/2211.01198v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Language Modeling using Perceptually-Guided Discrete\n  Representations", "abstract": "In this work, we study the task of Audio Language Modeling, in which we aim\nat learning probabilistic models for audio that can be used for generation and\ncompletion. We use a state-of-the-art perceptually-guided audio compression\nmodel, to encode audio to discrete representations. Next, we train a\ntransformer-based causal language model using these representations. At\ninference time, we perform audio auto-completion by encoding an audio prompt as\na discrete sequence, feeding it to the audio language model, sampling from the\nmodel, and synthesizing the corresponding time-domain signal. We evaluate the\nquality of samples generated by our method on Audioset, the largest dataset for\ngeneral audio to date, and show that it is superior to the evaluated baseline\naudio encoders. We additionally provide an extensive analysis to better\nunderstand the trade-off between audio-quality and language-modeling\ncapabilities. Samples:link.", "published": "2022-11-02 16:02:45", "link": "http://arxiv.org/abs/2211.01223v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LMD: A Learnable Mask Network to Detect Adversarial Examples for Speaker\n  Verification", "abstract": "Although the security of automatic speaker verification (ASV) is seriously\nthreatened by recently emerged adversarial attacks, there have been some\ncountermeasures to alleviate the threat. However, many defense approaches not\nonly require the prior knowledge of the attackers but also possess weak\ninterpretability. To address this issue, in this paper, we propose an\nattacker-independent and interpretable method, named learnable mask detector\n(LMD), to separate adversarial examples from the genuine ones. It utilizes\nscore variation as an indicator to detect adversarial examples, where the score\nvariation is the absolute discrepancy between the ASV scores of an original\naudio recording and its transformed audio synthesized from its masked complex\nspectrogram. A core component of the score variation detector is to generate\nthe masked spectrogram by a neural network. The neural network needs only\ngenuine examples for training, which makes it an attacker-independent approach.\nIts interpretability lies that the neural network is trained to minimize the\nscore variation of the targeted ASV, and maximize the number of the masked\nspectrogram bins of the genuine training examples. Its foundation is based on\nthe observation that, masking out the vast majority of the spectrogram bins\nwith little speaker information will inevitably introduce a large score\nvariation to the adversarial example, and a small score variation to the\ngenuine example. Experimental results with 12 attackers and two representative\nASV systems show that our proposed method outperforms five state-of-the-art\nbaselines. The extensive experimental results can also be a benchmark for the\ndetection-based ASV defenses.", "published": "2022-11-02 02:03:53", "link": "http://arxiv.org/abs/2211.00825v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "More Speaking or More Speakers?", "abstract": "Self-training (ST) and self-supervised learning (SSL) methods have\ndemonstrated strong improvements in automatic speech recognition (ASR). In\nspite of these advances, to the best of our knowledge, there is no analysis of\nhow the composition of the labelled and unlabelled datasets used in these\nmethods affects the results. In this work we aim to analyse the effect of\nnumber of speakers in the training data on a recent SSL algorithm (wav2vec\n2.0), and a recent ST algorithm (slimIPL). We perform a systematic analysis on\nboth labeled and unlabeled data by varying the number of speakers while keeping\nthe number of hours fixed and vice versa. Our findings suggest that SSL\nrequires a large amount of unlabeled data to produce high accuracy results,\nwhile ST requires a sufficient number of speakers in the labelled data,\nespecially in the low-regime setting. In this manner these two approaches\nimprove supervised learning in different regimes of data composition.", "published": "2022-11-02 03:50:40", "link": "http://arxiv.org/abs/2211.00854v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Pop2Piano : Pop Audio-based Piano Cover Generation", "abstract": "Piano covers of pop music are enjoyed by many people. However, the task of\nautomatically generating piano covers of pop music is still understudied. This\nis partly due to the lack of synchronized {Pop, Piano Cover} data pairs, which\nmade it challenging to apply the latest data-intensive deep learning-based\nmethods. To leverage the power of the data-driven approach, we make a large\namount of paired and synchronized {Pop, Piano Cover} data using an automated\npipeline. In this paper, we present Pop2Piano, a Transformer network that\ngenerates piano covers given waveforms of pop music. To the best of our\nknowledge, this is the first model to generate a piano cover directly from pop\naudio without using melody and chord extraction modules. We show that\nPop2Piano, trained with our dataset, is capable of producing plausible piano\ncovers.", "published": "2022-11-02 05:42:22", "link": "http://arxiv.org/abs/2211.00895v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A weighted-variance variational autoencoder model for speech enhancement", "abstract": "We address speech enhancement based on variational autoencoders, which\ninvolves learning a speech prior distribution in the time-frequency (TF)\ndomain. A zero-mean complex-valued Gaussian distribution is usually assumed for\nthe generative model, where the speech information is encoded in the variance\nas a function of a latent variable. In contrast to this commonly used approach,\nwe propose a weighted variance generative model, where the contribution of each\nspectrogram time-frame in parameter learning is weighted. We impose a Gamma\nprior distribution on the weights, which would effectively lead to a Student's\nt-distribution instead of Gaussian for speech generative modeling. We develop\nefficient training and speech enhancement algorithms based on the proposed\ngenerative model. Our experimental results on spectrogram auto-encoding and\nspeech enhancement demonstrate the effectiveness and robustness of the proposed\napproach compared to the standard unweighted variance model.", "published": "2022-11-02 09:51:15", "link": "http://arxiv.org/abs/2211.00990v2", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "I4U System Description for NIST SRE'20 CTS Challenge", "abstract": "This manuscript describes the I4U submission to the 2020 NIST Speaker\nRecognition Evaluation (SRE'20) Conversational Telephone Speech (CTS)\nChallenge. The I4U's submission was resulted from active collaboration among\nresearchers across eight research teams - I$^2$R (Singapore), UEF (Finland),\nVALPT (Italy, Spain), NEC (Japan), THUEE (China), LIA (France), NUS\n(Singapore), INRIA (France) and TJU (China). The submission was based on the\nfusion of top performing sub-systems and sub-fusion systems contributed by\nindividual teams. Efforts have been spent on the use of common development and\nvalidation sets, submission schedule and milestone, minimizing inconsistency in\ntrial list and score file format across sites.", "published": "2022-11-02 13:04:27", "link": "http://arxiv.org/abs/2211.01091v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Quantum Kernel Learning Approach to Acoustic Modeling for Spoken\n  Command Recognition", "abstract": "We propose a quantum kernel learning (QKL) framework to address the inherent\ndata sparsity issues often encountered in training large-scare acoustic models\nin low-resource scenarios. We project acoustic features based on\nclassical-to-quantum feature encoding. Different from existing quantum\nconvolution techniques, we utilize QKL with features in the quantum space to\ndesign kernel-based classifiers. Experimental results on challenging spoken\ncommand recognition tasks for a few low-resource languages, such as Arabic,\nGeorgian, Chuvash, and Lithuanian, show that the proposed QKL-based hybrid\napproach attains good improvements over existing classical and quantum\nsolutions.", "published": "2022-11-02 16:46:23", "link": "http://arxiv.org/abs/2211.01263v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "quant-ph"], "primary_category": "cs.SD"}
{"title": "Losses Can Be Blessings: Routing Self-Supervised Speech Representations\n  Towards Efficient Multilingual and Multitask Speech Processing", "abstract": "Self-supervised learning (SSL) for rich speech representations has achieved\nempirical success in low-resource Automatic Speech Recognition (ASR) and other\nspeech processing tasks, which can mitigate the necessity of a large amount of\ntranscribed speech and thus has driven a growing demand for on-device ASR and\nother speech processing. However, advanced speech SSL models have become\nincreasingly large, which contradicts the limited on-device resources. This gap\ncould be more severe in multilingual/multitask scenarios requiring\nsimultaneously recognizing multiple languages or executing multiple speech\nprocessing tasks. Additionally, strongly overparameterized speech SSL models\ntend to suffer from overfitting when being finetuned on low-resource speech\ncorpus. This work aims to enhance the practical usage of speech SSL models\ntowards a win-win in both enhanced efficiency and alleviated overfitting via\nour proposed S$^3$-Router framework, which for the first time discovers that\nsimply discarding no more than 10\\% of model weights via only finetuning model\nconnections of speech SSL models can achieve better accuracy over standard\nweight finetuning on downstream speech processing tasks. More importantly,\nS$^3$-Router can serve as an all-in-one technique to enable (1) a new\nfinetuning scheme, (2) an efficient multilingual/multitask solution, (3) a\nstate-of-the-art ASR pruning technique, and (4) a new tool to quantitatively\nanalyze the learned speech representation. We believe S$^3$-Router has provided\na new perspective for practical deployment of speech SSL models. Our codes are\navailable at: https://github.com/GATECH-EIC/S3-Router.", "published": "2022-11-02 23:47:55", "link": "http://arxiv.org/abs/2211.01522v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Fast and efficient speech enhancement with variational autoencoders", "abstract": "Unsupervised speech enhancement based on variational autoencoders has shown\npromising performance compared with the commonly used supervised methods. This\napproach involves the use of a pre-trained deep speech prior along with a\nparametric noise model, where the noise parameters are learned from the noisy\nspeech signal with an expectationmaximization (EM)-based method. The E-step\ninvolves an intractable latent posterior distribution. Existing algorithms to\nsolve this step are either based on computationally heavy Monte Carlo Markov\nChain sampling methods and variational inference, or inefficient\noptimization-based methods. In this paper, we propose a new approach based on\nLangevin dynamics that generates multiple sequences of samples and comes with a\ntotal variation-based regularization to incorporate temporal correlations of\nlatent vectors. Our experiments demonstrate that the developed framework makes\nan effective compromise between computational efficiency and enhancement\nquality, and outperforms existing methods.", "published": "2022-11-02 09:52:13", "link": "http://arxiv.org/abs/2211.02728v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Impact of annotation modality on label quality and model performance in\n  the automatic assessment of laughter in-the-wild", "abstract": "Laughter is considered one of the most overt signals of joy. Laughter is\nwell-recognized as a multimodal phenomenon but is most commonly detected by\nsensing the sound of laughter. It is unclear how perception and annotation of\nlaughter differ when annotated from other modalities like video, via the body\nmovements of laughter. In this paper we take a first step in this direction by\nasking if and how well laughter can be annotated when only audio, only video\n(containing full body movement information) or audiovisual modalities are\navailable to annotators. We ask whether annotations of laughter are congruent\nacross modalities, and compare the effect that labeling modality has on machine\nlearning model performance. We compare annotations and models for laughter\ndetection, intensity estimation, and segmentation, three tasks common in\nprevious studies of laughter. Our analysis of more than 4000 annotations\nacquired from 48 annotators revealed evidence for incongruity in the perception\nof laughter, and its intensity between modalities. Further analysis of\nannotations against consolidated audiovisual reference annotations revealed\nthat recall was lower on average for video when compared to the audio\ncondition, but tended to increase with the intensity of the laughter samples.\nOur machine learning experiments compared the performance of state-of-the-art\nunimodal (audio-based, video-based and acceleration-based) and multi-modal\nmodels for different combinations of input modalities, training label modality,\nand testing label modality. Models with video and acceleration inputs had\nsimilar performance regardless of training label modality, suggesting that it\nmay be entirely appropriate to train models for laughter detection from body\nmovements using video-acquired labels, despite their lower inter-rater\nagreement.", "published": "2022-11-02 00:18:08", "link": "http://arxiv.org/abs/2211.00794v1", "categories": ["cs.SD", "cs.CV", "cs.CY", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Fourier Shift for Binaural Speech Rendering", "abstract": "We present a neural network for rendering binaural speech from given monaural\naudio, position, and orientation of the source. Most of the previous works have\nfocused on synthesizing binaural speeches by conditioning the positions and\norientations in the feature space of convolutional neural networks. These\nsynthesis approaches are powerful in estimating the target binaural speeches\neven for in-the-wild data but are difficult to generalize for rendering the\naudio from out-of-distribution domains. To alleviate this, we propose Neural\nFourier Shift (NFS), a novel network architecture that enables binaural speech\nrendering in the Fourier space. Specifically, utilizing a geometric time delay\nbased on the distance between the source and the receiver, NFS is trained to\npredict the delays and scales of various early reflections. NFS is efficient in\nboth memory and computational cost, is interpretable, and operates\nindependently of the source domain by its design. Experimental results show\nthat NFS performs comparable to the previous studies on the benchmark dataset,\neven with its 25 times lighter memory and 6 times fewer calculations.", "published": "2022-11-02 04:55:09", "link": "http://arxiv.org/abs/2211.00878v2", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Audio-visual speech enhancement with a deep Kalman filter generative\n  model", "abstract": "Deep latent variable generative models based on variational autoencoder (VAE)\nhave shown promising performance for audiovisual speech enhancement (AVSE). The\nunderlying idea is to learn a VAEbased audiovisual prior distribution for clean\nspeech data, and then combine it with a statistical noise model to recover a\nspeech signal from a noisy audio recording and video (lip images) of the target\nspeaker. Existing generative models developed for AVSE do not take into account\nthe sequential nature of speech data, which prevents them from fully\nincorporating the power of visual data. In this paper, we present an\naudiovisual deep Kalman filter (AV-DKF) generative model which assumes a\nfirst-order Markov chain model for the latent variables and effectively fuses\naudiovisual data. Moreover, we develop an efficient inference methodology to\nestimate speech signals at test time. We conduct a set of experiments to\ncompare different variants of generative models for speech enhancement. The\nresults demonstrate the superiority of the AV-DKF model compared with both its\naudio-only version and the non-sequential audio-only and audiovisual VAE-based\nmodels.", "published": "2022-11-02 09:50:08", "link": "http://arxiv.org/abs/2211.00988v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Inference and Denoise: Causal Inference-based Neural Speech Enhancement", "abstract": "This study addresses the speech enhancement (SE) task within the causal\ninference paradigm by modeling the noise presence as an intervention. Based on\nthe potential outcome framework, the proposed causal inference-based speech\nenhancement (CISE) separates clean and noisy frames in an intervened noisy\nspeech using a noise detector and assigns both sets of frames to two mask-based\nenhancement modules (EMs) to perform noise-conditional SE. Specifically, we use\nthe presence of noise as guidance for EM selection during training, and the\nnoise detector selects the enhancement module according to the prediction of\nthe presence of noise for each frame. Moreover, we derived a SE-specific\naverage treatment effect to quantify the causal effect adequately. Experimental\nevidence demonstrates that CISE outperforms a non-causal mask-based SE approach\nin the studied settings and has better performance and efficiency than more\ncomplex SE models.", "published": "2022-11-02 15:03:50", "link": "http://arxiv.org/abs/2211.01189v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.NE", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Low-Resource Music Genre Classification with Cross-Modal Neural Model\n  Reprogramming", "abstract": "Transfer learning (TL) approaches have shown promising results when handling\ntasks with limited training data. However, considerable memory and\ncomputational resources are often required for fine-tuning pre-trained neural\nnetworks with target domain data. In this work, we introduce a novel method for\nleveraging pre-trained models for low-resource (music) classification based on\nthe concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a\npre-trained model from a source domain to a target domain by modifying the\ninput of a frozen pre-trained model. In addition to the known,\ninput-independent, reprogramming method, we propose an advanced reprogramming\nparadigm: Input-dependent NMR, to increase adaptability to complex input data\nsuch as musical audio. Experimental results suggest that a neural model\npre-trained on large-scale datasets can successfully perform music genre\nclassification by using this reprogramming method. The two proposed\nInput-dependent NMR TL methods outperform fine-tuning-based TL methods on a\nsmall genre classification dataset.", "published": "2022-11-02 17:38:33", "link": "http://arxiv.org/abs/2211.01317v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
