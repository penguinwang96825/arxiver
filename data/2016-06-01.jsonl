{"title": "Neural Network Translation Models for Grammatical Error Correction", "abstract": "Phrase-based statistical machine translation (SMT) systems have previously\nbeen used for the task of grammatical error correction (GEC) to achieve\nstate-of-the-art accuracy. The superiority of SMT systems comes from their\nability to learn text transformations from erroneous to corrected text, without\nexplicitly modeling error types. However, phrase-based SMT systems suffer from\nlimitations of discrete word representation, linear mapping, and lack of global\ncontext. In this paper, we address these limitations by using two different yet\ncomplementary neural network models, namely a neural network global lexicon\nmodel and a neural network joint model. These neural networks can generalize\nbetter by using continuous space representation of words and learn non-linear\nmappings. Moreover, they can leverage contextual information from the source\nsentence more effectively. By adding these two components, we achieve\nstatistically significant improvement in accuracy for grammatical error\ncorrection over a state-of-the-art GEC system.", "published": "2016-06-01 09:31:00", "link": "http://arxiv.org/abs/1606.00189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting N-Best Hypotheses to Improve an SMT Approach to Grammatical\n  Error Correction", "abstract": "Grammatical error correction (GEC) is the task of detecting and correcting\ngrammatical errors in texts written by second language learners. The\nstatistical machine translation (SMT) approach to GEC, in which sentences\nwritten by second language learners are translated to grammatically correct\nsentences, has achieved state-of-the-art accuracy. However, the SMT approach is\nunable to utilize global context. In this paper, we propose a novel approach to\nimprove the accuracy of GEC, by exploiting the n-best hypotheses generated by\nan SMT approach. Specifically, we build a classifier to score the edits in the\nn-best hypotheses. The classifier can be used to select appropriate edits or\nre-rank the n-best hypotheses. We apply these methods to a state-of-the-art GEC\nsystem that uses the SMT approach. Our experiments show that our methods\nachieve statistically significant improvements in accuracy over the best\npublished results on a benchmark test dataset on GEC.", "published": "2016-06-01 10:32:28", "link": "http://arxiv.org/abs/1606.00210v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Parsing for Argument-Clusters Coordination", "abstract": "Syntactic parsers perform poorly in prediction of Argument-Cluster\nCoordination (ACC). We change the PTB representation of ACC to be more suitable\nfor learning by a statistical PCFG parser, affecting 125 trees in the training\nset. Training on the modified trees yields a slight improvement in EVALB scores\non sections 22 and 23. The main evaluation is on a corpus of 4th grade science\nexams, in which ACC structures are prevalent. On this corpus, we obtain an\nimpressive x2.7 improvement in recovering ACC structures compared to a parser\ntrained on the original PTB trees.", "published": "2016-06-01 14:06:41", "link": "http://arxiv.org/abs/1606.00294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing and Hybridizing Count-based and Neural Language Models", "abstract": "Language models (LMs) are statistical models that calculate probabilities\nover sequences of words or other discrete symbols. Currently two major\nparadigms for language modeling exist: count-based n-gram models, which have\nadvantages of scalability and test-time speed, and neural LMs, which often\nachieve superior modeling performance. We demonstrate how both varieties of\nmodels can be unified in a single modeling framework that defines a set of\nprobability distributions over the vocabulary of words, and then dynamically\ncalculates mixture weights over these distributions. This formulation allows us\nto create novel hybrid models that combine the desirable features of\ncount-based and neural LMs, and experiments demonstrate the advantages of these\napproaches.", "published": "2016-06-01 23:26:20", "link": "http://arxiv.org/abs/1606.00499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "La representaci\u00f3n de la variaci\u00f3n contextual mediante definiciones\n  terminol\u00f3gicas flexibles", "abstract": "In this doctoral thesis, we apply premises of cognitive linguistics to\nterminological definitions and present a proposal called the flexible\nterminological definition. This consists of a set of definitions of the same\nconcept made up of a general definition (in this case, one encompassing the\nentire environmental domain) along with additional definitions describing the\nconcept from the perspective of the subdomains in which it is relevant. Since\ncontext is a determining factor in the construction of the meaning of lexical\nunits (including terms), we assume that terminological definitions can, and\nshould, reflect the effects of context, even though definitions have\ntraditionally been treated as the expression of meaning void of any contextual\neffect. The main objective of this thesis is to analyze the effects of\ncontextual variation on specialized environmental concepts with a view to their\nrepresentation in terminological definitions. Specifically, we focused on\ncontextual variation based on thematic restrictions. To accomplish the\nobjectives of this doctoral thesis, we conducted an empirical study consisting\nof the analysis of a set of contextually variable concepts and the creation of\na flexible definition for two of them. As a result of the first part of our\nempirical study, we divided our notion of domain-dependent contextual variation\ninto three different phenomena: modulation, perspectivization and\nsubconceptualization. These phenomena are additive in that all concepts\nexperience modulation, some concepts also undergo perspectivization, and\nfinally, a small number of concepts are additionally subjected to\nsubconceptualization. In the second part, we applied these notions to\nterminological definitions and we presented we presented guidelines on how to\nbuild flexible definitions, from the extraction of knowledge to the actual\nwriting of the definition.", "published": "2016-06-01 09:39:12", "link": "http://arxiv.org/abs/1607.06330v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Contextual Cues: The Case of Personalization and History\n  for Response Ranking", "abstract": "We investigate the task of modeling open-domain, multi-turn, unstructured,\nmulti-participant, conversational dialogue. We specifically study the effect of\nincorporating different elements of the conversation. Unlike previous efforts,\nwhich focused on modeling messages and responses, we extend the modeling to\nlong context and participant's history. Our system does not rely on handwritten\nrules or engineered features; instead, we train deep neural networks on a large\nconversational dataset. In particular, we exploit the structure of Reddit\ncomments and posts to extract 2.1 billion messages and 133 million\nconversations. We evaluate our models on the task of predicting the next\nresponse in a conversation, and we find that modeling both context and\nparticipants improves prediction accuracy.", "published": "2016-06-01 18:01:14", "link": "http://arxiv.org/abs/1606.00372v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On a Topic Model for Sentences", "abstract": "Probabilistic topic models are generative models that describe the content of\ndocuments by discovering the latent topics underlying them. However, the\nstructure of the textual input, and for instance the grouping of words in\ncoherent text spans such as sentences, contains much information which is\ngenerally lost with these models. In this paper, we propose sentenceLDA, an\nextension of LDA whose goal is to overcome this limitation by incorporating the\nstructure of the text in the generative and inference processes. We illustrate\nthe advantages of sentenceLDA by comparing it with LDA using both intrinsic\n(perplexity) and extrinsic (text classification) evaluation tasks on different\ntext collections.", "published": "2016-06-01 12:34:50", "link": "http://arxiv.org/abs/1606.00253v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Temporal Topic Modeling to Assess Associations between News Trends and\n  Infectious Disease Outbreaks", "abstract": "In retrospective assessments, internet news reports have been shown to\ncapture early reports of unknown infectious disease transmission prior to\nofficial laboratory confirmation. In general, media interest and reporting\npeaks and wanes during the course of an outbreak. In this study, we quantify\nthe extent to which media interest during infectious disease outbreaks is\nindicative of trends of reported incidence. We introduce an approach that uses\nsupervised temporal topic models to transform large corpora of news articles\ninto temporal topic trends. The key advantages of this approach include,\napplicability to a wide range of diseases, and ability to capture disease\ndynamics - including seasonality, abrupt peaks and troughs. We evaluated the\nmethod using data from multiple infectious disease outbreaks reported in the\nUnited States of America (U.S.), China and India. We noted that temporal topic\ntrends extracted from disease-related news reports successfully captured the\ndynamics of multiple outbreaks such as whooping cough in U.S. (2012), dengue\noutbreaks in India (2013) and China (2014). Our observations also suggest that\nefficient modeling of temporal topic trends using time-series regression\ntechniques can estimate disease case counts with increased precision before\nofficial reports by health organizations.", "published": "2016-06-01 19:30:07", "link": "http://arxiv.org/abs/1606.00411v1", "categories": ["cs.SI", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.SI"}
