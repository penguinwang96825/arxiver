{"title": "GeoLM: Empowering Language Models for Geospatially Grounded Language\n  Understanding", "abstract": "Humans subconsciously engage in geospatial reasoning when reading articles.\nWe recognize place names and their spatial relations in text and mentally\nassociate them with their physical locations on Earth. Although pretrained\nlanguage models can mimic this cognitive process using linguistic context, they\ndo not utilize valuable geospatial information in large, widely available\ngeographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a\ngeospatially grounded language model that enhances the understanding of\ngeo-entities in natural language. GeoLM leverages geo-entity mentions as\nanchors to connect linguistic information in text corpora with geospatial\ninformation extracted from geographical databases. GeoLM connects the two types\nof context through contrastive learning and masked language modeling. It also\nincorporates a spatial coordinate embedding mechanism to encode distance and\ndirection relations to capture geospatial context. In the experiment, we\ndemonstrate that GeoLM exhibits promising capabilities in supporting toponym\nrecognition, toponym linking, relation extraction, and geo-entity typing, which\nbridge the gap between natural language processing and geospatial sciences. The\ncode is publicly available at https://github.com/knowledge-computing/geolm.", "published": "2023-10-23 01:20:01", "link": "http://arxiv.org/abs/2310.14478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language\n  Models through Self-Consistency with Masked Predictions", "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable\nsuccess, but it has also raised concerns among people about the misuse of\nAI-generated texts. Therefore, an important question is how to detect whether\nthe texts are generated by ChatGPT or by humans. Existing detectors are built\non the assumption that there is a distribution gap between human-generated and\nAI-generated texts. These gaps are typically identified using statistical\ninformation or classifiers. In contrast to prior research methods, we find that\nlarge language models such as ChatGPT exhibit strong self-consistency in text\ngeneration and continuation. Self-consistency capitalizes on the intuition that\nAI-generated texts can still be reasoned with by large language models using\nthe same logical reasoning when portions of the texts are masked, which differs\nfrom human-generated texts. Using this observation, we subsequently proposed a\nnew method for AI-generated texts detection based on self-consistency with\nmasked predictions to determine whether a text is generated by LLMs. This\nmethod, which we call DetectGPT-SC. We conducted a series of experiments to\nevaluate the performance of DetectGPT-SC. In these experiments, we employed\nvarious mask scheme, zero-shot, and simple prompt for completing masked texts\nand self-consistency predictions. The results indicate that DetectGPT-SC\noutperforms the current state-of-the-art across different tasks.", "published": "2023-10-23 01:23:10", "link": "http://arxiv.org/abs/2310.14479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Fact Transfer", "abstract": "Text style transfer is a prominent task that aims to control the style of\ntext without inherently changing its factual content. To cover more text\nmodification applications, such as adapting past news for current events and\nrepurposing educational materials, we propose the task of text fact transfer,\nwhich seeks to transfer the factual content of a source text between topics\nwithout modifying its style. We find that existing language models struggle\nwith text fact transfer, due to their inability to preserve the specificity and\nphrasing of the source text, and tendency to hallucinate errors. To address\nthese issues, we design ModQGA, a framework that minimally modifies a source\ntext with a novel combination of end-to-end question generation and\nspecificity-aware question answering. Through experiments on four existing\ndatasets adapted for text fact transfer, we show that ModQGA can accurately\ntransfer factual content without sacrificing the style of the source text.", "published": "2023-10-23 01:36:44", "link": "http://arxiv.org/abs/2310.14486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning\n  Capabilities of Language Models", "abstract": "Recent work has shown that language models (LMs) have strong multi-step\n(i.e., procedural) reasoning capabilities. However, it is unclear whether LMs\nperform these tasks by cheating with answers memorized from pretraining corpus,\nor, via a multi-step reasoning mechanism. In this paper, we try to answer this\nquestion by exploring a mechanistic interpretation of LMs for multi-step\nreasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a\nreasoning tree resembling the correct reasoning process within it. We test this\nhypothesis by introducing a new probing approach (called MechanisticProbe) that\nrecovers the reasoning tree from the model's attention patterns. We use our\nprobe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element),\nand LLaMA on two simple language-based reasoning tasks (ProofWriter & AI2\nReasoning Challenge). We show that MechanisticProbe is able to detect the\ninformation of the reasoning tree from the model's attentions for most\nexamples, suggesting that the LM indeed is going through a process of\nmulti-step reasoning within its architecture in many cases.", "published": "2023-10-23 01:47:29", "link": "http://arxiv.org/abs/2310.14491v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversify Question Generation with Retrieval-Augmented Style Transfer", "abstract": "Given a textual passage and an answer, humans are able to ask questions with\nvarious expressions, but this ability is still challenging for most question\ngeneration (QG) systems. Existing solutions mainly focus on the internal\nknowledge within the given passage or the semantic word space for diverse\ncontent planning. These methods, however, have not considered the potential of\nexternal knowledge for expression diversity. To bridge this gap, we propose\nRAST, a framework for Retrieval-Augmented Style Transfer, where the objective\nis to utilize the style of diverse templates for question generation. For\ntraining RAST, we develop a novel Reinforcement Learning (RL) based approach\nthat maximizes a weighted combination of diversity reward and consistency\nreward. Here, the consistency reward is computed by a Question-Answering (QA)\nmodel, whereas the diversity reward measures how much the final output mimics\nthe retrieved template. Experimental results show that our method outperforms\nprevious diversity-driven baselines on diversity while being comparable in\nterms of consistency scores. Our code is available at\nhttps://github.com/gouqi666/RAST.", "published": "2023-10-23 02:27:31", "link": "http://arxiv.org/abs/2310.14503v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment analysis with adaptive multi-head attention in Transformer", "abstract": "We propose a novel framework based on the attention mechanism to identify the\nsentiment of a movie review document. Previous efforts on deep neural networks\nwith attention mechanisms focus on encoder and decoder with fixed numbers of\nmulti-head attention. Therefore, we need a mechanism to stop the attention\nprocess automatically if no more useful information can be read from the\nmemory.In this paper, we propose an adaptive multi-head attention architecture\n(AdaptAttn) which varies the number of attention heads based on length of\nsentences. AdaptAttn has a data preprocessing step where each document is\nclassified into any one of the three bins small, medium or large based on\nlength of the sentence. The document classified as small goes through two heads\nin each layer, the medium group passes four heads and the large group is\nprocessed by eight heads. We examine the merit of our model on the Stanford\nlarge movie review dataset. The experimental results show that the F1 score\nfrom our model is on par with the baseline model.", "published": "2023-10-23 02:32:30", "link": "http://arxiv.org/abs/2310.14505v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data\n  Augmentation for Multi-hop Fact Verification", "abstract": "Automatic multi-hop fact verification task has gained significant attention\nin recent years. Despite impressive results, these well-designed models perform\npoorly on out-of-domain data. One possible solution is to augment the training\ndata with counterfactuals, which are generated by minimally altering the causal\nfeatures of the original data. However, current counterfactual data\naugmentation techniques fail to handle multi-hop fact verification due to their\nincapability to preserve the complex logical relationships within multiple\ncorrelated texts. In this paper, we overcome this limitation by developing a\nrationale-sensitive method to generate linguistically diverse and\nlabel-flipping counterfactuals while preserving logical relationships. In\nspecific, the diverse and fluent counterfactuals are generated via an\nExplain-Edit-Generate architecture. Moreover, the checking and filtering\nmodules are proposed to regularize the counterfactual data with logical\nrelations and flipped labels. Experimental results show that the proposed\napproach outperforms the SOTA baselines and can generate linguistically diverse\ncounterfactual data without disrupting their logical relationships.", "published": "2023-10-23 02:39:14", "link": "http://arxiv.org/abs/2310.14508v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CITB: A Benchmark for Continual Instruction Tuning", "abstract": "Continual learning (CL) is a paradigm that aims to replicate the human\nability to learn and accumulate knowledge continually without forgetting\nprevious knowledge and transferring it to new tasks. Recent instruction tuning\n(IT) involves fine-tuning models to make them more adaptable to solving NLP\ntasks in general. However, it is still uncertain how instruction tuning works\nin the context of CL tasks. This challenging yet practical problem is\nformulated as Continual Instruction Tuning (CIT). In this work, we establish a\nCIT benchmark consisting of learning and evaluation protocols. We curate two\nlong dialogue task streams of different types, InstrDialog and InstrDialog++,\nto study various CL methods systematically. Our experiments show that existing\nCL methods do not effectively leverage the rich natural language instructions,\nand fine-tuning an instruction-tuned model sequentially can yield similar or\nbetter results. We further explore different aspects that might affect the\nlearning of CIT. We hope this benchmark will facilitate more research in this\ndirection.", "published": "2023-10-23 02:42:32", "link": "http://arxiv.org/abs/2310.14510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turn-Level Active Learning for Dialogue State Tracking", "abstract": "Dialogue state tracking (DST) plays an important role in task-oriented\ndialogue systems. However, collecting a large amount of turn-by-turn annotated\ndialogue data is costly and inefficient. In this paper, we propose a novel\nturn-level active learning framework for DST to actively select turns in\ndialogues to annotate. Given the limited labelling budget, experimental results\ndemonstrate the effectiveness of selective annotation of dialogue turns.\nAdditionally, our approach can effectively achieve comparable DST performance\nto traditional training approaches with significantly less annotated data,\nwhich provides a more efficient way to annotate new dialogue data.", "published": "2023-10-23 02:53:46", "link": "http://arxiv.org/abs/2310.14513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QUDEVAL: The Evaluation of Questions Under Discussion Discourse Parsing", "abstract": "Questions Under Discussion (QUD) is a versatile linguistic framework in which\ndiscourse progresses as continuously asking questions and answering them.\nAutomatic parsing of a discourse to produce a QUD structure thus entails a\ncomplex question generation task: given a document and an answer sentence,\ngenerate a question that satisfies linguistic constraints of QUD and can be\ngrounded in an anchor sentence in prior context. These questions are known to\nbe curiosity-driven and open-ended. This work introduces the first framework\nfor the automatic evaluation of QUD parsing, instantiating the theoretical\nconstraints of QUD in a concrete protocol. We present QUDeval, a dataset of\nfine-grained evaluation of 2,190 QUD questions generated from both fine-tuned\nsystems and LLMs. Using QUDeval, we show that satisfying all constraints of QUD\nis still challenging for modern LLMs, and that existing evaluation metrics\npoorly approximate parser quality. Encouragingly, human-authored QUDs are\nscored highly by our human evaluators, suggesting that there is headroom for\nfurther progress on language modeling to improve both QUD parsing and QUD\nevaluation.", "published": "2023-10-23 03:03:58", "link": "http://arxiv.org/abs/2310.14520v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Word-Level Auto-Completion in Computer-Aided Translation", "abstract": "Word-Level Auto-Completion (WLAC) plays a crucial role in Computer-Assisted\nTranslation. It aims at providing word-level auto-completion suggestions for\nhuman translators. While previous studies have primarily focused on designing\ncomplex model architectures, this paper takes a different perspective by\nrethinking the fundamental question: what kind of words are good\nauto-completions? We introduce a measurable criterion to answer this question\nand discover that existing WLAC models often fail to meet this criterion.\nBuilding upon this observation, we propose an effective approach to enhance\nWLAC performance by promoting adherence to the criterion. Notably, the proposed\napproach is general and can be applied to various encoder-based architectures.\nThrough extensive experiments, we demonstrate that our approach outperforms the\ntop-performing system submitted to the WLAC shared tasks in WMT2022, while\nutilizing significantly smaller model sizes.", "published": "2023-10-23 03:11:46", "link": "http://arxiv.org/abs/2310.14523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems", "abstract": "Efficient knowledge retrieval plays a pivotal role in ensuring the success of\nend-to-end task-oriented dialogue systems by facilitating the selection of\nrelevant information necessary to fulfill user requests. However, current\napproaches generally integrate knowledge retrieval and response generation,\nwhich poses scalability challenges when dealing with extensive knowledge bases.\nTaking inspiration from open-domain question answering, we propose a\nretriever-generator architecture that harnesses a retriever to retrieve\npertinent knowledge and a generator to generate system responses.~Due to the\nlack of retriever training labels, we propose relying on feedback from the\ngenerator as pseudo-labels to train the retriever. To achieve this, we\nintroduce a dual-feedback mechanism that generates both positive and negative\nfeedback based on the output of the generator. Our method demonstrates superior\nperformance in task-oriented dialogue tasks, as evidenced by experimental\nresults on three benchmark datasets.", "published": "2023-10-23 03:21:11", "link": "http://arxiv.org/abs/2310.14528v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Seq2Seq Grammatical Error Correction via Decoding\n  Interventions", "abstract": "The sequence-to-sequence (Seq2Seq) approach has recently been widely used in\ngrammatical error correction (GEC) and shows promising performance. However,\nthe Seq2Seq GEC approach still suffers from two issues. First, a Seq2Seq GEC\nmodel can only be trained on parallel data, which, in GEC task, is often noisy\nand limited in quantity. Second, the decoder of a Seq2Seq GEC model lacks an\nexplicit awareness of the correctness of the token being generated. In this\npaper, we propose a unified decoding intervention framework that employs an\nexternal critic to assess the appropriateness of the token to be generated\nincrementally, and then dynamically influence the choice of the next token. We\ndiscover and investigate two types of critics: a pre-trained left-to-right\nlanguage model critic and an incremental target-side grammatical error detector\ncritic. Through extensive experiments on English and Chinese datasets, our\nframework consistently outperforms strong baselines and achieves results\ncompetitive with state-of-the-art methods.", "published": "2023-10-23 03:36:37", "link": "http://arxiv.org/abs/2310.14534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Continual Named Entity Recognition without Catastrophic Forgetting", "abstract": "Continual Named Entity Recognition (CNER) is a burgeoning area, which\ninvolves updating an existing model by incorporating new entity types\nsequentially. Nevertheless, continual learning approaches are often severely\nafflicted by catastrophic forgetting. This issue is intensified in CNER due to\nthe consolidation of old entity types from previous steps into the non-entity\ntype at each step, leading to what is known as the semantic shift problem of\nthe non-entity type. In this paper, we introduce a pooled feature distillation\nloss that skillfully navigates the trade-off between retaining knowledge of old\nentity types and acquiring new ones, thereby more effectively mitigating the\nproblem of catastrophic forgetting. Additionally, we develop a confidence-based\npseudo-labeling for the non-entity type, \\emph{i.e.,} predicting entity types\nusing the old model to handle the semantic shift of the non-entity type.\nFollowing the pseudo-labeling process, we suggest an adaptive re-weighting\ntype-balanced learning strategy to handle the issue of biased type\ndistribution. We carried out comprehensive experiments on ten CNER settings\nusing three different datasets. The results illustrate that our method\nsignificantly outperforms prior state-of-the-art approaches, registering an\naverage improvement of $6.3$\\% and $8.0$\\% in Micro and Macro F1 scores,\nrespectively.", "published": "2023-10-23 03:45:30", "link": "http://arxiv.org/abs/2310.14541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models on Controlled Generation Tasks", "abstract": "While recent studies have looked into the abilities of large language models\nin various benchmark tasks, including question generation, reading\ncomprehension, multilingual and etc, there have been few studies looking into\nthe controllability of large language models on generation tasks. We present an\nextensive analysis of various benchmarks including a sentence planning\nbenchmark with different granularities. After comparing large language models\nagainst state-of-the-start finetuned smaller models, we present a spectrum\nshowing large language models falling behind, are comparable, or exceed the\nability of smaller models. We conclude that **large language models struggle at\nmeeting fine-grained hard constraints**.", "published": "2023-10-23 03:48:24", "link": "http://arxiv.org/abs/2310.14542v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Harnessing ChatGPT for thematic analysis: Are we ready?", "abstract": "ChatGPT is an advanced natural language processing tool with growing\napplications across various disciplines in medical research. Thematic analysis,\na qualitative research method to identify and interpret patterns in data, is\none application that stands to benefit from this technology. This viewpoint\nexplores the utilization of ChatGPT in three core phases of thematic analysis\nwithin a medical context: 1) direct coding of transcripts, 2) generating themes\nfrom a predefined list of codes, and 3) preprocessing quotes for manuscript\ninclusion. Additionally, we explore the potential of ChatGPT to generate\ninterview transcripts, which may be used for training purposes. We assess the\nstrengths and limitations of using ChatGPT in these roles, highlighting areas\nwhere human intervention remains necessary. Overall, we argue that ChatGPT can\nfunction as a valuable tool during analysis, enhancing the efficiency of the\nthematic analysis and offering additional insights into the qualitative data.", "published": "2023-10-23 03:55:13", "link": "http://arxiv.org/abs/2310.14545v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64\n  Languages", "abstract": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate\nremarkable performance in a wide range of tasks. Despite numerous recent\nstudies that examine the performance of instruction-tuned LLMs on various NLP\nbenchmarks, there remains a lack of comprehensive investigation into their\nability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning\nembedded within social and interactive contexts. This deficiency arises partly\nfrom SM not being adequately represented in any of the existing benchmarks. To\naddress this gap, we present SPARROW, an extensive multilingual benchmark\nspecifically designed for SM understanding. SPARROW comprises 169 datasets\ncovering 13 task types across six primary categories (e.g., anti-social\nlanguage detection, emotion recognition). SPARROW datasets encompass 64\ndifferent languages originating from 12 language families representing 16\nwriting scripts. We evaluate the performance of various multilingual pretrained\nlanguage models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT)\non SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our\ncomprehensive analysis reveals that existing open-source instruction tuned LLMs\nstill struggle to understand SM across various languages, performing close to a\nrandom baseline in some cases. We also find that although ChatGPT outperforms\nmany LLMs, it still falls behind task-specific finetuned models with a gap of\n12.19 SPARROW score. Our benchmark is available at:\nhttps://github.com/UBC-NLP/SPARROW", "published": "2023-10-23 04:22:44", "link": "http://arxiv.org/abs/2310.14557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models Hallucinate, but May Excel at Fact Verification", "abstract": "Recent progress in natural language processing (NLP) owes much to remarkable\nadvances in large language models (LLMs). Nevertheless, LLMs frequently\n\"hallucinate,\" resulting in non-factual outputs. Our carefully-designed human\nevaluation substantiates the serious hallucination issue, revealing that even\nGPT-3.5 produces factual outputs less than 25% of the time. This underscores\nthe importance of fact verifiers in order to measure and incentivize progress.\nOur systematic investigation affirms that LLMs can be repurposed as effective\nfact verifiers with strong correlations with human judgments. Surprisingly,\nFLAN-T5-11B, the least factual generator in our study, performs the best as a\nfact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT.\nDelving deeper, we analyze the reliance of these LLMs on high-quality evidence,\nas well as their deficiencies in robustness and generalization ability. Our\nstudy presents insights for developing trustworthy generation models.", "published": "2023-10-23 04:39:01", "link": "http://arxiv.org/abs/2310.14564v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Boundaries of GPT-4 in Radiology", "abstract": "The recent success of general-domain large language models (LLMs) has\nsignificantly changed the natural language processing paradigm towards a\nunified foundation model across domains and applications. In this paper, we\nfocus on assessing the performance of GPT-4, the most capable LLM so far, on\nthe text-based applications for radiology reports, comparing against\nstate-of-the-art (SOTA) radiology-specific models. Exploring various prompting\nstrategies, we evaluated GPT-4 on a diverse range of common radiology tasks and\nwe found GPT-4 either outperforms or is on par with current SOTA radiology\nmodels. With zero-shot prompting, GPT-4 already obtains substantial gains\n($\\approx$ 10% absolute improvement) over radiology models in temporal sentence\nsimilarity classification (accuracy) and natural language inference ($F_1$).\nFor tasks that require learning dataset-specific style or schema (e.g. findings\nsummarisation), GPT-4 improves with example-based prompting and matches\nsupervised SOTA. Our extensive error analysis with a board-certified\nradiologist shows GPT-4 has a sufficient level of radiology knowledge with only\noccasional errors in complex context that require nuanced domain knowledge. For\nfindings summarisation, GPT-4 outputs are found to be overall comparable with\nexisting manually-written impressions.", "published": "2023-10-23 05:13:03", "link": "http://arxiv.org/abs/2310.14573v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Pre-trained Transformer for Vietnamese Community-based\n  COVID-19 Question Answering", "abstract": "Recent studies have provided empirical evidence of the wide-ranging potential\nof Generative Pre-trained Transformer (GPT), a pretrained language model, in\nthe field of natural language processing. GPT has been effectively employed as\na decoder within state-of-the-art (SOTA) question answering systems, yielding\nexceptional performance across various tasks. However, the current research\nlandscape concerning GPT's application in Vietnamese remains limited. This\npaper aims to address this gap by presenting an implementation of GPT-2 for\ncommunity-based question answering specifically focused on COVID-19 related\nqueries in Vietnamese. We introduce a novel approach by conducting a\ncomparative analysis of different Transformers vs SOTA models in the\ncommunity-based COVID-19 question answering dataset. The experimental findings\ndemonstrate that the GPT-2 models exhibit highly promising outcomes,\noutperforming other SOTA models as well as previous community-based COVID-19\nquestion answering models developed for Vietnamese.", "published": "2023-10-23 06:14:07", "link": "http://arxiv.org/abs/2310.14602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Long Short-Term Planning for Conversational Recommendation Systems", "abstract": "In Conversational Recommendation Systems (CRS), the central question is how\nthe conversational agent can naturally ask for user preferences and provide\nsuitable recommendations. Existing works mainly follow the hierarchical\narchitecture, where a higher policy decides whether to invoke the conversation\nmodule (to ask questions) or the recommendation module (to make\nrecommendations). This architecture prevents these two components from fully\ninteracting with each other. In contrast, this paper proposes a novel\narchitecture, the long short-term feedback architecture, to connect these two\nessential components in CRS. Specifically, the recommendation predicts the\nlong-term recommendation target based on the conversational context and the\nuser history. Driven by the targeted recommendation, the conversational model\npredicts the next topic or attribute to verify if the user preference matches\nthe target. The balance feedback loop continues until the short-term planner\noutput matches the long-term planner output, that is when the system should\nmake the recommendation.", "published": "2023-10-23 06:34:39", "link": "http://arxiv.org/abs/2310.14609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "That was the last straw, we need more: Are Translation Systems Sensitive\n  to Disambiguating Context?", "abstract": "The translation of ambiguous text presents a challenge for translation\nsystems, as it requires using the surrounding context to disambiguate the\nintended meaning as much as possible. While prior work has studied ambiguities\nthat result from different grammatical features of the source and target\nlanguage, we study semantic ambiguities that exist in the source (English in\nthis work) itself. In particular, we focus on idioms that are open to both\nliteral and figurative interpretations (e.g., goose egg), and collect TIDE, a\ndataset of 512 pairs of English sentences containing idioms with disambiguating\ncontext such that one is literal (it laid a goose egg) and another is\nfigurative (they scored a goose egg, as in a score of zero). In experiments, we\ncompare MT-specific models and language models for (i) their preference when\ngiven an ambiguous subsentence, (ii) their sensitivity to disambiguating\ncontext, and (iii) the performance disparity between figurative and literal\nsource sentences. We find that current MT models consistently translate English\nidioms literally, even when the context suggests a figurative interpretation.\nOn the other hand, LMs are far more context-aware, although there remain\ndisparities across target languages. Our findings underline the potential of\nLMs as a strong backbone for context-aware translation.", "published": "2023-10-23 06:38:49", "link": "http://arxiv.org/abs/2310.14610v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Cross-Task Prompt Tuning for Few-Shot Conversational Emotion\n  Recognition", "abstract": "Emotion Recognition in Conversation (ERC) has been widely studied due to its\nimportance in developing emotion-aware empathetic machines. The rise of\npre-trained language models (PLMs) has further pushed the limit of ERC\nperformance. However, most recent works on ERC using PLMs are heavily\ndata-driven, and requires fine-tuning the entire PLMs. To improve both sample\nand computational efficiency, we propose a derivative-free optimization method\ncalled Cross-Task Prompt Tuning (CTPT) for few-shot conversational emotion\nrecognition. Unlike existing methods that learn independent knowledge from\nindividual tasks, CTPT leverages sharable cross-task knowledge by exploiting\nexternal knowledge from other source tasks to improve learning performance\nunder the few-shot setting. Moreover, CTPT only needs to optimize a vector\nunder the low intrinsic dimensionality without gradient, which is highly\nparameter-efficient compared with existing approaches. Experiments on five\ndifferent contextual conversation datasets demonstrate that our CTPT method has\nsuperior results on both few-shot scenarios and zero-shot transfers.", "published": "2023-10-23 06:46:03", "link": "http://arxiv.org/abs/2310.14614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts", "abstract": "As large language models (LLMs) have shown effectiveness with different\nprompting methods, such as Chain of Thought, Program of Thought, we find that\nthese methods have formed a great complementarity to each other on math\nreasoning tasks. In this work, we propose XoT, an integrated problem solving\nframework by prompting LLMs with diverse reasoning thoughts. For each question,\nXoT always begins with selecting the most suitable method then executes each\nmethod iteratively. Within each iteration, XoT actively checks the validity of\nthe generated answer and incorporates the feedback from external executors,\nallowing it to dynamically switch among different prompting methods. Through\nextensive experiments on 10 popular math reasoning datasets, we demonstrate the\neffectiveness of our proposed approach and thoroughly analyze the strengths of\neach module. Moreover, empirical results suggest that our framework is\northogonal to recent work that makes improvements on single reasoning methods\nand can further generalise to logical reasoning domain. By allowing method\nswitching, XoT provides a fresh perspective on the collaborative integration of\ndiverse reasoning thoughts in a unified framework. The code is available at\nhttps://github.com/tengxiaoliu/XoT.", "published": "2023-10-23 07:02:20", "link": "http://arxiv.org/abs/2310.14628v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual k-Nearest-Neighbor Machine Translation", "abstract": "k-nearest-neighbor machine translation has demonstrated remarkable\nimprovements in machine translation quality by creating a datastore of cached\nexamples. However, these improvements have been limited to high-resource\nlanguage pairs, with large datastores, and remain a challenge for low-resource\nlanguages. In this paper, we address this issue by combining representations\nfrom multiple languages into a single datastore. Our results consistently\ndemonstrate substantial improvements not only in low-resource translation\nquality (up to +3.6 BLEU), but also for high-resource translation quality (up\nto +0.5 BLEU). Our experiments show that it is possible to create multilingual\ndatastores that are a quarter of the size, achieving a 5.3x speed improvement,\nby using linguistic similarities for datastore creation.", "published": "2023-10-23 07:35:37", "link": "http://arxiv.org/abs/2310.14644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for\n  Natural Language Understanding", "abstract": "Human gaze data offer cognitive information that reflects natural language\ncomprehension. Indeed, augmenting language models with human scanpaths has\nproven beneficial for a range of NLP tasks, including language understanding.\nHowever, the applicability of this approach is hampered because the abundance\nof text corpora is contrasted by a scarcity of gaze data. Although models for\nthe generation of human-like scanpaths during reading have been developed, the\npotential of synthetic gaze data across NLP tasks remains largely unexplored.\nWe develop a model that integrates synthetic scanpath generation with a\nscanpath-augmented language model, eliminating the need for human gaze data.\nSince the model's error gradient can be propagated throughout all parts of the\nmodel, the scanpath generator can be fine-tuned to downstream tasks. We find\nthat the proposed model not only outperforms the underlying language model, but\nachieves a performance that is comparable to a language model augmented with\nreal human gaze data. Our code is publicly available.", "published": "2023-10-23 08:15:38", "link": "http://arxiv.org/abs/2310.14676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpEL: Structured Prediction for Entity Linking", "abstract": "Entity linking is a prominent thread of research focused on structured data\ncreation by linking spans of text to an ontology or knowledge source. We\nrevisit the use of structured prediction for entity linking which classifies\neach individual input token as an entity, and aggregates the token predictions.\nOur system, called SpEL (Structured prediction for Entity Linking) is a\nstate-of-the-art entity linking system that uses some new ideas to apply\nstructured prediction to the task of entity linking including: two refined\nfine-tuning steps; a context sensitive prediction aggregation strategy;\nreduction of the size of the model's output vocabulary, and; we address a\ncommon problem in entity-linking systems where there is a training vs.\ninference tokenization mismatch. Our experiments show that we can outperform\nthe state-of-the-art on the commonly used AIDA benchmark dataset for entity\nlinking to Wikipedia. Our method is also very compute efficient in terms of\nnumber of parameters and speed of inference.", "published": "2023-10-23 08:24:35", "link": "http://arxiv.org/abs/2310.14684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tree of Clarifications: Answering Ambiguous Questions with\n  Retrieval-Augmented Large Language Models", "abstract": "Questions in open-domain question answering are often ambiguous, allowing\nmultiple interpretations. One approach to handling them is to identify all\npossible interpretations of the ambiguous question (AQ) and to generate a\nlong-form answer addressing them all, as suggested by Stelmakh et al., (2022).\nWhile it provides a comprehensive response without bothering the user for\nclarification, considering multiple dimensions of ambiguity and gathering\ncorresponding knowledge remains a challenge. To cope with the challenge, we\npropose a novel framework, Tree of Clarifications (ToC): It recursively\nconstructs a tree of disambiguations for the AQ -- via few-shot prompting\nleveraging external knowledge -- and uses it to generate a long-form answer.\nToC outperforms existing baselines on ASQA in a few-shot setup across the\nmetrics, while surpassing fully-supervised baselines trained on the whole\ntraining set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at\nhttps://github.com/gankim/tree-of-clarifications.", "published": "2023-10-23 08:42:49", "link": "http://arxiv.org/abs/2310.14696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Establishing Vocabulary Tests as a Benchmark for Evaluating Large\n  Language Models", "abstract": "Vocabulary tests, once a cornerstone of language modeling evaluation, have\nbeen largely overlooked in the current landscape of Large Language Models\n(LLMs) like Llama, Mistral, and GPT. While most LLM evaluation benchmarks focus\non specific tasks or domain-specific knowledge, they often neglect the\nfundamental linguistic aspects of language understanding and production. In\nthis paper, we advocate for the revival of vocabulary tests as a valuable tool\nfor assessing LLM performance. We evaluate seven LLMs using two vocabulary test\nformats across two languages and uncover surprising gaps in their lexical\nknowledge. These findings shed light on the intricacies of LLM word\nrepresentations, their learning mechanisms, and performance variations across\nmodels and languages. Moreover, the ability to automatically generate and\nperform vocabulary tests offers new opportunities to expand the approach and\nprovide a more complete picture of LLMs' language skills.", "published": "2023-10-23 08:45:12", "link": "http://arxiv.org/abs/2310.14703v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strong and Efficient Baselines for Open Domain Conversational Question\n  Answering", "abstract": "Unlike the Open Domain Question Answering (ODQA) setting, the conversational\n(ODConvQA) domain has received limited attention when it comes to reevaluating\nbaselines for both efficiency and effectiveness. In this paper, we study the\nState-of-the-Art (SotA) Dense Passage Retrieval (DPR) retriever and\nFusion-in-Decoder (FiD) reader pipeline, and show that it significantly\nunderperforms when applied to ODConvQA tasks due to various limitations. We\nthen propose and evaluate strong yet simple and efficient baselines, by\nintroducing a fast reranking component between the retriever and the reader,\nand by performing targeted finetuning steps. Experiments on two ODConvQA tasks,\nnamely TopiOCQA and OR-QuAC, show that our method improves the SotA results,\nwhile reducing reader's latency by 60%. Finally, we provide new and valuable\ninsights into the development of challenging baselines that serve as a\nreference for future, more intricate approaches, including those that leverage\nLarge Language Models (LLMs).", "published": "2023-10-23 08:48:14", "link": "http://arxiv.org/abs/2310.14708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Once Upon a $\\textit{Time}$ in $\\textit{Graph}$: Relative-Time\n  Pretraining for Complex Temporal Reasoning", "abstract": "Our physical world is constantly evolving over time, rendering challenges for\npre-trained language models to understand and reason over the temporal contexts\nof texts. Existing work focuses on strengthening the direct association between\na piece of text and its time-stamp. However, the knowledge-time association is\nusually insufficient for the downstream tasks that require reasoning over\ntemporal dependencies between knowledge. In this work, we make use of the\nunderlying nature of time, all temporally-scoped sentences are strung together\nthrough a one-dimensional time axis, and suggest creating a graph structure\nbased on the relative placements of events along the time axis. Inspired by the\ngraph view, we propose RemeMo ($\\underline{Re}$lative Ti$\\underline{me}$\n$\\underline{Mo}$deling), which explicitly connects all temporally-scoped facts\nby modeling the time relations between any two sentences. Experimental results\nshow that RemeMo outperforms the baseline T5 on multiple temporal question\nanswering datasets under various settings. Further analysis suggests that\nRemeMo is especially good at modeling long-range complex temporal dependencies.\nWe release our code and pre-trained checkpoints at\n$\\href{https://github.com/DAMO-NLP-SG/RemeMo}{\\text{this url}}$.", "published": "2023-10-23 08:49:00", "link": "http://arxiv.org/abs/2310.14709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCC-KD: Multi-CoT Consistent Knowledge Distillation", "abstract": "Large language models (LLMs) have showcased remarkable capabilities in\ncomplex reasoning through chain of thought (CoT) prompting. Recently, there has\nbeen a growing interest in transferring these reasoning abilities from LLMs to\nsmaller models. However, achieving both the diversity and consistency in\nrationales presents a challenge. In this paper, we focus on enhancing these two\naspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to\nefficiently distill the reasoning capabilities. In MCC-KD, we generate multiple\nrationales for each question and enforce consistency among the corresponding\npredictions by minimizing the bidirectional KL-divergence between the answer\ndistributions. We investigate the effectiveness of MCC-KD with different model\narchitectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both\nmathematical reasoning and commonsense reasoning benchmarks. The empirical\nresults not only confirm MCC-KD's superior performance on in-distribution\ndatasets but also highlight its robust generalization ability on\nout-of-distribution datasets.", "published": "2023-10-23 09:32:53", "link": "http://arxiv.org/abs/2310.14747v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SuperTweetEval: A Challenging, Unified and Heterogeneous Benchmark for\n  Social Media NLP Research", "abstract": "Despite its relevance, the maturity of NLP for social media pales in\ncomparison with general-purpose models, metrics and benchmarks. This fragmented\nlandscape makes it hard for the community to know, for instance, given a task,\nwhich is the best performing model and how it compares with others. To\nalleviate this issue, we introduce a unified benchmark for NLP evaluation in\nsocial media, SuperTweetEval, which includes a heterogeneous set of tasks and\ndatasets combined, adapted and constructed from scratch. We benchmarked the\nperformance of a wide range of models on SuperTweetEval and our results suggest\nthat, despite the recent advances in language modelling, social media remains\nchallenging.", "published": "2023-10-23 09:48:25", "link": "http://arxiv.org/abs/2310.14757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Modal Conceptualization in Bottleneck Models", "abstract": "Concept Bottleneck Models (CBMs) assume that training examples (e.g., x-ray\nimages) are annotated with high-level concepts (e.g., types of abnormalities),\nand perform classification by first predicting the concepts, followed by\npredicting the label relying on these concepts. The main difficulty in using\nCBMs comes from having to choose concepts that are predictive of the label and\nthen having to label training examples with these concepts. In our approach, we\nadopt a more moderate assumption and instead use text descriptions (e.g.,\nradiology reports), accompanying the images in training, to guide the induction\nof concepts. Our cross-modal approach treats concepts as discrete latent\nvariables and promotes concepts that (1) are predictive of the label, and (2)\ncan be predicted reliably from both the image and text. Through experiments\nconducted on datasets ranging from synthetic datasets (e.g., synthetic images\nwith generated descriptions) to realistic medical imaging datasets, we\ndemonstrate that cross-modal learning encourages the induction of interpretable\nconcepts while also facilitating disentanglement. Our results also suggest that\nthis guidance leads to increased robustness by suppressing the reliance on\nshortcut features.", "published": "2023-10-23 11:00:19", "link": "http://arxiv.org/abs/2310.14805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Multilingual Competency of LLMs in Multi-Turn Instruction\n  Following: A Case Study of Arabic", "abstract": "While significant progress has been made in benchmarking Large Language\nModels (LLMs) across various tasks, there is a lack of comprehensive evaluation\nof their abilities in responding to multi-turn instructions in less-commonly\ntested languages like Arabic. Our paper offers a detailed examination of the\nproficiency of open LLMs in such scenarios in Arabic. Utilizing a customized\nArabic translation of the MT-Bench benchmark suite, we employ GPT-4 as a\nuniform evaluator for both English and Arabic queries to assess and compare the\nperformance of the LLMs on various open-ended tasks. Our findings reveal\nvariations in model responses on different task categories, e.g., logic vs.\nliteracy, when instructed in English or Arabic. We find that fine-tuned base\nmodels using multilingual and multi-turn datasets could be competitive to\nmodels trained from scratch on multilingual data. Finally, we hypothesize that\nan ensemble of small, open LLMs could perform competitively to proprietary LLMs\non the benchmark.", "published": "2023-10-23 11:40:04", "link": "http://arxiv.org/abs/2310.14819v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALCUNA: Large Language Models Meet New Knowledge", "abstract": "With the rapid development of NLP, large-scale language models (LLMs) excel\nin various tasks across multiple domains now. However, existing benchmarks may\nnot adequately measure these models' capabilities, especially when faced with\nnew knowledge. In this paper, we address the lack of benchmarks to evaluate\nLLMs' ability to handle new knowledge, an important and challenging aspect in\nthe rapidly evolving world. We propose an approach called KnowGen that\ngenerates new knowledge by altering existing entity attributes and\nrelationships, resulting in artificial entities that are distinct from\nreal-world entities. With KnowGen, we introduce a benchmark named ALCUNA to\nassess LLMs' abilities in knowledge understanding, differentiation, and\nassociation. We benchmark several LLMs, reveals that their performance in face\nof new knowledge is not satisfactory, particularly in reasoning between new and\ninternal knowledge. We also explore the impact of entity similarity on the\nmodel's understanding of entity knowledge and the influence of contextual\nentities. We appeal to the need for caution when using LLMs in new scenarios or\nwith new knowledge, and hope that our benchmarks can help drive the development\nof LLMs in face of new knowledge.", "published": "2023-10-23 11:40:05", "link": "http://arxiv.org/abs/2310.14820v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transparency at the Source: Evaluating and Interpreting Language Models\n  With Access to the True Distribution", "abstract": "We present a setup for training, evaluating and interpreting neural language\nmodels, that uses artificial, language-like data. The data is generated using a\nmassive probabilistic grammar (based on state-split PCFGs), that is itself\nderived from a large natural language corpus, but also provides us complete\ncontrol over the generative process. We describe and release both grammar and\ncorpus, and test for the naturalness of our generated data. This approach\nallows us to define closed-form expressions to efficiently compute exact lower\nbounds on obtainable perplexity using both causal and masked language\nmodelling. Our results show striking differences between neural language\nmodelling architectures and training objectives in how closely they allow\napproximating the lower bound on perplexity. Our approach also allows us to\ndirectly compare learned representations to symbolic rules in the underlying\nsource. We experiment with various techniques for interpreting model behaviour\nand learning dynamics. With access to the underlying true source, our results\nshow striking differences and outcomes in learning dynamics between different\nclasses of words.", "published": "2023-10-23 12:03:01", "link": "http://arxiv.org/abs/2310.14840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Domain Adaptation for Robust Handling of Distributional Shifts\n  in NLP", "abstract": "When deploying machine learning systems to the wild, it is highly desirable\nfor them to effectively leverage prior knowledge to the unfamiliar domain while\nalso firing alarms to anomalous inputs. In order to address these requirements,\nUniversal Domain Adaptation (UniDA) has emerged as a novel research area in\ncomputer vision, focusing on achieving both adaptation ability and robustness\n(i.e., the ability to detect out-of-distribution samples). While UniDA has led\nsignificant progress in computer vision, its application on language input\nstill needs to be explored despite its feasibility. In this paper, we propose a\ncomprehensive benchmark for natural language that offers thorough viewpoints of\nthe model's generalizability and robustness. Our benchmark encompasses multiple\ndatasets with varying difficulty levels and characteristics, including temporal\nshifts and diverse domains. On top of our testbed, we validate existing UniDA\nmethods from computer vision and state-of-the-art domain adaptation techniques\nfrom NLP literature, yielding valuable findings: We observe that UniDA methods\noriginally designed for image input can be effectively transferred to the\nnatural language domain while also underscoring the effect of adaptation\ndifficulty in determining the model's performance.", "published": "2023-10-23 12:15:25", "link": "http://arxiv.org/abs/2310.14849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Policy with Wait-$k$ Model for Simultaneous Translation", "abstract": "Simultaneous machine translation (SiMT) requires a robust read/write policy\nin conjunction with a high-quality translation model. Traditional methods rely\non either a fixed wait-$k$ policy coupled with a standalone wait-$k$\ntranslation model, or an adaptive policy jointly trained with the translation\nmodel. In this study, we propose a more flexible approach by decoupling the\nadaptive policy model from the translation model. Our motivation stems from the\nobservation that a standalone multi-path wait-$k$ model performs competitively\nwith adaptive policies utilized in state-of-the-art SiMT approaches.\nSpecifically, we introduce DaP, a divergence-based adaptive policy, that makes\nread/write decisions for any translation model based on the potential\ndivergence in translation distributions resulting from future information. DaP\nextends a frozen wait-$k$ model with lightweight parameters, and is both memory\nand computation efficient. Experimental results across various benchmarks\ndemonstrate that our approach offers an improved trade-off between translation\naccuracy and latency, outperforming strong baselines.", "published": "2023-10-23 12:16:32", "link": "http://arxiv.org/abs/2310.14853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrase Types for Generation and Detection", "abstract": "Current approaches in paraphrase generation and detection heavily rely on a\nsingle general similarity score, ignoring the intricate linguistic properties\nof language. This paper introduces two new tasks to address this shortcoming by\nconsidering paraphrase types - specific linguistic perturbations at particular\ntext positions. We name these tasks Paraphrase Type Generation and Paraphrase\nType Detection. Our results suggest that while current techniques perform well\nin a binary classification scenario, i.e., paraphrased or not, the inclusion of\nfine-grained paraphrase types poses a significant challenge. While most\napproaches are good at generating and detecting general semantic similar\ncontent, they fail to understand the intrinsic linguistic variables they\nmanipulate. Models trained in generating and identifying paraphrase types also\nshow improvements in tasks without them. In addition, scaling these models\nfurther improves their ability to understand paraphrase types. We believe\nparaphrase types can unlock a new paradigm for developing paraphrase models and\nsolving tasks in the future.", "published": "2023-10-23 12:32:41", "link": "http://arxiv.org/abs/2310.14863v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Step-by-Step Reasoning against Lexical Negation: A Case Study\n  on Syllogism", "abstract": "Large language models (LLMs) take advantage of step-by-step reasoning\ninstructions, e.g., chain-of-thought (CoT) prompting. Building on this, their\nability to perform CoT-style reasoning robustly is of interest from a probing\nperspective. In this study, we inspect the step-by-step reasoning ability of\nLLMs with a focus on negation, which is a core linguistic phenomenon that is\ndifficult to process. In particular, we introduce several controlled settings\n(e.g., reasoning in case of fictional entities) to evaluate the logical\nreasoning abilities of the models. We observed that dozens of modern LLMs were\nnot robust against lexical negation (e.g., plausible ->implausible) when\nperforming CoT-style reasoning, and the results highlight unique limitations in\neach LLM family.", "published": "2023-10-23 12:40:41", "link": "http://arxiv.org/abs/2310.14868v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can ChatGPT Perform Reasoning Using the IRAC Method in Analyzing Legal\n  Scenarios Like a Lawyer?", "abstract": "Large Language Models (LLMs), such as ChatGPT, have drawn a lot of attentions\nrecently in the legal domain due to its emergent ability to tackle a variety of\nlegal tasks. However, it is still unknown if LLMs are able to analyze a legal\ncase and perform reasoning in the same manner as lawyers. Therefore, we\nconstructed a novel corpus consisting of scenarios pertain to Contract Acts\nMalaysia and Australian Social Act for Dependent Child. ChatGPT is applied to\nperform analysis on the corpus using the IRAC method, which is a framework\nwidely used by legal professionals for organizing legal analysis. Each scenario\nin the corpus is annotated with a complete IRAC analysis in a semi-structured\nformat so that both machines and legal professionals are able to interpret and\nunderstand the annotations. In addition, we conducted the first empirical\nassessment of ChatGPT for IRAC analysis in order to understand how well it\naligns with the analysis of legal professionals. Our experimental results shed\nlights on possible future research directions to improve alignments between\nLLMs and legal experts in terms of legal reasoning.", "published": "2023-10-23 12:51:49", "link": "http://arxiv.org/abs/2310.14880v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time\n  Controllable Text Generation", "abstract": "Controllable text generation (CTG) aims to generate text with desired\nattributes, and decoding-time-based methods have shown promising performance on\nthis task. However, in this paper, we identify the phenomenon of Attribute\nCollapse for the first time. It causes the fluency of generated text to rapidly\ndecrease when the control strength exceeds a critical value, rendering the text\ncompletely unusable. This limitation hinders the effectiveness of decoding\nmethods in achieving high levels of controllability. To address this problem,\nwe propose a novel lightweight decoding framework named Air-Decoding. Its main\nidea is reconstructing the attribute distributions to balance the weights\nbetween attribute words and non-attribute words to generate more fluent text.\nSpecifically, we train prefixes by prefix-tuning to obtain attribute\ndistributions. Then we design a novel attribute distribution reconstruction\nmethod to balance the obtained distributions and use the reconstructed\ndistributions to guide language models for generation, effectively avoiding the\nissue of Attribute Collapse. Experiments on multiple CTG tasks prove that our\nmethod achieves a new state-of-the-art control performance.", "published": "2023-10-23 12:59:11", "link": "http://arxiv.org/abs/2310.14892v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling A Core Linguistic Region in Large Language Models", "abstract": "Brain localization, which describes the association between specific regions\nof the brain and their corresponding functions, is widely accepted in the field\nof cognitive science as an objective fact. Today's large language models (LLMs)\npossess human-level linguistic competence and can execute complex tasks\nrequiring abstract knowledge and reasoning. To deeply understand the inherent\nmechanisms of intelligence emergence in LLMs, this paper conducts an analogical\nresearch using brain localization as a prototype. We have discovered a core\nregion in LLMs that corresponds to linguistic competence, accounting for\napproximately 1% of the total model parameters. This core region exhibits\nsignificant dimension dependency, and perturbations to even a single parameter\non specific dimensions can lead to a loss of linguistic competence.\nFurthermore, we observe that an improvement in linguistic competence does not\nnecessarily accompany an elevation in the model's knowledge level, which might\nimply the existence of regions of domain knowledge that are dissociated from\nthe linguistic region. Overall, exploring the LLMs' functional regions provides\ninsights into the foundation of their intelligence. In the future, we will\ncontinue to investigate knowledge regions within LLMs and the interactions\nbetween them.", "published": "2023-10-23 13:31:32", "link": "http://arxiv.org/abs/2310.14928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "System Combination via Quality Estimation for Grammatical Error\n  Correction", "abstract": "Quality estimation models have been developed to assess the corrections made\nby grammatical error correction (GEC) models when the reference or\ngold-standard corrections are not available. An ideal quality estimator can be\nutilized to combine the outputs of multiple GEC systems by choosing the best\nsubset of edits from the union of all edits proposed by the GEC base systems.\nHowever, we found that existing GEC quality estimation models are not good\nenough in differentiating good corrections from bad ones, resulting in a low\nF0.5 score when used for system combination. In this paper, we propose GRECO, a\nnew state-of-the-art quality estimation model that gives a better estimate of\nthe quality of a corrected sentence, as indicated by having a higher\ncorrelation to the F0.5 score of a corrected sentence. It results in a combined\nGEC system with a higher F0.5 score. We also propose three methods for\nutilizing GEC quality estimation models for system combination with varying\ngenerality: model-agnostic, model-agnostic with voting bias, and\nmodel-dependent method. The combined GEC system outperforms the state of the\nart on the CoNLL-2014 test set and the BEA-2019 test set, achieving the highest\nF0.5 scores published to date.", "published": "2023-10-23 13:46:49", "link": "http://arxiv.org/abs/2310.14947v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards LLM-driven Dialogue State Tracking", "abstract": "Dialogue State Tracking (DST) is of paramount importance in ensuring accurate\ntracking of user goals and system actions within task-oriented dialogue\nsystems. The emergence of large language models (LLMs) such as GPT3 and ChatGPT\nhas sparked considerable interest in assessing their efficacy across diverse\napplications. In this study, we conduct an initial examination of ChatGPT's\ncapabilities in DST. Our evaluation uncovers the exceptional performance of\nChatGPT in this task, offering valuable insights to researchers regarding its\ncapabilities and providing useful directions for designing and enhancing\ndialogue systems. Despite its impressive performance, ChatGPT has significant\nlimitations including its closed-source nature, request restrictions, raising\ndata privacy concerns, and lacking local deployment capabilities. To address\nthese concerns, we present LDST, an LLM-driven DST framework based on smaller,\nopen-source foundation models. By utilizing a novel domain-slot instruction\ntuning method, LDST achieves performance on par with ChatGPT. Comprehensive\nevaluations across three distinct experimental settings, we find that LDST\nexhibits remarkable performance improvements in both zero-shot and few-shot\nsetting compared to previous SOTA methods. The source code is provided for\nreproducibility.", "published": "2023-10-23 14:15:28", "link": "http://arxiv.org/abs/2310.14970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Penalty Decoding: Well Suppress the Self-Reinforcement Effect in\n  Open-Ended Text Generation", "abstract": "The decoding algorithm is critical for open-ended text generation,\ntransforming latent representations into coherent and meaningful outputs. This\npaper investigates the self-reinforcement effect in text generation and the\neffectiveness of a repetition penalty to mitigate it. However, determining the\noptimal repetition penalty value is challenging. To tackle this, we propose a\nforgetting mechanism that disregards distant tokens, reducing the burden of\npenalty selection. In addition, we introduce a length penalty to address overly\nshort sentences caused by excessive penalties. Our penalty decoding approach\nincorporating three strategies helps resolve issues with sampling methods\ndeviating from factual information. Experimental results demonstrate the\nefficacy of our approach in generating high-quality sentences resembling human\noutput.", "published": "2023-10-23 14:20:04", "link": "http://arxiv.org/abs/2310.14971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fidelity-Enriched Contrastive Search: Reconciling the\n  Faithfulness-Diversity Trade-Off in Text Generation", "abstract": "In this paper, we address the hallucination problem commonly found in natural\nlanguage generation tasks. Language models often generate fluent and convincing\ncontent but can lack consistency with the provided source, resulting in\npotential inaccuracies. We propose a new decoding method called\nFidelity-Enriched Contrastive Search (FECS), which augments the contrastive\nsearch framework with context-aware regularization terms. FECS promotes tokens\nthat are semantically similar to the provided source while penalizing\nrepetitiveness in the generated text. We demonstrate its effectiveness across\ntwo tasks prone to hallucination: abstractive summarization and dialogue\ngeneration. Results show that FECS consistently enhances faithfulness across\nvarious language model sizes while maintaining output diversity comparable to\nwell-performing decoding algorithms.", "published": "2023-10-23 14:27:45", "link": "http://arxiv.org/abs/2310.14981v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Based Agent Society Investigation: Collaboration and Confrontation\n  in Avalon Gameplay", "abstract": "This paper explores the open research problem of understanding the social\nbehaviors of LLM-based agents. Using Avalon as a testbed, we employ system\nprompts to guide LLM agents in gameplay. While previous studies have touched on\ngameplay with LLM agents, research on their social behaviors is lacking. We\npropose a novel framework, tailored for Avalon, features a multi-agent system\nfacilitating efficient communication and interaction. We evaluate its\nperformance based on game success and analyze LLM agents' social behaviors.\nResults affirm the framework's effectiveness in creating adaptive agents and\nsuggest LLM-based agents' potential in navigating dynamic social interactions.\nBy examining collaboration and confrontation behaviors, we offer insights into\nthis field's research and applications. Our code is publicly available at\nhttps://github.com/3DAgentWorld/LLM-Game-Agent.", "published": "2023-10-23 14:35:26", "link": "http://arxiv.org/abs/2310.14985v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Language Models Fall in Love: Animacy Processing in Transformer\n  Language Models", "abstract": "Animacy - whether an entity is alive and sentient - is fundamental to\ncognitive processing, impacting areas such as memory, vision, and language.\nHowever, animacy is not always expressed directly in language: in English it\noften manifests indirectly, in the form of selectional constraints on verbs and\nadjectives. This poses a potential issue for transformer language models (LMs):\nthey often train only on text, and thus lack access to extralinguistic\ninformation from which humans learn about animacy. We ask: how does this impact\nLMs' animacy processing - do they still behave as humans do? We answer this\nquestion using open-source LMs. Like previous studies, we find that LMs behave\nmuch like humans when presented with entities whose animacy is typical.\nHowever, we also show that even when presented with stories about atypically\nanimate entities, such as a peanut in love, LMs adapt: they treat these\nentities as animate, though they do not adapt as well as humans. Even when the\ncontext indicating atypical animacy is very short, LMs pick up on subtle clues\nand change their behavior. We conclude that despite the limited signal through\nwhich LMs can learn about animacy, they are indeed sensitive to the relevant\nlexical semantic nuances available in English.", "published": "2023-10-23 14:57:52", "link": "http://arxiv.org/abs/2310.15004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical Depth for Ranking and Characterizing Transformer-Based Text\n  Embeddings", "abstract": "The popularity of transformer-based text embeddings calls for better\nstatistical tools for measuring distributions of such embeddings. One such tool\nwould be a method for ranking texts within a corpus by centrality, i.e.\nassigning each text a number signifying how representative that text is of the\ncorpus as a whole. However, an intrinsic center-outward ordering of\nhigh-dimensional text representations is not trivial. A statistical depth is a\nfunction for ranking k-dimensional objects by measuring centrality with respect\nto some observed k-dimensional distribution. We adopt a statistical depth to\nmeasure distributions of transformer-based text embeddings, transformer-based\ntext embedding (TTE) depth, and introduce the practical use of this depth for\nboth modeling and distributional inference in NLP pipelines. We first define\nTTE depth and an associated rank sum test for determining whether two corpora\ndiffer significantly in embedding space. We then use TTE depth for the task of\nin-context learning prompt selection, showing that this approach reliably\nimproves performance over statistical baseline approaches across six text\nclassification tasks. Finally, we use TTE depth and the associated rank sum\ntest to characterize the distributions of synthesized and human-generated\ncorpora, showing that five recent synthetic data augmentation processes cause a\nmeasurable distributional shift away from associated human-generated text.", "published": "2023-10-23 15:02:44", "link": "http://arxiv.org/abs/2310.15010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLOG: A Structural Generalization Benchmark for Semantic Parsing", "abstract": "The goal of compositional generalization benchmarks is to evaluate how well\nmodels generalize to new complex linguistic expressions. Existing benchmarks\noften focus on lexical generalization, the interpretation of novel lexical\nitems in syntactic structures familiar from training; structural generalization\ntasks, where a model needs to interpret syntactic structures that are\nthemselves unfamiliar from training, are often underrepresented, resulting in\noverly optimistic perceptions of how well models can generalize. We introduce\nSLOG, a semantic parsing dataset that extends COGS (Kim and Linzen, 2020) with\n17 structural generalization cases. In our experiments, the generalization\naccuracy of Transformer models, including pretrained ones, only reaches 40.6%,\nwhile a structure-aware parser only achieves 70.8%. These results are far from\nthe near-perfect accuracy existing models achieve on COGS, demonstrating the\nrole of SLOG in foregrounding the large discrepancy between models' lexical and\nstructural generalization capacities.", "published": "2023-10-23 15:39:09", "link": "http://arxiv.org/abs/2310.15040v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TableQAKit: A Comprehensive and Practical Toolkit for Table-based\n  Question Answering", "abstract": "Table-based question answering (TableQA) is an important task in natural\nlanguage processing, which requires comprehending tables and employing various\nreasoning ways to answer the questions. This paper introduces TableQAKit, the\nfirst comprehensive toolkit designed specifically for TableQA. The toolkit\ndesigns a unified platform that includes plentiful TableQA datasets and\nintegrates popular methods of this task as well as large language models\n(LLMs). Users can add their datasets and methods according to the friendly\ninterface. Also, pleasantly surprised using the modules in this toolkit\nachieves new SOTA on some datasets. Finally, \\tableqakit{} also provides an\nLLM-based TableQA Benchmark for evaluating the role of LLMs in TableQA.\nTableQAKit is open-source with an interactive interface that includes visual\noperations, and comprehensive data for ease of use.", "published": "2023-10-23 16:33:23", "link": "http://arxiv.org/abs/2310.15075v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "'Don't Get Too Technical with Me': A Discourse Structure-Based Framework\n  for Science Journalism", "abstract": "Science journalism refers to the task of reporting technical findings of a\nscientific paper as a less technical news article to the general public\naudience. We aim to design an automated system to support this real-world task\n(i.e., automatic science journalism) by 1) introducing a newly-constructed and\nreal-world dataset (SciTechNews), with tuples of a publicly-available\nscientific paper, its corresponding news article, and an expert-written short\nsummary snippet; 2) proposing a novel technical framework that integrates a\npaper's discourse structure with its metadata to guide generation; and, 3)\ndemonstrating with extensive automatic and human experiments that our framework\noutperforms other baseline methods (e.g. Alpaca and ChatGPT) in elaborating a\ncontent plan meaningful for the target audience, simplifying the information\nselected, and producing a coherent final report in a layman's style.", "published": "2023-10-23 16:35:05", "link": "http://arxiv.org/abs/2310.15077v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-in-the-loop: Leveraging Large Language Model for Thematic Analysis", "abstract": "Thematic analysis (TA) has been widely used for analyzing qualitative data in\nmany disciplines and fields. To ensure reliable analysis, the same piece of\ndata is typically assigned to at least two human coders. Moreover, to produce\nmeaningful and useful analysis, human coders develop and deepen their data\ninterpretation and coding over multiple iterations, making TA labor-intensive\nand time-consuming. Recently the emerging field of large language models (LLMs)\nresearch has shown that LLMs have the potential replicate human-like behavior\nin various tasks: in particular, LLMs outperform crowd workers on\ntext-annotation tasks, suggesting an opportunity to leverage LLMs on TA. We\npropose a human-LLM collaboration framework (i.e., LLM-in-the-loop) to conduct\nTA with in-context learning (ICL). This framework provides the prompt to frame\ndiscussions with a LLM (e.g., GPT-3.5) to generate the final codebook for TA.\nWe demonstrate the utility of this framework using survey datasets on the\naspects of the music listening experience and the usage of a password manager.\nResults of the two case studies show that the proposed framework yields similar\ncoding quality to that of human coders but reduces TA's labor and time demands.", "published": "2023-10-23 17:05:59", "link": "http://arxiv.org/abs/2310.15100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GRENADE: Graph-Centric Language Model for Self-Supervised Representation\n  Learning on Text-Attributed Graphs", "abstract": "Self-supervised representation learning on text-attributed graphs, which aims\nto create expressive and generalizable representations for various downstream\ntasks, has received increasing research attention lately. However, existing\nmethods either struggle to capture the full extent of structural context\ninformation or rely on task-specific training labels, which largely hampers\ntheir effectiveness and generalizability in practice. To solve the problem of\nself-supervised representation learning on text-attributed graphs, we develop a\nnovel Graph-Centric Language model -- GRENADE. Specifically, GRENADE exploits\nthe synergistic effect of both pre-trained language model and graph neural\nnetwork by optimizing with two specialized self-supervised learning algorithms:\ngraph-centric contrastive learning and graph-centric knowledge alignment. The\nproposed graph-centric self-supervised learning algorithms effectively help\nGRENADE to capture informative textual semantics as well as structural context\ninformation on text-attributed graphs. Through extensive experiments, GRENADE\nshows its superiority over state-of-the-art methods. Implementation is\navailable at \\url{https://github.com/bigheiniu/GRENADE}.", "published": "2023-10-23 17:18:35", "link": "http://arxiv.org/abs/2310.15109v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into\n  the Morphological Capabilities of a Large Language Model", "abstract": "Large language models (LLMs) have recently reached an impressive level of\nlinguistic capability, prompting comparisons with human language skills.\nHowever, there have been relatively few systematic inquiries into the\nlinguistic capabilities of the latest generation of LLMs, and those studies\nthat do exist (i) ignore the remarkable ability of humans to generalize, (ii)\nfocus only on English, and (iii) investigate syntax or semantics and overlook\nother capabilities that lie at the heart of human language, like morphology.\nHere, we close these gaps by conducting the first rigorous analysis of the\nmorphological capabilities of ChatGPT in four typologically varied languages\n(specifically, English, German, Tamil, and Turkish). We apply a version of\nBerko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for\nthe four examined languages. We find that ChatGPT massively underperforms\npurpose-built systems, particularly in English. Overall, our results -- through\nthe lens of morphology -- cast a new light on the linguistic capabilities of\nChatGPT, suggesting that claims of human-like language skills are premature and\nmisleading.", "published": "2023-10-23 17:21:03", "link": "http://arxiv.org/abs/2310.15113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How To Build Competitive Multi-gender Speech Translation Models For\n  Controlling Speaker Gender Translation", "abstract": "When translating from notional gender languages (e.g., English) into\ngrammatical gender languages (e.g., Italian), the generated translation\nrequires explicit gender assignments for various words, including those\nreferring to the speaker. When the source sentence does not convey the\nspeaker's gender, speech translation (ST) models either rely on the\npossibly-misleading vocal traits of the speaker or default to the masculine\ngender, the most frequent in existing training corpora. To avoid such biased\nand not inclusive behaviors, the gender assignment of speaker-related\nexpressions should be guided by externally-provided metadata about the\nspeaker's gender. While previous work has shown that the most effective\nsolution is represented by separate, dedicated gender-specific models, the goal\nof this paper is to achieve the same results by integrating the speaker's\ngender metadata into a single \"multi-gender\" neural ST model, easier to\nmaintain. Our experiments demonstrate that a single multi-gender model\noutperforms gender-specialized ones when trained from scratch (with gender\naccuracy gains up to 12.9 for feminine forms), while fine-tuning from existing\nST models does not lead to competitive results.", "published": "2023-10-23 17:21:32", "link": "http://arxiv.org/abs/2310.15114v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large\n  Language Models", "abstract": "The rapid development of Large Language Models (LLMs) has led to great\nstrides in model capabilities like long-context understanding and reasoning.\nHowever, as LLMs are able to process longer contexts, it becomes more\nchallenging to evaluate whether they have acquired certain capabilities, since\nthe length of text (e.g., 200K tokens) they can process far exceeds what humans\ncan reliably assess in a reasonable duration. In this paper, we propose using\ncomplex synthetic tasks as a proxy evaluation method, and present S3Eval, a\nSynthetic, Scalable, Systematic evaluation suite for LLMs evaluation. The\nsynthetic nature of S3Eval provides users full control over the dataset,\nallowing them to systematically probe LLM capabilities by scaling text length\nand varying task difficulty across diverse scenarios. The strong correlation\nbetween S3Eval and real-world benchmarks demonstrates the soundness of using\nS3Eval for evaluation of LLMs. S3Eval provides a flexible and infinite\nlong-context data generation method. We have generated a comprehensive dataset\ncalled S3Eval-Standard, and experimental results have shown that it poses\nsignificant challenges for all existing LLMs.", "published": "2023-10-23 17:52:06", "link": "http://arxiv.org/abs/2310.15147v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple\n  Experts Fine-tuning", "abstract": "We propose Multiple Experts Fine-tuning Framework to build a financial large\nlanguage model (LLM), DISC-FinLLM. Our methodology improves general LLMs by\nendowing them with multi-turn question answering abilities, domain text\nprocessing capabilities, mathematical computation skills, and\nretrieval-enhanced generation capabilities. We build a financial\ninstruction-tuning dataset named DISC-FIN-SFT, including instruction samples of\nfour categories (consulting, NLP tasks, computing and retrieval-augmented\ngeneration). Evaluations conducted on multiple benchmarks demonstrate that our\nmodel performs better than baseline models in various financial scenarios.\nFurther resources can be found at https://github.com/FudanDISC/DISC-FinLLM.", "published": "2023-10-23 11:33:41", "link": "http://arxiv.org/abs/2310.15205v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking the Language Barrier: Improving Cross-Lingual Reasoning with\n  Structured Self-Attention", "abstract": "In this work, we study whether multilingual language models (MultiLMs) can\ntransfer logical reasoning abilities to other languages when they are\nfine-tuned for reasoning in a different language. We evaluate the cross-lingual\nreasoning abilities of MultiLMs in two schemes: (1) where the language of the\ncontext and the question remain the same in the new languages that are tested\n(i.e., the reasoning is still monolingual, but the model must transfer the\nlearned reasoning ability across languages), and (2) where the language of the\ncontext and the question is different (which we term code-switched reasoning).\nOn two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate\nthat although MultiLMs can transfer reasoning ability across languages in a\nmonolingual setting, they struggle to transfer reasoning abilities in a\ncode-switched setting. Following this observation, we propose a novel attention\nmechanism that uses a dedicated set of parameters to encourage cross-lingual\nattention in code-switched sequences, which improves the reasoning performance\nby up to 14% and 4% on the RuleTaker and LeapOfThought datasets, respectively.", "published": "2023-10-23 18:06:38", "link": "http://arxiv.org/abs/2310.15258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation Techniques for Machine Translation of Code-Switched\n  Texts: A Comparative Study", "abstract": "Code-switching (CSW) text generation has been receiving increasing attention\nas a solution to address data scarcity. In light of this growing interest, we\nneed more comprehensive studies comparing different augmentation approaches. In\nthis work, we compare three popular approaches: lexical replacements,\nlinguistic theories, and back-translation (BT), in the context of Egyptian\nArabic-English CSW. We assess the effectiveness of the approaches on machine\ntranslation and the quality of augmentations through human evaluation. We show\nthat BT and CSW predictive-based lexical replacement, being trained on CSW\nparallel data, perform best on both tasks. Linguistic theories and random\nlexical replacement prove to be effective in the lack of CSW parallel data,\nwhere both approaches achieve similar results.", "published": "2023-10-23 18:09:41", "link": "http://arxiv.org/abs/2310.15262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Dimensionality of Sentence Embeddings", "abstract": "Learning sentence embeddings is a fundamental problem in natural language\nprocessing. While existing research primarily focuses on enhancing the quality\nof sentence embeddings, the exploration of sentence embedding dimensions is\nlimited. Here we present a comprehensive and empirical analysis of the\ndimensionality of sentence embeddings. First, we demonstrate that the optimal\ndimension of sentence embeddings is usually smaller than the default value.\nSubsequently, to compress the dimension of sentence embeddings with minimum\nperformance degradation, we identify two components contributing to the overall\nperformance loss: the encoder's performance loss and the pooler's performance\nloss. Therefore, we propose a two-step training method for sentence\nrepresentation learning models, wherein the encoder and the pooler are\noptimized separately to mitigate the overall performance loss in low-dimension\nscenarios. Experimental results on seven STS tasks and seven sentence\nclassification tasks demonstrate that our method significantly improves the\nperformance of low-dimensional sentence embeddings.", "published": "2023-10-23 18:51:00", "link": "http://arxiv.org/abs/2310.15285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot\n  Filling", "abstract": "Recently slot filling has witnessed great development thanks to deep learning\nand the availability of large-scale annotated data. However, it poses a\ncritical challenge to handle a novel domain whose samples are never seen during\ntraining. The recognition performance might be greatly degraded due to severe\ndomain shifts. Most prior works deal with this problem in a two-pass pipeline\nmanner based on metric learning. In practice, these dominant pipeline models\nmay be limited in computational efficiency and generalization capacity because\nof non-parallel inference and context-free discrete label embeddings. To this\nend, we re-examine the typical metric-based methods, and propose a new adaptive\nend-to-end metric learning scheme for the challenging zero-shot slot filling.\nConsidering simplicity, efficiency and generalizability, we present a\ncascade-style joint learning framework coupled with context-aware soft label\nrepresentations and slot-level contrastive representation learning to mitigate\nthe data and label shift problems effectively. Extensive experiments on public\nbenchmarks demonstrate the superiority of the proposed approach over a series\nof competitive baselines.", "published": "2023-10-23 19:01:16", "link": "http://arxiv.org/abs/2310.15294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Probing Representations for Document-level Event Extraction", "abstract": "The probing classifiers framework has been employed for interpreting deep\nneural network models for a variety of natural language processing (NLP)\napplications. Studies, however, have largely focused on sentencelevel NLP\ntasks. This work is the first to apply the probing paradigm to representations\nlearned for document-level information extraction (IE). We designed eight\nembedding probes to analyze surface, semantic, and event-understanding\ncapabilities relevant to document-level event extraction. We apply them to the\nrepresentations acquired by learning models from three different LLM-based\ndocument-level IE approaches on a standard dataset. We found that trained\nencoders from these models yield embeddings that can modestly improve argument\ndetections and labeling but only slightly enhance event-level tasks, albeit\ntrade-offs in information helpful for coherence and event-type prediction. We\nfurther found that encoder models struggle with document length and\ncross-sentence discourse.", "published": "2023-10-23 19:33:04", "link": "http://arxiv.org/abs/2310.15316v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks", "abstract": "The potential of large language models (LLMs) to simultaneously perform a\nwide range of natural language processing (NLP) tasks has been the subject of\nextensive research. Although instruction tuning has proven to be a\ndata-efficient method for transforming LLMs into such generalist models, their\nperformance still lags behind specialist models trained exclusively for\nspecific tasks. In this paper, we investigate whether incorporating\nbroad-coverage generalist instruction tuning can contribute to building a\nspecialist model. We hypothesize that its efficacy depends on task specificity\nand skill requirements. Our experiments assess four target tasks with distinct\ncoverage levels, revealing that integrating generalist instruction tuning\nconsistently enhances model performance when the task coverage is broad. The\neffect is particularly pronounced when the amount of task-specific training\ndata is limited. Further investigation into three target tasks focusing on\ndifferent capabilities demonstrates that generalist instruction tuning improves\nunderstanding and reasoning abilities. However, for tasks requiring factual\nknowledge, generalist data containing hallucinatory information may negatively\naffect the model's performance. Overall, our work provides a systematic guide\nfor developing specialist models with general instruction tuning. Our code and\nother related resources can be found at\nhttps://github.com/DavidFanzz/Generalist_or_Specialist.", "published": "2023-10-23 19:46:48", "link": "http://arxiv.org/abs/2310.15326v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GD-COMET: A Geo-Diverse Commonsense Inference Model", "abstract": "With the increasing integration of AI into everyday life, it's becoming\ncrucial to design AI systems that serve users from diverse backgrounds by\nmaking them culturally aware. In this paper, we present GD-COMET, a geo-diverse\nversion of the COMET commonsense inference model. GD-COMET goes beyond Western\ncommonsense knowledge and is capable of generating inferences pertaining to a\nbroad range of cultures. We demonstrate the effectiveness of GD-COMET through a\ncomprehensive human evaluation across 5 diverse cultures, as well as extrinsic\nevaluation on a geo-diverse task. The evaluation shows that GD-COMET captures\nand generates culturally nuanced commonsense knowledge, demonstrating its\npotential to benefit NLP applications across the board and contribute to making\nNLP more inclusive.", "published": "2023-10-23 22:03:56", "link": "http://arxiv.org/abs/2310.15383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions", "abstract": "There is growing interest in systems that generate captions for scientific\nfigures. However, assessing these systems output poses a significant challenge.\nHuman evaluation requires academic expertise and is costly, while automatic\nevaluation depends on often low-quality author-written captions. This paper\ninvestigates using large language models (LLMs) as a cost-effective,\nreference-free method for evaluating figure captions. We first constructed\nSCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600\nscientific figure captions, both original and machine-made, for 600 arXiv\nfigures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption\nbased on its potential to aid reader understanding, given relevant context such\nas figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot\nevaluator, outperformed all other models and even surpassed assessments made by\nComputer Science and Informatics undergraduates, achieving a Kendall\ncorrelation score of 0.401 with Ph.D. students rankings", "published": "2023-10-23 23:24:57", "link": "http://arxiv.org/abs/2310.15405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question\n  Answering via Pluggable Reward-Driven Contextual Adapter", "abstract": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented\nframework, composed of a retriever and generator. The generator formulates the\nanswer based on the documents retrieved by the retriever. Incorporating Large\nLanguage Models (LLMs) as generators is beneficial due to their advanced QA\ncapabilities, but they are typically too large to be fine-tuned with budget\nconstraints while some of them are only accessible via APIs. To tackle this\nissue and further improve ReQA performance, we propose a trainable Pluggable\nReward-Driven Contextual Adapter (PRCA), keeping the generator as a black box.\nPositioned between the retriever and generator in a Pluggable manner, PRCA\nrefines the retrieved information by operating in a token-autoregressive\nstrategy via maximizing rewards of the reinforcement learning phase. Our\nexperiments validate PRCA's effectiveness in enhancing ReQA performance on\nthree datasets by up to 20% improvement to fit black-box LLMs into existing\nframeworks, demonstrating its considerable potential in the LLMs era.", "published": "2023-10-23 03:12:00", "link": "http://arxiv.org/abs/2310.18347v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Boundary Offset Prediction Network for Named Entity Recognition", "abstract": "Named entity recognition (NER) is a fundamental task in natural language\nprocessing that aims to identify and classify named entities in text. However,\nspan-based methods for NER typically assign entity types to text spans,\nresulting in an imbalanced sample space and neglecting the connections between\nnon-entity and entity spans. To address these issues, we propose a novel\napproach for NER, named the Boundary Offset Prediction Network (BOPN), which\npredicts the boundary offsets between candidate spans and their nearest entity\nspans. By leveraging the guiding semantics of boundary offsets, BOPN\nestablishes connections between non-entity and entity spans, enabling\nnon-entity spans to function as additional positive samples for entity\ndetection. Furthermore, our method integrates entity type and span\nrepresentations to generate type-aware boundary offsets instead of using entity\ntypes as detection targets. We conduct experiments on eight widely-used NER\ndatasets, and the results demonstrate that our proposed BOPN outperforms\nprevious state-of-the-art methods.", "published": "2023-10-23 05:04:07", "link": "http://arxiv.org/abs/2310.18349v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Review of Reinforcement Learning for Natural Language Processing, and\n  Applications in Healthcare", "abstract": "Reinforcement learning (RL) has emerged as a powerful approach for tackling\ncomplex medical decision-making problems such as treatment planning,\npersonalized medicine, and optimizing the scheduling of surgeries and\nappointments. It has gained significant attention in the field of Natural\nLanguage Processing (NLP) due to its ability to learn optimal strategies for\ntasks such as dialogue systems, machine translation, and question-answering.\nThis paper presents a review of the RL techniques in NLP, highlighting key\nadvancements, challenges, and applications in healthcare. The review begins by\nvisualizing a roadmap of machine learning and its applications in healthcare.\nAnd then it explores the integration of RL with NLP tasks. We examined dialogue\nsystems where RL enables the learning of conversational strategies, RL-based\nmachine translation models, question-answering systems, text summarization, and\ninformation extraction. Additionally, ethical considerations and biases in\nRL-NLP systems are addressed.", "published": "2023-10-23 20:26:15", "link": "http://arxiv.org/abs/2310.18354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InstructExcel: A Benchmark for Natural Language Instruction in Excel", "abstract": "With the evolution of Large Language Models (LLMs) we can solve increasingly\nmore complex NLP tasks across various domains, including spreadsheets. This\nwork investigates whether LLMs can generate code (Excel OfficeScripts, a\nTypeScript API for executing many tasks in Excel) that solves Excel specific\ntasks provided via natural language user instructions. To do so we introduce a\nnew large-scale benchmark, InstructExcel, created by leveraging the 'Automate'\nfeature in Excel to automatically generate OfficeScripts from users' actions.\nOur benchmark includes over 10k samples covering 170+ Excel operations across\n2,000 publicly available Excel spreadsheets. Experiments across various\nzero-shot and few-shot settings show that InstructExcel is a hard benchmark for\nstate of the art models like GPT-4. We observe that (1) using GPT-4 over\nGPT-3.5, (2) providing more in-context examples, and (3) dynamic prompting can\nhelp improve performance on this benchmark.", "published": "2023-10-23 02:00:55", "link": "http://arxiv.org/abs/2310.14495v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Spatial Understanding of Large Language Models", "abstract": "Large language models (LLMs) show remarkable capabilities across a variety of\ntasks. Despite the models only seeing text in training, several recent studies\nsuggest that LLM representations implicitly capture aspects of the underlying\ngrounded concepts. Here, we explore LLM representations of a particularly\nsalient kind of grounded knowledge -- spatial relationships. We design\nnatural-language navigation tasks and evaluate the ability of LLMs, in\nparticular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and\nreason about spatial structures. These tasks reveal substantial variability in\nLLM performance across different spatial structures, including square,\nhexagonal, and triangular grids, rings, and trees. In extensive error analysis,\nwe find that LLMs' mistakes reflect both spatial and non-spatial factors. These\nfindings suggest that LLMs appear to capture certain aspects of spatial\nstructure implicitly, but room for improvement remains.", "published": "2023-10-23 03:44:40", "link": "http://arxiv.org/abs/2310.14540v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AlpaCare:Instruction-tuned Large Language Models for Medical Application", "abstract": "Instruction-finetuning (IFT) has become crucial in aligning Large Language\nModels (LLMs) with diverse human needs and has shown great potential in medical\napplications. However, previous studies mainly fine-tune LLMs on biomedical\ndatasets with limited diversity, which often rely on benchmarks or narrow task\nscopes, and hence significantly limit the effectiveness on their medical\ninstruction-following ability and generalizability. To bridge this gap, we\npropose creating a diverse, machine-generated medical IFT dataset,\nMedInstruct-52k, using GPT-4 and ChatGPT with a high-quality expert-curated\nseed set. We then fine-tune LLaMA-series models on the dataset to develop\nAlpaCare. Despite using a smaller domain-specific dataset than previous medical\nLLMs, AlpaCare not only demonstrates superior performance on medical\napplications, with up to 38.1% absolute gain over best baselines in medical\nfree-form instruction evaluations, but also achieves 6.7% absolute gains\naveraged over multiple general domain benchmarks. Human evaluation further\nshows that AlpaCare consistently outperforms best baselines in terms of both\ncorrectness and helpfulness. We offer public access to our data, model, and\ncodebase in https://github.com/XZhang97666/AlpaCare.", "published": "2023-10-23 04:22:50", "link": "http://arxiv.org/abs/2310.14558v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling\n  Social Norm Adherence and Violation", "abstract": "Social norms fundamentally shape interpersonal communication. We present\nNormDial, a high-quality dyadic dialogue dataset with turn-by-turn annotations\nof social norm adherences and violations for Chinese and American cultures.\nIntroducing the task of social norm observance detection, our dataset is\nsynthetically generated in both Chinese and English using a human-in-the-loop\npipeline by prompting large language models with a small collection of\nexpert-annotated social norms. We show that our generated dialogues are of high\nquality through human evaluation and further evaluate the performance of\nexisting large language models on this task. Our findings point towards new\ndirections for understanding the nuances of social norms as they manifest in\nconversational contexts that span across languages and cultures.", "published": "2023-10-23 04:38:34", "link": "http://arxiv.org/abs/2310.14563v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "HallusionBench: An Advanced Diagnostic Suite for Entangled Language\n  Hallucination and Visual Illusion in Large Vision-Language Models", "abstract": "We introduce HallusionBench, a comprehensive benchmark designed for the\nevaluation of image-context reasoning. This benchmark presents significant\nchallenges to advanced large visual-language models (LVLMs), such as\nGPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing\nnuanced understanding and interpretation of visual data. The benchmark\ncomprises 346 images paired with 1129 questions, all meticulously crafted by\nhuman experts. We introduce a novel structure for these visual questions\ndesigned to establish control groups. This structure enables us to conduct a\nquantitative analysis of the models' response tendencies, logical consistency,\nand various failure modes. In our evaluation on HallusionBench, we benchmarked\n15 different models, highlighting a 31.42% question-pair accuracy achieved by\nthe state-of-the-art GPT-4V. Notably, all other evaluated models achieve\naccuracy below 16%. Moreover, our analysis not only highlights the observed\nfailure modes, including language hallucination and visual illusion, but also\ndeepens an understanding of these pitfalls. Our comprehensive case studies\nwithin HallusionBench shed light on the challenges of hallucination and\nillusion in LVLMs. Based on these insights, we suggest potential pathways for\ntheir future improvement. The benchmark and codebase can be accessed at\nhttps://github.com/tianyi-lab/HallusionBench.", "published": "2023-10-23 04:49:09", "link": "http://arxiv.org/abs/2310.14566v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unveiling the Multi-Annotation Process: Examining the Influence of\n  Annotation Quantity and Instance Difficulty on Model Performance", "abstract": "The NLP community has long advocated for the construction of multi-annotator\ndatasets to better capture the nuances of language interpretation,\nsubjectivity, and ambiguity. This paper conducts a retrospective study to show\nhow performance scores can vary when a dataset expands from a single annotation\nper instance to multiple annotations. We propose a novel multi-annotator\nsimulation process to generate datasets with varying annotation budgets. We\nshow that similar datasets with the same annotation budget can lead to varying\nperformance gains. Our findings challenge the popular belief that models\ntrained on multi-annotation examples always lead to better performance than\nmodels trained on single or few-annotation examples.", "published": "2023-10-23 05:12:41", "link": "http://arxiv.org/abs/2310.14572v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeCrisisMB: Debiased Semi-Supervised Learning for Crisis Tweet\n  Classification via Memory Bank", "abstract": "During crisis events, people often use social media platforms such as Twitter\nto disseminate information about the situation, warnings, advice, and support.\nEmergency relief organizations leverage such information to acquire timely\ncrisis circumstances and expedite rescue operations. While existing works\nutilize such information to build models for crisis event analysis,\nfully-supervised approaches require annotating vast amounts of data and are\nimpractical due to limited response time. On the other hand, semi-supervised\nmodels can be biased, performing moderately well for certain classes while\nperforming extremely poorly for others, resulting in substantially negative\neffects on disaster monitoring and rescue. In this paper, we first study two\nrecent debiasing methods on semi-supervised crisis tweet classification. Then\nwe propose a simple but effective debiasing method, DeCrisisMB, that utilizes a\nMemory Bank to store and perform equal sampling for generated pseudo-labels\nfrom each class at each training iteration. Extensive experiments are conducted\nto compare different debiasing methods' performance and generalization ability\nin both in-distribution and out-of-distribution settings. The results\ndemonstrate the superior performance of our proposed method. Our code is\navailable at https://github.com/HenryPengZou/DeCrisisMB.", "published": "2023-10-23 05:25:51", "link": "http://arxiv.org/abs/2310.14577v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JointMatch: A Unified Approach for Diverse and Collaborative\n  Pseudo-Labeling to Semi-Supervised Text Classification", "abstract": "Semi-supervised text classification (SSTC) has gained increasing attention\ndue to its ability to leverage unlabeled data. However, existing approaches\nbased on pseudo-labeling suffer from the issues of pseudo-label bias and error\naccumulation. In this paper, we propose JointMatch, a holistic approach for\nSSTC that addresses these challenges by unifying ideas from recent\nsemi-supervised learning and the task of learning with noise. JointMatch\nadaptively adjusts classwise thresholds based on the learning status of\ndifferent classes to mitigate model bias towards current easy classes.\nAdditionally, JointMatch alleviates error accumulation by utilizing two\ndifferently initialized networks to teach each other in a cross-labeling\nmanner. To maintain divergence between the two networks for mutual learning, we\nintroduce a strategy that weighs more disagreement data while also allowing the\nutilization of high-quality agreement data for training. Experimental results\non benchmark datasets demonstrate the superior performance of JointMatch,\nachieving a significant 5.13% improvement on average. Notably, JointMatch\ndelivers impressive results even in the extremely-scarce-label setting,\nobtaining 86% accuracy on AG News with only 5 labels per class. We make our\ncode available at https://github.com/HenryPengZou/JointMatch.", "published": "2023-10-23 05:43:35", "link": "http://arxiv.org/abs/2310.14583v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Search Model: Redefining Search Stack in the Era of LLMs", "abstract": "Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.", "published": "2023-10-23 05:52:09", "link": "http://arxiv.org/abs/2310.14587v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Learning to Correct Noisy Labels for Fine-Grained Entity Typing via\n  Co-Prediction Prompt Tuning", "abstract": "Fine-grained entity typing (FET) is an essential task in natural language\nprocessing that aims to assign semantic types to entities in text. However, FET\nposes a major challenge known as the noise labeling problem, whereby current\nmethods rely on estimating noise distribution to identify noisy labels but are\nconfused by diverse noise distribution deviation. To address this limitation,\nwe introduce Co-Prediction Prompt Tuning for noise correction in FET, which\nleverages multiple prediction results to identify and correct noisy labels.\nSpecifically, we integrate prediction results to recall labeled labels and\nutilize a differentiated margin to identify inaccurate labels. Moreover, we\ndesign an optimization objective concerning divergent co-predictions during\nfine-tuning, ensuring that the model captures sufficient information and\nmaintains robustness in noise identification. Experimental results on three\nwidely-used FET datasets demonstrate that our noise correction approach\nsignificantly enhances the quality of various types of training samples,\nincluding those annotated using distant supervision, ChatGPT, and\ncrowdsourcing.", "published": "2023-10-23 06:04:07", "link": "http://arxiv.org/abs/2310.14596v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prefix-Tuning Based Unsupervised Text Style Transfer", "abstract": "Unsupervised text style transfer aims at training a generative model that can\nalter the style of the input sentence while preserving its content without\nusing any parallel data. In this paper, we employ powerful pre-trained large\nlanguage models and present a new prefix-tuning-based method for unsupervised\ntext style transfer. We construct three different kinds of prefixes, i.e.,\n\\textit{shared prefix, style prefix}, and \\textit{content prefix}, to encode\ntask-specific information, target style, and the content information of the\ninput sentence, respectively. Compared to embeddings used by previous works,\nthe proposed prefixes can provide richer information for the model.\nFurthermore, we adopt a recursive way of using language models in the process\nof style transfer. This strategy provides a more effective way for the\ninteractions between the input sentence and GPT-2, helps the model construct\nmore informative prefixes, and thus, helps improve the performance. Evaluations\non the well-known datasets show that our method outperforms the\nstate-of-the-art baselines. Results, analysis of ablation studies, and\nsubjective evaluations from humans are also provided for a deeper understanding\nof the proposed method.", "published": "2023-10-23 06:13:08", "link": "http://arxiv.org/abs/2310.14599v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal\n  Aspect-based Sentiment Analysis", "abstract": "Multimodal Aspect-based Sentiment Analysis (MABSA) is a fine-grained\nSentiment Analysis task, which has attracted growing research interests\nrecently. Existing work mainly utilizes image information to improve the\nperformance of MABSA task. However, most of the studies overestimate the\nimportance of images since there are many noise images unrelated to the text in\nthe dataset, which will have a negative impact on model learning. Although some\nwork attempts to filter low-quality noise images by setting thresholds, relying\non thresholds will inevitably filter out a lot of useful image information.\nTherefore, in this work, we focus on whether the negative impact of noisy\nimages can be reduced without modifying the data. To achieve this goal, we\nborrow the idea of Curriculum Learning and propose a Multi-grained\nMulti-curriculum Denoising Framework (M2DF), which can achieve denoising by\nadjusting the order of training data. Extensive experimental results show that\nour framework consistently outperforms state-of-the-art work on three sub-tasks\nof MABSA.", "published": "2023-10-23 06:22:39", "link": "http://arxiv.org/abs/2310.14605v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Confronting LLMs with Traditional ML: Rethinking the Fairness of Large\n  Language Models in Tabular Classifications", "abstract": "Recent literature has suggested the potential of using large language models\n(LLMs) to make classifications for tabular tasks. However, LLMs have been shown\nto exhibit harmful social biases that reflect the stereotypes and inequalities\npresent in society. To this end, as well as the widespread use of tabular data\nin many high-stake applications, it is important to explore the following\nquestions: what sources of information do LLMs draw upon when making\nclassifications for tabular tasks; whether and to what extent are LLM\nclassifications for tabular data influenced by social biases and stereotypes;\nand what are the consequential implications for fairness?\n  Through a series of experiments, we delve into these questions and show that\nLLMs tend to inherit social biases from their training data which significantly\nimpact their fairness in tabular classification tasks. Furthermore, our\ninvestigations show that in the context of bias mitigation, though in-context\nlearning and finetuning have a moderate effect, the fairness metric gap between\ndifferent subgroups is still larger than that in traditional machine learning\nmodels, such as Random Forest and shallow Neural Networks. This observation\nemphasizes that the social biases are inherent within the LLMs themselves and\ninherited from their pretraining corpus, not only from the downstream task\ndatasets. Besides, we demonstrate that label-flipping of in-context examples\ncan significantly reduce biases, further highlighting the presence of inherent\nbias within LLMs.", "published": "2023-10-23 06:31:28", "link": "http://arxiv.org/abs/2310.14607v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine\n  Chain-of-Thought Prompting for Multi-domain NLU Tasks", "abstract": "While Chain-of-Thought prompting is popular in reasoning tasks, its\napplication to Large Language Models (LLMs) in Natural Language Understanding\n(NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose\nCoarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks\ninto multiple reasoning steps where LLMs can learn to acquire and leverage\nessential concepts to solve tasks from different granularities. Moreover, we\npropose leveraging semantic-based Abstract Meaning Representation (AMR)\nstructured knowledge as an intermediate step to capture the nuances and diverse\nstructures of utterances, and to understand connections between their varying\nlevels of granularity. Our proposed approach is demonstrated effective in\nassisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot\nand few-shot multi-domain settings.", "published": "2023-10-23 06:54:51", "link": "http://arxiv.org/abs/2310.14623v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue", "abstract": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.", "published": "2023-10-23 07:00:51", "link": "http://arxiv.org/abs/2310.14626v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CrisisMatch: Semi-Supervised Few-Shot Learning for Fine-Grained Disaster\n  Tweet Classification", "abstract": "The shared real-time information about natural disasters on social media\nplatforms like Twitter and Facebook plays a critical role in informing\nvolunteers, emergency managers, and response organizations. However, supervised\nlearning models for monitoring disaster events require large amounts of\nannotated data, making them unrealistic for real-time use in disaster events.\nTo address this challenge, we present a fine-grained disaster tweet\nclassification model under the semi-supervised, few-shot learning setting where\nonly a small number of annotated data is required. Our model, CrisisMatch,\neffectively classifies tweets into fine-grained classes of interest using few\nlabeled data and large amounts of unlabeled data, mimicking the early stage of\na disaster. Through integrating effective semi-supervised learning ideas and\nincorporating TextMixUp, CrisisMatch achieves performance improvement on two\ndisaster datasets of 11.2\\% on average. Further analyses are also provided for\nthe influence of the number of labeled data and out-of-domain results.", "published": "2023-10-23 07:01:09", "link": "http://arxiv.org/abs/2310.14627v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extending Input Contexts of Language Models through Training on\n  Segmented Sequences", "abstract": "Effectively training language models on long inputs poses many technical\nchallenges. As a cost consideration, languages models are pretrained on a fixed\nsequence length before being adapted to longer sequences. We explore various\nmethods for adapting models to longer inputs by training on segmented sequences\nand an interpolation-based method for extending absolute positional embeddings.\nWe develop a training procedure to extend the input context size of pretrained\nmodels with no architectural changes and no additional memory costs than\ntraining on the original input lengths. By sub-sampling segments from long\ninputs while maintaining their original position the model is able to learn new\npositional interactions. Our method benefits both models trained with absolute\npositional embeddings, by extending their input contexts, as well as popular\nrelative positional embedding methods showing a reduced perplexity on sequences\nlonger than they were trained on. We demonstrate our method can extend input\ncontexts by a factor of 4x while improving perplexity.", "published": "2023-10-23 07:13:31", "link": "http://arxiv.org/abs/2310.14633v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab,\n  IIT Madras", "abstract": "India is home to a multitude of languages of which 22 languages are\nrecognised by the Indian Constitution as official. Building speech based\napplications for the Indian population is a difficult problem owing to limited\ndata and the number of languages and accents to accommodate. To encourage the\nlanguage technology community to build speech based applications in Indian\nlanguages, we are open sourcing SPRING-INX data which has about 2000 hours of\nlegally sourced and manually transcribed speech data for ASR system building in\nAssamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi\nand Tamil. This endeavor is by SPRING Lab , Indian Institute of Technology\nMadras and is a part of National Language Translation Mission (NLTM), funded by\nthe Indian Ministry of Electronics and Information Technology (MeitY),\nGovernment of India. We describe the data collection and data cleaning process\nalong with the data statistics in this paper.", "published": "2023-10-23 07:50:10", "link": "http://arxiv.org/abs/2310.14654v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reasoning about Ambiguous Definite Descriptions", "abstract": "Natural language reasoning plays an increasingly important role in improving\nlanguage models' ability to solve complex language understanding tasks. An\ninteresting use case for reasoning is the resolution of context-dependent\nambiguity. But no resources exist to evaluate how well Large Language Models\ncan use explicit reasoning to resolve ambiguity in language. We propose to use\nambiguous definite descriptions for this purpose and create and publish the\nfirst benchmark dataset consisting of such phrases. Our method includes all\ninformation required to resolve the ambiguity in the prompt, which means a\nmodel does not require anything but reasoning to do well. We find this to be a\nchallenging task for recent LLMs. Code and data available at:\nhttps://github.com/sfschouten/exploiting-ambiguity", "published": "2023-10-23 07:52:38", "link": "http://arxiv.org/abs/2310.14657v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DPP-TTS: Diversifying prosodic features of speech via determinantal\n  point processes", "abstract": "With the rapid advancement in deep generative models, recent neural\nText-To-Speech(TTS) models have succeeded in synthesizing human-like speech.\nThere have been some efforts to generate speech with various prosody beyond\nmonotonous prosody patterns. However, previous works have several limitations.\nFirst, typical TTS models depend on the scaled sampling temperature for\nboosting the diversity of prosody. Speech samples generated at high sampling\ntemperatures often lack perceptual prosodic diversity, which can adversely\naffect the naturalness of the speech. Second, the diversity among samples is\nneglected since the sampling procedure often focuses on a single speech sample\nrather than multiple ones. In this paper, we propose DPP-TTS: a text-to-speech\nmodel based on Determinantal Point Processes (DPPs) with a prosody diversifying\nmodule. Our TTS model is capable of generating speech samples that\nsimultaneously consider perceptual diversity in each sample and among multiple\nsamples. We demonstrate that DPP-TTS generates speech samples with more\ndiversified prosody than baselines in the side-by-side comparison test\nconsidering the naturalness of speech at the same time.", "published": "2023-10-23 07:59:46", "link": "http://arxiv.org/abs/2310.14663v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "API-Assisted Code Generation for Question Answering on Varied Table\n  Structures", "abstract": "A persistent challenge to table question answering (TableQA) by generating\nexecutable programs has been adapting to varied table structures, typically\nrequiring domain-specific logical forms. In response, this paper introduces a\nunified TableQA framework that: (1) provides a unified representation for\nstructured tables as multi-index Pandas data frames, (2) uses Python as a\npowerful querying language, and (3) uses few-shot prompting to translate NL\nquestions into Python programs, which are executable on Pandas data frames.\nFurthermore, to answer complex relational questions with extended program\nfunctionality and external knowledge, our framework allows customized APIs that\nPython programs can call. We experiment with four TableQA datasets that involve\ntables of different structures -- relational, multi-table, and hierarchical\nmatrix shapes -- and achieve prominent improvements over past state-of-the-art\nsystems. In ablation studies, we (1) show benefits from our multi-index\nrepresentation and APIs over baselines that use only an LLM, and (2)\ndemonstrate that our approach is modular and can incorporate additional APIs.", "published": "2023-10-23 08:26:28", "link": "http://arxiv.org/abs/2310.14687v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future\n  Directions", "abstract": "The powerful ability to understand, follow, and generate complex language\nemerging from large language models (LLMs) makes LLM-generated text flood many\nareas of our daily lives at an incredible speed and is widely accepted by\nhumans. As LLMs continue to expand, there is an imperative need to develop\ndetectors that can detect LLM-generated text. This is crucial to mitigate\npotential misuse of LLMs and safeguard realms like artistic expression and\nsocial networks from harmful influence of LLM-generated content. The\nLLM-generated text detection aims to discern if a piece of text was produced by\nan LLM, which is essentially a binary classification task. The detector\ntechniques have witnessed notable advancements recently, propelled by\ninnovations in watermarking techniques, statistics-based detectors, neural-base\ndetectors, and human-assisted methods. In this survey, we collate recent\nresearch breakthroughs in this area and underscore the pressing need to bolster\ndetector research. We also delve into prevalent datasets, elucidating their\nlimitations and developmental requirements. Furthermore, we analyze various\nLLM-generated text detection paradigms, shedding light on challenges like\nout-of-distribution problems, potential attacks, real-world data issues and the\nlack of effective evaluation framework. Conclusively, we highlight interesting\ndirections for future research in LLM-generated text detection to advance the\nimplementation of responsible artificial intelligence (AI). Our aim with this\nsurvey is to provide a clear and comprehensive introduction for newcomers while\nalso offering seasoned researchers a valuable update in the field of\nLLM-generated text detection. The useful resources are publicly available at:\nhttps://github.com/NLP2CT/LLM-generated-Text-Detection.", "published": "2023-10-23 09:01:13", "link": "http://arxiv.org/abs/2310.14724v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generating Prototypes for Contradiction Detection Using Large Language\n  Models and Linguistic Rules", "abstract": "We introduce a novel data generation method for contradiction detection,\nwhich leverages the generative power of large language models as well as\nlinguistic rules. Our vision is to provide a condensed corpus of prototypical\ncontradictions, allowing for in-depth linguistic analysis as well as efficient\nlanguage model fine-tuning. To this end, we instruct the generative models to\ncreate contradicting statements with respect to descriptions of specific\ncontradiction types. In addition, the model is also instructed to come up with\ncompletely new contradiction typologies. As an auxiliary approach, we use\nlinguistic rules to construct simple contradictions such as those arising from\nnegation, antonymy and numeric mismatch. We find that our methods yield\npromising results in terms of coherence and variety of the data. Further\nstudies, as well as manual refinement are necessary to make use of this data in\na machine learning setup.", "published": "2023-10-23 09:07:27", "link": "http://arxiv.org/abs/2310.14732v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unleashing the potential of prompt engineering in Large Language Models:\n  a comprehensive review", "abstract": "This comprehensive review delves into the pivotal role of prompt engineering\nin unleashing the capabilities of Large Language Models (LLMs). The development\nof Artificial Intelligence (AI), from its inception in the 1950s to the\nemergence of advanced neural networks and deep learning architectures, has made\na breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in\nVision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt\nengineering is the process of structuring inputs, which has emerged as a\ncrucial technique to maximize the utility and accuracy of these models. This\npaper explores both foundational and advanced methodologies of prompt\nengineering, including techniques such as self-consistency, chain-of-thought,\nand generated knowledge, which significantly enhance model performance.\nAdditionally, it examines the prompt method of VLMs through innovative\napproaches such as Context Optimization (CoOp), Conditional Context\nOptimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this\ndiscussion is the aspect of AI security, particularly adversarial attacks that\nexploit vulnerabilities in prompt engineering. Strategies to mitigate these\nrisks and enhance model robustness are thoroughly reviewed. The evaluation of\nprompt methods is also addressed, through both subjective and objective\nmetrics, ensuring a robust analysis of their efficacy. This review also\nreflects the essential role of prompt engineering in advancing AI capabilities,\nproviding a structured framework for future research and application.", "published": "2023-10-23 09:15:18", "link": "http://arxiv.org/abs/2310.14735v5", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Evaluating the Knowledge Base Completion Potential of GPT", "abstract": "Structured knowledge bases (KBs) are an asset for search engines and other\napplications, but are inevitably incomplete. Language models (LMs) have been\nproposed for unsupervised knowledge base completion (KBC), yet, their ability\nto do this at scale and with high accuracy remains an open question. Prior\nexperimental studies mostly fall short because they only evaluate on popular\nsubjects, or sample already existing facts from KBs. In this work, we perform a\ncareful evaluation of GPT's potential to complete the largest public KB:\nWikidata. We find that, despite their size and capabilities, models like GPT-3,\nChatGPT and GPT-4 do not achieve fully convincing results on this task.\nNonetheless, they provide solid improvements over earlier approaches with\nsmaller LMs. In particular, we show that, with proper thresholding, GPT-3\nenables to extend Wikidata by 27M facts at 90% precision.", "published": "2023-10-23 10:15:13", "link": "http://arxiv.org/abs/2310.14771v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Geographical Erasure in Language Generation", "abstract": "Large language models (LLMs) encode vast amounts of world knowledge. However,\nsince these models are trained on large swaths of internet data, they are at\nrisk of inordinately capturing information about dominant groups. This\nimbalance can propagate into generated language. In this work, we study and\noperationalise a form of geographical erasure, wherein language models\nunderpredict certain countries. We demonstrate consistent instances of erasure\nacross a range of LLMs. We discover that erasure strongly correlates with low\nfrequencies of country mentions in the training corpus. Lastly, we mitigate\nerasure by finetuning using a custom objective.", "published": "2023-10-23 10:26:14", "link": "http://arxiv.org/abs/2310.14777v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning\n  across Languages", "abstract": "Chain-of-thought (CoT) is capable of eliciting models to explicitly generate\nreasoning paths, thus promoting reasoning accuracy and attracting increasing\nattention. Specifically, zero-shot CoT achieves remarkable improvements in a\nwide range of reasoning tasks by simply instructing the LLM with the prompt\n\"Let's think step by step!\". Despite the success of zero-shot CoT, the existing\nzero-shot prompting techniques remain limited to a single language, making it\nchallenging to generalize to other languages and hindering global development.\nIn this work, we introduce cross-lingual prompting (CLP), aiming to improve\nzero-shot CoT reasoning across languages. Specifically, CLP consists of two\nmain components: (1) cross-lingual alignment prompting and (2) task-specific\nsolver prompting. The cross-lingual alignment prompting is responsible for\naligning representations across different languages, whereas the task-specific\nsolver prompting is used to generate the final chain of thoughts and results\nfor the reasoning task. In addition, we further introduce cross-lingual\nself-consistent prompting (CLSP) to ensemble different reasoning paths across\nlanguages. Our experimental evaluations on several benchmarks demonstrate that\nCLP and CLSP significantly outperform the existing prompting methods and\nachieve state-of-the-art performance. We hope this work will inspire further\nbreakthroughs in cross-lingual CoT.", "published": "2023-10-23 10:56:03", "link": "http://arxiv.org/abs/2310.14799v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Characterizing how 'distributional' NLP corpora distance metrics are", "abstract": "A corpus of vector-embedded text documents has some empirical distribution.\nGiven two corpora, we want to calculate a single metric of distance (e.g.,\nMauve, Frechet Inception) between them. We describe an abstract quality, called\n`distributionality', of such metrics. A non-distributional metric tends to use\nvery local measurements, or uses global measurements in a way that does not\nfully reflect the distributions' true distance. For example, if individual\npairwise nearest-neighbor distances are low, it may judge the two corpora to\nhave low distance, even if their two distributions are in fact far from each\nother. A more distributional metric will, in contrast, better capture the\ndistributions' overall distance. We quantify this quality by constructing a\nKnown-Similarity Corpora set from two paraphrase corpora and calculating the\ndistance between paired corpora from it. The distances' trend shape as set\nelement separation increases should quantify the distributionality of the\nmetric. We propose that Average Hausdorff Distance and energy distance between\ncorpora are representative examples of non-distributional and distributional\ndistance metrics, to which other metrics can be compared, to evaluate how\ndistributional they are.", "published": "2023-10-23 11:48:23", "link": "http://arxiv.org/abs/2310.14829v1", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Contextual Refinement of Translations: Large Language Models for\n  Sentence and Document-Level Post-Editing", "abstract": "Large Language Models (LLM's) have demonstrated considerable success in\nvarious Natural Language Processing tasks, but they have yet to attain\nstate-of-the-art performance in Neural Machine Translation (NMT). Nevertheless,\ntheir significant performance in tasks demanding a broad understanding and\ncontextual processing shows their potential for translation. To exploit these\nabilities, we investigate using LLM's for MT and explore recent\nparameter-efficient fine-tuning techniques. Surprisingly, our initial\nexperiments find that fine-tuning for translation purposes even led to\nperformance degradation. To overcome this, we propose an alternative approach:\nadapting LLM's as Automatic Post-Editors (APE) rather than direct translators.\nBuilding on the LLM's exceptional ability to process and generate lengthy\nsequences, we also propose extending our approach to document-level\ntranslation. We show that leveraging Low-Rank-Adapter fine-tuning for APE can\nyield significant improvements across both sentence and document-level metrics\nwhile generalizing to out-of-domain data. Most notably, we achieve a\nstate-of-the-art accuracy rate of 89\\% on the ContraPro test set, which\nspecifically assesses the model's ability to resolve pronoun ambiguities when\ntranslating from English to German. Lastly, we investigate a practical scenario\ninvolving manual post-editing for document-level translation, where reference\ncontext is made available. Here, we demonstrate that leveraging human\ncorrections can significantly reduce the number of edits required for\nsubsequent translations (Interactive Demo for integrating manual feedback can\nbe found here:\nhttps://huggingface.co/spaces/skoneru/contextual_refinement_ende).", "published": "2023-10-23 12:22:15", "link": "http://arxiv.org/abs/2310.14855v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for\n  Embodied Turn-Taking Prediction", "abstract": "Predicting turn-taking in multiparty conversations has many practical\napplications in human-computer/robot interaction. However, the complexity of\nhuman communication makes it a challenging task. Recent advances have shown\nthat synchronous multi-perspective egocentric data can significantly improve\nturn-taking prediction compared to asynchronous, single-perspective\ntranscriptions. Building on this research, we propose a new multimodal\ntransformer-based architecture for predicting turn-taking in embodied,\nsynchronized multi-perspective data. Our experimental results on the recently\nintroduced EgoCom dataset show a substantial performance improvement of up to\n14.01% on average compared to existing baselines and alternative\ntransformer-based approaches. The source code, and the pre-trained models of\nour 3M-Transformer will be available upon acceptance.", "published": "2023-10-23 12:29:10", "link": "http://arxiv.org/abs/2310.14859v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "We are Who We Cite: Bridges of Influence Between Natural Language\n  Processing and Other Academic Fields", "abstract": "Natural Language Processing (NLP) is poised to substantially influence the\nworld. However, significant progress comes hand-in-hand with substantial risks.\nAddressing them requires broad engagement with various fields of study. Yet,\nlittle empirical work examines the state of such engagement (past or current).\nIn this paper, we quantify the degree of influence between 23 fields of study\nand NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP\npapers to other papers, and ~1.8m citations from other papers to NLP papers. We\nshow that, unlike most fields, the cross-field engagement of NLP, measured by\nour proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in\n1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown\nmore insular -- citing increasingly more NLP papers and having fewer papers\nthat act as bridges between fields. NLP citations are dominated by computer\nscience; Less than 8% of NLP citations are to linguistics, and less than 3% are\nto math and psychology. These findings underscore NLP's urgent need to reflect\non its engagement with various fields.", "published": "2023-10-23 12:42:06", "link": "http://arxiv.org/abs/2310.14870v3", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Non-autoregressive Streaming Transformer for Simultaneous Translation", "abstract": "Simultaneous machine translation (SiMT) models are trained to strike a\nbalance between latency and translation quality. However, training these models\nto achieve high quality while maintaining low latency often leads to a tendency\nfor aggressive anticipation. We argue that such issue stems from the\nautoregressive architecture upon which most existing SiMT models are built. To\naddress those issues, we propose non-autoregressive streaming Transformer\n(NAST) which comprises a unidirectional encoder and a non-autoregressive\ndecoder with intra-chunk parallelism. We enable NAST to generate the blank\ntoken or repetitive tokens to adjust its READ/WRITE strategy flexibly, and\ntrain it to maximize the non-monotonic latent alignment with an alignment-based\nlatency loss. Experiments on various SiMT benchmarks demonstrate that NAST\noutperforms previous strong autoregressive SiMT baselines.", "published": "2023-10-23 12:52:24", "link": "http://arxiv.org/abs/2310.14883v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "PartialFormer: Modeling Part Instead of Whole for Machine Translation", "abstract": "The design choices in Transformer feed-forward neural networks have resulted\nin significant computational and parameter overhead. In this work, we emphasize\nthe importance of hidden dimensions in designing lightweight FFNs, a factor\noften overlooked in previous architectures. Guided by this principle, we\nintroduce PartialFormer, a parameter-efficient Transformer architecture\nutilizing multiple smaller FFNs to reduce parameters and computation while\nmaintaining essential hidden dimensions. These smaller FFNs are integrated into\na multi-head attention mechanism for effective collaboration. We also propose a\ntailored head scaling strategy to enhance PartialFormer's capabilities.\nFurthermore, we present a residual-like attention calculation to improve depth\nscaling within PartialFormer. Extensive experiments on 9 translation tasks and\n1 abstractive summarization task validate the effectiveness of our\nPartialFormer approach on machine translation and summarization tasks. Our code\nwould be available at: https://github.com/zhengkid/PartialFormer.", "published": "2023-10-23 13:25:54", "link": "http://arxiv.org/abs/2310.14921v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simple Hardware-Efficient PCFGs with Independent Left and Right\n  Productions", "abstract": "Scaling dense PCFGs to thousands of nonterminals via a low-rank\nparameterization of the rule probability tensor has been shown to be beneficial\nfor unsupervised parsing. However, PCFGs scaled this way still perform poorly\nas a language model, and even underperform similarly-sized HMMs. This work\nintroduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and\nright productions. Despite imposing a stronger independence assumption than the\nlow-rank approach, we find that this formalism scales more effectively both as\na language model and as an unsupervised parser. As an unsupervised parser, our\nsimple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language\nmodel, it obtains a perplexity of 119.0, outperforming similarly-sized low-rank\nPCFGs. We further introduce \\emph{FlashInside}, a hardware IO-aware\nimplementation of the inside algorithm for efficiently scaling simple PCFGs.", "published": "2023-10-23 14:48:51", "link": "http://arxiv.org/abs/2310.14997v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Data Learning for Open Information Extraction with Pre-trained\n  Language Models", "abstract": "Open Information Extraction (OpenIE) is a fundamental yet challenging task in\nNatural Language Processing, which involves extracting all triples (subject,\npredicate, object) from a given sentence. While labeling-based methods have\ntheir merits, generation-based techniques offer unique advantages, such as the\nability to generate tokens not present in the original sentence. However, these\ngeneration-based methods often require a significant amount of training data to\nlearn the task form of OpenIE and substantial training time to overcome slow\nmodel convergence due to the order penalty. In this paper, we introduce a novel\nframework, OK-IE, that ingeniously transforms the task form of OpenIE into the\npre-training task form of the T5 model, thereby reducing the need for extensive\ntraining data. Furthermore, we introduce an innovative concept of Anchor to\ncontrol the sequence of model outputs, effectively eliminating the impact of\norder penalty on model convergence and significantly reducing training time.\nExperimental results indicate that, compared to previous SOTA methods, OK-IE\nrequires only 1/100 of the training data (900 instances) and 1/120 of the\ntraining time (3 minutes) to achieve comparable results.", "published": "2023-10-23 15:19:24", "link": "http://arxiv.org/abs/2310.15021v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Localizing Active Objects from Egocentric Vision with Symbolic World\n  Knowledge", "abstract": "The ability to actively ground task instructions from an egocentric view is\ncrucial for AI agents to accomplish tasks or assist humans virtually. One\nimportant step towards this goal is to localize and track key active objects\nthat undergo major state change as a consequence of human actions/interactions\nto the environment without being told exactly what/where to ground (e.g.,\nlocalizing and tracking the `sponge` in video from the instruction \"Dip the\n`sponge` into the bucket.\"). While existing works approach this problem from a\npure vision perspective, we investigate to which extent the textual modality\n(i.e., task instructions) and their interaction with visual modality can be\nbeneficial. Specifically, we propose to improve phrase grounding models'\nability on localizing the active objects by: (1) learning the role of `objects\nundergoing change` and extracting them accurately from the instructions, (2)\nleveraging pre- and post-conditions of the objects during actions, and (3)\nrecognizing the objects more robustly with descriptional knowledge. We leverage\nlarge language models (LLMs) to extract the aforementioned action-object\nknowledge, and design a per-object aggregation masking technique to effectively\nperform joint inference on object phrases and symbolic knowledge. We evaluate\nour framework on Ego4D and Epic-Kitchens datasets. Extensive experiments\ndemonstrate the effectiveness of our proposed framework, which leads to>54%\nimprovements in all standard metrics on the TREK-150-OPE-Det localization +\ntracking task, >7% improvements in all standard metrics on the TREK-150-OPE\ntracking task, and >3% improvements in average precision (AP) on the Ego4D SCOD\ntask.", "published": "2023-10-23 16:14:05", "link": "http://arxiv.org/abs/2310.15066v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Causal Order: The Key to Leveraging Imperfect Experts in Causal\n  Inference", "abstract": "Large Language Models (LLMs) have been used as experts to infer causal\ngraphs, often by repeatedly applying a pairwise prompt that asks about the\ncausal relationship of each variable pair. However, such experts, including\nhuman domain experts, cannot distinguish between direct and indirect effects\ngiven a pairwise prompt. Therefore, instead of the graph, we propose that\ncausal order be used as a more stable output interface for utilizing expert\nknowledge. Even when querying a perfect expert with a pairwise prompt, we show\nthat the inferred graph can have significant errors whereas the causal order is\nalways correct. In practice, however, LLMs are imperfect experts and we find\nthat pairwise prompts lead to multiple cycles. Hence, we propose the triplet\nmethod, a novel querying strategy that introduces an auxiliary variable for\nevery variable pair and instructs the LLM to avoid cycles within this triplet.\nIt then uses a voting-based ensemble method that results in higher accuracy and\nfewer cycles while ensuring cost efficiency. Across multiple real-world graphs,\nsuch a triplet-based method yields a more accurate order than the pairwise\nprompt, using both LLMs and human annotators. The triplet method enhances\nrobustness by repeatedly querying an expert with different auxiliary variables,\nenabling smaller models like Phi-3 and Llama-3 8B Instruct to surpass GPT-4\nwith pairwise prompting. For practical usage, we show how the expert-provided\ncausal order from the triplet method can be used to reduce error in downstream\ngraph discovery and effect inference tasks.", "published": "2023-10-23 17:23:56", "link": "http://arxiv.org/abs/2310.15117v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Location-Aware Visual Question Generation with Lightweight Models", "abstract": "This work introduces a novel task, location-aware visual question generation\n(LocaVQG), which aims to generate engaging questions from data relevant to a\nparticular geographical location. Specifically, we represent such\nlocation-aware information with surrounding images and a GPS coordinate. To\ntackle this task, we present a dataset generation pipeline that leverages GPT-4\nto produce diverse and sophisticated questions. Then, we aim to learn a\nlightweight model that can address the LocaVQG task and fit on an edge device,\nsuch as a mobile phone. To this end, we propose a method which can reliably\ngenerate engaging questions from location-aware information. Our proposed\nmethod outperforms baselines regarding human evaluation (e.g., engagement,\ngrounding, coherence) and automatic evaluation metrics (e.g., BERTScore,\nROUGE-2). Moreover, we conduct extensive ablation studies to justify our\nproposed techniques for both generating the dataset and solving the task.", "published": "2023-10-23 17:33:31", "link": "http://arxiv.org/abs/2310.15129v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Quantifying the Dialect Gap and its Correlates Across Languages", "abstract": "Historically, researchers and consumers have noticed a decrease in quality\nwhen applying NLP tools to minority variants of languages (i.e. Puerto Rican\nSpanish or Swiss German), but studies exploring this have been limited to a\nselect few languages. Additionally, past studies have mainly been conducted in\na monolingual context, so cross-linguistic trends have not been identified and\ntied to external factors. In this work, we conduct a comprehensive evaluation\nof the most influential, state-of-the-art large language models (LLMs) across\ntwo high-use applications, machine translation and automatic speech\nrecognition, to assess their functionality on the regional dialects of several\nhigh- and low-resource languages. Additionally, we analyze how the regional\ndialect gap is correlated with economic, social, and linguistic factors. The\nimpact of training data, including related factors like dataset size and its\nconstruction procedure, is shown to be significant but not consistent across\nmodels or languages, meaning a one-size-fits-all approach cannot be taken in\nsolving the dialect gap. This work will lay the foundation for furthering the\nfield of dialectal NLP by laying out evident disparities and identifying\npossible pathways for addressing them through mindful data collection.", "published": "2023-10-23 17:42:01", "link": "http://arxiv.org/abs/2310.15135v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining\n  Language Models with First-Order Logic Provers", "abstract": "Logical reasoning, i.e., deductively inferring the truth value of a\nconclusion from a set of premises, is an important task for artificial\nintelligence with wide potential impacts on science, mathematics, and society.\nWhile many prompting-based strategies have been proposed to enable Large\nLanguage Models (LLMs) to do such reasoning more effectively, they still appear\nunsatisfactory, often failing in subtle and unpredictable ways. In this work,\nwe investigate the validity of instead reformulating such tasks as modular\nneurosymbolic programming, which we call LINC: Logical Inference via\nNeurosymbolic Computation. In LINC, the LLM acts as a semantic parser,\ntranslating premises and conclusions from natural language to expressions in\nfirst-order logic. These expressions are then offloaded to an external theorem\nprover, which symbolically performs deductive inference. Leveraging this\napproach, we observe significant performance gains on FOLIO and a balanced\nsubset of ProofWriter for three different models in nearly all experimental\nconditions we evaluate. On ProofWriter, augmenting the comparatively small\nopen-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5\nand GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%,\nrespectively. When used with GPT-4, LINC scores 26% higher than CoT on\nProofWriter while performing comparatively on FOLIO. Further analysis reveals\nthat although both methods on average succeed roughly equally often on this\ndataset, they exhibit distinct and complementary failure modes. We thus provide\npromising evidence for how logical reasoning over natural language can be\ntackled through jointly leveraging LLMs alongside symbolic provers. All\ncorresponding code is publicly available at https://github.com/benlipkin/linc", "published": "2023-10-23 17:58:40", "link": "http://arxiv.org/abs/2310.15164v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Visual Reasoning Coordinators", "abstract": "Visual reasoning requires multimodal perception and commonsense cognition of\nthe world. Recently, multiple vision-language models (VLMs) have been proposed\nwith excellent commonsense reasoning ability in various domains. However, how\nto harness the collective power of these complementary VLMs is rarely explored.\nExisting methods like ensemble still struggle to aggregate these models with\nthe desired higher-order communications. In this work, we propose Cola, a novel\nparadigm that coordinates multiple VLMs for visual reasoning. Our key insight\nis that a large language model (LLM) can efficiently coordinate multiple VLMs\nby facilitating natural language communication that leverages their distinct\nand complementary capabilities. Extensive experiments demonstrate that our\ninstruction tuning variant, Cola-FT, achieves state-of-the-art performance on\nvisual question answering (VQA), outside knowledge VQA, visual entailment, and\nvisual spatial reasoning tasks. Moreover, we show that our in-context learning\nvariant, Cola-Zero, exhibits competitive performance in zero and few-shot\nsettings, without finetuning. Through systematic ablation studies and\nvisualizations, we validate that a coordinator LLM indeed comprehends the\ninstruction prompts as well as the separate functionalities of VLMs; it then\ncoordinates them to enable impressive visual reasoning capabilities.", "published": "2023-10-23 17:59:31", "link": "http://arxiv.org/abs/2310.15166v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Function Vectors in Large Language Models", "abstract": "We report the presence of a simple neural mechanism that represents an\ninput-output function as a vector within autoregressive transformer language\nmodels (LMs). Using causal mediation analysis on a diverse range of\nin-context-learning (ICL) tasks, we find that a small number attention heads\ntransport a compact representation of the demonstrated task, which we call a\nfunction vector (FV). FVs are robust to changes in context, i.e., they trigger\nexecution of the task on inputs such as zero-shot and natural text settings\nthat do not resemble the ICL contexts from which they are collected. We test\nFVs across a range of tasks, models, and layers and find strong causal effects\nacross settings in middle layers. We investigate the internal structure of FVs\nand find while that they often contain information that encodes the output\nspace of the function, this information alone is not sufficient to reconstruct\nan FV. Finally, we test semantic vector composition in FVs, and find that to\nsome extent they can be summed to create vectors that trigger new complex\ntasks. Our findings show that compact, causal internal vector representations\nof function abstractions can be explicitly extracted from LLMs. Our code and\ndata are available at https://functions.baulab.info.", "published": "2023-10-23 17:55:24", "link": "http://arxiv.org/abs/2310.15213v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks", "abstract": "Recent efforts in natural language processing (NLP) commonsense reasoning\nresearch have yielded a considerable number of new datasets and benchmarks.\nHowever, most of these datasets formulate commonsense reasoning challenges in\nartificial scenarios that are not reflective of the tasks which real-world NLP\nsystems are designed to solve. In this work, we present CRoW, a\nmanually-curated, multi-task benchmark that evaluates the ability of models to\napply commonsense reasoning in the context of six real-world NLP tasks. CRoW is\nconstructed using a multi-stage data collection pipeline that rewrites examples\nfrom existing datasets using commonsense-violating perturbations. We use CRoW\nto study how NLP systems perform across different dimensions of commonsense\nknowledge, such as physical, temporal, and social reasoning. We find a\nsignificant performance gap when NLP systems are evaluated on CRoW compared to\nhumans, showcasing that commonsense reasoning is far from being solved in\nreal-world task settings. We make our dataset and leaderboard available to the\nresearch community at https://github.com/mismayil/crow.", "published": "2023-10-23 18:00:23", "link": "http://arxiv.org/abs/2310.15239v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Possibilities & Impossibilities of AI-generated Text Detection:\n  A Survey", "abstract": "Large Language Models (LLMs) have revolutionized the domain of natural\nlanguage processing (NLP) with remarkable capabilities of generating human-like\ntext responses. However, despite these advancements, several works in the\nexisting literature have raised serious concerns about the potential misuse of\nLLMs such as spreading misinformation, generating fake news, plagiarism in\nacademia, and contaminating the web. To address these concerns, a consensus\namong the research community is to develop algorithmic solutions to detect\nAI-generated text. The basic idea is that whenever we can tell if the given\ntext is either written by a human or an AI, we can utilize this information to\naddress the above-mentioned concerns. To that end, a plethora of detection\nframeworks have been proposed, highlighting the possibilities of AI-generated\ntext detection. But in parallel to the development of detection frameworks,\nresearchers have also concentrated on designing strategies to elude detection,\ni.e., focusing on the impossibilities of AI-generated text detection. This is a\ncrucial step in order to make sure the detection frameworks are robust enough\nand it is not too easy to fool a detector. Despite the huge interest and the\nflurry of research in this domain, the community currently lacks a\ncomprehensive analysis of recent developments. In this survey, we aim to\nprovide a concise categorization and overview of current work encompassing both\nthe prospects and the limitations of AI-generated text detection. To enrich the\ncollective knowledge, we engage in an exhaustive discussion on critical and\nchallenging open questions related to ongoing research on AI-generated text\ndetection.", "published": "2023-10-23 18:11:32", "link": "http://arxiv.org/abs/2310.15264v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GradSim: Gradient-Based Language Grouping for Effective Multilingual\n  Training", "abstract": "Most languages of the world pose low-resource challenges to natural language\nprocessing models. With multilingual training, knowledge can be shared among\nlanguages. However, not all languages positively influence each other and it is\nan open research question how to select the most suitable set of languages for\nmultilingual training and avoid negative interference among languages whose\ncharacteristics or data distributions are not compatible. In this paper, we\npropose GradSim, a language grouping method based on gradient similarity. Our\nexperiments on three diverse multilingual benchmark datasets show that it leads\nto the largest performance gains compared to other similarity measures and it\nis better correlated with cross-lingual model performance. As a result, we set\nthe new state of the art on AfriSenti, a benchmark dataset for sentiment\nanalysis on low-resource African languages. In our extensive analysis, we\nfurther reveal that besides linguistic features, the topics of the datasets\nplay an important role for language grouping and that lower layers of\ntransformer models encode language-specific features while higher layers\ncapture task-specific information.", "published": "2023-10-23 18:13:37", "link": "http://arxiv.org/abs/2310.15269v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages", "abstract": "The class of tree-adjoining languages can be characterized by various\ntwo-level formalisms, consisting of a context-free grammar (CFG) or pushdown\nautomaton (PDA) controlling another CFG or PDA. These four formalisms are\nequivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG),\npushdown-adjoining automata (PAA), and embedded pushdown automata (EPDA). We\ndefine semiring-weighted versions of the above two-level formalisms, and we\ndesign new algorithms for computing their stringsums (the weight of all\nderivations of a string) and allsums (the weight of all derivations). From\nthese, we also immediately obtain stringsum and allsum algorithms for TAG, LIG,\nPAA, and EPDA. For LIG, our algorithm is more time-efficient by a factor of\n$\\mathcal{O}(n|\\mathcal{N}|)$ (where $n$ is the string length and\n$|\\mathcal{N}|$ is the size of the nonterminal set) and more space-efficient by\na factor of $\\mathcal{O}(|\\Gamma|)$ (where $|\\Gamma|$ is the size of the stack\nalphabet) than the algorithm of Vijay-Shanker and Weir (1989). For EPDA, our\nalgorithm is both more space-efficient and time-efficient than the algorithm of\nAlonso et al. (2001) by factors of $\\mathcal{O}(|\\Gamma|^2)$ and\n$\\mathcal{O}(|\\Gamma|^3)$, respectively. Finally, we give the first PAA\nstringsum and allsum algorithms.", "published": "2023-10-23 18:26:00", "link": "http://arxiv.org/abs/2310.15276v1", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based\n  LLM", "abstract": "In the burgeoning field of natural language processing (NLP), Neural Topic\nModels (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged\nas areas of significant research interest. Despite this, NTMs primarily utilize\ncontextual embeddings from LLMs, which are not optimal for clustering or\ncapable for topic based text generation. NTMs have never been combined with\ndiffusion model for text generation. Our study addresses these gaps by\nintroducing a novel framework named Diffusion-Enhanced Topic Modeling using\nEncoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based\nLLMs to produce highly clusterable embeddings that could generate topics that\nexhibit both superior clusterability and enhanced semantic coherence compared\nto existing methods. Additionally, by exploiting the power of diffusion model,\nour framework also provides the capability to do topic based text generation.\nThis dual functionality allows users to efficiently produce highly clustered\ntopics and topic based text generation simultaneously. DeTiME's potential\nextends to generating clustered embeddings as well. Notably, our proposed\nframework(both encoder-decoder based LLM and diffusion model) proves to be\nefficient to train and exhibits high adaptability to other LLMs and diffusion\nmodel, demonstrating its potential for a wide array of applications.", "published": "2023-10-23 19:03:04", "link": "http://arxiv.org/abs/2310.15296v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TaskDiff: A Similarity Metric for Task-Oriented Conversations", "abstract": "The popularity of conversational digital assistants has resulted in the\navailability of large amounts of conversational data which can be utilized for\nimproved user experience and personalized response generation. Building these\nassistants using popular large language models like ChatGPT also require\nadditional emphasis on prompt engineering and evaluation methods. Textual\nsimilarity metrics are a key ingredient for such analysis and evaluations.\nWhile many similarity metrics have been proposed in the literature, they have\nnot proven effective for task-oriented conversations as they do not take\nadvantage of unique conversational features. To address this gap, we present\nTaskDiff, a novel conversational similarity metric that utilizes different\ndialogue components (utterances, intents, and slots) and their distributions to\ncompute similarity. Extensive experimental evaluation of TaskDiff on a\nbenchmark dataset demonstrates its superior performance and improved robustness\nover other related approaches.", "published": "2023-10-23 19:03:35", "link": "http://arxiv.org/abs/2310.15298v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring the Potential of Large Language Models in Generating\n  Code-Tracing Questions for Introductory Programming Courses", "abstract": "In this paper, we explore the application of large language models (LLMs) for\ngenerating code-tracing questions in introductory programming courses. We\ndesigned targeted prompts for GPT4, guiding it to generate code-tracing\nquestions based on code snippets and descriptions. We established a set of\nhuman evaluation metrics to assess the quality of questions produced by the\nmodel compared to those created by human experts. Our analysis provides\ninsights into the capabilities and potential of LLMs in generating diverse\ncode-tracing questions. Additionally, we present a unique dataset of human and\nLLM-generated tracing questions, serving as a valuable resource for both the\neducation and NLP research communities. This work contributes to the ongoing\ndialogue on the potential uses of LLMs in educational settings.", "published": "2023-10-23 19:35:01", "link": "http://arxiv.org/abs/2310.15317v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual,\n  Intensional, and Extensional Learning for Faithful Natural Language\n  Generation", "abstract": "We show that LLMs hallucinate because their output is not constrained to be\nsynonymous with claims for which they have evidence: a condition that we call\nevidential closure. Information about the truth or falsity of sentences is not\nstatistically identified in the standard neural probabilistic language model\nsetup, and so cannot be conditioned on to generate new strings. We then show\nhow to constrain LLMs to produce output that does satisfy evidential closure. A\nmultimodal LLM must learn about the external world (perceptual learning); it\nmust learn a mapping from strings to states of the world (extensional\nlearning); and, to achieve fluency when generalizing beyond a body of evidence,\nit must learn mappings from strings to their synonyms (intensional learning).\nThe output of a unimodal LLM must be synonymous with strings in a validated\nevidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune,\nthat yields faithful output from an LLM by rejecting output that is not\nsynonymous with claims for which the LLM has evidence.", "published": "2023-10-23 20:35:52", "link": "http://arxiv.org/abs/2310.15355v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "EpiK-Eval: Evaluation for Language Models as Epistemic Models", "abstract": "In the age of artificial intelligence, the role of large language models\n(LLMs) is becoming increasingly central. Despite their growing prevalence,\ntheir capacity to consolidate knowledge from different training documents - a\ncrucial ability in numerous applications - remains unexplored. This paper\npresents the first study examining the capability of LLMs to effectively\ncombine such information within their parameter space. We introduce EpiK-Eval,\na novel question-answering benchmark tailored to evaluate LLMs' proficiency in\nformulating a coherent and consistent knowledge representation from segmented\nnarratives. Evaluations across various LLMs reveal significant weaknesses in\nthis domain. We contend that these shortcomings stem from the intrinsic nature\nof prevailing training objectives. Consequently, we advocate for refining the\napproach towards knowledge consolidation, as it harbors the potential to\ndramatically improve their overall effectiveness and performance. The findings\nfrom this study offer insights for developing more robust and reliable LLMs.\nOur code and benchmark are available at\nhttps://github.com/chandar-lab/EpiK-Eval", "published": "2023-10-23 21:15:54", "link": "http://arxiv.org/abs/2310.15372v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"One-Size-Fits-All\"? Examining Expectations around What Constitute\n  \"Fair\" or \"Good\" NLG System Behaviors", "abstract": "Fairness-related assumptions about what constitute appropriate NLG system\nbehaviors range from invariance, where systems are expected to behave\nidentically for social groups, to adaptation, where behaviors should instead\nvary across them. To illuminate tensions around invariance and adaptation, we\nconduct five case studies, in which we perturb different types of\nidentity-related language features (names, roles, locations, dialect, and\nstyle) in NLG system inputs. Through these cases studies, we examine people's\nexpectations of system behaviors, and surface potential caveats of these\ncontrasting yet commonly held assumptions. We find that motivations for\nadaptation include social norms, cultural differences, feature-specific\ninformation, and accommodation; in contrast, motivations for invariance include\nperspectives that favor prescriptivism, view adaptation as unnecessary or too\ndifficult for NLG systems to do appropriately, and are wary of false\nassumptions. Our findings highlight open challenges around what constitute\n\"fair\" or \"good\" NLG system behaviors.", "published": "2023-10-23 23:00:34", "link": "http://arxiv.org/abs/2310.15398v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Chain-of-Factors Paper-Reviewer Matching", "abstract": "With the rapid increase in paper submissions to academic conferences, the\nneed for automated and accurate paper-reviewer matching is more critical than\never. Previous efforts in this area have considered various factors to assess\nthe relevance of a reviewer's expertise to a paper, such as the semantic\nsimilarity, shared topics, and citation connections between the paper and the\nreviewer's previous works. However, most of these studies focus on only one\nfactor, resulting in an incomplete evaluation of the paper-reviewer relevance.\nTo address this issue, we propose a unified model for paper-reviewer matching\nthat jointly considers semantic, topic, and citation factors. To be specific,\nduring training, we instruction-tune a contextualized language model shared\nacross all factors to capture their commonalities and characteristics; during\ninference, we chain the three factors to enable step-by-step, coarse-to-fine\nsearch for qualified reviewers given a submission. Experiments on four datasets\n(one of which is newly contributed by us) spanning various fields such as\nmachine learning, computer vision, information retrieval, and data mining\nconsistently demonstrate the effectiveness of our proposed Chain-of-Factors\nmodel in comparison with state-of-the-art paper-reviewer matching methods and\nscientific pre-trained language models.", "published": "2023-10-23 01:29:18", "link": "http://arxiv.org/abs/2310.14483v4", "categories": ["cs.IR", "cs.CL", "cs.DL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "CorefPrompt: Prompt-based Event Coreference Resolution by Measuring\n  Event Type and Argument Compatibilities", "abstract": "Event coreference resolution (ECR) aims to group event mentions referring to\nthe same real-world event into clusters. Most previous studies adopt the\n\"encoding first, then scoring\" framework, making the coreference judgment rely\non event encoding. Furthermore, current methods struggle to leverage\nhuman-summarized ECR rules, e.g., coreferential events should have the same\nevent type, to guide the model. To address these two issues, we propose a\nprompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM\n(masked language model) task. This allows for simultaneous event modeling and\ncoreference discrimination within a single template, with a fully shared\ncontext. In addition, we introduce two auxiliary prompt tasks, event-type\ncompatibility and argument compatibility, to explicitly demonstrate the\nreasoning process of ECR, which helps the model make final predictions.\nExperimental results show that our method CorefPrompt performs well in a\nstate-of-the-art (SOTA) benchmark.", "published": "2023-10-23 02:47:27", "link": "http://arxiv.org/abs/2310.14512v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared\n  Properties in Large Concept Vocabularies", "abstract": "Concepts play a central role in many applications. This includes settings\nwhere concepts have to be modelled in the absence of sentence context. Previous\nwork has therefore focused on distilling decontextualised concept embeddings\nfrom language models. But concepts can be modelled from different perspectives,\nwhereas concept embeddings typically mostly capture taxonomic structure. To\naddress this issue, we propose a strategy for identifying what different\nconcepts, from a potentially large concept vocabulary, have in common with\nothers. We then represent concepts in terms of the properties they share with\nthe other concepts. To demonstrate the practical usefulness of this way of\nmodelling concepts, we consider the task of ultra-fine entity typing, which is\na challenging multi-label classification problem. We show that by augmenting\nthe label set with shared properties, we can improve the performance of the\nstate-of-the-art models for this task.", "published": "2023-10-23 10:53:25", "link": "http://arxiv.org/abs/2310.14793v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models can Share Images, Too!", "abstract": "This paper explores the image-sharing capability of Large Language Models\n(LLMs), such as GPT-4 and LLaMA 2, in a zero-shot setting. To facilitate a\ncomprehensive evaluation of LLMs, we introduce the PhotoChat++ dataset, which\nincludes enriched annotations (i.e., intent, triggering sentence, image\ndescription, and salient information). Furthermore, we present the\ngradient-free and extensible Decide, Describe, and Retrieve (DribeR) framework.\nWith extensive experiments, we unlock the image-sharing capability of DribeR\nequipped with LLMs in zero-shot prompting, with ChatGPT achieving the best\nperformance. Our findings also reveal the emergent image-sharing ability in\nLLMs under zero-shot conditions, validating the effectiveness of DribeR. We use\nthis framework to demonstrate its practicality and effectiveness in two\nreal-world scenarios: (1) human-bot interaction and (2) dataset augmentation.\nTo the best of our knowledge, this is the first study to assess the\nimage-sharing ability of various LLMs in a zero-shot setting. We make our\nsource code and dataset publicly available at\nhttps://github.com/passing2961/DribeR.", "published": "2023-10-23 10:59:21", "link": "http://arxiv.org/abs/2310.14804v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Timestamp Information for Serialized Joint Streaming\n  Recognition and Translation", "abstract": "The growing need for instant spoken language transcription and translation is\ndriven by increased global communication and cross-lingual interactions. This\nhas made offering translations in multiple languages essential for user\napplications. Traditional approaches to automatic speech recognition (ASR) and\nspeech translation (ST) have often relied on separate systems, leading to\ninefficiencies in computational resources, and increased synchronization\ncomplexity in real time. In this paper, we propose a streaming\nTransformer-Transducer (T-T) model able to jointly produce many-to-one and\none-to-many transcription and translation using a single decoder. We introduce\na novel method for joint token-level serialized output training based on\ntimestamp information to effectively produce ASR and ST outputs in the\nstreaming setting. Experiments on {it,es,de}->en prove the effectiveness of our\napproach, enabling the generation of one-to-many joint outputs with a single\ndecoder for the first time.", "published": "2023-10-23 11:00:27", "link": "http://arxiv.org/abs/2310.14806v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Linking Surface Facts to Large-Scale Knowledge Graphs", "abstract": "Open Information Extraction (OIE) methods extract facts from natural language\ntext in the form of (\"subject\"; \"relation\"; \"object\") triples. These facts are,\nhowever, merely surface forms, the ambiguity of which impedes their downstream\nusage; e.g., the surface phrase \"Michael Jordan\" may refer to either the former\nbasketball player or the university professor. Knowledge Graphs (KGs), on the\nother hand, contain facts in a canonical (i.e., unambiguous) form, but their\ncoverage is limited by a static schema (i.e., a fixed set of entities and\npredicates). To bridge this gap, we need the best of both worlds: (i) high\ncoverage of free-text OIEs, and (ii) semantic precision (i.e., monosemy) of\nKGs. In order to achieve this goal, we propose a new benchmark with novel\nevaluation protocols that can, for example, measure fact linking performance on\na granular triple slot level, while also measuring if a system has the ability\nto recognize that a surface form has no match in the existing KG. Our extensive\nevaluation of several baselines show that detection of out-of-KG entities and\npredicates is more difficult than accurate linking to existing ones, thus\ncalling for more research efforts on this difficult task. We publicly release\nall resources (data, benchmark and code) on\nhttps://github.com/nec-research/fact-linking.", "published": "2023-10-23 13:18:49", "link": "http://arxiv.org/abs/2310.14909v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Key Frame Mechanism For Efficient Conformer Based End-to-end Speech\n  Recognition", "abstract": "Recently, Conformer as a backbone network for end-to-end automatic speech\nrecognition achieved state-of-the-art performance. The Conformer block\nleverages a self-attention mechanism to capture global information, along with\na convolutional neural network to capture local information, resulting in\nimproved performance. However, the Conformer-based model encounters an issue\nwith the self-attention mechanism, as computational complexity grows\nquadratically with the length of the input sequence. Inspired by previous\nConnectionist Temporal Classification (CTC) guided blank skipping during\ndecoding, we introduce intermediate CTC outputs as guidance into the\ndownsampling procedure of the Conformer encoder. We define the frame with\nnon-blank output as key frame. Specifically, we introduce the key frame-based\nself-attention (KFSA) mechanism, a novel method to reduce the computation of\nthe self-attention mechanism using key frames. The structure of our proposed\napproach comprises two encoders. Following the initial encoder, we introduce an\nintermediate CTC loss function to compute the label frame, enabling us to\nextract the key frames and blank frames for KFSA. Furthermore, we introduce the\nkey frame-based downsampling (KFDS) mechanism to operate on high-dimensional\nacoustic features directly and drop the frames corresponding to blank labels,\nwhich results in new acoustic feature sequences as input to the second encoder.\nBy using the proposed method, which achieves comparable or higher performance\nthan vanilla Conformer and other similar work such as Efficient Conformer.\nMeantime, our proposed method can discard more than 60\\% useless frames during\nmodel training and inference, which will accelerate the inference speed\nsignificantly. This work code is available in\n{https://github.com/scufan1990/Key-Frame-Mechanism-For-Efficient-Conformer}", "published": "2023-10-23 13:55:49", "link": "http://arxiv.org/abs/2310.14954v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ACTOR: Active Learning with Annotator-specific Classification Heads to\n  Embrace Human Label Variation", "abstract": "Label aggregation such as majority voting is commonly used to resolve\nannotator disagreement in dataset creation. However, this may disregard\nminority values and opinions. Recent studies indicate that learning from\nindividual annotations outperforms learning from aggregated labels, though they\nrequire a considerable amount of annotation. Active learning, as an annotation\ncost-saving strategy, has not been fully explored in the context of learning\nfrom disagreement. We show that in the active learning setting, a multi-head\nmodel performs significantly better than a single-head model in terms of\nuncertainty estimation. By designing and evaluating acquisition functions with\nannotator-specific heads on two datasets, we show that group-level entropy\nworks generally well on both datasets. Importantly, it achieves performance in\nterms of both prediction and uncertainty estimation comparable to full-scale\ntraining from disagreement, while saving up to 70% of the annotation budget.", "published": "2023-10-23 14:26:43", "link": "http://arxiv.org/abs/2310.14979v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Understanding the Inner Workings of Language Models Through\n  Representation Dissimilarity", "abstract": "As language models are applied to an increasing number of real-world\napplications, understanding their inner workings has become an important issue\nin model trust, interpretability, and transparency. In this work we show that\nrepresentation dissimilarity measures, which are functions that measure the\nextent to which two model's internal representations differ, can be a valuable\ntool for gaining insight into the mechanics of language models. Among our\ninsights are: (i) an apparent asymmetry in the internal representations of\nmodel using SoLU and GeLU activation functions, (ii) evidence that\ndissimilarity measures can identify and locate generalization properties of\nmodels that are invisible via in-distribution test set performance, and (iii)\nnew evaluations of how language model features vary as width and depth are\nincreased. Our results suggest that dissimilarity measures are a promising set\nof tools for shedding light on the inner workings of language models.", "published": "2023-10-23 14:46:20", "link": "http://arxiv.org/abs/2310.14993v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Did the Neurons Read your Book? Document-level Membership Inference for\n  Large Language Models", "abstract": "With large language models (LLMs) poised to become embedded in our daily\nlives, questions are starting to be raised about the data they learned from.\nThese questions range from potential bias or misinformation LLMs could retain\nfrom their training data to questions of copyright and fair use of\nhuman-generated text. However, while these questions emerge, developers of the\nrecent state-of-the-art LLMs become increasingly reluctant to disclose details\non their training corpus. We here introduce the task of document-level\nmembership inference for real-world LLMs, i.e. inferring whether the LLM has\nseen a given document during training or not. First, we propose a procedure for\nthe development and evaluation of document-level membership inference for LLMs\nby leveraging commonly used data sources for training and the model release\ndate. We then propose a practical, black-box method to predict document-level\nmembership and instantiate it on OpenLLaMA-7B with both books and academic\npapers. We show our methodology to perform very well, reaching an AUC of 0.856\nfor books and 0.678 for papers. We then show our approach to outperform the\nsentence-level membership inference attacks used in the privacy literature for\nthe document-level membership task. We further evaluate whether smaller models\nmight be less sensitive to document-level inference and show OpenLLaMA-3B to be\napproximately as sensitive as OpenLLaMA-7B to our approach. Finally, we\nconsider two mitigation strategies and find the AUC to slowly decrease when\nonly partial documents are considered but to remain fairly high when the model\nprecision is reduced. Taken together, our results show that accurate\ndocument-level membership can be inferred for LLMs, increasing the transparency\nof technology poised to change our lives.", "published": "2023-10-23 15:00:46", "link": "http://arxiv.org/abs/2310.15007v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Meta learning with language models: Challenges and opportunities in the\n  classification of imbalanced text", "abstract": "Detecting out of policy speech (OOPS) content is important but difficult.\nWhile machine learning is a powerful tool to tackle this challenging task, it\nis hard to break the performance ceiling due to factors like quantity and\nquality limitations on training data and inconsistencies in OOPS definition and\ndata labeling. To realize the full potential of available limited resources, we\npropose a meta learning technique (MLT) that combines individual models built\nwith different text representations. We analytically show that the resulting\ntechnique is numerically stable and produces reasonable combining weights. We\ncombine the MLT with a threshold-moving (TM) technique to further improve the\nperformance of the combined predictor on highly-imbalanced in-distribution and\nout-of-distribution datasets. We also provide computational results to show the\nstatistically significant advantages of the proposed MLT approach.\n  All authors contributed equally to this work.", "published": "2023-10-23 15:14:55", "link": "http://arxiv.org/abs/2310.15019v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Conceptualization of \"Fair Explanation\": Disparate Impacts of\n  anti-Asian Hate Speech Explanations on Content Moderators", "abstract": "Recent research at the intersection of AI explainability and fairness has\nfocused on how explanations can improve human-plus-AI task performance as\nassessed by fairness measures. We propose to characterize what constitutes an\nexplanation that is itself \"fair\" -- an explanation that does not adversely\nimpact specific populations. We formulate a novel evaluation method of \"fair\nexplanations\" using not just accuracy and label time, but also psychological\nimpact of explanations on different user groups across many metrics (mental\ndiscomfort, stereotype activation, and perceived workload). We apply this\nmethod in the context of content moderation of potential hate speech, and its\ndifferential impact on Asian vs. non-Asian proxy moderators, across explanation\napproaches (saliency map and counterfactual explanation). We find that saliency\nmaps generally perform better and show less evidence of disparate impact\n(group) and individual unfairness than counterfactual explanations.\n  Content warning: This paper contains examples of hate speech and racially\ndiscriminatory language. The authors do not support such content. Please\nconsider your risk of discomfort carefully before continuing reading!", "published": "2023-10-23 15:57:41", "link": "http://arxiv.org/abs/2310.15055v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained\n  Multimodal Models", "abstract": "Despite the impressive performance achieved by pre-trained\nlanguage-and-vision models in downstream tasks, it remains an open question\nwhether this reflects a proper understanding of image-text interaction. In this\nwork, we explore to what extent they handle basic linguistic constructions --\nactive-passive voice, coordination, and relative clauses -- that even preschool\nchildren can typically master. We present BLA, a novel, automatically\nconstructed benchmark to evaluate multimodal models on these Basic Language\nAbilities. We show that different types of Transformer-based systems, such as\nCLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting,\nin line with previous findings. Our experiments, in particular, show that most\nof the tested models only marginally benefit when fine-tuned or prompted with\nconstruction-specific samples. Yet, the generative BLIP2 shows promising\ntrends, especially in an in-context learning setting. This opens the door to\nusing BLA not only as an evaluation benchmark but also to improve models' basic\nlanguage abilities.", "published": "2023-10-23 16:05:13", "link": "http://arxiv.org/abs/2310.15061v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Affective and Dynamic Beam Search for Story Generation", "abstract": "Storytelling's captivating potential makes it a fascinating research area,\nwith implications for entertainment, education, therapy, and cognitive studies.\nIn this paper, we propose Affective Story Generator (AffGen) for generating\ninteresting narratives. AffGen introduces \"intriguing twists\" in narratives by\nemploying two novel techniques-Dynamic Beam Sizing and Affective Reranking.\nDynamic Beam Sizing encourages less predictable, more captivating word choices\nusing a contextual multi-arm bandit model. Affective Reranking prioritizes\nsentence candidates based on affect intensity. Our empirical evaluations, both\nautomatic and human, demonstrate AffGen's superior performance over existing\nbaselines in generating affectively charged and interesting narratives. Our\nablation study and analysis provide insights into the strengths and weaknesses\nof AffGen.", "published": "2023-10-23 16:37:14", "link": "http://arxiv.org/abs/2310.15079v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Federated Learning of Large Language Models with Parameter-Efficient\n  Prompt Tuning and Adaptive Optimization", "abstract": "Federated learning (FL) is a promising paradigm to enable collaborative model\ntraining with decentralized data. However, the training process of Large\nLanguage Models (LLMs) generally incurs the update of significant parameters,\nwhich limits the applicability of FL techniques to tackle the LLMs in real\nscenarios. Prompt tuning can significantly reduce the number of parameters to\nupdate, but it either incurs performance degradation or low training\nefficiency. The straightforward utilization of prompt tuning in the FL often\nraises non-trivial communication costs and dramatically degrades performance.\nIn addition, the decentralized data is generally non-Independent and\nIdentically Distributed (non-IID), which brings client drift problems and thus\npoor performance. This paper proposes a Parameter-efficient prompt Tuning\napproach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and\neffective FL of LLMs. First, an efficient partial prompt tuning approach is\nproposed to improve performance and efficiency simultaneously. Second, a novel\nadaptive optimization method is developed to address the client drift problems\non both the device and server sides to enhance performance further. Extensive\nexperiments based on 10 datasets demonstrate the superb performance (up to\n60.8\\% in terms of accuracy) and efficiency (up to 97.59\\% in terms of training\ntime) of FedPepTAO compared with 9 baseline approaches. Our code is available\nat https://github.com/llm-eff/FedPepTAO.", "published": "2023-10-23 16:37:59", "link": "http://arxiv.org/abs/2310.15080v3", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Branch-Solve-Merge Improves Large Language Model Evaluation and\n  Generation", "abstract": "Large Language Models (LLMs) are frequently used for multi-faceted language\ngeneration and evaluation tasks that involve satisfying intricate user\nconstraints or taking into account multiple aspects and criteria. However,\ntheir performance can fall short, due to the model's lack of coherence and\ninability to plan and decompose the problem. We propose Branch-Solve-Merge\n(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such\nchallenging natural language tasks. It consists of branch, solve, and merge\nmodules that are parameterized with specific prompts to the base LLM. These\nthree modules plan a decomposition of the task into multiple parallel\nsub-tasks, independently solve them, and fuse the solutions to the sub-tasks.\nWe apply our method to the tasks of LLM response evaluation and constrained\ntext generation and evaluate its effectiveness with multiple LLMs, including\nVicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and\nconsistency for each LLM by enhancing human-LLM agreement by up to 26%,\nreducing length and pairwise position biases by up to 50%, and allowing\nLLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story\ngeneration task, BSM improves the coherence of stories while also improving\nconstraint satisfaction by 12%.", "published": "2023-10-23 17:29:48", "link": "http://arxiv.org/abs/2310.15123v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large\n  Language Models", "abstract": "Pre-trained and frozen large language models (LLMs) can effectively map\nsimple scene rearrangement instructions to programs over a robot's visuomotor\nfunctions through appropriate few-shot example prompting. To parse open-domain\nnatural language and adapt to a user's idiosyncratic procedures, not known\nduring prompt engineering time, fixed prompts fall short. In this paper, we\nintroduce HELPER, an embodied agent equipped with an external memory of\nlanguage-program pairs that parses free-form human-robot dialogue into action\nprograms through retrieval-augmented LLM prompting: relevant memories are\nretrieved based on the current dialogue, instruction, correction, or VLM\ndescription, and used as in-context prompt examples for LLM querying. The\nmemory is expanded during deployment to include pairs of user's language and\naction plans, to assist future inferences and personalize them to the user's\nlanguage and routines. HELPER sets a new state-of-the-art in the TEACh\nbenchmark in both Execution from Dialog History (EDH) and Trajectory from\nDialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for\nTfD. Our models, code, and video results can be found in our project's website:\nhttps://helper-agent-llm.github.io.", "published": "2023-10-23 17:31:55", "link": "http://arxiv.org/abs/2310.15127v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.AI"}
{"title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large\n  Language Models", "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with\nmanual jailbreak attacks and (automatic) adversarial attacks. Recent studies\nsuggest that defending against these attacks is possible: adversarial attacks\ngenerate unlimited but unreadable gibberish prompts, detectable by\nperplexity-based filters; manual jailbreak attacks craft readable prompts, but\ntheir limited number due to the necessity of human creativity allows for easy\nblocking. In this paper, we show that these solutions may be too optimistic. We\nintroduce AutoDAN, an interpretable, gradient-based adversarial attack that\nmerges the strengths of both attack types. Guided by the dual goals of\njailbreak and readability, AutoDAN optimizes and generates tokens one by one\nfrom left to right, resulting in readable prompts that bypass perplexity\nfilters while maintaining high attack success rates. Notably, these prompts,\ngenerated from scratch using gradients, are interpretable and diverse, with\nemerging strategies commonly seen in manual jailbreak attacks. They also\ngeneralize to unforeseen harmful behaviors and transfer to black-box LLMs\nbetter than their unreadable counterparts when using limited training data or a\nsingle proxy model. Furthermore, we show the versatility of AutoDAN by\nautomatically leaking system prompts using a customized objective. Our work\noffers a new way to red-team LLMs and understand jailbreak mechanisms via\ninterpretability.", "published": "2023-10-23 17:46:07", "link": "http://arxiv.org/abs/2310.15140v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Verb Conjugation in Transformers Is Determined by Linear Encodings of\n  Subject Number", "abstract": "Deep architectures such as Transformers are sometimes criticized for having\nuninterpretable \"black-box\" representations. We use causal intervention\nanalysis to show that, in fact, some linguistic features are represented in a\nlinear, interpretable format. Specifically, we show that BERT's ability to\nconjugate verbs relies on a linear encoding of subject number that can be\nmanipulated with predictable effects on conjugation accuracy. This encoding is\nfound in the subject position at the first layer and the verb position at the\nlast layer, but distributed across positions at middle layers, particularly\nwhen there are multiple cues to subject number.", "published": "2023-10-23 17:53:47", "link": "http://arxiv.org/abs/2310.15151v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linear Representations of Sentiment in Large Language Models", "abstract": "Sentiment is a pervasive feature in natural language text, yet it is an open\nquestion how sentiment is represented within Large Language Models (LLMs). In\nthis study, we reveal that across a range of models, sentiment is represented\nlinearly: a single direction in activation space mostly captures the feature\nacross a range of tasks with one extreme for positive and the other for\nnegative. Through causal interventions, we isolate this direction and show it\nis causally relevant in both toy tasks and real world datasets such as Stanford\nSentiment Treebank. Through this case study we model a thorough investigation\nof what a single direction means on a broad data distribution.\n  We further uncover the mechanisms that involve this direction, highlighting\nthe roles of a small subset of attention heads and neurons. Finally, we\ndiscover a phenomenon which we term the summarization motif: sentiment is not\nsolely represented on emotionally charged words, but is additionally summarized\nat intermediate positions without inherent sentiment, such as punctuation and\nnames. We show that in Stanford Sentiment Treebank zero-shot classification,\n76% of above-chance classification accuracy is lost when ablating the sentiment\ndirection, nearly half of which (36%) is due to ablating the summarized\nsentiment direction exclusively at comma positions.", "published": "2023-10-23 17:55:31", "link": "http://arxiv.org/abs/2310.15154v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Reference Free Domain Adaptation for Translation of Noisy Questions with\n  Question Specific Rewards", "abstract": "Community Question-Answering (CQA) portals serve as a valuable tool for\nhelping users within an organization. However, making them accessible to\nnon-English-speaking users continues to be a challenge. Translating questions\ncan broaden the community's reach, benefiting individuals with similar\ninquiries in various languages. Translating questions using Neural Machine\nTranslation (NMT) poses more challenges, especially in noisy environments,\nwhere the grammatical correctness of the questions is not monitored. These\nquestions may be phrased as statements by non-native speakers, with incorrect\nsubject-verb order and sometimes even missing question marks. Creating a\nsynthetic parallel corpus from such data is also difficult due to its noisy\nnature. To address this issue, we propose a training methodology that\nfine-tunes the NMT system only using source-side data. Our approach balances\nadequacy and fluency by utilizing a loss function that combines BERTScore and\nMasked Language Model (MLM) Score. Our method surpasses the conventional\nMaximum Likelihood Estimation (MLE) based fine-tuning approach, which relies on\nsynthetic target data, by achieving a 1.9 BLEU score improvement. Our model\nexhibits robustness while we add noise to our baseline, and still achieve 1.1\nBLEU improvement and large improvements on TER and BLEURT metrics. Our proposed\nmethodology is model-agnostic and is only necessary during the training phase.\nWe make the codes and datasets publicly available at\n\\url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html#DomainAdapt} for\nfacilitating further research.", "published": "2023-10-23 18:08:01", "link": "http://arxiv.org/abs/2310.15259v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward a Critical Toponymy Framework for Named Entity Recognition: A\n  Case Study of Airbnb in New York City", "abstract": "Critical toponymy examines the dynamics of power, capital, and resistance\nthrough place names and the sites to which they refer. Studies here have\ntraditionally focused on the semantic content of toponyms and the top-down\ninstitutional processes that produce them. However, they have generally ignored\nthe ways in which toponyms are used by ordinary people in everyday discourse,\nas well as the other strategies of geospatial description that accompany and\ncontextualize toponymic reference. Here, we develop computational methods to\nmeasure how cultural and economic capital shape the ways in which people refer\nto places, through a novel annotated dataset of 47,440 New York City Airbnb\nlistings from the 2010s. Building on this dataset, we introduce a new named\nentity recognition (NER) model able to identify important discourse categories\nintegral to the characterization of place. Our findings point toward new\ndirections for critical toponymy and to a range of previously understudied\nlinguistic signals relevant to research on neighborhood status, housing and\ntourism markets, and gentrification.", "published": "2023-10-23 19:09:07", "link": "http://arxiv.org/abs/2310.15302v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Hallucination Detection for Grounded Instruction Generation", "abstract": "We investigate the problem of generating instructions to guide humans to\nnavigate in simulated residential environments. A major issue with current\nmodels is hallucination: they generate references to actions or objects that\nare inconsistent with what a human follower would perform or encounter along\nthe described path. We develop a model that detects these hallucinated\nreferences by adopting a model pre-trained on a large corpus of image-text\npairs, and fine-tuning it with a contrastive loss that separates correct\ninstructions from instructions containing synthesized hallucinations. Our final\nmodel outperforms several baselines, including using word probability estimated\nby the instruction-generation model, and supervised models based on LSTM and\nTransformer.", "published": "2023-10-23 19:36:28", "link": "http://arxiv.org/abs/2310.15319v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LXMERT Model Compression for Visual Question Answering", "abstract": "Large-scale pretrained models such as LXMERT are becoming popular for\nlearning cross-modal representations on text-image pairs for vision-language\ntasks. According to the lottery ticket hypothesis, NLP and computer vision\nmodels contain smaller subnetworks capable of being trained in isolation to\nfull performance. In this paper, we combine these observations to evaluate\nwhether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA\ntask. In addition, we perform a model size cost-benefit analysis by\ninvestigating how much pruning can be done without significant loss in\naccuracy. Our experiment results demonstrate that LXMERT can be effectively\npruned by 40%-60% in size with 3% loss in accuracy.", "published": "2023-10-23 19:46:41", "link": "http://arxiv.org/abs/2310.15325v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Moral Foundations of Large Language Models", "abstract": "Moral foundations theory (MFT) is a psychological assessment tool that\ndecomposes human moral reasoning into five factors, including care/harm,\nliberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary\nin the weight they place on these dimensions when making moral decisions, in\npart due to their cultural upbringing and political ideology. As large language\nmodels (LLMs) are trained on datasets collected from the internet, they may\nreflect the biases that are present in such corpora. This paper uses MFT as a\nlens to analyze whether popular LLMs have acquired a bias towards a particular\nset of moral values. We analyze known LLMs and find they exhibit particular\nmoral foundations, and show how these relate to human moral foundations and\npolitical affiliations. We also measure the consistency of these biases, or\nwhether they vary strongly depending on the context of how the model is\nprompted. Finally, we show that we can adversarially select prompts that\nencourage the moral to exhibit a particular set of moral foundations, and that\nthis can affect the model's behavior on downstream tasks. These findings help\nillustrate the potential risks and unintended consequences of LLMs assuming a\nparticular moral stance.", "published": "2023-10-23 20:05:37", "link": "http://arxiv.org/abs/2310.15337v1", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Semantic Data Management in Data Lakes", "abstract": "In recent years, data lakes emerged as away to manage large amounts of\nheterogeneous data for modern data analytics. One way to prevent data lakes\nfrom turning into inoperable data swamps is semantic data management. Some\napproaches propose the linkage of metadata to knowledge graphs based on the\nLinked Data principles to provide more meaning and semantics to the data in the\nlake. Such a semantic layer may be utilized not only for data management but\nalso to tackle the problem of data integration from heterogeneous sources, in\norder to make data access more expressive and interoperable. In this survey, we\nreview recent approaches with a specific focus on the application within data\nlake systems and scalability to Big Data. We classify the approaches into (i)\nbasic semantic data management, (ii) semantic modeling approaches for enriching\nmetadata in data lakes, and (iii) methods for ontologybased data access. In\neach category, we cover the main techniques and their background, and compare\nlatest research. Finally, we point out challenges for future work in this\nresearch area, which needs a closer integration of Big Data and Semantic Web\ntechnologies.", "published": "2023-10-23 21:16:50", "link": "http://arxiv.org/abs/2310.15373v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.DB"}
{"title": "Irreducible Curriculum for Language Model Pretraining", "abstract": "Automatic data selection and curriculum design for training large language\nmodels is challenging, with only a few existing methods showing improvements\nover standard training. Furthermore, current schemes focus on domain-level\nselection, overlooking the more fine-grained contributions of each individual\ntraining point. It is difficult to apply traditional datapoint selection\nmethods on large language models: most online batch selection methods perform\ntwo-times forward or backward passes, which introduces considerable extra costs\nwith large-scale models. To mitigate these obstacles, we propose irreducible\ncurriculum as a curriculum learning algorithm for language model pretraining,\nwhich prioritizes samples with higher learnability. Specifically, to avoid\nprohibitive extra computation overhead, we simulate the sample loss along the\nmain model's training trajectory using a small-scale proxy model. Our\nexperiments on the RedPajama-1B dataset demonstrate a consistent improvement on\nvalidation perplexity across all 7 domains compared to random uniform baseline\nand the anti-curriculum strategy. Our method also reduces the sharpness of the\nnetwork and illustrates a better 5-shot accuracy on MMLU benchmarks.", "published": "2023-10-23 22:41:33", "link": "http://arxiv.org/abs/2310.15389v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DoGE: Domain Reweighting with Generalization Estimation", "abstract": "The coverage and composition of the pretraining data significantly impacts\nthe generalization ability of Large Language Models (LLMs). Despite its\nimportance, recent LLMs still rely on heuristics and trial and error to\nincrease or reduce the influence of data-domains. We propose DOmain reweighting\nwith Generalization Estimation (DoGE), which optimizes the probability of\nsampling from each domain (domain weights) in a principled way. Our approach is\na two-stage process consisting of (i) training a proxy model to obtain domain\nweights using a bi-level optimization algorithm; (ii) training a larger base\nmodel by sampling training domains according to the learned domain weights. In\nour experiments, we extensively show how DoGE improves the generalization of\nthe base model to any target data mixture. On the SlimPajama dataset, our base\nmodel gets better perplexity and few-shot reasoning accuracies across $6$ tasks\ncompared to baseline methods. Moreover, aiming to generalize to out-of-domain\ntarget tasks, which is unseen in the pretraining corpus (OOD domain), DoGE can\neffectively identify inter-domain dependencies, and consistently achieves\nbetter test perplexity on the target domain.", "published": "2023-10-23 22:51:58", "link": "http://arxiv.org/abs/2310.15393v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Meaning Representations from Trajectories in Autoregressive Models", "abstract": "We propose to extract meaning representations from autoregressive language\nmodels by considering the distribution of all possible trajectories extending\nan input text. This strategy is prompt-free, does not require fine-tuning, and\nis applicable to any pre-trained autoregressive model. Moreover, unlike\nvector-based representations, distribution-based representations can also model\nasymmetric relations (e.g., direction of logical entailment, hypernym/hyponym\nrelations) by using algebraic operations between likelihood functions. These\nideas are grounded in distributional perspectives on semantics and are\nconnected to standard constructions in automata theory, but to our knowledge\nthey have not been applied to modern language models. We empirically show that\nthe representations obtained from large models align well with human\nannotations, outperform other zero-shot and prompt-free methods on semantic\nsimilarity tasks, and can be used to solve more complex entailment and\ncontainment tasks that standard embeddings cannot handle. Finally, we extend\nour method to represent data from different modalities (e.g., image and text)\nusing multimodal autoregressive models. Our code is available at:\nhttps://github.com/tianyu139/meaning-as-trajectories", "published": "2023-10-23 04:35:58", "link": "http://arxiv.org/abs/2310.18348v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Health Disparities through Generative AI Models: A Comparison Study\n  Using A Domain Specific large language model", "abstract": "Health disparities are differences in health outcomes and access to\nhealthcare between different groups, including racial and ethnic minorities,\nlow-income people, and rural residents. An artificial intelligence (AI) program\ncalled large language models (LLMs) can understand and generate human language,\nimproving health communication and reducing health disparities. There are many\nchallenges in using LLMs in human-doctor interaction, including the need for\ndiverse and representative data, privacy concerns, and collaboration between\nhealthcare providers and technology experts. We introduce the comparative\ninvestigation of domain-specific large language models such as SciBERT with a\nmulti-purpose LLMs BERT. We used cosine similarity to analyze text queries\nabout health disparities in exam rooms when factors such as race are used\nalone. Using text queries, SciBERT fails when it doesn't differentiate between\nqueries text: \"race\" alone and \"perpetuates health disparities.\" We believe\nclinicians can use generative AI to create a draft response when communicating\nasynchronously with patients. However, careful attention must be paid to ensure\nthey are developed and implemented ethically and equitably.", "published": "2023-10-23 21:24:05", "link": "http://arxiv.org/abs/2310.18355v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and\n  Beyond", "abstract": "Vision-language (VL) understanding tasks evaluate models' comprehension of\ncomplex visual scenes through multiple-choice questions. However, we have\nidentified two dataset biases that models can exploit as shortcuts to resolve\nvarious VL tasks correctly without proper understanding. The first type of\ndataset bias is \\emph{Unbalanced Matching} bias, where the correct answer\noverlaps the question and image more than the incorrect answers. The second\ntype of dataset bias is \\emph{Distractor Similarity} bias, where incorrect\nanswers are overly dissimilar to the correct answer but significantly similar\nto other incorrect answers within the same sample. To address these dataset\nbiases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic\ntraining and debiased evaluation data. We then introduce Intra-sample\nCounterfactual Training (ICT) to assist models in utilizing the synthesized\ntraining data, particularly the counterfactual data, via focusing on\nintra-sample differentiation. Extensive experiments demonstrate the\neffectiveness of ADS and ICT in consistently improving model performance across\ndifferent benchmarks, even in domain-shifted scenarios.", "published": "2023-10-23 08:09:42", "link": "http://arxiv.org/abs/2310.14670v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "SpecTr: Fast Speculative Decoding via Optimal Transport", "abstract": "Autoregressive sampling from large language models has led to\nstate-of-the-art results in several natural language tasks. However,\nautoregressive sampling generates tokens one at a time making it slow, and even\nprohibitive in certain tasks. One way to speed up sampling is\n$\\textit{speculative decoding}$: use a small model to sample a $\\textit{draft}$\n(block or sequence of tokens), and then score all tokens in the draft by the\nlarge language model in parallel. A subset of the tokens in the draft are\naccepted (and the rest rejected) based on a statistical method to guarantee\nthat the final output follows the distribution of the large model. In this\nwork, we provide a principled understanding of speculative decoding through the\nlens of optimal transport (OT) with $\\textit{membership cost}$. This framework\ncan be viewed as an extension of the well-known $\\textit{maximal-coupling}$\nproblem. This new formulation enables us to generalize the speculative decoding\nmethod to allow for a set of $k$ candidates at the token-level, which leads to\nan improved optimal membership cost. We show that the optimal draft selection\nalgorithm (transport plan) can be computed via linear programming, whose\nbest-known runtime is exponential in $k$. We then propose a valid draft\nselection algorithm whose acceptance probability is $(1-1/e)$-optimal\nmultiplicatively. Moreover, it can be computed in time almost linear with size\nof domain of a single token. Using this $new draft selection$ algorithm, we\ndevelop a new autoregressive sampling algorithm called $\\textit{SpecTr}$, which\nprovides speedup in decoding while ensuring that there is no quality\ndegradation in the decoded output. We experimentally demonstrate that for\nstate-of-the-art large language models, the proposed approach achieves a wall\nclock speedup of 2.13X, a further 1.37X speedup over speculative decoding on\nstandard benchmarks.", "published": "2023-10-23 17:47:34", "link": "http://arxiv.org/abs/2310.15141v2", "categories": ["cs.LG", "cs.CL", "cs.DS", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Acoustic BPE for Speech Generation with Discrete Tokens", "abstract": "Discrete audio tokens derived from self-supervised learning models have\ngained widespread usage in speech generation. However, current practice of\ndirectly utilizing audio tokens poses challenges for sequence modeling due to\nthe length of the token sequence. Additionally, this approach places the burden\non the model to establish correlations between tokens, further complicating the\nmodeling process. To address this issue, we propose acoustic BPE which encodes\nfrequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE\neffectively reduces the sequence length and leverages the prior morphological\ninformation present in token sequence, which alleviates the modeling challenges\nof token correlation. Through comprehensive investigations on a speech language\nmodel trained with acoustic BPE, we confirm the notable advantages it offers,\nincluding faster inference and improved syntax capturing capabilities. In\naddition, we propose a novel rescore method to select the optimal synthetic\nspeech among multiple candidates generated by rich-diversity TTS system.\nExperiments prove that rescore selection aligns closely with human preference,\nwhich highlights acoustic BPE's potential to other speech generation tasks.", "published": "2023-10-23 05:38:41", "link": "http://arxiv.org/abs/2310.14580v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Prompt-driven Target Speech Diarization", "abstract": "We introduce a novel task named `target speech diarization', which seeks to\ndetermine `when target event occurred' within an audio signal. We devise a\nneural architecture called Prompt-driven Target Speech Diarization (PTSD), that\nworks with diverse prompts that specify the target speech events of interest.\nWe train and evaluate PTSD using sim2spk, sim3spk and sim4spk datasets, which\nare derived from the Librispeech. We show that the proposed framework\naccurately localizes target speech events. Furthermore, our framework exhibits\nversatility through its impressive performance in three diarization-related\ntasks: target speaker voice activity detection, overlapped speech detection and\ngender diarization. In particular, PTSD achieves comparable performance to\nspecialized models across these tasks on both real and simulated data. This\nwork serves as a reference benchmark and provides valuable insights into\nprompt-driven target speech processing.", "published": "2023-10-23 11:42:38", "link": "http://arxiv.org/abs/2310.14823v3", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "8+8=4: Formalizing Time Units to Handle Symbolic Music Durations", "abstract": "This paper focuses on the nominal durations of musical events (notes and\nrests) in a symbolic musical score, and on how to conveniently handle these in\ncomputer applications. We propose the usage of a temporal unit that is directly\nrelated to the graphical symbols in musical scores and pair this with a set of\noperations that cover typical computations in music applications. We formalize\nthis time unit and the more commonly used approach in a single mathematical\nframework, as semirings, algebraic structures that enable an abstract\ndescription of algorithms/processing pipelines. We then discuss some practical\nuse cases and highlight when our system can improve such pipelines by making\nthem more efficient in terms of data type used and the number of computations.", "published": "2023-10-23 13:54:53", "link": "http://arxiv.org/abs/2310.14952v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GESI: Gammachirp Envelope Similarity Index for Predicting\n  Intelligibility of Simulated Hearing Loss Sounds", "abstract": "We propose an objective intelligibility measure (OIM), called the Gammachirp\nEnvelope Similarity Index (GESI), which can predict the speech intelligibility\n(SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners.\nGESI is an intrusive method that computes the SI metric using the gammachirp\nfilterbank (GCFB), the modulation filterbank, and the extended cosine\nsimilarity measure. The unique features of GESI are that i) it reflects the\nhearing impaired (HI) listener's HL that appears in the audiogram and is caused\nby active and passive cochlear dysfunction, ii) it provides a single goodness\nmetric, as in the widely used STOI and ESTOI, that can be used immediately to\nevaluate SE algorithms, and iii) it provides a simple control parameter to\naccept the level asymmetry of the reference and test sounds and to deal with\nindividual listening conditions and environments. We evaluated GESI and the\nconventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using\nfour SI experiments on words of male and female speech sounds in both\nlaboratory and remote environments. GESI was shown to outperform the other OIMs\nin the evaluations. GESI could be used to improve SE algorithms in assistive\nlistening devices for individual HI listeners.", "published": "2023-10-23 23:01:33", "link": "http://arxiv.org/abs/2310.15399v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Speaker Tracking: Progress, Challenges, and Future\n  Directions", "abstract": "Audio-visual speaker tracking has drawn increasing attention over the past\nfew years due to its academic values and wide applications. Audio and visual\nmodalities can provide complementary information for localization and tracking.\nWith audio and visual information, the Bayesian-based filter and deep\nlearning-based methods can solve the problem of data association, audio-visual\nfusion and track management. In this paper, we conduct a comprehensive overview\nof audio-visual speaker tracking. To our knowledge, this is the first extensive\nsurvey over the past five years. We introduce the family of Bayesian filters\nand summarize the methods for obtaining audio-visual measurements. In addition,\nthe existing trackers and their performance on the AV16.3 dataset are\nsummarized. In the past few years, deep learning techniques have thrived, which\nalso boost the development of audio-visual speaker tracking. The influence of\ndeep learning techniques in terms of measurement extraction and state\nestimation is also discussed. Finally, we discuss the connections between\naudio-visual speaker tracking and other areas such as speech separation and\ndistributed speaker tracking.", "published": "2023-10-23 10:29:33", "link": "http://arxiv.org/abs/2310.14778v4", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Intuitive Multilingual Audio-Visual Speech Recognition with a\n  Single-Trained Model", "abstract": "We present a novel approach to multilingual audio-visual speech recognition\ntasks by introducing a single model on a multilingual dataset. Motivated by a\nhuman cognitive system where humans can intuitively distinguish different\nlanguages without any conscious effort or guidance, we propose a model that can\ncapture which language is given as an input speech by distinguishing the\ninherent similarities and differences between languages. To do so, we design a\nprompt fine-tuning technique into the largely pre-trained audio-visual\nrepresentation model so that the network can recognize the language class as\nwell as the speech with the corresponding language. Our work contributes to\ndeveloping robust and efficient multilingual audio-visual speech recognition\nsystems, reducing the need for language-specific models.", "published": "2023-10-23 13:45:21", "link": "http://arxiv.org/abs/2310.14946v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Delayed Memory Unit: Modelling Temporal Dependency Through Delay Gate", "abstract": "Recurrent Neural Networks (RNNs) are widely recognized for their proficiency\nin modeling temporal dependencies, making them highly prevalent in sequential\ndata processing applications. Nevertheless, vanilla RNNs are confronted with\nthe well-known issue of gradient vanishing and exploding, posing a significant\nchallenge for learning and establishing long-range dependencies. Additionally,\ngated RNNs tend to be over-parameterized, resulting in poor computational\nefficiency and network generalization. To address these challenges, this paper\nproposes a novel Delayed Memory Unit (DMU). The DMU incorporates a delay line\nstructure along with delay gates into vanilla RNN, thereby enhancing temporal\ninteraction and facilitating temporal credit assignment. Specifically, the DMU\nis designed to directly distribute the input information to the optimal time\ninstant in the future, rather than aggregating and redistributing it over time\nthrough intricate network dynamics. Our proposed DMU demonstrates superior\ntemporal modeling capabilities across a broad range of sequential modeling\ntasks, utilizing considerably fewer parameters than other state-of-the-art\ngated RNN models in applications such as speech recognition, radar gesture\nrecognition, ECG waveform segmentation, and permuted sequential image\nclassification.", "published": "2023-10-23 14:29:48", "link": "http://arxiv.org/abs/2310.14982v2", "categories": ["cs.NE", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.NE"}
{"title": "Novel-View Acoustic Synthesis from 3D Reconstructed Rooms", "abstract": "We investigate the benefit of combining blind audio recordings with 3D scene\ninformation for novel-view acoustic synthesis. Given audio recordings from 2-4\nmicrophones and the 3D geometry and material of a scene containing multiple\nunknown sound sources, we estimate the sound anywhere in the scene. We identify\nthe main challenges of novel-view acoustic synthesis as sound source\nlocalization, separation, and dereverberation. While naively training an\nend-to-end network fails to produce high-quality results, we show that\nincorporating room impulse responses (RIRs) derived from 3D reconstructed rooms\nenables the same network to jointly tackle these tasks. Our method outperforms\nexisting methods designed for the individual tasks, demonstrating its\neffectiveness at utilizing 3D visual information. In a simulated study on the\nMatterport3D-NVAS dataset, our model achieves near-perfect accuracy on source\nlocalization, a PSNR of 26.44dB and a SDR of 14.23dB for source separation and\ndereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on\nnovel-view acoustic synthesis. We release our code and model on our project\nwebsite at https://github.com/apple/ml-nvas3d. Please wear headphones when\nlistening to the results.", "published": "2023-10-23 17:34:31", "link": "http://arxiv.org/abs/2310.15130v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Modality Dropout for Multimodal Device Directed Speech Detection using\n  Verbal and Non-Verbal Features", "abstract": "Device-directed speech detection (DDSD) is the binary classification task of\ndistinguishing between queries directed at a voice assistant versus side\nconversation or background speech. State-of-the-art DDSD systems use verbal\ncues, e.g acoustic, text and/or automatic speech recognition system (ASR)\nfeatures, to classify speech as device-directed or otherwise, and often have to\ncontend with one or more of these modalities being unavailable when deployed in\nreal-world settings. In this paper, we investigate fusion schemes for DDSD\nsystems that can be made more robust to missing modalities. Concurrently, we\nstudy the use of non-verbal cues, specifically prosody features, in addition to\nverbal cues for DDSD. We present different approaches to combine scores and\nembeddings from prosody with the corresponding verbal cues, finding that\nprosody improves DDSD performance by upto 8.5% in terms of false acceptance\nrate (FA) at a given fixed operating point via non-linear intermediate fusion,\nwhile our use of modality dropout techniques improves the performance of these\nmodels by 7.4% in terms of FA when evaluated with missing modalities during\ninference time.", "published": "2023-10-23 18:09:31", "link": "http://arxiv.org/abs/2310.15261v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cascaded Multi-task Adaptive Learning Based on Neural Architecture\n  Search", "abstract": "Cascading multiple pre-trained models is an effective way to compose an\nend-to-end system. However, fine-tuning the full cascaded model is parameter\nand memory inefficient and our observations reveal that only applying adapter\nmodules on cascaded model can not achieve considerable performance as\nfine-tuning. We propose an automatic and effective adaptive learning method to\noptimize end-to-end cascaded multi-task models based on Neural Architecture\nSearch (NAS) framework. The candidate adaptive operations on each specific\nmodule consist of frozen, inserting an adapter and fine-tuning. We further add\na penalty item on the loss to limit the learned structure which takes the\namount of trainable parameters into account. The penalty item successfully\nrestrict the searched architecture and the proposed approach is able to search\nsimilar tuning scheme with hand-craft, compressing the optimizing parameters to\n8.7% corresponding to full fine-tuning on SLURP with an even better\nperformance.", "published": "2023-10-23 06:43:50", "link": "http://arxiv.org/abs/2310.17664v1", "categories": ["cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis", "abstract": "Sound design involves creatively selecting, recording, and editing sound\neffects for various media like cinema, video games, and virtual/augmented\nreality. One of the most time-consuming steps when designing sound is\nsynchronizing audio with video. In some cases, environmental recordings from\nvideo shoots are available, which can aid in the process. However, in video\ngames and animations, no reference audio exists, requiring manual annotation of\nevent timings from the video. We propose a system to extract repetitive actions\nonsets from a video, which are then used - in conjunction with audio or textual\nembeddings - to condition a diffusion model trained to generate a new\nsynchronized sound effects audio track. In this way, we leave complete creative\ncontrol to the sound designer while removing the burden of synchronization with\nvideo. Furthermore, editing the onset track or changing the conditioning\nembedding requires much less effort than editing the audio track itself,\nsimplifying the sonification process. We provide sound examples, source code,\nand pretrained models to faciliate reproducibility", "published": "2023-10-23 18:01:36", "link": "http://arxiv.org/abs/2310.15247v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
