{"title": "Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss", "abstract": "The task of Fine-grained Entity Type Classification (FETC) consists of\nassigning types from a hierarchy to entity mentions in text. Existing methods\nrely on distant supervision and are thus susceptible to noisy labels that can\nbe out-of-context or overly-specific for the training sentence. Previous\nmethods that attempt to address these issues do so with heuristics or with the\nhelp of hand-crafted features. Instead, we propose an end-to-end solution with\na neural network model that uses a variant of cross- entropy loss function to\nhandle out-of-context labels, and hierarchical loss normalization to cope with\noverly-specific ones. Also, previous work solve FETC a multi-label\nclassification followed by ad-hoc post-processing. In contrast, our solution is\nmore elegant: we use public word embeddings to train a single-label that\njointly learns representations for entity mentions and their context. We show\nexperimentally that our approach is robust against noise and consistently\noutperforms the state-of-the-art on established benchmarks for the task.", "published": "2018-03-09 04:15:22", "link": "http://arxiv.org/abs/1803.03378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Unsupervised Model with Attention Autoencoders for Question Retrieval", "abstract": "Question retrieval is a crucial subtask for community question answering.\nPrevious research focus on supervised models which depend heavily on training\ndata and manual feature engineering. In this paper, we propose a novel\nunsupervised framework, namely reduced attentive matching network (RAMN), to\ncompute semantic matching between two questions. Our RAMN integrates together\nthe deep semantic representations, the shallow lexical mismatching information\nand the initial rank produced by an external search engine. For the first time,\nwe propose attention autoencoders to generate semantic representations of\nquestions. In addition, we employ lexical mismatching to capture surface\nmatching between two questions, which is derived from the importance of each\nword in a question. We conduct experiments on the open CQA datasets of\nSemEval-2016 and SemEval-2017. The experimental results show that our\nunsupervised model obtains comparable performance with the state-of-the-art\nsupervised methods in SemEval-2016 Task 3, and outperforms the best system in\nSemEval-2017 Task 3 by a wide margin.", "published": "2018-03-09 11:44:39", "link": "http://arxiv.org/abs/1803.03476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Importance of Being Recurrent for Modeling Hierarchical Structure", "abstract": "Recent work has shown that recurrent neural networks (RNNs) can implicitly\ncapture and exploit hierarchical information when trained to solve common\nnatural language processing tasks such as language modeling (Linzen et al.,\n2016) and neural machine translation (Shi et al., 2016). In contrast, the\nability to model structured data with non-recurrent neural networks has\nreceived little attention despite their success in many NLP tasks (Gehring et\nal., 2017; Vaswani et al., 2017). In this work, we compare the two\narchitectures---recurrent versus non-recurrent---with respect to their ability\nto model hierarchical structure and find that recurrency is indeed important\nfor this purpose.", "published": "2018-03-09 16:13:02", "link": "http://arxiv.org/abs/1803.03585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expert Finding in Heterogeneous Bibliographic Networks with\n  Locally-trained Embeddings", "abstract": "Expert finding is an important task in both industry and academia. It is\nchallenging to rank candidates with appropriate expertise for various queries.\nIn addition, different types of objects interact with one another, which\nnaturally forms heterogeneous information networks. We study the task of expert\nfinding in heterogeneous bibliographical networks based on two aspects: textual\ncontent analysis and authority ranking. Regarding the textual content analysis,\nwe propose a new method for query expansion via locally-trained embedding\nlearning with concept hierarchy as guidance, which is particularly tailored for\nspecific queries with narrow semantic meanings. Compared with global embedding\nlearning, locally-trained embedding learning projects the terms into a latent\nsemantic space constrained on relevant topics, therefore it preserves more\nprecise and subtle information for specific queries. Considering the candidate\nranking, the heterogeneous information network structure, while being largely\nignored in the previous studies of expert finding, provides additional\ninformation. Specifically, different types of interactions among objects play\ndifferent roles. We propose a ranking algorithm to estimate the authority of\nobjects in the network, treating each strongly-typed edge type individually. To\ndemonstrate the effectiveness of the proposed framework, we apply the proposed\nmethod to a large-scale bibliographical dataset with over two million entries\nand one million researcher candidates. The experiment results show that the\nproposed framework outperforms existing methods for both general and specific\nqueries.", "published": "2018-03-09 03:28:36", "link": "http://arxiv.org/abs/1803.03370v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Learning Approximate Inference Networks for Structured Prediction", "abstract": "Structured prediction energy networks (SPENs; Belanger & McCallum 2016) use\nneural network architectures to define energy functions that can capture\narbitrary dependencies among parts of structured outputs. Prior work used\ngradient descent for inference, relaxing the structured output to a set of\ncontinuous variables and then optimizing the energy with respect to them. We\nreplace this use of gradient descent with a neural network trained to\napproximate structured argmax inference. This \"inference network\" outputs\ncontinuous values that we treat as the output structure. We develop\nlarge-margin training criteria for joint training of the structured energy\nfunction and inference network. On multi-label classification we report\nspeed-ups of 10-60x compared to (Belanger et al, 2017) while also improving\naccuracy. For sequence labeling with simple structured energies, our approach\nperforms comparably to exact inference while being much faster at test time. We\nthen demonstrate improved accuracy by augmenting the energy with a \"label\nlanguage model\" that scores entire output label sequences, showing it can\nimprove handling of long-distance dependencies in part-of-speech tagging.\nFinally, we show how inference networks can replace dynamic programming for\ntest-time inference in conditional random fields, suggestive for their general\nuse for fast inference in structured settings.", "published": "2018-03-09 03:50:24", "link": "http://arxiv.org/abs/1803.03376v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improved and Scalable Online Learning of Spatial Concepts and Language\n  Models with Mapping", "abstract": "We propose a novel online learning algorithm, called SpCoSLAM 2.0, for\nspatial concepts and lexical acquisition with high accuracy and scalability.\nPreviously, we proposed SpCoSLAM as an online learning algorithm based on\nunsupervised Bayesian probabilistic model that integrates multimodal place\ncategorization, lexical acquisition, and SLAM. However, our original algorithm\nhad limited estimation accuracy owing to the influence of the early stages of\nlearning, and increased computational complexity with added training data.\nTherefore, we introduce techniques such as fixed-lag rejuvenation to reduce the\ncalculation time while maintaining an accuracy higher than that of the original\nalgorithm. The results show that, in terms of estimation accuracy, the proposed\nalgorithm exceeds the original algorithm and is comparable to batch learning.\nIn addition, the calculation time of the proposed algorithm does not depend on\nthe amount of training data and becomes constant for each step of the scalable\nalgorithm. Our approach will contribute to the realization of long-term spatial\nlanguage interactions between humans and robots.", "published": "2018-03-09 12:06:04", "link": "http://arxiv.org/abs/1803.03481v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Community Interaction and Conflict on the Web", "abstract": "Users organize themselves into communities on web platforms. These\ncommunities can interact with one another, often leading to conflicts and toxic\ninteractions. However, little is known about the mechanisms of interactions\nbetween communities and how they impact users.\n  Here we study intercommunity interactions across 36,000 communities on\nReddit, examining cases where users of one community are mobilized by negative\nsentiment to comment in another community. We show that such conflicts tend to\nbe initiated by a handful of communities---less than 1% of communities start\n74% of conflicts. While conflicts tend to be initiated by highly active\ncommunity members, they are carried out by significantly less active members.\nWe find that conflicts are marked by formation of echo chambers, where users\nprimarily talk to other users from their own community. In the long-term,\nconflicts have adverse effects and reduce the overall activity of users in the\ntargeted communities.\n  Our analysis of user interactions also suggests strategies for mitigating the\nnegative impact of conflicts---such as increasing direct engagement between\nattackers and defenders. Further, we accurately predict whether a conflict will\noccur by creating a novel LSTM model that combines graph embeddings, user,\ncommunity, and text features. This model can be used toreate early-warning\nsystems for community moderators to prevent conflicts. Altogether, this work\npresents a data-driven view of community interactions and conflict, and paves\nthe way towards healthier online communities.", "published": "2018-03-09 21:26:13", "link": "http://arxiv.org/abs/1803.03697v1", "categories": ["cs.SI", "cs.CL", "cs.HC"], "primary_category": "cs.SI"}
{"title": "Homomorphic Encryption for Speaker Recognition: Protection of Biometric\n  Templates and Vendor Model Parameters", "abstract": "Data privacy is crucial when dealing with biometric data. Accounting for the\nlatest European data privacy regulation and payment service directive,\nbiometric template protection is essential for any commercial application.\nEnsuring unlinkability across biometric service operators, irreversibility of\nleaked encrypted templates, and renewability of e.g., voice models following\nthe i-vector paradigm, biometric voice-based systems are prepared for the\nlatest EU data privacy legislation. Employing Paillier cryptosystems, Euclidean\nand cosine comparators are known to ensure data privacy demands, without loss\nof discrimination nor calibration performance. Bridging gaps from template\nprotection to speaker recognition, two architectures are proposed for the\ntwo-covariance comparator, serving as a generative model in this study. The\nfirst architecture preserves privacy of biometric data capture subjects. In the\nsecond architecture, model parameters of the comparator are encrypted as well,\nsuch that biometric service providers can supply the same comparison modules\nemploying different key pairs to multiple biometric service operators. An\nexperimental proof-of-concept and complexity analysis is carried out on the\ndata from the 2013-2014 NIST i-vector machine learning challenge.", "published": "2018-03-09 15:25:32", "link": "http://arxiv.org/abs/1803.03559v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
