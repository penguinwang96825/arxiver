{"title": "Shift-Reduce Constituent Parsing with Neural Lookahead Features", "abstract": "Transition-based models can be fast and accurate for constituent parsing.\nCompared with chart-based models, they leverage richer features by extracting\nhistory information from a parser stack, which spans over non-local\nconstituents. On the other hand, during incremental parsing, constituent\ninformation on the right hand side of the current word is not utilized, which\nis a relative weakness of shift-reduce parsing. To address this limitation, we\nleverage a fast neural model to extract lookahead features. In particular, we\nbuild a bidirectional LSTM model, which leverages the full sentence information\nto predict the hierarchy of constituents that each word starts and ends. The\nresults are then passed to a strong transition-based constituent parser as\nlookahead features. The resulting parser gives 1.3% absolute improvement in WSJ\nand 2.3% in CTB compared to the baseline, given the highest reported accuracies\nfor fully-supervised parsing.", "published": "2016-12-02 04:55:24", "link": "http://arxiv.org/abs/1612.00567v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Alleviating Overfitting for Polysemous Words for Word Representation\n  Estimation Using Lexicons", "abstract": "Though there are some works on improving distributed word representations\nusing lexicons, the improper overfitting of the words that have multiple\nmeanings is a remaining issue deteriorating the learning when lexicons are\nused, which needs to be solved. An alternative method is to allocate a vector\nper sense instead of a vector per word. However, the word representations\nestimated in the former way are not as easy to use as the latter one. Our\nprevious work uses a probabilistic method to alleviate the overfitting, but it\nis not robust with a small corpus. In this paper, we propose a new neural\nnetwork to estimate distributed word representations using a lexicon and a\ncorpus. We add a lexicon layer in the continuous bag-of-words model and a\nthreshold node after the output of the lexicon layer. The threshold rejects the\nunreliable outputs of the lexicon layer that are less likely to be the same\nwith their inputs. In this way, it alleviates the overfitting of the polysemous\nwords. The proposed neural network can be trained using negative sampling,\nwhich maximizing the log probabilities of target words given the context words,\nby distinguishing the target words from random noises. We compare the proposed\nneural network with the continuous bag-of-words model, the other works\nimproving it, and the previous works estimating distributed word\nrepresentations using both a lexicon and a corpus. The experimental results\nshow that the proposed neural network is more efficient and balanced for both\nsemantic tasks and syntactic tasks than the previous works, and robust to the\nsize of the corpus.", "published": "2016-12-02 07:45:40", "link": "http://arxiv.org/abs/1612.00584v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated assessment of non-native learner essays: Investigating the\n  role of linguistic features", "abstract": "Automatic essay scoring (AES) refers to the process of scoring free text\nresponses to given prompts, considering human grader scores as the gold\nstandard. Writing such essays is an essential component of many language and\naptitude exams. Hence, AES became an active and established area of research,\nand there are many proprietary systems used in real life applications today.\nHowever, not much is known about which specific linguistic features are useful\nfor prediction and how much of this is consistent across datasets. This article\naddresses that by exploring the role of various linguistic features in\nautomatic essay scoring using two publicly available datasets of non-native\nEnglish essays written in test taking scenarios. The linguistic properties are\nmodeled by encoding lexical, syntactic, discourse and error types of learner\nlanguage in the feature set. Predictive models are then developed using these\nfeatures on both datasets and the most predictive features are compared. While\nthe results show that the feature set used results in good predictive models\nwith both datasets, the question \"what are the most predictive features?\" has a\ndifferent answer for each dataset.", "published": "2016-12-02 16:22:49", "link": "http://arxiv.org/abs/1612.00729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating a Real-Time, Reproducible Event Dataset", "abstract": "The generation of political event data has remained much the same since the\nmid-1990s, both in terms of data acquisition and the process of coding text\ninto data. Since the 1990s, however, there have been significant improvements\nin open-source natural language processing software and in the availability of\ndigitized news content. This paper presents a new, next-generation event\ndataset, named Phoenix, that builds from these and other advances. This dataset\nincludes improvements in the underlying news collection process and event\ncoding software, along with the creation of a general processing pipeline\nnecessary to produce daily-updated data. This paper provides a face validity\nchecks by briefly examining the data for the conflict in Syria, and a\ncomparison between Phoenix and the Integrated Crisis Early Warning System data.", "published": "2016-12-02 21:28:00", "link": "http://arxiv.org/abs/1612.00866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in\n  Visual Question Answering", "abstract": "Problems at the intersection of vision and language are of significant\nimportance both as challenging research questions and for the rich set of\napplications they enable. However, inherent structure in our world and bias in\nour language tend to be a simpler signal for learning than visual modalities,\nresulting in models that ignore visual information, leading to an inflated\nsense of their capability.\n  We propose to counter these language priors for the task of Visual Question\nAnswering (VQA) and make vision (the V in VQA) matter! Specifically, we balance\nthe popular VQA dataset by collecting complementary images such that every\nquestion in our balanced dataset is associated with not just a single image,\nbut rather a pair of similar images that result in two different answers to the\nquestion. Our dataset is by construction more balanced than the original VQA\ndataset and has approximately twice the number of image-question pairs. Our\ncomplete balanced dataset is publicly available at www.visualqa.org as part of\nthe 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA\nv2.0).\n  We further benchmark a number of state-of-art VQA models on our balanced\ndataset. All models perform significantly worse on our balanced dataset,\nsuggesting that these models have indeed learned to exploit language priors.\nThis finding provides the first concrete empirical evidence for what seems to\nbe a qualitative sense among practitioners.\n  Finally, our data collection protocol for identifying complementary images\nenables us to develop a novel interpretable model, which in addition to\nproviding an answer to the given (image, question) pair, also provides a\ncounter-example based explanation. Specifically, it identifies an image that is\nsimilar to the original image, but it believes has a different answer to the\nsame question. This can help in building trust for machines among their users.", "published": "2016-12-02 20:57:07", "link": "http://arxiv.org/abs/1612.00837v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
