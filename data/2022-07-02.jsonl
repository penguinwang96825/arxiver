{"title": "Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk", "abstract": "Language is the principal tool for human communication, in which humor is one\nof the most attractive parts. Producing natural language like humans using\ncomputers, a.k.a, Natural Language Generation (NLG), has been widely used for\ndialogue systems, chatbots, machine translation, as well as computer-aid\ncreation e.g., idea generations, scriptwriting. However, the humor aspect of\nnatural language is relatively under-investigated, especially in the age of\npre-trained language models. In this work, we aim to preliminarily test whether\nNLG can generate humor as humans do. We build a new dataset consisting of\nnumerous digitized Chinese Comical Crosstalk scripts (called C$^3$ in short),\nwhich is for a popular Chinese performing art called `Xiangsheng' since 1800s.\n(For convenience for non-Chinese speakers, we called `crosstalk' for\n`Xiangsheng' in this paper.) We benchmark various generation approaches\nincluding training-from-scratch Seq2seq, fine-tuned middle-scale PLMs, and\nlarge-scale PLMs (with and without fine-tuning). Moreover, we also conduct a\nhuman assessment, showing that 1) large-scale pretraining largely improves\ncrosstalk generation quality; and 2) even the scripts generated from the best\nPLM is far from what we expect, with only 65% quality of human-created\ncrosstalk. We conclude, humor generation could be largely improved using\nlarge-scaled PLMs, but it is still in its infancy.\n  The data and benchmarking code is publicly available in\n\\url{https://github.com/anonNo2/crosstalk-generation}.", "published": "2022-07-02 04:30:07", "link": "http://arxiv.org/abs/2207.00735v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INSCIT: Information-Seeking Conversations with Mixed-Initiative\n  Interactions", "abstract": "In an information-seeking conversation, a user may ask questions that are\nunder-specified or unanswerable. An ideal agent would interact by initiating\ndifferent response types according to the available knowledge sources. However,\nmost current studies either fail to or artificially incorporate such agent-side\ninitiative. This work presents InSCIt, a dataset for Information-Seeking\nConversations with mixed-initiative Interactions. It contains 4.7K user-agent\nturns from 805 human-human conversations where the agent searches over\nWikipedia and either directly answers, asks for clarification, or provides\nrelevant information to address user queries. The data supports two subtasks,\nevidence passage identification and response generation, as well as a human\nevaluation protocol to assess model performance. We report results of two\nsystems based on state-of-the-art models of conversational knowledge\nidentification and open-domain question answering. Both systems significantly\nunderperform humans, suggesting ample room for improvement in future studies.", "published": "2022-07-02 06:18:12", "link": "http://arxiv.org/abs/2207.00746v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rationale-Augmented Ensembles in Language Models", "abstract": "Recent research has shown that rationales, or step-by-step chains of thought,\ncan be used to improve performance in multi-step reasoning tasks. We reconsider\nrationale-augmented prompting for few-shot in-context learning, where (input ->\noutput) prompts are expanded to (input, rationale -> output) prompts. For\nrationale-augmented prompting we demonstrate how existing approaches, which\nrely on manual prompt engineering, are subject to sub-optimal rationales that\nmay harm performance. To mitigate this brittleness, we propose a unified\nframework of rationale-augmented ensembles, where we identify rationale\nsampling in the output space as the key component to robustly improve\nperformance. This framework is general and can easily be extended to common\nnatural language processing tasks, even those that do not traditionally\nleverage intermediate steps, such as question answering, word sense\ndisambiguation, and sentiment analysis. We demonstrate that rationale-augmented\nensembles achieve more accurate and interpretable results than existing\nprompting approaches--including standard prompting without rationales and\nrationale-based chain-of-thought prompting--while simultaneously improving\ninterpretability of model predictions through the associated rationales.", "published": "2022-07-02 06:20:57", "link": "http://arxiv.org/abs/2207.00747v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence-aware multimodal page classification of Brazilian legal\n  documents", "abstract": "The Brazilian Supreme Court receives tens of thousands of cases each\nsemester. Court employees spend thousands of hours to execute the initial\nanalysis and classification of those cases -- which takes effort away from\nposterior, more complex stages of the case management workflow. In this paper,\nwe explore multimodal classification of documents from Brazil's Supreme Court.\nWe train and evaluate our methods on a novel multimodal dataset of 6,510\nlawsuits (339,478 pages) with manual annotation assigning each page to one of\nsix classes. Each lawsuit is an ordered sequence of pages, which are stored\nboth as an image and as a corresponding text extracted through optical\ncharacter recognition. We first train two unimodal classifiers: a ResNet\npre-trained on ImageNet is fine-tuned on the images, and a convolutional\nnetwork with filters of multiple kernel sizes is trained from scratch on\ndocument texts. We use them as extractors of visual and textual features, which\nare then combined through our proposed Fusion Module. Our Fusion Module can\nhandle missing textual or visual input by using learned embeddings for missing\ndata. Moreover, we experiment with bi-directional Long Short-Term Memory\n(biLSTM) networks and linear-chain conditional random fields to model the\nsequential nature of the pages. The multimodal approaches outperform both\ntextual and visual classifiers, especially when leveraging the sequential\nnature of the pages.", "published": "2022-07-02 06:23:25", "link": "http://arxiv.org/abs/2207.00748v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An End-to-End Set Transformer for User-Level Classification of\n  Depression and Gambling Disorder", "abstract": "This work proposes a transformer architecture for user-level classification\nof gambling addiction and depression that is trainable end-to-end. As opposed\nto other methods that operate at the post level, we process a set of social\nmedia posts from a particular individual, to make use of the interactions\nbetween posts and eliminate label noise at the post level. We exploit the fact\nthat, by not injecting positional encodings, multi-head attention is\npermutation invariant and we process randomly sampled sets of texts from a user\nafter being encoded with a modern pretrained sentence encoder (RoBERTa /\nMiniLM). Moreover, our architecture is interpretable with modern feature\nattribution methods and allows for automatic dataset creation by identifying\ndiscriminating posts in a user's text-set. We perform ablation studies on\nhyper-parameters and evaluate our method for the eRisk 2022 Lab on early\ndetection of signs of pathological gambling and early risk detection of\ndepression. The method proposed by our team BLUE obtained the best ERDE5 score\nof 0.015, and the second-best ERDE50 score of 0.009 for pathological gambling\ndetection. For the early detection of depression, we obtained the second-best\nERDE50 of 0.027.", "published": "2022-07-02 06:40:56", "link": "http://arxiv.org/abs/2207.00753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question\n  Answering for 16 Diverse Languages", "abstract": "We present the results of the Workshop on Multilingual Information Access\n(MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question\nanswering (QA) systems in 16 typologically diverse languages. In this task, we\nadapted two large-scale cross-lingual open-retrieval QA datasets in 14\ntypologically diverse languages, and newly annotated open-retrieval QA data in\n2 underrepresented languages: Tagalog and Tamil. Four teams submitted their\nsystems. The best system leveraging iteratively mined diverse negative examples\nand larger pretrained models achieves 32.2 F1, outperforming our baseline by\n4.5 points. The second best system uses entity-aware contextualized\nrepresentations for document retrieval, and achieves significant improvements\nin Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores.", "published": "2022-07-02 06:54:10", "link": "http://arxiv.org/abs/2207.00758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language statistics at different spatial, temporal, and grammatical\n  scales", "abstract": "Statistical linguistics has advanced considerably in recent decades as data\nhas become available. This has allowed researchers to study how statistical\nproperties of languages change over time. In this work, we use data from\nTwitter to explore English and Spanish considering the rank diversity at\ndifferent scales: temporal (from 3 to 96 hour intervals), spatial (from 3km to\n3000+km radii), and grammatical (from monograms to pentagrams). We find that\nall three scales are relevant. However, the greatest changes come from\nvariations in the grammatical scale. At the lowest grammatical scale\n(monograms), the rank diversity curves are most similar, independently on the\nvalues of other scales, languages, and countries. As the grammatical scale\ngrows, the rank diversity curves vary more depending on the temporal and\nspatial scales, as well as on the language and country. We also study the\nstatistics of Twitter-specific tokens: emojis, hashtags, and user mentions.\nThese particular type of tokens show a sigmoid kind of behaviour as a rank\ndiversity function. Our results are helpful to quantify aspects of language\nstatistics that seem universal and what may lead to variations.", "published": "2022-07-02 01:38:48", "link": "http://arxiv.org/abs/2207.00709v2", "categories": ["cs.CL", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers", "abstract": "Despite their omnipresence in modern NLP, characterizing the computational\npower of transformer neural nets remains an interesting open question. We prove\nthat transformers whose arithmetic precision is logarithmic in the number of\ninput tokens (and whose feedforward nets are computable using space linear in\ntheir input) can be simulated by constant-depth logspace-uniform threshold\ncircuits. This provides insight on the power of transformers using known\nresults in complexity theory. For example, if $\\mathsf L \\neq \\mathsf P$ (i.e.,\nnot all poly-time problems can be solved using logarithmic space), then\ntransformers cannot even accurately solve linear equalities or check membership\nin an arbitrary context-free grammar with empty productions. Our result\nintuitively emerges from the transformer architecture's high parallelizability.\nWe thus speculatively introduce the idea of a fundamental parallelism tradeoff:\nany model architecture as parallelizable as the transformer will obey\nlimitations similar to it. Since parallelism is key to training models at\nmassive scale, this suggests a potential inherent weakness of the scaling\nparadigm.", "published": "2022-07-02 03:49:34", "link": "http://arxiv.org/abs/2207.00729v4", "categories": ["cs.CC", "cs.CL"], "primary_category": "cs.CC"}
{"title": "A Multi-Task BERT Model for Schema-Guided Dialogue State Tracking", "abstract": "Task-oriented dialogue systems often employ a Dialogue State Tracker (DST) to\nsuccessfully complete conversations. Recent state-of-the-art DST\nimplementations rely on schemata of diverse services to improve model\nrobustness and handle zero-shot generalization to new domains [1], however such\nmethods [2, 3] typically require multiple large scale transformer models and\nlong input sequences to perform well. We propose a single multi-task BERT-based\nmodel that jointly solves the three DST tasks of intent prediction, requested\nslot prediction and slot filling. Moreover, we propose an efficient and\nparsimonious encoding of the dialogue history and service schemata that is\nshown to further improve performance. Evaluation on the SGD dataset shows that\nour approach outperforms the baseline SGP-DST by a large margin and performs\nwell compared to the state-of-the-art, while being significantly more\ncomputationally efficient. Extensive ablation studies are performed to examine\nthe contributing factors to the success of our model.", "published": "2022-07-02 13:27:59", "link": "http://arxiv.org/abs/2207.00828v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Biomedical Pipeline to Detect Clinical and Non-Clinical Named Entities", "abstract": "There are a few challenges related to the task of biomedical named entity\nrecognition, which are: the existing methods consider a fewer number of\nbiomedical entities (e.g., disease, symptom, proteins, genes); and these\nmethods do not consider the social determinants of health (age, gender,\nemployment, race), which are the non-medical factors related to patients'\nhealth. We propose a machine learning pipeline that improves on previous\nefforts in the following ways: first, it recognizes many biomedical entity\ntypes other than the standard ones; second, it considers non-clinical factors\nrelated to patient's health. This pipeline also consists of stages, such as\npreprocessing, tokenization, mapping embedding lookup and named entity\nrecognition task to extract biomedical named entities from the free texts. We\npresent a new dataset that we prepare by curating the COVID-19 case reports.\nThe proposed approach outperforms the baseline methods on five benchmark\ndatasets with macro-and micro-average F1 scores around 90, as well as our\ndataset with a macro-and micro-average F1 score of 95.25 and 93.18\nrespectively.", "published": "2022-07-02 16:30:36", "link": "http://arxiv.org/abs/2207.00876v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "UserLibri: A Dataset for ASR Personalization Using Only Text", "abstract": "Personalization of speech models on mobile devices (on-device\npersonalization) is an active area of research, but more often than not, mobile\ndevices have more text-only data than paired audio-text data. We explore\ntraining a personalized language model on text-only data, used during inference\nto improve speech recognition performance for that user. We experiment on a\nuser-clustered LibriSpeech corpus, supplemented with personalized text-only\ndata for each user from Project Gutenberg. We release this User-Specific\nLibriSpeech (UserLibri) dataset to aid future personalization research.\nLibriSpeech audio-transcript pairs are grouped into 55 users from the\ntest-clean dataset and 52 users from test-other. We are able to lower the\naverage word error rate per user across both sets in streaming and nonstreaming\nmodels, including an improvement of 2.5 for the harder set of test-other users\nwhen streaming.", "published": "2022-07-02 01:03:01", "link": "http://arxiv.org/abs/2207.00706v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "FRAME: Evaluating Rationale-Label Consistency Metrics for Free-Text\n  Rationales", "abstract": "Following how humans communicate, free-text rationales aim to use natural\nlanguage to explain neural language model (LM) behavior. However, free-text\nrationales' unconstrained nature makes them prone to hallucination, so it is\nimportant to have metrics for free-text rationale quality. Existing free-text\nrationale metrics measure how consistent the rationale is with the LM's\npredicted label, but there is no protocol for assessing such metrics'\nreliability. Thus, we propose FRAME, a framework for evaluating rationale-label\nconsistency (RLC) metrics for free-text rationales. FRAME is based on three\naxioms: (1) good metrics should yield highest scores for reference rationales,\nwhich maximize RLC by construction; (2) good metrics should be appropriately\nsensitive to semantic perturbation of rationales; and (3) good metrics should\nbe robust to variation in the LM's task performance. Across three text\nclassification datasets, we show that existing RLC metrics cannot satisfy all\nthree FRAME axioms, since they are implemented via model pretraining which\nmuddles the metric's signal. Then, we introduce a non-pretraining RLC metric\nthat greatly outperforms baselines on (1) and (3), while performing\ncompetitively on (2). Finally, we discuss the limitations of using RLC to\nevaluate free-text rationales.", "published": "2022-07-02 09:25:29", "link": "http://arxiv.org/abs/2207.00779v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ANEC: An Amharic Named Entity Corpus and Transformer Based Recognizer", "abstract": "Named Entity Recognition is an information extraction task that serves as a\npreprocessing step for other natural language processing tasks, such as machine\ntranslation, information retrieval, and question answering. Named entity\nrecognition enables the identification of proper names as well as temporal and\nnumeric expressions in an open domain text. For Semitic languages such as\nArabic, Amharic, and Hebrew, the named entity recognition task is more\nchallenging due to the heavily inflected structure of these languages. In this\npaper, we present an Amharic named entity recognition system based on\nbidirectional long short-term memory with a conditional random fields layer. We\nannotate a new Amharic named entity recognition dataset (8,070 sentences, which\nhas 182,691 tokens) and apply Synthetic Minority Over-sampling Technique to our\ndataset to mitigate the imbalanced classification problem. Our named entity\nrecognition system achieves an F_1 score of 93%, which is the new\nstate-of-the-art result for Amharic named entity recognition.", "published": "2022-07-02 09:50:37", "link": "http://arxiv.org/abs/2207.00785v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tree-constrained Pointer Generator with Graph Neural Network Encodings\n  for Contextual Speech Recognition", "abstract": "Incorporating biasing words obtained as contextual knowledge is critical for\nmany automatic speech recognition (ASR) applications. This paper proposes the\nuse of graph neural network (GNN) encodings in a tree-constrained pointer\ngenerator (TCPGen) component for end-to-end contextual ASR. By encoding the\nbiasing words in the prefix-tree with a tree-based GNN, lookahead for future\nwordpieces in end-to-end ASR decoding is achieved at each tree node by\nincorporating information about all wordpieces on the tree branches rooted from\nit, which allows a more accurate prediction of the generation probability of\nthe biasing words. Systems were evaluated on the Librispeech corpus using\nsimulated biasing tasks, and on the AMI corpus by proposing a novel\nvisual-grounded contextual ASR pipeline that extracts biasing words from slides\nalongside each meeting. Results showed that TCPGen with GNN encodings achieved\nabout a further 15% relative WER reduction on the biasing words compared to the\noriginal TCPGen, with a negligible increase in the computation cost for\ndecoding.", "published": "2022-07-02 15:12:18", "link": "http://arxiv.org/abs/2207.00857v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Linguistic Blind Spot of Value-Aligned Agency, Natural and\n  Artificial", "abstract": "The value-alignment problem for artificial intelligence (AI) asks how we can\nensure that the 'values' (i.e., objective functions) of artificial systems are\naligned with the values of humanity. In this paper, I argue that linguistic\ncommunication (natural language) is a necessary condition for robust value\nalignment. I discuss the consequences that the truth of this claim would have\nfor research programmes that attempt to ensure value alignment for AI systems;\nor, more loftily, designing robustly beneficial or ethical artificial agents.", "published": "2022-07-02 15:47:33", "link": "http://arxiv.org/abs/2207.00868v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Improving Transformer-based Conversational ASR by Inter-Sentential\n  Attention Mechanism", "abstract": "Transformer-based models have demonstrated their effectiveness in automatic\nspeech recognition (ASR) tasks and even shown superior performance over the\nconventional hybrid framework. The main idea of Transformers is to capture the\nlong-range global context within an utterance by self-attention layers.\nHowever, for scenarios like conversational speech, such utterance-level\nmodeling will neglect contextual dependencies that span across utterances. In\nthis paper, we propose to explicitly model the inter-sentential information in\na Transformer based end-to-end architecture for conversational speech\nrecognition. Specifically, for the encoder network, we capture the contexts of\nprevious speech and incorporate such historic information into current input by\na context-aware residual attention mechanism. For the decoder, the prediction\nof current utterance is also conditioned on the historic linguistic information\nthrough a conditional decoder framework. We show the effectiveness of our\nproposed method on several open-source dialogue corpora and the proposed method\nconsistently improved the performance from the utterance-level\nTransformer-based ASR models.", "published": "2022-07-02 17:17:47", "link": "http://arxiv.org/abs/2207.00883v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech Emotion: Investigating Model Representations, Multi-Task Learning\n  and Knowledge Distillation", "abstract": "Estimating dimensional emotions, such as activation, valence and dominance,\nfrom acoustic speech signals has been widely explored over the past few years.\nWhile accurate estimation of activation and dominance from speech seem to be\npossible, the same for valence remains challenging. Previous research has shown\nthat the use of lexical information can improve valence estimation performance.\nLexical information can be obtained from pre-trained acoustic models, where the\nlearned representations can improve valence estimation from speech. We\ninvestigate the use of pre-trained model representations to improve valence\nestimation from acoustic speech signal. We also explore fusion of\nrepresentations to improve emotion estimation across all three emotion\ndimensions: activation, valence and dominance. Additionally, we investigate if\nrepresentations from pre-trained models can be distilled into models trained\nwith low-level features, resulting in models with a less number of parameters.\nWe show that fusion of pre-trained model embeddings result in a 79% relative\nimprovement in concordance correlation coefficient CCC on valence estimation\ncompared to standard acoustic feature baseline (mel-filterbank energies), while\ndistillation from pre-trained model embeddings to lower-dimensional\nrepresentations yielded a relative 12% improvement. Such performance gains were\nobserved over two evaluation sets, indicating that our proposed architecture\ngeneralizes across those evaluation sets. We report new state-of-the-art\n\"text-free\" acoustic-only dimensional emotion estimation $CCC$ values on two\nMSP-Podcast evaluation sets.", "published": "2022-07-02 17:34:44", "link": "http://arxiv.org/abs/2207.03334v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning Noise-independent Speech Representation for High-quality Voice\n  Conversion for Noisy Target Speakers", "abstract": "Building a voice conversion system for noisy target speakers, such as users\nproviding noisy samples or Internet found data, is a challenging task since the\nuse of contaminated speech in model training will apparently degrade the\nconversion performance. In this paper, we leverage the advances of our recently\nproposed Glow-WaveGAN and propose a noise-independent speech representation\nlearning approach for high-quality voice conversion for noisy target speakers.\nSpecifically, we learn a latent feature space where we ensure that the target\ndistribution modeled by the conversion model is exactly from the modeled\ndistribution of the waveform generator. With this premise, we further manage to\nmake the latent feature to be noise-invariant. Specifically, we introduce a\nnoise-controllable WaveGAN, which directly learns the noise-independent\nacoustic representation from waveform by the encoder and conducts noise control\nin the hidden space through a FiLM module in the decoder. As for the conversion\nmodel, importantly, we use a flow-based model to learn the distribution of\nnoise-independent but speaker-related latent features from phoneme\nposteriorgrams. Experimental results demonstrate that the proposed model\nachieves high speech quality and speaker similarity in the voice conversion for\nnoisy target speakers.", "published": "2022-07-02 06:51:12", "link": "http://arxiv.org/abs/2207.00756v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Computer-assisted Pronunciation Training -- Speech synthesis is almost\n  all you need", "abstract": "The research community has long studied computer-assisted pronunciation\ntraining (CAPT) methods in non-native speech. Researchers focused on studying\nvarious model architectures, such as Bayesian networks and deep learning\nmethods, as well as on the analysis of different representations of the speech\nsignal. Despite significant progress in recent years, existing CAPT methods are\nnot able to detect pronunciation errors with high accuracy (only 60\\% precision\nat 40\\%-80\\% recall). One of the key problems is the low availability of\nmispronounced speech that is needed for the reliable training of pronunciation\nerror detection models. If we had a generative model that could mimic\nnon-native speech and produce any amount of training data, then the task of\ndetecting pronunciation errors would be much easier. We present three\ninnovative techniques based on phoneme-to-phoneme (P2P), text-to-speech (T2S),\nand speech-to-speech (S2S) conversion to generate correctly pronounced and\nmispronounced synthetic speech. We show that these techniques not only improve\nthe accuracy of three machine learning models for detecting pronunciation\nerrors but also help establish a new state-of-the-art in the field. Earlier\nstudies have used simple speech generation techniques such as P2P conversion,\nbut only as an additional mechanism to improve the accuracy of pronunciation\nerror detection. We, on the other hand, consider speech generation to be the\nfirst-class method of detecting pronunciation errors. The effectiveness of\nthese techniques is assessed in the tasks of detecting pronunciation and\nlexical stress errors. Non-native English speech corpora of German, Italian,\nand Polish speakers are used in the evaluations. The best proposed S2S\ntechnique improves the accuracy of detecting pronunciation errors in AUC metric\nby 41\\% from 0.528 to 0.749 compared to the state-of-the-art approach.", "published": "2022-07-02 08:33:33", "link": "http://arxiv.org/abs/2207.00774v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Unsupervised Symbolic Music Segmentation using Ensemble Temporal\n  Prediction Errors", "abstract": "Symbolic music segmentation is the process of dividing symbolic melodies into\nsmaller meaningful groups, such as melodic phrases. We proposed an unsupervised\nmethod for segmenting symbolic music. The proposed model is based on an\nensemble of temporal prediction error models. During training, each model\npredicts the next token to identify musical phrase changes. While at test time,\nwe perform a peak detection algorithm to select segment candidates. Finally, we\naggregate the predictions of each of the models participating in the ensemble\nto predict the final segmentation. Results suggest the proposed method reaches\nstate-of-the-art performance on the Essen Folksong dataset under the\nunsupervised setting when considering F-Score and R-value. We additionally\nprovide an ablation study to better assess the contribution of each of the\nmodel components to the final results. As expected, the proposed method is\ninferior to the supervised setting, which leaves room for improvement in future\nresearch considering closing the gap between unsupervised and supervised\nmethods.", "published": "2022-07-02 07:13:54", "link": "http://arxiv.org/abs/2207.00760v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
