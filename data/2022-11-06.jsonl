{"title": "Prompt-based Text Entailment for Low-Resource Named Entity Recognition", "abstract": "Pre-trained Language Models (PLMs) have been applied in NLP tasks and achieve\npromising results. Nevertheless, the fine-tuning procedure needs labeled data\nof the target domain, making it difficult to learn in low-resource and\nnon-trivial labeled scenarios. To address these challenges, we propose\nPrompt-based Text Entailment (PTE) for low-resource named entity recognition,\nwhich better leverages knowledge in the PLMs. We first reformulate named entity\nrecognition as the text entailment task. The original sentence with entity\ntype-specific prompts is fed into PLMs to get entailment scores for each\ncandidate. The entity type with the top score is then selected as final label.\nThen, we inject tagging labels into prompts and treat words as basic units\ninstead of n-gram spans to reduce time complexity in generating candidates by\nn-grams enumeration. Experimental results demonstrate that the proposed method\nPTE achieves competitive performance on the CoNLL03 dataset, and better than\nfine-tuned counterparts on the MIT Movie and Few-NERD dataset in low-resource\nsettings.", "published": "2022-11-06 06:13:38", "link": "http://arxiv.org/abs/2211.03039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Suffix Retrieval-Augmented Language Modeling", "abstract": "Causal language modeling (LM) uses word history to predict the next word.\nBERT, on the other hand, makes use of bi-directional word information in a\nsentence to predict words at masked positions. While BERT is effective in\nsequence encoding, it is non-causal by nature and is not designed for sequence\ngeneration. In this paper, we propose a novel language model, SUffix\nREtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual\neffect in an autoregressive manner. SUREALM employs an embedding retriever to\nsearch for training sentences in a data store that share similar word history\nduring sequence generation. In particular, the suffix portions of the retrieved\nsentences mimick the \"future\" context. We evaluated our proposed model on the\nDSTC9 spoken dialogue corpus and showed promising word perplexity reduction on\nthe validation and test set compared to competitive baselines.", "published": "2022-11-06 07:53:19", "link": "http://arxiv.org/abs/2211.03053v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Domain Adaptation and Generalization of Pretrained Language\n  Models: A Survey", "abstract": "Recent advances in NLP are brought by a range of large-scale pretrained\nlanguage models (PLMs). These PLMs have brought significant performance gains\nfor a range of NLP tasks, circumventing the need to customize complex designs\nfor specific tasks. However, most current work focus on finetuning PLMs on a\ndomain-specific datasets, ignoring the fact that the domain gap can lead to\noverfitting and even performance drop. Therefore, it is practically important\nto find an appropriate method to effectively adapt PLMs to a target domain of\ninterest. Recently, a range of methods have been proposed to achieve this\npurpose. Early surveys on domain adaptation are not suitable for PLMs due to\nthe sophisticated behavior exhibited by PLMs from traditional models trained\nfrom scratch and that domain adaptation of PLMs need to be redesigned to take\neffect. This paper aims to provide a survey on these newly proposed methods and\nshed light in how to apply traditional machine learning methods to newly\nevolved and future technologies. By examining the issues of deploying PLMs for\ndownstream tasks, we propose a taxonomy of domain adaptation approaches from a\nmachine learning system view, covering methods for input augmentation, model\noptimization and personalization. We discuss and compare those methods and\nsuggest promising future research directions.", "published": "2022-11-06 15:32:00", "link": "http://arxiv.org/abs/2211.03154v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computing and Exploiting Document Structure to Improve Unsupervised\n  Extractive Summarization of Legal Case Decisions", "abstract": "Though many algorithms can be used to automatically summarize legal case\ndecisions, most fail to incorporate domain knowledge about how important\nsentences in a legal decision relate to a representation of its document\nstructure. For example, analysis of a legal case summarization dataset\ndemonstrates that sentences serving different types of argumentative roles in\nthe decision appear in different sections of the document. In this work, we\npropose an unsupervised graph-based ranking model that uses a reweighting\nalgorithm to exploit properties of the document structure of legal case\ndecisions. We also explore the impact of using different methods to compute the\ndocument structure. Results on the Canadian Legal Case Law dataset show that\nour proposed method outperforms several strong baselines.", "published": "2022-11-06 22:20:42", "link": "http://arxiv.org/abs/2211.03229v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parallel Attention Forcing for Machine Translation", "abstract": "Attention-based autoregressive models have achieved state-of-the-art\nperformance in various sequence-to-sequence tasks, including Text-To-Speech\n(TTS) and Neural Machine Translation (NMT), but can be difficult to train. The\nstandard training approach, teacher forcing, guides a model with the reference\nback-history. During inference, the generated back-history must be used. This\nmismatch limits the evaluation performance. Attention forcing has been\nintroduced to address the mismatch, guiding the model with the generated\nback-history and reference attention. While successful in tasks with continuous\noutputs like TTS, attention forcing faces additional challenges in tasks with\ndiscrete outputs like NMT. This paper introduces the two extensions of\nattention forcing to tackle these challenges. (1) Scheduled attention forcing\nautomatically turns attention forcing on and off, which is essential for tasks\nwith discrete outputs. (2) Parallel attention forcing makes training parallel,\nand is applicable to Transformer-based models. The experiments show that the\nproposed approaches improve the performance of models based on RNNs and\nTransformers.", "published": "2022-11-06 23:29:07", "link": "http://arxiv.org/abs/2211.03237v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Lottery Tickets for Pre-trained Language Models", "abstract": "Recent works on Lottery Ticket Hypothesis have shown that pre-trained\nlanguage models (PLMs) contain smaller matching subnetworks(winning tickets)\nwhich are capable of reaching accuracy comparable to the original models.\nHowever, these tickets are proved to be notrobust to adversarial examples, and\neven worse than their PLM counterparts. To address this problem, we propose a\nnovel method based on learning binary weight masks to identify robust tickets\nhidden in the original PLMs. Since the loss is not differentiable for the\nbinary mask, we assign the hard concrete distribution to the masks and\nencourage their sparsity using a smoothing approximation of L0\nregularization.Furthermore, we design an adversarial loss objective to guide\nthe search for robust tickets and ensure that the tickets perform well bothin\naccuracy and robustness. Experimental results show the significant improvement\nof the proposed method over previous work on adversarial robustness evaluation.", "published": "2022-11-06 02:59:27", "link": "http://arxiv.org/abs/2211.03013v1", "categories": ["cs.CL", "cs.AI", "68-06"], "primary_category": "cs.CL"}
{"title": "Calibration Meets Explanation: A Simple and Effective Approach for Model\n  Confidence Estimates", "abstract": "Calibration strengthens the trustworthiness of black-box models by producing\nbetter accurate confidence estimates on given examples. However, little is\nknown about if model explanations can help confidence calibration. Intuitively,\nhumans look at important features attributions and decide whether the model is\ntrustworthy. Similarly, the explanations can tell us when the model may or may\nnot know. Inspired by this, we propose a method named CME that leverages model\nexplanations to make the model less confident with non-inductive attributions.\nThe idea is that when the model is not highly confident, it is difficult to\nidentify strong indications of any class, and the tokens accordingly do not\nhave high attribution scores for any class and vice versa. We conduct extensive\nexperiments on six datasets with two popular pre-trained language models in the\nin-domain and out-of-domain settings. The results show that CME improves\ncalibration performance in all settings. The expected calibration errors are\nfurther reduced when combined with temperature scaling. Our findings highlight\nthat model explanations can help calibrate posterior estimates.", "published": "2022-11-06 06:17:21", "link": "http://arxiv.org/abs/2211.03041v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tuning Language Models as Training Data Generators for\n  Augmentation-Enhanced Few-Shot Learning", "abstract": "Recent studies have revealed the intriguing few-shot learning ability of\npretrained language models (PLMs): They can quickly adapt to a new task when\nfine-tuned on a small amount of labeled data formulated as prompts, without\nrequiring abundant task-specific annotations. Despite their promising\nperformance, most existing few-shot approaches that only learn from the small\ntraining set still underperform fully supervised training by nontrivial\nmargins. In this work, we study few-shot learning with PLMs from a different\nperspective: We first tune an autoregressive PLM on the few-shot samples and\nthen use it as a generator to synthesize a large amount of novel training\nsamples which augment the original training set. To encourage the generator to\nproduce label-discriminative samples, we train it via weighted maximum\nlikelihood where the weight of each token is automatically adjusted based on a\ndiscriminative meta-learning objective. A classification PLM can then be\nfine-tuned on both the few-shot and the synthetic samples with regularization\nfor better generalization and stability. Our approach FewGen achieves an\noverall better result across seven classification tasks of the GLUE benchmark\nthan existing few-shot learning methods, improving no-augmentation methods by\n5+ average points, and outperforming augmentation methods by 3+ average points.", "published": "2022-11-06 06:46:47", "link": "http://arxiv.org/abs/2211.03044v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge is Power: Understanding Causality Makes Legal judgment\n  Prediction Models More Generalizable and Robust", "abstract": "Legal Judgment Prediction (LJP), aiming to predict a judgment based on fact\ndescriptions according to rule of law, serves as legal assistance to mitigate\nthe great work burden of limited legal practitioners. Most existing methods\napply various large-scale pre-trained language models (PLMs) finetuned in LJP\ntasks to obtain consistent improvements. However, we discover the fact that the\nstate-of-the-art (SOTA) model makes judgment predictions according to\nirrelevant (or non-casual) information. The violation of rule of law not only\nweakens the robustness and generalization ability of models but also results in\nsevere social problems like discrimination. In this paper, we use causal\nstructural models (SCMs) to theoretically analyze how LJP models learn to make\ndecisions and why they can succeed in passing the traditional testing paradigm\nwithout learning causality. According to our analysis, we provide two solutions\nintervening on data and model by causality, respectively. In detail, we first\ndistinguish non-causal information by applying the open information extraction\n(OIE) technique. Then, we propose a method named the Causal Information\nEnhanced SAmpling Method (CIESAM) to eliminate the non-causal information from\ndata. To validate our theoretical analysis, we further propose another method\nusing our proposed Causality-Aware Self-Attention Mechanism (CASAM) to guide\nthe model to learn the underlying causality knowledge in legal texts. The\nconfidence of CASAM in learning causal information is higher than that of\nCIESAM. The extensive experimental results show that both our proposed methods\nachieve state-of-the-art (SOTA) performance on three commonly used\nlegal-specific datasets. The stronger performance of CASAM further demonstrates\nthat causality is the key to the robustness and generalization ability of\nmodels.", "published": "2022-11-06 07:03:31", "link": "http://arxiv.org/abs/2211.03046v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improved Target-specific Stance Detection on Social Media Platforms by\n  Delving into Conversation Threads", "abstract": "Target-specific stance detection on social media, which aims at classifying a\ntextual data instance such as a post or a comment into a stance class of a\ntarget issue, has become an emerging opinion mining paradigm of importance. An\nexample application would be to overcome vaccine hesitancy in combating the\ncoronavirus pandemic. However, existing stance detection strategies rely merely\non the individual instances which cannot always capture the expressed stance of\na given target. In response, we address a new task called conversational stance\ndetection which is to infer the stance towards a given target (e.g., COVID-19\nvaccination) when given a data instance and its corresponding conversation\nthread. To tackle the task, we first propose a benchmarking conversational\nstance detection (CSD) dataset with annotations of stances and the structures\nof conversation threads among the instances based on six major social media\nplatforms in Hong Kong. To infer the desired stances from both data instances\nand conversation threads, we propose a model called Branch-BERT that\nincorporates contextual information in conversation threads. Extensive\nexperiments on our CSD dataset show that our proposed model outperforms all the\nbaseline models that do not make use of contextual information. Specifically,\nit improves the F1 score by 10.3% compared with the state-of-the-art method in\nthe SemEval-2016 Task 6 competition. This shows the potential of incorporating\nrich contextual information on detecting target-specific stances on social\nmedia platforms and implies a more practical way to construct future stance\ndetection tasks.", "published": "2022-11-06 08:40:48", "link": "http://arxiv.org/abs/2211.03061v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "MAIL: Malware Analysis Intermediate Language", "abstract": "This paper introduces and presents a new language named MAIL (Malware\nAnalysis Intermediate Language). MAIL is basically used for building malware\nanalysis and detection tools. MAIL provides an abstract representation of an\nassembly program and hence the ability of a tool to automate malware analysis\nand detection. By translating binaries compiled for different platforms to\nMAIL, a tool can achieve platform independence. Each MAIL statement is\nannotated with patterns that can be used by a tool to optimize malware analysis\nand detection.", "published": "2022-11-06 09:48:06", "link": "http://arxiv.org/abs/2211.03068v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Noisy Channel for Automatic Text Simplification", "abstract": "In this paper we present a simple re-ranking method for Automatic Sentence\nSimplification based on the noisy channel scheme. Instead of directly computing\nthe best simplification given a complex text, the re-ranking method also\nconsiders the probability of the simple sentence to produce the complex\ncounterpart, as well as the probability of the simple text itself, according to\na language model. Our experiments show that combining these scores outperform\nthe original system in three different English datasets, yielding the best\nknown result in one of them. Adopting the noisy channel scheme opens new ways\nto infuse additional information into ATS systems, and thus to control\nimportant aspects of them, a known limitation of end-to-end neural seq2seq\ngenerative models.", "published": "2022-11-06 15:28:42", "link": "http://arxiv.org/abs/2211.03152v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deliberation Networks and How to Train Them", "abstract": "Deliberation networks are a family of sequence-to-sequence models, which have\nachieved state-of-the-art performance in a wide range of tasks such as machine\ntranslation and speech synthesis. A deliberation network consists of multiple\nstandard sequence-to-sequence models, each one conditioned on the initial input\nand the output of the previous model. During training, there are several key\nquestions: whether to apply Monte Carlo approximation to the gradients or the\nloss, whether to train the standard models jointly or separately, whether to\nrun an intermediate model in teacher forcing or free running mode, whether to\napply task-specific techniques. Previous work on deliberation networks\ntypically explores one or two training options for a specific task. This work\nintroduces a unifying framework, covering various training options, and\naddresses the above questions. In general, it is simpler to approximate the\ngradients. When parallel training is essential, separate training should be\nadopted. Regardless of the task, the intermediate model should be in free\nrunning mode. For tasks where the output is continuous, a guided attention loss\ncan be used to prevent degradation into a standard model.", "published": "2022-11-06 20:47:18", "link": "http://arxiv.org/abs/2211.03217v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Bridging Speech and Textual Pre-trained Models with Unsupervised ASR", "abstract": "Spoken language understanding (SLU) is a task aiming to extract high-level\nsemantics from spoken utterances. Previous works have investigated the use of\nspeech self-supervised models and textual pre-trained models, which have shown\nreasonable improvements to various SLU tasks. However, because of the\nmismatched modalities between speech signals and text tokens, previous methods\nusually need complex designs of the frameworks. This work proposes a simple yet\nefficient unsupervised paradigm that connects speech and textual pre-trained\nmodels, resulting in an unsupervised speech-to-semantic pre-trained model for\nvarious tasks in SLU. To be specific, we propose to use unsupervised automatic\nspeech recognition (ASR) as a connector that bridges different modalities used\nin speech and textual pre-trained models. Our experiments show that\nunsupervised ASR itself can improve the representations from speech\nself-supervised models. More importantly, it is shown as an efficient connector\nbetween speech and textual pre-trained models, improving the performances of\nfive different SLU tasks. Notably, on spoken question answering, we reach the\nstate-of-the-art result over the challenging NMSQA benchmark.", "published": "2022-11-06 04:50:37", "link": "http://arxiv.org/abs/2211.03025v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Preserving background sound in noise-robust voice conversion via\n  multi-task learning", "abstract": "Background sound is an informative form of art that is helpful in providing a\nmore immersive experience in real-application voice conversion (VC) scenarios.\nHowever, prior research about VC, mainly focusing on clean voices, pay rare\nattention to VC with background sound. The critical problem for preserving\nbackground sound in VC is inevitable speech distortion by the neural separation\nmodel and the cascade mismatch between the source separation model and the VC\nmodel. In this paper, we propose an end-to-end framework via multi-task\nlearning which sequentially cascades a source separation (SS) module, a\nbottleneck feature extraction module and a VC module. Specifically, the source\nseparation task explicitly considers critical phase information and confines\nthe distortion caused by the imperfect separation process. The source\nseparation task, the typical VC task and the unified task shares a uniform\nreconstruction loss constrained by joint training to reduce the mismatch\nbetween the SS and VC modules. Experimental results demonstrate that our\nproposed framework significantly outperforms the baseline systems while\nachieving comparable quality and speaker similarity to the VC models trained\nwith clean data.", "published": "2022-11-06 06:00:50", "link": "http://arxiv.org/abs/2211.03036v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Empirical Study on L2 Accents of Cross-lingual Text-to-Speech Systems\n  via Vowel Space", "abstract": "With the recent developments in cross-lingual Text-to-Speech (TTS) systems,\nL2 (second-language, or foreign) accent problems arise. Moreover, running a\nsubjective evaluation for such cross-lingual TTS systems is troublesome. The\nvowel space analysis, which is often utilized to explore various aspects of\nlanguage including L2 accents, is a great alternative analysis tool. In this\nstudy, we apply the vowel space analysis method to explore L2 accents of\ncross-lingual TTS systems. Through the vowel space analysis, we observe the\nthree followings: a) a parallel architecture (Glow-TTS) is less L2-accented\nthan an auto-regressive one (Tacotron); b) L2 accents are more dominant in\nnon-shared vowels in a language pair; and c) L2 accents of cross-lingual TTS\nsystems share some phenomena with those of human L2 learners. Our findings\nimply that it is necessary for TTS systems to handle each language pair\ndifferently, depending on their linguistic characteristics such as non-shared\nvowels. They also hint that we can further incorporate linguistics knowledge in\ndeveloping cross-lingual TTS systems.", "published": "2022-11-06 10:45:01", "link": "http://arxiv.org/abs/2211.03078v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "I Hear Your True Colors: Image Guided Audio Generation", "abstract": "We propose Im2Wav, an image guided open-domain audio generation system. Given\nan input image or a sequence of images, Im2Wav generates a semantically\nrelevant sound. Im2Wav is based on two Transformer language models, that\noperate over a hierarchical discrete audio representation obtained from a\nVQ-VAE based model. We first produce a low-level audio representation using a\nlanguage model. Then, we upsample the audio tokens using an additional language\nmodel to generate a high-fidelity audio sample. We use the rich semantics of a\npre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a\nvisual representation to condition the language model. In addition, to steer\nthe generation process towards the conditioning image, we apply the\nclassifier-free guidance method. Results suggest that Im2Wav significantly\noutperforms the evaluated baselines in both fidelity and relevance evaluation\nmetrics. Additionally, we provide an ablation study to better assess the impact\nof each of the method components on overall performance. Lastly, to better\nevaluate image-to-audio models, we propose an out-of-domain image dataset,\ndenoted as ImageHear. ImageHear can be used as a benchmark for evaluating\nfuture image-to-audio models. Samples and code can be found inside the\nmanuscript.", "published": "2022-11-06 11:48:20", "link": "http://arxiv.org/abs/2211.03089v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Distinguishable Speaker Anonymization based on Formant and Fundamental\n  Frequency Scaling", "abstract": "Speech data on the Internet are proliferating exponentially because of the\nemergence of social media, and the sharing of such personal data raises obvious\nsecurity and privacy concerns. One solution to mitigate these concerns involves\nconcealing speaker identities before sharing speech data, also referred to as\nspeaker anonymization. In our previous work, we have developed an automatic\nspeaker verification (ASV)-model-free anonymization framework to protect\nspeaker privacy while preserving speech intelligibility. Although the framework\nranked first place in VoicePrivacy 2022 challenge, the anonymization was\nimperfect, since the speaker distinguishability of the anonymized speech was\ndeteriorated. To address this issue, in this paper, we directly model the\nformant distribution and fundamental frequency (F0) to represent speaker\nidentity and anonymize the source speech by the uniformly scaling formant and\nF0. By directly scaling the formant and F0, the speaker distinguishability\ndegradation of the anonymized speech caused by the introduction of other\nspeakers is prevented. The experimental results demonstrate that our proposed\nframework can improve the speaker distinguishability and significantly\noutperforms our previous framework in voice distinctiveness. Furthermore, our\nproposed method also can trade off the privacy-utility by using different\nscaling factors.", "published": "2022-11-06 06:08:44", "link": "http://arxiv.org/abs/2211.03038v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Going In Style: Audio Backdoors Through Stylistic Transformations", "abstract": "This work explores stylistic triggers for backdoor attacks in the audio\ndomain: dynamic transformations of malicious samples through guitar effects. We\nfirst formalize stylistic triggers - currently missing in the literature.\nSecond, we explore how to develop stylistic triggers in the audio domain by\nproposing JingleBack. Our experiments confirm the effectiveness of the attack,\nachieving a 96% attack success rate. Our code is available in\nhttps://github.com/skoffas/going-in-style.", "published": "2022-11-06 13:39:45", "link": "http://arxiv.org/abs/2211.03117v3", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "\"Seeing Sound\": Audio Classification with the Wigner-Wille Distribution\n  and Convolutional Neural Networks", "abstract": "With big data becoming increasingly available, IoT hardware becoming widely\nadopted, and AI capabilities becoming more powerful, organizations are\ncontinuously investing in sensing. Data coming from sensor networks are\ncurrently combined with sensor fusion and AI algorithms to drive innovation in\nfields such as self-driving cars. Data from these sensors can be utilized in\nnumerous use cases, including alerts in safety systems of urban settings, for\nevents such as gun shots and explosions. Moreover, diverse types of sensors,\nsuch as sound sensors, can be utilized in low-light conditions or at locations\nwhere a camera is not available. This paper investigates the potential of the\nutilization of sound-sensor data in an urban context. Technically, we propose a\nnovel approach of classifying sound data using the Wigner-Ville distribution\nand Convolutional Neural Networks. In this paper, we report on the performance\nof the approach on open-source datasets. The concept and work presented is\nbased on my doctoral thesis, which was performed as part of the Engineering\nDoctorate program in Data Science at the University of Eindhoven, in\ncollaboration with the Dutch National Police. Additional work on real-world\ndatasets was performed during the thesis, which are not presented here due to\nconfidentiality.", "published": "2022-11-06 19:01:02", "link": "http://arxiv.org/abs/2211.03202v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
