{"title": "Zipf's law is a consequence of coherent language production", "abstract": "The task of text segmentation may be undertaken at many levels in text\nanalysis---paragraphs, sentences, words, or even letters. Here, we focus on a\nrelatively fine scale of segmentation, hypothesizing it to be in accord with a\nstochastic model of language generation, as the smallest scale where\nindependent units of meaning are produced. Our goals in this letter include the\ndevelopment of methods for the segmentation of these minimal independent units,\nwhich produce feature-representations of texts that align with the independence\nassumption of the bag-of-terms model, commonly used for prediction and\nclassification in computational text analysis. We also propose the measurement\nof texts' association (with respect to realized segmentations) to the model of\nlanguage generation. We find (1) that our segmentations of phrases exhibit much\nbetter associations to the generation model than words and (2), that texts\nwhich are well fit are generally topically homogeneous. Because our generative\nmodel produces Zipf's law, our study further suggests that Zipf's law may be a\nconsequence of homogeneity in language production.", "published": "2016-01-29 02:39:56", "link": "http://arxiv.org/abs/1601.07969v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lipreading with Long Short-Term Memory", "abstract": "Lipreading, i.e. speech recognition from visual-only recordings of a\nspeaker's face, can be achieved with a processing pipeline based solely on\nneural networks, yielding significantly better accuracy than conventional\nmethods. Feed-forward and recurrent neural network layers (namely Long\nShort-Term Memory; LSTM) are stacked to form a single structure which is\ntrained by back-propagating error gradients through all the layers. The\nperformance of such a stacked network was experimentally evaluated and compared\nto a standard Support Vector Machine classifier using conventional computer\nvision features (Eigenlips and Histograms of Oriented Gradients). The\nevaluation was performed on data from 19 speakers of the publicly available\nGRID corpus. With 51 different words to classify, we report a best word\naccuracy on held-out evaluation speakers of 79.6% using the end-to-end neural\nnetwork-based solution (11.6% improvement over the best feature-based solution\nevaluated).", "published": "2016-01-29 16:48:07", "link": "http://arxiv.org/abs/1601.08188v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
