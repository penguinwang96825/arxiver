{"title": "Unsupervised Explanation Generation via Correct Instantiations", "abstract": "While large pre-trained language models (PLM) have shown their great skills\nat solving discriminative tasks, a significant gap remains when compared with\nhumans for explanation-related tasks. Among them, explaining the reason why a\nstatement is wrong (e.g., against commonsense) is incredibly challenging. The\nmajor difficulty is finding the conflict point, where the statement contradicts\nour real world. This paper proposes Neon, a two-phrase, unsupervised\nexplanation generation framework. Neon first generates corrected instantiations\nof the statement (phase I), then uses them to prompt large PLMs to find the\nconflict point and complete the explanation (phase II). We conduct extensive\nexperiments on two standard explanation benchmarks, i.e., ComVE and e-SNLI.\nAccording to both automatic and human evaluations, Neon outperforms baselines,\neven for those with human-annotated instantiations. In addition to explaining a\nnegative prediction, we further demonstrate that Neon remains effective when\ngeneralizing to different scenarios.", "published": "2022-11-21 03:10:24", "link": "http://arxiv.org/abs/2211.11160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion\n  Recognition", "abstract": "Multimodal sentiment analysis (MSA) and emotion recognition in conversation\n(ERC) are key research topics for computers to understand human behaviors. From\na psychological perspective, emotions are the expression of affect or feelings\nduring a short period, while sentiments are formed and held for a longer\nperiod. However, most existing works study sentiment and emotion separately and\ndo not fully exploit the complementary knowledge behind the two. In this paper,\nwe propose a multimodal sentiment knowledge-sharing framework (UniMSE) that\nunifies MSA and ERC tasks from features, labels, and models. We perform\nmodality fusion at the syntactic and semantic levels and introduce contrastive\nlearning between modalities and samples to better capture the difference and\nconsistency between sentiments and emotions. Experiments on four public\nbenchmark datasets, MOSI, MOSEI, MELD, and IEMOCAP, demonstrate the\neffectiveness of the proposed method and achieve consistent improvements\ncompared with state-of-the-art methods.", "published": "2022-11-21 08:46:01", "link": "http://arxiv.org/abs/2211.11256v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In-sample Curriculum Learning by Sequence Completion for Natural\n  Language Generation", "abstract": "Curriculum learning has shown promising improvements in multiple domains by\ntraining machine learning models from easy samples to hard ones. Previous works\nwhich either design rules or train models for scoring the difficulty highly\nrely on task-specific expertise, and cannot generalize. Inspired by the\n\"easy-to-hard\" intuition, we propose to do in-sample curriculum learning for\nnatural language generation tasks. Our learning strategy starts training the\nmodel to generate the last few words, i.e., do sequence completion, and\ngradually extends to generate the whole output sequence. Comprehensive\nexperiments show that it generalizes well to different tasks and achieves\nsignificant improvements over strong baselines.", "published": "2022-11-21 09:38:59", "link": "http://arxiv.org/abs/2211.11297v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TCBERT: A Technical Report for Chinese Topic Classification BERT", "abstract": "Bidirectional Encoder Representations from Transformers or\nBERT~\\cite{devlin-etal-2019-bert} has been one of the base models for various\nNLP tasks due to its remarkable performance. Variants customized for different\nlanguages and tasks are proposed to further improve the performance. In this\nwork, we investigate supervised continued\npre-training~\\cite{gururangan-etal-2020-dont} on BERT for Chinese topic\nclassification task. Specifically, we incorporate prompt-based learning and\ncontrastive learning into the pre-training. To adapt to the task of Chinese\ntopic classification, we collect around 2.1M Chinese data spanning various\ntopics. The pre-trained Chinese Topic Classification BERTs (TCBERTs) with\ndifferent parameter sizes are open-sourced at\n\\url{https://huggingface.co/IDEA-CCNL}.", "published": "2022-11-21 09:45:15", "link": "http://arxiv.org/abs/2211.11304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Crisis-Related Tweet Classification with Entity-Masked\n  Language Modeling and Multi-Task Learning", "abstract": "Social media has become an important information source for crisis management\nand provides quick access to ongoing developments and critical information.\nHowever, classification models suffer from event-related biases and highly\nimbalanced label distributions which still poses a challenging task. To address\nthese challenges, we propose a combination of entity-masked language modeling\nand hierarchical multi-label classification as a multi-task learning problem.\nWe evaluate our method on tweets from the TREC-IS dataset and show an absolute\nperformance gain w.r.t. F1-score of up to 10% for actionable information types.\nMoreover, we found that entity-masking reduces the effect of overfitting to\nin-domain events and enables improvements in cross-event generalization.", "published": "2022-11-21 13:54:10", "link": "http://arxiv.org/abs/2211.11468v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Programming by Example and Text-to-Code Translation for Conversational\n  Code Generation", "abstract": "Dialogue systems is an increasingly popular task of natural language\nprocessing. However, the dialogue paths tend to be deterministic, restricted to\nthe system rails, regardless of the given request or input text. Recent\nadvances in program synthesis have led to systems which can synthesize programs\nfrom very general search spaces, e.g. Programming by Example, and to systems\nwith very accessible interfaces for writing programs, e.g. text-to-code\ntranslation, but have not achieved both of these qualities in the same system.\nWe propose Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a\nmethod for integrating Programming by Example and text-to-code systems which\noffers an accessible natural language interface for synthesizing general\nprograms. We present a program representation that allows our method to be\napplied to the problem of task-oriented dialogue. Finally, we demo MPaTHS using\nour program representation.", "published": "2022-11-21 15:20:45", "link": "http://arxiv.org/abs/2211.11554v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog\n  Evaluation", "abstract": "Practical dialog systems need to deal with various knowledge sources, noisy\nuser expressions, and the shortage of annotated data. To better solve the above\nproblems, we propose CGoDial, new challenging and comprehensive Chinese\nbenchmark for multi-domain Goal-oriented Dialog evaluation. It contains 96,763\ndialog sessions and 574,949 dialog turns totally, covering three datasets with\ndifferent knowledge sources: 1) a slot-based dialog (SBD) dataset with\ntable-formed knowledge, 2) a flow-based dialog (FBD) dataset with tree-formed\nknowledge, and a retrieval-based dialog (RBD) dataset with candidate-formed\nknowledge. To bridge the gap between academic benchmarks and spoken dialog\nscenarios, we either collect data from real conversations or add spoken\nfeatures to existing datasets via crowd-sourcing. The proposed experimental\nsettings include the combinations of training with either the entire training\nset or a few-shot training set, and testing with either the standard test set\nor a hard test subset, which can assess model capabilities in terms of general\nprediction, fast adaptability and reliable robustness.", "published": "2022-11-21 16:21:41", "link": "http://arxiv.org/abs/2211.11617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring Harmful Representations in Scandinavian Language Models", "abstract": "Scandinavian countries are perceived as role-models when it comes to gender\nequality. With the advent of pre-trained language models and their widespread\nusage, we investigate to what extent gender-based harmful and toxic content\nexist in selected Scandinavian language models. We examine nine models,\ncovering Danish, Swedish, and Norwegian, by manually creating template-based\nsentences and probing the models for completion. We evaluate the completions\nusing two methods for measuring harmful and toxic completions and provide a\nthorough analysis of the results. We show that Scandinavian pre-trained\nlanguage models contain harmful and gender-based stereotypes with similar\nvalues across all languages. This finding goes against the general expectations\nrelated to gender equality in Scandinavian countries and shows the possible\nproblematic outcomes of using such models in real-world settings.", "published": "2022-11-21 17:46:39", "link": "http://arxiv.org/abs/2211.11678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Legal and Political Stance Detection of SCOTUS Language", "abstract": "We analyze publicly available US Supreme Court documents using automated\nstance detection. In the first phase of our work, we investigate the extent to\nwhich the Court's public-facing language is political. We propose and calculate\ntwo distinct ideology metrics of SCOTUS justices using oral argument\ntranscripts. We then compare these language-based metrics to existing social\nscientific measures of the ideology of the Supreme Court and the public.\nThrough this cross-disciplinary analysis, we find that justices who are more\nresponsive to public opinion tend to express their ideology during oral\narguments. This observation provides a new kind of evidence in favor of the\nattitudinal change hypothesis of Supreme Court justice behavior. As a natural\nextension of this political stance detection, we propose the more specialized\ntask of legal stance detection with our new dataset SC-stance, which matches\nwritten opinions to legal questions. We find competitive performance on this\ndataset using language adapters trained on legal documents.", "published": "2022-11-21 18:45:57", "link": "http://arxiv.org/abs/2211.11724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts", "abstract": "Classroom discourse is a core medium of instruction - analyzing it can\nprovide a window into teaching and learning as well as driving the development\nof new tools for improving instruction. We introduce the largest dataset of\nmathematics classroom transcripts available to researchers, and demonstrate how\nthis data can help improve instruction. The dataset consists of 1,660 45-60\nminute long 4th and 5th grade elementary mathematics observations collected by\nthe National Center for Teacher Effectiveness (NCTE) between 2010-2013. The\nanonymized transcripts represent data from 317 teachers across 4 school\ndistricts that serve largely historically marginalized students. The\ntranscripts come with rich metadata, including turn-level annotations for\ndialogic discourse moves, classroom observation scores, demographic\ninformation, survey responses and student test scores. We demonstrate that our\nnatural language processing model, trained on our turn-level annotations, can\nlearn to identify dialogic discourse moves and these moves are correlated with\nbetter classroom observation scores and learning outcomes. This dataset opens\nup several possibilities for researchers, educators and policymakers to learn\nabout and improve K-12 instruction. The dataset can be found at\nhttps://github.com/ddemszky/classroom-transcript-analysis.", "published": "2022-11-21 19:00:01", "link": "http://arxiv.org/abs/2211.11772v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised extraction, labelling and clustering of segments from\n  clinical notes", "abstract": "This work is motivated by the scarcity of tools for accurate, unsupervised\ninformation extraction from unstructured clinical notes in computationally\nunderrepresented languages, such as Czech. We introduce a stepping stone to a\nbroad array of downstream tasks such as summarisation or integration of\nindividual patient records, extraction of structured information for national\ncancer registry reporting or building of semi-structured semantic patient\nrepresentations for computing patient embeddings. More specifically, we present\na method for unsupervised extraction of semantically-labelled textual segments\nfrom clinical notes and test it out on a dataset of Czech breast cancer\npatients, provided by Masaryk Memorial Cancer Institute (the largest Czech\nhospital specialising in oncology). Our goal was to extract, classify (i.e.\nlabel) and cluster segments of the free-text notes that correspond to specific\nclinical features (e.g., family background, comorbidities or toxicities). The\npresented results demonstrate the practical relevance of the proposed approach\nfor building more sophisticated extraction and analytical pipelines deployed on\nCzech clinical notes.", "published": "2022-11-21 19:05:06", "link": "http://arxiv.org/abs/2211.11799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Knowledge Dependency of Questions", "abstract": "The automatic generation of Multiple Choice Questions (MCQ) has the potential\nto reduce the time educators spend on student assessment significantly.\nHowever, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE,\nand METEOR, focus on the n-gram based similarity of the generated MCQ to the\ngold sample in the dataset and disregard their educational value. They fail to\nevaluate the MCQ's ability to assess the student's knowledge of the\ncorresponding target fact. To tackle this issue, we propose a novel automatic\nevaluation metric, coined Knowledge Dependent Answerability (KDA), which\nmeasures the MCQ's answerability given knowledge of the target fact.\nSpecifically, we first show how to measure KDA based on student responses from\na human survey. Then, we propose two automatic evaluation metrics, KDA_disc and\nKDA_cont, that approximate KDA by leveraging pre-trained language models to\nimitate students' problem-solving behavior. Through our human studies, we show\nthat KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2)\nusability in an actual classroom setting, labeled by experts. Furthermore, when\ncombined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown\nto have a strong predictive power for various expert-labeled MCQ quality\nmeasures.", "published": "2022-11-21 23:08:30", "link": "http://arxiv.org/abs/2211.11902v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language in a Bottle: Language Model Guided Concept Bottlenecks for\n  Interpretable Image Classification", "abstract": "Concept Bottleneck Models (CBM) are inherently interpretable models that\nfactor model decisions into human-readable concepts. They allow people to\neasily understand why a model is failing, a critical feature for high-stakes\napplications. CBMs require manually specified concepts and often under-perform\ntheir black box counterparts, preventing their broad adoption. We address these\nshortcomings and are first to show how to construct high-performance CBMs\nwithout manual specification of similar accuracy to black box models. Our\napproach, Language Guided Bottlenecks (LaBo), leverages a language model,\nGPT-3, to define a large space of possible bottlenecks. Given a problem domain,\nLaBo uses GPT-3 to produce factual sentences about categories to form candidate\nconcepts. LaBo efficiently searches possible bottlenecks through a novel\nsubmodular utility that promotes the selection of discriminative and diverse\ninformation. Ultimately, GPT-3's sentential concepts can be aligned to images\nusing CLIP, to form a bottleneck layer. Experiments demonstrate that LaBo is a\nhighly effective prior for concepts important to visual recognition. In the\nevaluation with 11 diverse datasets, LaBo bottlenecks excel at few-shot\nclassification: they are 11.7% more accurate than black box linear probes at 1\nshot and comparable with more data. Overall, LaBo demonstrates that inherently\ninterpretable models can be widely applied at similar, or better, performance\nthan black box approaches.", "published": "2022-11-21 03:05:02", "link": "http://arxiv.org/abs/2211.11158v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking\n  BERT Sentence Representations for Hindi and Marathi", "abstract": "Sentence representation from vanilla BERT models does not work well on\nsentence similarity tasks. Sentence-BERT models specifically trained on STS or\nNLI datasets are shown to provide state-of-the-art performance. However,\nbuilding these models for low-resource languages is not straightforward due to\nthe lack of these specialized datasets. This work focuses on two low-resource\nIndian languages, Hindi and Marathi. We train sentence-BERT models for these\nlanguages using synthetic NLI and STS datasets prepared using machine\ntranslation. We show that the strategy of NLI pre-training followed by STSb\nfine-tuning is effective in generating high-performance sentence-similarity\nmodels for Hindi and Marathi. The vanilla BERT models trained using this simple\nstrategy outperform the multilingual LaBSE trained using a complex training\nstrategy. These models are evaluated on downstream text classification and\nsimilarity tasks. We evaluate these models on real text classification datasets\nto show embeddings obtained from synthetic data training are generalizable to\nreal datasets as well and thus represent an effective training strategy for\nlow-resource languages. We also provide a comparative analysis of sentence\nembeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,\nxlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and\nmonolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release\nL3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for\nMarathi and Hindi respectively. Our work also serves as a guide to building\nlow-resource sentence embedding models.", "published": "2022-11-21 05:15:48", "link": "http://arxiv.org/abs/2211.11187v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learn from Yesterday: A Semi-Supervised Continual Learning Method for\n  Supervision-Limited Text-to-SQL Task Streams", "abstract": "Conventional text-to-SQL studies are limited to a single task with a\nfixed-size training and test set. When confronted with a stream of tasks common\nin real-world applications, existing methods struggle with the problems of\ninsufficient supervised data and high retraining costs. The former tends to\ncause overfitting on unseen databases for the new task, while the latter makes\na full review of instances from past tasks impractical for the model, resulting\nin forgetting of learned SQL structures and database schemas. To address the\nproblems, this paper proposes integrating semi-supervised learning (SSL) and\ncontinual learning (CL) in a stream of text-to-SQL tasks and offers two\npromising solutions in turn. The first solution Vanilla is to perform\nself-training, augmenting the supervised training data with predicted\npseudo-labeled instances of the current task, while replacing the full volume\nretraining with episodic memory replay to balance the training efficiency with\nthe performance of previous tasks. The improved solution SFNet takes advantage\nof the intrinsic connection between CL and SSL. It uses in-memory past\ninformation to help current SSL, while adding high-quality pseudo instances in\nmemory to improve future replay. The experiments on two datasets shows that\nSFNet outperforms the widely-used SSL-only and CL-only baselines on multiple\nmetrics.", "published": "2022-11-21 07:40:28", "link": "http://arxiv.org/abs/2211.11226v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in\n  Text", "abstract": "Self-supervised representation learning has proved to be a valuable component\nfor out-of-distribution (OoD) detection with only the texts of in-distribution\n(ID) examples. These approaches either train a language model from scratch or\nfine-tune a pre-trained language model using ID examples, and then take the\nperplexity output by the language model as OoD scores. In this paper, we\nanalyze the complementary characteristics of both OoD detection methods and\npropose a multi-level knowledge distillation approach that integrates their\nstrengths while mitigating their limitations. Specifically, we use a fine-tuned\nmodel as the teacher to teach a randomly initialized student model on the ID\nexamples. Besides the prediction layer distillation, we present a\nsimilarity-based intermediate layer distillation method to thoroughly explore\nthe representation space of the teacher model. In this way, the learned student\ncan better represent the ID data manifold while gaining a stronger ability to\nmap OoD examples outside the ID data manifold with the regularization inherited\nfrom pre-training. Besides, the student model sees only ID examples during\nparameter learning, further promoting more distinguishable features for OoD\ndetection. We conduct extensive experiments over multiple benchmark datasets,\ni.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the\nproposed method yields new state-of-the-art performance. We also explore its\napplication as an AIGC detector to distinguish between answers generated by\nChatGPT and human experts. It is observed that our model exceeds human\nevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.", "published": "2022-11-21 09:41:25", "link": "http://arxiv.org/abs/2211.11300v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AF Adapter: Continual Pretraining for Building Chinese Biomedical\n  Language Model", "abstract": "Continual pretraining is a popular way of building a domain-specific\npretrained language model from a general-domain language model. In spite of its\nhigh efficiency, continual pretraining suffers from catastrophic forgetting,\nwhich may harm the model's performance in downstream tasks. To alleviate the\nissue, in this paper, we propose a continual pretraining method for the\nBERT-based model, named Attention-FFN Adapter. Its main idea is to introduce a\nsmall number of attention heads and hidden units inside each self-attention\nlayer and feed-forward network. Furthermore, we train a domain-specific\nlanguage model named AF Adapter based RoBERTa for the Chinese biomedical\ndomain. In experiments, models are applied to downstream tasks for evaluation.\nThe results demonstrate that with only about 17% of model parameters trained,\nAF Adapter achieves 0.6%, 2% gain in performance on average, compared to strong\nbaselines. Further experimental results show that our method alleviates the\ncatastrophic forgetting problem by 11% compared to the fine-tuning method.", "published": "2022-11-21 11:30:13", "link": "http://arxiv.org/abs/2211.11363v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAILD: Towards Leveraging Relation Features for Inductive Link\n  Prediction In Knowledge Graphs", "abstract": "Due to the open world assumption, Knowledge Graphs (KGs) are never complete.\nIn order to address this issue, various Link Prediction (LP) methods are\nproposed so far. Some of these methods are inductive LP models which are\ncapable of learning representations for entities not seen during training.\nHowever, to the best of our knowledge, none of the existing inductive LP models\nfocus on learning representations for unseen relations. In this work, a novel\nRelation Aware Inductive Link preDiction (RAILD) is proposed for KG completion\nwhich learns representations for both unseen entities and unseen relations. In\naddition to leveraging textual literals associated with both entities and\nrelations by employing language models, RAILD also introduces a novel\ngraph-based approach to generate features for relations. Experiments are\nconducted with different existing and newly created challenging benchmark\ndatasets and the results indicate that RAILD leads to performance improvement\nover the state-of-the-art models. Moreover, since there are no existing\ninductive LP models which learn representations for unseen relations, we have\ncreated our own baselines and the results obtained with RAILD also outperform\nthese baselines.", "published": "2022-11-21 12:35:30", "link": "http://arxiv.org/abs/2211.11407v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "L3Cube-HindBERT and DevBERT: Pre-Trained BERT Transformer models for\n  Devanagari based Hindi and Marathi Languages", "abstract": "The monolingual Hindi BERT models currently available on the model hub do not\nperform better than the multi-lingual models on downstream tasks. We present\nL3Cube-HindBERT, a Hindi BERT model pre-trained on Hindi monolingual corpus.\nFurther, since Indic languages, Hindi and Marathi share the Devanagari script,\nwe train a single model for both languages. We release DevBERT, a Devanagari\nBERT model trained on both Marathi and Hindi monolingual datasets. We evaluate\nthese models on downstream Hindi and Marathi text classification and named\nentity recognition tasks. The HindBERT and DevBERT-based models show\nsignificant improvements over multi-lingual MuRIL, IndicBERT, and XLM-R. Based\non these observations we also release monolingual BERT models for other Indic\nlanguages Kannada, Telugu, Malayalam, Tamil, Gujarati, Assamese, Odia, Bengali,\nand Punjabi. These models are shared at https://huggingface.co/l3cube-pune .", "published": "2022-11-21 13:02:52", "link": "http://arxiv.org/abs/2211.11418v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deanthropomorphising NLP: Can a Language Model Be Conscious?", "abstract": "This work is intended as a voice in the discussion over previous claims that\na pretrained large language model (LLM) based on the Transformer model\narchitecture can be sentient. Such claims have been made concerning the LaMDA\nmodel and also concerning the current wave of LLM-powered chatbots, such as\nChatGPT. This claim, if confirmed, would have serious ramifications in the\nNatural Language Processing (NLP) community due to wide-spread use of similar\nmodels. However, here we take the position that such a large language model\ncannot be sentient, or conscious, and that LaMDA in particular exhibits no\nadvances over other similar models that would qualify it. We justify this by\nanalysing the Transformer architecture through Integrated Information Theory of\nconsciousness. We see the claims of sentience as part of a wider tendency to\nuse anthropomorphic language in NLP reporting. Regardless of the veracity of\nthe claims, we consider this an opportune moment to take stock of progress in\nlanguage modelling and consider the ethical implications of the task. In order\nto make this work helpful for readers outside the NLP community, we also\npresent the necessary background in language modelling.", "published": "2022-11-21 14:18:25", "link": "http://arxiv.org/abs/2211.11483v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multitask Vision-Language Prompt Tuning", "abstract": "Prompt Tuning, conditioning on task-specific learned prompt vectors, has\nemerged as a data-efficient and parameter-efficient method for adapting large\npretrained vision-language models to multiple downstream tasks. However,\nexisting approaches usually consider learning prompt vectors for each task\nindependently from scratch, thereby failing to exploit the rich shareable\nknowledge across different vision-language tasks. In this paper, we propose\nmultitask vision-language prompt tuning (MVLPT), which incorporates cross-task\nknowledge into prompt tuning for vision-language models. Specifically, (i) we\ndemonstrate the effectiveness of learning a single transferable prompt from\nmultiple source tasks to initialize the prompt for each target task; (ii) we\nshow many target tasks can benefit each other from sharing prompt vectors and\nthus can be jointly learned via multitask prompt tuning. We benchmark the\nproposed MVLPT using three representative prompt tuning methods, namely text\nprompt tuning, visual prompt tuning, and the unified vision-language prompt\ntuning. Results in 20 vision tasks demonstrate that the proposed approach\noutperforms all single-task baseline prompt tuning methods, setting the new\nstate-of-the-art on the few-shot ELEVATER benchmarks and cross-task\ngeneralization benchmarks. To understand where the cross-task knowledge is most\neffective, we also conduct a large-scale study on task transferability with 20\nvision tasks in 400 combinations for each prompt tuning method. It shows that\nthe most performant MVLPT for each prompt tuning method prefers different task\ncombinations and many tasks can benefit each other, depending on their visual\nsimilarity and label similarity. Code is available at\nhttps://github.com/sIncerass/MVLPT.", "published": "2022-11-21 18:41:44", "link": "http://arxiv.org/abs/2211.11720v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Can You Label Less by Using Out-of-Domain Data? Active & Transfer\n  Learning with Few-shot Instructions", "abstract": "Labeling social-media data for custom dimensions of toxicity and social bias\nis challenging and labor-intensive. Existing transfer and active learning\napproaches meant to reduce annotation effort require fine-tuning, which suffers\nfrom over-fitting to noise and can cause domain shift with small sample sizes.\nIn this work, we propose a novel Active Transfer Few-shot Instructions (ATF)\napproach which requires no fine-tuning. ATF leverages the internal linguistic\nknowledge of pre-trained language models (PLMs) to facilitate the transfer of\ninformation from existing pre-labeled datasets (source-domain task) with\nminimum labeling effort on unlabeled target data (target-domain task). Our\nstrategy can yield positive transfer achieving a mean AUC gain of 10.5%\ncompared to no transfer with a large 22b parameter PLM. We further show that\nannotation of just a few target-domain samples via active learning can be\nbeneficial for transfer, but the impact diminishes with more annotation effort\n(26% drop in gain between 100 and 2000 annotated examples). Finally, we find\nthat not all transfer scenarios yield a positive gain, which seems related to\nthe PLMs initial performance on the target-domain task.", "published": "2022-11-21 19:03:31", "link": "http://arxiv.org/abs/2211.11798v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Self-Consistency and Performance of Pre-Trained Language\n  Models through Natural Language Inference", "abstract": "While large pre-trained language models are powerful, their predictions often\nlack logical consistency across test inputs. For example, a state-of-the-art\nMacaw question-answering (QA) model answers 'Yes' to 'Is a sparrow a bird?' and\n'Does a bird have feet?' but answers 'No' to 'Does a sparrow have feet?'. To\naddress this failure mode, we propose a framework, Consistency Correction\nthrough Relation Detection, or ConCoRD, for boosting the consistency and\naccuracy of pre-trained NLP models using pre-trained natural language inference\n(NLI) models without fine-tuning or re-training. Given a batch of test inputs,\nConCoRD samples several candidate outputs for each input and instantiates a\nfactor graph that accounts for both the model's belief about the likelihood of\neach answer choice in isolation and the NLI model's beliefs about pair-wise\nanswer choice compatibility. We show that a weighted MaxSAT solver can\nefficiently compute high-quality answer choices under this factor graph,\nimproving over the raw model's predictions. Our experiments demonstrate that\nConCoRD consistently boosts accuracy and consistency of off-the-shelf\nclosed-book QA and VQA models using off-the-shelf NLI models, notably\nincreasing accuracy of LXMERT on ConVQA by 5% absolute. See\nhttps://ericmitchell.ai/emnlp-2022-concord/ for code and data.", "published": "2022-11-21 21:58:30", "link": "http://arxiv.org/abs/2211.11875v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TEMPERA: Test-Time Prompting via Reinforcement Learning", "abstract": "Careful prompt design is critical to the use of large language models in\nzero-shot or few-shot learning. As a consequence, there is a growing interest\nin automated methods to design optimal prompts. In this work, we propose\nTest-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to\nprior prompt generation methods, TEMPERA can efficiently leverage prior\nknowledge, is adaptive to different queries and provides an interpretable\nprompt for every query. To achieve this, we design a novel action space that\nallows flexible editing of the initial prompts covering a wide set of\ncommonly-used components like instructions, few-shot exemplars, and\nverbalizers. The proposed method achieves significant gains compared with\nrecent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a\nvariety of tasks including sentiment analysis, topic classification, natural\nlanguage inference, and reading comprehension. Our method achieves 5.33x on\naverage improvement in sample efficiency when compared to the traditional\nfine-tuning methods.", "published": "2022-11-21 22:38:20", "link": "http://arxiv.org/abs/2211.11890v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying gender bias in blockbuster movies through the lens of\n  machine learning", "abstract": "The problem of gender bias is highly prevalent and well known. In this paper,\nwe have analysed the portrayal of gender roles in English movies, a medium that\neffectively influences society in shaping people's beliefs and opinions. First,\nwe gathered scripts of films from different genres and derived sentiments and\nemotions using natural language processing techniques. Afterwards, we converted\nthe scripts into embeddings, i.e. a way of representing text in the form of\nvectors. With a thorough investigation, we found specific patterns in male and\nfemale characters' personality traits in movies that align with societal\nstereotypes. Furthermore, we used mathematical and machine learning techniques\nand found some biases wherein men are shown to be more dominant and envious\nthan women, whereas women have more joyful roles in movies. In our work, we\nintroduce, to the best of our knowledge, a novel technique to convert dialogues\ninto an array of emotions by combining it with Plutchik's wheel of emotions.\nOur study aims to encourage reflections on gender equality in the domain of\nfilm and facilitate other researchers in analysing movies automatically instead\nof using manual approaches.", "published": "2022-11-21 09:41:53", "link": "http://arxiv.org/abs/2211.12504v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Validating Large Language Models with ReLM", "abstract": "Although large language models (LLMs) have been touted for their ability to\ngenerate natural-sounding text, there are growing concerns around possible\nnegative effects of LLMs such as data memorization, bias, and inappropriate\nlanguage. Unfortunately, the complexity and generation capacities of LLMs make\nvalidating (and correcting) such concerns difficult. In this work, we introduce\nReLM, a system for validating and querying LLMs using standard regular\nexpressions. ReLM formalizes and enables a broad range of language model\nevaluations, reducing complex evaluation rules to simple regular expression\nqueries. Our results exploring queries surrounding memorization, gender bias,\ntoxicity, and language understanding show that ReLM achieves up to 15x higher\nsystem efficiency, 2.5x data efficiency, and increased statistical and\nprompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers\na competitive and general baseline for the increasingly important problem of\nLLM validation.", "published": "2022-11-21 21:40:35", "link": "http://arxiv.org/abs/2211.15458v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating\n  Unified Vision Language Model", "abstract": "Large-scale Transformer models bring significant improvements for various\ndownstream vision language tasks with a unified architecture. The performance\nimprovements come with increasing model size, resulting in slow inference speed\nand increased cost for severing. While some certain predictions benefit from\nthe full complexity of the large-scale model, not all of inputs need the same\namount of computation to conduct, potentially leading to computation resource\nwaste. To handle this challenge, early exiting is proposed to adaptively\nallocate computational power in term of input complexity to improve inference\nefficiency. The existing early exiting strategies usually adopt output\nconfidence based on intermediate layers as a proxy of input complexity to incur\nthe decision of skipping following layers. However, such strategies cannot\napply to encoder in the widely-used unified architecture with both encoder and\ndecoder due to difficulty of output confidence estimation in the encoder. It is\nsuboptimal in term of saving computation power to ignore the early exiting in\nencoder component. To handle this challenge, we propose a novel early exiting\nstrategy for unified visual language models, which allows dynamically skip the\nlayers in encoder and decoder simultaneously in term of input layer-wise\nsimilarities with multiple times of early exiting, namely \\textbf{MuE}. By\ndecomposing the image and text modalities in the encoder, MuE is flexible and\ncan skip different layers in term of modalities, advancing the inference\nefficiency while minimizing performance drop. Experiments on the SNLI-VE and MS\nCOCO datasets show that the proposed approach MuE can reduce expected inference\ntime by up to 50\\% and 40\\% while maintaining 99\\% and 96\\% performance\nrespectively.", "published": "2022-11-21 02:32:25", "link": "http://arxiv.org/abs/2211.11152v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Unifying Vision-Language Representation Space with Single-tower\n  Transformer", "abstract": "Contrastive learning is a form of distance learning that aims to learn\ninvariant features from two related representations. In this paper, we explore\nthe bold hypothesis that an image and its caption can be simply regarded as two\ndifferent views of the underlying mutual information, and train a model to\nlearn a unified vision-language representation space that encodes both\nmodalities at once in a modality-agnostic manner. We first identify\ndifficulties in learning a generic one-tower model for vision-language\npretraining (VLP), and propose OneR as a simple yet effective framework for our\ngoal. We discover intriguing properties that distinguish OneR from the previous\nworks that learn modality-specific representation spaces such as zero-shot\nobject localization, text-guided visual reasoning and multi-modal retrieval,\nand present analyses to provide insights into this new form of multi-modal\nrepresentation learning. Thorough evaluations demonstrate the potential of a\nunified modality-agnostic VLP framework.", "published": "2022-11-21 02:34:21", "link": "http://arxiv.org/abs/2211.11153v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Cultural Re-contextualization of Fairness Research in Language\n  Technologies in India", "abstract": "Recent research has revealed undesirable biases in NLP data and models.\nHowever, these efforts largely focus on social disparities in the West, and are\nnot directly portable to other geo-cultural contexts. In this position paper,\nwe outline a holistic research agenda to re-contextualize NLP fairness research\nfor the Indian context, accounting for Indian societal context, bridging\ntechnological gaps in capability and resources, and adapting to Indian cultural\nvalues. We also summarize findings from an empirical study on various social\nbiases along different axes of disparities relevant to India, demonstrating\ntheir prevalence in corpora and models.", "published": "2022-11-21 06:37:45", "link": "http://arxiv.org/abs/2211.11206v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music\n  Generation Task", "abstract": "Benefiting from large-scale datasets and pre-trained models, the field of\ngenerative models has recently gained significant momentum. However, most\ndatasets for symbolic music are very small, which potentially limits the\nperformance of data-driven multimodal models. An intuitive solution to this\nproblem is to leverage pre-trained models from other modalities (e.g., natural\nlanguage) to improve the performance of symbolic music-related multimodal\ntasks. In this paper, we carry out the first study of generating complete and\nsemantically consistent symbolic music scores from text descriptions, and\nexplore the efficacy of using publicly available checkpoints (i.e., BERT,\nGPT-2, and BART) for natural language processing in the task of text-to-music\ngeneration. Our experimental results show that the improvement from using\npre-trained checkpoints is statistically significant in terms of BLEU score and\nedit distance similarity. We analyse the capabilities and limitations of our\nmodel to better understand the potential of language-music models.", "published": "2022-11-21 07:19:17", "link": "http://arxiv.org/abs/2211.11216v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Embedding a Differentiable Mel-cepstral Synthesis Filter to a Neural\n  Speech Synthesis System", "abstract": "This paper integrates a classic mel-cepstral synthesis filter into a modern\nneural speech synthesis system towards end-to-end controllable speech\nsynthesis. Since the mel-cepstral synthesis filter is explicitly embedded in\nneural waveform models in the proposed system, both voice characteristics and\nthe pitch of synthesized speech are highly controlled via a frequency warping\nparameter and fundamental frequency, respectively. We implement the\nmel-cepstral synthesis filter as a differentiable and GPU-friendly module to\nenable the acoustic and waveform models in the proposed system to be\nsimultaneously optimized in an end-to-end manner. Experiments show that the\nproposed system improves speech quality from a baseline system maintaining\ncontrollability. The core PyTorch modules used in the experiments will be\npublicly available on GitHub.", "published": "2022-11-21 07:35:21", "link": "http://arxiv.org/abs/2211.11222v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LSTM based models stability in the context of Sentiment Analysis for\n  social media", "abstract": "Deep learning techniques have proven their effectiveness for Sentiment\nAnalysis (SA) related tasks. Recurrent neural networks (RNN), especially Long\nShort-Term Memory (LSTM) and Bidirectional LSTM, have become a reference for\nbuilding accurate predictive models. However, the models complexity and the\nnumber of hyperparameters to configure raises several questions related to\ntheir stability. In this paper, we present various LSTM models and their key\nparameters, and we perform experiments to test the stability of these models in\nthe context of Sentiment Analysis.", "published": "2022-11-21 08:31:30", "link": "http://arxiv.org/abs/2211.11246v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "DreamArtist++: Controllable One-Shot Text-to-Image Generation via\n  Positive-Negative Adapter", "abstract": "State-of-the-arts text-to-image generation models such as Imagen and Stable\nDiffusion Model have succeed remarkable progresses in synthesizing\nhigh-quality, feature-rich images with high resolution guided by human text\nprompts. Since certain characteristics of image content \\emph{e.g.}, very\nspecific object entities or styles, are very hard to be accurately described by\ntext, some example-based image generation approaches have been proposed,\n\\emph{i.e.} generating new concepts based on absorbing the salient features of\na few input references. Despite of acknowledged successes, these methods have\nstruggled on accurately capturing the reference examples' characteristics while\nkeeping diverse and high-quality image generation, particularly in the one-shot\nscenario (\\emph{i.e.} given only one reference). To tackle this problem, we\npropose a simple yet effective framework, namely DreamArtist, which adopts a\nnovel positive-negative prompt-tuning learning strategy on the pre-trained\ndiffusion model, and it has shown to well handle the trade-off between the\naccurate controllability and fidelity of image generation with only one\nreference example. Specifically, our proposed framework incorporates both\npositive and negative embeddings or adapters and optimizes them in a joint\nmanner. The positive part aggressively captures the salient characteristics of\nthe reference image to drive diversified generation and the negative part\nrectifies inadequacies from the positive part. We have conducted extensive\nexperiments and evaluated the proposed method from image similarity (fidelity)\nand diversity, generation controllability, and style cloning. And our\nDreamArtist has achieved a superior generation performance over existing\nmethods. Besides, our additional evaluation on extended tasks, including\nconcept compositions and prompt-guided image editing, demonstrates its\neffectiveness for more applications.", "published": "2022-11-21 10:37:56", "link": "http://arxiv.org/abs/2211.11337v4", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Challenges and Applications of Automated Extraction of Socio-political\n  Events from Text (CASE 2022): Workshop and Shared Task Report", "abstract": "We provide a summary of the fifth edition of the CASE workshop that is held\nin the scope of EMNLP 2022. The workshop consists of regular papers, two\nkeynotes, working papers of shared task participants, and task overview papers.\nThis workshop has been bringing together all aspects of event information\ncollection across technical and social science fields. In addition to the\nprogress in depth, the submission and acceptance of multimodal approaches show\nthe widening of this interdisciplinary research topic.", "published": "2022-11-21 11:22:32", "link": "http://arxiv.org/abs/2211.11359v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extended Multilingual Protest News Detection -- Shared Task 1, CASE 2021\n  and 2022", "abstract": "We report results of the CASE 2022 Shared Task 1 on Multilingual Protest\nEvent Detection. This task is a continuation of CASE 2021 that consists of four\nsubtasks that are i) document classification, ii) sentence classification, iii)\nevent sentence coreference identification, and iv) event extraction. The CASE\n2022 extension consists of expanding the test data with more data in previously\navailable languages, namely, English, Hindi, Portuguese, and Spanish, and\nadding new test data in Mandarin, Turkish, and Urdu for Sub-task 1, document\nclassification. The training data from CASE 2021 in English, Portuguese and\nSpanish were utilized. Therefore, predicting document labels in Hindi,\nMandarin, Turkish, and Urdu occurs in a zero-shot setting. The CASE 2022\nworkshop accepts reports on systems developed for predicting test data of CASE\n2021 as well. We observe that the best systems submitted by CASE 2022\nparticipants achieve between 79.71 and 84.06 F1-macro for new languages in a\nzero-shot setting. The winning approaches are mainly ensembling models and\nmerging data in multiple languages. The best two submissions on CASE 2021 data\noutperform submissions from last year for Subtask 1 and Subtask 2 in all\nlanguages. Only the following scenarios were not outperformed by new\nsubmissions on CASE 2021: Subtask 3 Portuguese \\& Subtask 4 English.", "published": "2022-11-21 11:23:01", "link": "http://arxiv.org/abs/2211.11360v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SSCFormer: Push the Limit of Chunk-wise Conformer for Streaming ASR\n  Using Sequentially Sampled Chunks and Chunked Causal Convolution", "abstract": "Currently, the chunk-wise schemes are often used to make Automatic Speech\nRecognition (ASR) models to support streaming deployment. However, existing\napproaches are unable to capture the global context, lack support for parallel\ntraining, or exhibit quadratic complexity for the computation of multi-head\nself-attention (MHSA). On the other side, the causal convolution, no future\ncontext used, has become the de facto module in streaming Conformer. In this\npaper, we propose SSCFormer to push the limit of chunk-wise Conformer for\nstreaming ASR using the following two techniques: 1) A novel cross-chunks\ncontext generation method, named Sequential Sampling Chunk (SSC) scheme, to\nre-partition chunks from regular partitioned chunks to facilitate efficient\nlong-term contextual interaction within local chunks. 2)The Chunked Causal\nConvolution (C2Conv) is designed to concurrently capture the left context and\nchunk-wise future context. Evaluations on AISHELL-1 show that an End-to-End\n(E2E) CER 5.33% can achieve, which even outperforms a strong time-restricted\nbaseline U2. Moreover, the chunk-wise MHSA computation in our model enables it\nto train with a large batch size and perform inference with linear complexity.", "published": "2022-11-21 13:04:37", "link": "http://arxiv.org/abs/2211.11419v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SMAUG: Sparse Masked Autoencoder for Efficient Video-Language\n  Pre-training", "abstract": "Video-language pre-training is crucial for learning powerful multi-modal\nrepresentation. However, it typically requires a massive amount of computation.\nIn this paper, we develop SMAUG, an efficient pre-training framework for\nvideo-language models. The foundation component in SMAUG is masked\nautoencoders. Different from prior works which only mask textual inputs, our\nmasking strategy considers both visual and textual modalities, providing a\nbetter cross-modal alignment and saving more pre-training costs. On top of\nthat, we introduce a space-time token sparsification module, which leverages\ncontext information to further select only \"important\" spatial regions and\ntemporal frames for pre-training. Coupling all these designs allows our method\nto enjoy both competitive performances on text-to-video retrieval and video\nquestion answering tasks, and much less pre-training costs by 1.9X or more. For\nexample, our SMAUG only needs about 50 NVIDIA A6000 GPU hours for pre-training\nto attain competitive performances on these two video-language tasks across six\npopular benchmarks.", "published": "2022-11-21 13:34:34", "link": "http://arxiv.org/abs/2211.11446v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Perceiver-VL: Efficient Vision-and-Language Modeling with Iterative\n  Latent Attention", "abstract": "We present Perceiver-VL, a vision-and-language framework that efficiently\nhandles high-dimensional multimodal inputs such as long videos and text.\nPowered by the iterative latent cross-attention of Perceiver, our framework\nscales with linear complexity, in contrast to the quadratic complexity of\nself-attention used in many state-of-the-art transformer-based models. To\nfurther improve the efficiency of our framework, we also study applying\nLayerDrop on cross-attention layers and introduce a mixed-stream architecture\nfor cross-modal retrieval. We evaluate Perceiver-VL on diverse video-text and\nimage-text benchmarks, where Perceiver-VL achieves the lowest GFLOPs and\nlatency while maintaining competitive performance. In addition, we also provide\ncomprehensive analyses of various aspects of our framework, including\npretraining data, scalability of latent size and input size, dropping\ncross-attention layers at inference to reduce latency, modality aggregation\nstrategy, positional encoding, and weight initialization strategy. Our code and\ncheckpoints are available at: https://github.com/zinengtang/Perceiver_VL", "published": "2022-11-21 18:22:39", "link": "http://arxiv.org/abs/2211.11701v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards continually learning new languages", "abstract": "Multilingual speech recognition with neural networks is often implemented\nwith batch-learning, when all of the languages are available before training.\nAn ability to add new languages after the prior training sessions can be\neconomically beneficial, but the main challenge is catastrophic forgetting. In\nthis work, we combine the qualities of weight factorization and elastic weight\nconsolidation in order to counter catastrophic forgetting and facilitate\nlearning new languages quickly. Such combination allowed us to eliminate\ncatastrophic forgetting while still achieving performance for the new languages\ncomparable with having all languages at once, in experiments of learning from\nan initial 10 languages to achieve 26 languages without catastrophic forgetting\nand a reasonable performance compared to training all languages from scratch.", "published": "2022-11-21 18:24:34", "link": "http://arxiv.org/abs/2211.11703v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SpeechNet: Weakly Supervised, End-to-End Speech Recognition at\n  Industrial Scale", "abstract": "End-to-end automatic speech recognition systems represent the state of the\nart, but they rely on thousands of hours of manually annotated speech for\ntraining, as well as heavyweight computation for inference. Of course, this\nimpedes commercialization since most companies lack vast human and\ncomputational resources. In this paper, we explore training and deploying an\nASR system in the label-scarce, compute-limited setting. To reduce human labor,\nwe use a third-party ASR system as a weak supervision source, supplemented with\nlabeling functions derived from implicit user feedback. To accelerate\ninference, we propose to route production-time queries across a pool of CUDA\ngraphs of varying input lengths, the distribution of which best matches the\ntraffic's. Compared to our third-party ASR, we achieve a relative improvement\nin word-error rate of 8% and a speedup of 600%. Our system, called SpeechNet,\ncurrently serves 12 million queries per day on our voice-enabled smart\ntelevision. To our knowledge, this is the first time a large-scale,\nWav2vec-based deployment has been described in the academic literature.", "published": "2022-11-21 18:58:36", "link": "http://arxiv.org/abs/2211.11740v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Investigating Prompt Engineering in Diffusion Models", "abstract": "With the spread of the use of Text2Img diffusion models such as DALL-E 2,\nImagen, Mid Journey and Stable Diffusion, one challenge that artists face is\nselecting the right prompts to achieve the desired artistic output. We present\ntechniques for measuring the effect that specific words and phrases in prompts\nhave, and (in the Appendix) present guidance on the selection of prompts to\nproduce desired effects.", "published": "2022-11-21 07:07:19", "link": "http://arxiv.org/abs/2211.15462v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for\n  Speech Representation Learning", "abstract": "Although speech is a simple and effective way for humans to communicate with\nthe outside world, a more realistic speech interaction contains multimodal\ninformation, e.g., vision, text. How to design a unified framework to integrate\ndifferent modal information and leverage different resources (e.g.,\nvisual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to\nfacilitate speech representation learning was not well explored. In this paper,\nwe propose a unified cross-modal representation learning framework VATLM\n(Visual-Audio-Text Language Model). The proposed VATLM employs a unified\nbackbone network to model the modality-independent information and utilizes\nthree simple modality-dependent modules to preprocess visual, speech, and text\ninputs. In order to integrate these three modalities into one shared semantic\nspace, VATLM is optimized with a masked prediction task of unified tokens,\ngiven by our proposed unified tokenizer. We evaluate the pre-trained VATLM on\naudio-visual related downstream tasks, including audio-visual speech\nrecognition (AVSR), visual speech recognition (VSR) tasks. Results show that\nthe proposed VATLM outperforms previous the state-of-the-art models, such as\naudio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that\nVATLM is capable of aligning different modalities into the same space. To\nfacilitate future research, we release the code and pre-trained models at\nhttps://aka.ms/vatlm.", "published": "2022-11-21 09:10:10", "link": "http://arxiv.org/abs/2211.11275v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TimbreCLIP: Connecting Timbre to Text and Images", "abstract": "We present work in progress on TimbreCLIP, an audio-text cross modal\nembedding trained on single instrument notes. We evaluate the models with a\ncross-modal retrieval task on synth patches. Finally, we demonstrate the\napplication of TimbreCLIP on two tasks: text-driven audio equalization and\ntimbre to image generation.", "published": "2022-11-21 07:40:01", "link": "http://arxiv.org/abs/2211.11225v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "J.5"], "primary_category": "cs.SD"}
{"title": "Video Background Music Generation: Dataset, Method and Evaluation", "abstract": "Music is essential when editing videos, but selecting music manually is\ndifficult and time-consuming. Thus, we seek to automatically generate\nbackground music tracks given video input. This is a challenging task since it\nrequires music-video datasets, efficient architectures for video-to-music\ngeneration, and reasonable metrics, none of which currently exist. To close\nthis gap, we introduce a complete recipe including dataset, benchmark model,\nand evaluation metric for video background music generation. We present SymMV,\na video and symbolic music dataset with various musical annotations. To the\nbest of our knowledge, it is the first video-music dataset with rich musical\nannotations. We also propose a benchmark video background music generation\nframework named V-MusProd, which utilizes music priors of chords, melody, and\naccompaniment along with video-music relations of semantic, color, and motion\nfeatures. To address the lack of objective metrics for video-music\ncorrespondence, we design a retrieval-based metric VMCP built upon a powerful\nvideo-music representation learning model. Experiments show that with our\ndataset, V-MusProd outperforms the state-of-the-art method in both music\nquality and correspondence with videos. We believe our dataset, benchmark\nmodel, and evaluation metric will boost the development of video background\nmusic generation. Our dataset and code are available at\nhttps://github.com/zhuole1025/SymMV.", "published": "2022-11-21 08:39:48", "link": "http://arxiv.org/abs/2211.11248v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "LISA: Localized Image Stylization with Audio via Implicit Neural\n  Representation", "abstract": "We present a novel framework, Localized Image Stylization with Audio (LISA)\nwhich performs audio-driven localized image stylization. Sound often provides\ninformation about the specific context of the scene and is closely related to a\ncertain part of the scene or object. However, existing image stylization works\nhave focused on stylizing the entire image using an image or text input.\nStylizing a particular part of the image based on audio input is natural but\nchallenging. In this work, we propose a framework that a user provides an audio\ninput to localize the sound source in the input image and another for locally\nstylizing the target object or scene. LISA first produces a delicate\nlocalization map with an audio-visual localization network by leveraging CLIP\nembedding space. We then utilize implicit neural representation (INR) along\nwith the predicted localization map to stylize the target object or scene based\non sound information. The proposed INR can manipulate the localized pixel\nvalues to be semantically consistent with the provided audio input. Through a\nseries of experiments, we show that the proposed framework outperforms the\nother audio-guided stylization methods. Moreover, LISA constructs concise\nlocalization maps and naturally manipulates the target object or scene in\naccordance with the given audio input.", "published": "2022-11-21 11:51:48", "link": "http://arxiv.org/abs/2211.11381v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "A Dataset for Greek Traditional and Folk Music: Lyra", "abstract": "Studying under-represented music traditions under the MIR scope is crucial,\nnot only for developing novel analysis tools, but also for unveiling musical\nfunctions that might prove useful in studying world musics. This paper presents\na dataset for Greek Traditional and Folk music that includes 1570 pieces,\nsumming in around 80 hours of data. The dataset incorporates YouTube\ntimestamped links for retrieving audio and video, along with rich metadata\ninformation with regards to instrumentation, geography and genre, among others.\nThe content has been collected from a Greek documentary series that is\navailable online, where academics present music traditions of Greece with live\nmusic and dance performance during the show, along with discussions about\nsocial, cultural and musicological aspects of the presented music. Therefore,\nthis procedure has resulted in a significant wealth of descriptions regarding a\nvariety of aspects, such as musical genre, places of origin and musical\ninstruments. In addition, the audio recordings were performed under strict\nproduction-level specifications, in terms of recording equipment, leading to\nvery clean and homogeneous audio content. In this work, apart from presenting\nthe dataset in detail, we propose a baseline deep-learning classification\napproach to recognize the involved musicological attributes. The dataset, the\nbaseline classification methods and the models are provided in public\nrepositories. Future directions for further refining the dataset are also\ndiscussed.", "published": "2022-11-21 14:15:43", "link": "http://arxiv.org/abs/2211.11479v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The applicability of transperceptual and deep learning approaches to the\n  study and mimicry of complex cartilaginous tissues", "abstract": "Complex soft tissues, for example the knee meniscus, play a crucial role in\nmobility and joint health, but when damaged are incredibly difficult to repair\nand replace. This is due to their highly hierarchical and porous nature which\nin turn leads to their unique mechanical properties. In order to design tissue\nsubstitutes, the internal architecture of the native tissue needs to be\nunderstood and replicated. Here we explore a combined audio-visual approach -\nso called transperceptual - to generate artificial architectures mimicking the\nnative ones. The proposed method uses both traditional imagery, and sound\ngenerated from each image as a method of rapidly comparing and contrasting the\nporosity and pore size within the samples. We have trained and tested a\ngenerative adversarial network (GAN) on the 2D image stacks. The impact of the\ntraining set of images on the similarity of the artificial to the original\ndataset was assessed by analyzing two samples. The first consisting of n=478\npairs of audio and image files for which the images were downsampled to 64\n$\\times$ 64 pixels, the second one consisting of n=7640 pairs of audio and\nimage files for which the full resolution 256 $\\times$ 256 pixels is retained\nbut each image is divided into 16 squares to maintain the limit of 64 $\\times$\n64 pixels required by the GAN. We reconstruct the 2D stacks of artificially\ngenerated datasets into 3D objects and run image analysis algorithms to\ncharacterize statistically the architectural parameters - pore size, tortuosity\nand pore connectivity - and compare them with the original dataset. Results\nshow that the artificially generated dataset that undergoes downsampling\nperforms better in terms of parameter matching. Our audiovisual approach has\nthe potential to be extended to larger data sets to explore both how\nsimilarities and differences can be audibly recognized across multiple samples.", "published": "2022-11-21 08:51:52", "link": "http://arxiv.org/abs/2211.14314v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS", "eess.IV", "physics.med-ph", "q-bio.QM"], "primary_category": "cs.CV"}
