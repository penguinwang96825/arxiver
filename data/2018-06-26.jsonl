{"title": "A Practical Incremental Learning Framework For Sparse Entity Extraction", "abstract": "This work addresses challenges arising from extracting entities from textual\ndata, including the high cost of data annotation, model accuracy, selecting\nappropriate evaluation criteria, and the overall quality of annotation. We\npresent a framework that integrates Entity Set Expansion (ESE) and Active\nLearning (AL) to reduce the annotation cost of sparse data and provide an\nonline evaluation method as feedback. This incremental and interactive learning\nframework allows for rapid annotation and subsequent extraction of sparse data\nwhile maintaining high accuracy. We evaluate our framework on three publicly\navailable datasets and show that it drastically reduces the cost of sparse\nentity annotation by an average of 85% and 45% to reach 0.9 and 1.0 F-Scores\nrespectively. Moreover, the method exhibited robust performance across all\ndatasets.", "published": "2018-06-26 01:36:44", "link": "http://arxiv.org/abs/1806.09751v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Multi-Modal Chinese Poetry Generation Model", "abstract": "Recent studies in sequence-to-sequence learning demonstrate that RNN\nencoder-decoder structure can successfully generate Chinese poetry. However,\nexisting methods can only generate poetry with a given first line or user's\nintent theme. In this paper, we proposed a three-stage multi-modal Chinese\npoetry generation approach. Given a picture, the first line, the title and the\nother lines of the poem are successively generated in three stages. According\nto the characteristics of Chinese poems, we propose a hierarchy-attention\nseq2seq model which can effectively capture character, phrase, and sentence\ninformation between contexts and improve the symmetry delivered in poems. In\naddition, the Latent Dirichlet allocation (LDA) model is utilized for title\ngeneration and improve the relevance of the whole poem and the title. Compared\nwith strong baseline, the experimental results demonstrate the effectiveness of\nour approach, using machine evaluations as well as human judgments.", "published": "2018-06-26 05:01:51", "link": "http://arxiv.org/abs/1806.09792v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Generators of Words Definitions", "abstract": "We explore recently introduced definition modeling technique that provided\nthe tool for evaluation of different distributed vector representations of\nwords through modeling dictionary definitions of words. In this work, we study\nthe problem of word ambiguities in definition modeling and propose a possible\nsolution by employing latent variable modeling and soft attention mechanisms.\nOur quantitative and qualitative evaluation and analysis of the model shows\nthat taking into account words ambiguity and polysemy leads to performance\nimprovement.", "published": "2018-06-26 16:07:50", "link": "http://arxiv.org/abs/1806.10090v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Cross-Lingual Coreference Resolution and its Application to\n  Entity Linking", "abstract": "We propose an entity-centric neural cross-lingual coreference model that\nbuilds on multi-lingual embeddings and language-independent features. We\nperform both intrinsic and extrinsic evaluations of our model. In the intrinsic\nevaluation, we show that our model, when trained on English and tested on\nChinese and Spanish, achieves competitive results to the models trained\ndirectly on Chinese and Spanish respectively. In the extrinsic evaluation, we\nshow that our English model helps achieve superior entity linking accuracy on\nChinese and Spanish test sets than the top 2015 TAC system without using any\nannotated data from Chinese or Spanish.", "published": "2018-06-26 20:24:52", "link": "http://arxiv.org/abs/1806.10201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Language Model Adaptation for Conversational Agents", "abstract": "Statistical language models (LM) play a key role in Automatic Speech\nRecognition (ASR) systems used by conversational agents. These ASR systems\nshould provide a high accuracy under a variety of speaking styles, domains,\nvocabulary and argots. In this paper, we present a DNN-based method to adapt\nthe LM to each user-agent interaction based on generalized contextual\ninformation, by predicting an optimal, context-dependent set of LM\ninterpolation weights. We show that this framework for contextual adaptation\nprovides accuracy improvements under different possible mixture LM partitions\nthat are relevant for both (1) Goal-oriented conversational agents where it's\nnatural to partition the data by the requested application and for (2) Non-goal\noriented conversational agents where the data can be partitioned using topic\nlabels that come from predictions of a topic classifier. We obtain a relative\nWER improvement of 3% with a 1-pass decoding strategy and 6% in a 2-pass\ndecoding framework, over an unadapted model. We also show up to a 15% relative\nimprovement in recognizing named entities which is of significant value for\nconversational ASR systems.", "published": "2018-06-26 21:17:09", "link": "http://arxiv.org/abs/1806.10215v4", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Graph-to-Sequence Learning using Gated Graph Neural Networks", "abstract": "Many NLP applications can be framed as a graph-to-sequence learning problem.\nPrevious work proposing neural architectures on this setting obtained promising\nresults compared to grammar-based approaches but still rely on linearisation\nheuristics and/or standard recurrent networks to achieve the best performance.\nIn this work, we propose a new model that encodes the full structural\ninformation contained in the graph. Our architecture couples the recently\nproposed Gated Graph Neural Networks with an input transformation that allows\nnodes and edges to have their own hidden representations, while tackling the\nparameter explosion problem present in previous work. Experimental results show\nthat our model outperforms strong baselines in generation from AMR graphs and\nsyntax-based neural machine translation.", "published": "2018-06-26 08:08:30", "link": "http://arxiv.org/abs/1806.09835v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Computational Analysis of Insurance Complaints: GEICO Case Study", "abstract": "The online environment has provided a great opportunity for insurance\npolicyholders to share their complaints with respect to different services.\nThese complaints can reveal valuable information for insurance companies who\nseek to improve their services; however, analyzing a huge number of online\ncomplaints is a complicated task for human and must involve computational\nmethods to create an efficient process. This research proposes a computational\napproach to characterize the major topics of a large number of online\ncomplaints. Our approach is based on using the topic modeling approach to\ndisclose the latent semantic of complaints. The proposed approach deployed on\nthousands of GEICO negative reviews. Analyzing 1,371 GEICO complaints indicates\nthat there are 30 major complains in four categories: (1) customer service, (2)\ninsurance coverage, paperwork, policy, and reports, (3) legal issues, and (4)\ncosts, estimates, and payments. This research approach can be used in other\napplications to explore a large number of reviews.", "published": "2018-06-26 00:12:14", "link": "http://arxiv.org/abs/1806.09736v1", "categories": ["stat.AP", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "stat.AP"}
{"title": "Deep Generative Models with Learnable Knowledge Constraints", "abstract": "The broad set of deep generative models (DGMs) has achieved remarkable\nadvances. However, it is often difficult to incorporate rich structured domain\nknowledge with the end-to-end DGMs. Posterior regularization (PR) offers a\nprincipled framework to impose structured constraints on probabilistic models,\nbut has limited applicability to the diverse DGMs that can lack a Bayesian\nformulation or even explicit density evaluation. PR also requires constraints\nto be fully specified a priori, which is impractical or suboptimal for complex\nknowledge with learnable uncertain parts. In this paper, we establish\nmathematical correspondence between PR and reinforcement learning (RL), and,\nbased on the connection, expand PR to learn constraints as the extrinsic reward\nin RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is\nflexible to adapt arbitrary constraints with the model jointly. Experiments on\nhuman image generation and templated sentence generation show models with\nlearned knowledge constraints by our algorithm greatly improve over base\ngenerative models.", "published": "2018-06-26 02:31:35", "link": "http://arxiv.org/abs/1806.09764v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unveiling the semantic structure of text documents using paragraph-aware\n  Topic Models", "abstract": "Classic Topic Models are built under the Bag Of Words assumption, in which\nword position is ignored for simplicity. Besides, symmetric priors are\ntypically used in most applications. In order to easily learn topics with\ndifferent properties among the same corpus, we propose a new line of work in\nwhich the paragraph structure is exploited. Our proposal is based on the\nfollowing assumption: in many text document corpora there are formal\nconstraints shared across all the collection, e.g. sections. When this\nassumption is satisfied, some paragraphs may be related to general concepts\nshared by all documents in the corpus, while others would contain the genuine\ndescription of documents. Assuming each paragraph can be semantically more\ngeneral, specific, or hybrid, we look for ways to measure this, transferring\nthis distinction to topics and being able to learn what we call specific and\ngeneral topics. Experiments show that this is a proper methodology to highlight\ncertain paragraphs in structured documents at the same time we learn\ninteresting and more diverse topics.", "published": "2018-06-26 07:50:37", "link": "http://arxiv.org/abs/1806.09827v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Enhancing Sentence Embedding with Generalized Pooling", "abstract": "Pooling is an essential component of a wide variety of sentence\nrepresentation and embedding models. This paper explores generalized pooling\nmethods to enhance sentence embedding. We propose vector-based multi-head\nattention that includes the widely used max pooling, mean pooling, and scalar\nself-attention as special cases. The model benefits from properly designed\npenalization terms to reduce redundancy in multi-head attention. We evaluate\nthe proposed model on three different tasks: natural language inference (NLI),\nauthor profiling, and sentiment classification. The experiments show that the\nproposed model achieves significant improvement over strong\nsentence-encoding-based methods, resulting in state-of-the-art performances on\nfour datasets. The proposed approach can be easily implemented for more\nproblems than we discuss in this paper.", "published": "2018-06-26 07:50:46", "link": "http://arxiv.org/abs/1806.09828v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text-Independent Speaker Verification Based on Deep Neural Networks and\n  Segmental Dynamic Time Warping", "abstract": "In this paper we present a new method for text-independent speaker\nverification that combines segmental dynamic time warping (SDTW) and the\nd-vector approach. The d-vectors, generated from a feed forward deep neural\nnetwork trained to distinguish between speakers, are used as features to\nperform alignment and hence calculate the overall distance between the\nenrolment and test utterances.We present results on the NIST 2008 data set for\nspeaker verification where the proposed method outperforms the conventional\ni-vector baseline with PLDA scores and outperforms d-vector approach with local\ndistances based on cosine and PLDA scores. Also score combination with the\ni-vector/PLDA baseline leads to significant gains over both methods.", "published": "2018-06-26 12:05:33", "link": "http://arxiv.org/abs/1806.09932v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditioning Deep Generative Raw Audio Models for Structured Automatic\n  Music", "abstract": "Existing automatic music generation approaches that feature deep learning can\nbe broadly classified into two types: raw audio models and symbolic models.\nSymbolic models, which train and generate at the note level, are currently the\nmore prevalent approach; these models can capture long-range dependencies of\nmelodic structure, but fail to grasp the nuances and richness of raw audio\ngenerations. Raw audio models, such as DeepMind's WaveNet, train directly on\nsampled audio waveforms, allowing them to produce realistic-sounding, albeit\nunstructured music. In this paper, we propose an automatic music generation\nmethodology combining both of these approaches to create structured,\nrealistic-sounding compositions. We consider a Long Short Term Memory network\nto learn the melodic structure of different styles of music, and then use the\nunique symbolic generations from this model as a conditioning input to a\nWaveNet-based raw audio generator, creating a model for automatic, novel music.\nWe then evaluate this approach by showcasing results of this work.", "published": "2018-06-26 11:10:19", "link": "http://arxiv.org/abs/1806.09905v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "The challenge of realistic music generation: modelling raw audio at\n  scale", "abstract": "Realistic music generation is a challenging task. When building generative\nmodels of music that are learnt from data, typically high-level representations\nsuch as scores or MIDI are used that abstract away the idiosyncrasies of a\nparticular performance. But these nuances are very important for our perception\nof musicality and realism, so in this work we embark on modelling music in the\nraw audio domain. It has been shown that autoregressive models excel at\ngenerating raw audio waveforms of speech, but when applied to music, we find\nthem biased towards capturing local signal structure at the expense of\nmodelling long-range correlations. This is problematic because music exhibits\nstructure at many different timescales. In this work, we explore autoregressive\ndiscrete autoencoders (ADAs) as a means to enable autoregressive models to\ncapture long-range correlations in waveforms. We find that they allow us to\nunconditionally generate piano music directly in the raw audio domain, which\nshows stylistic consistency across tens of seconds.", "published": "2018-06-26 16:48:59", "link": "http://arxiv.org/abs/1806.10474v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
