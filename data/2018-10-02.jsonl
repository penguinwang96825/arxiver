{"title": "Findings of the E2E NLG Challenge", "abstract": "This paper summarises the experimental setup and results of the first shared\ntask on end-to-end (E2E) natural language generation (NLG) in spoken dialogue\nsystems. Recent end-to-end generation systems are promising since they reduce\nthe need for data annotation. However, they are currently limited to small,\ndelexicalised datasets. The E2E NLG shared task aims to assess whether these\nnovel approaches can generate better-quality output by learning from a dataset\ncontaining higher lexical richness, syntactic complexity and diverse discourse\nphenomena. We compare 62 systems submitted by 17 institutions, covering a wide\nrange of approaches, including machine learning architectures -- with the\nmajority implementing sequence-to-sequence models (seq2seq) -- as well as\nsystems based on grammatical rules and templates.", "published": "2018-10-02 11:06:35", "link": "http://arxiv.org/abs/1810.01170v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Knowledge Hunting Framework for Common Sense Reasoning", "abstract": "We introduce an automatic system that achieves state-of-the-art results on\nthe Winograd Schema Challenge (WSC), a common sense reasoning task that\nrequires diverse, complex forms of inference and knowledge. Our method uses a\nknowledge hunting module to gather text from the web, which serves as evidence\nfor candidate problem resolutions. Given an input problem, our system generates\nrelevant queries to send to a search engine, then extracts and classifies\nknowledge from the returned results and weighs them to make a resolution. Our\napproach improves F1 performance on the full WSC by 0.21 over the previous best\nand represents the first system to exceed 0.5 F1. We further demonstrate that\nthe approach is competitive on the Choice of Plausible Alternatives (COPA)\ntask, which suggests that it is generally applicable.", "published": "2018-10-02 17:12:44", "link": "http://arxiv.org/abs/1810.01375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who is Addressed in this Comment? Automatically Classifying\n  Meta-Comments in News Comments", "abstract": "User comments have become an essential part of online journalism. However,\nnewsrooms are often overwhelmed by the vast number of diverse comments, for\nwhich a manual analysis is barely feasible. Identifying meta-comments that\naddress or mention newsrooms, individual journalists, or moderators and that\nmay call for reactions is particularly critical. In this paper, we present an\nautomated approach to identify and classify meta-comments. We compare comment\nclassification based on manually extracted features with an end-to-end learning\napproach. We develop, optimize, and evaluate multiple classifiers on a comment\ndataset of the large German online newsroom SPIEGEL Online and the 'One Million\nPosts' corpus of DER STANDARD, an Austrian newspaper. Both optimized\nclassification approaches achieved encouraging $F_{0.5}$ values between 76% and\n91%. We report on the most significant classification features with the results\nof a qualitative analysis and discuss how our work contributes to making\nparticipation in online journalism more constructive.", "published": "2018-10-02 08:32:08", "link": "http://arxiv.org/abs/1810.01114v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Learning to Segment Inputs for NMT Favors Character-Level Processing", "abstract": "Most modern neural machine translation (NMT) systems rely on presegmented\ninputs. Segmentation granularity importantly determines the input and output\nsequence lengths, hence the modeling depth, and source and target vocabularies,\nwhich in turn determine model size, computational costs of softmax\nnormalization, and handling of out-of-vocabulary words. However, the current\npractice is to use static, heuristic-based segmentations that are fixed before\nNMT training. This begs the question whether the chosen segmentation is optimal\nfor the translation task. To overcome suboptimal segmentation choices, we\npresent an algorithm for dynamic segmentation based on the Adaptative\nComputation Time algorithm (Graves 2016), that is trainable end-to-end and\ndriven by the NMT objective. In an evaluation on four translation tasks we\nfound that, given the freedom to navigate between different segmentation\nlevels, the model prefers to operate on (almost) character level, providing\nsupport for purely character-level NMT models from a novel angle.", "published": "2018-10-02 19:52:38", "link": "http://arxiv.org/abs/1810.01480v3", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improving Sentence Representations with Consensus Maximisation", "abstract": "Consensus maximisation learning can provide self-supervision when different\nviews are available of the same data. The distributional hypothesis provides\nanother form of useful self-supervision from adjacent sentences which are\nplentiful in large unlabelled corpora. Motivated by the observation that\ndifferent learning architectures tend to emphasise different aspects of\nsentence meaning, we present a new self-supervised learning framework for\nlearning sentence representations which minimises the disagreement between two\nviews of the same sentence where one view encodes the sentence with a recurrent\nneural network (RNN), and the other view encodes the same sentence with a\nsimple linear model. After learning, the individual views (networks) result in\nhigher quality sentence representations than their single-view learnt\ncounterparts (learnt using only the distributional hypothesis) as judged by\nperformance on standard downstream tasks. An ensemble of both views provides\neven better generalisation on both supervised and unsupervised downstream\ntasks. Also, importantly the ensemble of views trained with consensus\nmaximisation between the two different architectures performs better on\ndownstream tasks than an analogous ensemble made from the single-view trained\ncounterparts.", "published": "2018-10-02 04:51:33", "link": "http://arxiv.org/abs/1810.01064v4", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Text Regression with Conditional Generative Adversarial\n  Networks", "abstract": "Enormous online textual information provides intriguing opportunities for\nunderstandings of social and economic semantics. In this paper, we propose a\nnovel text regression model based on a conditional generative adversarial\nnetwork (GAN), with an attempt to associate textual data and social outcomes in\na semi-supervised manner. Besides promising potential of predicting\ncapabilities, our superiorities are twofold: (i) the model works with\nunbalanced datasets of limited labelled data, which align with real-world\nscenarios; and (ii) predictions are obtained by an end-to-end framework,\nwithout explicitly selecting high-level representations. Finally we point out\nrelated datasets for experiments and future research directions.", "published": "2018-10-02 10:35:13", "link": "http://arxiv.org/abs/1810.01165v2", "categories": ["cs.CL", "cs.AI", "q-fin.CP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Efficient Dialog Policy Learning via Positive Memory Retention", "abstract": "This paper is concerned with the training of recurrent neural networks as\ngoal-oriented dialog agents using reinforcement learning. Training such agents\nwith policy gradients typically requires a large amount of samples. However,\nthe collection of the required data in form of conversations between chat-bots\nand human agents is time-consuming and expensive. To mitigate this problem, we\ndescribe an efficient policy gradient method using positive memory retention,\nwhich significantly increases the sample-efficiency. We show that our method is\n10 times more sample-efficient than policy gradients in extensive experiments\non a new synthetic number guessing game. Moreover, in a real-word visual object\ndiscovery game, the proposed method is twice as sample-efficient as policy\ngradients and shows state-of-the-art performance.", "published": "2018-10-02 17:01:28", "link": "http://arxiv.org/abs/1810.01371v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Optimal Completion Distillation for Sequence Learning", "abstract": "We present Optimal Completion Distillation (OCD), a training procedure for\noptimizing sequence to sequence models based on edit distance. OCD is\nefficient, has no hyper-parameters of its own, and does not require pretraining\nor joint optimization with conditional log-likelihood. Given a partial sequence\ngenerated by the model, we first identify the set of optimal suffixes that\nminimize the total edit distance, using an efficient dynamic programming\nalgorithm. Then, for each position of the generated sequence, we use a target\ndistribution that puts equal probability on the first token of all the optimal\nsuffixes. OCD achieves the state-of-the-art performance on end-to-end speech\nrecognition, on both Wall Street Journal and Librispeech datasets, achieving\n$9.3\\%$ WER and $4.5\\%$ WER respectively.", "published": "2018-10-02 17:44:44", "link": "http://arxiv.org/abs/1810.01398v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Semi-supervised and Active-learning Scenarios: Efficient Acoustic Model\n  Refinement for a Low Resource Indian Language", "abstract": "We address the problem of efficient acoustic-model refinement (continuous\nretraining) using semi-supervised and active learning for a low resource Indian\nlanguage, wherein the low resource constraints are having i) a small labeled\ncorpus from which to train a baseline `seed' acoustic model and ii) a large\ntraining corpus without orthographic labeling or from which to perform a data\nselection for manual labeling at low costs. The proposed semi-supervised\nlearning decodes the unlabeled large training corpus using the seed model and\nthrough various protocols, selects the decoded utterances with high reliability\nusing confidence levels (that correlate to the WER of the decoded utterances)\nand iterative bootstrapping. The proposed active learning protocol uses\nconfidence level based metric to select the decoded utterances from the large\nunlabeled corpus for further labeling. The semi-supervised learning protocols\ncan offer a WER reduction, from a poorly trained seed model, by as much as 50%\nof the best WER-reduction realizable from the seed model's WER, if the large\ncorpus were labeled and used for acoustic-model training. The active learning\nprotocols allow that only 60% of the entire training corpus be manually\nlabeled, to reach the same performance as the entire data.", "published": "2018-10-02 07:23:42", "link": "http://arxiv.org/abs/1810.06635v1", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Phasebook and Friends: Leveraging Discrete Representations for Source\n  Separation", "abstract": "Deep learning based speech enhancement and source separation systems have\nrecently reached unprecedented levels of quality, to the point that performance\nis reaching a new ceiling. Most systems rely on estimating the magnitude of a\ntarget source by estimating a real-valued mask to be applied to a\ntime-frequency representation of the mixture signal. A limiting factor in such\napproaches is a lack of phase estimation: the phase of the mixture is most\noften used when reconstructing the estimated time-domain signal. Here, we\npropose \"magbook\", \"phasebook\", and \"combook\", three new types of layers based\non discrete representations that can be used to estimate complex time-frequency\nmasks. Magbook layers extend classical sigmoidal units and a recently\nintroduced convex softmax activation for mask-based magnitude estimation.\nPhasebook layers use a similar structure to give an estimate of the phase mask\nwithout suffering from phase wrapping issues. Combook layers are an alternative\nto the magbook-phasebook combination that directly estimate complex masks. We\npresent various training and inference schemes involving these representations,\nand explain in particular how to include them in an end-to-end learning\nframework. We also present an oracle study to assess upper bounds on\nperformance for various types of masks using discrete phase representations. We\nevaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus\nfor single-channel speaker-independent speaker separation, matching the\nperformance of state-of-the-art mask-based approaches without requiring\nadditional phase reconstruction steps.", "published": "2018-10-02 17:36:23", "link": "http://arxiv.org/abs/1810.01395v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
