{"title": "Open-Set Language Identification", "abstract": "We present the first open-set language identification experiments using\none-class classification. We first highlight the shortcomings of traditional\nfeature extraction methods and propose a hashing-based feature vectorization\napproach as a solution. Using a dataset of 10 languages from different writing\nsystems, we train a One- Class Support Vector Machine using only a monolingual\ncorpus for each language. Each model is evaluated against a test set of data\nfrom all 10 languages and we achieve an average F-score of 0.99, highlighting\nthe effectiveness of this approach for open-set language identification.", "published": "2017-07-16 04:17:49", "link": "http://arxiv.org/abs/1707.04817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Neural Nets Learn Statistical Laws behind Natural Language?", "abstract": "The performance of deep learning in natural language processing has been\nspectacular, but the reasons for this success remain unclear because of the\ninherent complexity of deep learning. This paper provides empirical evidence of\nits effectiveness and of a limitation of neural networks for language\nengineering. Precisely, we demonstrate that a neural language model based on\nlong short-term memory (LSTM) effectively reproduces Zipf's law and Heaps' law,\ntwo representative statistical properties underlying natural language. We\ndiscuss the quality of reproducibility and the emergence of Zipf's law and\nHeaps' law as training progresses. We also point out that the neural language\nmodel has a limitation in reproducing long-range correlation, another\nstatistical property of natural language. This understanding could provide a\ndirection for improving the architectures of neural networks.", "published": "2017-07-16 09:08:42", "link": "http://arxiv.org/abs/1707.04848v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Detection of Non-Relevant Posts on the Russian Imageboard\n  \"2ch\": Importance of the Choice of Word Representations", "abstract": "This study considers the problem of automated detection of non-relevant posts\non Web forums and discusses the approach of resolving this problem by\napproximation it with the task of detection of semantic relatedness between the\ngiven post and the opening post of the forum discussion thread. The\napproximated task could be resolved through learning the supervised classifier\nwith a composed word embeddings of two posts. Considering that the success in\nthis task could be quite sensitive to the choice of word representations, we\npropose a comparison of the performance of different word embedding models. We\ntrain 7 models (Word2Vec, Glove, Word2Vec-f, Wang2Vec, AdaGram, FastText,\nSwivel), evaluate embeddings produced by them on dataset of human judgements\nand compare their performance on the task of non-relevant posts detection. To\nmake the comparison, we propose a dataset of semantic relatedness with posts\nfrom one of the most popular Russian Web forums, imageboard \"2ch\", which has\nchallenging lexical and grammatical features.", "published": "2017-07-16 11:08:08", "link": "http://arxiv.org/abs/1707.04860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Information Extraction without Token-Level Supervision", "abstract": "Most state-of-the-art information extraction approaches rely on token-level\nlabels to find the areas of interest in text. Unfortunately, these labels are\ntime-consuming and costly to create, and consequently, not available for many\nreal-life IE tasks. To make matters worse, token-level labels are usually not\nthe desired output, but just an intermediary step. End-to-end (E2E) models,\nwhich take raw text as input and produce the desired output directly, need not\ndepend on token-level labels. We propose an E2E model based on pointer\nnetworks, which can be trained directly on pairs of raw input and output text.\nWe evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT\nmovie corpus and compare to neural baselines that do use token-level labels. We\nachieve competitive results, within a few percentage points of the baselines,\nshowing the feasibility of E2E information extraction without the need for\ntoken-level labels. This opens up new possibilities, as for many tasks\ncurrently addressed by human extractors, raw input and output data are\navailable, but not token-level labels.", "published": "2017-07-16 16:57:36", "link": "http://arxiv.org/abs/1707.04913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Listening while Speaking: Speech Chain by Deep Learning", "abstract": "Despite the close relationship between speech perception and production,\nresearch in automatic speech recognition (ASR) and text-to-speech synthesis\n(TTS) has progressed more or less independently without exerting much mutual\ninfluence on each other. In human communication, on the other hand, a\nclosed-loop speech chain mechanism with auditory feedback from the speaker's\nmouth to her ear is crucial. In this paper, we take a step further and develop\na closed-loop speech chain model based on deep learning. The\nsequence-to-sequence model in close-loop architecture allows us to train our\nmodel on the concatenation of both labeled and unlabeled data. While ASR\ntranscribes the unlabeled speech features, TTS attempts to reconstruct the\noriginal speech waveform based on the text from ASR. In the opposite direction,\nASR also attempts to reconstruct the original text transcription given the\nsynthesized speech. To the best of our knowledge, this is the first deep\nlearning model that integrates human speech perception and production\nbehaviors. Our experimental results show that the proposed approach\nsignificantly improved the performance more than separate systems that were\nonly trained with labeled data.", "published": "2017-07-16 13:27:56", "link": "http://arxiv.org/abs/1707.04879v1", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Automatized Generation of Alphabets of Symbols", "abstract": "In this paper, we discuss the generation of symbols (and alphabets) based on\nspecific user requirements (medium, priorities, type of information that needs\nto be conveyed). A framework for the generation of alphabets is proposed, and\nits use for the generation of a shorthand writing system is explored. We\ndiscuss the possible use of machine learning and genetic algorithms to gather\ninputs for generation of such alphabets and for optimization of already\ngenerated ones. The alphabets generated using such methods may be used in very\ndifferent fields, from the creation of synthetic languages and constructed\nscripts to the creation of sensible commands for multimodal interaction through\nHuman-Computer Interfaces, such as mouse gestures, touchpads, body gestures,\neye-tracking cameras, and brain-computing Interfaces, especially in\napplications for elderly care and people with disabilities.", "published": "2017-07-16 19:40:26", "link": "http://arxiv.org/abs/1707.04935v1", "categories": ["cs.HC", "cs.CL", "cs.CY"], "primary_category": "cs.HC"}
