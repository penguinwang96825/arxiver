{"title": "Integrating Semantic and Structural Information with Graph Convolutional\n  Network for Controversy Detection", "abstract": "Identifying controversial posts on social media is a fundamental task for\nmining public sentiment, assessing the influence of events, and alleviating the\npolarized views. However, existing methods fail to 1) effectively incorporate\nthe semantic information from content-related posts; 2) preserve the structural\ninformation for reply relationship modeling; 3) properly handle posts from\ntopics dissimilar to those in the training set. To overcome the first two\nlimitations, we propose Topic-Post-Comment Graph Convolutional Network\n(TPC-GCN), which integrates the information from the graph structure and\ncontent of topics, posts, and comments for post-level controversy detection. As\nto the third limitation, we extend our model to Disentangled TPC-GCN\n(DTPC-GCN), to disentangle topic-related and topic-unrelated features and then\nfuse dynamically. Extensive experiments on two real-world datasets demonstrate\nthat our models outperform existing methods. Analysis of the results and cases\nproves that our models can integrate both semantic and structural information\nwith significant generalizability.", "published": "2020-05-16 06:29:14", "link": "http://arxiv.org/abs/2005.07886v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ApplicaAI at SemEval-2020 Task 11: On RoBERTa-CRF, Span CLS and Whether\n  Self-Training Helps Them", "abstract": "This paper presents the winning system for the propaganda Technique\nClassification (TC) task and the second-placed system for the propaganda Span\nIdentification (SI) task. The purpose of TC task was to identify an applied\npropaganda technique given propaganda text fragment. The goal of SI task was to\nfind specific text fragments which contain at least one propaganda technique.\nBoth of the developed solutions used semi-supervised learning technique of\nself-training. Interestingly, although CRF is barely used with\ntransformer-based language models, the SI task was approached with RoBERTa-CRF\narchitecture. An ensemble of RoBERTa-based models was proposed for the TC task,\nwith one of them making use of Span CLS layers we introduce in the present\npaper. In addition to describing the submitted systems, an impact of\narchitectural decisions and training schemes is investigated along with remarks\nregarding training models of the same or better quality with lower\ncomputational budget. Finally, the results of error analysis are presented.", "published": "2020-05-16 10:15:49", "link": "http://arxiv.org/abs/2005.07934v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Logical Inferences with Comparatives and Generalized Quantifiers", "abstract": "Comparative constructions pose a challenge in Natural Language Inference\n(NLI), which is the task of determining whether a text entails a hypothesis.\nComparatives are structurally complex in that they interact with other\nlinguistic phenomena such as quantifiers, numerals, and lexical antonyms. In\nformal semantics, there is a rich body of work on comparatives and gradable\nexpressions using the notion of degree. However, a logical inference system for\ncomparatives has not been sufficiently developed for use in the NLI task. In\nthis paper, we present a compositional semantics that maps various comparative\nconstructions in English to semantic representations via Combinatory Categorial\nGrammar (CCG) parsers and combine it with an inference system based on\nautomated theorem proving. We evaluate our system on three NLI datasets that\ncontain complex logical inferences with comparatives, generalized quantifiers,\nand numerals. We show that the system outperforms previous logic-based systems\nas well as recent deep learning-based models.", "published": "2020-05-16 11:11:48", "link": "http://arxiv.org/abs/2005.07954v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Text Reassembling Approach to Natural Language Generation", "abstract": "Recent years have seen a number of proposals for performing Natural Language\nGeneration (NLG) based in large part on statistical techniques. Despite having\nmany attractive features, we argue that these existing approaches nonetheless\nhave some important drawbacks, sometimes because the approach in question is\nnot fully statistical (i.e., relies on a certain amount of handcrafting),\nsometimes because the approach in question lacks transparency. Focussing on\nsome of the key NLG tasks (namely Content Selection, Lexical Choice, and\nLinguistic Realisation), we propose a novel approach, called the Text\nReassembling approach to NLG (TRG), which approaches the ideal of a purely\nstatistical approach very closely, and which is at the same time highly\ntransparent. We evaluate the TRG approach and discuss how TRG may be extended\nto deal with other NLG tasks, such as Document Structuring, and Aggregation. We\ndiscuss the strengths and limitations of TRG, concluding that the method may\nhold particular promise for domain experts who want to build an NLG system\ndespite having little expertise in linguistics and NLG.", "published": "2020-05-16 13:28:17", "link": "http://arxiv.org/abs/2005.07988v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Probabilistic Sentence Representations from Paraphrases", "abstract": "Probabilistic word embeddings have shown effectiveness in capturing notions\nof generality and entailment, but there is very little work on doing the\nanalogous type of investigation for sentences. In this paper we define\nprobabilistic models that produce distributions for sentences. Our\nbest-performing model treats each word as a linear transformation operator\napplied to a multivariate Gaussian distribution. We train our models on\nparaphrases and demonstrate that they naturally capture sentence specificity.\nWhile our proposed model achieves the best performance overall, we also show\nthat specificity is represented by simpler architectures via the norm of the\nsentence vectors. Qualitative analysis shows that our probabilistic model\ncaptures sentential entailment and provides ways to analyze the specificity and\npreciseness of individual words.", "published": "2020-05-16 21:10:28", "link": "http://arxiv.org/abs/2005.08105v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RPD: A Distance Function Between Word Embeddings", "abstract": "It is well-understood that different algorithms, training processes, and\ncorpora produce different word embeddings. However, less is known about the\nrelation between different embedding spaces, i.e. how far different sets of\nembeddings deviate from each other. In this paper, we propose a novel metric\ncalled Relative pairwise inner Product Distance (RPD) to quantify the distance\nbetween different sets of word embeddings. This metric has a unified scale for\ncomparing different sets of word embeddings. Based on the properties of RPD, we\nstudy the relations of word embeddings of different algorithms systematically\nand investigate the influence of different training processes and corpora. The\nresults shed light on the poorly understood word embeddings and justify RPD as\na measure of the distance of embedding spaces.", "published": "2020-05-16 21:53:31", "link": "http://arxiv.org/abs/2005.08113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Arabic Offensive Language Detection Using Machine Learning and Ensemble\n  Machine Learning Approaches", "abstract": "This study aims at investigating the effect of applying single learner\nmachine learning approach and ensemble machine learning approach for offensive\nlanguage detection on Arabic language. Classifying Arabic social media text is\na very challenging task due to the ambiguity and informality of the written\nformat of the text. Arabic language has multiple dialects with diverse\nvocabularies and structures, which increase the complexity of obtaining high\nclassification performance. Our study shows significant impact for applying\nensemble machine learning approach over the single learner machine learning\napproach. Among the trained ensemble machine learning classifiers, bagging\nperforms the best in offensive language detection with F1 score of 88%, which\nexceeds the score obtained by the best single learner classifier by 6%. Our\nfindings highlight the great opportunities of investing more efforts in\npromoting the ensemble machine learning approach solutions for offensive\nlanguage detection models.", "published": "2020-05-16 06:40:36", "link": "http://arxiv.org/abs/2005.08946v1", "categories": ["cs.CL", "68W99"], "primary_category": "cs.CL"}
{"title": "Leveraging Affective Bidirectional Transformers for Offensive Language\n  Detection", "abstract": "Social media are pervasive in our life, making it necessary to ensure safe\nonline experiences by detecting and removing offensive and hate speech. In this\nwork, we report our submission to the Offensive Language and hate-speech\nDetection shared task organized with the 4th Workshop on Open-Source Arabic\nCorpora and Processing Tools Arabic (OSACT4). We focus on developing purely\ndeep learning systems, without a need for feature engineering. For that\npurpose, we develop an effective method for automatic data augmentation and\nshow the utility of training both offensive and hate speech models off (i.e.,\nby fine-tuning) previously trained affective models (i.e., sentiment and\nemotion). Our best models are significantly better than a vanilla BERT model,\nwith 89.60% acc (82.31% macro F1) for hate speech and 95.20% acc (70.51% macro\nF1) on official TEST data.", "published": "2020-05-16 04:55:35", "link": "http://arxiv.org/abs/2006.01266v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MicroNet for Efficient Language Modeling", "abstract": "It is important to design compact language models for efficient deployment.\nWe improve upon recent advances in both the language modeling domain and the\nmodel-compression domain to construct parameter and computation efficient\nlanguage models. We use an efficient transformer-based architecture with\nadaptive embedding and softmax, differentiable non-parametric cache, Hebbian\nsoftmax, knowledge distillation, network pruning, and low-bit quantization. In\nthis paper, we provide the winning solution to the NeurIPS 2019 MicroNet\nChallenge in the language modeling track. Compared to the baseline language\nmodel provided by the MicroNet Challenge, our model is 90 times more\nparameter-efficient and 36 times more computation-efficient while achieving the\nrequired test perplexity of 35 on the Wikitext-103 dataset. We hope that this\nwork will aid future research into efficient language models, and we have\nreleased our full source code at\nhttps://github.com/mit-han-lab/neurips-micronet.", "published": "2020-05-16 05:42:57", "link": "http://arxiv.org/abs/2005.07877v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Sequential Sentence Matching Network for Multi-turn Response Selection\n  in Retrieval-based Chatbots", "abstract": "Recently, open domain multi-turn chatbots have attracted much interest from\nlots of researchers in both academia and industry. The dominant retrieval-based\nmethods use context-response matching mechanisms for multi-turn response\nselection. Specifically, the state-of-the-art methods perform the\ncontext-response matching by word or segment similarity. However, these models\nlack a full exploitation of the sentence-level semantic information, and make\nsimple mistakes that humans can easily avoid. In this work, we propose a\nmatching network, called sequential sentence matching network (S2M), to use the\nsentence-level semantic information to address the problem. Firstly and most\nimportantly, we find that by using the sentence-level semantic information, the\nnetwork successfully addresses the problem and gets a significant improvement\non matching, resulting in a state-of-the-art performance. Furthermore, we\nintegrate the sentence matching we introduced here and the usual word\nsimilarity matching reported in the current literature, to match at different\nsemantic levels. Experiments on three public data sets show that such\nintegration further improves the model performance.", "published": "2020-05-16 09:47:19", "link": "http://arxiv.org/abs/2005.07923v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Embedding-based Detection of Lexical Semantic Changes", "abstract": "This paper describes EmbLexChange, a system introduced by the \"Life-Language\"\nteam for SemEval-2020 Task 1, on unsupervised detection of lexical-semantic\nchanges. EmbLexChange is defined as the divergence between the embedding based\nprofiles of word w (calculated with respect to a set of reference words) in the\nsource and the target domains (source and target domains can be simply two time\nframes t1 and t2). The underlying assumption is that the lexical-semantic\nchange of word w would affect its co-occurring words and subsequently alters\nthe neighborhoods in the embedding spaces. We show that using a resampling\nframework for the selection of reference words, we can reliably detect\nlexical-semantic changes in English, German, Swedish, and Latin. EmbLexChange\nachieved second place in the binary detection of semantic changes in the\nSemEval-2020.", "published": "2020-05-16 13:05:47", "link": "http://arxiv.org/abs/2005.07979v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IntelliCode Compose: Code Generation Using Transformer", "abstract": "In software development through integrated development environments (IDEs),\ncode completion is one of the most widely used features. Nevertheless, majority\nof integrated development environments only support completion of methods and\nAPIs, or arguments.\n  In this paper, we introduce IntelliCode Compose $-$ a general-purpose\nmultilingual code completion tool which is capable of predicting sequences of\ncode tokens of arbitrary types, generating up to entire lines of syntactically\ncorrect code. It leverages state-of-the-art generative transformer model\ntrained on 1.2 billion lines of source code in Python, $C\\#$, JavaScript and\nTypeScript programming languages. IntelliCode Compose is deployed as a\ncloud-based web service. It makes use of client-side tree-based caching,\nefficient parallel implementation of the beam search decoder, and compute graph\noptimizations to meet edit-time completion suggestion requirements in the\nVisual Studio Code IDE and Azure Notebook.\n  Our best model yields an average edit similarity of $86.7\\%$ and a perplexity\nof 1.82 for Python programming language.", "published": "2020-05-16 15:47:53", "link": "http://arxiv.org/abs/2005.08025v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Streaming Transformer-based Acoustic Models Using Self-attention with\n  Augmented Memory", "abstract": "Transformer-based acoustic modeling has achieved great suc-cess for both\nhybrid and sequence-to-sequence speech recogni-tion. However, it requires\naccess to the full sequence, and thecomputational cost grows quadratically with\nrespect to the in-put sequence length. These factors limit its adoption for\nstream-ing applications. In this work, we proposed a novel augmentedmemory\nself-attention, which attends on a short segment of theinput sequence and a\nbank of memories. The memory bankstores the embedding information for all the\nprocessed seg-ments. On the librispeech benchmark, our proposed\nmethodoutperforms all the existing streamable transformer methods bya large\nmargin and achieved over 15% relative error reduction,compared with the widely\nused LC-BLSTM baseline. Our find-ings are also confirmed on some large internal\ndatasets.", "published": "2020-05-16 16:54:52", "link": "http://arxiv.org/abs/2005.08042v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Recurrent Chunking Mechanisms for Long-Text Machine Reading\n  Comprehension", "abstract": "In this paper, we study machine reading comprehension (MRC) on long texts,\nwhere a model takes as inputs a lengthy document and a question and then\nextracts a text span from the document as an answer. State-of-the-art models\ntend to use a pretrained transformer model (e.g., BERT) to encode the joint\ncontextual information of document and question. However, these\ntransformer-based models can only take a fixed-length (e.g., 512) text as its\ninput. To deal with even longer text inputs, previous approaches usually chunk\nthem into equally-spaced segments and predict answers based on each segment\nindependently without considering the information from other segments. As a\nresult, they may form segments that fail to cover the correct answer span or\nretain insufficient contexts around it, which significantly degrades the\nperformance. Moreover, they are less capable of answering questions that need\ncross-segment information.\n  We propose to let a model learn to chunk in a more flexible way via\nreinforcement learning: a model can decide the next segment that it wants to\nprocess in either direction. We also employ recurrent mechanisms to enable\ninformation to flow across segments. Experiments on three MRC datasets -- CoQA,\nQuAC, and TriviaQA -- demonstrate the effectiveness of our proposed recurrent\nchunking mechanisms: we can obtain segments that are more likely to contain\ncomplete answers and at the same time provide sufficient contexts around the\nground truth answers for better predictions.", "published": "2020-05-16 18:08:58", "link": "http://arxiv.org/abs/2005.08056v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rethinking and Improving Natural Language Generation with Layer-Wise\n  Multi-View Decoding", "abstract": "In sequence-to-sequence learning, e.g., natural language generation, the\ndecoder relies on the attention mechanism to efficiently extract information\nfrom the encoder. While it is common practice to draw information from only the\nlast encoder layer, recent work has proposed to use representations from\ndifferent encoder layers for diversified levels of information. Nonetheless,\nthe decoder still obtains only a single view of the source sequences, which\nmight lead to insufficient training of the encoder layer stack due to the\nhierarchy bypassing problem. In this work, we propose layer-wise multi-view\ndecoding, where for each decoder layer, together with the representations from\nthe last encoder layer, which serve as a global view, those from other encoder\nlayers are supplemented for a stereoscopic view of the source sequences.\nSystematic experiments and analyses show that we successfully address the\nhierarchy bypassing problem, require almost negligible parameter increase, and\nsubstantially improve the performance of sequence-to-sequence learning with\ndeep representations on five diverse tasks, i.e., machine translation,\nabstractive summarization, image captioning, video captioning, medical report\ngeneration, and paraphrase generation. In particular, our approach achieves new\nstate-of-the-art results on ten benchmark datasets, including a low-resource\nmachine translation dataset and two low-resource medical report generation\ndatasets.", "published": "2020-05-16 20:00:39", "link": "http://arxiv.org/abs/2005.08081v7", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "iCapsNets: Towards Interpretable Capsule Networks for Text\n  Classification", "abstract": "Many text classification applications require models with satisfying\nperformance as well as good interpretability. Traditional machine learning\nmethods are easy to interpret but have low accuracies. The development of deep\nlearning models boosts the performance significantly. However, deep learning\nmodels are typically hard to interpret. In this work, we propose interpretable\ncapsule networks (iCapsNets) to bridge this gap. iCapsNets use capsules to\nmodel semantic meanings and explore novel methods to increase interpretability.\nThe design of iCapsNets is consistent with human intuition and enables it to\nproduce human-understandable interpretation results. Notably, iCapsNets can be\ninterpreted both locally and globally. In terms of local interpretability,\niCapsNets offer a simple yet effective method to explain the predictions for\neach data sample. On the other hand, iCapsNets explore a novel way to explain\nthe model's general behavior, achieving global interpretability. Experimental\nstudies show that our iCapsNets yield meaningful local and global\ninterpretation results, without suffering from significant performance loss\ncompared to non-interpretable methods.", "published": "2020-05-16 04:11:44", "link": "http://arxiv.org/abs/2006.00075v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Dialogic Instruction Detection for K-12 Online One-on-one\n  Classes", "abstract": "Online one-on-one class is created for highly interactive and immersive\nlearning experience. It demands a large number of qualified online instructors.\nIn this work, we develop six dialogic instructions and help teachers achieve\nthe benefits of one-on-one learning paradigm. Moreover, we utilize neural\nlanguage models, i.e., long short-term memory (LSTM), to detect above six\ninstructions automatically. Experiments demonstrate that the LSTM approach\nachieves AUC scores from 0.840 to 0.979 among all six types of instructions on\nour real-world educational dataset.", "published": "2020-05-16 01:55:31", "link": "http://arxiv.org/abs/2006.01204v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neural Multi-Task Learning for Teacher Question Detection in Online\n  Classrooms", "abstract": "Asking questions is one of the most crucial pedagogical techniques used by\nteachers in class. It not only offers open-ended discussions between teachers\nand students to exchange ideas but also provokes deeper student thought and\ncritical analysis. Providing teachers with such pedagogical feedback will\nremarkably help teachers improve their overall teaching quality over time in\nclassrooms. Therefore, in this work, we build an end-to-end neural framework\nthat automatically detects questions from teachers' audio recordings. Compared\nwith traditional methods, our approach not only avoids cumbersome feature\nengineering, but also adapts to the task of multi-class question detection in\nreal education scenarios. By incorporating multi-task learning techniques, we\nare able to strengthen the understanding of semantic relations among different\ntypes of questions. We conducted extensive experiments on the question\ndetection tasks in a real-world online classroom dataset and the results\ndemonstrate the superiority of our model in terms of various evaluation\nmetrics.", "published": "2020-05-16 02:17:04", "link": "http://arxiv.org/abs/2005.07845v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large scale weakly and semi-supervised learning for low-resource video\n  ASR", "abstract": "Many semi- and weakly-supervised approaches have been investigated for\novercoming the labeling cost of building high quality speech recognition\nsystems. On the challenging task of transcribing social media videos in\nlow-resource conditions, we conduct a large scale systematic comparison between\ntwo self-labeling methods on one hand, and weakly-supervised pretraining using\ncontextual metadata on the other. We investigate distillation methods at the\nframe level and the sequence level for hybrid, encoder-only CTC-based, and\nencoder-decoder speech recognition systems on Dutch and Romanian languages\nusing 27,000 and 58,000 hours of unlabeled audio respectively. Although all\napproaches improved upon their respective baseline WERs by more than 8%,\nsequence-level distillation for encoder-decoder models provided the largest\nrelative WER reduction of 20% compared to the strongest data-augmented\nsupervised baseline.", "published": "2020-05-16 03:08:45", "link": "http://arxiv.org/abs/2005.07850v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Glottal Source Estimation using an Automatic Chirp Decomposition", "abstract": "In a previous work, we showed that the glottal source can be estimated from\nspeech signals by computing the Zeros of the Z-Transform (ZZT). Decomposition\nwas achieved by separating the roots inside (causal contribution) and outside\n(anticausal contribution) the unit circle. In order to guarantee a correct\ndeconvolution, time alignment on the Glottal Closure Instants (GCIs) was shown\nto be essential. This paper extends the formalism of ZZT by evaluating the\nZ-transform on a contour possibly different from the unit circle. A method is\nproposed for determining automatically this contour by inspecting the root\ndistribution. The derived Zeros of the Chirp Z-Transform (ZCZT)-based technique\nturns out to be much more robust to GCI location errors.", "published": "2020-05-16 08:10:38", "link": "http://arxiv.org/abs/2005.07897v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Oscillating Statistical Moments for Speech Polarity Detection", "abstract": "An inversion of the speech polarity may have a dramatic detrimental effect on\nthe performance of various techniques of speech processing. An automatic method\nfor determining the speech polarity (which is dependent upon the recording\nsetup) is thus required as a preliminary step for ensuring the well-behaviour\nof such techniques. This paper proposes a new approach of polarity detection\nrelying on oscillating statistical moments. These moments have the property to\noscillate at the local fundamental frequency and to exhibit a phase shift which\ndepends on the speech polarity. This dependency stems from the introduction of\nnon-linearity or higher-order statistics in the moment calculation. The\nresulting method is shown on 10 speech corpora to provide a substantial\nimprovement compared to state-of-the-art techniques.", "published": "2020-05-16 08:16:43", "link": "http://arxiv.org/abs/2005.07901v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spike-Triggered Non-Autoregressive Transformer for End-to-End Speech\n  Recognition", "abstract": "Non-autoregressive transformer models have achieved extremely fast inference\nspeed and comparable performance with autoregressive sequence-to-sequence\nmodels in neural machine translation. Most of the non-autoregressive\ntransformers decode the target sequence from a predefined-length mask sequence.\nIf the predefined length is too long, it will cause a lot of redundant\ncalculations. If the predefined length is shorter than the length of the target\nsequence, it will hurt the performance of the model. To address this problem\nand improve the inference speed, we propose a spike-triggered\nnon-autoregressive transformer model for end-to-end speech recognition, which\nintroduces a CTC module to predict the length of the target sequence and\naccelerate the convergence. All the experiments are conducted on a public\nChinese mandarin dataset AISHELL-1. The results show that the proposed model\ncan accurately predict the length of the target sequence and achieve a\ncompetitive performance with the advanced transformers. What's more, the model\neven achieves a real-time factor of 0.0056, which exceeds all mainstream speech\nrecognition models.", "published": "2020-05-16 08:27:20", "link": "http://arxiv.org/abs/2005.07903v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reducing Spelling Inconsistencies in Code-Switching ASR using\n  Contextualized CTC Loss", "abstract": "Code-Switching (CS) remains a challenge for Automatic Speech Recognition\n(ASR), especially character-based models. With the combined choice of\ncharacters from multiple languages, the outcome from character-based models\nsuffers from phoneme duplication, resulting in language-inconsistent spellings.\nWe propose Contextualized Connectionist Temporal Classification (CCTC) loss to\nencourage spelling consistencies of a character-based non-autoregressive ASR\nwhich allows for faster inference. The CCTC loss conditions the main prediction\non the predicted contexts to ensure language consistency in the spellings. In\ncontrast to existing CTC-based approaches, CCTC loss does not require\nframe-level alignments, since the context ground truth is obtained from the\nmodel's estimated path. Compared to the same model trained with regular CTC\nloss, our method consistently improved the ASR performance on both CS and\nmonolingual corpora.", "published": "2020-05-16 09:36:58", "link": "http://arxiv.org/abs/2005.07920v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AccentDB: A Database of Non-Native English Accents to Assist Neural\n  Speech Recognition", "abstract": "Modern Automatic Speech Recognition (ASR) technology has evolved to identify\nthe speech spoken by native speakers of a language very well. However,\nidentification of the speech spoken by non-native speakers continues to be a\nmajor challenge for it. In this work, we first spell out the key requirements\nfor creating a well-curated database of speech samples in non-native accents\nfor training and testing robust ASR systems. We then introduce AccentDB, one\nsuch database that contains samples of 4 Indian-English accents collected by\nus, and a compilation of samples from 4 native-English, and a metropolitan\nIndian-English accent. We also present an analysis on separability of the\ncollected accent data. Further, we present several accent classification models\nand evaluate them thoroughly against human-labelled accent classes. We test the\ngeneralization of our classifier models in a variety of setups of seen and\nunseen data. Finally, we introduce the task of accent neutralization of\nnon-native accents to native accents using autoencoder models with\ntask-specific architectures. Thus, our work aims to aid ASR systems at every\nstage of development with a database for training, classification models for\nfeature augmentation, and neutralization systems for acoustic transformations\nof non-native accents of English.", "published": "2020-05-16 12:38:30", "link": "http://arxiv.org/abs/2005.07973v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Semi-supervised Learning for Multi-speaker Text-to-speech Synthesis\n  Using Discrete Speech Representation", "abstract": "Recently, end-to-end multi-speaker text-to-speech (TTS) systems gain success\nin the situation where a lot of high-quality speech plus their corresponding\ntranscriptions are available. However, laborious paired data collection\nprocesses prevent many institutes from building multi-speaker TTS systems of\ngreat performance. In this work, we propose a semi-supervised learning approach\nfor multi-speaker TTS. A multi-speaker TTS model can learn from the\nuntranscribed audio via the proposed encoder-decoder framework with discrete\nspeech representation. The experiment results demonstrate that with only an\nhour of paired speech data, no matter the paired data is from multiple speakers\nor a single speaker, the proposed model can generate intelligible speech in\ndifferent voices. We found the model can benefit from the proposed\nsemi-supervised learning approach even when part of the unpaired speech data is\nnoisy. In addition, our analysis reveals that different speaker characteristics\nof the paired data have an impact on the effectiveness of semi-supervised TTS.", "published": "2020-05-16 15:47:11", "link": "http://arxiv.org/abs/2005.08024v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Exploration of Audio Quality Assessment and Anomaly Localisation Using\n  Attention Models", "abstract": "Many applications of speech technology require more and more audio data.\nAutomatic assessment of the quality of the collected recordings is important to\nensure they meet the requirements of the related applications. However,\neffective and high performing assessment remains a challenging task without a\nclean reference. In this paper, a novel model for audio quality assessment is\nproposed by jointly using bidirectional long short-term memory and an attention\nmechanism. The former is to mimic a human auditory perception ability to learn\ninformation from a recording, and the latter is to further discriminate\ninterferences from desired signals by highlighting target related features. To\nevaluate our proposed approach, the TIMIT dataset is used and augmented by\nmixing with various natural sounds. In our experiments, two tasks are explored.\nThe first task is to predict an utterance quality score, and the second is to\nidentify where an anomalous distortion takes place in a recording. The obtained\nresults show that the use of our proposed approach outperforms a strong\nbaseline method and gains about 5% improvements after being measured by three\nmetrics, Linear Correlation Coefficient and Spearman Rank Correlation\nCoefficient, and F1.", "published": "2020-05-16 17:54:07", "link": "http://arxiv.org/abs/2005.08053v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "That Sounds Familiar: an Analysis of Phonetic Representations Transfer\n  Across Languages", "abstract": "Only a handful of the world's languages are abundant with the resources that\nenable practical applications of speech processing technologies. One of the\nmethods to overcome this problem is to use the resources existing in other\nlanguages to train a multilingual automatic speech recognition (ASR) model,\nwhich, intuitively, should learn some universal phonetic representations. In\nthis work, we focus on gaining a deeper understanding of how general these\nrepresentations might be, and how individual phones are getting improved in a\nmultilingual setting. To that end, we select a phonetically diverse set of\nlanguages, and perform a series of monolingual, multilingual and crosslingual\n(zero-shot) experiments. The ASR is trained to recognize the International\nPhonetic Alphabet (IPA) token sequences. We observe significant improvements\nacross all languages in the multilingual setting, and stark degradation in the\ncrosslingual setting, where the model, among other errors, considers Javanese\nas a tone language. Notably, as little as 10 hours of the target language\ntraining data tremendously reduces ASR error rates. Our analysis uncovered that\neven the phones that are unique to a single language can benefit greatly from\nadding training data from other languages - an encouraging result for the\nlow-resource speech community.", "published": "2020-05-16 22:28:09", "link": "http://arxiv.org/abs/2005.08118v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Dynamic Sparsity Neural Networks for Automatic Speech Recognition", "abstract": "In automatic speech recognition (ASR), model pruning is a widely adopted\ntechnique that reduces model size and latency to deploy neural network models\non edge devices with resource constraints. However, multiple models with\ndifferent sparsity levels usually need to be separately trained and deployed to\nheterogeneous target hardware with different resource specifications and for\napplications that have various latency requirements. In this paper, we present\nDynamic Sparsity Neural Networks (DSNN) that, once trained, can instantly\nswitch to any predefined sparsity configuration at run-time. We demonstrate the\neffectiveness and flexibility of DSNN using experiments on internal production\ndatasets with Google Voice Search data, and show that the performance of a DSNN\nmodel is on par with that of individually trained single sparsity networks. Our\ntrained DSNN model, therefore, can greatly ease the training process and\nsimplify deployment in diverse scenarios with resource constraints.", "published": "2020-05-16 22:08:54", "link": "http://arxiv.org/abs/2005.10627v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CERT: Contrastive Self-supervised Learning for Language Understanding", "abstract": "Pretrained language models such as BERT, GPT have shown great effectiveness\nin language understanding. The auxiliary predictive tasks in existing\npretraining approaches are mostly defined on tokens, thus may not be able to\ncapture sentence-level semantics very well. To address this issue, we propose\nCERT: Contrastive self-supervised Encoder Representations from Transformers,\nwhich pretrains language representation models using contrastive\nself-supervised learning at the sentence level. CERT creates augmentations of\noriginal sentences using back-translation. Then it finetunes a pretrained\nlanguage encoder (e.g., BERT) by predicting whether two augmented sentences\noriginate from the same sentence. CERT is simple to use and can be flexibly\nplugged into any pretraining-finetuning NLP pipeline. We evaluate CERT on 11\nnatural language understanding tasks in the GLUE benchmark where CERT\noutperforms BERT on 7 tasks, achieves the same performance as BERT on 2 tasks,\nand performs worse than BERT on 2 tasks. On the averaged score of the 11 tasks,\nCERT outperforms BERT. The data and code are available at\nhttps://github.com/UCSD-AI4H/CERT", "published": "2020-05-16 16:20:38", "link": "http://arxiv.org/abs/2005.12766v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improved Prosody from Learned F0 Codebook Representations for VQ-VAE\n  Speech Waveform Reconstruction", "abstract": "Vector Quantized Variational AutoEncoders (VQ-VAE) are a powerful\nrepresentation learning framework that can discover discrete groups of features\nfrom a speech signal without supervision. Until now, the VQ-VAE architecture\nhas previously modeled individual types of speech features, such as only phones\nor only F0. This paper introduces an important extension to VQ-VAE for learning\nF0-related suprasegmental information simultaneously along with traditional\nphone features.The proposed framework uses two encoders such that the F0\ntrajectory and speech waveform are both input to the system, therefore two\nseparate codebooks are learned. We used a WaveRNN vocoder as the decoder\ncomponent of VQ-VAE. Our speaker-independent VQ-VAE was trained with raw speech\nwaveforms from multi-speaker Japanese speech databases. Experimental results\nshow that the proposed extension reduces F0 distortion of reconstructed speech\nfor all unseen test speakers, and results in significantly higher preference\nscores from a listening test. We additionally conducted experiments using\nsingle-speaker Mandarin speech to demonstrate advantages of our architecture in\nanother language which relies heavily on F0.", "published": "2020-05-16 06:12:48", "link": "http://arxiv.org/abs/2005.07884v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Target Speech Extraction Based on Blind Source Separation and\n  X-vector-based Speaker Selection Trained with Data Augmentation", "abstract": "Extracting the desired speech from a mixture is a meaningful and challenging\ntask. The end-to-end DNN-based methods, though attractive, face the problem of\ngeneralization. In this paper, we explore a sequential approach for target\nspeech extraction by combining blind source separation (BSS) with the x-vector\nbased speaker recognition (SR) module. Two promising BSS methods based on\nsource independence assumption, independent low-rank matrix analysis (ILRMA)\nand multi-channel variational autoencoder (MVAE), are utilized and compared.\nILRMA employs nonnegative matrix factorization (NMF) to capture spectral\nstructures of source signals and MVAE utilizes the strong modeling power of\ndeep neural networks (DNN). However, the investigation of MVAE has been limited\nto the training with very few speakers and the speech signals of test speakers\nare usually included. We extend the training of MVAE using clean speech signals\nof 500 speakers to evaluate its generalization to unseen speakers. To improve\nthe correct extraction rate, two data augmentation strategies are implemented\nto train the SR module. The performance of the proposed cascaded approach is\ninvestigated with test data constructed with real room impulse responses under\nvaried environments.", "published": "2020-05-16 12:58:31", "link": "http://arxiv.org/abs/2005.07976v2", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The INTERSPEECH 2020 Far-Field Speaker Verification Challenge", "abstract": "The INTERSPEECH 2020 Far-Field Speaker Verification Challenge (FFSVC 2020)\naddresses three different research problems under well-defined conditions:\nfar-field text-dependent speaker verification from single microphone array,\nfar-field text-independent speaker verification from single microphone array,\nand far-field text-dependent speaker verification from distributed microphone\narrays. All three tasks pose a cross-channel challenge to the participants. To\nsimulate the real-life scenario, the enrollment utterances are recorded from\nclose-talk cellphone, while the test utterances are recorded from the far-field\nmicrophone arrays. In this paper, we describe the database, the challenge, and\nthe baseline system, which is based on a ResNet-based deep speaker network with\ncosine similarity scoring. For a given utterance, the speaker embeddings of\ndifferent channels are equally averaged as the final embedding. The baseline\nsystem achieves minDCFs of 0.62, 0.66, and 0.64 and EERs of 6.27%, 6.55%, and\n7.18% for task 1, task 2, and task 3, respectively.", "published": "2020-05-16 17:13:28", "link": "http://arxiv.org/abs/2005.08046v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Recognition and Multi-Speaker Diarization of Long Conversations", "abstract": "Speech recognition (ASR) and speaker diarization (SD) models have\ntraditionally been trained separately to produce rich conversation transcripts\nwith speaker labels. Recent advances have shown that joint ASR and SD models\ncan learn to leverage audio-lexical inter-dependencies to improve word\ndiarization performance. We introduce a new benchmark of hour-long podcasts\ncollected from the weekly This American Life radio program to better compare\nthese approaches when applied to extended multi-speaker conversations. We find\nthat training separate ASR and SD models perform better when utterance\nboundaries are known but otherwise joint models can perform better. To handle\nlong conversations with unknown utterance boundaries, we introduce a striding\nattention decoding algorithm and data augmentation techniques which, combined\nwith model pre-training, improves ASR and SD.", "published": "2020-05-16 19:29:33", "link": "http://arxiv.org/abs/2005.08072v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "abstract": "Recently Transformer and Convolution neural network (CNN) based models have\nshown promising results in Automatic Speech Recognition (ASR), outperforming\nRecurrent neural networks (RNNs). Transformer models are good at capturing\ncontent-based global interactions, while CNNs exploit local features\neffectively. In this work, we achieve the best of both worlds by studying how\nto combine convolution neural networks and transformers to model both local and\nglobal dependencies of an audio sequence in a parameter-efficient way. To this\nregard, we propose the convolution-augmented transformer for speech\nrecognition, named Conformer. Conformer significantly outperforms the previous\nTransformer and CNN based models achieving state-of-the-art accuracies. On the\nwidely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without\nusing a language model and 1.9%/3.9% with an external language model on\ntest/testother. We also observe competitive performance of 2.7%/6.3% with a\nsmall model of only 10M parameters.", "published": "2020-05-16 20:56:25", "link": "http://arxiv.org/abs/2005.08100v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sparse Mixture of Local Experts for Efficient Speech Enhancement", "abstract": "In this paper, we investigate a deep learning approach for speech denoising\nthrough an efficient ensemble of specialist neural networks. By splitting up\nthe speech denoising task into non-overlapping subproblems and introducing a\nclassifier, we are able to improve denoising performance while also reducing\ncomputational complexity. More specifically, the proposed model incorporates a\ngating network which assigns noisy speech signals to an appropriate specialist\nnetwork based on either speech degradation level or speaker gender. In our\nexperiments, a baseline recurrent network is compared against an ensemble of\nsimilarly-designed smaller recurrent networks regulated by the auxiliary gating\nnetwork. Using stochastically generated batches from a large noisy speech\ncorpus, the proposed model learns to estimate a time-frequency masking matrix\nbased on the magnitude spectrogram of an input mixture signal. Both baseline\nand specialist networks are trained to estimate the ideal ratio mask, while the\ngating network is trained to perform subproblem classification. Our findings\ndemonstrate that a fine-tuned ensemble network is able to exceed the speech\ndenoising capabilities of a generalist network, doing so with fewer model\nparameters.", "published": "2020-05-16 23:23:22", "link": "http://arxiv.org/abs/2005.08128v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Saving the Sonorine: Photovisual Audio Recovery Using Image Processing\n  and Computer Vision Techniques", "abstract": "This paper presents a novel technique to recover audio from sonorines, an\nearly 20th century form of analogue sound storage. Our method uses high\nresolution photographs of sonorines under different lighting conditions to\nobserve the change in reflection behavior of the physical surface features and\ncreate a three-dimensional height map of the surface. Sound can then be\nextracted using height information within the surface's grooves, mimicking a\nphysical stylus on a phonograph. Unlike traditional playback methods, our\nmethod has the advantage of being contactless: the medium will not incur damage\nand wear from being played repeatedly. We compare the results of our technique\nto a previously successful contactless method using flatbed scans of the\nsonorines, and conclude with future research that can be applied to this\nphotovisual approach to audio recovery.", "published": "2020-05-16 00:45:26", "link": "http://arxiv.org/abs/2005.08944v3", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Joint Articulatory-Acoustic Representations with Normalizing\n  Flows", "abstract": "The articulatory geometric configurations of the vocal tract and the acoustic\nproperties of the resultant speech sound are considered to have a strong causal\nrelationship. This paper aims at finding a joint latent representation between\nthe articulatory and acoustic domain for vowel sounds via invertible neural\nnetwork models, while simultaneously preserving the respective domain-specific\nfeatures. Our model utilizes a convolutional autoencoder architecture and\nnormalizing flow-based models to allow both forward and inverse mappings in a\nsemi-supervised manner, between the mid-sagittal vocal tract geometry of a two\ndegrees-of-freedom articulatory synthesizer with 1D acoustic wave model and the\nMel-spectrogram representation of the synthesized speech sounds. Our approach\nachieves satisfactory performance in achieving both articulatory-to-acoustic as\nwell as acoustic-to-articulatory mapping, thereby demonstrating our success in\nachieving a joint encoding of both the domains.", "published": "2020-05-16 04:34:36", "link": "http://arxiv.org/abs/2005.09463v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "The INTERSPEECH 2020 Deep Noise Suppression Challenge: Datasets,\n  Subjective Testing Framework, and Challenge Results", "abstract": "The INTERSPEECH 2020 Deep Noise Suppression (DNS) Challenge is intended to\npromote collaborative research in real-time single-channel Speech Enhancement\naimed to maximize the subjective (perceptual) quality of the enhanced speech. A\ntypical approach to evaluate the noise suppression methods is to use objective\nmetrics on the test set obtained by splitting the original dataset. While the\nperformance is good on the synthetic test set, often the model performance\ndegrades significantly on real recordings. Also, most of the conventional\nobjective metrics do not correlate well with subjective tests and lab\nsubjective tests are not scalable for a large test set. In this challenge, we\nopen-sourced a large clean speech and noise corpus for training the noise\nsuppression models and a representative test set to real-world scenarios\nconsisting of both synthetic and real recordings. We also open-sourced an\nonline subjective test framework based on ITU-T P.808 for researchers to\nreliably test their developments. We evaluated the results using P.808 on a\nblind test set. The results and the key learnings from the challenge are\ndiscussed. The datasets and scripts can be found here for quick access\nhttps://github.com/microsoft/DNS-Challenge.", "published": "2020-05-16 23:48:37", "link": "http://arxiv.org/abs/2005.13981v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
