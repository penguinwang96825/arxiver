{"title": "idT5: Indonesian Version of Multilingual T5 Transformer", "abstract": "Indonesian language is spoken by almost 200 million people and is the 10th\nmost spoken language in the world, but it is under-represented in NLP (Natural\nLanguage Processing) research. A sparsity of language resources has hampered\nprevious work on Indonesian. The Transformer is a new architecture rapidly\nbecoming dominant for NLP, surpassing alternatives like convolutional and\nrecurrent neural networks. T5 (Text-to-Text Transfer Transformer) is a\nTransformer model that converts all text-based language problems to\ntext-to-text format for English. The multilingual variant is mT5 (multilingual\nT5) which has shown promising results on many NLP tasks across languages.\nHowever, the size of this multilingual model is a drawback for its application\nin real production applications, which sometimes require only one language. In\nthis study, the mT5 model was adapted for only one language, Indonesian,\nresulting in a pre-trained T5 model that was specific only for Indonesian with\na smaller size. For performance comparison, we fine-tuned this model and the\nmT5 model to the Sentiment Analysis (SA), Question Generation (QG), and\nQuestion Answering (QA) tasks with the exact mechanism and dataset. Fine-tuned\nmodel based on our model achieved 77.18% accuracy on SA, 8% higher than the\nmT5-based model, and obtained nearly the same score as the mT5-based model on\nQG and QA. The results confirm that it is possible to produce a smaller\npre-trained model that maintains comparable yields while reducing the model\nsize by up to 58%. In addition, the resulting model requires less memory, loads\nfaster, and inference times faster.", "published": "2023-02-02 03:56:16", "link": "http://arxiv.org/abs/2302.00856v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Using In-Context Learning to Improve Dialogue Safety", "abstract": "While large neural-based conversational models have become increasingly\nproficient dialogue agents, recent work has highlighted safety issues with\nthese systems. For example, these systems can be goaded into generating toxic\ncontent, which often perpetuates social biases or stereotypes. We investigate a\nretrieval-based method for reducing bias and toxicity in responses from\nchatbots. It uses in-context learning to steer a model towards safer\ngenerations. Concretely, to generate a response to an unsafe dialogue context,\nwe retrieve demonstrations of safe responses to similar dialogue contexts. We\nfind our method performs competitively with strong baselines without requiring\ntraining. For instance, using automatic evaluation, we find our best fine-tuned\nbaseline only generates safe responses to unsafe dialogue contexts from\nDiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking\nprocedure which can further improve response safeness.", "published": "2023-02-02 04:46:03", "link": "http://arxiv.org/abs/2302.00871v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How to choose \"Good\" Samples for Text Data Augmentation", "abstract": "Deep learning-based text classification models need abundant labeled data to\nobtain competitive performance. Unfortunately, annotating large-size corpus is\ntime-consuming and laborious. To tackle this, multiple researches try to use\ndata augmentation to expand the corpus size. However, data augmentation may\npotentially produce some noisy augmented samples. There are currently no works\nexploring sample selection for augmented samples in nature language processing\nfield. In this paper, we propose a novel self-training selection framework with\ntwo selectors to select the high-quality samples from data augmentation.\nSpecifically, we firstly use an entropy-based strategy and the model prediction\nto select augmented samples. Considering some samples with high quality at the\nabove step may be wrongly filtered, we propose to recall them from two\nperspectives of word overlap and semantic similarity. Experimental results show\nthe effectiveness and simplicity of our framework.", "published": "2023-02-02 06:01:50", "link": "http://arxiv.org/abs/2302.00894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "History-Aware Hierarchical Transformer for Multi-session Open-domain\n  Dialogue System", "abstract": "With the evolution of pre-trained language models, current open-domain\ndialogue systems have achieved great progress in conducting one-session\nconversations. In contrast, Multi-Session Conversation (MSC), which consists of\nmultiple sessions over a long term with the same user, is under-investigated.\nIn this paper, we propose History-Aware Hierarchical Transformer (HAHT) for\nmulti-session open-domain dialogue. HAHT maintains a long-term memory of\nhistory conversations and utilizes history information to understand current\nconversation context and generate well-informed and context-relevant responses.\nSpecifically, HAHT first encodes history conversation sessions hierarchically\ninto a history memory. Then, HAHT leverages historical information to\nfacilitate the understanding of the current conversation context by encoding\nthe history memory together with the current context with attention-based\nmechanisms. Finally, to explicitly utilize historical information, HAHT uses a\nhistory-aware response generator that switches between a generic vocabulary and\na history-aware vocabulary. Experimental results on a large-scale MSC dataset\nsuggest that the proposed HAHT model consistently outperforms baseline models.\nHuman evaluation results support that HAHT generates more human-like,\ncontext-relevant and history-relevant responses than baseline models.", "published": "2023-02-02 06:54:33", "link": "http://arxiv.org/abs/2302.00907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransFool: An Adversarial Attack against Neural Machine Translation\n  Models", "abstract": "Deep neural networks have been shown to be vulnerable to small perturbations\nof their inputs, known as adversarial attacks. In this paper, we investigate\nthe vulnerability of Neural Machine Translation (NMT) models to adversarial\nattacks and propose a new attack algorithm called TransFool. To fool NMT\nmodels, TransFool builds on a multi-term optimization problem and a gradient\nprojection step. By integrating the embedding representation of a language\nmodel, we generate fluent adversarial examples in the source language that\nmaintain a high level of semantic similarity with the clean samples.\nExperimental results demonstrate that, for different translation tasks and NMT\narchitectures, our white-box attack can severely degrade the translation\nquality while the semantic similarity between the original and the adversarial\nsentences stays high. Moreover, we show that TransFool is transferable to\nunknown target models. Finally, based on automatic and human evaluations,\nTransFool leads to improvement in terms of success rate, semantic similarity,\nand fluency compared to the existing attacks both in white-box and black-box\nsettings. Thus, TransFool permits us to better characterize the vulnerability\nof NMT models and outlines the necessity to design strong defense mechanisms\nand more robust NMT systems for real-life applications.", "published": "2023-02-02 08:35:34", "link": "http://arxiv.org/abs/2302.00944v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "New Linear-time Algorithm for SubTree Kernel Computation based on\n  Root-Weighted Tree Automata", "abstract": "Tree kernels have been proposed to be used in many areas as the automatic\nlearning of natural language applications. In this paper, we propose a new\nlinear time algorithm based on the concept of weighted tree automata for\nSubTree kernel computation. First, we introduce a new class of weighted tree\nautomata, called Root-Weighted Tree Automata, and their associated formal tree\nseries. Then we define, from this class, the SubTree automata that represent\ncompact computational models for finite tree languages. This allows us to\ndesign a theoretically guaranteed linear-time algorithm for computing the\nSubTree Kernel based on weighted tree automata intersection. The key idea\nbehind the proposed algorithm is to replace DAG reduction and nodes sorting\nsteps used in previous approaches by states equivalence classes computation\nallowed in the weighted tree automata approach. Our approach has three major\nadvantages: it is output-sensitive, it is free sensitive from the tree types\n(ordered trees versus unordered trees), and it is well adapted to any\nincremental tree kernel based learning methods. Finally, we conduct a variety\nof comparative experiments on a wide range of synthetic tree languages datasets\nadapted for a deep algorithm analysis. The obtained results show that the\nproposed algorithm outperforms state-of-the-art methods.", "published": "2023-02-02 13:37:48", "link": "http://arxiv.org/abs/2302.01097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Deep Neural Reranking and Unsupervised Extraction for\n  Multi-Query Focused Summarization", "abstract": "The CrisisFACTS Track aims to tackle challenges such as multi-stream\nfact-finding in the domain of event tracking; participants' systems extract\nimportant facts from several disaster-related events while incorporating the\ntemporal order. We propose a combination of retrieval, reranking, and the\nwell-known Integer Linear Programming (ILP) and Maximal Marginal Relevance\n(MMR) frameworks. In the former two modules, we explore various methods\nincluding an entity-based baseline, pre-trained and fine-tuned Question\nAnswering systems, and ColBERT. We then use the latter module as an extractive\nsummarization component by taking diversity and novelty criteria into account.\nThe automatic scoring runs show strong results across the evaluation setups but\nalso reveal shortcomings and challenges.", "published": "2023-02-02 15:08:25", "link": "http://arxiv.org/abs/2302.01148v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Large Language Model Decoding with Speculative Sampling", "abstract": "We present speculative sampling, an algorithm for accelerating transformer\ndecoding by enabling the generation of multiple tokens from each transformer\ncall. Our algorithm relies on the observation that the latency of parallel\nscoring of short continuations, generated by a faster but less powerful draft\nmodel, is comparable to that of sampling a single token from the larger target\nmodel. This is combined with a novel modified rejection sampling scheme which\npreserves the distribution of the target model within hardware numerics. We\nbenchmark speculative sampling with Chinchilla, a 70 billion parameter language\nmodel, achieving a 2-2.5x decoding speedup in a distributed setup, without\ncompromising the sample quality or making modifications to the model itself.", "published": "2023-02-02 18:44:11", "link": "http://arxiv.org/abs/2302.01318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Curriculum-Guided Abstractive Summarization", "abstract": "Recent Transformer-based summarization models have provided a promising\napproach to abstractive summarization. They go beyond sentence selection and\nextractive strategies to deal with more complicated tasks such as novel word\ngeneration and sentence paraphrasing. Nonetheless, these models have two\nshortcomings: (1) they often perform poorly in content selection, and (2) their\ntraining strategy is not quite efficient, which restricts model performance. In\nthis paper, we explore two orthogonal ways to compensate for these pitfalls.\nFirst, we augment the Transformer network with a sentence cross-attention\nmodule in the decoder, encouraging more abstraction of salient content. Second,\nwe include a curriculum learning approach to reweight the training samples,\nbringing about an efficient learning procedure. Our second approach to enhance\nthe training strategy of Transformers networks makes stronger gains as compared\nto the first approach. We apply our model on extreme summarization dataset of\nReddit TIFU posts. We further look into three cross-domain summarization\ndatasets (Webis-TLDR-17, CNN/DM, and XSum), measuring the efficacy of\ncurriculum learning when applied in summarization. Moreover, a human evaluation\nis conducted to show the efficacy of the proposed method in terms of\nqualitative criteria, namely, fluency, informativeness, and overall quality.", "published": "2023-02-02 11:09:37", "link": "http://arxiv.org/abs/2302.01342v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The unreasonable effectiveness of few-shot learning for machine\n  translation", "abstract": "We demonstrate the potential of few-shot translation systems, trained with\nunpaired language data, for both high and low-resource language pairs. We show\nthat with only 5 examples of high-quality translation data shown at inference,\na transformer decoder-only model trained solely with self-supervised learning,\nis able to match specialized supervised state-of-the-art models as well as more\ngeneral commercial translation systems. In particular, we outperform the best\nperforming system on the WMT'21 English - Chinese news translation task by only\nusing five examples of English - Chinese parallel data at inference. Moreover,\nour approach in building these models does not necessitate joint multilingual\ntraining or back-translation, is conceptually simple and shows the potential to\nextend to the multilingual setting. Furthermore, the resulting models are two\norders of magnitude smaller than state-of-the-art language models. We then\nanalyze the factors which impact the performance of few-shot translation\nsystems, and highlight that the quality of the few-shot demonstrations heavily\ndetermines the quality of the translations generated by our models. Finally, we\nshow that the few-shot paradigm also provides a way to control certain\nattributes of the translation -- we show that we are able to control for\nregional varieties and formality using only a five examples at inference,\npaving the way towards controllable machine translation systems.", "published": "2023-02-02 20:19:46", "link": "http://arxiv.org/abs/2302.01398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Fewer Splits are Better: Deconstructing Readability in Sentence\n  Splitting", "abstract": "In this work, we focus on sentence splitting, a subfield of text\nsimplification, motivated largely by an unproven idea that if you divide a\nsentence in pieces, it should become easier to understand. Our primary goal in\nthis paper is to find out whether this is true. In particular, we ask, does it\nmatter whether we break a sentence into two or three? We report on our findings\nbased on Amazon Mechanical Turk.\n  More specifically, we introduce a Bayesian modeling framework to further\ninvestigate to what degree a particular way of splitting the complex sentence\naffects readability, along with a number of other parameters adopted from\ndiverse perspectives, including clinical linguistics, and cognitive\nlinguistics. The Bayesian modeling experiment provides clear evidence that\nbisecting the sentence leads to enhanced readability to a degree greater than\nwhat we create by trisection.", "published": "2023-02-02 08:25:48", "link": "http://arxiv.org/abs/2302.00937v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Curriculum-guided Abstractive Summarization for Mental Health Online\n  Posts", "abstract": "Automatically generating short summaries from users' online mental health\nposts could save counselors' reading time and reduce their fatigue so that they\ncan provide timely responses to those seeking help for improving their mental\nstate. Recent Transformers-based summarization models have presented a\npromising approach to abstractive summarization. They go beyond sentence\nselection and extractive strategies to deal with more complicated tasks such as\nnovel word generation and sentence paraphrasing. Nonetheless, these models have\na prominent shortcoming; their training strategy is not quite efficient, which\nrestricts the model's performance. In this paper, we include a curriculum\nlearning approach to reweigh the training samples, bringing about an efficient\nlearning procedure. We apply our model on extreme summarization dataset of\nMentSum posts -- a dataset of mental health related posts from Reddit social\nmedia. Compared to the state-of-the-art model, our proposed method makes\nsubstantial gains in terms of Rouge and Bertscore evaluation metrics, yielding\n3.5% (Rouge-1), 10.4% (Rouge-2), and 4.7% (Rouge-L), 1.5% (Bertscore) relative\nimprovements.", "published": "2023-02-02 08:48:26", "link": "http://arxiv.org/abs/2302.00954v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predefined domain specific embeddings of food concepts and recipes: A\n  case study on heterogeneous recipe datasets", "abstract": "Although recipe data are very easy to come by nowadays, it is really hard to\nfind a complete recipe dataset - with a list of ingredients, nutrient values\nper ingredient, and per recipe, allergens, etc. Recipe datasets are usually\ncollected from social media websites where users post and publish recipes.\nUsually written with little to no structure, using both standardized and\nnon-standardized units of measurement. We collect six different recipe\ndatasets, publicly available, in different formats, and some including data in\ndifferent languages. Bringing all of these datasets to the needed format for\napplying a machine learning (ML) pipeline for nutrient prediction [1], [2],\nincludes data normalization using dictionary-based named entity recognition\n(NER), rule-based NER, as well as conversions using external domain-specific\nresources. From the list of ingredients, domain-specific embeddings are created\nusing the same embedding space for all recipes - one ingredient dataset is\ngenerated. The result from this normalization process is two corpora - one with\npredefined ingredient embeddings and one with predefined recipe embeddings. On\nall six recipe datasets, the ML pipeline is evaluated. The results from this\nuse case also confirm that the embeddings merged using the domain heuristic\nyield better results than the baselines.", "published": "2023-02-02 10:49:06", "link": "http://arxiv.org/abs/2302.01005v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Coherence Markers for the Early Diagnosis of the Alzheimer\n  Disease", "abstract": "In this work we explore how language models can be employed to analyze\nlanguage and discriminate between mentally impaired and healthy subjects\nthrough the perplexity metric. Perplexity was originally conceived as an\ninformation-theoretic measure to assess how much a given language model is\nsuited to predict a text sequence or, equivalently, how much a word sequence\nfits into a specific language model. We carried out an extensive\nexperimentation with the publicly available data, and employed language models\nas diverse as N-grams, from 2-grams to 5-grams, and GPT-2, a transformer-based\nlanguage model. We investigated whether perplexity scores may be used to\ndiscriminate between the transcripts of healthy subjects and subjects suffering\nfrom Alzheimer Disease (AD). Our best performing models achieved full accuracy\nand F-score (1.00 in both precision/specificity and recall/sensitivity) in\ncategorizing subjects from both the AD class and control subjects. These\nresults suggest that perplexity can be a valuable analytical metrics with\npotential application to supporting early diagnosis of symptoms of mental\ndisorders.", "published": "2023-02-02 11:40:16", "link": "http://arxiv.org/abs/2302.01025v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Creating a Large Language Model of a Philosopher", "abstract": "Can large language models be trained to produce philosophical texts that are\ndifficult to distinguish from texts produced by human philosophers? To address\nthis question, we fine-tuned OpenAI's GPT-3 with the works of philosopher\nDaniel C. Dennett as additional training data. To explore the Dennett model, we\nasked the real Dennett ten philosophical questions and then posed the same\nquestions to the language model, collecting four responses for each question\nwithout cherry-picking. We recruited 425 participants to distinguish Dennett's\nanswer from the four machine-generated answers. Experts on Dennett's work (N =\n25) succeeded 51% of the time, above the chance rate of 20% but short of our\nhypothesized rate of 80% correct. For two of the ten questions, the language\nmodel produced at least one answer that experts selected more frequently than\nDennett's own answer. Philosophy blog readers (N = 302) performed similarly to\nthe experts, while ordinary research participants (N = 98) were near chance\ndistinguishing GPT-3's responses from those of an \"actual human philosopher\".", "published": "2023-02-02 01:10:26", "link": "http://arxiv.org/abs/2302.01339v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Commonsense-Aware Prompting for Controllable Empathetic Dialogue\n  Generation", "abstract": "Improving the emotional awareness of pre-trained language models is an\nemerging important problem for dialogue generation tasks. Although prior\nstudies have introduced methods to improve empathetic dialogue generation, few\nhave discussed how to incorporate commonsense knowledge into pre-trained\nlanguage models for controllable dialogue generation. In this study, we propose\na novel framework that improves empathetic dialogue generation using\npre-trained language models by 1) incorporating commonsense knowledge through\nprompt verbalization, and 2) controlling dialogue generation using a\nstrategy-driven future discriminator. We conducted experiments to reveal that\nboth the incorporation of social commonsense knowledge and enforcement of\ncontrol over generation help to improve generation performance. Finally, we\ndiscuss the implications of our study for future research.", "published": "2023-02-02 22:04:07", "link": "http://arxiv.org/abs/2302.01441v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CTE: A Dataset for Contextualized Table Extraction", "abstract": "Relevant information in documents is often summarized in tables, helping the\nreader to identify useful facts. Most benchmark datasets support either\ndocument layout analysis or table understanding, but lack in providing data to\napply both tasks in a unified way. We define the task of Contextualized Table\nExtraction (CTE), which aims to extract and define the structure of tables\nconsidering the textual context of the document. The dataset comprises 75k\nfully annotated pages of scientific papers, including more than 35k tables.\nData are gathered from PubMed Central, merging the information provided by\nannotations in the PubTables-1M and PubLayNet datasets. The dataset can support\nCTE and adds new classes to the original ones. The generated annotations can be\nused to develop end-to-end pipelines for various tasks, including document\nlayout analysis, table detection, structure recognition, and functional\nanalysis. We formally define CTE and evaluation metrics, showing which subtasks\ncan be tackled, describing advantages, limitations, and future works of this\ncollection of data. Annotations and code will be accessible a\nhttps://github.com/AILab-UniFI/cte-dataset.", "published": "2023-02-02 22:38:23", "link": "http://arxiv.org/abs/2302.01451v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Improving Rare Words Recognition through Homophone Extension and Unified\n  Writing for Low-resource Cantonese Speech Recognition", "abstract": "Homophone characters are common in tonal syllable-based languages, such as\nMandarin and Cantonese. The data-intensive end-to-end Automatic Speech\nRecognition (ASR) systems are more likely to mis-recognize homophone characters\nand rare words under low-resource settings. For the problem of lowresource\nCantonese speech recognition, this paper presents a novel homophone extension\nmethod to integrate human knowledge of the homophone lexicon into the beam\nsearch decoding process with language model re-scoring. Besides, we propose an\nautomatic unified writing method to merge the variants of Cantonese characters\nand standardize speech annotation guidelines, which enables more efficient\nutilization of labeled utterances by providing more samples for the merged\ncharacters. We empirically show that both homophone extension and unified\nwriting improve the recognition performance significantly on both in-domain and\nout-of-domain test sets, with an absolute Character Error Rate (CER) decrease\nof around 5% and 18%.", "published": "2023-02-02 02:46:32", "link": "http://arxiv.org/abs/2302.00836v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language Quantized AutoEncoders: Towards Unsupervised Text-Image\n  Alignment", "abstract": "Recent progress in scaling up large language models has shown impressive\ncapabilities in performing few-shot learning across a wide range of text-based\ntasks. However, a key limitation is that these language models fundamentally\nlack visual perception - a crucial attribute needed to extend these models to\nbe able to interact with the real world and solve vision tasks, such as in\nvisual-question answering and robotics. Prior works have largely connected\nimage to text through pretraining and/or fine-tuning on curated image-text\ndatasets, which can be a costly and expensive process. In order to resolve this\nlimitation, we propose a simple yet effective approach called\nLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to\nalign text-image data in an unsupervised manner by leveraging pretrained\nlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image as\nsequences of text tokens by directly quantizing image embeddings using a\npretrained language codebook. We then apply random masking followed by a BERT\nmodel, and have the decoder reconstruct the original image from BERT predicted\ntext token embeddings. By doing so, LQAE learns to represent similar images\nwith similar clusters of text tokens, thereby aligning these two modalities\nwithout the use of aligned text-image pairs. This enables few-shot image\nclassification with large language models (e.g., GPT-3) as well as linear\nclassification of images based on BERT text features. To the best of our\nknowledge, our work is the first work that uses unaligned images for multimodal\ntasks by leveraging the power of pretrained language models.", "published": "2023-02-02 06:38:44", "link": "http://arxiv.org/abs/2302.00902v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Multimodal Chain-of-Thought Reasoning in Language Models", "abstract": "Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies have primarily focused on the language modality. We\npropose Multimodal-CoT that incorporates language (text) and vision (images)\nmodalities into a two-stage framework that separates rationale generation and\nanswer inference. In this way, answer inference can leverage better generated\nrationales that are based on multimodal information. Experimental results on\nScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed\napproach. With Multimodal-CoT, our model under 1 billion parameters achieves\nstate-of-the-art performance on the ScienceQA benchmark. Our analysis indicates\nthat Multimodal-CoT offers the advantages of mitigating hallucination and\nenhancing convergence speed. Code is publicly available at\nhttps://github.com/amazon-science/mm-cot.", "published": "2023-02-02 07:51:19", "link": "http://arxiv.org/abs/2302.00923v5", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Large language models predict human sensory judgments across six\n  modalities", "abstract": "Determining the extent to which the perceptual world can be recovered from\nlanguage is a longstanding problem in philosophy and cognitive science. We show\nthat state-of-the-art large language models can unlock new insights into this\nproblem by providing a lower bound on the amount of perceptual information that\ncan be extracted from language. Specifically, we elicit pairwise similarity\njudgments from GPT models across six psychophysical datasets. We show that the\njudgments are significantly correlated with human data across all domains,\nrecovering well-known representations like the color wheel and pitch spiral.\nSurprisingly, we find that a model (GPT-4) co-trained on vision and language\ndoes not necessarily lead to improvements specific to the visual modality. To\nstudy the influence of specific languages on perception, we also apply the\nmodels to a multilingual color-naming task. We find that GPT-4 replicates\ncross-linguistic variation in English and Russian illuminating the interaction\nof language and perception.", "published": "2023-02-02 18:32:46", "link": "http://arxiv.org/abs/2302.01308v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Double Equivariance for Inductive Link Prediction for Both New Nodes and\n  New Relation Types", "abstract": "The task of fully inductive link prediction in knowledge graphs has gained\nsignificant attention, with various graph neural networks being proposed to\naddress it. This task presents greater challenges than traditional inductive\nlink prediction tasks with only new nodes, as models must be capable of\nzero-shot generalization to both unseen nodes and unseen relation types in the\ninference graph. Despite the development of novel models, a unifying\ntheoretical understanding of their success remains elusive, and the limitations\nof these methods are not well-studied. In this work, we introduce the concept\nof double permutation-equivariant representations and demonstrate its necessity\nfor effective performance in this task. We show that many existing models,\ndespite their diverse architectural designs, conform to this framework.\nHowever, we also identify inherent limitations in double\npermutation-equivariant representations, which restrict these models's ability\nto learn effectively on datasets with varying characteristics. Our findings\nsuggest that while double equivariance is necessary for meta-learning across\nknowledge graphs from different domains, it is not sufficient. There remains a\nfundamental gap between double permutation-equivariant models and the concept\nof foundation models designed to learn patterns across all domains.", "published": "2023-02-02 18:39:30", "link": "http://arxiv.org/abs/2302.01313v8", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "IC3: Image Captioning by Committee Consensus", "abstract": "If you ask a human to describe an image, they might do so in a thousand\ndifferent ways. Traditionally, image captioning models are trained to generate\na single \"best\" (most like a reference) image caption. Unfortunately, doing so\nencourages captions that are \"informationally impoverished,\" and focus on only\na subset of the possible details, while ignoring other potentially useful\ninformation in the scene. In this work, we introduce a simple, yet novel,\nmethod: \"Image Captioning by Committee Consensus\" (IC3), designed to generate a\nsingle caption that captures high-level details from several annotator\nviewpoints. Humans rate captions produced by IC3 at least as helpful as\nbaseline SOTA models more than two thirds of the time, and IC3 can improve the\nperformance of SOTA automated recall systems by up to 84%, outperforming single\nhuman-generated reference captions, and indicating significant improvements\nover SOTA approaches for visual description. Code is available at\nhttps://davidmchan.github.io/caption-by-committee/", "published": "2023-02-02 18:58:05", "link": "http://arxiv.org/abs/2302.01328v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Speech Enhancement for Virtual Meetings on Cellular Networks", "abstract": "We study speech enhancement using deep learning (DL) for virtual meetings on\ncellular devices, where transmitted speech has background noise and\ntransmission loss that affects speech quality. Since the Deep Noise Suppression\n(DNS) Challenge dataset does not contain practical disturbance, we collect a\ntransmitted DNS (t-DNS) dataset using Zoom Meetings over T-Mobile network. We\nselect two baseline models: Demucs and FullSubNet. The Demucs is an end-to-end\nmodel that takes time-domain inputs and outputs time-domain denoised speech,\nand the FullSubNet takes time-frequency-domain inputs and outputs the energy\nratio of the target speech in the inputs. The goal of this project is to\nenhance the speech transmitted over the cellular networks using deep learning\nmodels.", "published": "2023-02-02 04:35:48", "link": "http://arxiv.org/abs/2302.00868v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Goniometers are a Powerful Acoustic Feature for Music Information\n  Retrieval Tasks", "abstract": "Goniometers, also known as Phase Scopes or Vector Scopes, are audio metering\ntools that help music producers and mixing engineers monitor spatial aspects of\na music mix, such as the stereo panorama, the width of single sources, the\namount and diffuseness of reverberation as well as phase cancellations that may\noccur on the sweet-spot and in a mono-mixdown. In addition, they implicitly\ninform about the dynamics of the sound. Self-organizing maps trained with a\ngoniometer, are consulted to explore the usefulness of this acoustic feature\nfor music information retrieval tasks. One can see that goniometers are able to\nclassify different genres and cluster a single album. The advantage of\ngoniometers is the causality: Music producers and mixing engineers consciously\nconsult goniometers to reach their desired sound, which is not the case for\nother acoustic features, from Zero-Crossing Rate to Mel-Frequency Cepstral\nCoefficients.", "published": "2023-02-02 13:23:54", "link": "http://arxiv.org/abs/2302.01090v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Machine Learning Extreme Acoustic Non-reciprocity in a Linear Waveguide\n  with Multiple Nonlinear Asymmetric Gates", "abstract": "This work is a study of acoustic non-reciprocity exhibited by a passive\none-dimensional linear waveguide incorporating two local strongly nonlinear,\nasymmetric gates. Two local nonlinear gates break the symmetry and linearity of\nthe waveguide, yielding strong global non-reciprocal acoustics, in the way that\nextremely different acoustical responses occur depending on the side of\napplication of harmonic excitation. To the authors' best knowledge that the\npresent two-gated waveguide is capable of extremely high acoustic\nnon-reciprocity, at a much higher level to what is reported by active or\npassive devices in the current literature; moreover, this extreme performance\ncombines with acceptable levels of transmissibility in the desired direction of\nwave propagation. Machine learning is utilized for predictive design of this\ngated waveguide in terms of the measures of transmissibility and\nnon-reciprocity, with the aim of reducing the required computational time for\nhigh-dimensional parameter space analysis. The study sheds new light into the\nphysics of these media and considers the advantages and limitations of using\nneural networks to analyze this type of physical problems. In the predicted\ndesirable parameter space for intense non-reciprocity, the maximum\ntransmissibility reaches as much as 40%, and the transmitted energy from\nupstream to downstream varies up to nine orders of magnitude, depending on the\ndirection of wave transmission. The machine learning tools along with the\nnumerical methods of this work can inform predictive designs of practical\nnon-reciprocal waveguides and acoustic metamaterials that incorporate local\nnonlinear gates. The current paper shows that combinations of nonlinear gates\ncan lead to extremely high non-reciprocity while maintaining desired levels of\ntransmissibility.", "published": "2023-02-02 17:28:04", "link": "http://arxiv.org/abs/2302.01746v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Listen2Scene: Interactive material-aware binaural sound propagation for\n  reconstructed 3D scenes", "abstract": "We present an end-to-end binaural audio rendering approach (Listen2Scene) for\nvirtual reality (VR) and augmented reality (AR) applications. We propose a\nnovel neural-network-based binaural sound propagation method to generate\nacoustic effects for indoor 3D models of real environments. Any clean audio or\ndry audio can be convolved with the generated acoustic effects to render audio\ncorresponding to the real environment. We propose a graph neural network that\nuses both the material and the topology information of the 3D scenes and\ngenerates a scene latent vector. Moreover, we use a conditional generative\nadversarial network (CGAN) to generate acoustic effects from the scene latent\nvector. Our network can handle holes or other artifacts in the reconstructed 3D\nmesh model. We present an efficient cost function for the generator network to\nincorporate spatial audio effects. Given the source and the listener position,\nour learning-based binaural sound propagation approach can generate an acoustic\neffect in 0.1 milliseconds on an NVIDIA GeForce RTX 2080 Ti GPU. We have\nevaluated the accuracy of our approach with binaural acoustic effects generated\nusing an interactive geometric sound propagation algorithm and captured real\nacoustic effects / real-world recordings. We also performed a perceptual\nevaluation and observed that the audio rendered by our approach is more\nplausible than audio rendered using prior learning-based and geometric-based\nsound propagation algorithms. We quantitatively evaluated the accuracy of our\napproach using statistical acoustic parameters, and energy decay curves. The\ndemo videos, code and dataset are available online\n(https://anton-jeran.github.io/Listen2Scene/).", "published": "2023-02-02 04:09:23", "link": "http://arxiv.org/abs/2302.02809v4", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
