{"title": "EVJVQA Challenge: Multilingual Visual Question Answering", "abstract": "Visual Question Answering (VQA) is a challenging task of natural language\nprocessing (NLP) and computer vision (CV), attracting significant attention\nfrom researchers. English is a resource-rich language that has witnessed\nvarious developments in datasets and models for visual question answering.\nVisual question answering in other languages also would be developed for\nresources and models. In addition, there is no multilingual dataset targeting\nthe visual content of a particular country with its own objects and cultural\ncharacteristics. To address the weakness, we provide the research community\nwith a benchmark dataset named EVJVQA, including 33,000+ pairs of\nquestion-answer over three languages: Vietnamese, English, and Japanese, on\napproximately 5,000 images taken from Vietnam for evaluating multilingual VQA\nsystems or models. EVJVQA is used as a benchmark dataset for the challenge of\nmultilingual visual question answering at the 9th Workshop on Vietnamese\nLanguage and Speech Processing (VLSP 2022). This task attracted 62 participant\nteams from various universities and organizations. In this article, we present\ndetails of the organization of the challenge, an overview of the methods\nemployed by shared-task participants, and the results. The highest performances\nare 0.4392 in F1-score and 0.4009 in BLUE on the private test set. The\nmultilingual QA systems proposed by the top 2 teams use ViT for the pre-trained\nvision model and mT5 for the pre-trained language model, a powerful pre-trained\nlanguage model based on the transformer architecture. EVJVQA is a challenging\ndataset that motivates NLP and CV researchers to further explore the\nmultilingual models or systems for visual question answering systems. We\nreleased the challenge on the Codalab evaluation system for further research.", "published": "2023-02-23 02:38:39", "link": "http://arxiv.org/abs/2302.11752v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empathetic Response Generation via Emotion Cause Transition Graph", "abstract": "Empathetic dialogue is a human-like behavior that requires the perception of\nboth affective factors (e.g., emotion status) and cognitive factors (e.g.,\ncause of the emotion). Besides concerning emotion status in early work, the\nlatest approaches study emotion causes in empathetic dialogue. These approaches\nfocus on understanding and duplicating emotion causes in the context to show\nempathy for the speaker. However, instead of only repeating the contextual\ncauses, the real empathic response often demonstrate a logical and\nemotion-centered transition from the causes in the context to those in the\nresponses. In this work, we propose an emotion cause transition graph to\nexplicitly model the natural transition of emotion causes between two adjacent\nturns in empathetic dialogue. With this graph, the concept words of the emotion\ncauses in the next turn can be predicted and used by a specifically designed\nconcept-aware decoder to generate the empathic response. Automatic and human\nexperimental results on the benchmark dataset demonstrate that our method\nproduces more empathetic, coherent, informative, and specific responses than\nexisting models.", "published": "2023-02-23 05:51:17", "link": "http://arxiv.org/abs/2302.11787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coarse-to-Fine Knowledge Selection for Document Grounded Dialogs", "abstract": "Multi-document grounded dialogue systems (DGDS) belong to a class of\nconversational agents that answer users' requests by finding supporting\nknowledge from a collection of documents. Most previous studies aim to improve\nthe knowledge retrieval model or propose more effective ways to incorporate\nexternal knowledge into a parametric generation model. These methods, however,\nfocus on retrieving knowledge from mono-granularity language units (e.g.\npassages, sentences, or spans in documents), which is not enough to effectively\nand efficiently capture precise knowledge in long documents. This paper\nproposes Re3G, which aims to optimize both coarse-grained knowledge retrieval\nand fine-grained knowledge extraction in a unified framework. Specifically, the\nformer efficiently finds relevant passages in a retrieval-and-reranking\nprocess, whereas the latter effectively extracts finer-grain spans within those\npassages to incorporate into a parametric answer generation model (BART, T5).\nExperiments on DialDoc Shared Task demonstrate the effectiveness of our method.", "published": "2023-02-23 08:28:29", "link": "http://arxiv.org/abs/2302.11849v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Training of Mixture-of-Experts Language GANs", "abstract": "Despite the dramatic success in image generation, Generative Adversarial\nNetworks (GANs) still face great challenges in synthesizing sequences of\ndiscrete elements, in particular human language. The difficulty in generator\ntraining arises from the limited representation capacity and uninformative\nlearning signals obtained from the discriminator. In this work, we (1) first\nempirically show that the mixture-of-experts approach is able to enhance the\nrepresentation capacity of the generator for language GANs and (2) harness the\nFeature Statistics Alignment (FSA) paradigm to render fine-grained learning\nsignals to advance the generator training. Specifically, FSA forces the mean\nstatistics of the distribution of fake data to approach that of real samples as\nclose as possible in the finite-dimensional feature space. Empirical study on\nsynthetic and real benchmarks shows the superior performance in quantitative\nevaluation and demonstrates the effectiveness of our approach to adversarial\ntext generation.", "published": "2023-02-23 09:25:46", "link": "http://arxiv.org/abs/2302.11875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Agents and Children: Let Children Learn", "abstract": "Using online information discovery as a case study, in this position paper we\ndiscuss the need to design, develop, and deploy (conversational) agents that\ncan -- non-intrusively -- guide children in their quest for online resources\nrather than simply finding resources for them. We argue that agents should \"let\nchildren learn\" and should be built to take on a teacher-facilitator function,\nallowing children to develop their technical and critical thinking abilities as\nthey interact with varied technology in a broad range of use cases.", "published": "2023-02-23 14:12:03", "link": "http://arxiv.org/abs/2302.12043v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Social Media for Early Detection of Depression in COVID-19\n  Patients", "abstract": "The COVID-19 pandemic has caused substantial damage to global health. Even\nthough three years have passed, the world continues to struggle with the virus.\nConcerns are growing about the impact of COVID-19 on the mental health of\ninfected individuals, who are more likely to experience depression, which can\nhave long-lasting consequences for both the affected individuals and the world.\nDetection and intervention at an early stage can reduce the risk of depression\nin COVID-19 patients. In this paper, we investigated the relationship between\nCOVID-19 infection and depression through social media analysis. Firstly, we\nmanaged a dataset of COVID-19 patients that contains information about their\nsocial media activity both before and after infection. Secondly,We conducted an\nextensive analysis of this dataset to investigate the characteristic of\nCOVID-19 patients with a higher risk of depression. Thirdly, we proposed a deep\nneural network for early prediction of depression risk. This model considers\ndaily mood swings as a psychiatric signal and incorporates textual and\nemotional characteristics via knowledge distillation. Experimental results\ndemonstrate that our proposed framework outperforms baselines in detecting\ndepression risk, with an AUROC of 0.9317 and an AUPRC of 0.8116. Our model has\nthe potential to enable public health organizations to initiate prompt\nintervention with high-risk patients", "published": "2023-02-23 14:13:52", "link": "http://arxiv.org/abs/2302.12044v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Sentiment Transfer via Adaptive Masking", "abstract": "Sentiment transfer aims at revising the input text to satisfy a given\nsentiment polarity while retaining the original semantic content. The nucleus\nof sentiment transfer lies in precisely separating the sentiment information\nfrom the content information. Existing explicit approaches generally identify\nand mask sentiment tokens simply based on prior linguistic knowledge and\nmanually-defined rules, leading to low generality and undesirable transfer\nperformance. In this paper, we view the positions to be masked as the learnable\nparameters, and further propose a novel AM-ST model to learn adaptive\ntask-relevant masks based on the attention mechanism. Moreover, a\nsentiment-aware masked language model is further proposed to fill in the blanks\nin the masked positions by incorporating both context and sentiment polarity to\ncapture the multi-grained semantics comprehensively. AM-ST is thoroughly\nevaluated on two popular datasets, and the experimental results demonstrate the\nsuperiority of our proposal.", "published": "2023-02-23 14:17:34", "link": "http://arxiv.org/abs/2302.12045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Generalization Ability of Retrieval-Enhanced Transformers", "abstract": "Recent work on the Retrieval-Enhanced Transformer (RETRO) model has shown\nthat off-loading memory from trainable weights to a retrieval database can\nsignificantly improve language modeling and match the performance of\nnon-retrieval models that are an order of magnitude larger in size. It has been\nsuggested that at least some of this performance gain is due to non-trivial\ngeneralization based on both model weights and retrieval. In this paper, we try\nto better understand the relative contributions of these two components. We\nfind that the performance gains from retrieval largely originate from\noverlapping tokens between the database and the test data, suggesting less\nnon-trivial generalization than previously assumed. More generally, our results\npoint to the challenges of evaluating the generalization of retrieval-augmented\nlanguage models such as RETRO, as even limited token overlap may significantly\ndecrease test-time loss. We release our code and model at\nhttps://github.com/TobiasNorlund/retro", "published": "2023-02-23 16:11:04", "link": "http://arxiv.org/abs/2302.12128v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prosodic features improve sentence segmentation and parsing", "abstract": "Parsing spoken dialogue presents challenges that parsing text does not,\nincluding a lack of clear sentence boundaries. We know from previous work that\nprosody helps in parsing single sentences (Tran et al. 2018), but we want to\nshow the effect of prosody on parsing speech that isn't segmented into\nsentences. In experiments on the English Switchboard corpus, we find prosody\nhelps our model both with parsing and with accurately identifying sentence\nboundaries. However, we find that the best-performing parser is not necessarily\nthe parser that produces the best sentence segmentation performance. We suggest\nthat the best parses instead come from modelling sentence boundaries jointly\nwith other constituent boundaries.", "published": "2023-02-23 17:03:36", "link": "http://arxiv.org/abs/2302.12165v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and Scalable Nearest Neighbor Machine Translation", "abstract": "$k$NN-MT is a straightforward yet powerful approach for fast domain\nadaptation, which directly plugs pre-trained neural machine translation (NMT)\nmodels with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval\nto achieve domain adaptation without retraining. Despite being conceptually\nattractive, $k$NN-MT is burdened with massive storage requirements and high\ncomputational complexity since it conducts nearest neighbor searches over the\nentire reference corpus. In this paper, we propose a simple and scalable\nnearest neighbor machine translation framework to drastically promote the\ndecoding and storage efficiency of $k$NN-based models while maintaining the\ntranslation performance. To this end, we dynamically construct an extremely\nsmall datastore for each input via sentence-level retrieval to avoid searching\nthe entire datastore in vanilla $k$NN-MT, based on which we further introduce a\ndistance-aware adapter to adaptively incorporate the $k$NN retrieval results\ninto the pre-trained NMT models. Experiments on machine translation in two\ngeneral settings, static domain adaptation and online learning, demonstrate\nthat our proposed approach not only achieves almost 90% speed as the NMT model\nwithout performance degradation, but also significantly reduces the storage\nrequirements of $k$NN-MT.", "published": "2023-02-23 17:28:29", "link": "http://arxiv.org/abs/2302.12188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What makes a language easy to deep-learn? Deep neural networks and\n  humans similarly benefit from compositional structure", "abstract": "Deep neural networks drive the success of natural language processing. A\nfundamental property of language is its compositional structure, allowing\nhumans to systematically produce forms for new meanings. For humans, languages\nwith more compositional and transparent structures are typically easier to\nlearn than those with opaque and irregular structures. However, this\nlearnability advantage has not yet been shown for deep neural networks,\nlimiting their use as models for human language learning. Here, we directly\ntest how neural networks compare to humans in learning and generalizing\ndifferent languages that vary in their degree of compositional structure. We\nevaluate the memorization and generalization capabilities of a large language\nmodel and recurrent neural networks, and show that both deep neural networks\nexhibit a learnability advantage for more structured linguistic input: neural\nnetworks exposed to more compositional languages show more systematic\ngeneralization, greater agreement between different agents, and greater\nsimilarity to human learners.", "published": "2023-02-23 18:57:34", "link": "http://arxiv.org/abs/2302.12239v4", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Active Prompting with Chain-of-Thought for Large Language Models", "abstract": "The increasing scale of large language models (LLMs) brings emergent\nabilities to various complex tasks requiring reasoning, such as arithmetic and\ncommonsense reasoning. It is known that the effective design of task-specific\nprompts is critical for LLMs' ability to produce high-quality answers. In\nparticular, an effective approach for complex question-and-answer tasks is\nexample-based prompting with chain-of-thought (CoT) reasoning, which\nsignificantly improves the performance of LLMs. However, current CoT methods\nrely on a fixed set of human-annotated exemplars, which are not necessarily the\nmost effective examples for different tasks. This paper proposes a new method,\nActive-Prompt, to adapt LLMs to different tasks with task-specific example\nprompts (annotated with human-designed CoT reasoning). For this purpose, we\npropose a solution to the key problem of determining which questions are the\nmost important and helpful ones to annotate from a pool of task-specific\nqueries. By borrowing ideas from the related problem of uncertainty-based\nactive learning, we introduce several metrics to characterize the uncertainty\nso as to select the most uncertain questions for annotation. Experimental\nresults demonstrate the superiority of our proposed method, achieving\nstate-of-the-art on eight complex reasoning tasks. Further analyses of\ndifferent uncertainty metrics, pool sizes, zero-shot learning, and\naccuracy-uncertainty relationship demonstrate the effectiveness of our method.\nOur code will be available at https://github.com/shizhediao/active-prompt.", "published": "2023-02-23 18:58:59", "link": "http://arxiv.org/abs/2302.12246v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift\n  with Multiple Views", "abstract": "Temporal concept drift refers to the problem of data changing over time. In\nNLP, that would entail that language (e.g. new expressions, meaning shifts) and\nfactual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing\non the latter, we benchmark $11$ pretrained masked language models (MLMs) on a\nseries of tests designed to evaluate the effect of temporal concept drift, as\nit is crucial that widely used language models remain up-to-date with the\never-evolving factual updates of the real world. Specifically, we provide a\nholistic framework that (1) dynamically creates temporal test sets of any time\ngranularity (e.g. month, quarter, year) of factual data from Wikidata, (2)\nconstructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to\nensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways\n(single-token probing, multi-token generation, MLM scoring). In contrast to\nprior work, our framework aims to unveil how robust an MLM is over time and\nthus to provide a signal in case it has become outdated, by leveraging multiple\nviews of evaluation.", "published": "2023-02-23 19:24:55", "link": "http://arxiv.org/abs/2302.12297v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "In What Languages are Generative Language Models the Most Formal?\n  Analyzing Formality Distribution across Languages", "abstract": "Multilingual generative language models (LMs) are increasingly fluent in a\nlarge variety of languages. Trained on the concatenation of corpora in multiple\nlanguages, they enable powerful transfer from high-resource languages to\nlow-resource ones. However, it is still unknown what cultural biases are\ninduced in the predictions of these models. In this work, we focus on one\nlanguage property highly influenced by culture: formality. We analyze the\nformality distributions of XGLM and BLOOM's predictions, two popular generative\nmultilingual language models, in 5 languages. We classify 1,200 generations per\nlanguage as formal, informal, or incohesive and measure the impact of the\nprompt formality on the predictions. Overall, we observe a diversity of\nbehaviors across the models and languages. For instance, XGLM generates\ninformal text in Arabic and Bengali when conditioned with informal prompts,\nmuch more than BLOOM. In addition, even though both models are highly biased\ntoward the formal style when prompted neutrally, we find that the models\ngenerate a significant amount of informal predictions even when prompted with\nformal text. We release with this work 6,000 annotated samples, paving the way\nfor future work on the formality of generative multilingual LMs.", "published": "2023-02-23 19:39:52", "link": "http://arxiv.org/abs/2302.12299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summaries as Captions: Generating Figure Captions for Scientific\n  Documents with Automated Text Summarization", "abstract": "Good figure captions help paper readers understand complex scientific\nfigures. Unfortunately, even published papers often have poorly written\ncaptions. Automatic caption generation could aid paper writers by providing\ngood starting captions that can be refined for better quality. Prior work often\ntreated figure caption generation as a vision-to-language task. In this paper,\nwe show that it can be more effectively tackled as a text summarization task in\nscientific documents. We fine-tuned PEGASUS, a pre-trained abstractive\nsummarization model, to specifically summarize figure-referencing paragraphs\n(e.g., \"Figure 3 shows...\") into figure captions. Experiments on large-scale\narXiv figures show that our method outperforms prior vision methods in both\nautomatic and human evaluations. We further conducted an in-depth investigation\nfocused on two key challenges: (i) the common presence of low-quality\nauthor-written captions and (ii) the lack of clear standards for good captions.\nOur code and data are available at:\nhttps://github.com/Crowd-AI-Lab/Generating-Figure-Captions-as-a-Text-Summarization-Task.", "published": "2023-02-23 20:39:06", "link": "http://arxiv.org/abs/2302.12324v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MUTANT: A Multi-sentential Code-mixed Hinglish Dataset", "abstract": "The multi-sentential long sequence textual data unfolds several interesting\nresearch directions pertaining to natural language processing and generation.\nThough we observe several high-quality long-sequence datasets for English and\nother monolingual languages, there is no significant effort in building such\nresources for code-mixed languages such as Hinglish (code-mixing of\nHindi-English). In this paper, we propose a novel task of identifying\nmulti-sentential code-mixed text (MCT) from multilingual articles. As a use\ncase, we leverage multilingual articles from two different data sources and\nbuild a first-of-its-kind multi-sentential code-mixed Hinglish dataset i.e.,\nMUTANT. We propose a token-level language-aware pipeline and extend the\nexisting metrics measuring the degree of code-mixing to a multi-sentential\nframework and automatically identify MCT in the multilingual articles. The\nMUTANT dataset comprises 67k articles with 85k identified Hinglish MCTs. To\nfacilitate future research, we make the publicly available.", "published": "2023-02-23 04:04:18", "link": "http://arxiv.org/abs/2302.11766v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FiTs: Fine-grained Two-stage Training for Knowledge-aware Question\n  Answering", "abstract": "Knowledge-aware question answering (KAQA) requires the model to answer\nquestions over a knowledge base, which is essential for both open-domain QA and\ndomain-specific QA, especially when language models alone cannot provide all\nthe knowledge needed. Despite the promising result of recent KAQA systems which\ntend to integrate linguistic knowledge from pre-trained language models (PLM)\nand factual knowledge from knowledge graphs (KG) to answer complex questions, a\nbottleneck exists in effectively fusing the representations from PLMs and KGs\nbecause of (i) the semantic and distributional gaps between them, and (ii) the\ndifficulties in joint reasoning over the provided knowledge from both\nmodalities. To address the above two problems, we propose a Fine-grained\nTwo-stage training framework (FiTs) to boost the KAQA system performance: The\nfirst stage aims at aligning representations from the PLM and the KG, thus\nbridging the modality gaps between them, named knowledge adaptive\npost-training. The second stage, called knowledge-aware fine-tuning, aims to\nimprove the model's joint reasoning ability based on the aligned\nrepresentations. In detail, we fine-tune the post-trained model via two\nauxiliary self-supervised tasks in addition to the QA supervision. Extensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\non three benchmarks in the commonsense reasoning (i.e., CommonsenseQA,\nOpenbookQA) and medical question answering (i.e., MedQA-USMILE) domains.", "published": "2023-02-23 06:25:51", "link": "http://arxiv.org/abs/2302.11799v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teacher Intervention: Improving Convergence of Quantization Aware\n  Training for Ultra-Low Precision Transformers", "abstract": "Pre-trained Transformer models such as BERT have shown great success in a\nwide range of applications, but at the cost of substantial increases in model\ncomplexity. Quantization-aware training (QAT) is a promising method to lower\nthe implementation cost and energy consumption. However, aggressive\nquantization below 2-bit causes considerable accuracy degradation due to\nunstable convergence, especially when the downstream dataset is not abundant.\nThis work proposes a proactive knowledge distillation method called Teacher\nIntervention (TI) for fast converging QAT of ultra-low precision pre-trained\nTransformers. TI intervenes layer-wise signal propagation with the intact\nsignal from the teacher to remove the interference of propagated quantization\nerrors, smoothing loss surface of QAT and expediting the convergence.\nFurthermore, we propose a gradual intervention mechanism to stabilize the\nrecovery of subsections of Transformer layers from quantization. The proposed\nschemes enable fast convergence of QAT and improve the model accuracy\nregardless of the diverse characteristics of downstream fine-tuning tasks. We\ndemonstrate that TI consistently achieves superior accuracy with significantly\nlower fine-tuning iterations on well-known Transformers of natural language\nprocessing as well as computer vision compared to the state-of-the-art QAT\nmethods.", "published": "2023-02-23 06:48:24", "link": "http://arxiv.org/abs/2302.11812v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sentence Simplification via Large Language Models", "abstract": "Sentence Simplification aims to rephrase complex sentences into simpler\nsentences while retaining original meaning. Large Language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\ntasks. However, it is not yet known whether LLMs can be served as a\nhigh-quality sentence simplification system. In this work, we empirically\nanalyze the zero-/few-shot learning ability of LLMs by evaluating them on a\nnumber of benchmark test sets. Experimental results show LLMs outperform\nstate-of-the-art sentence simplification methods, and are judged to be on a par\nwith human annotators.", "published": "2023-02-23 12:11:58", "link": "http://arxiv.org/abs/2302.11957v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does Deep Learning Learn to Abstract? A Systematic Probing Framework", "abstract": "Abstraction is a desirable capability for deep learning models, which means\nto induce abstract concepts from concrete instances and flexibly apply them\nbeyond the learning context. At the same time, there is a lack of clear\nunderstanding about both the presence and further characteristics of this\ncapability in deep learning models. In this paper, we introduce a systematic\nprobing framework to explore the abstraction capability of deep learning models\nfrom a transferability perspective. A set of controlled experiments are\nconducted based on this framework, providing strong evidence that two probed\npre-trained language models (PLMs), T5 and GPT2, have the abstraction\ncapability. We also conduct in-depth analysis, thus shedding further light: (1)\nthe whole training phase exhibits a \"memorize-then-abstract\" two-stage process;\n(2) the learned abstract concepts are gathered in a few middle-layer attention\nheads, rather than being evenly distributed throughout the model; (3) the\nprobed abstraction capabilities exhibit robustness against concept mutations,\nand are more robust to low-level/source-side mutations than\nhigh-level/target-side ones; (4) generic pre-training is critical to the\nemergence of abstraction capability, and PLMs exhibit better abstraction with\nlarger model sizes and data scales.", "published": "2023-02-23 12:50:02", "link": "http://arxiv.org/abs/2302.11978v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Natural Language Processing in the Legal Domain", "abstract": "In this paper, we summarize the current state of the field of NLP & Law with\na specific focus on recent technical and substantive developments. To support\nour analysis, we construct and analyze a nearly complete corpus of more than\nsix hundred NLP & Law related papers published over the past decade. Our\nanalysis highlights several major trends. Namely, we document an increasing\nnumber of papers written, tasks undertaken, and languages covered over the\ncourse of the past decade. We observe an increase in the sophistication of the\nmethods which researchers deployed in this applied context. Slowly but surely,\nLegal NLP is beginning to match not only the methodological sophistication of\ngeneral NLP but also the professional standards of data availability and code\nreproducibility observed within the broader scientific community. We believe\nall of these trends bode well for the future of the field, but many questions\nin both the academic and commercial sphere still remain open.", "published": "2023-02-23 14:02:47", "link": "http://arxiv.org/abs/2302.12039v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep learning model for Mongolian Citizens Feedback Analysis using Word\n  Vector Embeddings", "abstract": "A large amount of feedback was collected over the years. Many feedback\nanalysis models have been developed focusing on the English language.\nRecognizing the concept of feedback is challenging and crucial in languages\nwhich do not have applicable corpus and tools employed in Natural Language\nProcessing (i.e., vocabulary corpus, sentence structure rules, etc). However,\nin this paper, we study a feedback classification in Mongolian language using\ntwo different word embeddings for deep learning. We compare the results of\nproposed approaches. We use feedback data in Cyrillic collected from 2012-2018.\nThe result indicates that word embeddings using their own dataset improve the\ndeep learning based proposed model with the best accuracy of 80.1% and 82.7%\nfor two classification tasks.", "published": "2023-02-23 14:49:31", "link": "http://arxiv.org/abs/2302.12069v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HL Dataset: Visually-grounded Description of Scenes, Actions and\n  Rationales", "abstract": "Current captioning datasets focus on object-centric captions, describing the\nvisible objects in the image, e.g. \"people eating food in a park\". Although\nthese datasets are useful to evaluate the ability of Vision & Language models\nto recognize and describe visual content, they do not support controlled\nexperiments involving model testing or fine-tuning, with more high-level\ncaptions, which humans find easy and natural to produce. For example, people\noften describe images based on the type of scene they depict ('people at a\nholiday resort') and the actions they perform ('people having a picnic'). Such\ndescriptions draw on personal experience and commonsense assumptions. We\npresent the High-Level Dataset a dataset extending 14997 images from the COCO\ndataset, aligned with a new set of 134,973 human-annotated (high-level)\ncaptions collected along three axes: scenes, actions, and rationales. We\nfurther extend this dataset with confidence scores collected from an\nindependent set of readers, as well as a set of narrative captions generated\nsynthetically, by combining each of the three axes. We describe this dataset\nand analyse it extensively. We also present baseline results for the High-Level\nCaptioning task.", "published": "2023-02-23 17:30:18", "link": "http://arxiv.org/abs/2302.12189v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "A Neural Span-Based Continual Named Entity Recognition Model", "abstract": "Named Entity Recognition (NER) models capable of Continual Learning (CL) are\nrealistically valuable in areas where entity types continuously increase (e.g.,\npersonal assistants). Meanwhile the learning paradigm of NER advances to new\npatterns such as the span-based methods. However, its potential to CL has not\nbeen fully explored. In this paper, we propose SpanKL, a simple yet effective\nSpan-based model with Knowledge distillation (KD) to preserve memories and\nmulti-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence\nlabeling approaches, the inherently independent modeling in span and entity\nlevel with the designed coherent optimization on SpanKL promotes its learning\nat each incremental step and mitigates the forgetting. Experiments on synthetic\nCL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly\noutperforms previous SoTA in many aspects, and obtains the smallest gap from CL\nto the upper bound revealing its high practiced value. The code is available at\nhttps://github.com/Qznan/SpanKL.", "published": "2023-02-23 17:51:29", "link": "http://arxiv.org/abs/2302.12200v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Testing AI on language comprehension tasks reveals insensitivity to\n  underlying meaning", "abstract": "Large Language Models (LLMs) are recruited in applications that span from\nclinical assistance and legal support to question answering and education.\nTheir success in specialized tasks has led to the claim that they possess\nhuman-like linguistic capabilities related to compositional understanding and\nreasoning. Yet, reverse-engineering is bound by Moravec's Paradox, according to\nwhich easy skills are hard. We systematically assess 7 state-of-the-art models\non a novel benchmark. Models answered a series of comprehension questions, each\nprompted multiple times in two settings, permitting one-word or open-length\nreplies. Each question targets a short text featuring high-frequency linguistic\nconstructions. To establish a baseline for achieving human-like performance, we\ntested 400 humans on the same prompts. Based on a dataset of n=26,680\ndatapoints, we discovered that LLMs perform at chance accuracy and waver\nconsiderably in their answers. Quantitatively, the tested models are\noutperformed by humans, and qualitatively their answers showcase distinctly\nnon-human errors in language understanding. We interpret this evidence as\nsuggesting that, despite their usefulness in various tasks, current AI models\nfall short of understanding language in a way that matches humans, and we argue\nthat this may be due to their lack of a compositional operator for regulating\ngrammatical and semantic information.", "published": "2023-02-23 20:18:52", "link": "http://arxiv.org/abs/2302.12313v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Pre-trained Vision and Language Models Answer Visual\n  Information-Seeking Questions?", "abstract": "Pre-trained vision and language models have demonstrated state-of-the-art\ncapabilities over existing tasks involving images and texts, including visual\nquestion answering. However, it remains unclear whether these models possess\nthe capability to answer questions that are not only querying visual content\nbut knowledge-intensive and information-seeking. In this study, we introduce\nInfoSeek, a visual question answering dataset tailored for information-seeking\nquestions that cannot be answered with only common sense knowledge. Using\nInfoSeek, we analyze various pre-trained visual question answering models and\ngain insights into their characteristics. Our findings reveal that\nstate-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)\nface challenges in answering visual information-seeking questions, but\nfine-tuning on the InfoSeek dataset elicits models to use fine-grained\nknowledge that was learned during their pre-training. Furthermore, we show that\naccurate visual entity recognition can be used to improve performance on\nInfoSeek by retrieving relevant documents, showing a significant space for\nimprovement.", "published": "2023-02-23 00:33:54", "link": "http://arxiv.org/abs/2302.11713v5", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Metric-oriented Speech Enhancement using Diffusion Probabilistic Model", "abstract": "Deep neural network based speech enhancement technique focuses on learning a\nnoisy-to-clean transformation supervised by paired training data. However, the\ntask-specific evaluation metric (e.g., PESQ) is usually non-differentiable and\ncan not be directly constructed in the training criteria. This mismatch between\nthe training objective and evaluation metric likely results in sub-optimal\nperformance. To alleviate it, we propose a metric-oriented speech enhancement\nmethod (MOSE), which leverages the recent advances in the diffusion\nprobabilistic model and integrates a metric-oriented training strategy into its\nreverse process. Specifically, we design an actor-critic based framework that\nconsiders the evaluation metric as a posterior reward, thus guiding the reverse\nprocess to the metric-increasing direction. The experimental results\ndemonstrate that MOSE obviously benefits from metric-oriented training and\nsurpasses the generative baselines in terms of all evaluation metrics.", "published": "2023-02-23 13:12:35", "link": "http://arxiv.org/abs/2302.11989v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating Automatic Speech Recognition in an Incremental Setting", "abstract": "The increasing reliability of automatic speech recognition has proliferated\nits everyday use. However, for research purposes, it is often unclear which\nmodel one should choose for a task, particularly if there is a requirement for\nspeed as well as accuracy. In this paper, we systematically evaluate six speech\nrecognizers using metrics including word error rate, latency, and the number of\nupdates to already recognized words on English test data, as well as propose\nand compare two methods for streaming audio into recognizers for incremental\nrecognition. We further propose Revokes per Second as a new metric for\nevaluating incremental recognition and demonstrate that it provides insights\ninto overall model performance. We find that, generally, local recognizers are\nfaster and require fewer updates than cloud-based recognizers. Finally, we find\nMeta's Wav2Vec model to be the fastest, and find Mozilla's DeepSpeech model to\nbe the most stable in its predictions.", "published": "2023-02-23 14:22:40", "link": "http://arxiv.org/abs/2302.12049v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SPINDLE: Spinning Raw Text into Lambda Terms with Graph Attention", "abstract": "This paper describes SPINDLE - an open source Python module implementing an\nefficient and accurate parser for written Dutch that transforms raw text input\nto programs for meaning composition, expressed as {\\lambda} terms. The parser\nintegrates a number of breakthrough advances made in recent years. Its output\nconsists of hi-res derivations of a multimodal type-logical grammar, capturing\ntwo orthogonal axes of syntax, namely deep function-argument structures and\ndependency relations. These are produced by three interdependent systems: a\nstatic type-checker asserting the well-formedness of grammatical analyses, a\nstate-of-the-art, structurally-aware supertagger based on heterogeneous graph\nconvolutions, and a massively parallel proof search component based on Sinkhorn\niterations. Packed in the software are also handy utilities and extras for\nproof visualization and inference, intended to facilitate end-user utilization.", "published": "2023-02-23 14:22:45", "link": "http://arxiv.org/abs/2302.12050v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "ProsAudit, a prosodic benchmark for self-supervised speech models", "abstract": "We present ProsAudit, a benchmark in English to assess structural prosodic\nknowledge in self-supervised learning (SSL) speech models. It consists of two\nsubtasks, their corresponding metrics, and an evaluation dataset. In the\nprotosyntax task, the model must correctly identify strong versus weak prosodic\nboundaries. In the lexical task, the model needs to correctly distinguish\nbetween pauses inserted between words and within words. We also provide human\nevaluation scores on this benchmark. We evaluated a series of SSL models and\nfound that they were all able to perform above chance on both tasks, even when\nevaluated on an unseen language. However, non-native models performed\nsignificantly worse than native ones on the lexical task, highlighting the\nimportance of lexical knowledge in this task. We also found a clear effect of\nsize with models trained on more data performing better in the two subtasks.", "published": "2023-02-23 14:30:23", "link": "http://arxiv.org/abs/2302.12057v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate\n  Political Stance Prediction", "abstract": "The political stance prediction for news articles has been widely studied to\nmitigate the echo chamber effect -- people fall into their thoughts and\nreinforce their pre-existing beliefs. The previous works for the political\nstance problem focus on (1) identifying political factors that could reflect\nthe political stance of a news article and (2) capturing those factors\neffectively. Despite their empirical successes, they are not sufficiently\njustified in terms of how effective their identified factors are in the\npolitical stance prediction. Motivated by this, in this work, we conduct a user\nstudy to investigate important factors in political stance prediction, and\nobserve that the context and tone of a news article (implicit) and external\nknowledge for real-world entities appearing in the article (explicit) are\nimportant in determining its political stance. Based on this observation, we\npropose a novel knowledge-aware approach to political stance prediction (KHAN),\nemploying (1) hierarchical attention networks (HAN) to learn the relationships\namong words and sentences in three different levels and (2) knowledge encoding\n(KE) to incorporate external knowledge for real-world entities into the process\nof political stance prediction. Also, to take into account the subtle and\nimportant difference between opposite political stances, we build two\nindependent political knowledge graphs (KG) (i.e., KG-lib and KG-con) by\nourselves and learn to fuse the different political knowledge. Through\nextensive evaluations on three real-world datasets, we demonstrate the\nsuperiority of DASH in terms of (1) accuracy, (2) efficiency, and (3)\neffectiveness.", "published": "2023-02-23 16:09:42", "link": "http://arxiv.org/abs/2302.12126v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Not what you've signed up for: Compromising Real-World LLM-Integrated\n  Applications with Indirect Prompt Injection", "abstract": "Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.", "published": "2023-02-23 17:14:38", "link": "http://arxiv.org/abs/2302.12173v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.CR"}
{"title": "MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for\n  Real-Time Fake News Mitigation in Social Media", "abstract": "The widespread availability of internet access and handheld devices confers\nto social media a power similar to the one newspapers used to have. People seek\naffordable information on social media and can reach it within seconds. Yet\nthis convenience comes with dangers; any user may freely post whatever they\nplease and the content can stay online for a long period, regardless of its\ntruthfulness. A need to detect untruthful information, also known as fake news,\narises. In this paper, we present an end-to-end solution that accurately\ndetects fake news and immunizes network nodes that spread them in real-time. To\ndetect fake news, we propose two new stack deep learning architectures that\nutilize convolutional and bidirectional LSTM layers. To mitigate the spread of\nfake news, we propose a real-time network-aware strategy that (1) constructs a\nminimum-cost weighted directed spanning tree for a detected node, and (2)\nimmunizes nodes in that tree by scoring their harmfulness using a novel ranking\nfunction. We demonstrate the effectiveness of our solution on five real-world\ndatasets.", "published": "2023-02-23 17:31:40", "link": "http://arxiv.org/abs/2302.12190v2", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.SI"}
{"title": "Federated Nearest Neighbor Machine Translation", "abstract": "To protect user privacy and meet legal regulations, federated learning (FL)\nis attracting significant attention. Training neural machine translation (NMT)\nmodels with traditional FL algorithm (e.g., FedAvg) typically relies on\nmulti-round model-based interactions. However, it is impractical and\ninefficient for machine translation tasks due to the vast communication\noverheads and heavy synchronization. In this paper, we propose a novel\nfederated nearest neighbor (FedNN) machine translation framework that, instead\nof multi-round model-based interactions, leverages one-round memorization-based\ninteraction to share knowledge across different clients to build low-overhead\nprivacy-preserving systems. The whole approach equips the public NMT model\ntrained on large-scale accessible data with a $k$-nearest-neighbor ($$kNN)\nclassifier and integrates the external datastore constructed by private text\ndata in all clients to form the final FL model. A two-phase datastore\nencryption strategy is introduced to achieve privacy-preserving during this\nprocess. Extensive experiments show that FedNN significantly reduces\ncomputational and communication costs compared with FedAvg, while maintaining\npromising performance in different FL settings.", "published": "2023-02-23 18:04:07", "link": "http://arxiv.org/abs/2302.12211v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical\n  Notes with Large Language Models", "abstract": "We propose CHiLL (Crafting High-Level Latents), an approach for\nnatural-language specification of features for linear models. CHiLL prompts\nLLMs with expert-crafted queries to generate interpretable features from health\nrecords. The resulting noisy labels are then used to train a simple linear\nclassifier. Generating features based on queries to an LLM can empower\nphysicians to use their domain expertise to craft features that are clinically\nmeaningful for a downstream task of interest, without having to manually\nextract these from raw EHR. We are motivated by a real-world risk prediction\ntask, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and\nstandard predictive tasks (e.g., 30-day readmission) to evaluate this approach.\nWe find that linear models using automatically extracted features are\ncomparably performant to models using reference features, and provide greater\ninterpretability than linear models using \"Bag-of-Words\" features. We verify\nthat learned feature weights align well with clinical expectations.", "published": "2023-02-23 21:23:06", "link": "http://arxiv.org/abs/2302.12343v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Victim Counts from Text", "abstract": "Decision-makers in the humanitarian sector rely on timely and exact\ninformation during crisis events. Knowing how many civilians were injured\nduring an earthquake is vital to allocate aids properly. Information about such\nvictim counts is often only available within full-text event descriptions from\nnewspapers and other reports. Extracting numbers from text is challenging:\nnumbers have different formats and may require numeric reasoning. This renders\npurely string matching-based approaches insufficient. As a consequence,\nfine-grained counts of injured, displaced, or abused victims beyond fatalities\nare often not extracted and remain unseen. We cast victim count extraction as a\nquestion answering (QA) task with a regression or classification objective. We\ncompare regex, dependency parsing, semantic role labeling-based approaches, and\nadvanced text-to-text models. Beyond model accuracy, we analyze extraction\nreliability and robustness which are key for this sensitive task. In\nparticular, we discuss model calibration and investigate few-shot and\nout-of-distribution performance. Ultimately, we make a comprehensive\nrecommendation on which model to select for different desiderata and data\ndomains. Our work is among the first to apply numeracy-focused large language\nmodels in a real-world use case with a positive impact.", "published": "2023-02-23 23:50:24", "link": "http://arxiv.org/abs/2302.12367v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; J.0"], "primary_category": "cs.CL"}
{"title": "Neural networks for learning personality traits from natural language", "abstract": "Personality is considered one of the most influential research topics in\npsychology, as it predicts many consequential outcomes such as mental and\nphysical health and explains human behaviour. With the widespread use of social\nnetworks as a means of communication, it is becoming increasingly important to\ndevelop models that can automatically and accurately read the essence of\nindividuals based solely on their writing. In particular, the convergence of\nsocial and computer sciences has led researchers to develop automatic\napproaches for extracting and studying \"hidden\" information in textual data on\nthe internet. The nature of this thesis project is highly experimental, and the\nmotivation behind this work is to present detailed analyses on the topic, as\ncurrently there are no significant investigations of this kind. The objective\nis to identify an adequate semantic space that allows for defining the\npersonality of the object to which a certain text refers. The starting point is\na dictionary of adjectives that psychological literature defines as markers of\nthe five major personality traits, or Big Five. In this work, we started with\nthe implementation of fully-connected neural networks as a basis for\nunderstanding how simple deep learning models can provide information on hidden\npersonality characteristics. Finally, we use a class of distributional\nalgorithms invented in 2013 by Tomas Mikolov, which consists of using a\nconvolutional neural network that learns the contexts of words in an\nunsupervised way. In this way, we construct an embedding that contains the\nsemantic information on the text, obtaining a kind of \"geometry of meaning\" in\nwhich concepts are translated into linear relationships. With this last\nexperiment, we hypothesize that an individual writing style is largely coupled\nwith their personality traits.", "published": "2023-02-23 10:33:40", "link": "http://arxiv.org/abs/2302.13782v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts\n  health answer correctness", "abstract": "Generative pre-trained language models (GPLMs) like ChatGPT encode in the\nmodel's parameters knowledge the models observe during the pre-training phase.\nThis knowledge is then used at inference to address the task specified by the\nuser in their prompt. For example, for the question-answering task, the GPLMs\nleverage the knowledge and linguistic patterns learned at training to produce\nan answer to a user question. Aside from the knowledge encoded in the model\nitself, answers produced by GPLMs can also leverage knowledge provided in the\nprompts. For example, a GPLM can be integrated into a retrieve-then-generate\nparadigm where a search engine is used to retrieve documents relevant to the\nquestion; the content of the documents is then transferred to the GPLM via the\nprompt. In this paper we study the differences in answer correctness generated\nby ChatGPT when leveraging the model's knowledge alone vs. in combination with\nthe prompt knowledge. We study this in the context of consumers seeking health\nadvice from the model. Aside from measuring the effectiveness of ChatGPT in\nthis context, we show that the knowledge passed in the prompt can overturn the\nknowledge encoded in the model and this is, in our experiments, to the\ndetriment of answer correctness. This work has important implications for the\ndevelopment of more robust and transparent question-answering systems based on\ngenerative pre-trained language models.", "published": "2023-02-23 22:14:01", "link": "http://arxiv.org/abs/2302.13793v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)", "abstract": "We study the performance of a commercially available large language model\n(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.\nTo our knowledge, this is the first independent evaluation of ChatGPT. We found\nthat ChatGPT's performance changes dramatically based on the requirement to\nshow its work, failing 20% of the time when it provides work compared with 84%\nwhen it does not. Further several factors about MWPs relating to the number of\nunknowns and number of operations that lead to a higher probability of failure\nwhen compared with the prior, specifically noting (across all experiments) that\nthe probability of failure increases linearly with the number of addition and\nsubtraction operations. We also have released the dataset of ChatGPT's\nresponses to the MWPs to support further work on the characterization of LLM\nperformance and present baseline machine learning models to predict if ChatGPT\ncan correctly answer an MWP. We have released a dataset comprised of ChatGPT's\nresponses to support further research in this area.", "published": "2023-02-23 16:06:16", "link": "http://arxiv.org/abs/2302.13814v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring celebrity influence on public attitude towards the COVID-19\n  pandemic: social media shared sentiment analysis", "abstract": "The COVID-19 pandemic has introduced new opportunities for health\ncommunication, including an increase in the public use of online outlets for\nhealth-related emotions. People have turned to social media networks to share\nsentiments related to the impacts of the COVID-19 pandemic. In this paper we\nexamine the role of social messaging shared by Persons in the Public Eye (i.e.\nathletes, politicians, news personnel) in determining overall public discourse\ndirection. We harvested approximately 13 million tweets ranging from 1 January\n2020 to 1 March 2022. The sentiment was calculated for each tweet using a\nfine-tuned DistilRoBERTa model, which was used to compare COVID-19\nvaccine-related Twitter posts (tweets) that co-occurred with mentions of People\nin the Public Eye. Our findings suggest the presence of consistent patterns of\nemotional content co-occurring with messaging shared by Persons in the Public\nEye for the first two years of the COVID-19 pandemic influenced public opinion\nand largely stimulated online public discourse. We demonstrate that as the\npandemic progressed, public sentiment shared on social networks was shaped by\nrisk perceptions, political ideologies and health-protective behaviours shared\nby Persons in the Public Eye, often in a negative light.", "published": "2023-02-23 21:56:50", "link": "http://arxiv.org/abs/2303.16759v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Data leakage in cross-modal retrieval training: A case study", "abstract": "The recent progress in text-based audio retrieval was largely propelled by\nthe release of suitable datasets. Since the manual creation of such datasets is\na laborious task, obtaining data from online resources can be a cheap solution\nto create large-scale datasets. We study the recently proposed SoundDesc\nbenchmark dataset, which was automatically sourced from the BBC Sound Effects\nweb page. In our analysis, we find that SoundDesc contains several duplicates\nthat cause leakage of training data to the evaluation data. This data leakage\nultimately leads to overly optimistic retrieval performance estimates in\nprevious benchmarks. We propose new training, validation, and testing splits\nfor the dataset that we make available online. To avoid weak contamination of\nthe test data, we pool audio files that share similar recording setups. In our\nexperiments, we find that the new splits serve as a more challenging benchmark.", "published": "2023-02-23 09:51:03", "link": "http://arxiv.org/abs/2302.12258v1", "categories": ["cs.SD", "cs.CL", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Quantifying & Modeling Multimodal Interactions: An Information\n  Decomposition Framework", "abstract": "The recent explosion of interest in multimodal applications has resulted in a\nwide selection of datasets and methods for representing and integrating\ninformation from different modalities. Despite these empirical advances, there\nremain fundamental research questions: How can we quantify the interactions\nthat are necessary to solve a multimodal task? Subsequently, what are the most\nsuitable multimodal models to capture these interactions? To answer these\nquestions, we propose an information-theoretic approach to quantify the degree\nof redundancy, uniqueness, and synergy relating input modalities with an output\ntask. We term these three measures as the PID statistics of a multimodal\ndistribution (or PID for short), and introduce two new estimators for these PID\nstatistics that scale to high-dimensional distributions. To validate PID\nestimation, we conduct extensive experiments on both synthetic datasets where\nthe PID is known and on large-scale multimodal benchmarks where PID estimations\nare compared with human annotations. Finally, we demonstrate their usefulness\nin (1) quantifying interactions within multimodal datasets, (2) quantifying\ninteractions captured by multimodal models, (3) principled approaches for model\nselection, and (4) three real-world case studies engaging with domain experts\nin pathology, mood prediction, and robotic perception where our framework helps\nto recommend strong multimodal models for each application.", "published": "2023-02-23 18:59:05", "link": "http://arxiv.org/abs/2302.12247v5", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Incorporating Uncertainty from Speaker Embedding Estimation to Speaker\n  Verification", "abstract": "Speech utterances recorded under differing conditions exhibit varying degrees\nof confidence in their embedding estimates, i.e., uncertainty, even if they are\nextracted using the same neural network. This paper aims to incorporate the\nuncertainty estimate produced in the xi-vector network front-end with a\nprobabilistic linear discriminant analysis (PLDA) back-end scoring for speaker\nverification. To achieve this we derive a posterior covariance matrix, which\nmeasures the uncertainty, from the frame-wise precisions to the embedding\nspace. We propose a log-likelihood ratio function for the PLDA scoring with the\nuncertainty propagation. We also propose to replace the length normalization\npre-processing technique with a length scaling technique for the application of\nuncertainty propagation in the back-end. Experimental results on the\nVoxCeleb-1, SITW test sets as well as a domain-mismatched CNCeleb1-E set show\nthe effectiveness of the proposed techniques with 14.5%-41.3% EER reductions\nand 4.6%-25.3% minDCF reductions.", "published": "2023-02-23 03:42:50", "link": "http://arxiv.org/abs/2302.11763v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Framework for Unified Real-time Personalized and Non-Personalized\n  Speech Enhancement", "abstract": "In this study, we present an approach to train a single speech enhancement\nnetwork that can perform both personalized and non-personalized speech\nenhancement. This is achieved by incorporating a frame-wise conditioning input\nthat specifies the type of enhancement output. To improve the quality of the\nenhanced output and mitigate oversuppression, we experiment with re-weighting\nframes by the presence or absence of speech activity and applying augmentations\nto speaker embeddings. By training under a multi-task learning setting, we\nempirically show that the proposed unified model obtains promising results on\nboth personalized and non-personalized speech enhancement benchmarks and\nreaches similar performance to models that are trained specialized for either\ntask. The strong performance of the proposed method demonstrates that the\nunified model is a more economical alternative compared to keeping separate\ntask-specific models during inference.", "published": "2023-02-23 04:10:54", "link": "http://arxiv.org/abs/2302.11768v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "D2Former: A Fully Complex Dual-Path Dual-Decoder Conformer Network using\n  Joint Complex Masking and Complex Spectral Mapping for Monaural Speech\n  Enhancement", "abstract": "Monaural speech enhancement has been widely studied using real networks in\nthe time-frequency (TF) domain. However, the input and the target are naturally\ncomplex-valued in the TF domain, a fully complex network is highly desirable\nfor effectively learning the feature representation and modelling the sequence\nin the complex domain. Moreover, phase, an important factor for perceptual\nquality of speech, has been proved learnable together with magnitude from noisy\nspeech using complex masking or complex spectral mapping. Many recent studies\nfocus on either complex masking or complex spectral mapping, ignoring their\nperformance boundaries. To address above issues, we propose a fully complex\ndual-path dual-decoder conformer network (D2Former) using joint complex masking\nand complex spectral mapping for monaural speech enhancement. In D2Former, we\nextend the conformer network into the complex domain and form a dual-path\ncomplex TF self-attention architecture for effectively modelling the\ncomplex-valued TF sequence. We further boost the TF feature representation in\nthe encoder and the decoders using a dual-path learning structure by exploiting\ncomplex dilated convolutions on time dependency and complex feedforward\nsequential memory networks (CFSMN) for frequency recurrence. In addition, we\nimprove the performance boundaries of complex masking and complex spectral\nmapping by combining the strengths of the two training targets into a\njoint-learning framework. As a consequence, D2Former takes fully advantages of\nthe complex-valued operations, the dual-path processing, and the joint-training\ntargets. Compared to the previous models, D2Former achieves state-of-the-art\nresults on the VoiceBank+Demand benchmark with the smallest model size of 0.87M\nparameters.", "published": "2023-02-23 07:43:49", "link": "http://arxiv.org/abs/2302.11832v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Frequency bin-wise single channel speech presence probability estimation\n  using multiple DNNs", "abstract": "In this work, we propose a frequency bin-wise method to estimate the\nsingle-channel speech presence probability (SPP) with multiple deep neural\nnetworks (DNNs) in the short-time Fourier transform domain. Since all frequency\nbins are typically considered simultaneously as input features for conventional\nDNN-based SPP estimators, high model complexity is inevitable. To reduce the\nmodel complexity and the requirements on the training data, we take a single\nfrequency bin and some of its neighboring frequency bins into account to train\nseparate gate recurrent units. In addition, the noisy speech and the a\nposteriori probability SPP representation are used to train our model. The\nexperiments were performed on the Deep Noise Suppression challenge dataset. The\nexperimental results show that the speech detection accuracy can be improved\nwhen we employ the frequency bin-wise model. Finally, we also demonstrate that\nour proposed method outperforms most of the state-of-the-art SPP estimation\nmethods in terms of speech detection accuracy and model complexity.", "published": "2023-02-23 14:20:13", "link": "http://arxiv.org/abs/2302.12048v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MossFormer: Pushing the Performance Limit of Monaural Speech Separation\n  using Gated Single-Head Transformer with Convolution-Augmented Joint\n  Self-Attentions", "abstract": "Transformer based models have provided significant performance improvements\nin monaural speech separation. However, there is still a performance gap\ncompared to a recent proposed upper bound. The major limitation of the current\ndual-path Transformer models is the inefficient modelling of long-range\nelemental interactions and local feature patterns. In this work, we achieve the\nupper bound by proposing a gated single-head transformer architecture with\nconvolution-augmented joint self-attentions, named \\textit{MossFormer}\n(\\textit{Mo}naural \\textit{s}peech \\textit{s}eparation Trans\\textit{Former}).\nTo effectively solve the indirect elemental interactions across chunks in the\ndual-path architecture, MossFormer employs a joint local and global\nself-attention architecture that simultaneously performs a full-computation\nself-attention on local chunks and a linearised low-cost self-attention over\nthe full sequence. The joint attention enables MossFormer model full-sequence\nelemental interaction directly. In addition, we employ a powerful attentive\ngating mechanism with simplified single-head self-attentions. Besides the\nattentive long-range modelling, we also augment MossFormer with convolutions\nfor the position-wise local pattern modelling. As a consequence, MossFormer\nsignificantly outperforms the previous models and achieves the state-of-the-art\nresults on WSJ0-2/3mix and WHAM!/WHAMR! benchmarks. Our model achieves the\nSI-SDRi upper bound of 21.2 dB on WSJ0-3mix and only 0.3 dB below the upper\nbound of 23.1 dB on WSJ0-2mix.", "published": "2023-02-23 07:17:12", "link": "http://arxiv.org/abs/2302.11824v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unsupervised Noise adaptation using Data Simulation", "abstract": "Deep neural network based speech enhancement approaches aim to learn a\nnoisy-to-clean transformation using a supervised learning paradigm. However,\nsuch a trained-well transformation is vulnerable to unseen noises that are not\nincluded in training set. In this work, we focus on the unsupervised noise\nadaptation problem in speech enhancement, where the ground truth of target\ndomain data is completely unavailable. Specifically, we propose a generative\nadversarial network based method to efficiently learn a converse clean-to-noisy\ntransformation using a few minutes of unpaired target domain data. Then this\ntransformation is utilized to generate sufficient simulated data for domain\nadaptation of the enhancement model. Experimental results show that our method\neffectively mitigates the domain mismatch between training and test sets, and\nsurpasses the best baseline by a large margin.", "published": "2023-02-23 12:57:20", "link": "http://arxiv.org/abs/2302.11981v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
