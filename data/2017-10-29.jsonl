{"title": "Personalized word representations Carrying Personalized Semantics\n  Learned from Social Network Posts", "abstract": "Distributed word representations have been shown to be very useful in various\nnatural language processing (NLP) application tasks. These word vectors learned\nfrom huge corpora very often carry both semantic and syntactic information of\nwords. However, it is well known that each individual user has his own language\npatterns because of different factors such as interested topics, friend groups,\nsocial activities, wording habits, etc., which may imply some kind of\npersonalized semantics. With such personalized semantics, the same word may\nimply slightly differently for different users. For example, the word\n\"Cappuccino\" may imply \"Leisure\", \"Joy\", \"Excellent\" for a user enjoying\ncoffee, by only a kind of drink for someone else. Such personalized semantics\nof course cannot be carried by the standard universal word vectors trained with\nhuge corpora produced by many people. In this paper, we propose a framework to\ntrain different personalized word vectors for different users based on the very\nsuccessful continuous skip-gram model using the social network data posted by\nmany individual users. In this framework, universal background word vectors are\nfirst learned from the background corpora, and then adapted by the personalized\ncorpus for each individual user to learn the personalized word vectors. We use\ntwo application tasks to evaluate the quality of the personalized word vectors\nobtained in this way, the user prediction task and the sentence completion\ntask. These personalized word vectors were shown to carry some personalized\nsemantics and offer improved performance on these two evaluation tasks.", "published": "2017-10-29 08:04:24", "link": "http://arxiv.org/abs/1710.10574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Path-Based Attention Neural Model for Fine-Grained Entity Typing", "abstract": "Fine-grained entity typing aims to assign entity mentions in the free text\nwith types arranged in a hierarchical structure. Traditional distant\nsupervision based methods employ a structured data source as a weak supervision\nand do not need hand-labeled data, but they neglect the label noise in the\nautomatically labeled training corpus. Although recent studies use many\nfeatures to prune wrong data ahead of training, they suffer from error\npropagation and bring much complexity. In this paper, we propose an end-to-end\ntyping model, called the path-based attention neural model (PAN), to learn a\nnoise- robust performance by leveraging the hierarchical structure of types.\nExperiments demonstrate its effectiveness.", "published": "2017-10-29 09:24:56", "link": "http://arxiv.org/abs/1710.10585v2", "categories": ["cs.CL", "I.2.4"], "primary_category": "cs.CL"}
{"title": "Evaluation of Automatic Video Captioning Using Direct Assessment", "abstract": "We present Direct Assessment, a method for manually assessing the quality of\nautomatically-generated captions for video. Evaluating the accuracy of video\ncaptions is particularly difficult because for any given video clip there is no\ndefinitive ground truth or correct answer against which to measure. Automatic\nmetrics for comparing automatic video captions against a manual caption such as\nBLEU and METEOR, drawn from techniques used in evaluating machine translation,\nwere used in the TRECVid video captioning task in 2016 but these are shown to\nhave weaknesses. The work presented here brings human assessment into the\nevaluation by crowdsourcing how well a caption describes a video. We\nautomatically degrade the quality of some sample captions which are assessed\nmanually and from this we are able to rate the quality of the human assessors,\na factor we take into account in the evaluation. Using data from the TRECVid\nvideo-to-text task in 2016, we show how our direct assessment method is\nreplicable and robust and should scale to where there many caption-generation\ntechniques to be evaluated.", "published": "2017-10-29 09:37:02", "link": "http://arxiv.org/abs/1710.10586v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Finding Dominant User Utterances And System Responses in Conversations", "abstract": "There are several dialog frameworks which allow manual specification of\nintents and rule based dialog flow. The rule based framework provides good\ncontrol to dialog designers at the expense of being more time consuming and\nlaborious. The job of a dialog designer can be reduced if we could identify\npairs of user intents and corresponding responses automatically from prior\nconversations between users and agents. In this paper we propose an approach to\nfind these frequent user utterances (which serve as examples for intents) and\ncorresponding agent responses. We propose a novel SimCluster algorithm that\nextends standard K-means algorithm to simultaneously cluster user utterances\nand agent utterances by taking their adjacency information into account. The\nmethod also aligns these clusters to provide pairs of intents and response\ngroups. We compare our results with those produced by using simple Kmeans\nclustering on a real dataset and observe upto 10% absolute improvement in\nF1-scores. Through our experiments on synthetic dataset, we show that our\nalgorithm gains more advantage over K-means algorithm when the data has large\nvariance.", "published": "2017-10-29 13:21:18", "link": "http://arxiv.org/abs/1710.10609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JESC: Japanese-English Subtitle Corpus", "abstract": "In this paper we describe the Japanese-English Subtitle Corpus (JESC). JESC\nis a large Japanese-English parallel corpus covering the underrepresented\ndomain of conversational dialogue. It consists of more than 3.2 million\nexamples, making it the largest freely available dataset of its kind. The\ncorpus was assembled by crawling and aligning subtitles found on the web. The\nassembly process incorporates a number of novel preprocessing elements to\nensure high monolingual fluency and accurate bilingual alignments. We summarize\nits contents and evaluate its quality using human experts and baseline machine\ntranslation (MT) systems.", "published": "2017-10-29 16:15:30", "link": "http://arxiv.org/abs/1710.10639v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple and Effective Multi-Paragraph Reading Comprehension", "abstract": "We consider the problem of adapting neural paragraph-level question answering\nmodels to the case where entire documents are given as input. Our proposed\nsolution trains models to produce well calibrated confidence scores for their\nresults on individual paragraphs. We sample multiple paragraphs from the\ndocuments during training, and use a shared-normalization training objective\nthat encourages the model to produce globally correct output. We combine this\nmethod with a state-of-the-art pipeline for training models on document QA\ndata. Experiments demonstrate strong performance on several document QA\ndatasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion\nof TriviaQA, a large improvement from the 56.7 F1 of the previous best system.", "published": "2017-10-29 23:47:49", "link": "http://arxiv.org/abs/1710.10723v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural-Symbolic Approach to Design of CAPTCHA", "abstract": "CAPTCHAs based on reading text are susceptible to machine-learning-based\nattacks due to recent significant advances in deep learning (DL). To address\nthis, this paper promotes image/visual captioning based CAPTCHAs, which is\nrobust against machine-learning-based attacks. To develop\nimage/visual-captioning-based CAPTCHAs, this paper proposes a new image\ncaptioning architecture by exploiting tensor product representations (TPR), a\nstructured neural-symbolic framework developed in cognitive science over the\npast 20 years, with the aim of integrating DL with explicit language structures\nand rules. We call it the Tensor Product Generation Network (TPGN). The key\nideas of TPGN are: 1) unsupervised learning of role-unbinding vectors of words\nvia a TPR-based deep neural network, and 2) integration of TPR with typical DL\narchitectures including Long Short-Term Memory (LSTM) models. The novelty of\nour approach lies in its ability to generate a sentence and extract partial\ngrammatical structure of the sentence by using role-unbinding vectors, which\nare obtained in an unsupervised manner. Experimental results demonstrate the\neffectiveness of the proposed approach.", "published": "2017-10-29 09:18:51", "link": "http://arxiv.org/abs/1710.11475v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
