{"title": "Unify word-level and span-level tasks: NJUNLP's Participation for the\n  WMT2023 Quality Estimation Shared Task", "abstract": "We introduce the submissions of the NJUNLP team to the WMT 2023 Quality\nEstimation (QE) shared task. Our team submitted predictions for the\nEnglish-German language pair on all two sub-tasks: (i) sentence- and word-level\nquality prediction; and (ii) fine-grained error span detection. This year, we\nfurther explore pseudo data methods for QE based on NJUQE framework\n(https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel\ndata from the WMT translation task. We pre-train the XLMR large model on pseudo\nQE data, then fine-tune it on real QE data. At both stages, we jointly learn\nsentence-level scores and word-level tags. Empirically, we conduct experiments\nto find the key hyper-parameters that improve the performance. Technically, we\npropose a simple method that covert the word-level outputs to fine-grained\nerror span results. Overall, our models achieved the best results in\nEnglish-German for both word-level and fine-grained error span detection\nsub-tasks by a considerable margin.", "published": "2023-09-23 01:52:14", "link": "http://arxiv.org/abs/2309.13230v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "User Simulation with Large Language Models for Evaluating Task-Oriented\n  Dialogue", "abstract": "One of the major impediments to the development of new task-oriented dialogue\n(TOD) systems is the need for human evaluation at multiple stages and\niterations of the development process. In an effort to move toward automated\nevaluation of TOD, we propose a novel user simulator built using recently\ndeveloped large pretrained language models (LLMs). In order to increase the\nlinguistic diversity of our system relative to the related previous work, we do\nnot fine-tune the LLMs used by our system on existing TOD datasets; rather we\nuse in-context learning to prompt the LLMs to generate robust and\nlinguistically diverse output with the goal of simulating the behavior of human\ninterlocutors. Unlike previous work, which sought to maximize goal success rate\n(GSR) as the primary metric of simulator performance, our goal is a system\nwhich achieves a GSR similar to that observed in human interactions with TOD\nsystems. Using this approach, our current simulator is effectively able to\ninteract with several TOD systems, especially on single-intent conversational\ngoals, while generating lexically and syntactically diverse output relative to\nprevious simulators that rely upon fine-tuned models. Finally, we collect a\nHuman2Bot dataset of humans interacting with the same TOD systems with which we\nexperimented in order to better quantify these achievements.", "published": "2023-09-23 02:04:57", "link": "http://arxiv.org/abs/2309.13233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education", "abstract": "The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale, real-world interactions between students and AI\nsystems still remain limited. In this study, we present ChEDDAR, ChatGPT & EFL\nLearner's Dialogue Dataset As Revising an essay, which is collected from a\nsemester-long longitudinal experiment involving 212 college students enrolled\nin English as Foreign Langauge (EFL) writing courses. The students were asked\nto revise their essays through dialogues with ChatGPT. ChEDDAR includes a\nconversation log, utterance-level essay edit history, self-rated satisfaction,\nand students' intent, in addition to session-level pre-and-post surveys\ndocumenting their objectives and overall experiences. We analyze students'\nusage patterns and perceptions regarding generative AI with respect to their\nintent and satisfaction. As a foundational step, we establish baseline results\nfor two pivotal tasks in task-oriented dialogue systems within educational\ncontexts: intent detection and satisfaction estimation. We finally suggest\nfurther research to refine the integration of generative AI into education\nsettings, outlining potential scenarios utilizing ChEDDAR. ChEDDAR is publicly\navailable at https://github.com/zeunie/ChEDDAR.", "published": "2023-09-23 03:28:25", "link": "http://arxiv.org/abs/2309.13243v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Document-Level Information Extraction", "abstract": "Document-level information extraction (IE) is a crucial task in natural\nlanguage processing (NLP). This paper conducts a systematic review of recent\ndocument-level IE literature. In addition, we conduct a thorough error analysis\nwith current state-of-the-art algorithms and identify their limitations as well\nas the remaining challenges for the task of document-level IE. According to our\nfindings, labeling noises, entity coreference resolution, and lack of\nreasoning, severely affect the performance of document-level IE. The objective\nof this survey paper is to provide more insights and help NLP researchers to\nfurther enhance document-level IE performance.", "published": "2023-09-23 04:18:24", "link": "http://arxiv.org/abs/2309.13249v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for\n  Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) delves into understanding sentiments\nspecific to distinct elements within a user-generated review. It aims to\nanalyze user-generated reviews to determine a) the target entity being\nreviewed, b) the high-level aspect to which it belongs, c) the sentiment words\nused to express the opinion, and d) the sentiment expressed toward the targets\nand the aspects. While various benchmark datasets have fostered advancements in\nABSA, they often come with domain limitations and data granularity challenges.\nAddressing these, we introduce the OATS dataset, which encompasses three fresh\ndomains and consists of 27,470 sentence-level quadruples and 17,092\nreview-level tuples. Our initiative seeks to bridge specific observed gaps: the\nrecurrent focus on familiar domains like restaurants and laptops, limited data\nfor intricate quadruple extraction tasks, and an occasional oversight of the\nsynergy between sentence and review-level sentiments. Moreover, to elucidate\nOATS's potential and shed light on various ABSA subtasks that OATS can solve,\nwe conducted experiments, establishing initial baselines. We hope the OATS\ndataset augments current resources, paving the way for an encompassing\nexploration of ABSA (https://github.com/RiTUAL-UH/OATS-ABSA).", "published": "2023-09-23 07:39:16", "link": "http://arxiv.org/abs/2309.13297v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating LLM-Based Evaluator", "abstract": "Recent advancements in large language models (LLMs) on language modeling and\nemergent capabilities make them a promising reference-free evaluator of natural\nlanguage generation quality, and a competent alternative to human evaluation.\nHowever, hindered by the closed-source or high computational demand to host and\ntune, there is a lack of practice to further calibrate an off-the-shelf\nLLM-based evaluator towards better human alignment. In this work, we propose\nAutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate\nand align an LLM-based evaluator toward human preference. Instead of explicitly\nmodeling human preferences, we first implicitly encompass them within a set of\nhuman labels. Then, an initial set of scoring criteria is drafted by the\nlanguage model itself, leveraging in-context learning on different few-shot\nexamples. To further calibrate this set of criteria, we select the best\nperformers and re-draft them with self-refinement. Our experiments on multiple\ntext quality evaluation datasets illustrate a significant improvement in\ncorrelation with expert evaluation through calibration. Our comprehensive\nqualitative analysis conveys insightful intuitions and observations on the\nessence of effective scoring criteria.", "published": "2023-09-23 08:46:11", "link": "http://arxiv.org/abs/2309.13308v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spanish Resource Grammar version 2023", "abstract": "We present the latest version of the Spanish Resource Grammar (SRG), a\ngrammar of Spanish implemented in the HPSG formalism. Such grammars encode a\ncomplex set of hypotheses about syntax making them a resource for empirical\ntesting of linguistic theory. They also encode a strict notion of\ngrammaticality which makes them a resource for natural language processing\napplications in computer-assisted language learning. This version of the SRG\nuses the recent version of the Freeling morphological analyzer and is released\nalong with an automatically created, manually verified treebank of 2,291\nsentences. We explain the treebanking process, emphasizing how it is different\nfrom treebanking with manual annotation and how it contributes to\nempirically-driven development of syntactic theory. The treebanks' high level\nof consistency and detail makes them a resource for training high-quality\nsemantic parsers and generally systems that benefit from precise and detailed\nsemantics. Finally, we present the grammar's coverage and overgeneration on 100\nsentences from a learner corpus, a new research line related to developing\nmethodologies for robust empirical evaluation of hypotheses in second language\nacquisition.", "published": "2023-09-23 09:24:05", "link": "http://arxiv.org/abs/2309.13318v2", "categories": ["cs.CL", "03B65"], "primary_category": "cs.CL"}
{"title": "GlotScript: A Resource and Tool for Low Resource Writing System\n  Identification", "abstract": "We present GlotScript, an open resource and tool for low resource writing\nsystem identification. GlotScript-R is a resource that provides the attested\nwriting systems for more than 7,000 languages. It is compiled by aggregating\ninformation from existing writing system resources. GlotScript-T is a writing\nsystem identification tool that covers all 161 Unicode 15.0 scripts. For an\ninput text, it returns its script distribution where scripts are identified by\nISO 15924 codes. We also present two use cases for GlotScript. First, we\ndemonstrate that GlotScript can help cleaning multilingual corpora such as mC4\nand OSCAR. Second, we analyze the tokenization of a number of language models\nsuch as GPT-4 using GlotScript and provide insights on the coverage of low\nresource scripts and languages by each language model. We hope that GlotScript\nwill become a useful resource for work on low resource languages in the NLP\ncommunity. GlotScript-R and GlotScript-T are available at\nhttps://github.com/cisnlp/GlotScript.", "published": "2023-09-23 09:35:55", "link": "http://arxiv.org/abs/2309.13320v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Text to Source: Results in Detecting Large Language Model-Generated\n  Content", "abstract": "The widespread use of Large Language Models (LLMs), celebrated for their\nability to generate human-like text, has raised concerns about misinformation\nand ethical implications. Addressing these concerns necessitates the\ndevelopment of robust methods to detect and attribute text generated by LLMs.\nThis paper investigates \"Cross-Model Detection,\" by evaluating whether a\nclassifier trained to distinguish between source LLM-generated and\nhuman-written text can also detect text from a target LLM without further\ntraining. The study comprehensively explores various LLM sizes and families,\nand assesses the impact of conversational fine-tuning techniques, quantization,\nand watermarking on classifier generalization. The research also explores Model\nAttribution, encompassing source model identification, model family, and model\nsize classification, in addition to quantization and watermarking detection.\nOur results reveal several key findings: a clear inverse relationship between\nclassifier effectiveness and model size, with larger LLMs being more\nchallenging to detect, especially when the classifier is trained on data from\nsmaller models. Training on data from similarly sized LLMs can improve\ndetection performance from larger models but may lead to decreased performance\nwhen dealing with smaller models. Additionally, model attribution experiments\nshow promising results in identifying source models and model families,\nhighlighting detectable signatures in LLM-generated text, with particularly\nremarkable outcomes in watermarking detection, while no detectable signatures\nof quantization were observed. Overall, our study contributes valuable insights\ninto the interplay of model size, family, and training data in LLM detection\nand attribution.", "published": "2023-09-23 09:51:37", "link": "http://arxiv.org/abs/2309.13322v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling\n  Capacities of Large Language Models", "abstract": "Large language models (LLMs) have achieved dramatic proficiency over NLP\ntasks with normal length. Recently, multiple studies have committed to\nextending the context length and enhancing the long text modeling capabilities\nof LLMs. To comprehensively evaluate the long context ability of LLMs, we\npropose BAMBOO, a multi-task long context benchmark. BAMBOO has been designed\nwith four principles: comprehensive capacity evaluation, avoidance of data\ncontamination, accurate automatic evaluation, and different length levels. It\nconsists of 10 datasets from 5 different long text understanding tasks, i.e.\nquestion answering, hallucination detection, text sorting, language modeling,\nand code completion, to cover core capacities and various domains of LLMs. We\nconduct experiments with five long context models on BAMBOO and further discuss\nfour key research questions of long text. We also qualitatively analyze current\nlong context models and point out future directions for enhancing long text\nmodeling capacities. We release our data, prompts, and code at\nhttps://github.com/RUCAIBox/BAMBOO.", "published": "2023-09-23 11:36:15", "link": "http://arxiv.org/abs/2309.13345v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounding Description-Driven Dialogue State Trackers with\n  Knowledge-Seeking Turns", "abstract": "Schema-guided dialogue state trackers can generalise to new domains without\nfurther training, yet they are sensitive to the writing style of the schemata.\nAugmenting the training set with human or synthetic schema paraphrases improves\nthe model robustness to these variations but can be either costly or difficult\nto control. We propose to circumvent these issues by grounding the state\ntracking model in knowledge-seeking turns collected from the dialogue corpus as\nwell as the schema. Including these turns in prompts during finetuning and\ninference leads to marked improvements in model robustness, as demonstrated by\nlarge average joint goal accuracy and schema sensitivity improvements on SGD\nand SGD-X.", "published": "2023-09-23 18:33:02", "link": "http://arxiv.org/abs/2309.13448v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hindi to English: Transformer-Based Neural Machine Translation", "abstract": "Machine Translation (MT) is one of the most prominent tasks in Natural\nLanguage Processing (NLP) which involves the automatic conversion of texts from\none natural language to another while preserving its meaning and fluency.\nAlthough the research in machine translation has been going on since multiple\ndecades, the newer approach of integrating deep learning techniques in natural\nlanguage processing has led to significant improvements in the translation\nquality. In this paper, we have developed a Neural Machine Translation (NMT)\nsystem by training the Transformer model to translate texts from Indian\nLanguage Hindi to English. Hindi being a low resource language has made it\ndifficult for neural networks to understand the language thereby leading to a\nslow growth in the development of neural machine translators. Thus, to address\nthis gap, we implemented back-translation to augment the training data and for\ncreating the vocabulary, we experimented with both word and subword level\ntokenization using Byte Pair Encoding (BPE) thereby ending up training the\nTransformer in 10 different configurations. This led us to achieve a\nstate-of-the-art BLEU score of 24.53 on the test set of IIT Bombay\nEnglish-Hindi Corpus in one of the configurations.", "published": "2023-09-23 00:00:09", "link": "http://arxiv.org/abs/2309.13222v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for Requirements Formalization: How to\n  Derive New Approaches?", "abstract": "It is a long-standing desire of industry and research to automate the\nsoftware development and testing process as much as possible. In this process,\nrequirements engineering (RE) plays a fundamental role for all other steps that\nbuild on it. Model-based design and testing methods have been developed to\nhandle the growing complexity and variability of software systems. However,\nmajor effort is still required to create specification models from a large set\nof functional requirements provided in natural language. Numerous approaches\nbased on natural language processing (NLP) have been proposed in the literature\nto generate requirements models using mainly syntactic properties. Recent\nadvances in NLP show that semantic quantities can also be identified and used\nto provide better assistance in the requirements formalization process. In this\nwork, we present and discuss principal ideas and state-of-the-art methodologies\nfrom the field of NLP in order to guide the readers on how to create a set of\nrules and methods for the semi-automated formalization of requirements\naccording to their specific use case and needs. We discuss two different\napproaches in detail and highlight the iterative development of rule sets. The\nrequirements models are represented in a human- and machine-readable format in\nthe form of pseudocode. The presented methods are demonstrated on two\nindustrial use cases from the automotive and railway domains. It shows that\nusing current pre-trained NLP models requires less effort to create a set of\nrules and can be easily adapted to specific use cases and domains. In addition,\nfindings and shortcomings of this research area are highlighted and an outlook\non possible future developments is given.", "published": "2023-09-23 05:45:19", "link": "http://arxiv.org/abs/2309.13272v1", "categories": ["cs.SE", "cs.CL", "68T50, 68N30", "I.2.7; D.2.1"], "primary_category": "cs.SE"}
{"title": "Probing the Moral Development of Large Language Models through Defining\n  Issues Test", "abstract": "In this study, we measure the moral reasoning ability of LLMs using the\nDefining Issues Test - a psychometric instrument developed for measuring the\nmoral development stage of a person according to the Kohlberg's Cognitive Moral\nDevelopment Model. DIT uses moral dilemmas followed by a set of ethical\nconsiderations that the respondent has to judge for importance in resolving the\ndilemma, and then rank-order them by importance. A moral development stage\nscore of the respondent is then computed based on the relevance rating and\nranking.\n  Our study shows that early LLMs such as GPT-3 exhibit a moral reasoning\nability no better than that of a random baseline, while ChatGPT, Llama2-Chat,\nPaLM-2 and GPT-4 show significantly better performance on this task, comparable\nto adult humans. GPT-4, in fact, has the highest post-conventional moral\nreasoning score, equivalent to that of typical graduate school students.\nHowever, we also observe that the models do not perform consistently across all\ndilemmas, pointing to important gaps in their understanding and reasoning\nabilities.", "published": "2023-09-23 12:17:10", "link": "http://arxiv.org/abs/2309.13356v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "D-Separation for Causal Self-Explanation", "abstract": "Rationalization is a self-explaining framework for NLP models. Conventional\nwork typically uses the maximum mutual information (MMI) criterion to find the\nrationale that is most indicative of the target label. However, this criterion\ncan be influenced by spurious features that correlate with the causal rationale\nor the target label. Instead of attempting to rectify the issues of the MMI\ncriterion, we propose a novel criterion to uncover the causal rationale, termed\nthe Minimum Conditional Dependence (MCD) criterion, which is grounded on our\nfinding that the non-causal features and the target label are\n\\emph{d-separated} by the causal rationale. By minimizing the dependence\nbetween the unselected parts of the input and the target label conditioned on\nthe selected rationale candidate, all the causes of the label are compelled to\nbe selected. In this study, we employ a simple and practical measure of\ndependence, specifically the KL-divergence, to validate our proposed MCD\ncriterion. Empirically, we demonstrate that MCD improves the F1 score by up to\n$13.7\\%$ compared to previous state-of-the-art MMI-based methods. Our code is\navailable at: \\url{https://github.com/jugechengzi/Rationalization-MCD}.", "published": "2023-09-23 14:23:19", "link": "http://arxiv.org/abs/2309.13391v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Chat About Boring Problems: Studying GPT-based text normalization", "abstract": "Text normalization - the conversion of text from written to spoken form - is\ntraditionally assumed to be an ill-formed task for language models. In this\nwork, we argue otherwise. We empirically show the capacity of Large-Language\nModels (LLM) for text normalization in few-shot scenarios. Combining\nself-consistency reasoning with linguistic-informed prompt engineering, we find\nLLM based text normalization to achieve error rates around 40\\% lower than top\nnormalization systems. Further, upon error analysis, we note key limitations in\nthe conventional design of text normalization tasks. We create a new taxonomy\nof text normalization errors and apply it to results from GPT-3.5-Turbo and\nGPT-4.0. Through this new framework, we can identify strengths and weaknesses\nof GPT-based TN, opening opportunities for future work.", "published": "2023-09-23 16:32:59", "link": "http://arxiv.org/abs/2309.13426v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diversifying Question Generation over Knowledge Base via External\n  Natural Questions", "abstract": "Previous methods on knowledge base question generation (KBQG) primarily focus\non enhancing the quality of a single generated question. Recognizing the\nremarkable paraphrasing ability of humans, we contend that diverse texts should\nconvey the same semantics through varied expressions. The above insights make\ndiversifying question generation an intriguing task, where the first challenge\nis evaluation metrics for diversity. Current metrics inadequately assess the\nabove diversity since they calculate the ratio of unique n-grams in the\ngenerated question itself, which leans more towards measuring duplication\nrather than true diversity. Accordingly, we devise a new diversity evaluation\nmetric, which measures the diversity among top-k generated questions for each\ninstance while ensuring their relevance to the ground truth. Clearly, the\nsecond challenge is how to enhance diversifying question generation. To address\nthis challenge, we introduce a dual model framework interwoven by two selection\nstrategies to generate diverse questions leveraging external natural questions.\nThe main idea of our dual framework is to extract more diverse expressions and\nintegrate them into the generation model to enhance diversifying question\ngeneration. Extensive experiments on widely used benchmarks for KBQG\ndemonstrate that our proposed approach generates highly diverse questions and\nimproves the performance of question answering tasks.", "published": "2023-09-23 10:37:57", "link": "http://arxiv.org/abs/2309.14362v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An In-depth Survey of Large Language Model-based Artificial Intelligence\n  Agents", "abstract": "Due to the powerful capabilities demonstrated by large language model (LLM),\nthere has been a recent surge in efforts to integrate them with AI agents to\nenhance their performance. In this paper, we have explored the core differences\nand characteristics between LLM-based AI agents and traditional AI agents.\nSpecifically, we first compare the fundamental characteristics of these two\ntypes of agents, clarifying the significant advantages of LLM-based agents in\nhandling natural language, knowledge storage, and reasoning capabilities.\nSubsequently, we conducted an in-depth analysis of the key components of AI\nagents, including planning, memory, and tool use. Particularly, for the crucial\ncomponent of memory, this paper introduced an innovative classification scheme,\nnot only departing from traditional classification methods but also providing a\nfresh perspective on the design of an AI agent's memory system. We firmly\nbelieve that in-depth research and understanding of these core components will\nlay a solid foundation for the future advancement of AI agent technology. At\nthe end of the paper, we provide directional suggestions for further research\nin this field, with the hope of offering valuable insights to scholars and\nresearchers in the field.", "published": "2023-09-23 11:25:45", "link": "http://arxiv.org/abs/2309.14365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models\n  through Logic", "abstract": "Recent advancements in large language models have showcased their remarkable\ngeneralizability across various domains. However, their reasoning abilities\nstill have significant room for improvement, especially when confronted with\nscenarios requiring multi-step reasoning. Although large language models\npossess extensive knowledge, their reasoning often fails to effectively utilize\nthis knowledge to establish a coherent thinking paradigm. These models\nsometimes show hallucinations as their reasoning procedures are unconstrained\nby logical principles. Aiming at improving the zero-shot chain-of-thought\nreasoning ability of large language models, we propose LoT (Logical Thoughts),\na self-improvement prompting framework that leverages principles rooted in\nsymbolic logic, particularly Reductio ad Absurdum, to systematically verify and\nrectify the reasoning processes step by step. Experimental evaluations\nconducted on language tasks in diverse domains, including arithmetic,\ncommonsense, symbolic, causal inference, and social problems, demonstrate the\nefficacy of enhanced reasoning by logic. The implementation code for LoT can be\naccessed at: https://github.com/xf-zhao/LoT.", "published": "2023-09-23 11:21:12", "link": "http://arxiv.org/abs/2309.13339v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Towards LLM-guided Causal Explainability for Black-box Text Classifiers", "abstract": "With the advent of larger and more complex deep learning models, such as in\nNatural Language Processing (NLP), model qualities like explainability and\ninterpretability, albeit highly desirable, are becoming harder challenges to\ntackle and solve. For example, state-of-the-art models in text classification\nare black-box by design. Although standard explanation methods provide some\ndegree of explainability, these are mostly correlation-based methods and do not\nprovide much insight into the model. The alternative of causal explainability\nis more desirable to achieve but extremely challenging in NLP due to a variety\nof reasons. Inspired by recent endeavors to utilize Large Language Models\n(LLMs) as experts, in this work, we aim to leverage the instruction-following\nand textual understanding capabilities of recent state-of-the-art LLMs to\nfacilitate causal explainability via counterfactual explanation generation for\nblack-box text classifiers. To do this, we propose a three-step pipeline via\nwhich, we use an off-the-shelf LLM to: (1) identify the latent or unobserved\nfeatures in the input text, (2) identify the input features associated with the\nlatent features, and finally (3) use the identified input features to generate\na counterfactual explanation. We experiment with our pipeline on multiple NLP\ntext classification datasets, with several recent LLMs, and present interesting\nand promising findings.", "published": "2023-09-23 11:22:28", "link": "http://arxiv.org/abs/2309.13340v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "My Science Tutor (MyST) -- A Large Corpus of Children's Conversational\n  Speech", "abstract": "This article describes the MyST corpus developed as part of the My Science\nTutor project -- one of the largest collections of children's conversational\nspeech comprising approximately 400 hours, spanning some 230K utterances across\nabout 10.5K virtual tutor sessions by around 1.3K third, fourth and fifth grade\nstudents. 100K of all utterances have been transcribed thus far. The corpus is\nfreely available (https://myst.cemantix.org) for non-commercial use using a\ncreative commons license. It is also available for commercial use\n(https://boulderlearning.com/resources/myst-corpus/). To date, ten\norganizations have licensed the corpus for commercial use, and approximately 40\nuniversity and other not-for-profit research groups have downloaded the corpus.\nIt is our hope that the corpus can be used to improve automatic speech\nrecognition algorithms, build and evaluate conversational AI agents for\neducation, and together help accelerate development of multimodal applications\nto improve children's excitement and learning about science, and help them\nlearn remotely.", "published": "2023-09-23 11:52:36", "link": "http://arxiv.org/abs/2309.13347v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lexical Squad@Multimodal Hate Speech Event Detection 2023: Multimodal\n  Hate Speech Detection using Fused Ensemble Approach", "abstract": "With a surge in the usage of social media postings to express opinions,\nemotions, and ideologies, there has been a significant shift towards the\ncalibration of social media as a rapid medium of conveying viewpoints and\noutlooks over the globe. Concurrently, the emergence of a multitude of\nconflicts between two entities has given rise to a stream of social media\ncontent containing propaganda, hate speech, and inconsiderate views. Thus, the\nissue of monitoring social media postings is rising swiftly, attracting major\nattention from those willing to solve such problems. One such problem is Hate\nSpeech detection. To mitigate this problem, we present our novel ensemble\nlearning approach for detecting hate speech, by classifying text-embedded\nimages into two labels, namely \"Hate Speech\" and \"No Hate Speech\". We have\nincorporated state-of-art models including InceptionV3, BERT, and XLNet. Our\nproposed ensemble model yielded promising results with 75.21 and 74.96 as\naccuracy and F-1 score (respectively). We also present an empirical evaluation\nof the text-embedded images to elaborate on how well the model was able to\npredict and classify. We release our codebase here\n(https://github.com/M0hammad-Kashif/MultiModalHateSpeech).", "published": "2023-09-23 12:06:05", "link": "http://arxiv.org/abs/2309.13354v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Resolving References in Visually-Grounded Dialogue via Text Generation", "abstract": "Vision-language models (VLMs) have shown to be effective at image retrieval\nbased on simple text queries, but text-image retrieval based on conversational\ninput remains a challenge. Consequently, if we want to use VLMs for reference\nresolution in visually-grounded dialogue, the discourse processing capabilities\nof these models need to be augmented. To address this issue, we propose\nfine-tuning a causal large language model (LLM) to generate definite\ndescriptions that summarize coreferential information found in the linguistic\ncontext of references. We then use a pretrained VLM to identify referents based\non the generated descriptions, zero-shot. We evaluate our approach on a\nmanually annotated dataset of visually-grounded dialogues and achieve results\nthat, on average, exceed the performance of the baselines we compare against.\nFurthermore, we find that using referent descriptions based on larger context\nwindows has the potential to yield higher returns.", "published": "2023-09-23 17:07:54", "link": "http://arxiv.org/abs/2309.13430v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Hierarchical attention interpretation: an interpretable speech-level\n  transformer for bi-modal depression detection", "abstract": "Depression is a common mental disorder. Automatic depression detection tools\nusing speech, enabled by machine learning, help early screening of depression.\nThis paper addresses two limitations that may hinder the clinical\nimplementations of such tools: noise resulting from segment-level labelling and\na lack of model interpretability. We propose a bi-modal speech-level\ntransformer to avoid segment-level labelling and introduce a hierarchical\ninterpretation approach to provide both speech-level and sentence-level\ninterpretations, based on gradient-weighted attention maps derived from all\nattention layers to track interactions between input features. We show that the\nproposed model outperforms a model that learns at a segment level ($p$=0.854,\n$r$=0.947, $F1$=0.897 compared to $p$=0.732, $r$=0.808, $F1$=0.768). For model\ninterpretation, using one true positive sample, we show which sentences within\na given speech are most relevant to depression detection; and which text tokens\nand Mel-spectrogram regions within these sentences are most relevant to\ndepression detection. These interpretations allow clinicians to verify the\nvalidity of predictions made by depression detection tools, promoting their\nclinical implementations.", "published": "2023-09-23 20:48:58", "link": "http://arxiv.org/abs/2309.13476v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples\n  for Image-Text Pairs", "abstract": "Counterfactual examples have proven to be valuable in the field of natural\nlanguage processing (NLP) for both evaluating and improving the robustness of\nlanguage models to spurious correlations in datasets. Despite their\ndemonstrated utility for NLP, multimodal counterfactual examples have been\nrelatively unexplored due to the difficulty of creating paired image-text data\nwith minimal counterfactual changes. To address this challenge, we introduce a\nscalable framework for automatic generation of counterfactual examples using\ntext-to-image diffusion models. We use our framework to create\nCOCO-Counterfactuals, a multimodal counterfactual dataset of paired image and\ntext captions based on the MS-COCO dataset. We validate the quality of\nCOCO-Counterfactuals through human evaluations and show that existing\nmultimodal models are challenged by our counterfactual image-text pairs.\nAdditionally, we demonstrate the usefulness of COCO-Counterfactuals for\nimproving out-of-domain generalization of multimodal vision-language models via\ntraining data augmentation.", "published": "2023-09-23 00:16:47", "link": "http://arxiv.org/abs/2309.14356v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "A Survey on Image-text Multimodal Models", "abstract": "With the significant advancements of Large Language Models (LLMs) in the\nfield of Natural Language Processing (NLP), the development of image-text\nmultimodal models has garnered widespread attention. Current surveys on\nimage-text multimodal models mainly focus on representative models or\napplication domains, but lack a review on how general technical models\ninfluence the development of domain-specific models, which is crucial for\ndomain researchers. Based on this, this paper first reviews the technological\nevolution of image-text multimodal models, from early explorations of feature\nspace to visual language encoding structures, and then to the latest large\nmodel architectures. Next, from the perspective of technological evolution, we\nexplain how the development of general image-text multimodal technologies\npromotes the progress of multimodal technologies in the biomedical field, as\nwell as the importance and complexity of specific datasets in the biomedical\ndomain. Then, centered on the tasks of image-text multimodal models, we analyze\ntheir common components and challenges. After that, we summarize the\narchitecture, components, and data of general image-text multimodal models, and\nintroduce the applications and improvements of image-text multimodal models in\nthe biomedical field. Finally, we categorize the challenges faced in the\ndevelopment and application of general models into external factors and\nintrinsic factors, further refining them into 2 external factors and 5\nintrinsic factors, and propose targeted solutions, providing guidance for\nfuture research directions. For more details and data, please visit our GitHub\npage: \\url{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.", "published": "2023-09-23 15:21:15", "link": "http://arxiv.org/abs/2309.15857v3", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Experimental Evidence on Negative Impact of Generative AI on Scientific\n  Learning Outcomes", "abstract": "In this study, I explored the impact of Generative AI on learning efficacy in\nacademic reading materials using experimental methods. College-educated\nparticipants engaged in three cycles of reading and writing tasks. After each\ncycle, they responded to comprehension questions related to the material. After\nadjusting for background knowledge and demographic factors, complete reliance\non AI for writing tasks led to a 25.1% reduction in accuracy. In contrast,\nAI-assisted reading resulted in a 12% decline. Interestingly, using AI for\nsummarization significantly improved both quality and output. Accuracy\nexhibited notable variance in the AI-assisted section. Further analysis\nrevealed that individuals with a robust background in the reading topic and\nsuperior reading/writing skills benefitted the most. I conclude the research by\ndiscussing educational policy implications, emphasizing the need for educators\nto warn students about the dangers of over-dependence on AI and provide\nguidance on its optimal use in educational settings.", "published": "2023-09-23 21:59:40", "link": "http://arxiv.org/abs/2311.05629v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Contrastive Speaker Embedding With Sequential Disentanglement", "abstract": "Contrastive speaker embedding assumes that the contrast between the positive\nand negative pairs of speech segments is attributed to speaker identity only.\nHowever, this assumption is incorrect because speech signals contain not only\nspeaker identity but also linguistic content. In this paper, we propose a\ncontrastive learning framework with sequential disentanglement to remove\nlinguistic content by incorporating a disentangled sequential variational\nautoencoder (DSVAE) into the conventional SimCLR framework. The DSVAE aims to\ndisentangle speaker factors from content factors in an embedding space so that\nonly the speaker factors are used for constructing a contrastive loss\nobjective. Because content factors have been removed from the contrastive\nlearning, the resulting speaker embeddings will be content-invariant.\nExperimental results on VoxCeleb1-test show that the proposed method\nconsistently outperforms SimCLR. This suggests that applying sequential\ndisentanglement is beneficial to learning speaker-discriminative embeddings.", "published": "2023-09-23 04:22:25", "link": "http://arxiv.org/abs/2309.13253v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Two vs. Four-Channel Sound Event Localization and Detection", "abstract": "Sound event localization and detection (SELD) systems estimate both the\ndirection-of-arrival (DOA) and class of sound sources over time. In the DCASE\n2022 SELD Challenge (Task 3), models are designed to operate in a 4-channel\nsetting. While beneficial to further the development of SELD systems using a\nmultichannel recording setup such as first-order Ambisonics (FOA), most\nconsumer electronics devices rarely are able to record using more than two\nchannels. For this reason, in this work we investigate the performance of the\nDCASE 2022 SELD baseline model using three audio input representations: FOA,\nbinaural, and stereo. We perform a novel comparative analysis illustrating the\neffect of these audio input representations on SELD performance. Crucially, we\nshow that binaural and stereo (i.e. 2-channel) audio-based SELD models are\nstill able to localize and detect sound sources laterally quite well, despite\noverall performance degrading as less audio information is provided. Further,\nwe segment our analysis by scenes containing varying degrees of sound source\npolyphony to better understand the effect of audio input representation on\nlocalization and detection performance as scene conditions become increasingly\ncomplex.", "published": "2023-09-23 11:32:53", "link": "http://arxiv.org/abs/2309.13343v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention Is All You Need For Blind Room Volume Estimation", "abstract": "In recent years, dynamic parameterization of acoustic environments has raised\nincreasing attention in the field of audio processing. One of the key\nparameters that characterize the local room acoustics in isolation from\norientation and directivity of sources and receivers is the geometric room\nvolume. Convolutional neural networks (CNNs) have been widely selected as the\nmain models for conducting blind room acoustic parameter estimation, which aims\nto learn a direct mapping from audio spectrograms to corresponding labels. With\nthe recent trend of self-attention mechanisms, this paper introduces a purely\nattention-based model to blindly estimate room volumes based on single-channel\nnoisy speech signals. We demonstrate the feasibility of eliminating the\nreliance on CNN for this task and the proposed Transformer architecture takes\nGammatone magnitude spectral coefficients and phase spectrograms as inputs. To\nenhance the model performance given the task-specific dataset, cross-modality\ntransfer learning is also applied. Experimental results demonstrate that the\nproposed model outperforms traditional CNN models across a wide range of\nreal-world acoustics spaces, especially with the help of the dedicated\npretraining and data augmentation schemes.", "published": "2023-09-23 23:58:43", "link": "http://arxiv.org/abs/2309.13504v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Importance of negative sampling in weak label learning", "abstract": "Weak-label learning is a challenging task that requires learning from data\n\"bags\" containing positive and negative instances, but only the bag labels are\nknown. The pool of negative instances is usually larger than positive\ninstances, thus making selecting the most informative negative instance\ncritical for performance. Such a selection strategy for negative instances from\neach bag is an open problem that has not been well studied for weak-label\nlearning. In this paper, we study several sampling strategies that can measure\nthe usefulness of negative instances for weak-label learning and select them\naccordingly. We test our method on CIFAR-10 and AudioSet datasets and show that\nit improves the weak-label classification performance and reduces the\ncomputational cost compared to random sampling methods. Our work reveals that\nnegative instances are not all equally irrelevant, and selecting them wisely\ncan benefit weak-label learning.", "published": "2023-09-23 01:11:15", "link": "http://arxiv.org/abs/2309.13227v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "WikiMT++ Dataset Card", "abstract": "WikiMT++ is an expanded and refined version of WikiMusicText (WikiMT),\nfeaturing 1010 curated lead sheets in ABC notation. To expand application\nscenarios of WikiMT, we add both objective (album, lyrics, video) and\nsubjective emotion (12 emotion adjectives) and emo\\_4q (Russell 4Q) attributes,\nenhancing its usability for music information retrieval, conditional music\ngeneration, automatic composition, and emotion classification, etc.\nAdditionally, CLaMP is implemented to correct the attributes inherited from\nWikiMT to reduce errors introduced during original data collection and enhance\nthe accuracy and completeness of our dataset.", "published": "2023-09-23 04:46:28", "link": "http://arxiv.org/abs/2309.13259v1", "categories": ["cs.IR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Beyond Fairness: Age-Harmless Parkinson's Detection via Voice", "abstract": "Parkinson's disease (PD), a neurodegenerative disorder, often manifests as\nspeech and voice dysfunction. While utilizing voice data for PD detection has\ngreat potential in clinical applications, the widely used deep learning models\ncurrently have fairness issues regarding different ages of onset. These deep\nmodels perform well for the elderly group (age $>$ 55) but are less accurate\nfor the young group (age $\\leq$ 55). Through our investigation, the discrepancy\nbetween the elderly and the young arises due to 1) an imbalanced dataset and 2)\nthe milder symptoms often seen in early-onset patients. However, traditional\ndebiasing methods are impractical as they typically impair the prediction\naccuracy for the majority group while minimizing the discrepancy. To address\nthis issue, we present a new debiasing method using GradCAM-based feature\nmasking combined with ensemble models, ensuring that neither fairness nor\naccuracy is compromised. Specifically, the GradCAM-based feature masking\nselectively obscures age-related features in the input voice data while\npreserving essential information for PD detection. The ensemble models further\nimprove the prediction accuracy for the minority (young group). Our approach\neffectively improves detection accuracy for early-onset patients without\nsacrificing performance for the elderly group. Additionally, we propose a\ntwo-step detection strategy for the young group, offering a practical risk\nassessment for potential early-onset PD patients.", "published": "2023-09-23 07:23:44", "link": "http://arxiv.org/abs/2309.13292v1", "categories": ["cs.LG", "cs.CY", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Asca: less audio data is more insightful", "abstract": "Audio recognition in specialized areas such as birdsong and submarine\nacoustics faces challenges in large-scale pre-training due to the limitations\nin available samples imposed by sampling environments and specificity\nrequirements. While the Transformer model excels in audio recognition, its\ndependence on vast amounts of data becomes restrictive in resource-limited\nsettings. Addressing this, we introduce the Audio Spectrogram Convolution\nAttention (ASCA) based on CoAtNet, integrating a Transformer-convolution hybrid\narchitecture, novel network design, and attention techniques, further augmented\nwith data enhancement and regularization strategies. On the BirdCLEF2023 and\nAudioSet(Balanced), ASCA achieved accuracies of 81.2% and 35.1%, respectively,\nsignificantly outperforming competing methods. The unique structure of our\nmodel enriches output, enabling generalization across various audio detection\ntasks. Our code can be found at https://github.com/LeeCiang/ASCA.", "published": "2023-09-23 13:24:06", "link": "http://arxiv.org/abs/2309.13373v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpeakEasy: A Conversational Intelligence Chatbot for Enhancing College\n  Students' Communication Skills", "abstract": "Social interactions and conversation skills separate the successful from the\nrest and the confident from the shy. For college students in particular, the\nability to converse can be an outlet for the stress and anxiety experienced on\na daily basis along with a foundation for all-important career skills. In light\nof this, we designed SpeakEasy: a chatbot with some degree of intelligence that\nprovides feedback to the user on their ability to engage in free-form\nconversations with the chatbot. SpeakEasy attempts to help college students\nimprove their communication skills by engaging in a seven-minute spoken\nconversation with the user, analyzing the user's responses with metrics\ndesigned based on previous psychology and linguistics research, and providing\nfeedback to the user on how they can improve their conversational ability. To\nsimulate natural conversation, SpeakEasy converses with the user on a wide\nassortment of topics that two people meeting for the first time might discuss:\ntravel, sports, and entertainment. Unlike most other chatbots with the goal of\nimproving conversation skills, SpeakEasy actually records the user speaking,\ntranscribes the audio into tokens, and uses macros-e.g., sequences that\ncalculate the pace of speech, determine if the user has an over-reliance on\ncertain words, and identifies awkward transitions-to evaluate the quality of\nthe conversation. Based on the evaluation, SpeakEasy provides elaborate\nfeedback on how the user can improve their conversations. In turn, SpeakEasy\nupdates its algorithms based on a series of questions that the user responds to\nregarding SpeakEasy's performance.", "published": "2023-09-23 17:19:32", "link": "http://arxiv.org/abs/2310.14891v1", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
