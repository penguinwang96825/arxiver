{"title": "Leveraging Natural Supervision for Language Representation Learning and\n  Generation", "abstract": "Recent breakthroughs in Natural Language Processing (NLP) have been driven by\nlanguage models trained on a massive amount of plain text. While powerful,\nderiving supervision from textual resources is still an open question. For\nexample, language model pretraining often neglects the rich, freely-available\nstructures in textual data. In this thesis, we describe three lines of work\nthat seek to improve the training and evaluation of neural models using\nnaturally-occurring supervision.\n  We first investigate self-supervised training losses to help enhance the\nperformance of pretrained language models for various NLP tasks. Specifically,\nwe alter the sentence prediction loss to make it better suited to other\npretraining losses and more challenging to solve. We design an intermediate\nfinetuning step that uses self-supervised training to promote models' ability\nin cross-task generalization.\n  Then we describe methods to leverage the structures in Wikipedia and\nparaphrases. In particular, we propose training losses to exploit hyperlinks,\narticle structures, and article category graphs for entity-, discourse-,\nentailment-related knowledge. We propose a framework that uses paraphrase pairs\nto disentangle semantics and syntax in sentence representations. We extend the\nframework for a novel generation task that controls the syntax of output text\nwith a sentential exemplar.\n  Lastly, we discuss our work on tailoring textual resources for establishing\nchallenging evaluation tasks. We introduce three datasets by defining novel\ntasks using various fan-contributed websites, including a long-form\ndata-to-text generation dataset, a screenplay summarization dataset, and a\nlong-form story generation dataset. These datasets have unique characteristics\noffering challenges to future work in their respective task settings.", "published": "2022-07-21 17:26:03", "link": "http://arxiv.org/abs/2207.10617v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Birth of Bias: A case study on the evolution of gender bias in an\n  English language model", "abstract": "Detecting and mitigating harmful biases in modern language models are widely\nrecognized as crucial, open problems. In this paper, we take a step back and\ninvestigate how language models come to be biased in the first place. We use a\nrelatively small language model, using the LSTM architecture trained on an\nEnglish Wikipedia corpus. With full access to the data and to the model\nparameters as they change during every step while training, we can map in\ndetail how the representation of gender develops, what patterns in the dataset\ndrive this, and how the model's internal state relates to the bias in a\ndownstream task (semantic textual similarity). We find that the representation\nof gender is dynamic and identify different phases during training.\nFurthermore, we show that gender information is represented increasingly\nlocally in the input embeddings of the model and that, as a consequence,\ndebiasing these can be effective in reducing the downstream bias. Monitoring\nthe training dynamics, allows us to detect an asymmetry in how the female and\nmale gender are represented in the input embeddings. This is important, as it\nmay cause naive mitigation strategies to introduce new undesirable biases. We\ndiscuss the relevance of the findings for mitigation strategies more generally\nand the prospects of generalizing our methods to larger language models, the\nTransformer architecture, other languages and other undesirable biases.", "published": "2022-07-21 00:59:04", "link": "http://arxiv.org/abs/2207.10245v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Model Cascades", "abstract": "Prompted models have demonstrated impressive few-shot learning abilities.\nRepeated interactions at test-time with a single model, or the composition of\nmultiple models together, further expands capabilities. These compositions are\nprobabilistic models, and may be expressed in the language of graphical models\nwith random variables whose values are complex data types such as strings.\nCases with control flow and dynamic structure require techniques from\nprobabilistic programming, which allow implementing disparate model structures\nand inference strategies in a unified language. We formalize several existing\ntechniques from this perspective, including scratchpads / chain of thought,\nverifiers, STaR, selection-inference, and tool use. We refer to the resulting\nprograms as language model cascades.", "published": "2022-07-21 07:35:18", "link": "http://arxiv.org/abs/2207.10342v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NusaCrowd: A Call for Open and Reproducible NLP Research in Indonesian\n  Languages", "abstract": "At the center of the underlying issues that halt Indonesian natural language\nprocessing (NLP) research advancement, we find data scarcity. Resources in\nIndonesian languages, especially the local ones, are extremely scarce and\nunderrepresented. Many Indonesian researchers do not publish their dataset.\nFurthermore, the few public datasets that we have are scattered across\ndifferent platforms, thus makes performing reproducible and data-centric\nresearch in Indonesian NLP even more arduous. Rising to this challenge, we\ninitiate the first Indonesian NLP crowdsourcing effort, NusaCrowd. NusaCrowd\nstrives to provide the largest datasheets aggregation with standardized data\nloading for NLP tasks in all Indonesian languages. By enabling open and\ncentralized access to Indonesian NLP resources, we hope NusaCrowd can tackle\nthe data scarcity problem hindering NLP progress in Indonesia and bring NLP\npractitioners to move towards collaboration.", "published": "2022-07-21 15:05:42", "link": "http://arxiv.org/abs/2207.10524v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Laws vs Model Architectures: How does Inductive Bias Influence\n  Scaling?", "abstract": "There have been a lot of interest in the scaling properties of Transformer\nmodels. However, not much has been done on the front of investigating the\neffect of scaling properties of different inductive biases and model\narchitectures. Do model architectures scale differently? If so, how does\ninductive bias affect scaling behaviour? How does this influence upstream\n(pretraining) and downstream (transfer)? This paper conducts a systematic study\nof scaling behaviour of ten diverse model architectures such as Transformers,\nSwitch Transformers, Universal Transformers, Dynamic convolutions, Performers,\nand recently proposed MLP-Mixers. Via extensive experiments, we show that (1)\narchitecture is an indeed an important consideration when performing scaling\nand (2) the best performing model can fluctuate at different scales. We believe\nthat the findings outlined in this work has significant implications to how\nmodel architectures are currently evaluated in the community.", "published": "2022-07-21 15:50:22", "link": "http://arxiv.org/abs/2207.10551v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Rethinking Textual Adversarial Defense for Pre-trained Language Models", "abstract": "Although pre-trained language models (PrLMs) have achieved significant\nsuccess, recent studies demonstrate that PrLMs are vulnerable to adversarial\nattacks. By generating adversarial examples with slight perturbations on\ndifferent levels (sentence / word / character), adversarial attacks can fool\nPrLMs to generate incorrect predictions, which questions the robustness of\nPrLMs. However, we find that most existing textual adversarial examples are\nunnatural, which can be easily distinguished by both human and machine. Based\non a general anomaly detector, we propose a novel metric (Degree of Anomaly) as\na constraint to enable current adversarial attack approaches to generate more\nnatural and imperceptible adversarial examples. Under this new constraint, the\nsuccess rate of existing attacks drastically decreases, which reveals that the\nrobustness of PrLMs is not as fragile as they claimed. In addition, we find\nthat four types of randomization can invalidate a large portion of textual\nadversarial examples. Based on anomaly detector and randomization, we design a\nuniversal defense framework, which is among the first to perform textual\nadversarial defense without knowing the specific attack. Empirical results show\nthat our universal defense framework achieves comparable or even higher\nafter-attack accuracy with other specific defenses, while preserving higher\noriginal accuracy at the same time. Our work discloses the essence of textual\nadversarial attacks, and indicates that (1) further works of adversarial\nattacks should focus more on how to overcome the detection and resist the\nrandomization, otherwise their adversarial examples would be easily detected\nand invalidated; and (2) compared with the unnatural and perceptible\nadversarial examples, it is those undetectable adversarial examples that pose\nreal risks for PrLMs and require more attention for future robustness-enhancing\nstrategies.", "published": "2022-07-21 07:51:45", "link": "http://arxiv.org/abs/2208.10251v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi Resolution Analysis (MRA) for Approximate Self-Attention", "abstract": "Transformers have emerged as a preferred model for many tasks in natural\nlangugage processing and vision. Recent efforts on training and deploying\nTransformers more efficiently have identified many strategies to approximate\nthe self-attention matrix, a key module in a Transformer architecture.\nEffective ideas include various prespecified sparsity patterns, low-rank basis\nexpansions and combinations thereof. In this paper, we revisit classical\nMultiresolution Analysis (MRA) concepts such as Wavelets, whose potential value\nin this setting remains underexplored thus far. We show that simple\napproximations based on empirical feedback and design choices informed by\nmodern hardware and implementation challenges, eventually yield a MRA-based\napproach for self-attention with an excellent performance profile across most\ncriteria of interest. We undertake an extensive set of experiments and\ndemonstrate that this multi-resolution scheme outperforms most efficient\nself-attention proposals and is favorable for both short and long sequences.\nCode is available at \\url{https://github.com/mlpen/mra-attention}.", "published": "2022-07-21 03:36:30", "link": "http://arxiv.org/abs/2207.10284v1", "categories": ["cs.LG", "cs.CL", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Grounding Visual Representations with Texts for Domain Generalization", "abstract": "Reducing the representational discrepancy between source and target domains\nis a key component to maximize the model generalization. In this work, we\nadvocate for leveraging natural language supervision for the domain\ngeneralization task. We introduce two modules to ground visual representations\nwith texts containing typical reasoning of humans: (1) Visual and Textual Joint\nEmbedder and (2) Textual Explanation Generator. The former learns the\nimage-text joint embedding space where we can ground high-level\nclass-discriminative information into the model. The latter leverages an\nexplainable model and generates explanations justifying the rationale behind\nits decision. To the best of our knowledge, this is the first work to leverage\nthe vision-and-language cross-modality approach for the domain generalization\ntask. Our experiments with a newly created CUB-DG benchmark dataset demonstrate\nthat cross-modality supervision can be successfully used to ground\ndomain-invariant visual representations and improve the model generalization.\nFurthermore, in the large-scale DomainBed benchmark, our proposed method\nachieves state-of-the-art results and ranks 1st in average performance for five\nmulti-domain datasets. The dataset and codes are available at\nhttps://github.com/mswzeus/GVRT.", "published": "2022-07-21 03:43:38", "link": "http://arxiv.org/abs/2207.10285v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CodeT: Code Generation with Generated Tests", "abstract": "The task of generating code solutions for a given programming problem can\nbenefit from the use of pre-trained language models such as Codex, which can\nproduce multiple diverse samples. However, a major challenge for this task is\nto select the most appropriate solution from the multiple samples generated by\nthe pre-trained language models. A natural way to evaluate the quality and\ncorrectness of a code solution is to run it against a set of test cases, but\nthe manual creation of such test cases is often costly and time-consuming. In\nthis paper, we propose a novel method, CodeT, that leverages the same\npre-trained language models to automatically generate test cases for the code\nsamples, thus reducing the human effort and increasing the coverage of the test\nscenarios. CodeT then executes the code samples using the generated test cases,\nand performs a dual execution agreement, which considers both the consistency\nof the outputs against the generated test cases and the agreement of the\noutputs with other code samples. We conduct comprehensive experiments on four\nbenchmarks, HumanEval, MBPP, APPS and CodeContests, using five different\npre-trained language models with varying sizes and capabilities. Our results\nshow that CodeT can significantly improve the performance of code solution\nselection over previous methods, achieving remarkable and consistent gains\nacross different models and benchmarks. For instance, CodeT improves the pass@1\nmetric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%\nover the code-davinci-002 model, and an absolute improvement of more than 20%\nover the previous state-of-the-art results.", "published": "2022-07-21 10:18:37", "link": "http://arxiv.org/abs/2207.10397v2", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning for Depression Detection in Dialogs", "abstract": "Depression is a serious mental illness that impacts the way people\ncommunicate, especially through their emotions, and, allegedly, the way they\ninteract with others. This work examines depression signals in dialogs, a less\nstudied setting that suffers from data sparsity. We hypothesize that depression\nand emotion can inform each other, and we propose to explore the influence of\ndialog structure through topic and dialog act prediction. We investigate a\nMulti-Task Learning (MTL) approach, where all tasks mentioned above are learned\njointly with dialog-tailored hierarchical modeling. We experiment on the DAIC\nand DailyDialog corpora-both contain dialogs in English-and show important\nimprovements over state-ofthe-art on depression detection (at best 70.6% F 1),\nwhich demonstrates the correlation of depression with emotion and dialog\norganization and the power of MTL to leverage information from different\nsources.", "published": "2022-07-21 07:03:46", "link": "http://arxiv.org/abs/2208.10250v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Jointly Predicting Emotion, Age, and Country Using Pre-Trained Acoustic\n  Embedding", "abstract": "In this paper, we demonstrated the benefit of using pre-trained model to\nextract acoustic embedding to jointly predict (multitask learning) three tasks:\nemotion, age, and native country. The pre-trained model was trained with\nwav2vec 2.0 large robust model on the speech emotion corpus. The emotion and\nage tasks were regression problems, while country prediction was a\nclassification task. A single harmonic mean from three metrics was used to\nevaluate the performance of multitask learning. The classifier was a linear\nnetwork with two independent layers and shared layers, including the output\nlayers. This study explores multitask learning on different acoustic features\n(including the acoustic embedding extracted from a model trained on an\naffective speech dataset), seed numbers, batch sizes, and normalizations for\npredicting paralinguistic information from speech.", "published": "2022-07-21 07:04:41", "link": "http://arxiv.org/abs/2207.10333v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Room geometry blind inference based on the localization of real sound\n  source and first order reflections", "abstract": "The conventional room geometry blind inference techniques with acoustic\nsignals are conducted based on the prior knowledge of the environment, such as\nthe room impulse response (RIR) or the sound source position, which will limit\nits application under unknown scenarios. To solve this problem, we have\nproposed a room geometry reconstruction method in this paper by using the\ngeometric relation between the direct signal and first-order reflections. In\naddition to the information of the compact microphone array itself, this method\ndoes not need any precognition of the environmental parameters. Besides, the\nlearning-based DNN models are designed and used to improve the accuracy and\nintegrity of the localization results of the direct source and first-order\nreflections. The direction of arrival (DOA) and time difference of arrival\n(TDOA) information of the direct and reflected signals are firstly estimated\nusing the proposed DCNN and TD-CNN models, which have higher sensitivity and\naccuracy than the conventional methods. Then the position of the sound source\nis inferred by integrating the DOA, TDOA and array height using the proposed\nDNN model. After that, the positions of image sources and corresponding\nboundaries are derived based on the geometric relation. Experimental results of\nboth simulations and real measurements verify the effectiveness and accuracy of\nthe proposed techniques compared with the conventional methods under different\nreverberant environments.", "published": "2022-07-21 13:44:10", "link": "http://arxiv.org/abs/2207.10478v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Surrey System for DCASE 2022 Task 5: Few-shot Bioacoustic Event\n  Detection with Segment-level Metric Learning", "abstract": "Few-shot audio event detection is a task that detects the occurrence time of\na novel sound class given a few examples. In this work, we propose a system\nbased on segment-level metric learning for the DCASE 2022 challenge of few-shot\nbioacoustic event detection (task 5). We make better utilization of the\nnegative data within each sound class to build the loss function, and use\ntransductive inference to gain better adaptation on the evaluation set. For the\ninput feature, we find the per-channel energy normalization concatenated with\ndelta mel-frequency cepstral coefficients to be the most effective combination.\nWe also introduce new data augmentation and post-processing procedures for this\ntask. Our final system achieves an f-measure of 68.74 on the DCASE task 5\nvalidation set, outperforming the baseline performance of 29.5 by a large\nmargin. Our system is fully open-sourced at\nhttps://github.com/haoheliu/DCASE_2022_Task_5.", "published": "2022-07-21 15:43:36", "link": "http://arxiv.org/abs/2207.10547v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Audio Waveform Prior", "abstract": "Convolutional neural networks contain strong priors for generating natural\nlooking images [1]. These priors enable image denoising, super resolution, and\ninpainting in an unsupervised manner. Previous attempts to demonstrate similar\nideas in audio, namely deep audio priors, (i) use hand picked architectures\nsuch as harmonic convolutions, (ii) only work with spectrogram input, and (iii)\nhave been used mostly for eliminating Gaussian noise [2]. In this work we show\nthat existing SOTA architectures for audio source separation contain deep\npriors even when working with the raw waveform. Deep priors can be discovered\nby training a neural network to generate a single corrupted signal when given\nwhite noise as input. A network with relevant deep priors is likely to generate\na cleaner version of the signal before converging on the corrupted signal. We\ndemonstrate this restoration effect with several corruptions: background noise,\nreverberations, and a gap in the signal (audio inpainting).", "published": "2022-07-21 12:25:03", "link": "http://arxiv.org/abs/2207.10441v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Proposal for Foley Sound Synthesis Challenge", "abstract": "\"Foley\" refers to sound effects that are added to multimedia during\npost-production to enhance its perceived acoustic properties, e.g., by\nsimulating the sounds of footsteps, ambient environmental sounds, or visible\nobjects on the screen. While foley is traditionally produced by foley artists,\nthere is increasing interest in automatic or machine-assisted techniques\nbuilding upon recent advances in sound synthesis and generative models. To\nfoster more participation in this growing research area, we propose a challenge\nfor automatic foley synthesis. Through case studies on successful previous\nchallenges in audio and machine learning, we set the goals of the proposed\nchallenge: rigorous, unified, and efficient evaluation of different foley\nsynthesis systems, with an overarching goal of drawing active participation\nfrom the research community. We outline the details and design considerations\nof a foley sound synthesis challenge, including task definition, dataset\nrequirements, and evaluation criteria.", "published": "2022-07-21 21:19:07", "link": "http://arxiv.org/abs/2207.10760v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Unsupervised Hierarchies of Audio Concepts", "abstract": "Music signals are difficult to interpret from their low-level features,\nperhaps even more than images: e.g. highlighting part of a spectrogram or an\nimage is often insufficient to convey high-level ideas that are genuinely\nrelevant to humans. In computer vision, concept learning was therein proposed\nto adjust explanations to the right abstraction level (e.g. detect clinical\nconcepts from radiographs). These methods have yet to be used for MIR.\n  In this paper, we adapt concept learning to the realm of music, with its\nparticularities. For instance, music concepts are typically non-independent and\nof mixed nature (e.g. genre, instruments, mood), unlike previous work that\nassumed disentangled concepts. We propose a method to learn numerous music\nconcepts from audio and then automatically hierarchise them to expose their\nmutual relationships. We conduct experiments on datasets of playlists from a\nmusic streaming service, serving as a few annotated examples for diverse\nconcepts. Evaluations show that the mined hierarchies are aligned with both\nground-truth hierarchies of concepts -- when available -- and with proxy\nsources of concept similarity in the general case.", "published": "2022-07-21 16:34:31", "link": "http://arxiv.org/abs/2207.11231v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
