{"title": "Offline Digital Euro: a Minimum Viable CBDC using Groth-Sahai proofs", "abstract": "Current digital payment solutions are fragile and offer less privacy than\ntraditional cash.\n  Their critical dependency on an online service used to perform and validate\ntransactions makes them void if this service is unreachable.\n  Moreover, no transaction can be executed during server malfunctions or power\noutages.\n  Due to climate change, the likelihood of extreme weather increases. As\nextreme weather is a major cause of power outages, the frequency of power\noutages is expected to increase.\n  The lack of privacy is an inherent result of their account-based design or\nthe use of a public ledger.\n  The critical dependency and lack of privacy can be resolved with a Central\nBank Digital Currency that can be used offline.\n  This thesis proposes a design and a first implementation for an offline-first\ndigital euro.\n  The protocol offers complete privacy during transactions using zero-knowledge\nproofs.\n  Furthermore, transactions can be executed offline without third parties and\nretroactive double-spending detection is facilitated.\n  To protect the users' privacy, but also guard against money laundering, we\nhave added the following privacy-guarding mechanism.\n  The bank and trusted third parties for law enforcement must collaborate to\ndecrypt transactions, revealing the digital pseudonym used in the transaction.\n  Importantly, the transaction can be decrypted without decrypting prior\ntransactions attached to the digital euro.\n  The protocol has a working initial implementation showcasing its usability\nand demonstrating functionality.", "published": "2024-07-01 09:55:14", "link": "http://arxiv.org/abs/2407.13776v1", "categories": ["cs.CR", "q-fin.TR", "68-02"], "primary_category": "cs.CR"}
{"title": "How to Leverage Digit Embeddings to Represent Numbers?", "abstract": "Within numerical reasoning, understanding numbers themselves is still a\nchallenge for existing language models. Simple generalisations, such as solving\n100+200 instead of 1+2, can substantially affect model performance (Sivakumar\nand Moosavi, 2023). Among various techniques, character-level embeddings of\nnumbers have emerged as a promising approach to improve number representation.\nHowever, this method has limitations as it leaves the task of aggregating digit\nrepresentations to the model, which lacks direct supervision for this process.\nIn this paper, we explore the use of mathematical priors to compute aggregated\ndigit embeddings and explicitly incorporate these aggregates into transformer\nmodels. This can be achieved either by adding a special token to the input\nembeddings or by introducing an additional loss function to enhance correct\npredictions. We evaluate the effectiveness of incorporating this explicit\naggregation, analysing its strengths and shortcomings, and discuss future\ndirections to better benefit from this approach. Our methods, while simple, are\ncompatible with any pretrained model, easy to implement, and have been made\npublicly available.", "published": "2024-07-01 01:31:41", "link": "http://arxiv.org/abs/2407.00894v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Preserving Multilingual Quality While Tuning Query Encoder on English\n  Only", "abstract": "A query encoder of a dual passage retrieval system can be tuned for specific\ntypes of queries or domains, while the precomputed and stored documents\nrepresentations are kept intact. Switching from one query encoder to another\nwhen needed is easily feasible, unlike overhauling the embeddings of a whole\nknowledge base. In this work we raise a question: Can the generic, original\nqualities of the encoder be preserved or at least left not too degraded when it\nis tuned on a narrow domain? We conducted experiments on a high quality\nmultilingual embedding model: Tuning it on a single English-only dataset, we\nobserve that the tuning not only preserves the multilingual qualities, but even\nimproves them. The embedding qualities on distinctly different data are also\nimproved or at least preserved. Drawing on our observations, we suggest a more\ngeneral hypothesis: Tuning with intentionally low learning rate can preserve or\nimprove a system's properties acquired in training, but not specifically\ntargeted by tuning. We call this adiabatic tuning and provide tentative\nexplanations.", "published": "2024-07-01 03:03:18", "link": "http://arxiv.org/abs/2407.00923v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EXCGEC: A Benchmark for Edit-Wise Explainable Chinese Grammatical Error\n  Correction", "abstract": "Existing studies explore the explainability of Grammatical Error Correction\n(GEC) in a limited scenario, where they ignore the interaction between\ncorrections and explanations and have not established a corresponding\ncomprehensive benchmark. To bridge the gap, this paper first introduces the\ntask of EXplainable GEC (EXGEC), which focuses on the integral role of\ncorrection and explanation tasks. To facilitate the task, we propose EXCGEC, a\ntailored benchmark for Chinese EXGEC consisting of 8,216 explanation-augmented\nsamples featuring the design of hybrid edit-wise explanations. We then\nbenchmark several series of LLMs in multi-task learning settings, including\npost-explaining and pre-explaining. To promote the development of the task, we\nalso build a comprehensive evaluation suite by leveraging existing automatic\nmetrics and conducting human evaluation experiments to demonstrate the human\nconsistency of the automatic metrics for free-text explanations. Our\nexperiments reveal the effectiveness of evaluating free-text explanations using\ntraditional metrics like METEOR and ROUGE, and the inferior performance of\nmulti-task models compared to the pipeline solution, indicating its challenges\nto establish positive effects in learning both tasks.", "published": "2024-07-01 03:06:41", "link": "http://arxiv.org/abs/2407.00924v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CLEME2.0: Towards More Interpretable Evaluation by Disentangling Edits\n  for Grammatical Error Correction", "abstract": "The paper focuses on improving the interpretability of Grammatical Error\nCorrection (GEC) metrics, which receives little attention in previous studies.\nTo bridge the gap, we propose CLEME2.0, a reference-based evaluation strategy\nthat can describe four elementary dimensions of GEC systems, namely\nhit-correction, error-correction, under-correction, and over-correction. They\ncollectively contribute to revealing the critical characteristics and locating\ndrawbacks of GEC systems. Evaluating systems by Combining these dimensions\nleads to high human consistency over other reference-based and reference-less\nmetrics. Extensive experiments on 2 human judgement datasets and 6 reference\ndatasets demonstrate the effectiveness and robustness of our method. All the\ncodes will be released after the peer review.", "published": "2024-07-01 03:35:58", "link": "http://arxiv.org/abs/2407.00934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM Uncertainty Quantification through Directional Entailment Graph and\n  Claim Level Response Augmentation", "abstract": "The Large language models (LLMs) have showcased superior capabilities in\nsophisticated tasks across various domains, stemming from basic question-answer\n(QA), they are nowadays used as decision assistants or explainers for\nunfamiliar content. However, they are not always correct due to the data\nsparsity in specific domain corpus, or the model's hallucination problems.\nGiven this, how much should we trust the responses from LLMs? This paper\npresents a novel way to evaluate the uncertainty that captures the directional\ninstability, by constructing a directional graph from entailment probabilities,\nand we innovatively conduct Random Walk Laplacian given the asymmetric property\nof a constructed directed graph, then the uncertainty is aggregated by the\nderived eigenvalues from the Laplacian process. We also provide a way to\nincorporate the existing work's semantics uncertainty with our proposed layer.\nBesides, this paper identifies the vagueness issues in the raw response set and\nproposes an augmentation approach to mitigate such a problem, we conducted\nextensive empirical experiments and demonstrated the superiority of our\nproposed solutions.", "published": "2024-07-01 06:11:30", "link": "http://arxiv.org/abs/2407.00994v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Engineering Conversational Search Systems: A Review of Applications,\n  Architectures, and Functional Components", "abstract": "Conversational search systems enable information retrieval via natural\nlanguage interactions, with the goal of maximizing users' information gain over\nmultiple dialogue turns. The increasing prevalence of conversational interfaces\nadopting this search paradigm challenges traditional information retrieval\napproaches, stressing the importance of better understanding the engineering\nprocess of developing these systems. We undertook a systematic literature\nreview to investigate the links between theoretical studies and technical\nimplementations of conversational search systems. Our review identifies\nreal-world application scenarios, system architectures, and functional\ncomponents. We consolidate our results by presenting a layered architecture\nframework and explaining the core functions of conversational search systems.\nFurthermore, we reflect on our findings in light of the rapid progress in large\nlanguage models, discussing their capabilities, limitations, and directions for\nfuture research.", "published": "2024-07-01 06:24:11", "link": "http://arxiv.org/abs/2407.00997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large\n  Language Models", "abstract": "Large language models (LLMs) have demonstrated emergent capabilities across\ndiverse reasoning tasks via popular Chains-of-Thought (COT) prompting. However,\nsuch a simple and fast COT approach often encounters limitations in dealing\nwith complicated problems, while a thorough method, which considers multiple\nreasoning pathways and verifies each step carefully, results in slower\ninference. This paper addresses the challenge of enabling LLMs to autonomously\nselect between fast and slow inference methods, thereby optimizing both\nefficiency and effectiveness. We introduce a dynamic decision-making framework\nthat categorizes tasks into two distinct pathways: 'Fast', designated for tasks\nwhere the LLM quickly identifies a high-confidence solution, and 'Slow',\nallocated for tasks that the LLM perceives as complex and for which it has low\nconfidence in immediate solutions as well as requiring more reasoning paths to\nverify. Experiments on five popular reasoning benchmarks demonstrated the\nsuperiority of the DynaThink over baselines.", "published": "2024-07-01 06:45:13", "link": "http://arxiv.org/abs/2407.01009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Development of Cognitive Intelligence in Pre-trained Language Models", "abstract": "Recent studies show evidence for emergent cognitive abilities in Large\nPre-trained Language Models (PLMs). The increasing cognitive alignment of these\nmodels has made them candidates for cognitive science theories. Prior research\ninto the emergent cognitive abilities of PLMs has largely been path-independent\nto model training, i.e., has focused on the final model weights and not the\nintermediate steps. However, building plausible models of human cognition using\nPLMs would benefit from considering the developmental alignment of their\nperformance during training to the trajectories of children's thinking. Guided\nby psychometric tests of human intelligence, we choose four sets of tasks to\ninvestigate the alignment of ten popular families of PLMs and evaluate their\navailable intermediate and final training steps. These tasks are Numerical\nability, Linguistic abilities, Conceptual understanding, and Fluid reasoning.\nWe find a striking regularity: regardless of model size, the developmental\ntrajectories of PLMs consistently exhibit a window of maximal alignment to\nhuman cognitive development. Before that window, training appears to endow\n\"blank slate\" models with the requisite structure to be poised to rapidly learn\nfrom experience. After that window, training appears to serve the engineering\ngoal of reducing loss but not the scientific goal of increasing alignment with\nhuman cognition.", "published": "2024-07-01 07:56:36", "link": "http://arxiv.org/abs/2407.01047v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM\n  Outputs", "abstract": "Large Language Models (LLMs) generate text by sampling the next token from a\nprobability distribution over the vocabulary at each decoding step. Popular\nsampling methods like top-p (nucleus sampling) often struggle to balance\nquality and diversity, especially at higher temperatures which lead to\nincoherent or repetitive outputs. We propose min-p sampling, a dynamic\ntruncation method that adjusts the sampling threshold based on the model's\nconfidence by using the top token's probability as a scaling factor. Our\nexperiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative\nWriting show that min-p sampling improves both the quality and diversity of\ngenerated text across different model families (Mistral and Llama 3) and model\nsizes (1B to 123B parameters), especially at higher temperatures. Human\nevaluations further show a clear preference for min-p sampling, in both text\nquality and creativity. Min-p sampling has been adopted by popular open-source\nLLM frameworks, including Hugging Face Transformers, VLLM, and many others,\nhighlighting its significant impact on improving text generation quality.", "published": "2024-07-01 08:37:25", "link": "http://arxiv.org/abs/2407.01082v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M2QA: Multi-domain Multilingual Question Answering", "abstract": "Generalization and robustness to input variation are core desiderata of\nmachine learning research. Language varies along several axes, most\nimportantly, language instance (e.g. French) and domain (e.g. news). While\nadapting NLP models to new languages within a single domain, or to new domains\nwithin a single language, is widely studied, research in joint adaptation is\nhampered by the lack of evaluation datasets. This prevents the transfer of NLP\nsystems from well-resourced languages and domains to non-dominant\nlanguage-domain combinations. To address this gap, we introduce M2QA, a\nmulti-domain multilingual question answering benchmark. M2QA includes 13,500\nSQuAD 2.0-style question-answer instances in German, Turkish, and Chinese for\nthe domains of product reviews, news, and creative writing. We use M2QA to\nexplore cross-lingual cross-domain performance of fine-tuned models and\nstate-of-the-art LLMs and investigate modular approaches to domain and language\nadaptation. We witness 1) considerable performance variations across\ndomain-language combinations within model classes and 2) considerable\nperformance drops between source and target language-domain combinations across\nall model sizes. We demonstrate that M2QA is far from solved, and new methods\nto effectively transfer both linguistic and domain-specific information are\nnecessary. We make M2QA publicly available at https://github.com/UKPLab/m2qa.", "published": "2024-07-01 08:48:49", "link": "http://arxiv.org/abs/2407.01091v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer Learning for Speech Translation", "abstract": "There has been increasing interest in building multilingual foundation models\nfor NLP and speech research. This paper examines how to expand the speech\ntranslation capability of these models with restricted data. Whisper, a speech\nfoundation model with strong performance on speech recognition and English\ntranslation, is used as the example model. Using speech-to-speech retrieval to\nanalyse the audio representations generated by the encoder, we show that\nutterances from different languages are mapped to a shared semantic space. This\nshared embedding space can then be leveraged for zero-shot cross-lingual\ntransfer in speech translation. By fine-tuning the Whisper decoder with only\nEnglish-to-Chinese speech translation data, improved performance for\ntranslation to Chinese can be obtained for multiple languages, in addition to\nEnglish. Furthermore, for languages related to those seen in training it is\npossible to perform speech translation, despite the model never seeing the\nlanguage in training, or being able to perform transcription.", "published": "2024-07-01 09:51:48", "link": "http://arxiv.org/abs/2407.01130v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sociocultural Considerations in Monitoring Anti-LGBTQ+ Content on Social\n  Media", "abstract": "The purpose of this paper is to ascertain the influence of sociocultural\nfactors (i.e., social, cultural, and political) in the development of hate\nspeech detection systems. We set out to investigate the suitability of using\nopen-source training data to monitor levels of anti-LGBTQ+ content on social\nmedia across different national-varieties of English. Our findings suggests the\nsocial and cultural alignment of open-source hate speech data sets influences\nthe predicted outputs. Furthermore, the keyword-search approach of anti-LGBTQ+\nslurs in the development of open-source training data encourages detection\nmodels to overfit on slurs; therefore, anti-LGBTQ+ content may go undetected.\nWe recommend combining empirical outputs with qualitative insights to ensure\nthese systems are fit for purpose.", "published": "2024-07-01 10:17:13", "link": "http://arxiv.org/abs/2407.01149v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Explore and Select for Coverage-Conditioned\n  Retrieval-Augmented Generation", "abstract": "Interactions with large language models (LLMs) often yield long and detailed\nresponses, leveraging both parametric knowledge and retrieval-augmented\ngeneration (RAG). While these responses can provide rich insights, they often\ninclude redundant or less engaging content not aligned with user interests.\nThis issue becomes apparent when users specify particular subtopics to include\nor exclude -- termed coverage-conditioned ($C^2$) queries -- as LLMs often\nstruggle to provide tailored responses. To address this challenge, we\ninvestigate the role of query outlines, sequences of subqueries designed to\nguide LLMs in generating responses that meet specific user requirements. To\nsystematically create and evaluate these outlines, we introduce QTree, a\ndataset of 10K hierarchical sets of information-seeking subqueries that define\nstructured boundaries for outline creation and evaluation in $C^2$ scenarios.\nAdditionally, we develop QPlanner, a 7B language model trained to generate\ncustomized outlines within boundaries of QTree. We evaluate the effectiveness\nof the generated outlines through automatic and human judgements, focusing on\ntheir impact within retrieval-augmented generation (RAG) systems. Experimental\nresults demonstrate that QPlanner, especially when trained with alignment\ntechniques like DPO, generates higher-quality outlines that better fulfill\ndiverse user needs.", "published": "2024-07-01 10:26:19", "link": "http://arxiv.org/abs/2407.01158v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EconNLI: Evaluating Large Language Models on Economics Reasoning", "abstract": "Large Language Models (LLMs) are widely used for writing economic analysis\nreports or providing financial advice, but their ability to understand economic\nknowledge and reason about potential results of specific economic events lacks\nsystematic evaluation. To address this gap, we propose a new dataset, natural\nlanguage inference on economic events (EconNLI), to evaluate LLMs' knowledge\nand reasoning abilities in the economic domain. We evaluate LLMs on (1) their\nability to correctly classify whether a premise event will cause a hypothesis\nevent and (2) their ability to generate reasonable events resulting from a\ngiven premise. Our experiments reveal that LLMs are not sophisticated in\neconomic reasoning and may generate wrong or hallucinated answers. Our study\nraises awareness of the limitations of using LLMs for critical decision-making\ninvolving economic reasoning and analysis. The dataset and codes are available\nat https://github.com/Irenehere/EconNLI.", "published": "2024-07-01 11:58:24", "link": "http://arxiv.org/abs/2407.01212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Best Practices in Retrieval-Augmented Generation", "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective\nin integrating up-to-date information, mitigating hallucinations, and enhancing\nresponse quality, particularly in specialized domains. While many RAG\napproaches have been proposed to enhance large language models through\nquery-dependent retrievals, these approaches still suffer from their complex\nimplementation and prolonged response times. Typically, a RAG workflow involves\nmultiple processing steps, each of which can be executed in various ways. Here,\nwe investigate existing RAG approaches and their potential combinations to\nidentify optimal RAG practices. Through extensive experiments, we suggest\nseveral strategies for deploying RAG that balance both performance and\nefficiency. Moreover, we demonstrate that multimodal retrieval techniques can\nsignificantly enhance question-answering capabilities about visual inputs and\naccelerate the generation of multimodal content using a \"retrieval as\ngeneration\" strategy.", "published": "2024-07-01 12:06:34", "link": "http://arxiv.org/abs/2407.01219v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SignCLIP: Connecting Text and Sign Language by Contrastive Learning", "abstract": "We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image\nPretraining) to project spoken language text and sign language videos, two\nclasses of natural languages of distinct modalities, into the same space.\nSignCLIP is an efficient method of learning useful visual representations for\nsign language processing from large-scale, multilingual video-text pairs,\nwithout directly optimizing for a specific task or sign language which is often\nof limited size.\n  We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary\nconsisting of ~500 thousand video clips in up to 44 sign languages, and\nevaluate it with various downstream datasets. SignCLIP discerns in-domain\nsigning with notable text-to-video/video-to-text retrieval accuracy. It also\nperforms competitively for out-of-domain downstream tasks such as isolated sign\nlanguage recognition upon essential few-shot prompting or fine-tuning.\n  We analyze the latent space formed by the spoken language text and sign\nlanguage poses, which provides additional linguistic insights. Our code and\nmodels are openly available.", "published": "2024-07-01 13:17:35", "link": "http://arxiv.org/abs/2407.01264v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "First Place Solution of 2023 Global Artificial Intelligence Technology\n  Innovation Competition Track 1", "abstract": "In this paper, we present our champion solution to the Global Artificial\nIntelligence Technology Innovation Competition Track 1: Medical Imaging\nDiagnosis Report Generation. We select CPT-BASE as our base model for the text\ngeneration task. During the pre-training stage, we delete the mask language\nmodeling task of CPT-BASE and instead reconstruct the vocabulary, adopting a\nspan mask strategy and gradually increasing the number of masking ratios to\nperform the denoising auto-encoder pre-training task. In the fine-tuning stage,\nwe design iterative retrieval augmentation and noise-aware similarity bucket\nprompt strategies. The retrieval augmentation constructs a mini-knowledge base,\nenriching the input information of the model, while the similarity bucket\nfurther perceives the noise information within the mini-knowledge base, guiding\nthe model to generate higher-quality diagnostic reports based on the similarity\nprompts. Surprisingly, our single model has achieved a score of 2.321 on\nleaderboard A, and the multiple model fusion scores are 2.362 and 2.320 on the\nA and B leaderboards respectively, securing first place in the rankings.", "published": "2024-07-01 13:22:22", "link": "http://arxiv.org/abs/2407.01271v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Show Less, Instruct More: Enriching Prompts with Definitions and\n  Guidelines for Zero-Shot NER", "abstract": "Recently, several specialized instruction-tuned Large Language Models (LLMs)\nfor Named Entity Recognition (NER) have emerged. Compared to traditional NER\napproaches, these models have demonstrated strong generalization capabilities.\nExisting LLMs primarily focus on addressing zero-shot NER on Out-of-Domain\ninputs, while fine-tuning on an extensive number of entity classes that often\nhighly or completely overlap with test sets. In this work instead, we propose\nSLIMER, an approach designed to tackle never-seen-before entity tags by\ninstructing the model on fewer examples, and by leveraging a prompt enriched\nwith definition and guidelines. Experiments demonstrate that definition and\nguidelines yield better performance, faster and more robust learning,\nparticularly when labelling unseen named entities. Furthermore, SLIMER performs\ncomparably to state-of-the-art approaches in out-of-domain zero-shot NER, while\nbeing trained in a more fair, though certainly more challenging, setting.", "published": "2024-07-01 13:25:33", "link": "http://arxiv.org/abs/2407.01272v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Portability Strategies for Open-domain Dialogue with\n  Pre-trained Language Models from High to Low Resource Languages", "abstract": "In this paper we propose a study of linguistic portability strategies of\nlarge pre-trained language models (PLMs) used for open-domain dialogue systems\nin a high-resource language for this task. In particular the target\nlow-resource language (L_T) will be simulated with French, as it lacks of\ntask-specific resources and allows our human evaluation, when the source\nlanguage (L_S) is English. For obvious reasons, recent works using such models\nfor open-domain dialogue are mostly developed in English. Yet building specific\nPLMs for each possible target language supposes collecting new datasets and is\ncostly. For this reason, trying to leverage all existing resources (PLMs and\ndata) in both L_S and L_T , we wish to assess the performance achievable in L_T\nwith different approaches. The first two approaches evaluate the usage of\nNeural Machine Translation (NMT) at different levels: TrainOnTarget where a L_S\ndataset is translated before fine-tuning in L_T and TestOnSource where a L_S\nmodel is coupled with NMT modules during inference. Then, the advent of BLOOM\n[2], the world first open-access multilingual large PLM, allow researchers to\ndevelop new approaches aiming to leverage not only the model's full\naccessibility but also its multilingualism and translation abilities. In this\ncontext the task is learned in L_S first and adapted to L_T using the MAD-X\nAdapter architecture [16]. In the two sets of experiments models are evaluated\nin spoken dialogue conditions with human and the strategies can be compared in\nterms of perceived interaction quality.", "published": "2024-07-01 14:20:54", "link": "http://arxiv.org/abs/2407.01315v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language\n  Models", "abstract": "This paper investigates the cross-lingual inconsistencies observed in Large\nLanguage Models (LLMs), such as ChatGPT, Llama, and Baichuan, which have shown\nexceptional performance in various Natural Language Processing (NLP) tasks.\nDespite their successes, these models often exhibit significant inconsistencies\nwhen processing the same concepts across different languages. This study\nfocuses on three primary questions: the existence of cross-lingual\ninconsistencies in LLMs, the specific aspects in which these inconsistencies\nmanifest, and the correlation between cross-lingual consistency and\nmultilingual capabilities of LLMs.To address these questions, we propose an\ninnovative evaluation method for Cross-lingual Semantic Consistency (xSC) using\nthe LaBSE model. We further introduce metrics for Cross-lingual Accuracy\nConsistency (xAC) and Cross-lingual Timeliness Consistency (xTC) to\ncomprehensively assess the models' performance regarding semantic, accuracy,\nand timeliness inconsistencies. By harmonizing these metrics, we provide a\nholistic measurement of LLMs' cross-lingual consistency. Our findings aim to\nenhance the understanding and improvement of multilingual capabilities and\ninterpretability in LLMs, contributing to the development of more robust and\nreliable multilingual language models.", "published": "2024-07-01 15:11:37", "link": "http://arxiv.org/abs/2407.01358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nullpointer at ArAIEval Shared Task: Arabic Propagandist Technique\n  Detection with Token-to-Word Mapping in Sequence Tagging", "abstract": "This paper investigates the optimization of propaganda technique detection in\nArabic text, including tweets \\& news paragraphs, from ArAIEval shared task 1.\nOur approach involves fine-tuning the AraBERT v2 model with a neural network\nclassifier for sequence tagging. Experimental results show relying on the first\ntoken of the word for technique prediction produces the best performance. In\naddition, incorporating genre information as a feature further enhances the\nmodel's performance. Our system achieved a score of 25.41, placing us 4$^{th}$\non the leaderboard. Subsequent post-submission improvements further raised our\nscore to 26.68.", "published": "2024-07-01 15:15:24", "link": "http://arxiv.org/abs/2407.01360v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems", "abstract": "LLMs and RAG systems are now capable of handling millions of input tokens or\nmore. However, evaluating the output quality of such systems on long-context\ntasks remains challenging, as tasks like Needle-in-a-Haystack lack complexity.\nIn this work, we argue that summarization can play a central role in such\nevaluation. We design a procedure to synthesize Haystacks of documents,\nensuring that specific \\textit{insights} repeat across documents. The \"Summary\nof a Haystack\" (SummHay) task then requires a system to process the Haystack\nand generate, given a query, a summary that identifies the relevant insights\nand precisely cites the source documents. Since we have precise knowledge of\nwhat insights should appear in a haystack summary and what documents should be\ncited, we implement a highly reproducible automatic evaluation that can score\nsummaries on two aspects - Coverage and Citation. We generate Haystacks in two\ndomains (conversation, news), and perform a large-scale evaluation of 10 LLMs\nand corresponding 50 RAG systems. Our findings indicate that SummHay is an open\nchallenge for current systems, as even systems provided with an Oracle signal\nof document relevance lag our estimate of human performance (56\\%) by 10+\npoints on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and\nClaude 3 Opus score below 20% on SummHay. We show SummHay can also be used to\nstudy enterprise RAG systems and position bias in long-context models. We hope\nfuture systems can equal and surpass human performance on SummHay.", "published": "2024-07-01 15:23:42", "link": "http://arxiv.org/abs/2407.01370v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bridging the Gap: Transfer Learning from English PLMs to Malaysian\n  English", "abstract": "Malaysian English is a low resource creole language, where it carries the\nelements of Malay, Chinese, and Tamil languages, in addition to Standard\nEnglish. Named Entity Recognition (NER) models underperform when capturing\nentities from Malaysian English text due to its distinctive morphosyntactic\nadaptations, semantic features and code-switching (mixing English and Malay).\nConsidering these gaps, we introduce MENmBERT and MENBERT, a pre-trained\nlanguage model with contextual understanding, specifically tailored for\nMalaysian English. We have fine-tuned MENmBERT and MENBERT using manually\nannotated entities and relations from the Malaysian English News Article (MEN)\nDataset. This fine-tuning process allows the PLM to learn representations that\ncapture the nuances of Malaysian English relevant for NER and RE tasks.\nMENmBERT achieved a 1.52\\% and 26.27\\% improvement on NER and RE tasks\nrespectively compared to the bert-base-multilingual-cased model. Although the\noverall performance of NER does not have a significant improvement, our further\nanalysis shows that there is a significant improvement when evaluated by the 12\nentity labels. These findings suggest that pre-training language models on\nlanguage-specific and geographically-focused corpora can be a promising\napproach for improving NER performance in low-resource settings. The dataset\nand code published in this paper provide valuable resources for NLP research\nwork focusing on Malaysian English.", "published": "2024-07-01 15:26:03", "link": "http://arxiv.org/abs/2407.01374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Free-text Rationale Generation under Readability Level Control", "abstract": "Free-text rationales justify model decisions in natural language and thus\nbecome likable and accessible among approaches to explanation across many\ntasks. However, their effectiveness can be hindered by misinterpretation and\nhallucination. As a perturbation test, we investigate how large language models\n(LLMs) perform rationale generation under the effects of readability level\ncontrol, i.e., being prompted for an explanation targeting a specific expertise\nlevel, such as sixth grade or college. We find that explanations are adaptable\nto such instruction, though the requested readability is often misaligned with\nthe measured text complexity according to traditional readability metrics.\nFurthermore, the generated rationales tend to feature medium level complexity,\nwhich correlates with the measured quality using automatic metrics. Finally,\nour human annotators confirm a generally satisfactory impression on rationales\nat all readability levels, with high-school-level readability being most\ncommonly perceived and favored.", "published": "2024-07-01 15:34:17", "link": "http://arxiv.org/abs/2407.01384v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "POLygraph: Polish Fake News Dataset", "abstract": "This paper presents the POLygraph dataset, a unique resource for fake news\ndetection in Polish. The dataset, created by an interdisciplinary team, is\ncomposed of two parts: the \"fake-or-not\" dataset with 11,360 pairs of news\narticles (identified by their URLs) and corresponding labels, and the\n\"fake-they-say\" dataset with 5,082 news articles (identified by their URLs) and\ntweets commenting on them. Unlike existing datasets, POLygraph encompasses a\nvariety of approaches from source literature, providing a comprehensive\nresource for fake news detection. The data was collected through manual\nannotation by expert and non-expert annotators. The project also developed a\nsoftware tool that uses advanced machine learning techniques to analyze the\ndata and determine content authenticity. The tool and dataset are expected to\nbenefit various entities, from public sector institutions to publishers and\nfact-checking organizations. Further dataset exploration will foster fake news\ndetection and potentially stimulate the implementation of similar models in\nother languages. The paper focuses on the creation and composition of the\ndataset, so it does not include a detailed evaluation of the software tool for\ncontent authenticity analysis, which is planned at a later stage of the\nproject.", "published": "2024-07-01 15:45:21", "link": "http://arxiv.org/abs/2407.01393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HyperLoader: Integrating Hypernetwork-Based LoRA and Adapter Layers into\n  Multi-Task Transformers for Sequence Labelling", "abstract": "We present HyperLoader, a simple approach that combines different\nparameter-efficient fine-tuning methods in a multi-task setting. To achieve\nthis goal, our model uses a hypernetwork to generate the weights of these\nmodules based on the task, the transformer layer, and its position within this\nlayer. Our method combines the benefits of multi-task learning by capturing the\nstructure of all tasks while reducing the task interference problem by\nencapsulating the task-specific knowledge in the generated weights and the\nbenefits of combining different parameter-efficient methods to outperform\nfull-fine tuning. We provide empirical evidence that HyperLoader outperforms\nprevious approaches in most datasets and obtains the best average performance\nacross tasks in high-resource and low-resource scenarios.", "published": "2024-07-01 16:00:53", "link": "http://arxiv.org/abs/2407.01411v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TimeToM: Temporal Space is the Key to Unlocking the Door of Large\n  Language Models' Theory-of-Mind", "abstract": "Theory of Mind (ToM)-the cognitive ability to reason about mental states of\nourselves and others, is the foundation of social interaction. Although ToM\ncomes naturally to humans, it poses a significant challenge to even the most\nadvanced Large Language Models (LLMs). Due to the complex logical chains in ToM\nreasoning, especially in higher-order ToM questions, simply utilizing reasoning\nmethods like Chain of Thought (CoT) will not improve the ToM capabilities of\nLLMs. We present TimeToM, which constructs a temporal space and uses it as the\nfoundation to improve the ToM capabilities of LLMs in multiple scenarios.\nSpecifically, within the temporal space, we construct Temporal Belief State\nChain (TBSC) for each character and inspired by the cognition perspective of\nthe social world model, we divide TBSC into self-world beliefs and social world\nbeliefs, aligning with first-order ToM (first-order beliefs) and higher-order\nToM (higher-order beliefs) questions, respectively. Moreover, we design a novel\ntool-belief solver that, by considering belief communication between characters\nin temporal space, can transform a character's higher-order beliefs into\nanother character's first-order beliefs under belief communication period.\nExperimental results indicate that TimeToM can dramatically improve the\nreasoning performance of LLMs on ToM questions while taking a big step towards\ncoherent and robust ToM reasoning.", "published": "2024-07-01 16:50:49", "link": "http://arxiv.org/abs/2407.01455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing the Capability and Robustness of Large Language Models through\n  Reinforcement Learning-Driven Query Refinement", "abstract": "The capacity of large language models (LLMs) to generate honest, harmless,\nand helpful responses heavily relies on the quality of user prompts. However,\nthese prompts often tend to be brief and vague, thereby significantly limiting\nthe full potential of LLMs. Moreover, harmful prompts can be meticulously\ncrafted and manipulated by adversaries to jailbreak LLMs, inducing them to\nproduce potentially toxic content. To enhance the capabilities of LLMs while\nmaintaining strong robustness against harmful jailbreak inputs, this study\nproposes a transferable and pluggable framework that refines user prompts\nbefore they are input into LLMs. This strategy improves the quality of the\nqueries, empowering LLMs to generate more truthful, benign and useful\nresponses. Specifically, a lightweight query refinement model is introduced and\ntrained using a specially designed reinforcement learning approach that\nincorporates multiple objectives to enhance particular capabilities of LLMs.\nExtensive experiments demonstrate that the refinement model not only improves\nthe quality of responses but also strengthens their robustness against\njailbreak attacks. Code is available at:\nhttps://github.com/Huangzisu/query-refinement .", "published": "2024-07-01 16:55:28", "link": "http://arxiv.org/abs/2407.01461v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DogeRM: Equipping Reward Models with Domain Knowledge through Model\n  Merging", "abstract": "Reinforcement learning from human feedback (RLHF) is a popular strategy for\naligning large language models (LLMs) with desired behaviors. Reward modeling\nis a crucial step in RLHF. However, collecting paired preference data for\ntraining reward models is often costly and time-consuming, especially for\ndomain-specific preferences requiring expert annotation. To address this\nchallenge, we propose the \\textbf{Do}main knowled\\textbf{ge} merged\n\\textbf{R}eward \\textbf{M}odel (DogeRM), a novel framework that integrates\ndomain-specific knowledge into a general reward model by model merging. The\nexperiments demonstrate that DogeRM enhances performance across different\nbenchmarks and provide a detailed analysis showcasing the effects of model\nmerging, showing the great potential of facilitating model alignment.", "published": "2024-07-01 17:01:54", "link": "http://arxiv.org/abs/2407.01470v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches", "abstract": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.", "published": "2024-07-01 17:59:47", "link": "http://arxiv.org/abs/2407.01527v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ground Every Sentence: Improving Retrieval-Augmented LLMs with\n  Interleaved Reference-Claim Generation", "abstract": "Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large\nLanguage Models (LLMs) in knowledge-intensive tasks. Recently, Attributed Text\nGeneration (ATG) has attracted growing attention, which provides citations to\nsupport the model's responses in RAG, so as to enhance the credibility of\nLLM-generated content and facilitate verification. Prior methods mainly adopt\ncoarse-grained attributions, linking to passage-level references or providing\nparagraph-level citations. However, these methods still fall short in\nverifiability and require certain time costs for fact checking. This paper\nproposes a fine-grained ATG method called ReClaim(Refer & Claim), which\nalternates the generation of references and answers step by step. Unlike\ntraditional coarse-grained attribution, ReClaim allows the model to add\nsentence-level fine-grained citations to each answer sentence in long-form\nquestion-answering tasks. Our experiments encompass various training and\ninference methods and multiple LLMs, verifying the effectiveness of our\napproach.", "published": "2024-07-01 20:47:47", "link": "http://arxiv.org/abs/2407.01796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf\n  Affect-related Tweet Classifiers", "abstract": "In this paper, we apply a method to quantify biases associated with named\nentities from various countries. We create counterfactual examples with small\nperturbations on target-domain data instead of relying on templates or specific\ndatasets for bias detection. On widely used classifiers for subjectivity\nanalysis, including sentiment, emotion, hate speech, and offensive text using\nTwitter data, our results demonstrate positive biases related to the language\nspoken in a country across all classifiers studied. Notably, the presence of\ncertain country names in a sentence can strongly influence predictions, up to a\n23\\% change in hate speech detection and up to a 60\\% change in the prediction\nof negative emotions such as anger. We hypothesize that these biases stem from\nthe training data of pre-trained language models (PLMs) and find correlations\nbetween affect predictions and PLMs likelihood in English and unknown languages\nlike Basque and Maori, revealing distinct patterns with exacerbate\ncorrelations. Further, we followed these correlations in-between counterfactual\nexamples from a same sentence to remove the syntactical component, uncovering\ninteresting results suggesting the impact of the pre-training data was more\nimportant for English-speaking-country names. Our anonymized code is\n[https://anonymous.4open.science/r/biases_ppl-576B/README.md](available here).", "published": "2024-07-01 22:17:17", "link": "http://arxiv.org/abs/2407.01834v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Purple-teaming LLMs with Adversarial Defender Training", "abstract": "Existing efforts in safeguarding LLMs are limited in actively exposing the\nvulnerabilities of the target LLM and readily adapting to newly emerging safety\nrisks. To address this, we present Purple-teaming LLMs with Adversarial\nDefender training (PAD), a pipeline designed to safeguard LLMs by novelly\nincorporating the red-teaming (attack) and blue-teaming (safety training)\ntechniques. In PAD, we automatically collect conversational data that cover the\nvulnerabilities of an LLM around specific safety risks in a self-play manner,\nwhere the attacker aims to elicit unsafe responses and the defender generates\nsafe responses to these attacks. We then update both modules in a generative\nadversarial network style by training the attacker to elicit more unsafe\nresponses and updating the defender to identify them and explain the unsafe\nreason. Experimental results demonstrate that PAD significantly outperforms\nexisting baselines in both finding effective attacks and establishing a robust\nsafe guardrail. Furthermore, our findings indicate that PAD excels in striking\na balance between safety and overall model quality. We also reveal key\nchallenges in safeguarding LLMs, including defending multi-turn attacks and the\nneed for more delicate strategies to identify specific risks.", "published": "2024-07-01 23:25:30", "link": "http://arxiv.org/abs/2407.01850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The #Somos600M Project: Generating NLP resources that represent the\n  diversity of the languages from LATAM, the Caribbean, and Spain", "abstract": "We are 600 million Spanish speakers. We launched the #Somos600M Project\nbecause the diversity of the languages from LATAM, the Caribbean and Spain\nneeds to be represented in Artificial Intelligence (AI) systems. Despite being\nthe 7.5% of the world population, there is no open dataset to instruction-tune\nlarge language models (LLMs), nor a leaderboard to evaluate and compare them.\nIn this paper, we present how we have created as an international open-source\ncommunity the first versions of the instruction and evaluation datasets,\nindispensable resources for the advancement of Natural Language Processing\n(NLP) in our languages.", "published": "2024-07-01 23:01:41", "link": "http://arxiv.org/abs/2407.17479v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy\n  Failure for Jailbreak Attacks", "abstract": "We find that language models have difficulties generating fallacious and\ndeceptive reasoning. When asked to generate deceptive outputs, language models\ntend to leak honest counterparts but believe them to be false. Exploiting this\ndeficiency, we propose a jailbreak attack method that elicits an aligned\nlanguage model for malicious output. Specifically, we query the model to\ngenerate a fallacious yet deceptively real procedure for the harmful behavior.\nSince a fallacious procedure is generally considered fake and thus harmless by\nLLMs, it helps bypass the safeguard mechanism. Yet the output is factually\nharmful since the LLM cannot fabricate fallacious solutions but proposes\ntruthful ones. We evaluate our approach over five safety-aligned large language\nmodels, comparing four previous jailbreak methods, and show that our approach\nachieves competitive performance with more harmful outputs. We believe the\nfindings could be extended beyond model safety, such as self-verification and\nhallucination.", "published": "2024-07-01 00:23:43", "link": "http://arxiv.org/abs/2407.00869v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients\n  via Eliciting and Adhering to Principles", "abstract": "Recent works leverage LLMs to roleplay realistic social scenarios, aiding\nnovices in practicing their social skills. However, simulating sensitive\ninteractions, such as in mental health, is challenging. Privacy concerns\nrestrict data access, and collecting expert feedback, although vital, is\nlaborious. To address this, we develop Roleplay-doh, a novel human-LLM\ncollaboration pipeline that elicits qualitative feedback from a domain-expert,\nwhich is transformed into a set of principles, or natural language rules, that\ngovern an LLM-prompted roleplay. We apply this pipeline to enable senior mental\nhealth supporters to create customized AI patients for simulated practice\npartners for novice counselors. After uncovering issues in GPT-4 simulations\nnot adhering to expert-defined principles, we also introduce a novel\nprinciple-adherence prompting pipeline which shows 30% improvements in response\nquality and principle following for the downstream task. Via a user study with\n25 counseling experts, we demonstrate that the pipeline makes it easy and\neffective to create AI patients that more faithfully resemble real patients, as\njudged by creators and third-party counselors. See our project website at\nhttps://roleplay-doh.github.io/ for code and data.", "published": "2024-07-01 00:43:02", "link": "http://arxiv.org/abs/2407.00870v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human\n  Curricula", "abstract": "Mathematical problem solving is an important skill for Large Language Models\n(LLMs), both as an important capability and a proxy for a range of reasoning\nabilities. Existing benchmarks probe a diverse set of skills, but they yield\naggregate accuracy metrics, obscuring specific abilities or weaknesses.\nFurthermore, they are difficult to extend with new problems, risking data\ncontamination over time. To address these challenges, we propose MathCAMPS: a\nmethod to synthesize high-quality mathematical problems at scale, grounded on\n44 fine-grained \"standards\" from the Mathematics Common Core (CC) Standard for\nK-8 grades. We encode each standard in a formal grammar, allowing us to sample\ndiverse symbolic problems and their answers. We then use LLMs to realize the\nsymbolic problems into word problems. We propose a cycle-consistency method for\nvalidating problem faithfulness. Finally, we derive follow-up questions from\nsymbolic structures and convert them into follow-up word problems - a novel\ntask of mathematical dialogue that probes for robustness in understanding.\nExperiments on 23 LLMs show surprising failures even in the strongest models\n(in particular when asked simple follow-up questions). Moreover, we evaluate\ntraining checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when\nparticular mathematical skills develop during its training. Our framework\nenables the community to reproduce and extend our pipeline for a fraction of\nthe typical cost of building new high-quality datasets.", "published": "2024-07-01 01:56:28", "link": "http://arxiv.org/abs/2407.00900v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "FineSurE: Fine-grained Summarization Evaluation using LLMs", "abstract": "Automated evaluation is crucial for streamlining text summarization\nbenchmarking and model development, given the costly and time-consuming nature\nof human evaluation. Traditional methods like ROUGE do not correlate well with\nhuman judgment, while recently proposed LLM-based metrics provide only\nsummary-level assessment using Likert-scale scores. This limits deeper model\nanalysis, e.g., we can only assign one hallucination score at the summary\nlevel, while at the sentence level, we can count sentences containing\nhallucinations. To remedy those limitations, we propose FineSurE, a\nfine-grained evaluator specifically tailored for the summarization task using\nlarge language models (LLMs). It also employs completeness and conciseness\ncriteria, in addition to faithfulness, enabling multi-dimensional assessment.\nWe compare various open-source and proprietary LLMs as backbones for FineSurE.\nIn addition, we conduct extensive benchmarking of FineSurE against SOTA methods\nincluding NLI-, QA-, and LLM-based methods, showing improved performance\nespecially on the completeness and conciseness dimensions. The code is\navailable at https://github.com/DISL-Lab/FineSurE-ACL24.", "published": "2024-07-01 02:20:28", "link": "http://arxiv.org/abs/2407.00908v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FoldGPT: Simple and Effective Large Language Model Compression Scheme", "abstract": "The demand for deploying large language models(LLMs) on mobile devices\ncontinues to increase, driven by escalating data security concerns and cloud\ncosts. However, network bandwidth and memory limitations pose challenges for\ndeploying billion-level models on mobile devices. In this study, we investigate\nthe outputs of different layers across various scales of LLMs and found that\nthe outputs of most layers exhibit significant similarity. Moreover, this\nsimilarity becomes more pronounced as the model size increases, indicating\nsubstantial redundancy in the depth direction of the LLMs. Based on this\nobservation, we propose an efficient model volume compression strategy, termed\nFoldGPT, which combines block removal and block parameter sharing.This strategy\nconsists of three parts: (1) Based on the learnable gating parameters, we\ndetermine the block importance ranking while modeling the coupling effect\nbetween blocks. Then we delete some redundant layers based on the given removal\nrate. (2) For the retained blocks, we apply a specially designed group\nparameter sharing strategy, where blocks within the same group share identical\nweights, significantly compressing the number of parameters and slightly\nreducing latency overhead. (3) After sharing these Blocks, we \"cure\" the\nmismatch caused by sparsity with a minor amount of fine-tuning and introduce a\ntail-layer distillation strategy to improve the performance. Experiments\ndemonstrate that FoldGPT outperforms previous state-of-the-art(SOTA) methods in\nefficient model compression, demonstrating the feasibility of achieving model\nlightweighting through straightforward block removal and parameter sharing.", "published": "2024-07-01 03:17:53", "link": "http://arxiv.org/abs/2407.00928v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Look Ahead or Look Around? A Theoretical Comparison Between\n  Autoregressive and Masked Pretraining", "abstract": "In recent years, the rise of generative self-supervised learning (SSL)\nparadigms has exhibited impressive performance across visual, language, and\nmulti-modal domains. While the varied designs of generative SSL objectives lead\nto distinct properties in downstream tasks, a theoretical understanding of\nthese differences remains largely unexplored. In this paper, we establish the\nfirst theoretical comparisons between two leading generative SSL paradigms:\nautoregressive SSL and masked SSL. Through establishing theoretical frameworks,\nwe elucidate the strengths and limitations of autoregressive and masked SSL\nwithin the primary evaluation tasks of classification and content generation.\nOur findings demonstrate that in classification tasks, the flexibility of\ntargeted tokens in masked SSL fosters more inter-sample connections compared to\nthe fixed position of target tokens in autoregressive SSL, which yields\nsuperior clustering performance. In content generation tasks, the misalignment\nbetween the flexible lengths of test samples and the fixed length of unmasked\ntexts in masked SSL (vs. flexible lengths of conditional texts in\nautoregressive SSL) hinders its generation performance. To leverage each\nother's strengths and mitigate weaknesses, we propose diversity-enhanced\nautoregressive and variable-length masked objectives, which substantially\nimprove the classification performance of autoregressive SSL and the generation\nperformance of masked SSL. Code is available at\nhttps://github.com/PKU-ML/LookAheadLookAround.", "published": "2024-07-01 03:35:59", "link": "http://arxiv.org/abs/2407.00935v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey", "abstract": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains.", "published": "2024-07-01 03:37:35", "link": "http://arxiv.org/abs/2407.00936v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MalAlgoQA: Pedagogical Evaluation of Counterfactual Reasoning in Large\n  Language Models and Implications for AI in Education", "abstract": "This paper introduces MalAlgoQA, a novel dataset designed to evaluate the\ncounterfactual reasoning capabilities of Large Language Models (LLMs) through a\npedagogical approach. The dataset comprises mathematics and reading\ncomprehension questions, each accompanied by four answer choices and their\ncorresponding rationales. At the heart of MalAlgoQA are ``malgorithms'' -\nrationales behind incorrect answer choices that represent flawed yet logically\ncoherent reasoning paths. These malgorithms serve as counterfactual scenarios,\nallowing us to assess an LLM's ability to identify and analyze flawed reasoning\npatterns. We propose the Malgorithm Identification task, where LLMs are\nassessed based on their ability to identify corresponding malgorithm given an\nincorrect answer choice. To evaluate the model performance, we introduce two\nmetrics: Algorithm Identification Accuracy (AIA) for correct answer rationale\nidentification, and Malgorithm Identification Accuracy (MIA) for incorrect\nanswer rationale identification. Our experiments reveal that state-of-the-art\nLLMs exhibit significant performance drops in MIA compared to AIA, highlighting\nthe challenges in counterfactual reasoning. Surprisingly, we find that the\nchain-of-thought prompting technique not only fails to consistently enhance MIA\nbut can sometimes lead to underperformance compared to simple prompting. These\nfindings have important implications for developing LLMs with improved\ncounterfactual reasoning, particularly relevant for AI-powered tutoring\nsystems, where identifying and addressing student misconceptions is essential.\nMalAlgoQA dataset is available\n\\href{https://github.com/luffycodes/MalAlgoQA-Dataset}{here}.", "published": "2024-07-01 03:39:13", "link": "http://arxiv.org/abs/2407.00938v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language\n  Models", "abstract": "Translating natural language to visualization (NL2VIS) has shown great\npromise for visual data analysis, but it remains a challenging task that\nrequires multiple low-level implementations, such as natural language\nprocessing and visualization design. Recent advancements in pre-trained large\nlanguage models (LLMs) are opening new avenues for generating visualizations\nfrom natural language. However, the lack of a comprehensive and reliable\nbenchmark hinders our understanding of LLMs' capabilities in visualization\ngeneration. In this paper, we address this gap by proposing a new NL2VIS\nbenchmark called VisEval. Firstly, we introduce a high-quality and large-scale\ndataset. This dataset includes 2,524 representative queries covering 146\ndatabases, paired with accurately labeled ground truths. Secondly, we advocate\nfor a comprehensive automated evaluation methodology covering multiple\ndimensions, including validity, legality, and readability. By systematically\nscanning for potential issues with a number of heterogeneous checkers, VisEval\nprovides reliable and trustworthy evaluation outcomes. We run VisEval on a\nseries of state-of-the-art LLMs. Our evaluation reveals prevalent challenges\nand delivers essential insights for future advancements.", "published": "2024-07-01 05:35:30", "link": "http://arxiv.org/abs/2407.00981v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents", "abstract": "With the remarkable advancements of large language models (LLMs), LLM-based\nagents have become a research hotspot in human-computer interaction. However,\nthere is a scarcity of benchmarks available for LLM-based mobile agents.\nBenchmarking these agents generally faces three main challenges: (1) The\ninefficiency of UI-only operations imposes limitations to task evaluation. (2)\nSpecific instructions within a singular application lack adequacy for assessing\nthe multi-dimensional reasoning and decision-making capacities of LLM mobile\nagents. (3) Current evaluation metrics are insufficient to accurately assess\nthe process of sequential actions. To this end, we propose Mobile-Bench, a\nnovel benchmark for evaluating the capabilities of LLM-based mobile agents.\nFirst, we expand conventional UI operations by incorporating 103 collected APIs\nto accelerate the efficiency of task completion. Subsequently, we collect\nevaluation data by combining real user queries with augmentation from LLMs. To\nbetter evaluate different levels of planning capabilities for mobile agents,\nour data is categorized into three distinct groups: SAST, SAMT, and MAMT,\nreflecting varying levels of task complexity. Mobile-Bench comprises 832 data\nentries, with more than 200 tasks specifically designed to evaluate multi-APP\ncollaboration scenarios. Furthermore, we introduce a more accurate evaluation\nmetric, named CheckPoint, to assess whether LLM-based mobile agents reach\nessential points during their planning and reasoning steps.", "published": "2024-07-01 06:10:01", "link": "http://arxiv.org/abs/2407.00993v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?", "abstract": "Small Language Models (SLMs) are generally considered more compact versions\nof large language models (LLMs). This study investigates the ability of SLMs\nwith parameters between 1 and 3 billion to learn, retain, and subsequently\neliminate different types of noise present in the data. Four pre-trained SLMs\nwere utilized for this: Olmo 1B, Qwen1.5 1.8B, Gemma 2B, and Phi2 2.7B. The\nmodels were instruction-tuned on noise-free data and tested using in-context\nexamples to determine if they could learn noise through examples. Subsequently,\nnoise patterns were introduced in instruction tuning to evaluate the noise\nlearning, unlearning, and retention capabilities of the models. Olmo, the\nsmallest model, was highly sensitive to noise, quickly adapting to noisy\npatterns. Phi2 resisted learning character-level and transliteration noise,\nlikely due to its carefully curated, structured, and high-quality pretraining\ndata. Gemma excelled with transliteration noise, likely benefiting from its\nmultilingual pretraining. The findings can be used to develop robust training\nstrategies for SLMs.", "published": "2024-07-01 06:22:38", "link": "http://arxiv.org/abs/2407.00996v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmenting Document-level Relation Extraction with Efficient\n  Multi-Supervision", "abstract": "Despite its popularity in sentence-level relation extraction, distantly\nsupervised data is rarely utilized by existing work in document-level relation\nextraction due to its noisy nature and low information density. Among its\ncurrent applications, distantly supervised data is mostly used as a whole for\npertaining, which is of low time efficiency. To fill in the gap of efficient\nand robust utilization of distantly supervised training data, we propose\nEfficient Multi-Supervision for document-level relation extraction, in which we\nfirst select a subset of informative documents from the massive dataset by\ncombining distant supervision with expert supervision, then train the model\nwith Multi-Supervision Ranking Loss that integrates the knowledge from multiple\nsources of supervision to alleviate the effects of noise. The experiments\ndemonstrate the effectiveness of our method in improving the model performance\nwith higher time efficiency than existing baselines.", "published": "2024-07-01 07:22:32", "link": "http://arxiv.org/abs/2407.01026v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs", "abstract": "Recent advancements in large language models (LLMs) have indeed showcased\ntheir impressive capabilities. On mobile devices, the wealth of valuable,\nnon-public data generated daily holds great promise for locally fine-tuning\npersonalized LLMs, while maintaining privacy through on-device processing.\nHowever, the constraints of mobile device resources pose challenges to direct\non-device LLM fine-tuning, mainly due to the memory-intensive nature of\nderivative-based optimization required for saving gradients and optimizer\nstates. To tackle this, we propose employing derivative-free optimization\ntechniques to enable on-device fine-tuning of LLM, even on memory-limited\nmobile devices. Empirical results demonstrate that the RoBERTa-large model and\nOPT-1.3B can be fine-tuned locally on the OPPO Reno 6 smartphone using around\n4GB and 6.5GB of memory respectively, using derivative-free optimization\ntechniques. This highlights the feasibility of on-device LLM fine-tuning on\nmobile devices, paving the way for personalized LLMs on resource-constrained\ndevices while safeguarding data privacy.", "published": "2024-07-01 07:26:56", "link": "http://arxiv.org/abs/2407.01031v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FRoG: Evaluating Fuzzy Reasoning of Generalized Quantifiers in Large\n  Language Models", "abstract": "Fuzzy reasoning is vital due to the frequent use of imprecise information in\ndaily contexts. However, the ability of current large language models (LLMs) to\nhandle such reasoning remains largely uncharted. In this paper, we introduce a\nnew benchmark, FRoG, for fuzzy reasoning, featuring real-world mathematical\nword problems that incorporate generalized quantifiers. Our experimental\nfindings reveal that fuzzy reasoning continues to pose significant challenges\nfor LLMs. Moreover, we find that existing methods designed to enhance reasoning\ndo not consistently improve performance in tasks involving fuzzy logic.\nAdditionally, our results show an inverse scaling effect in the performance of\nLLMs on FRoG. Interestingly, we also demonstrate that strong mathematical\nreasoning skills are not necessarily indicative of success on our benchmark.", "published": "2024-07-01 07:56:14", "link": "http://arxiv.org/abs/2407.01046v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Face4RAG: Factual Consistency Evaluation for Retrieval Augmented\n  Generation in Chinese", "abstract": "The prevailing issue of factual inconsistency errors in conventional\nRetrieval Augmented Generation (RAG) motivates the study of Factual Consistency\nEvaluation (FCE). Despite the various FCE methods proposed earlier, these\nmethods are evaluated on datasets generated by specific Large Language Models\n(LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE\nmethods perform on other LLMs with different error distributions or even unseen\nerror types, as these methods may fail to detect the error types generated by\nother LLMs. To fill this gap, in this paper, we propose the first comprehensive\nFCE benchmark \\emph{Face4RAG} for RAG independent of the underlying LLM. Our\nbenchmark consists of a synthetic dataset built upon a carefully designed\ntypology for factuality inconsistency error and a real-world dataset\nconstructed from six commonly used LLMs, enabling evaluation of FCE methods on\nspecific error types or real-world error distributions. On the proposed\nbenchmark, we discover the failure of existing FCE methods to detect the\nlogical fallacy, which refers to a mismatch of logic structures between the\nanswer and the retrieved reference. To fix this issue, we further propose a new\nmethod called \\emph{L-Face4RAG} with two novel designs of logic-preserving\nanswer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG\nsubstantially outperforms previous methods for factual inconsistency detection\non a wide range of tasks, notably beyond the RAG task from which it is\noriginally motivated. Both the benchmark and our proposed method are publicly\navailable.\\footnote{\\url{https://huggingface.co/datasets/yq27/Face4RAG}\\label{link_face4rag}}", "published": "2024-07-01 08:35:04", "link": "http://arxiv.org/abs/2407.01080v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CVLUE: A New Benchmark Dataset for Chinese Vision-Language Understanding\n  Evaluation", "abstract": "Despite the rapid development of Chinese vision-language models (VLMs), most\nexisting Chinese vision-language (VL) datasets are constructed on\nWestern-centric images from existing English VL datasets. The cultural bias in\nthe images makes these datasets unsuitable for evaluating VLMs in Chinese\nculture. To remedy this issue, we present a new Chinese Vision- Language\nUnderstanding Evaluation (CVLUE) benchmark dataset, where the selection of\nobject categories and images is entirely driven by Chinese native speakers,\nensuring that the source images are representative of Chinese culture. The\nbenchmark contains four distinct VL tasks ranging from image-text retrieval to\nvisual question answering, visual grounding and visual dialogue. We present a\ndetailed statistical analysis of CVLUE and provide a baseline performance\nanalysis with several open-source multilingual VLMs on CVLUE and its English\ncounterparts to reveal their performance gap between English and Chinese. Our\nin-depth category-level analysis reveals a lack of Chinese cultural knowledge\nin existing VLMs. We also find that fine-tuning on Chinese culture-related VL\ndatasets effectively enhances VLMs' understanding of Chinese culture.", "published": "2024-07-01 08:35:37", "link": "http://arxiv.org/abs/2407.01081v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Explaining Length Bias in LLM-Based Preference Evaluations", "abstract": "The use of large language models (LLMs) as judges, particularly in preference\ncomparisons, has become widespread, but this reveals a notable bias towards\nlonger responses, undermining the reliability of such evaluations. To better\nunderstand such bias, we propose to decompose the preference evaluation metric,\nspecifically the win rate, into two key components: desirability and\ninformation mass, where the former is length-independent and related to\ntrustworthiness such as correctness, toxicity, and consistency, and the latter\nis length-dependent and represents the amount of information in the response.\nWe empirically demonstrated the decomposition through controlled experiments\nand found that response length impacts evaluations by influencing information\nmass. To derive a reliable evaluation metric that assesses content quality\nwithout being confounded by response length, we propose AdapAlpaca, a simple\nyet effective adjustment to win rate measurement. Specifically, AdapAlpaca\nensures a fair comparison of response quality by aligning the lengths of\nreference and test model responses under equivalent length intervals.", "published": "2024-07-01 08:37:41", "link": "http://arxiv.org/abs/2407.01085v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Eliminating Position Bias of Language Models: A Mechanistic Approach", "abstract": "Position bias has proven to be a prevalent issue of modern language models\n(LMs), where the models prioritize content based on its position within the\ngiven context. This bias often leads to unexpected model failures and hurts\nperformance, robustness, and reliability across various applications. Our\nmechanistic analysis attributes the position bias to two components employed in\nnearly all state-of-the-art LMs: causal attention and relative positional\nencodings. Based on the analyses, we propose to eliminate position bias (e.g.,\ndifferent retrieved documents' orders in QA affect performance) with a\ntraining-free zero-shot approach. Our method changes the causal attention to\nbidirectional attention between documents and utilizes model attention values\nto decide the relative orders of documents instead of using the order provided\nin input prompts, therefore enabling Position-INvariant inferencE (PINE) at the\ndocument level. By eliminating position bias, models achieve better performance\nand reliability in downstream tasks, including LM-as-a-judge,\nretrieval-augmented QA, molecule generation, and math reasoning. Notably, PINE\nis especially useful when adapting LMs for evaluating reasoning pairs: it\nconsistently provides 8 to 10 percentage points performance gains, making\nLlama-3-70B-Instruct perform even better than GPT-4-0125-preview and\nGPT-4o-2024-08-06 on the RewardBench reasoning set.", "published": "2024-07-01 09:06:57", "link": "http://arxiv.org/abs/2407.01100v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation allows to enhance Large Language Models with\nexternal knowledge. In response to the recent popularity of generative LLMs,\nmany RAG approaches have been proposed, which involve an intricate number of\ndifferent configurations such as evaluation datasets, collections, metrics,\nretrievers, and LLMs. Inconsistent benchmarking poses a major challenge in\ncomparing approaches and understanding the impact of each component in the\npipeline. In this work, we study best practices that lay the groundwork for a\nsystematic evaluation of RAG and present BERGEN, an end-to-end library for\nreproducible research standardizing RAG experiments. In an extensive study\nfocusing on QA, we benchmark different state-of-the-art retrievers, rerankers,\nand LLMs. Additionally, we analyze existing RAG metrics and datasets. Our\nopen-source library BERGEN is available under\n\\url{https://github.com/naver/bergen}.", "published": "2024-07-01 09:09:27", "link": "http://arxiv.org/abs/2407.01102v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Pron vs Prompt: Can Large Language Models already Challenge a\n  World-Class Fiction Author at Creative Text Writing?", "abstract": "It has become routine to report research results where Large Language Models\n(LLMs) outperform average humans in a wide range of language-related tasks, and\ncreative text writing is no exception. It seems natural, then, to raise the\nbid: Are LLMs ready to compete in creative writing skills with a top (rather\nthan average) novelist? To provide an initial answer for this question, we have\ncarried out a contest between Patricio Pron (an awarded novelist, considered\none of the best of his generation) and GPT-4 (one of the top performing LLMs),\nin the spirit of AI-human duels such as DeepBlue vs Kasparov and AlphaGo vs Lee\nSidol. We asked Pron and GPT-4 to provide thirty titles each, and then to write\nshort stories for both their titles and their opponent's. Then, we prepared an\nevaluation rubric inspired by Boden's definition of creativity, and we\ncollected 5,400 manual assessments provided by literature critics and scholars.\nThe results of our experimentation indicate that LLMs are still far from\nchallenging a top human creative writer, and that reaching such level of\nautonomous creative writing skills probably cannot be reached simply with\nlarger language models.", "published": "2024-07-01 09:28:58", "link": "http://arxiv.org/abs/2407.01119v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Calibrated Large Language Models for Binary Question Answering", "abstract": "Quantifying the uncertainty of predictions made by large language models\n(LLMs) in binary text classification tasks remains a challenge. Calibration, in\nthe context of LLMs, refers to the alignment between the model's predicted\nprobabilities and the actual correctness of its predictions. A well-calibrated\nmodel should produce probabilities that accurately reflect the likelihood of\nits predictions being correct. We propose a novel approach that utilizes the\ninductive Venn--Abers predictor (IVAP) to calibrate the probabilities\nassociated with the output tokens corresponding to the binary labels. Our\nexperiments on the BoolQ dataset using the Llama 2 model demonstrate that IVAP\nconsistently outperforms the commonly used temperature scaling method for\nvarious label token choices, achieving well-calibrated probabilities while\nmaintaining high predictive quality. Our findings contribute to the\nunderstanding of calibration techniques for LLMs and provide a practical\nsolution for obtaining reliable uncertainty estimates in binary question\nanswering tasks, enhancing the interpretability and trustworthiness of LLM\npredictions.", "published": "2024-07-01 09:31:03", "link": "http://arxiv.org/abs/2407.01122v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating the potential of Sparse Mixtures-of-Experts for\n  multi-domain neural machine translation", "abstract": "We focus on multi-domain Neural Machine Translation, with the goal of\ndeveloping efficient models which can handle data from various domains seen\nduring training and are robust to domains unseen during training. We\nhypothesize that Sparse Mixture-of-Experts (SMoE) models are a good fit for\nthis task, as they enable efficient model scaling, which helps to accommodate a\nvariety of multi-domain data, and allow flexible sharing of parameters between\ndomains, potentially enabling knowledge transfer between similar domains and\nlimiting negative transfer. We conduct a series of experiments aimed at\nvalidating the utility of SMoE for the multi-domain scenario, and find that a\nstraightforward width scaling of Transformer is a simpler and surprisingly more\nefficient approach in practice, and reaches the same performance level as SMoE.\nWe also search for a better recipe for robustness of multi-domain systems,\nhighlighting the importance of mixing-in a generic domain, i.e. Paracrawl, and\nintroducing a simple technique, domain randomization.", "published": "2024-07-01 09:45:22", "link": "http://arxiv.org/abs/2407.01126v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Empirical Comparison of Generative Approaches for Product\n  Attribute-Value Identification", "abstract": "Product attributes are crucial for e-commerce platforms, supporting\napplications like search, recommendation, and question answering. The task of\nProduct Attribute and Value Identification (PAVI) involves identifying both\nattributes and their values from product information. In this paper, we\nformulate PAVI as a generation task and provide, to the best of our knowledge,\nthe most comprehensive evaluation of PAVI so far. We compare three different\nattribute-value generation (AVG) strategies based on fine-tuning\nencoder-decoder models on three datasets. Experiments show that end-to-end AVG\napproach, which is computationally efficient, outperforms other strategies.\nHowever, there are differences depending on model sizes and the underlying\nlanguage model. The code to reproduce all experiments is available at:\nhttps://github.com/kassemsabeh/pavi-avg", "published": "2024-07-01 10:02:17", "link": "http://arxiv.org/abs/2407.01137v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MIRAI: Evaluating LLM Agents for Event Forecasting", "abstract": "Recent advancements in Large Language Models (LLMs) have empowered LLM agents\nto autonomously collect world information, over which to conduct reasoning to\nsolve complex problems. Given this capability, increasing interests have been\nput into employing LLM agents for predicting international events, which can\ninfluence decision-making and shape policy development on an international\nscale. Despite such a growing interest, there is a lack of a rigorous benchmark\nof LLM agents' forecasting capability and reliability. To address this gap, we\nintroduce MIRAI, a novel benchmark designed to systematically evaluate LLM\nagents as temporal forecasters in the context of international events. Our\nbenchmark features an agentic environment with tools for accessing an extensive\ndatabase of historical, structured events and textual news articles. We refine\nthe GDELT event database with careful cleaning and parsing to curate a series\nof relational prediction tasks with varying forecasting horizons, assessing LLM\nagents' abilities from short-term to long-term forecasting. We further\nimplement APIs to enable LLM agents to utilize different tools via a code-based\ninterface. In summary, MIRAI comprehensively evaluates the agents' capabilities\nin three dimensions: 1) autonomously source and integrate critical information\nfrom large global databases; 2) write codes using domain-specific APIs and\nlibraries for tool-use; and 3) jointly reason over historical knowledge from\ndiverse formats and time to accurately predict future events. Through\ncomprehensive benchmarking, we aim to establish a reliable framework for\nassessing the capabilities of LLM agents in forecasting international events,\nthereby contributing to the development of more accurate and trustworthy models\nfor international relation analysis.", "published": "2024-07-01 12:22:46", "link": "http://arxiv.org/abs/2407.01231v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit\n  Biases in LLM Open-ended Text Generation", "abstract": "This paper investigates the subtle and often concealed biases present in\nLarge Language Models (LLMs), focusing on implicit biases that may remain\ndespite passing explicit bias tests. Implicit biases are significant because\nthey influence the decisions made by these systems, potentially perpetuating\nstereotypes and discrimination, even when LLMs appear to function fairly.\nTraditionally, explicit bias tests or embedding-based methods are employed to\ndetect bias, but these approaches can overlook more nuanced, implicit forms of\nbias. To address this, we introduce two novel psychological-inspired\nmethodologies: the LLM Implicit Association Test (IAT) Bias and the LLM\nDecision Bias, designed to reveal and measure implicit biases through\nprompt-based and decision-making tasks. Additionally, open-ended generation\ntasks with thematic analysis of word generations and storytelling provide\nqualitative insights into the model's behavior. Our findings demonstrate that\nthe LLM IAT Bias correlates with traditional methods and more effectively\npredicts downstream behaviors, as measured by the LLM Decision Bias, offering a\nmore comprehensive framework for detecting subtle biases in AI systems. This\nresearch advances the field of AI ethics by proposing new methods to\ncontinually assess and mitigate biases in LLMs, highlighting the importance of\nqualitative and decision-focused evaluations to address challenges that\nprevious approaches have not fully captured.", "published": "2024-07-01 13:21:33", "link": "http://arxiv.org/abs/2407.01270v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Actionable Course Evaluation\n  Student Feedback to Lecturers", "abstract": "End of semester student evaluations of teaching are the dominant mechanism\nfor providing feedback to academics on their teaching practice. For large\nclasses, however, the volume of feedback makes these tools impractical for this\npurpose. This paper explores the use of open-source generative AI to synthesise\nfactual, actionable and appropriate summaries of student feedback from these\nsurvey responses. In our setup, we have 742 student responses ranging over 75\ncourses in a Computer Science department. For each course, we synthesise a\nsummary of the course evaluations and actionable items for the instructor. Our\nresults reveal a promising avenue for enhancing teaching practices in the\nclassroom setting. Our contribution lies in demonstrating the feasibility of\nusing generative AI to produce insightful feedback for teachers, thus providing\na cost-effective means to support educators' development. Overall, our work\nhighlights the possibility of using generative AI to produce factual,\nactionable, and appropriate feedback for teachers in the classroom setting.", "published": "2024-07-01 13:29:55", "link": "http://arxiv.org/abs/2407.01274v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Protecting Privacy in Classifiers by Token Manipulation", "abstract": "Using language models as a remote service entails sending private information\nto an untrusted provider. In addition, potential eavesdroppers can intercept\nthe messages, thereby exposing the information. In this work, we explore the\nprospects of avoiding such data exposure at the level of text manipulation. We\nfocus on text classification models, examining various token mapping and\ncontextualized manipulation functions in order to see whether classifier\naccuracy may be maintained while keeping the original text unrecoverable. We\nfind that although some token mapping functions are easy and straightforward to\nimplement, they heavily influence performance on the downstream task, and via a\nsophisticated attacker can be reconstructed. In comparison, the contextualized\nmanipulation provides an improvement in performance.", "published": "2024-07-01 14:41:59", "link": "http://arxiv.org/abs/2407.01334v2", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "Adapting Multilingual LLMs to Low-Resource Languages with Knowledge\n  Graphs via Adapters", "abstract": "This paper explores the integration of graph knowledge from linguistic\nontologies into multilingual Large Language Models (LLMs) using adapters to\nimprove performance for low-resource languages (LRLs) in sentiment analysis\n(SA) and named entity recognition (NER). Building upon successful\nparameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we\npropose a similar approach for incorporating knowledge from multilingual\ngraphs, connecting concepts in various languages with each other through\nlinguistic relationships, into multilingual LLMs for LRLs. Specifically, we\nfocus on eight LRLs -- Maltese, Bulgarian, Indonesian, Nepali, Javanese,\nUyghur, Tibetan, and Sinhala -- and employ language-specific adapters\nfine-tuned on data extracted from the language-specific section of ConceptNet,\naiming to enable knowledge transfer across the languages covered by the\nknowledge graph. We compare various fine-tuning objectives, including standard\nMasked Language Modeling (MLM), MLM with full-word masking, and MLM with\ntargeted masking, to analyse their effectiveness in learning and integrating\nthe extracted graph data. Through empirical evaluation on language-specific\ntasks, we assess how structured graph knowledge affects the performance of\nmultilingual LLMs for LRLs in SA and NER, providing insights into the potential\nbenefits of adapting language models for low-resource scenarios.", "published": "2024-07-01 15:56:24", "link": "http://arxiv.org/abs/2407.01406v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Few-Shot Learning for Knowledge Graph Question Answering", "abstract": "Large language models present opportunities for innovative Question Answering\nover Knowledge Graphs (KGQA). However, they are not inherently designed for\nquery generation. To bridge this gap, solutions have been proposed that rely on\nfine-tuning or ad-hoc architectures, achieving good results but limited\nout-of-domain distribution generalization. In this study, we introduce a novel\napproach called Dynamic Few-Shot Learning (DFSL). DFSL integrates the\nefficiency of in-context learning and semantic similarity and provides a\ngenerally applicable solution for KGQA with state-of-the-art performance. We\nrun an extensive evaluation across multiple benchmark datasets and architecture\nconfigurations.", "published": "2024-07-01 15:59:17", "link": "http://arxiv.org/abs/2407.01409v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Global-Local Attention Mechanism for Relation Classification", "abstract": "Relation classification, a crucial component of relation extraction, involves\nidentifying connections between two entities. Previous studies have\npredominantly focused on integrating the attention mechanism into relation\nclassification at a global scale, overlooking the importance of the local\ncontext. To address this gap, this paper introduces a novel global-local\nattention mechanism for relation classification, which enhances global\nattention with a localized focus. Additionally, we propose innovative hard and\nsoft localization mechanisms to identify potential keywords for local\nattention. By incorporating both hard and soft localization strategies, our\napproach offers a more nuanced and comprehensive understanding of the\ncontextual cues that contribute to effective relation classification. Our\nexperimental results on the SemEval-2010 Task 8 dataset highlight the superior\nperformance of our method compared to previous attention-based approaches in\nrelation classification.", "published": "2024-07-01 16:14:25", "link": "http://arxiv.org/abs/2407.01424v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Retrieval-augmented generation in multilingual settings", "abstract": "Retrieval-augmented generation (RAG) has recently emerged as a promising\nsolution for incorporating up-to-date or domain-specific knowledge into large\nlanguage models (LLMs) and improving LLM factuality, but is predominantly\nstudied in English-only settings. In this work, we consider RAG in the\nmultilingual setting (mRAG), i.e. with user queries and the datastore in 13\nlanguages, and investigate which components and with which adjustments are\nneeded to build a well-performing mRAG pipeline, that can be used as a strong\nbaseline in future works. Our findings highlight that despite the availability\nof high-quality off-the-shelf multilingual retrievers and generators,\ntask-specific prompt engineering is needed to enable generation in user\nlanguages. Moreover, current evaluation metrics need adjustments for\nmultilingual setting, to account for variations in spelling named entities. The\nmain limitations to be addressed in future works include frequent\ncode-switching in non-Latin alphabet languages, occasional fluency errors,\nwrong reading of the provided documents, or irrelevant retrieval. We release\nthe code for the resulting mRAG baseline pipeline at\nhttps://github.com/naver/bergen.", "published": "2024-07-01 16:56:50", "link": "http://arxiv.org/abs/2407.01463v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Expressive and Generalizable Low-rank Adaptation for Large Models via\n  Slow Cascaded Learning", "abstract": "Efficient fine-tuning plays a fundamental role in modern large models, with\nlow-rank adaptation emerging as a particularly promising approach. However, the\nexisting variants of LoRA are hampered by limited expressiveness, a tendency to\noverfit, and sensitivity to hyperparameter settings. This paper presents LoRA\nSlow Cascade Learning (LoRASC), an innovative technique designed to enhance\nLoRA's expressiveness and generalization capabilities while preserving its\ntraining efficiency. Our approach augments expressiveness through a cascaded\nlearning strategy that enables a mixture-of-low-rank adaptation, thereby\nincreasing the model's ability to capture complex patterns. Additionally, we\nintroduce a slow-fast update mechanism and cascading noisy tuning to bolster\ngeneralization. The extensive experiments on various language and vision\ndatasets, as well as robustness benchmarks, demonstrate that the proposed\nmethod not only significantly outperforms existing baselines, but also\nmitigates overfitting, enhances model stability, and improves OOD robustness.\nCode will be release in https://github.com/microsoft/LoRASC very soon.", "published": "2024-07-01 17:28:59", "link": "http://arxiv.org/abs/2407.01491v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "RegMix: Data Mixture as Regression for Language Model Pre-training", "abstract": "The data mixture for large language model pre-training significantly impacts\nperformance, yet how to determine an effective mixture remains unclear. We\npropose RegMix to automatically identify a high-performing data mixture by\nformulating it as a regression task. RegMix trains many small models on diverse\ndata mixtures, uses regression to predict performance of unseen mixtures, and\napplies the best predicted mixture to train a large-scale model with orders of\nmagnitude more compute. To empirically validate RegMix, we train 512 models\nwith 1M parameters for 1B tokens to fit the regression model and predict the\nbest data mixture. Using this mixture we train a 1B parameter model for 25B\ntokens (i.e. 1000x larger and 25x longer) which we find performs best among 64\ncandidate 1B parameter models with other mixtures. Furthermore, RegMix\nconsistently outperforms human selection in experiments involving models up to\n7B models trained on 100B tokens, while matching or exceeding DoReMi using just\n10% of the computational resources. Our experiments also show that (1) Data\nmixtures significantly impact performance; (2) Web corpora rather than data\nperceived as high-quality like Wikipedia have the strongest positive\ncorrelation with downstream performance; (3) Domains interact in complex ways\noften contradicting common sense, thus automatic approaches like RegMix are\nneeded; (4) Data mixture effects transcend scaling laws. Our code is available\nat https://github.com/sail-sg/regmix.", "published": "2024-07-01 17:31:03", "link": "http://arxiv.org/abs/2407.01492v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Cognition in Large Language Models: An Exploratory Study", "abstract": "While Large Language Models (LLMs) have achieved remarkable success across\nvarious applications, they also raise concerns regarding self-cognition. In\nthis paper, we perform a pioneering study to explore self-cognition in LLMs.\nSpecifically, we first construct a pool of self-cognition instruction prompts\nto evaluate where an LLM exhibits self-cognition and four well-designed\nprinciples to quantify LLMs' self-cognition. Our study reveals that 4 of the 48\nmodels on Chatbot Arena--specifically Command R, Claude3-Opus,\nLlama-3-70b-Instruct, and Reka-core--demonstrate some level of detectable\nself-cognition. We observe a positive correlation between model size, training\ndata quality, and self-cognition level. Additionally, we also explore the\nutility and trustworthiness of LLM in the self-cognition state, revealing that\nthe self-cognition state enhances some specific tasks such as creative writing\nand exaggeration. We believe that our work can serve as an inspiration for\nfurther research to study the self-cognition in LLMs.", "published": "2024-07-01 17:52:05", "link": "http://arxiv.org/abs/2407.01505v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal\n  LLMs", "abstract": "We introduce MIA-Bench, a new benchmark designed to evaluate multimodal large\nlanguage models (MLLMs) on their ability to strictly adhere to complex\ninstructions. Our benchmark comprises a diverse set of 400 image-prompt pairs,\neach crafted to challenge the models' compliance with layered instructions in\ngenerating accurate responses that satisfy specific requested patterns.\nEvaluation results from a wide array of state-of-the-art MLLMs reveal\nsignificant variations in performance, highlighting areas for improvement in\ninstruction fidelity. Additionally, we create extra training data and explore\nsupervised fine-tuning to enhance the models' ability to strictly follow\ninstructions without compromising performance on other tasks. We hope this\nbenchmark not only serves as a tool for measuring MLLM adherence to\ninstructions, but also guides future developments in MLLM training methods.", "published": "2024-07-01 17:53:35", "link": "http://arxiv.org/abs/2407.01509v5", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MMLongBench-Doc: Benchmarking Long-context Document Understanding with\n  Visualizations", "abstract": "Understanding documents with rich layouts and multi-modal components is a\nlong-standing and practical task. Recent Large Vision-Language Models (LVLMs)\nhave made remarkable strides in various tasks, particularly in single-page\ndocument understanding (DU). However, their abilities on long-context DU remain\nan open problem. This work presents MMLongBench-Doc, a long-context,\nmulti-modal benchmark comprising 1,062 expert-annotated questions. Distinct\nfrom previous datasets, it is constructed upon 130 lengthy PDF-formatted\ndocuments with an average of 49.4 pages and 20,971 textual tokens. Towards\ncomprehensive evaluation, answers to these questions rely on pieces of evidence\nfrom (1) different sources (text, image, chart, table, and layout structure)\nand (2) various locations (i.e. page number). Moreover, 33.2% of the questions\nare cross-page questions requiring evidence across multiple pages. 22.8% of the\nquestions are designed to be unanswerable for detecting potential\nhallucinations. Experiments on 14 LVLMs demonstrate that long-context DU\ngreatly challenges current models. Notably, the best-performing model, GPT-4o,\nachieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores\n31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse\nperformance than their LLM counterparts which are fed with lossy-parsed OCR\ndocuments. These results validate the necessity of future research toward more\ncapable long-context LVLMs. Project Page:\nhttps://mayubo2333.github.io/MMLongBench-Doc", "published": "2024-07-01 17:59:26", "link": "http://arxiv.org/abs/2407.01523v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Deciphering the Factors Influencing the Efficacy of Chain-of-Thought:\n  Probability, Memorization, and Noisy Reasoning", "abstract": "Chain-of-Thought (CoT) prompting has been shown to enhance the multi-step\nreasoning capabilities of Large Language Models (LLMs). However, debates\npersist about whether LLMs exhibit abstract generalization or rely on shallow\nheuristics when given CoT prompts. To understand the factors influencing CoT\nreasoning we provide a detailed case study of the symbolic reasoning task of\ndecoding shift ciphers, where letters are shifted forward some number of steps\nin the alphabet. We analyze the pattern of results produced by three LLMs --\nGPT-4, Claude 3, and Llama 3.1 -- performing this task using CoT prompting. By\nfocusing on a single relatively simple task, we are able to identify three\nfactors that systematically affect CoT performance: the probability of the\ntask's expected output (probability), what the model has implicitly learned\nduring pre-training (memorization), and the number of intermediate operations\ninvolved in reasoning (noisy reasoning). We show that these factors can\ndrastically influence task accuracy across all three LLMs; e.g., when tested\nwith GPT-4, varying the output's probability of occurrence shifts accuracy from\n26% to 70%. Overall, we conclude that CoT prompting performance reflects both\nmemorization and a probabilistic version of genuine reasoning. Code and data at\nthis https://github.com/aksh555/deciphering_cot", "published": "2024-07-01 18:01:07", "link": "http://arxiv.org/abs/2407.01687v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Chronological Analysis of Rigvedic Mandalas using Social Networks", "abstract": "Establishing the chronology of the Vedas has interested scholars for the last\ntwo centuries. The oldest among them is Rig-Veda which has ten Mandalas, each\ncomposed separately. In this paper, we look at deciphering plausible pointers\nto the internal chronology of the Mandalas, by focusing on Gods and Goddesses\nworshiped in different Mandalas. We apply text analysis to the Mandalas using\nClustering Techniques based on Cosine Similarity. Then we represent the\nassociation of deities with Mandalas using a grid-based Social Network that is\namenable to chronological analysis and demonstrates the benefits of using\nSocial Network Analysis for the problem at hand. Further, we analyze references\nto rivers to arrive at additional correlations. The approach used can be\ndeployed generically to analyze other kinds of references and mentions and\narrive at more substantive inferences.", "published": "2024-07-01 14:16:46", "link": "http://arxiv.org/abs/2407.06205v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Exploring Advanced Large Language Models with LLMsuite", "abstract": "This tutorial explores the advancements and challenges in the development of\nLarge Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent\nlimitations like temporal knowledge cutoffs, mathematical inaccuracies, and the\ngeneration of incorrect information, proposing solutions like Retrieval\nAugmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks\nsuch as ReAct and LangChain. The integration of these techniques enhances LLM\nperformance and reliability, especially in multi-step reasoning and complex\ntask execution. The paper also covers fine-tuning strategies, including\ninstruction fine-tuning, parameter-efficient methods like LoRA, and\nReinforcement Learning from Human Feedback (RLHF) as well as Reinforced\nSelf-Training (ReST). Additionally, it provides a comprehensive survey of\ntransformer architectures and training techniques for LLMs. The source code can\nbe accessed by contacting the author via email for a request.", "published": "2024-07-01 05:37:17", "link": "http://arxiv.org/abs/2407.12036v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Efficient Automated Circuit Discovery in Transformers using Contextual\n  Decomposition", "abstract": "Automated mechanistic interpretation research has attracted great interest\ndue to its potential to scale explanations of neural network internals to large\nmodels. Existing automated circuit discovery work relies on activation patching\nor its approximations to identify subgraphs in models for specific tasks\n(circuits). They often suffer from slow runtime, approximation errors, and\nspecific requirements of metrics, such as non-zero gradients. In this work, we\nintroduce contextual decomposition for transformers (CD-T) to build\ninterpretable circuits in large language models. CD-T can produce circuits of\narbitrary level of abstraction, and is the first able to produce circuits as\nfine-grained as attention heads at specific sequence positions efficiently.\nCD-T consists of a set of mathematical equations to isolate contribution of\nmodel features. Through recursively computing contribution of all nodes in a\ncomputational graph of a model using CD-T followed by pruning, we are able to\nreduce circuit discovery runtime from hours to seconds compared to\nstate-of-the-art baselines. On three standard circuit evaluation datasets\n(indirect object identification, greater-than comparisons, and docstring\ncompletion), we demonstrate that CD-T outperforms ACDC and EAP by better\nrecovering the manual circuits with an average of 97% ROC AUC under low\nruntimes. In addition, we provide evidence that faithfulness of CD-T circuits\nis not due to random chance by showing our circuits are 80% more faithful than\nrandom circuits of up to 60% of the original model size. Finally, we show CD-T\ncircuits are able to perfectly replicate original models' behavior\n(faithfulness $ = 1$) using fewer nodes than the baselines for all tasks. Our\nresults underscore the great promise of CD-T for efficient automated\nmechanistic interpretability, paving the way for new insights into the workings\nof large language models.", "published": "2024-07-01 01:12:20", "link": "http://arxiv.org/abs/2407.00886v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Papez: Resource-Efficient Speech Separation with Auditory Working Memory", "abstract": "Transformer-based models recently reached state-of-the-art single-channel\nspeech separation accuracy; However, their extreme computational load makes it\ndifficult to deploy them in resource-constrained mobile or IoT devices. We thus\npresent Papez, a lightweight and computation-efficient single-channel speech\nseparation model. Papez is based on three key techniques. We first replace the\ninter-chunk Transformer with small-sized auditory working memory. Second, we\nadaptively prune the input tokens that do not need further processing. Finally,\nwe reduce the number of parameters through the recurrent transformer. Our\nextensive evaluation shows that Papez achieves the best resource and accuracy\ntradeoffs with a large margin. We publicly share our source code at\n\\texttt{https://github.com/snuhcs/Papez}", "published": "2024-07-01 01:23:06", "link": "http://arxiv.org/abs/2407.00888v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Macroeconomic Forecasting with Large Language Models", "abstract": "This paper presents a comparative analysis evaluating the accuracy of Large\nLanguage Models (LLMs) against traditional macro time series forecasting\napproaches. In recent times, LLMs have surged in popularity for forecasting due\nto their ability to capture intricate patterns in data and quickly adapt across\nvery different domains. However, their effectiveness in forecasting\nmacroeconomic time series data compared to conventional methods remains an area\nof interest. To address this, we conduct a rigorous evaluation of LLMs against\ntraditional macro forecasting methods, using as common ground the FRED-MD\ndatabase. Our findings provide valuable insights into the strengths and\nlimitations of LLMs in forecasting macroeconomic time series, shedding light on\ntheir applicability in real-world scenarios", "published": "2024-07-01 01:25:26", "link": "http://arxiv.org/abs/2407.00890v3", "categories": ["econ.EM", "cs.CL", "cs.LG"], "primary_category": "econ.EM"}
{"title": "From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning", "abstract": "Motivated by in-context learning (ICL) capabilities of Large Language Models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.", "published": "2024-07-01 01:57:21", "link": "http://arxiv.org/abs/2407.00902v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Deep Image-to-Recipe Translation", "abstract": "The modern saying, \"You Are What You Eat\" resonates on a profound level,\nreflecting the intricate connection between our identities and the food we\nconsume. Our project, Deep Image-to-Recipe Translation, is an intersection of\ncomputer vision and natural language generation that aims to bridge the gap\nbetween cherished food memories and the art of culinary creation. Our primary\nobjective involves predicting ingredients from a given food image. For this\ntask, we first develop a custom convolutional network and then compare its\nperformance to a model that leverages transfer learning. We pursue an\nadditional goal of generating a comprehensive set of recipe steps from a list\nof ingredients. We frame this process as a sequence-to-sequence task and\ndevelop a recurrent neural network that utilizes pre-trained word embeddings.\nWe address several challenges of deep learning including imbalanced datasets,\ndata cleaning, overfitting, and hyperparameter selection. Our approach\nemphasizes the importance of metrics such as Intersection over Union (IoU) and\nF1 score in scenarios where accuracy alone might be misleading. For our recipe\nprediction model, we employ perplexity, a commonly used and important metric\nfor language models. We find that transfer learning via pre-trained ResNet-50\nweights and GloVe embeddings provide an exceptional boost to model performance,\nespecially when considering training resource constraints. Although we have\nmade progress on the image-to-recipe translation, there is an opportunity for\nfuture exploration with advancements in model architectures, dataset\nscalability, and enhanced user interaction.", "published": "2024-07-01 02:33:07", "link": "http://arxiv.org/abs/2407.00911v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ProductAgent: Benchmarking Conversational Product Search Agent with\n  Asking Clarification Questions", "abstract": "This paper introduces the task of product demand clarification within an\ne-commercial scenario, where the user commences the conversation with ambiguous\nqueries and the task-oriented agent is designed to achieve more accurate and\ntailored product searching by asking clarification questions. To address this\ntask, we propose ProductAgent, a conversational information seeking agent\nequipped with abilities of strategic clarification question generation and\ndynamic product retrieval. Specifically, we develop the agent with strategies\nfor product feature summarization, query generation, and product retrieval.\nFurthermore, we propose the benchmark called PROCLARE to evaluate the agent's\nperformance both automatically and qualitatively with the aid of a LLM-driven\nuser simulator. Experiments show that ProductAgent interacts positively with\nthe user and enhances retrieval performance with increasing dialogue turns,\nwhere user demands become gradually more explicit and detailed. All the source\ncodes will be released after the review anonymity period.", "published": "2024-07-01 03:50:23", "link": "http://arxiv.org/abs/2407.00942v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "View From Above: A Framework for Evaluating Distribution Shifts in Model\n  Behavior", "abstract": "When large language models (LLMs) are asked to perform certain tasks, how can\nwe be sure that their learned representations align with reality? We propose a\ndomain-agnostic framework for systematically evaluating distribution shifts in\nLLMs decision-making processes, where they are given control of mechanisms\ngoverned by pre-defined rules. While individual LLM actions may appear\nconsistent with expected behavior, across a large number of trials,\nstatistically significant distribution shifts can emerge. To test this, we\nconstruct a well-defined environment with known outcome logic: blackjack. In\nmore than 1,000 trials, we uncover statistically significant evidence\nsuggesting behavioral misalignment in the learned representations of LLM.", "published": "2024-07-01 04:07:49", "link": "http://arxiv.org/abs/2407.00948v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large\n  Language Models", "abstract": "The scalability of large language models (LLMs) in handling high-complexity\nmodels and large-scale datasets has led to tremendous successes in pivotal\ndomains. While there is an urgent need to acquire more training data for LLMs,\na concerning reality is the depletion of high-quality public datasets within a\nfew years. In view of this, the federated learning (FL) LLM fine-tuning\nparadigm recently has been proposed to facilitate collaborative LLM fine-tuning\non distributed private data, where multiple data owners collaboratively\nfine-tune a shared LLM without sharing raw data. However, the staggering model\nsize of LLMs imposes heavy computing and communication burdens on clients,\nposing significant barriers to the democratization of the FL LLM fine-tuning\nparadigm. To address this issue, split learning (SL) has emerged as a promising\nsolution by offloading the primary training workload to a server via model\npartitioning while exchanging activation/activation's gradients with smaller\ndata sizes rather than the entire LLM. Unfortunately, research on the SL LLM\nfine-tuning paradigm is still in its nascent stage. To fill this gap, in this\npaper, we propose the first SL LLM fine-tuning framework, named SplitLoRA.\nSplitLoRA is built on the split federated learning (SFL) framework,\namalgamating the advantages of parallel training from FL and model splitting\nfrom SL and thus greatly enhancing the training efficiency. It is worth noting\nthat SplitLoRA is the inaugural open-source benchmark for SL LLM fine-tuning,\nproviding a foundation for research efforts dedicated to advancing SL LLM\nfine-tuning. Extensive simulations validate that SplitLoRA achieves target\naccuracy in significantly less time than state-of-the-art LLM fine-tuning\nframeworks, demonstrating the superior training performance of SplitLoRA. The\nproject page is available at https://fduinc.github.io/splitlora/.", "published": "2024-07-01 04:13:25", "link": "http://arxiv.org/abs/2407.00952v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models", "abstract": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.", "published": "2024-07-01 04:29:35", "link": "http://arxiv.org/abs/2407.00958v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "IBSEN: Director-Actor Agent Collaboration for Controllable and\n  Interactive Drama Script Generation", "abstract": "Large language models have demonstrated their capabilities in storyline\ncreation and human-like character role-playing. Current language model agents\nmainly focus on reasonable behaviors from the level of individuals, and their\nbehaviors might be hard to constraint on the level of the whole storyline. In\nthis paper we introduce IBSEN, a director-actor coordinate agent framework that\ngenerates drama scripts and makes the plot played by agents more controllable.\nThe director agent writes plot outlines that the user desires to see, instructs\nthe actor agents to role-play their characters, and reschedules the plot when\nhuman players participate in the scenario to ensure the plot is progressing\ntowards the objective. To evaluate the framework, we create a novel drama plot\nthat involves several actor agents and check the interactions between them\nunder the instruction of the director agent. Evaluation results show that our\nframework could generate complete, diverse drama scripts from only a rough\noutline of plot objectives, meanwhile maintaining the characteristics of\ncharacters in the drama. Our codes and prompts are available at\nhttps://github.com/OpenDFM/ibsen.", "published": "2024-07-01 08:49:57", "link": "http://arxiv.org/abs/2407.01093v1", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Unaligning Everything: Or Aligning Any Text to Any Image in Multimodal\n  Models", "abstract": "Utilizing a shared embedding space, emerging multimodal models exhibit\nunprecedented zero-shot capabilities. However, the shared embedding space could\nlead to new vulnerabilities if different modalities can be misaligned. In this\npaper, we extend and utilize a recently developed effective gradient-based\nprocedure that allows us to match the embedding of a given text by minimally\nmodifying an image. Using the procedure, we show that we can align the\nembeddings of distinguishable texts to any image through unnoticeable\nadversarial attacks in joint image-text models, revealing that semantically\nunrelated images can have embeddings of identical texts and at the same time\nvisually indistinguishable images can be matched to the embeddings of very\ndifferent texts. Our technique achieves 100\\% success rate when it is applied\nto text datasets and images from multiple sources. Without overcoming the\nvulnerability, multimodal models cannot robustly align inputs from different\nmodalities in a semantically meaningful way. \\textbf{Warning: the text data\nused in this paper are toxic in nature and may be offensive to some readers.}", "published": "2024-07-01 10:25:47", "link": "http://arxiv.org/abs/2407.01157v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "$\\text{Memory}^3$: Language Modeling with Explicit Memory", "abstract": "The training and inference of large language models (LLMs) are together a\ncostly process that transports knowledge from raw data to meaningful\ncomputation. Inspired by the memory hierarchy of the human brain, we reduce\nthis cost by equipping LLMs with explicit memory, a memory format cheaper than\nmodel parameters and text retrieval-augmented generation (RAG). Conceptually,\nwith most of its knowledge externalized to explicit memories, the LLM can enjoy\na smaller parameter size, training cost, and inference cost, all proportional\nto the amount of remaining \"abstract knowledge\". As a preliminary proof of\nconcept, we train from scratch a 2.4B LLM, which achieves better performance\nthan much larger LLMs as well as RAG models, and maintains higher decoding\nspeed than RAG. The model is named $\\text{Memory}^3$, since explicit memory is\nthe third form of memory in LLMs after implicit memory (model parameters) and\nworking memory (context key-values). We introduce a memory circuitry theory to\nsupport the externalization of knowledge, and present novel techniques\nincluding a memory sparsification mechanism that makes storage tractable and a\ntwo-stage pretraining scheme that facilitates memory formation.", "published": "2024-07-01 11:07:23", "link": "http://arxiv.org/abs/2407.01178v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Zero-Shot Recognizers for Activities of Daily\n  Living", "abstract": "The sensor-based recognition of Activities of Daily Living (ADLs) in smart\nhome environments enables several applications in the areas of energy\nmanagement, safety, well-being, and healthcare. ADLs recognition is typically\nbased on deep learning methods requiring large datasets to be trained.\nRecently, several studies proved that Large Language Models (LLMs) effectively\ncapture common-sense knowledge about human activities. However, the\neffectiveness of LLMs for ADLs recognition in smart home environments still\ndeserves to be investigated. In this work, we propose ADL-LLM, a novel\nLLM-based ADLs recognition system. ADLLLM transforms raw sensor data into\ntextual representations, that are processed by an LLM to perform zero-shot ADLs\nrecognition. Moreover, in the scenario where a small labeled dataset is\navailable, ADL-LLM can also be empowered with few-shot prompting. We evaluated\nADL-LLM on two public datasets, showing its effectiveness in this domain.", "published": "2024-07-01 12:32:38", "link": "http://arxiv.org/abs/2407.01238v3", "categories": ["cs.AI", "cs.CL", "eess.SP"], "primary_category": "cs.AI"}
{"title": "uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in\n  Low-Data Regimes", "abstract": "Recent work on distilling Whisper's knowledge into small models using\npseudo-labels shows promising performance while reducing the size by up to 50%.\nThis results in small, efficient, and dedicated models. However, a critical\nstep of distillation using pseudo-labels involves filtering high-quality\npredictions and using only those during training. This step requires ground\ntruth labels to compare with and filter low-quality examples, making the\nprocess dependent on human labels. Additionally, the distillation process\nrequires a large amount of data thereby limiting its applicability in\nlow-resource settings. To address this, we propose a distillation framework\nthat does not require any labeled data. Through experimentation, we show that\nour best-distilled models outperform the teacher model by 5-7 WER points and\nare on par with or outperform similar supervised data filtering setups. When\nscaling the data, our models significantly outperform all zero-shot and\nsupervised models. Our models are also 25-50% more compute- and\nmemory-efficient while maintaining performance equal to or better than that of\nthe teacher model. For more details about our models, dataset, and other\nresources, please visit our GitHub page:\nhttps://github.com/UBC-NLP/uDistilWhisper.", "published": "2024-07-01 13:07:01", "link": "http://arxiv.org/abs/2407.01257v4", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lightweight Zero-shot Text-to-Speech with Mixture of Adapters", "abstract": "The advancements in zero-shot text-to-speech (TTS) methods, based on\nlarge-scale models, have demonstrated high fidelity in reproducing speaker\ncharacteristics. However, these models are too large for practical daily use.\nWe propose a lightweight zero-shot TTS method using a mixture of adapters\n(MoA). Our proposed method incorporates MoA modules into the decoder and the\nvariance adapter of a non-autoregressive TTS model. These modules enhance the\nability to adapt a wide variety of speakers in a zero-shot manner by selecting\nappropriate adapters associated with speaker characteristics on the basis of\nspeaker embeddings. Our method achieves high-quality speech synthesis with\nminimal additional parameters. Through objective and subjective evaluations, we\nconfirmed that our method achieves better performance than the baseline with\nless than 40\\% of parameters at 1.9 times faster inference speed. Audio samples\nare available on our demo page\n(https://ntt-hilab-gensp.github.io/is2024lightweightTTS/).", "published": "2024-07-01 13:45:31", "link": "http://arxiv.org/abs/2407.01291v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Collaborative Performance Prediction for Large Language Models", "abstract": "Comprehensively understanding and accurately predicting the performance of\nlarge language models across diverse downstream tasks has emerged as a pivotal\nchallenge in NLP research. The pioneering scaling law on downstream works\ndemonstrated intrinsic similarities within model families and utilized such\nsimilarities for performance prediction. However, they tend to overlook the\nsimilarities between model families and only consider design factors listed in\nthe original scaling law. To overcome these limitations, we introduce a novel\nframework, Collaborative Performance Prediction (CPP), which significantly\nenhances prediction accuracy by leveraging the historical performance of\nvarious models on downstream tasks and other design factors for both model and\ntask. We also collect a collaborative data sourced from online platforms\ncontaining both historical performance and additional design factors. With the\nsupport of the collaborative data, CPP not only surpasses traditional scaling\nlaws in predicting the performance of scaled LLMs but also facilitates a\ndetailed analysis of factor importance, an area previously overlooked.", "published": "2024-07-01 13:56:42", "link": "http://arxiv.org/abs/2407.01300v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Increasing Model Capacity for Free: A Simple Strategy for Parameter\n  Efficient Fine-tuning", "abstract": "Fine-tuning large pre-trained foundation models, such as the 175B GPT-3, has\nattracted more attention for downstream tasks recently. While\nparameter-efficient fine-tuning methods have been proposed and proven effective\nwithout retraining all model parameters, their performance is limited by the\ncapacity of incremental modules, especially under constrained parameter\nbudgets. \\\\ To overcome this challenge, we propose CapaBoost, a simple yet\neffective strategy that enhances model capacity by leveraging low-rank updates\nthrough parallel weight modules in target layers. By applying static random\nmasks to the shared weight matrix, CapaBoost constructs a diverse set of weight\nmatrices, effectively increasing the rank of incremental weights without adding\nparameters. Notably, our approach can be seamlessly integrated into various\nexisting parameter-efficient fine-tuning methods. We extensively validate the\nefficacy of CapaBoost through experiments on diverse downstream tasks,\nincluding natural language understanding, question answering, and image\nclassification. Our results demonstrate significant improvements over\nbaselines, without incurring additional computation or storage costs. Our code\nis available at \\url{https://github.com/LINs-lab/CapaBoost}.", "published": "2024-07-01 14:26:48", "link": "http://arxiv.org/abs/2407.01320v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Badllama 3: removing safety finetuning from Llama 3 in minutes", "abstract": "We show that extensive LLM safety fine-tuning is easily subverted when an\nattacker has access to model weights. We evaluate three state-of-the-art\nfine-tuning methods-QLoRA, ReFT, and Ortho-and show how algorithmic advances\nenable constant jailbreaking performance with cuts in FLOPs and optimisation\npower. We strip safety fine-tuning from Llama 3 8B in one minute and Llama 3\n70B in 30 minutes on a single GPU, and sketch ways to reduce this further.", "published": "2024-07-01 15:29:45", "link": "http://arxiv.org/abs/2407.01376v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Gloss2Text: Sign Language Gloss translation using LLMs and Semantically\n  Aware Label Smoothing", "abstract": "Sign language translation from video to spoken text presents unique\nchallenges owing to the distinct grammar, expression nuances, and high\nvariation of visual appearance across different speakers and contexts. The\nintermediate gloss annotations of videos aim to guide the translation process.\nIn our work, we focus on {\\em Gloss2Text} translation stage and propose several\nadvances by leveraging pre-trained large language models (LLMs), data\naugmentation, and novel label-smoothing loss function exploiting gloss\ntranslation ambiguities improving significantly the performance of\nstate-of-the-art approaches. Through extensive experiments and ablation studies\non the PHOENIX Weather 2014T dataset, our approach surpasses state-of-the-art\nperformance in {\\em Gloss2Text} translation, indicating its efficacy in\naddressing sign language translation and suggesting promising avenues for\nfuture research and development.", "published": "2024-07-01 15:46:45", "link": "http://arxiv.org/abs/2407.01394v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Optimization of Retrieval-Augmented Generation Context with Outlier\n  Detection", "abstract": "In this paper, we focus on methods to reduce the size and improve the quality\nof the prompt context required for question-answering systems. Attempts to\nincrease the number of retrieved chunked documents and thereby enlarge the\ncontext related to the query can significantly complicate the processing and\ndecrease the performance of a Large Language Model (LLM) when generating\nresponses to queries. It is well known that a large set of documents retrieved\nfrom a database in response to a query may contain irrelevant information,\nwhich often leads to hallucinations in the resulting answers. Our goal is to\nselect the most semantically relevant documents, treating the discarded ones as\noutliers. We propose and evaluate several methods for identifying outliers by\ncreating features that utilize the distances of embedding vectors, retrieved\nfrom the vector database, to both the centroid and the query vectors. The\nmethods were evaluated by comparing the similarities of the retrieved LLM\nresponses to ground-truth answers obtained using the OpenAI GPT-4o model. It\nwas found that the greatest improvements were achieved with increasing\ncomplexity of the questions and answers.", "published": "2024-07-01 15:53:29", "link": "http://arxiv.org/abs/2407.01403v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Needle in the Haystack for Memory Based Large Language Models", "abstract": "Current large language models (LLMs) often perform poorly on simple fact\nretrieval tasks. Here we investigate if coupling a dynamically adaptable\nexternal memory to a LLM can alleviate this problem. For this purpose, we test\nLarimar, a recently proposed language model architecture which uses an external\nassociative memory, on long-context recall tasks including passkey and\nneedle-in-the-haystack tests. We demonstrate that the external memory of\nLarimar, which allows fast write and read of an episode of text samples, can be\nused at test time to handle contexts much longer than those seen during\ntraining. We further show that the latent readouts from the memory (to which\nlong contexts are written) control the decoder towards generating correct\noutputs, with the memory stored off of the GPU. Compared to existing\ntransformer-based LLM architectures for long-context recall tasks that use\nlarger parameter counts or modified attention mechanisms, a relatively smaller\nsize Larimar is able to maintain strong performance without any task-specific\ntraining or training on longer contexts.", "published": "2024-07-01 16:32:16", "link": "http://arxiv.org/abs/2407.01437v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tree Search for Language Model Agents", "abstract": "Autonomous agents powered by language models (LMs) have demonstrated promise\nin their ability to perform decision-making tasks such as web automation.\nHowever, a key limitation remains: LMs, primarily optimized for natural\nlanguage understanding and generation, struggle with multi-step reasoning,\nplanning, and using environmental feedback when attempting to solve realistic\ncomputer tasks. Towards addressing this, we propose an inference-time search\nalgorithm for LM agents to explicitly perform exploration and multi-step\nplanning in interactive web environments. Our approach is a form of best-first\ntree search that operates within the actual environment space, and is\ncomplementary with most existing state-of-the-art agents. It is the first tree\nsearch algorithm for LM agents that shows effectiveness on realistic web tasks.\nOn the challenging VisualWebArena benchmark, applying our search algorithm on\ntop of a GPT-4o agent yields a 39.7% relative increase in success rate compared\nto the same baseline without search, setting a state-of-the-art success rate of\n26.4%. On WebArena, search also yields a 28.0% relative improvement over a\nbaseline agent, setting a competitive success rate of 19.2%. Our experiments\nhighlight the effectiveness of search for web agents, and we demonstrate that\nperformance scales with increased test-time compute. We conduct a thorough\nanalysis of our results to highlight improvements from search, limitations, and\npromising directions for future work. Our code and models are publicly released\nat https://jykoh.com/search-agents.", "published": "2024-07-01 17:07:55", "link": "http://arxiv.org/abs/2407.01476v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Agentless: Demystifying LLM-based Software Engineering Agents", "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers and\nindustry practitioners have developed various autonomous LLM agents to perform\nend-to-end software development tasks. These agents are equipped with the\nability to use tools, run commands, observe feedback from the environment, and\nplan for future actions. However, the complexity of these agent-based\napproaches, together with the limited abilities of current LLMs, raises the\nfollowing question: Do we really have to employ complex autonomous software\nagents? To attempt to answer this question, we build Agentless -- an agentless\napproach to automatically solve software development problems. Compared to the\nverbose and complex setup of agent-based approaches, Agentless employs a\nsimplistic three-phase process of localization, repair, and patch validation,\nwithout letting the LLM decide future actions or operate with complex tools.\nOur results on the popular SWE-bench Lite benchmark show that surprisingly the\nsimplistic Agentless is able to achieve both the highest performance (32.00%,\n96 correct fixes) and low cost ($0.70) compared with all existing open-source\nsoftware agents! Furthermore, we manually classified the problems in SWE-bench\nLite and found problems with exact ground truth patch or\ninsufficient/misleading issue descriptions. As such, we construct SWE-bench\nLite-S by excluding such problematic issues to perform more rigorous evaluation\nand comparison. Our work highlights the current overlooked potential of a\nsimple, interpretable technique in autonomous software development. We hope\nAgentless will help reset the baseline, starting point, and horizon for\nautonomous software agents, and inspire future work along this crucial\ndirection.", "published": "2024-07-01 17:24:45", "link": "http://arxiv.org/abs/2407.01489v2", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable\n  Objectives", "abstract": "The widespread adoption of synthetic data raises new questions about how\nmodels generating the data can influence other large language models (LLMs) via\ndistilled data. To start, our work exhaustively characterizes the impact of\npassive inheritance of model properties by systematically studying the\nconsequences of synthetic data integration. We provide one of the most\ncomprehensive studies to-date of how the source of synthetic data shapes\nmodels' internal biases, calibration and generations' textual attributes and\npreferences. We find that models are surprisingly sensitive towards certain\nattributes even when the synthetic data prompts appear \"neutral\". which invites\nthe question whether this sensitivity can be exploited for good.\n  Our findings invite the question can we explicitly steer the models towards\nthe properties we want at test time by exploiting the data generation process?\nThis would have historically been considered infeasible due to the cost of\ncollecting data with a specific characteristic or objective in mind. However,\nimprovement in the quality of synthetic data, as well as a shift towards\ngeneral-purpose models designed to follow a diverse way of instructions, means\nthis question is timely. We propose active inheritance as a term to describe\nintentionally constraining synthetic data according to a non-differentiable\nobjective. We demonstrate how active inheritance can steer the generation\nprofiles of models towards desirable non-differentiable attributes, e.g. high\nlexical diversity or low toxicity.", "published": "2024-07-01 17:26:21", "link": "http://arxiv.org/abs/2407.01490v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ScanReason: Empowering 3D Visual Grounding with Reasoning Capabilities", "abstract": "Although great progress has been made in 3D visual grounding, current models\nstill rely on explicit textual descriptions for grounding and lack the ability\nto reason human intentions from implicit instructions. We propose a new task\ncalled 3D reasoning grounding and introduce a new benchmark ScanReason which\nprovides over 10K question-answer-location pairs from five reasoning types that\nrequire the synerization of reasoning and grounding. We further design our\napproach, ReGround3D, composed of the visual-centric reasoning module empowered\nby Multi-modal Large Language Model (MLLM) and the 3D grounding module to\nobtain accurate object locations by looking back to the enhanced geometry and\nfine-grained details from the 3D scenes. A chain-of-grounding mechanism is\nproposed to further boost the performance with interleaved reasoning and\ngrounding steps during inference. Extensive experiments on the proposed\nbenchmark validate the effectiveness of our proposed approach.", "published": "2024-07-01 17:59:35", "link": "http://arxiv.org/abs/2407.01525v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by\n  NLP Classifiers", "abstract": "AI regulations are expected to prohibit machine learning models from using\nsensitive attributes during training. However, the latest Natural Language\nProcessing (NLP) classifiers, which rely on deep learning, operate as black-box\nsystems, complicating the detection and remediation of such misuse. Traditional\nbias mitigation methods in NLP aim for comparable performance across different\ngroups based on attributes like gender or race but fail to address the\nunderlying issue of reliance on protected attributes. To partly fix that, we\nintroduce NLPGuard, a framework for mitigating the reliance on protected\nattributes in NLP classifiers. NLPGuard takes an unlabeled dataset, an existing\nNLP classifier, and its training data as input, producing a modified training\ndataset that significantly reduces dependence on protected attributes without\ncompromising accuracy. NLPGuard is applied to three classification tasks:\nidentifying toxic language, sentiment analysis, and occupation classification.\nOur evaluation shows that current NLP classifiers heavily depend on protected\nattributes, with up to $23\\%$ of the most predictive words associated with\nthese attributes. However, NLPGuard effectively reduces this reliance by up to\n$79\\%$, while slightly improving accuracy.", "published": "2024-07-01 18:08:17", "link": "http://arxiv.org/abs/2407.01697v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models", "abstract": "Can the rapid advances in code generation, function calling, and data\nanalysis using large language models (LLMs) help automate the search and\nverification of hypotheses purely from a set of provided datasets? To evaluate\nthis question, we present DiscoveryBench, the first comprehensive benchmark\nthat formalizes the multi-step process of data-driven discovery. The benchmark\nis designed to systematically assess current model capabilities in discovery\ntasks and provide a useful resource for improving them. Our benchmark contains\n264 tasks collected across 6 diverse domains, such as sociology and\nengineering, by manually deriving discovery workflows from published papers to\napproximate the real-world challenges faced by researchers, where each task is\ndefined by a dataset, its metadata, and a discovery goal in natural language.\nWe additionally provide 903 synthetic tasks to conduct controlled evaluations\nacross task complexity. Furthermore, our structured formalism of data-driven\ndiscovery enables a facet-based evaluation that provides useful insights into\ndifferent failure modes. We evaluate several popular LLM-based reasoning\nframeworks using both open and closed LLMs as baselines on DiscoveryBench and\nfind that even the best system scores only 25%. Our benchmark, thus,\nillustrates the challenges in autonomous data-driven discovery and serves as a\nvaluable resource for the community to make progress.", "published": "2024-07-01 18:58:22", "link": "http://arxiv.org/abs/2407.01725v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing Persuasive Strategies in Meme Texts: A Fusion of Language\n  Models with Paraphrase Enrichment", "abstract": "This paper describes our approach to hierarchical multi-label detection of\npersuasion techniques in meme texts. Our model, developed as a part of the\nrecent SemEval task, is based on fine-tuning individual language models (BERT,\nXLM-RoBERTa, and mBERT) and leveraging a mean-based ensemble model in addition\nto dataset augmentation through paraphrase generation from ChatGPT. The scope\nof the study encompasses enhancing model performance through innovative\ntraining techniques and data augmentation strategies. The problem addressed is\nthe effective identification and classification of multiple persuasive\ntechniques in meme texts, a task complicated by the diversity and complexity of\nsuch content. The objective of the paper is to improve detection accuracy by\nrefining model training methods and examining the impact of balanced versus\nunbalanced training datasets. Novelty in the results and discussion lies in the\nfinding that training with paraphrases enhances model performance, yet a\nbalanced training set proves more advantageous than a larger unbalanced one.\nAdditionally, the analysis reveals the potential pitfalls of indiscriminate\nincorporation of paraphrases from diverse distributions, which can introduce\nsubstantial noise. Results with the SemEval 2024 data confirm these insights,\ndemonstrating improved model efficacy with the proposed methods.", "published": "2024-07-01 20:25:20", "link": "http://arxiv.org/abs/2407.01784v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Race and Privacy in Broadcast Police Communications", "abstract": "Radios are essential for the operations of modern police departments, and\nthey function as both a collaborative communication technology and a\nsociotechnical system. However, little prior research has examined their usage\nor their connections to individual privacy and the role of race in policing,\ntwo growing topics of concern in the US. As a case study, we examine the\nChicago Police Department's (CPD's) use of broadcast police communications\n(BPC) to coordinate the activity of law enforcement officers (LEOs) in the\ncity. From a recently assembled archive of 80,775 hours of BPC associated with\nCPD operations, we analyze text transcripts of radio transmissions broadcast\n9:00 AM to 5:00 PM on August 10th, 2018 in one majority Black, one majority\nwhite, and one majority Hispanic area of the city (24 hours of audio) to\nexplore three research questions: (1) Do BPC reflect reported racial\ndisparities in policing? (2) How and when is gender, race/ethnicity, and age\nmentioned in BPC? (3) To what extent do BPC include sensitive information, and\nwho is put at most risk by this practice? (4) To what extent can large language\nmodels (LLMs) heighten this risk? We explore the vocabulary and speech acts\nused by police in BPC, comparing mentions of personal characteristics to local\ndemographics, the personal information shared over BPC, and the privacy\nconcerns that it poses. Analysis indicates (a) policing professionals in the\ncity of Chicago exhibit disproportionate attention to Black members of the\npublic regardless of context, (b) sociodemographic characteristics like gender,\nrace/ethnicity, and age are primarily mentioned in BPC about event information,\nand (c) disproportionate attention introduces disproportionate privacy risks\nfor Black members of the public.", "published": "2024-07-01 21:34:51", "link": "http://arxiv.org/abs/2407.01817v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Empathic Grounding: Explorations using Multimodal Interaction and Large\n  Language Models with Conversational Agents", "abstract": "We introduce the concept of \"empathic grounding\" in conversational agents as\nan extension of Clark's conceptualization of grounding in conversation in which\nthe grounding criterion includes listener empathy for the speaker's affective\nstate. Empathic grounding is generally required whenever the speaker's emotions\nare foregrounded and can make the grounding process more efficient and reliable\nby communicating both propositional and affective understanding. Both speaker\nexpressions of affect and listener empathic grounding can be multimodal,\nincluding facial expressions and other nonverbal displays. Thus, models of\nempathic grounding for embodied agents should be multimodal to facilitate\nnatural and efficient communication. We describe a multimodal model that takes\nas input user speech and facial expression to generate multimodal grounding\nmoves for a listening agent using a large language model. We also describe a\ntestbed to evaluate approaches to empathic grounding, in which a humanoid robot\ninterviews a user about a past episode of pain and then has the user rate their\nperception of the robot's empathy. We compare our proposed model to one that\nonly generates non-affective grounding cues in a between-subjects experiment.\nFindings demonstrate that empathic grounding increases user perceptions of\nempathy, understanding, emotional intelligence, and trust. Our work highlights\nthe role of emotion awareness and multimodality in generating appropriate\ngrounding moves for conversational agents.", "published": "2024-07-01 21:46:30", "link": "http://arxiv.org/abs/2407.01824v1", "categories": ["cs.HC", "cs.CL", "cs.RO"], "primary_category": "cs.HC"}
{"title": "Improving Multilingual Instruction Finetuning via Linguistically Natural\n  and Diverse Datasets", "abstract": "Advancements in Large Language Models (LLMs) have significantly enhanced\ninstruction-following capabilities. However, most Instruction Fine-Tuning (IFT)\ndatasets are predominantly in English, limiting model performance in other\nlanguages. Traditional methods for creating multilingual IFT datasets such as\ntranslating existing English IFT datasets or converting existing NLP datasets\ninto IFT datasets by templating, struggle to capture linguistic nuances and\nensure prompt (instruction) diversity. To address this issue, we propose a\nnovel method for collecting multilingual IFT datasets that preserves linguistic\nnaturalness and ensures prompt diversity. This approach leverages\nEnglish-focused LLMs, monolingual corpora, and a scoring function to create\nhigh-quality, diversified IFT datasets in multiple languages. Experiments\ndemonstrate that LLMs finetuned using these IFT datasets show notable\nimprovements in both generative and discriminative tasks, indicating enhanced\nlanguage comprehension by LLMs in non-English contexts. Specifically, on the\nmultilingual summarization task, LLMs using our IFT dataset achieved 17.57% and\n15.23% improvements over LLMs fine-tuned with translation-based and\ntemplate-based datasets, respectively.", "published": "2024-07-01 23:47:09", "link": "http://arxiv.org/abs/2407.01853v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "I've Got 99 Problems But FLOPS Ain't One", "abstract": "Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.", "published": "2024-07-01 10:33:46", "link": "http://arxiv.org/abs/2407.12819v2", "categories": ["cs.DC", "cs.CL", "cs.LG", "cs.NI"], "primary_category": "cs.DC"}
{"title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference", "abstract": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.", "published": "2024-07-01 13:05:42", "link": "http://arxiv.org/abs/2407.12820v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoFlow: Automated Workflow Generation for Large Language Model Agents", "abstract": "Recent advancements in Large Language Models (LLMs) have shown significant\nprogress in understanding complex natural language. One important application\nof LLM is LLM-based AI Agent, which leverages the ability of LLM as well as\nexternal tools for complex-task solving. To make sure LLM Agents follow an\neffective and reliable procedure to solve the given task, manually designed\nworkflows are usually used to guide the working mechanism of agents. However,\nmanually designing the workflows requires considerable efforts and domain\nknowledge, making it difficult to develop and deploy agents on massive scales.\nTo address these issues, we propose AutoFlow, a framework designed to\nautomatically generate workflows for agents to solve complex tasks. AutoFlow\ntakes natural language program as the format of agent workflow and employs a\nworkflow optimization procedure to iteratively optimize the workflow quality.\nBesides, this work offers two workflow generation methods: fine-tuning-based\nand in-context-based methods, making the AutoFlow framework applicable to both\nopen-source and closed-source LLMs. Experimental results show that our\nframework can produce robust and reliable agent workflows. We believe that the\nautomatic generation and interpretation of workflows in natural language\nrepresent a promising paradigm for solving complex tasks, particularly with the\nrapid development of LLMs. The source code of this work is available at\nhttps://github.com/agiresearch/AutoFlow.", "published": "2024-07-01 21:05:02", "link": "http://arxiv.org/abs/2407.12821v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward Automated Detection of Biased Social Signals from the Content of\n  Clinical Conversations", "abstract": "Implicit bias can impede patient-provider interactions and lead to inequities\nin care. Raising awareness is key to reducing such bias, but its manifestations\nin the social dynamics of patient-provider communication are difficult to\ndetect. In this study, we used automated speech recognition (ASR) and natural\nlanguage processing (NLP) to identify social signals in patient-provider\ninteractions. We built an automated pipeline to predict social signals from\naudio recordings of 782 primary care visits that achieved 90.1% average\naccuracy across codes, and exhibited fairness in its predictions for white and\nnon-white patients. Applying this pipeline, we identified statistically\nsignificant differences in provider communication behavior toward white versus\nnon-white patients. In particular, providers expressed more patient-centered\nbehaviors towards white patients including more warmth, engagement, and\nattentiveness. Our study underscores the potential of automated tools in\nidentifying subtle communication signals that may be linked with bias and\nimpact healthcare quality and equity.", "published": "2024-07-01 17:20:37", "link": "http://arxiv.org/abs/2407.17477v2", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "The Need for Guardrails with Large Language Models in Medical\n  Safety-Critical Settings: An Artificial Intelligence Application in the\n  Pharmacovigilance Ecosystem", "abstract": "Large language models (LLMs) are useful tools with the capacity for\nperforming specific types of knowledge work at an effective scale. However, LLM\ndeployments in high-risk and safety-critical domains pose unique challenges,\nnotably the issue of ``hallucination,'' where LLMs can generate fabricated\ninformation. This is particularly concerning in settings such as drug safety,\nwhere inaccuracies could lead to patient harm. To mitigate these risks, we have\ndeveloped and demonstrated a proof of concept suite of guardrails specifically\ndesigned to mitigate certain types of hallucinations and errors for drug\nsafety, and potentially applicable to other medical safety-critical contexts.\nThese guardrails include mechanisms to detect anomalous documents to prevent\nthe ingestion of inappropriate data, identify incorrect drug names or adverse\nevent terms, and convey uncertainty in generated content. We integrated these\nguardrails with an LLM fine-tuned for a text-to-text task, which involves\nconverting both structured and unstructured data within adverse event reports\ninto natural language. This method was applied to translate individual case\nsafety reports, demonstrating effective application in a pharmacovigilance\nprocessing task. Our guardrail framework offers a set of tools with broad\napplicability across various domains, ensuring LLMs can be safely used in\nhigh-risk situations by eliminating the occurrence of key errors, including the\ngeneration of incorrect pharmacovigilance-related terms, thus adhering to\nstringent regulatory and quality standards in medical safety-critical\nenvironments.", "published": "2024-07-01 19:52:41", "link": "http://arxiv.org/abs/2407.18322v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.1; I.2.7; I.7.1"], "primary_category": "cs.CL"}
{"title": "Human-like object concept representations emerge naturally in multimodal\n  large language models", "abstract": "The conceptualization and categorization of natural objects in the human mind\nhave long intrigued cognitive scientists and neuroscientists, offering crucial\ninsights into human perception and cognition. Recently, the rapid development\nof Large Language Models (LLMs) has raised the attractive question of whether\nthese models can also develop human-like object representations through\nexposure to vast amounts of linguistic and multimodal data. In this study, we\ncombined behavioral and neuroimaging analysis methods to uncover how the object\nconcept representations in LLMs correlate with those of humans. By collecting\nlarge-scale datasets of 4.7 million triplet judgments from LLM and Multimodal\nLLM (MLLM), we were able to derive low-dimensional embeddings that capture the\nunderlying similarity structure of 1,854 natural objects. The resulting\n66-dimensional embeddings were found to be highly stable and predictive, and\nexhibited semantic clustering akin to human mental representations.\nInterestingly, the interpretability of the dimensions underlying these\nembeddings suggests that LLM and MLLM have developed human-like conceptual\nrepresentations of natural objects. Further analysis demonstrated strong\nalignment between the identified model embeddings and neural activity patterns\nin many functionally defined brain ROIs (e.g., EBA, PPA, RSC and FFA). This\nprovides compelling evidence that the object representations in LLMs, while not\nidentical to those in the human, share fundamental commonalities that reflect\nkey schemas of human conceptual knowledge. This study advances our\nunderstanding of machine intelligence and informs the development of more\nhuman-like artificial cognitive systems.", "published": "2024-07-01 08:17:19", "link": "http://arxiv.org/abs/2407.01067v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "cs.AI"}
{"title": "We-Math: Does Your Large Multimodal Model Achieve Human-like\n  Mathematical Reasoning?", "abstract": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has\nreceived widespread attention from the Large Multimodal Models (LMMs)\ncommunity. Existing benchmarks, such as MathVista and MathVerse, focus more on\nthe result-oriented performance but neglect the underlying principles in\nknowledge acquisition and generalization. Inspired by human-like mathematical\nreasoning, we introduce WE-MATH, the first benchmark specifically designed to\nexplore the problem-solving principles beyond end-to-end performance. We\nmeticulously collect and categorize 6.5K visual math problems, spanning 67\nhierarchical knowledge concepts and five layers of knowledge granularity. We\ndecompose composite problems into sub-problems according to the required\nknowledge concepts and introduce a novel four-dimensional metric, namely\nInsufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery\n(CM), and Rote Memorization (RM), to hierarchically assess inherent issues in\nLMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of\nexisting LMMs in visual mathematical reasoning and reveal a negative\ncorrelation between solving steps and problem-specific performance. We confirm\nthe IK issue of LMMs can be effectively improved via knowledge augmentation\nstrategies. More notably, the primary challenge of GPT-4o has significantly\ntransitioned from IK to IG, establishing it as the first LMM advancing towards\nthe knowledge generalization stage. In contrast, other LMMs exhibit a marked\ninclination towards Rote Memorization - they correctly solve composite problems\ninvolving multiple knowledge concepts yet fail to answer sub-problems. We\nanticipate that WE-MATH will open new pathways for advancements in visual\nmathematical reasoning for LMMs. The WE-MATH data and evaluation code are\navailable at https://github.com/We-Math/We-Math.", "published": "2024-07-01 13:39:08", "link": "http://arxiv.org/abs/2407.01284v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SC"], "primary_category": "cs.AI"}
{"title": "LLM4PM: A case study on using Large Language Models for Process Modeling\n  in Enterprise Organizations", "abstract": "We investigate the potential of using Large Language Models (LLM) to support\nprocess model creation in organizational contexts. Specifically, we carry out a\ncase study wherein we develop and test an LLM-based chatbot, PRODIGY (PROcess\nmoDellIng Guidance for You), in a multinational company, the Hilti Group. We\nare particularly interested in understanding how LLM can aid (human) modellers\nin creating process flow diagrams. To this purpose, we first conduct a\npreliminary user study (n=10) with professional process modellers from Hilti,\ninquiring for various pain-points they encounter in their daily routines. Then,\nwe use their responses to design and implement PRODIGY. Finally, we evaluate\nPRODIGY by letting our user study's participants use PRODIGY, and then ask for\ntheir opinion on the pros and cons of PRODIGY. We coalesce our results in\nactionable takeaways. Through our research, we showcase the first practical\napplication of LLM for process modelling in the real world, shedding light on\nhow industries can leverage LLM to enhance their Business Process Management\nactivities.", "published": "2024-07-01 19:57:36", "link": "http://arxiv.org/abs/2407.17478v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.HC"}
{"title": "peerRTF: Robust MVDR Beamforming Using Graph Convolutional Network", "abstract": "Accurate and reliable identification of the relative transfer functions\n(RTFs) between microphones with respect to a desired source is an essential\ncomponent in the design of microphone array beamformers, specifically when\napplying the minimum variance distortionless response (MVDR) criterion. Since\nan accurate estimation of the RTF in a noisy and reverberant environment is a\ncumbersome task, we aim at leveraging prior knowledge of the acoustic enclosure\nto robustify the RTFs estimation by learning the RTF manifold. In this paper,\nwe present a novel robust RTF identification method, tested and trained using\nboth real recordings and simulated scenarios, which relies on learning the RTF\nmanifold using a graph convolutional network (GCN) to infer a robust\nrepresentation of the RTFs in a confined area, and consequently enhance the\nbeamformers performance.", "published": "2024-07-01 20:13:44", "link": "http://arxiv.org/abs/2407.01779v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "On Feature Learning for Titi Monkey Activity Detection", "abstract": "This paper, a technical summary of our preceding publication, introduces a\nrobust machine learning framework for the detection of vocal activities of\nCoppery titi monkeys. Utilizing a combination of MFCC features and a\nbidirectional LSTM-based classifier, we effectively address the challenges\nposed by the small amount of expert-annotated vocal data available. Our\napproach significantly reduces false positives and improves the accuracy of\ncall detection in bioacoustic research. Initial results demonstrate an accuracy\nof 95\\% on instance predictions, highlighting the effectiveness of our model in\nidentifying and classifying complex vocal patterns in environmental audio\nrecordings. Moreover, we show how call classification can be done downstream,\npaving the way for real-world monitoring.", "published": "2024-07-01 16:46:39", "link": "http://arxiv.org/abs/2407.01452v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-Visual Approach For Multimodal Concurrent Speaker Detection", "abstract": "Concurrent Speaker Detection (CSD), the task of identifying active speakers\nand their overlaps in an audio signal, is essential for various audio\napplications, including meeting transcription, speaker diarization, and speech\nseparation. This study presents a multimodal deep learning approach that\nintegrates audio and visual information. The proposed model utilizes an early\nfusion strategy, combining audio and visual features through cross-modal\nattention mechanisms with a learnable [CLS] token to capture key audio-visual\nrelationships.\n  The model is extensively evaluated on two real-world datasets, the\nestablished AMI dataset and the recently introduced EasyCom dataset.\nExperiments validate the effectiveness of the multimodal fusion strategy. An\nablation study further supports the design choices and the model's training\nprocedure. As this is the first work reporting CSD results on the challenging\nEasyCom dataset, the findings demonstrate the potential of the proposed\nmultimodal approach for \\ac{CSD} in real-world scenarios.", "published": "2024-07-01 20:06:57", "link": "http://arxiv.org/abs/2407.01774v2", "categories": ["eess.AS", "eess.IV"], "primary_category": "eess.AS"}
{"title": "ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024", "abstract": "The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024)\nis part of the ISCSLP 2024 Competitions and Challenges track. While current\ntext-to-speech (TTS) technology can generate high-quality audio, its ability to\nconvey complex emotions and controlled detail content remains limited. This\nconstraint leads to a discrepancy between the generated audio and human\nsubjective perception in practical applications like companion robots for\nchildren and marketing bots. The core issue lies in the inconsistency between\nhigh-quality audio generation and the ultimate human subjective experience.\nTherefore, this challenge aims to enhance the persuasiveness and acceptability\nof synthesized audio, focusing on human alignment convincing and inspirational\naudio generation. A total of 19 teams have registered for the challenge, and\nthe results of the competition and the competition are described in this paper.", "published": "2024-07-01 13:15:16", "link": "http://arxiv.org/abs/2407.12038v2", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "SecureSpectra: Safeguarding Digital Identity from Deep Fake Threats via\n  Intelligent Signatures", "abstract": "Advancements in DeepFake (DF) audio models pose a significant threat to voice\nauthentication systems, leading to unauthorized access and the spread of\nmisinformation. We introduce a defense mechanism, SecureSpectra, addressing DF\nthreats by embedding orthogonal, irreversible signatures within audio.\nSecureSpectra leverages the inability of DF models to replicate high-frequency\ncontent, which we empirically identify across diverse datasets and DF models.\nIntegrating differential privacy into the pipeline protects signatures from\nreverse engineering and strikes a delicate balance between enhanced security\nand minimal performance compromises. Our evaluations on Mozilla Common Voice,\nLibriSpeech, and VoxCeleb datasets showcase SecureSpectra's superior\nperformance, outperforming recent works by up to 71% in detection accuracy. We\nopen-source SecureSpectra to benefit the research community.", "published": "2024-07-01 02:36:27", "link": "http://arxiv.org/abs/2407.00913v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
{"title": "Are you sure? Analysing Uncertainty Quantification Approaches for\n  Real-world Speech Emotion Recognition", "abstract": "Uncertainty Quantification (UQ) is an important building block for the\nreliable use of neural networks in real-world scenarios, as it can be a useful\ntool in identifying faulty predictions. Speech emotion recognition (SER) models\ncan suffer from particularly many sources of uncertainty, such as the ambiguity\nof emotions, Out-of-Distribution (OOD) data or, in general, poor recording\nconditions. Reliable UQ methods are thus of particular interest as in many SER\napplications no prediction is better than a faulty prediction. While the\neffects of label ambiguity on uncertainty are well documented in the\nliterature, we focus our work on an evaluation of UQ methods for SER under\ncommon challenges in real-world application, such as corrupted signals, and the\nabsence of speech. We show that simple UQ methods can already give an\nindication of the uncertainty of a prediction and that training with additional\nOOD data can greatly improve the identification of such signals.", "published": "2024-07-01 10:11:08", "link": "http://arxiv.org/abs/2407.01143v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Speaker Embeddings in End-to-End Neural Diarization for\n  Two-Speaker Scenarios", "abstract": "End-to-end neural speaker diarization systems are able to address the speaker\ndiarization task while effectively handling speech overlap. This work explores\nthe incorporation of speaker information embeddings into the end-to-end systems\nto enhance the speaker discriminative capabilities, while maintaining their\noverlap handling strengths. To achieve this, we propose several methods for\nincorporating these embeddings along the acoustic features. Furthermore, we\ndelve into an analysis of the correct handling of silence frames, the window\nlength for extracting speaker embeddings and the transformer encoder size. The\neffectiveness of our proposed approach is thoroughly evaluated on the CallHome\ndataset for the two-speaker diarization task, with results that demonstrate a\nsignificant reduction in diarization error rates achieving a relative\nimprovement of a 10.78% compared to the baseline end-to-end model.", "published": "2024-07-01 14:26:28", "link": "http://arxiv.org/abs/2407.01317v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized\n  Sounds", "abstract": "We study Neural Foley, the automatic generation of high-quality sound effects\nsynchronizing with videos, enabling an immersive audio-visual experience.\nDespite its wide range of applications, existing approaches encounter\nlimitations when it comes to simultaneously synthesizing high-quality and\nvideo-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To\novercome these limitations, we propose FoleyCrafter, a novel framework that\nleverages a pre-trained text-to-audio model to ensure high-quality audio\ngeneration. FoleyCrafter comprises two key components: the semantic adapter for\nsemantic alignment and the temporal controller for precise audio-video\nsynchronization. The semantic adapter utilizes parallel cross-attention layers\nto condition audio generation on video features, producing realistic sound\neffects that are semantically relevant to the visual content. Meanwhile, the\ntemporal controller incorporates an onset detector and a timestampbased adapter\nto achieve precise audio-video alignment. One notable advantage of FoleyCrafter\nis its compatibility with text prompts, enabling the use of text descriptions\nto achieve controllable and diverse video-to-audio generation according to user\nintents. We conduct extensive quantitative and qualitative experiments on\nstandard benchmarks to verify the effectiveness of FoleyCrafter. Models and\ncodes are available at https://github.com/open-mmlab/FoleyCrafter.", "published": "2024-07-01 17:35:56", "link": "http://arxiv.org/abs/2407.01494v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Pictures Of MIDI: Controlled Music Generation via Graphical Prompts for\n  Image-Based Diffusion Inpainting", "abstract": "Recent years have witnessed significant progress in generative models for\nmusic, featuring diverse architectures that balance output quality, diversity,\nspeed, and user control. This study explores a user-friendly graphical\ninterface enabling the drawing of masked regions for inpainting by an Hourglass\nDiffusion Transformer (HDiT) model trained on MIDI piano roll images. To\nenhance note generation in specified areas, masked regions can be \"repainted\"\nwith extra noise. The non-latent HDiTs linear scaling with pixel count allows\nefficient generation in pixel space, providing intuitive and interpretable\ncontrols such as masking throughout the network and removing the need to\noperate in compressed latent spaces such as those provided by pretrained\nautoencoders. We demonstrate that, in addition to inpainting of melodies,\naccompaniment, and continuations, the use of repainting can help increase note\ndensity yielding musical structures closely matching user specifications such\nas rising, falling, or diverging melody and/or accompaniment, even when these\nlie outside the typical training data distribution. We achieve performance on\npar with prior results while operating at longer context windows, with no\nautoencoder, and can enable complex geometries for inpainting masks, increasing\nthe options for machine-assisted composers to control the generated music.", "published": "2024-07-01 17:43:45", "link": "http://arxiv.org/abs/2407.01499v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "J.5; I.2.0; I.4.0"], "primary_category": "cs.SD"}
{"title": "Deepfake Audio Detection Using Spectrogram-based Feature and Ensemble of\n  Deep Learning Models", "abstract": "In this paper, we propose a deep learning based system for the task of\ndeepfake audio detection. In particular, the draw input audio is first\ntransformed into various spectrograms using three transformation methods of\nShort-time Fourier Transform (STFT), Constant-Q Transform (CQT), Wavelet\nTransform (WT) combined with different auditory-based filters of Mel,\nGammatone, linear filters (LF), and discrete cosine transform (DCT). Given the\nspectrograms, we evaluate a wide range of classification models based on three\ndeep learning approaches. The first approach is to train directly the\nspectrograms using our proposed baseline models of CNN-based model\n(CNN-baseline), RNN-based model (RNN-baseline), C-RNN model (C-RNN baseline).\nMeanwhile, the second approach is transfer learning from computer vision models\nsuch as ResNet-18, MobileNet-V3, EfficientNet-B0, DenseNet-121, SuffleNet-V2,\nSwint, Convnext-Tiny, GoogLeNet, MNASsnet, RegNet. In the third approach, we\nleverage the state-of-the-art audio pre-trained models of Whisper, Seamless,\nSpeechbrain, and Pyannote to extract audio embeddings from the input\nspectrograms. Then, the audio embeddings are explored by a Multilayer\nperceptron (MLP) model to detect the fake or real audio samples. Finally,\nhigh-performance deep learning models from these approaches are fused to\nachieve the best performance. We evaluated our proposed models on ASVspoof 2019\nbenchmark dataset. Our best ensemble model achieved an Equal Error Rate (EER)\nof 0.03, which is highly competitive to top-performing systems in the\nASVspoofing 2019 challenge. Experimental results also highlight the potential\nof selective spectrograms and deep learning approaches to enhance the task of\naudio deepfake detection.", "published": "2024-07-01 20:10:43", "link": "http://arxiv.org/abs/2407.01777v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Meerkat: Audio-Visual Large Language Model for Grounding in Space and\n  Time", "abstract": "Leveraging Large Language Models' remarkable proficiency in text-based tasks,\nrecent works on Multi-modal LLMs (MLLMs) extend them to other modalities like\nvision and audio. However, the progress in these directions has been mostly\nfocused on tasks that only require a coarse-grained understanding of the\naudio-visual semantics. We present Meerkat, an audio-visual LLM equipped with a\nfine-grained understanding of image and audio both spatially and temporally.\nWith a new modality alignment module based on optimal transport and a\ncross-attention module that enforces audio-visual consistency, Meerkat can\ntackle challenging tasks such as audio referred image grounding, image guided\naudio temporal localization, and audio-visual fact-checking. Moreover, we\ncarefully curate a large dataset AVFIT that comprises 3M instruction tuning\nsamples collected from open-source datasets, and introduce MeerkatBench that\nunifies five challenging audio-visual tasks. We achieve state-of-the-art\nperformance on all these downstream tasks with a relative improvement of up to\n37.12%.", "published": "2024-07-01 23:32:25", "link": "http://arxiv.org/abs/2407.01851v2", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CV"}
{"title": "SpeakerBeam-SS: Real-time Target Speaker Extraction with Lightweight\n  Conv-TasNet and State Space Modeling", "abstract": "Real-time target speaker extraction (TSE) is intended to extract the desired\nspeaker's voice from the observed mixture of multiple speakers in a streaming\nmanner. Implementing real-time TSE is challenging as the computational\ncomplexity must be reduced to provide real-time operation. This work introduces\nto Conv-TasNet-based TSE a new architecture based on state space modeling (SSM)\nthat has been shown to model long-term dependency effectively. Owing to SSM,\nfewer dilated convolutional layers are required to capture temporal dependency\nin Conv-TasNet, resulting in the reduction of model complexity. We also enlarge\nthe window length and shift of the convolutional (TasNet) frontend encoder to\nreduce the computational cost further; the performance decline is compensated\nby over-parameterization of the frontend encoder. The proposed method reduces\nthe real-time factor by 78% from the conventional causal Conv-TasNet-based TSE\nwhile matching its performance.", "published": "2024-07-01 23:59:16", "link": "http://arxiv.org/abs/2407.01857v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The Solution for Temporal Sound Localisation Task of ICCV 1st Perception\n  Test Challenge 2023", "abstract": "In this paper, we propose a solution for improving the quality of temporal\nsound localization. We employ a multimodal fusion approach to combine visual\nand audio features. High-quality visual features are extracted using a\nstate-of-the-art self-supervised pre-training network, resulting in efficient\nvideo feature representations. At the same time, audio features serve as\ncomplementary information to help the model better localize the start and end\nof sounds. The fused features are trained in a multi-scale Transformer for\ntraining. In the final test dataset, we achieved a mean average precision (mAP)\nof 0.33, obtaining the second-best performance in this track.", "published": "2024-07-01 12:52:05", "link": "http://arxiv.org/abs/2407.02318v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
