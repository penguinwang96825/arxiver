{"title": "Parallel Rescoring with Transformer for Streaming On-Device Speech\n  Recognition", "abstract": "Recent advances of end-to-end models have outperformed conventional models\nthrough employing a two-pass model. The two-pass model provides better\nspeed-quality trade-offs for on-device speech recognition, where a 1st-pass\nmodel generates hypotheses in a streaming fashion, and a 2nd-pass model\nre-scores the hypotheses with full audio sequence context. The 2nd-pass model\nplays a key role in the quality improvement of the end-to-end model to surpass\nthe conventional model. One main challenge of the two-pass model is the\ncomputation latency introduced by the 2nd-pass model. Specifically, the\noriginal design of the two-pass model uses LSTMs for the 2nd-pass model, which\nare subject to long latency as they are constrained by the recurrent nature and\nhave to run inference sequentially. In this work we explore replacing the LSTM\nlayers in the 2nd-pass rescorer with Transformer layers, which can process the\nentire hypothesis sequences in parallel and can therefore utilize the on-device\ncomputation resources more efficiently. Compared with an LSTM-based baseline,\nour proposed Transformer rescorer achieves more than 50% latency reduction with\nquality improvement.", "published": "2020-08-30 05:17:31", "link": "http://arxiv.org/abs/2008.13093v3", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "LIMSI_UPV at SemEval-2020 Task 9: Recurrent Convolutional Neural Network\n  for Code-mixed Sentiment Analysis", "abstract": "This paper describes the participation of LIMSI UPV team in SemEval-2020 Task\n9: Sentiment Analysis for Code-Mixed Social Media Text. The proposed approach\ncompeted in SentiMix Hindi-English subtask, that addresses the problem of\npredicting the sentiment of a given Hindi-English code-mixed tweet. We propose\nRecurrent Convolutional Neural Network that combines both the recurrent neural\nnetwork and the convolutional network to better capture the semantics of the\ntext, for code-mixed sentiment analysis. The proposed system obtained 0.69\n(best run) in terms of F1 score on the given test data and achieved the 9th\nplace (Codalab username: somban) in the SentiMix Hindi-English subtask.", "published": "2020-08-30 13:52:24", "link": "http://arxiv.org/abs/2008.13173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Temporal Mental Health Dynamics on Social Media", "abstract": "We describe a set of experiments for building a temporal mental health\ndynamics system. We utilise a pre-existing methodology for distant-supervision\nof mental health data mining from social media platforms and deploy the system\nduring the global COVID-19 pandemic as a case study. Despite the challenging\nnature of the task, we produce encouraging results, both explicit to the global\npandemic and implicit to a global phenomenon, Christmas Depression, supported\nby the literature. We propose a methodology for providing insight into temporal\nmental health dynamics to be utilised for strategic decision-making.", "published": "2020-08-30 09:05:11", "link": "http://arxiv.org/abs/2008.13121v3", "categories": ["cs.CL", "cs.IR", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "QMUL-SDS at CheckThat! 2020: Determining COVID-19 Tweet Check-Worthiness\n  Using an Enhanced CT-BERT with Numeric Expressions", "abstract": "This paper describes the participation of the QMUL-SDS team for Task 1 of the\nCLEF 2020 CheckThat! shared task. The purpose of this task is to determine the\ncheck-worthiness of tweets about COVID-19 to identify and prioritise tweets\nthat need fact-checking. The overarching aim is to further support ongoing\nefforts to protect the public from fake news and help people find reliable\ninformation. We describe and analyse the results of our submissions. We show\nthat a CNN using COVID-Twitter-BERT (CT-BERT) enhanced with numeric expressions\ncan effectively boost performance from baseline results. We also show results\nof training data augmentation with rumours on other topics. Our best system\nranked fourth in the task with encouraging outcomes showing potential for\nimproved results in the future.", "published": "2020-08-30 13:03:53", "link": "http://arxiv.org/abs/2008.13160v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SEEC: Semantic Vector Federation across Edge Computing Environments", "abstract": "Semantic vector embedding techniques have proven useful in learning semantic\nrepresentations of data across multiple domains. A key application enabled by\nsuch techniques is the ability to measure semantic similarity between given\ndata samples and find data most similar to a given sample. State-of-the-art\nembedding approaches assume all data is available on a single site. However, in\nmany business settings, data is distributed across multiple edge locations and\ncannot be aggregated due to a variety of constraints. Hence, the applicability\nof state-of-the-art embedding approaches is limited to freely shared datasets,\nleaving out applications with sensitive or mission-critical data. This paper\naddresses this gap by proposing novel unsupervised algorithms called\n\\emph{SEEC} for learning and applying semantic vector embedding in a variety of\ndistributed settings. Specifically, for scenarios where multiple edge locations\ncan engage in joint learning, we adapt the recently proposed federated learning\ntechniques for semantic vector embedding. Where joint learning is not possible,\nwe propose novel semantic vector translation algorithms to enable semantic\nquery across multiple edge locations, each with its own semantic vector-space.\nExperimental results on natural language as well as graph datasets show that\nthis may be a promising new direction.", "published": "2020-08-30 23:51:41", "link": "http://arxiv.org/abs/2008.13298v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mixture of Speaker-type PLDAs for Children's Speech Diarization", "abstract": "In diarization, the PLDA is typically used to model an inference structure\nwhich assumes the variation in speech segments be induced by various speakers.\nThe speaker variation is then learned from the training data. However, human\nperception can differentiate speakers by age, gender, among other\ncharacteristics. In this paper, we investigate a speaker-type informed model\nthat explicitly captures the known variation of speakers. We explore a mixture\nof three PLDA models, where each model represents an adult female, male, or\nchild category. The weighting of each model is decided by the prior probability\nof its respective class, which we study. The evaluation is performed on a\nsubset of the BabyTrain corpus. We examine the expected performance gain using\nthe oracle speaker type labels, which yields an 11.7% DER reduction. We\nintroduce a novel baby vocalization augmentation technique and then compare the\nmixture model to the single model. Our experimental result shows an effective\n0.9% DER reduction obtained by adding vocalizations. We discover empirically\nthat a balanced dataset is important to train the mixture PLDA model, which\noutperforms the single PLDA by 1.3% using the same training data and achieving\na 35.8% DER. The same setup improves over a standard baseline by 2.8% DER.", "published": "2020-08-30 16:45:23", "link": "http://arxiv.org/abs/2008.13213v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Speech Pseudonymisation Assessment Using Voice Similarity Matrices", "abstract": "The proliferation of speech technologies and rising privacy legislation calls\nfor the development of privacy preservation solutions for speech applications.\nThese are essential since speech signals convey a wealth of rich, personal and\npotentially sensitive information. Anonymisation, the focus of the recent\nVoicePrivacy initiative, is one strategy to protect speaker identity\ninformation. Pseudonymisation solutions aim not only to mask the speaker\nidentity and preserve the linguistic content, quality and naturalness, as is\nthe goal of anonymisation, but also to preserve voice distinctiveness. Existing\nmetrics for the assessment of anonymisation are ill-suited and those for the\nassessment of pseudonymisation are completely lacking. Based upon voice\nsimilarity matrices, this paper proposes the first intuitive visualisation of\npseudonymisation performance for speech signals and two novel metrics for\nobjective assessment. They reflect the two, key pseudonymisation requirements\nof de-identification and voice distinctiveness.", "published": "2020-08-30 11:31:07", "link": "http://arxiv.org/abs/2008.13144v1", "categories": ["eess.AS", "cs.CR"], "primary_category": "eess.AS"}
{"title": "Multimodal Inductive Transfer Learning for Detection of Alzheimer's\n  Dementia and its Severity", "abstract": "Alzheimer's disease is estimated to affect around 50 million people worldwide\nand is rising rapidly, with a global economic burden of nearly a trillion\ndollars. This calls for scalable, cost-effective, and robust methods for\ndetection of Alzheimer's dementia (AD). We present a novel architecture that\nleverages acoustic, cognitive, and linguistic features to form a multimodal\nensemble system. It uses specialized artificial neural networks with temporal\ncharacteristics to detect AD and its severity, which is reflected through\nMini-Mental State Exam (MMSE) scores. We first evaluate it on the ADReSS\nchallenge dataset, which is a subject-independent and balanced dataset matched\nfor age and gender to mitigate biases, and is available through DementiaBank.\nOur system achieves state-of-the-art test accuracy, precision, recall, and\nF1-score of 83.3% each for AD classification, and state-of-the-art test root\nmean squared error (RMSE) of 4.60 for MMSE score regression. To the best of our\nknowledge, the system further achieves state-of-the-art AD classification\naccuracy of 88.0% when evaluated on the full benchmark DementiaBank Pitt\ndatabase. Our work highlights the applicability and transferability of\nspontaneous speech to produce a robust inductive transfer learning model, and\ndemonstrates generalizability through a task-agnostic feature-space. The source\ncode is available at https://github.com/wazeerzulfikar/alzheimers-dementia", "published": "2020-08-30 21:47:26", "link": "http://arxiv.org/abs/2009.00700v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Hierarchical Timbre-Painting and Articulation Generation", "abstract": "We present a fast and high-fidelity method for music generation, based on\nspecified f0 and loudness, such that the synthesized audio mimics the timbre\nand articulation of a target instrument. The generation process consists of\nlearned source-filtering networks, which reconstruct the signal at increasing\nresolutions. The model optimizes a multi-resolution spectral loss as the\nreconstruction loss, an adversarial loss to make the audio sound more\nrealistic, and a perceptual f0 loss to align the output to the desired input\npitch contour. The proposed architecture enables high-quality fitting of an\ninstrument, given a sample that can be as short as a few minutes, and the\nmethod demonstrates state-of-the-art timbre transfer capabilities. Code and\naudio samples are shared at https://github.com/mosheman5/timbre_painting.", "published": "2020-08-30 05:27:39", "link": "http://arxiv.org/abs/2008.13095v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improved Lite Audio-Visual Speech Enhancement", "abstract": "Numerous studies have investigated the effectiveness of audio-visual\nmultimodal learning for speech enhancement (AVSE) tasks, seeking a solution\nthat uses visual data as auxiliary and complementary input to reduce the noise\nof noisy speech signals. Recently, we proposed a lite audio-visual speech\nenhancement (LAVSE) algorithm for a car-driving scenario. Compared to\nconventional AVSE systems, LAVSE requires less online computation and to some\nextent solves the user privacy problem on facial data. In this study, we extend\nLAVSE to improve its ability to address three practical issues often\nencountered in implementing AVSE systems, namely, the additional cost of\nprocessing visual data, audio-visual asynchronization, and low-quality visual\ndata. The proposed system is termed improved LAVSE (iLAVSE), which uses a\nconvolutional recurrent neural network architecture as the core AVSE model. We\nevaluate iLAVSE on the Taiwan Mandarin speech with video dataset. Experimental\nresults confirm that compared to conventional AVSE systems, iLAVSE can\neffectively overcome the aforementioned three practical issues and can improve\nenhancement performance. The results also confirm that iLAVSE is suitable for\nreal-world scenarios, where high-quality audio-visual sensors may not always be\navailable.", "published": "2020-08-30 17:29:19", "link": "http://arxiv.org/abs/2008.13222v3", "categories": ["eess.AS", "cs.LG", "eess.IV"], "primary_category": "eess.AS"}
