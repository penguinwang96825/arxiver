{"title": "Discourse-Aware Emotion Cause Extraction in Conversations", "abstract": "Emotion Cause Extraction in Conversations (ECEC) aims to extract the\nutterances which contain the emotional cause in conversations. Most prior\nresearch focuses on modelling conversational contexts with sequential encoding,\nignoring the informative interactions between utterances and\nconversational-specific features for ECEC. In this paper, we investigate the\nimportance of discourse structures in handling utterance interactions and\nconversationspecific features for ECEC. To this end, we propose a\ndiscourse-aware model (DAM) for this task. Concretely, we jointly model ECEC\nwith discourse parsing using a multi-task learning (MTL) framework and\nexplicitly encode discourse structures via gated graph neural network (gated\nGNN), integrating rich utterance interaction information to our model. In\naddition, we use gated GNN to further enhance our ECEC model with\nconversation-specific features. Results on the benchmark corpus show that DAM\noutperform the state-of-theart (SOTA) systems in the literature. This suggests\nthat the discourse structure may contain a potential link between emotional\nutterances and their corresponding cause expressions. It also verifies the\neffectiveness of conversationalspecific features. The codes of this paper will\nbe available on GitHub.", "published": "2022-10-26 02:11:01", "link": "http://arxiv.org/abs/2210.14419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geographic Citation Gaps in NLP Research", "abstract": "In a fair world, people have equitable opportunities to education, to conduct\nscientific research, to publish, and to get credit for their work, regardless\nof where they live. However, it is common knowledge among researchers that a\nvast number of papers accepted at top NLP venues come from a handful of western\ncountries and (lately) China; whereas, very few papers from Africa and South\nAmerica get published. Similar disparities are also believed to exist for paper\ncitation counts. In the spirit of \"what we do not measure, we cannot improve\",\nthis work asks a series of questions on the relationship between geographical\nlocation and publication success (acceptance in top NLP venues and citation\nimpact). We first created a dataset of 70,000 papers from the ACL Anthology,\nextracted their meta-information, and generated their citation network. We then\nshow that not only are there substantial geographical disparities in paper\nacceptance and citation but also that these disparities persist even when\ncontrolling for a number of variables such as venue of publication and\nsub-field of NLP. Further, despite some steps taken by the NLP community to\nimprove geographical diversity, we show that the disparity in publication\nmetrics across locations is still on an increasing trend since the early 2000s.\nWe release our code and dataset here:\nhttps://github.com/iamjanvijay/acl-cite-net", "published": "2022-10-26 02:25:23", "link": "http://arxiv.org/abs/2210.14424v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ReSel: N-ary Relation Extraction from Scientific Text and Tables by\n  Learning to Retrieve and Select", "abstract": "We study the problem of extracting N-ary relation tuples from scientific\narticles. This task is challenging because the target knowledge tuples can\nreside in multiple parts and modalities of the document. Our proposed method\nReSel decomposes this task into a two-stage procedure that first retrieves the\nmost relevant paragraph/table and then selects the target entity from the\nretrieved component. For the high-level retrieval stage, ReSel designs a simple\nand effective feature set, which captures multi-level lexical and semantic\nsimilarities between the query and components. For the low-level selection\nstage, ReSel designs a cross-modal entity correlation graph along with a\nmulti-view architecture, which models both semantic and document-structural\nrelations between entities. Our experiments on three scientific information\nextraction datasets show that ReSel outperforms state-of-the-art baselines\nsignificantly.", "published": "2022-10-26 02:28:02", "link": "http://arxiv.org/abs/2210.14427v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question-Interlocutor Scope Realized Graph Modeling over Key Utterances\n  for Dialogue Reading Comprehension", "abstract": "In this work, we focus on dialogue reading comprehension (DRC), a task\nextracting answer spans for questions from dialogues. Dialogue context modeling\nin DRC is tricky due to complex speaker information and noisy dialogue context.\nTo solve the two problems, previous research proposes two self-supervised tasks\nrespectively: guessing who a randomly masked speaker is according to the\ndialogue and predicting which utterance in the dialogue contains the answer.\nAlthough these tasks are effective, there are still urging problems: (1)\nrandomly masking speakers regardless of the question cannot map the speaker\nmentioned in the question to the corresponding speaker in the dialogue, and\nignores the speaker-centric nature of utterances. This leads to wrong answer\nextraction from utterances in unrelated interlocutors' scopes; (2) the single\nutterance prediction, preferring utterances similar to the question, is limited\nin finding answer-contained utterances not similar to the question. To\nalleviate these problems, we first propose a new key utterances extracting\nmethod. It performs prediction on the unit formed by several contiguous\nutterances, which can realize more answer-contained utterances. Based on\nutterances in the extracted units, we then propose Question-Interlocutor Scope\nRealized Graph (QuISG) modeling. As a graph constructed on the text of\nutterances, QuISG additionally involves the question and question-mentioning\nspeaker names as nodes. To realize interlocutor scopes, speakers in the\ndialogue are connected with the words in their corresponding utterances.\nExperiments on the benchmarks show that our method can achieve better and\ncompetitive results against previous works.", "published": "2022-10-26 04:00:42", "link": "http://arxiv.org/abs/2210.14456v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bi-Link: Bridging Inductive Link Predictions from Text via Contrastive\n  Learning of Transformers and Prompts", "abstract": "Inductive knowledge graph completion requires models to comprehend the\nunderlying semantics and logic patterns of relations. With the advance of\npretrained language models, recent research have designed transformers for link\nprediction tasks. However, empirical studies show that linearizing triples\naffects the learning of relational patterns, such as inversion and symmetry. In\nthis paper, we propose Bi-Link, a contrastive learning framework with\nprobabilistic syntax prompts for link predictions. Using grammatical knowledge\nof BERT, we efficiently search for relational prompts according to learnt\nsyntactical patterns that generalize to large knowledge graphs. To better\nexpress symmetric relations, we design a symmetric link prediction model,\nestablishing bidirectional linking between forward prediction and backward\nprediction. This bidirectional linking accommodates flexible self-ensemble\nstrategies at test time. In our experiments, Bi-Link outperforms recent\nbaselines on link prediction datasets (WN18RR, FB15K-237, and Wikidata5M).\nFurthermore, we construct Zeshel-Ind as an in-domain inductive entity linking\nthe environment to evaluate Bi-Link. The experimental results demonstrate that\nour method yields robust representations which can generalize under domain\nshift.", "published": "2022-10-26 04:31:07", "link": "http://arxiv.org/abs/2210.14463v1", "categories": ["cs.CL", "H.3; I.7"], "primary_category": "cs.CL"}
{"title": "Eeny, meeny, miny, moe. How to choose data for morphological inflection", "abstract": "Data scarcity is a widespread problem in numerous natural language processing\n(NLP) tasks for low-resource languages. Within morphology, the labour-intensive\nwork of tagging/glossing data is a serious bottleneck for both NLP and language\ndocumentation. Active learning (AL) aims to reduce the cost of data annotation\nby selecting data that is most informative for improving the model. In this\npaper, we explore four sampling strategies for the task of morphological\ninflection using a Transformer model: a pair of oracle experiments where data\nis chosen based on whether the model already can or cannot inflect the test\nforms correctly, as well as strategies based on high/low model confidence,\nentropy, as well as random selection. We investigate the robustness of each\nstrategy across 30 typologically diverse languages. We also perform a more\nin-depth case study of Nat\\\"ugu. Our results show a clear benefit to selecting\ndata based on model confidence and entropy. Unsurprisingly, the oracle\nexperiment, where only incorrectly handled forms are chosen for further\ntraining, which is presented as a proxy for linguist/language consultant\nfeedback, shows the most improvement. This is followed closely by choosing\nlow-confidence and high-entropy predictions. We also show that despite the\nconventional wisdom of larger data sets yielding better accuracy, introducing\nmore instances of high-confidence or low-entropy forms, or forms that the model\ncan already inflect correctly, can reduce model performance.", "published": "2022-10-26 04:33:18", "link": "http://arxiv.org/abs/2210.14465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sinhala Sentence Embedding: A Two-Tiered Structure for Low-Resource\n  Languages", "abstract": "In the process of numerically modeling natural languages, developing language\nembeddings is a vital step. However, it is challenging to develop functional\nembeddings for resource-poor languages such as Sinhala, for which sufficiently\nlarge corpora, effective language parsers, and any other required resources are\ndifficult to find. In such conditions, the exploitation of existing models to\ncome up with an efficacious embedding methodology to numerically represent text\ncould be quite fruitful. This paper explores the effectivity of several\none-tiered and two-tiered embedding architectures in representing Sinhala text\nin the sentiment analysis domain. With our findings, the two-tiered embedding\narchitecture where the lower-tier consists of a word embedding and the\nupper-tier consists of a sentence embedding has been proven to perform better\nthan one-tier word embeddings, by achieving a maximum F1 score of 88.04% in\ncontrast to the 83.76% achieved by word embedding models. Furthermore,\nembeddings in the hyperbolic space are also developed and compared with\nEuclidean embeddings in terms of performance. A sentiment data set consisting\nof Facebook posts and associated reactions have been used for this research. To\neffectively compare the performance of different embedding systems, the same\ndeep neural network structure has been trained on sentiment data with each of\nthe embedding systems used to encode the text associated.", "published": "2022-10-26 04:46:23", "link": "http://arxiv.org/abs/2210.14472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Language Models for Code Syntax Understanding", "abstract": "Pre-trained language models have demonstrated impressive performance in both\nnatural language processing and program understanding, which represent the\ninput as a token sequence without explicitly modeling its structure. Some prior\nworks show that pre-trained language models can capture the syntactic rules of\nnatural languages without finetuning on syntax understanding tasks. However,\nthere is limited understanding of how well pre-trained models understand the\ncode structure so far. In this work, we perform the first thorough benchmarking\nof the state-of-the-art pre-trained models for identifying the syntactic\nstructures of programs. Specifically, we introduce CodeSyntax, a large-scale\ndataset of programs annotated with the syntactic relationships in their\ncorresponding abstract syntax trees. Our key observation is that existing\nlanguage models pretrained on code still lack the understanding of code syntax.\nIn fact, these pre-trained programming language models fail to match the\nperformance of simple baselines based on positional offsets and keywords. We\nalso present a natural language benchmark to highlight the differences between\nnatural languages and programming languages in terms of syntactic structure\nunderstanding. Our findings point out key limitations of existing pre-training\nmethods for programming languages, and suggest the importance of modeling code\nsyntactic structures.", "published": "2022-10-26 04:47:18", "link": "http://arxiv.org/abs/2210.14473v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Affirmative Interpretations from Negation Improves Natural\n  Language Understanding", "abstract": "Negation poses a challenge in many natural language understanding tasks.\nInspired by the fact that understanding a negated statement often requires\nhumans to infer affirmative interpretations, in this paper we show that doing\nso benefits models for three natural language understanding tasks. We present\nan automated procedure to collect pairs of sentences with negation and their\naffirmative interpretations, resulting in over 150,000 pairs. Experimental\nresults show that leveraging these pairs helps (a) T5 generate affirmative\ninterpretations from negations in a previous benchmark, and (b) a RoBERTa-based\nclassifier solve the task of natural language inference. We also leverage our\npairs to build a plug-and-play neural generator that given a negated statement\ngenerates an affirmative interpretation. Then, we incorporate the pretrained\ngenerator into a RoBERTa-based classifier for sentiment analysis and show that\ndoing so improves the results. Crucially, our proposal does not require any\nmanual effort.", "published": "2022-10-26 05:22:27", "link": "http://arxiv.org/abs/2210.14486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CS1QA: A Dataset for Assisting Code-based Question Answering in an\n  Introductory Programming Course", "abstract": "We introduce CS1QA, a dataset for code-based question answering in the\nprogramming education domain. CS1QA consists of 9,237 question-answer pairs\ngathered from chat logs in an introductory programming class using Python, and\n17,698 unannotated chat data with code. Each question is accompanied with the\nstudent's code, and the portion of the code relevant to answering the question.\nWe carefully design the annotation process to construct CS1QA, and analyze the\ncollected dataset in detail. The tasks for CS1QA are to predict the question\ntype, the relevant code snippet given the question and the code and retrieving\nan answer from the annotated corpus. Results for the experiments on several\nbaseline models are reported and thoroughly analyzed. The tasks for CS1QA\nchallenge models to understand both the code and natural language. This unique\ndataset can be used as a benchmark for source code comprehension and question\nanswering in the educational setting.", "published": "2022-10-26 05:40:34", "link": "http://arxiv.org/abs/2210.14494v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SentBS: Sentence-level Beam Search for Controllable Summarization", "abstract": "A wide range of control perspectives have been explored in controllable text\ngeneration. Structure-controlled summarization is recently proposed as a useful\nand interesting research direction. However, current structure-controlling\nmethods have limited effectiveness in enforcing the desired structure. To\naddress this limitation, we propose a sentence-level beam search generation\nmethod (SentBS), where evaluation is conducted throughout the generation\nprocess to select suitable sentences for subsequent generations. We experiment\nwith different combinations of decoding methods to be used as subcomponents by\nSentBS and evaluate results on the structure-controlled dataset MReD.\nExperiments show that all explored combinations for SentBS can improve the\nagreement between the generated text and the desired structure, with the best\nmethod significantly reducing the structural discrepancies suffered by the\nexisting model, by approximately 68%.", "published": "2022-10-26 06:21:01", "link": "http://arxiv.org/abs/2210.14502v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with\n  User Simulator", "abstract": "Task-Oriented Dialogue (TOD) systems are drawing more and more attention in\nrecent studies. Current methods focus on constructing pre-trained models or\nfine-tuning strategies while the evaluation of TOD is limited by a policy\nmismatch problem. That is, during evaluation, the user utterances are from the\nannotated dataset while these utterances should interact with previous\nresponses which can have many alternatives besides annotated texts. Therefore,\nin this work, we propose an interactive evaluation framework for TOD. We first\nbuild a goal-oriented user simulator based on pre-trained models and then use\nthe user simulator to interact with the dialogue system to generate dialogues.\nBesides, we introduce a sentence-level and a session-level score to measure the\nsentence fluency and session coherence in the interactive evaluation.\nExperimental results show that RL-based TOD systems trained by our proposed\nuser simulator can achieve nearly 98% inform and success rates in the\ninteractive evaluation of MultiWOZ dataset and the proposed scores measure the\nresponse quality besides the inform and success rates. We are hoping that our\nwork will encourage simulator-based interactive evaluations in the TOD task.", "published": "2022-10-26 07:41:32", "link": "http://arxiv.org/abs/2210.14529v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unifying Data Perspectivism and Personalization: An Application to\n  Social Norms", "abstract": "Instead of using a single ground truth for language processing tasks, several\nrecent studies have examined how to represent and predict the labels of the set\nof annotators. However, often little or no information about annotators is\nknown, or the set of annotators is small. In this work, we examine a corpus of\nsocial media posts about conflict from a set of 13k annotators and 210k\njudgements of social norms. We provide a novel experimental setup that applies\npersonalization methods to the modeling of annotators and compare their\neffectiveness for predicting the perception of social norms. We further provide\nan analysis of performance across subsets of social situations that vary by the\ncloseness of the relationship between parties in conflict, and assess where\npersonalization helps the most.", "published": "2022-10-26 07:43:26", "link": "http://arxiv.org/abs/2210.14531v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Look to the Right: Mitigating Relative Position Bias in Extractive\n  Question Answering", "abstract": "Extractive question answering (QA) models tend to exploit spurious\ncorrelations to make predictions when a training set has unintended biases.\nThis tendency results in models not being generalizable to examples where the\ncorrelations do not hold. Determining the spurious correlations QA models can\nexploit is crucial in building generalizable QA models in real-world\napplications; moreover, a method needs to be developed that prevents these\nmodels from learning the spurious correlations even when a training set is\nbiased. In this study, we discovered that the relative position of an answer,\nwhich is defined as the relative distance from an answer span to the closest\nquestion-context overlap word, can be exploited by QA models as superficial\ncues for making predictions. Specifically, we find that when the relative\npositions in a training set are biased, the performance on examples with\nrelative positions unseen during training is significantly degraded. To\nmitigate the performance degradation for unseen relative positions, we propose\nan ensemble-based debiasing method that does not require prior knowledge about\nthe distribution of relative positions. We demonstrate that the proposed method\nmitigates the models' reliance on relative positions using the biased and full\nSQuAD dataset. We hope that this study can help enhance the generalization\nability of QA models in real-world applications.", "published": "2022-10-26 08:01:38", "link": "http://arxiv.org/abs/2210.14541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Robust Bias Mitigation Procedure Based on the Stereotype Content Model", "abstract": "The Stereotype Content model (SCM) states that we tend to perceive minority\ngroups as cold, incompetent or both. In this paper we adapt existing work to\ndemonstrate that the Stereotype Content model holds for contextualised word\nembeddings, then use these results to evaluate a fine-tuning process designed\nto drive a language model away from stereotyped portrayals of minority groups.\nWe find the SCM terms are better able to capture bias than demographic agnostic\nterms related to pleasantness. Further, we were able to reduce the presence of\nstereotypes in the model through a simple fine-tuning procedure that required\nminimal human and computer resources, without harming downstream performance.\nWe present this work as a prototype of a debiasing procedure that aims to\nremove the need for a priori knowledge of the specifics of bias in the model.", "published": "2022-10-26 08:13:58", "link": "http://arxiv.org/abs/2210.14552v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Uncertainty Sentence Sampling by Virtual Adversarial Perturbation", "abstract": "Active learning for sentence understanding attempts to reduce the annotation\ncost by identifying the most informative examples. Common methods for active\nlearning use either uncertainty or diversity sampling in the pool-based\nscenario. In this work, to incorporate both predictive uncertainty and sample\ndiversity, we propose Virtual Adversarial Perturbation for Active Learning\n(VAPAL) , an uncertainty-diversity combination framework, using virtual\nadversarial perturbation (Miyato et al., 2019) as model uncertainty\nrepresentation. VAPAL consistently performs equally well or even better than\nthe strong baselines on four sentence understanding datasets: AGNEWS, IMDB,\nPUBMED, and SST-2, offering a potential option for active learning on sentence\nunderstanding tasks.", "published": "2022-10-26 09:18:00", "link": "http://arxiv.org/abs/2210.14576v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A practical method for occupational skills detection in Vietnamese job\n  listings", "abstract": "Vietnamese labor market has been under an imbalanced development. The number\nof university graduates is growing, but so is the unemployment rate. This\nsituation is often caused by the lack of accurate and timely labor market\ninformation, which leads to skill miss-matches between worker supply and the\nactual market demands. To build a data monitoring and analytic platform for the\nlabor market, one of the main challenges is to be able to automatically detect\noccupational skills from labor-related data, such as resumes and job listings.\nTraditional approaches rely on existing taxonomy and/or large annotated data to\nbuild Named Entity Recognition (NER) models. They are expensive and require\nhuge manual efforts. In this paper, we propose a practical methodology for\nskill detection in Vietnamese job listings. Rather than viewing the task as a\nNER task, we consider the task as a ranking problem. We propose a pipeline in\nwhich phrases are first extracted and ranked in semantic similarity with the\nphrases' contexts. Then we employ a final classification to detect skill\nphrases. We collected three datasets and conducted extensive experiments. The\nresults demonstrated that our methodology achieved better performance than a\nNER model in scarce datasets.", "published": "2022-10-26 10:23:18", "link": "http://arxiv.org/abs/2210.14607v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MOCHA: A Multi-Task Training Approach for Coherent Text Generation from\n  Cognitive Perspective", "abstract": "Teaching neural models to generate narrative coherent texts is a critical\nproblem. Recent pre-trained language models have achieved promising results,\nbut there is still a gap between human written texts and machine-generated\noutputs. In this work, we propose a novel multi-task training strategy for\ncoherent text generation grounded on the cognitive theory of writing, which\nempowers the model to learn essential subskills needed for writing including\nplanning and reviewing besides end-to-end generation. We extensively evaluate\nour model on three open-ended generation tasks including story generation, news\narticle writing and argument generation. Experiments show that our model\nachieves better results on both few-shot and fully-supervised settings than\nstrong baselines, and human evaluations confirm that our model can generate\nmore coherent outputs.", "published": "2022-10-26 11:55:41", "link": "http://arxiv.org/abs/2210.14650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Bilingual Parallel Corpus with Discourse Annotations", "abstract": "Machine translation (MT) has almost achieved human parity at sentence-level\ntranslation. In response, the MT community has, in part, shifted its focus to\ndocument-level translation. However, the development of document-level MT\nsystems is hampered by the lack of parallel document corpora. This paper\ndescribes BWB, a large parallel corpus first introduced in Jiang et al. (2022),\nalong with an annotated test set. The BWB corpus consists of Chinese novels\ntranslated by experts into English, and the annotated test set is designed to\nprobe the ability of machine translation systems to model various discourse\nphenomena. Our resource is freely available, and we hope it will serve as a\nguide and inspiration for more work in document-level machine translation.", "published": "2022-10-26 12:33:53", "link": "http://arxiv.org/abs/2210.14667v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Role of Centering Theory in the Context of Neural\n  Coreference Resolution Systems", "abstract": "Centering theory (CT; Grosz et al., 1995) provides a linguistic analysis of\nthe structure of discourse. According to the theory, local coherence of\ndiscourse arises from the manner and extent to which successive utterances make\nreference to the same entities. In this paper, we investigate the connection\nbetween centering theory and modern coreference resolution systems. We provide\nan operationalization of centering and systematically investigate if neural\ncoreference resolvers adhere to the rules of centering theory by defining\nvarious discourse metrics and developing a search-based methodology. Our\ninformation-theoretic analysis reveals a positive dependence between\ncoreference and centering; but also shows that high-quality neural coreference\nresolvers may not benefit much from explicitly modeling centering ideas. Our\nanalysis further shows that contextualized embeddings contain much of the\ncoherence information, which helps explain why CT can only provide little gains\nto modern neural coreference resolvers which make use of pretrained\nrepresentations. Finally, we discuss factors that contribute to coreference\nwhich are not modeled by CT such as world knowledge and recency bias. We\nformulate a version of CT that also models recency and show that it captures\ncoreference information better compared to vanilla CT.", "published": "2022-10-26 12:55:26", "link": "http://arxiv.org/abs/2210.14678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoregressive Structured Prediction with Language Models", "abstract": "Recent years have seen a paradigm shift in NLP towards using pretrained\nlanguage models ({PLM}) for a wide range of tasks.\n  However, there are many difficult design decisions to represent structures\n(e.g. tagged text, coreference chains) in a way such that they can be captured\nby PLMs.\n  Prior work on structured prediction with PLMs typically flattens the\nstructured output into a sequence, which limits the quality of structural\ninformation being learned and leads to inferior performance compared to classic\ndiscriminative models.\n  In this work, we describe an approach to model structures as sequences of\nactions in an autoregressive manner with PLMs, allowing in-structure\ndependencies to be learned without any loss.\n  Our approach achieves the new state-of-the-art on all the structured\nprediction tasks we looked at, namely, named entity recognition, end-to-end\nrelation extraction, and coreference resolution.", "published": "2022-10-26 13:27:26", "link": "http://arxiv.org/abs/2210.14698v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProSiT! Latent Variable Discovery with PROgressive SImilarity Thresholds", "abstract": "The most common ways to explore latent document dimensions are topic models\nand clustering methods. However, topic models have several drawbacks: e.g.,\nthey require us to choose the number of latent dimensions a priori, and the\nresults are stochastic. Most clustering methods have the same issues and lack\nflexibility in various ways, such as not accounting for the influence of\ndifferent topics on single documents, forcing word-descriptors to belong to a\nsingle topic (hard-clustering) or necessarily relying on word representations.\nWe propose PROgressive SImilarity Thresholds - ProSiT, a deterministic and\ninterpretable method, agnostic to the input format, that finds the optimal\nnumber of latent dimensions and only has two hyper-parameters, which can be set\nefficiently via grid search. We compare this method with a wide range of topic\nmodels and clustering methods on four benchmark data sets. In most setting,\nProSiT matches or outperforms the other methods in terms six metrics of topic\ncoherence and distinctiveness, producing replicable, deterministic results.", "published": "2022-10-26 14:52:44", "link": "http://arxiv.org/abs/2210.14763v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Curious Case of $\\ell_2$ norm of Sense Embeddings", "abstract": "We show that the $\\ell_2$ norm of a static sense embedding encodes\ninformation related to the frequency of that sense in the training corpus used\nto learn the sense embeddings. This finding can be seen as an extension of a\npreviously known relationship for word embeddings to sense embeddings. Our\nexperimental results show that, in spite of its simplicity, the $\\ell_2$ norm\nof sense embeddings is a surprisingly effective feature for several word sense\nrelated tasks such as (a) most frequent sense prediction, (b) Word-in-Context\n(WiC), and (c) Word Sense Disambiguation (WSD). In particular, by simply\nincluding the $\\ell_2$ norm of a sense embedding as a feature in a classifier,\nwe show that we can improve WiC and WSD methods that use static sense\nembeddings.", "published": "2022-10-26 16:04:15", "link": "http://arxiv.org/abs/2210.14815v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ProVe: A Pipeline for Automated Provenance Verification of Knowledge\n  Graphs against Textual Sources", "abstract": "Knowledge Graphs are repositories of information that gather data from a\nmultitude of domains and sources in the form of semantic triples, serving as a\nsource of structured data for various crucial applications in the modern web\nlandscape, from Wikipedia infoboxes to search engines. Such graphs mainly serve\nas secondary sources of information and depend on well-documented and\nverifiable provenance to ensure their trustworthiness and usability. However,\ntheir ability to systematically assess and assure the quality of this\nprovenance, most crucially whether it properly supports the graph's\ninformation, relies mainly on manual processes that do not scale with size.\nProVe aims at remedying this, consisting of a pipelined approach that\nautomatically verifies whether a Knowledge Graph triple is supported by text\nextracted from its documented provenance. ProVe is intended to assist\ninformation curators and consists of four main steps involving rule-based\nmethods and machine learning models: text extraction, triple verbalisation,\nsentence selection, and claim verification. ProVe is evaluated on a Wikidata\ndataset, achieving promising results overall and excellent performance on the\nbinary classification task of detecting support from provenance, with 87.5%\naccuracy and 82.9% F1-macro on text-rich sources. The evaluation data and\nscripts used in this paper are available on GitHub and Figshare.", "published": "2022-10-26 16:47:36", "link": "http://arxiv.org/abs/2210.14846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Causality Detection using Multiple Annotation Decisions", "abstract": "The paper describes the work that has been submitted to the 5th workshop on\nChallenges and Applications of Automated Extraction of socio-political events\nfrom text (CASE 2022). The work is associated with Subtask 1 of Shared Task 3\nthat aims to detect causality in protest news corpus. The authors used\ndifferent large language models with customized cross-entropy loss functions\nthat exploit annotation information. The experiments showed that\nbert-based-uncased with refined cross-entropy outperformed the others,\nachieving a F1 score of 0.8501 on the Causal News Corpus dataset.", "published": "2022-10-26 16:50:10", "link": "http://arxiv.org/abs/2210.14852v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Goldilocks of Pragmatic Understanding: Fine-Tuning Strategy Matters\n  for Implicature Resolution by LLMs", "abstract": "Despite widespread use of LLMs as conversational agents, evaluations of\nperformance fail to capture a crucial aspect of communication: interpreting\nlanguage in context -- incorporating its pragmatics. Humans interpret language\nusing beliefs and prior knowledge about the world. For example, we intuitively\nunderstand the response \"I wore gloves\" to the question \"Did you leave\nfingerprints?\" as meaning \"No\". To investigate whether LLMs have the ability to\nmake this type of inference, known as an implicature, we design a simple task\nand evaluate four categories of widely used state-of-the-art models. We find\nthat, despite only evaluating on utterances that require a binary inference\n(yes or no), models in three of these categories perform close to random.\nHowever, LLMs instruction-tuned at the example-level perform significantly\nbetter. These results suggest that certain fine-tuning strategies are far\nbetter at inducing pragmatic understanding in models. We present our findings\nas the starting point for further research into evaluating how LLMs interpret\nlanguage in context and to drive the development of more pragmatic and useful\nmodels of human discourse.", "published": "2022-10-26 19:04:23", "link": "http://arxiv.org/abs/2210.14986v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Four-in-One: A Joint Approach to Inverse Text Normalization,\n  Punctuation, Capitalization, and Disfluency for Automatic Speech Recognition", "abstract": "Features such as punctuation, capitalization, and formatting of entities are\nimportant for readability, understanding, and natural language processing\ntasks. However, Automatic Speech Recognition (ASR) systems produce spoken-form\ntext devoid of formatting, and tagging approaches to formatting address just\none or two features at a time. In this paper, we unify spoken-to-written text\nconversion via a two-stage process: First, we use a single transformer tagging\nmodel to jointly produce token-level tags for inverse text normalization (ITN),\npunctuation, capitalization, and disfluencies. Then, we apply the tags to\ngenerate written-form text and use weighted finite state transducer (WFST)\ngrammars to format tagged ITN entity spans. Despite joining four models into\none, our unified tagging approach matches or outperforms task-specific models\nacross all four tasks on benchmark test sets across several domains.", "published": "2022-10-26 22:21:03", "link": "http://arxiv.org/abs/2210.15063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "arXivEdits: Understanding the Human Revision Process in Scientific\n  Writing", "abstract": "Scientific publications are the primary means to communicate research\ndiscoveries, where the writing quality is of crucial importance. However, prior\nwork studying the human editing process in this domain mainly focused on the\nabstract or introduction sections, resulting in an incomplete picture. In this\nwork, we provide a complete computational framework for studying text revision\nin scientific writing. We first introduce arXivEdits, a new annotated corpus of\n751 full papers from arXiv with gold sentence alignment across their multiple\nversions of revision, as well as fine-grained span-level edits and their\nunderlying intentions for 1,000 sentence pairs. It supports our data-driven\nanalysis to unveil the common strategies practiced by researchers for revising\ntheir papers. To scale up the analysis, we also develop automatic methods to\nextract revision at document-, sentence-, and word-levels. A neural CRF\nsentence alignment model trained on our corpus achieves 93.8 F1, enabling the\nreliable matching of sentences between different versions. We formulate the\nedit extraction task as a span alignment problem, and our proposed method\nextracts more fine-grained and explainable edits, compared to the commonly used\ndiff algorithm. An intention classifier trained on our dataset achieves 78.9 F1\non the fine-grained intent classification task. Our data and system are\nreleased at tiny.one/arxivedits.", "published": "2022-10-26 22:50:24", "link": "http://arxiv.org/abs/2210.15067v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling the Graphotactics of Low-Resource Languages Using Sequential\n  GANs", "abstract": "Generative Adversarial Networks (GANs) have been shown to aid in the creation\nof artificial data in situations where large amounts of real data are difficult\nto come by. This issue is especially salient in the computational linguistics\nspace, where researchers are often tasked with modeling the complex morphologic\nand grammatical processes of low-resource languages. This paper will discuss\nthe implementation and testing of a GAN that attempts to model and reproduce\nthe graphotactics of a language using only 100 example strings. These\nartificial, yet graphotactically compliant, strings are meant to aid in\nmodeling the morphological inflection of low-resource languages.", "published": "2022-10-26 01:21:00", "link": "http://arxiv.org/abs/2210.14409v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$N$-gram Is Back: Residual Learning of Neural Text Generation with\n  $n$-gram Language Model", "abstract": "$N$-gram language models (LM) have been largely superseded by neural LMs as\nthe latter exhibits better performance. However, we find that $n$-gram models\ncan achieve satisfactory performance on a large proportion of testing cases,\nindicating they have already captured abundant knowledge of the language with\nrelatively low computational cost. With this observation, we propose to learn a\nneural LM that fits the residual between an $n$-gram LM and the real-data\ndistribution. The combination of $n$-gram and neural LMs not only allows the\nneural part to focus on the deeper understanding of language but also provides\na flexible way to customize an LM by switching the underlying $n$-gram model\nwithout changing the neural model. Experimental results on three typical\nlanguage tasks (i.e., language modeling, machine translation, and\nsummarization) demonstrate that our approach attains additional performance\ngains over popular standalone neural models consistently. We also show that our\napproach allows for effective domain adaptation by simply switching to a\ndomain-specific $n$-gram model, without any extra training. Our code is\nreleased at https://github.com/ghrua/NgramRes.", "published": "2022-10-26 02:42:53", "link": "http://arxiv.org/abs/2210.14431v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inducer-tuning: Connecting Prefix-tuning and Adapter-tuning", "abstract": "Prefix-tuning, or more generally continuous prompt tuning, has become an\nessential paradigm of parameter-efficient transfer learning. Using a large\npre-trained language model (PLM), prefix-tuning can obtain strong performance\nby training only a small portion of parameters. In this paper, we propose to\nunderstand and further develop prefix-tuning through the kernel lens.\nSpecifically, we make an analogy between \\textit{prefixes} and \\textit{inducing\nvariables} in kernel methods and hypothesize that \\textit{prefixes} serving as\n\\textit{inducing variables} would improve their overall mechanism. From the\nkernel estimator perspective, we suggest a new variant of prefix-tuning --\n\\textit{inducer-tuning}, which shares the exact mechanism as prefix-tuning\nwhile leveraging the residual form found in adapter-tuning. This mitigates the\ninitialization issue in prefix-tuning. Through comprehensive empirical\nexperiments on natural language understanding and generation tasks, we\ndemonstrate that inducer-tuning can close the performance gap between\nprefix-tuning and fine-tuning.", "published": "2022-10-26 04:39:42", "link": "http://arxiv.org/abs/2210.14469v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal\n  Prediction for Multimodal Sentiment Analysis", "abstract": "Multimodal representation learning is a challenging task in which previous\nwork mostly focus on either uni-modality pre-training or cross-modality fusion.\nIn fact, we regard modeling multimodal representation as building a skyscraper,\nwhere laying stable foundation and designing the main structure are equally\nessential. The former is like encoding robust uni-modal representation while\nthe later is like integrating interactive information among different\nmodalities, both of which are critical to learning an effective multimodal\nrepresentation. Recently, contrastive learning has been successfully applied in\nrepresentation learning, which can be utilized as the pillar of the skyscraper\nand benefit the model to extract the most important features contained in the\nmultimodal data. In this paper, we propose a novel framework named MultiModal\nContrastive Learning (MMCL) for multimodal representation to capture intra- and\ninter-modality dynamics simultaneously. Specifically, we devise uni-modal\ncontrastive coding with an efficient uni-modal feature augmentation strategy to\nfilter inherent noise contained in acoustic and visual modality and acquire\nmore robust uni-modality representations. Besides, a pseudo siamese network is\npresented to predict representation across different modalities, which\nsuccessfully captures cross-modal dynamics. Moreover, we design two contrastive\nlearning tasks, instance- and sentiment-based contrastive learning, to promote\nthe process of prediction and learn more interactive information related to\nsentiment. Extensive experiments conducted on two public datasets demonstrate\nthat our method surpasses the state-of-the-art methods.", "published": "2022-10-26 08:24:15", "link": "http://arxiv.org/abs/2210.14556v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing Multi-Task Learning for Abstractive Text Summarization", "abstract": "Despite the recent success of multi-task learning and pre-finetuning for\nnatural language understanding, few works have studied the effects of task\nfamilies on abstractive text summarization. Task families are a form of task\ngrouping during the pre-finetuning stage to learn common skills, such as\nreading comprehension. To close this gap, we analyze the influence of\nmulti-task learning strategies using task families for the English abstractive\ntext summarization task. We group tasks into one of three strategies, i.e.,\nsequential, simultaneous, and continual multi-task learning, and evaluate\ntrained models through two downstream tasks. We find that certain combinations\nof task families (e.g., advanced reading comprehension and natural language\ninference) positively impact downstream performance. Further, we find that\nchoice and combinations of task families influence downstream performance more\nthan the training scheme, supporting the use of task families for abstractive\ntext summarization.", "published": "2022-10-26 10:22:43", "link": "http://arxiv.org/abs/2210.14606v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bloom Library: Multimodal Datasets in 300+ Languages for a Variety of\n  Downstream Tasks", "abstract": "We present Bloom Library, a linguistically diverse set of multimodal and\nmultilingual datasets for language modeling, image captioning, visual\nstorytelling, and speech synthesis/recognition. These datasets represent either\nthe most, or among the most, multilingual datasets for each of the included\ndownstream tasks. In total, the initial release of the Bloom Library datasets\ncovers 363 languages across 32 language families. We train downstream task\nmodels for various languages represented in the data, showing the viability of\nthe data for future work in low-resource, multimodal NLP and establishing the\nfirst known baselines for these downstream tasks in certain languages (e.g.,\nBisu [bzi], with an estimated population of 700 users). Some of these\nfirst-of-their-kind baselines are comparable to state-of-the-art performance\nfor higher-resourced languages. The Bloom Library datasets are released under\nCreative Commons licenses on the Hugging Face datasets hub to catalyze more\nlinguistically diverse research in the included downstream tasks.", "published": "2022-10-26 13:45:14", "link": "http://arxiv.org/abs/2210.14712v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A New Task: Deriving Semantic Class Targets for the Physical Sciences", "abstract": "We define deriving semantic class targets as a novel multi-modal task. By\ndoing so, we aim to improve classification schemes in the physical sciences\nwhich can be severely abstracted and obfuscating. We address this task for\nupcoming radio astronomy surveys and present the derived semantic radio galaxy\nmorphology class targets.", "published": "2022-10-26 14:48:50", "link": "http://arxiv.org/abs/2210.14760v2", "categories": ["astro-ph.IM", "cs.CL"], "primary_category": "astro-ph.IM"}
{"title": "Beyond English-Centric Bitexts for Better Multilingual Language\n  Representation Learning", "abstract": "In this paper, we elaborate upon recipes for building multilingual\nrepresentation models that are not only competitive with existing\nstate-of-the-art models but are also more parameter efficient, thereby\npromoting better adoption in resource-constrained scenarios and practical\napplications. We show that going beyond English-centric bitexts, coupled with a\nnovel sampling strategy aimed at reducing under-utilization of training data,\nsubstantially boosts performance across model sizes for both Electra and MLM\npre-training objectives. We introduce XY-LENT: X-Y bitext enhanced Language\nENcodings using Transformers which not only achieves state-of-the-art\nperformance over 5 cross-lingual tasks within all model size bands, is also\ncompetitive across bands. Our XY-LENT XL variant outperforms XLM-RXXL and\nexhibits competitive performance with mT5 XXL while being 5x and 6x smaller\nrespectively. We then show that our proposed method helps ameliorate the curse\nof multilinguality, with the XY-LENT XL achieving 99.3% GLUE performance and\n98.5% SQuAD 2.0 performance compared to a SoTA English only model in the same\nsize band. We then analyze our models performance on extremely low resource\nlanguages and posit that scaling alone may not be sufficient for improving the\nperformance in this scenario", "published": "2022-10-26 17:16:52", "link": "http://arxiv.org/abs/2210.14867v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-lingual Evaluation of Code Generation Models", "abstract": "We present new benchmarks on evaluation code generation models: MBXP and\nMultilingual HumanEval, and MathQA-X. These datasets cover over 10 programming\nlanguages and are generated using a scalable conversion framework that\ntranspiles prompts and test cases from the original Python datasets into the\ncorresponding data in the target language. Using these benchmarks, we are able\nto assess the performance of code generation models in a multi-lingual fashion,\nand discovered generalization ability of language models on out-of-domain\nlanguages, advantages of multi-lingual models over mono-lingual, the ability of\nfew-shot prompting to teach the model new languages, and zero-shot translation\nabilities even on mono-lingual settings. Furthermore, we use our code\ngeneration model to perform large-scale bootstrapping to obtain synthetic\ncanonical solutions in several languages, which can be used for other\ncode-related evaluations such as code insertion, robustness, or summarization\ntasks. Overall, our benchmarks represents a significant step towards a deeper\nunderstanding of language models' code generation abilities. We publicly\nrelease our code and datasets at https://github.com/amazon-research/mxeval.", "published": "2022-10-26 17:17:06", "link": "http://arxiv.org/abs/2210.14868v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Disentangled Text Representation Learning with Information-Theoretic\n  Perspective for Adversarial Robustness", "abstract": "Adversarial vulnerability remains a major obstacle to constructing reliable\nNLP systems. When imperceptible perturbations are added to raw input text, the\nperformance of a deep learning model may drop dramatically under attacks.\nRecent work argues the adversarial vulnerability of the model is caused by the\nnon-robust features in supervised training. Thus in this paper, we tackle the\nadversarial robustness challenge from the view of disentangled representation\nlearning, which is able to explicitly disentangle robust and non-robust\nfeatures in text. Specifically, inspired by the variation of information (VI)\nin information theory, we derive a disentangled learning objective composed of\nmutual information to represent both the semantic representativeness of latent\nembeddings and differentiation of robust and non-robust features. On the basis\nof this, we design a disentangled learning network to estimate these mutual\ninformation. Experiments on text classification and entailment tasks show that\nour method significantly outperforms the representative methods under\nadversarial attacks, indicating that discarding non-robust features is critical\nfor improving adversarial robustness.", "published": "2022-10-26 18:14:39", "link": "http://arxiv.org/abs/2210.14957v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MABEL: Attenuating Gender Bias using Textual Entailment Data", "abstract": "Pre-trained language models encode undesirable social biases, which are\nfurther exacerbated in downstream use. To this end, we propose MABEL (a Method\nfor Attenuating Gender Bias using Entailment Labels), an intermediate\npre-training approach for mitigating gender bias in contextualized\nrepresentations. Key to our approach is the use of a contrastive learning\nobjective on counterfactually augmented, gender-balanced entailment pairs from\nnatural language inference (NLI) datasets. We also introduce an alignment\nregularizer that pulls identical entailment pairs along opposite gender\ndirections closer. We extensively evaluate our approach on intrinsic and\nextrinsic metrics, and show that MABEL outperforms previous task-agnostic\ndebiasing approaches in terms of fairness. It also preserves task performance\nafter fine-tuning on downstream tasks. Together, these findings demonstrate the\nsuitability of NLI data as an effective means of bias mitigation, as opposed to\nonly using unlabeled sentences in the literature. Finally, we identify that\nexisting approaches often use evaluation settings that are insufficient or\ninconsistent. We make an effort to reproduce and compare previous methods, and\ncall for unifying the evaluation settings across gender debiasing methods for\nbetter future comparison.", "published": "2022-10-26 18:36:58", "link": "http://arxiv.org/abs/2210.14975v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Robust Domain Adaptation for Pre-trained Multilingual Neural Machine\n  Translation Models", "abstract": "Recent literature has demonstrated the potential of multilingual Neural\nMachine Translation (mNMT) models. However, the most efficient models are not\nwell suited to specialized industries. In these cases, internal data is scarce\nand expensive to find in all language pairs. Therefore, fine-tuning a mNMT\nmodel on a specialized domain is hard. In this context, we decided to focus on\na new task: Domain Adaptation of a pre-trained mNMT model on a single pair of\nlanguage while trying to maintain model quality on generic domain data for all\nlanguage pairs. The risk of loss on generic domain and on other pairs is high.\nThis task is key for mNMT model adoption in the industry and is at the border\nof many others. We propose a fine-tuning procedure for the generic mNMT that\ncombines embeddings freezing and adversarial loss. Our experiments demonstrated\nthat the procedure improves performances on specialized data with a minimal\nloss in initial performances on generic domain for all languages pairs,\ncompared to a naive standard approach (+10.0 BLEU score on specialized data,\n-0.01 to -0.5 BLEU on WMT and Tatoeba datasets on the other pairs with M2M100).", "published": "2022-10-26 18:47:45", "link": "http://arxiv.org/abs/2210.14979v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalization Differences between End-to-End and Neuro-Symbolic\n  Vision-Language Reasoning Systems", "abstract": "For vision-and-language reasoning tasks, both fully connectionist, end-to-end\nmethods and hybrid, neuro-symbolic methods have achieved high in-distribution\nperformance. In which out-of-distribution settings does each paradigm excel? We\ninvestigate this question on both single-image and multi-image visual\nquestion-answering through four types of generalization tests: a novel\nsegment-combine test for multi-image queries, contrast set, compositional\ngeneralization, and cross-benchmark transfer. Vision-and-language end-to-end\ntrained systems exhibit sizeable performance drops across all these tests.\nNeuro-symbolic methods suffer even more on cross-benchmark transfer from GQA to\nVQA, but they show smaller accuracy drops on the other generalization tests and\ntheir performance quickly improves by few-shot training. Overall, our results\ndemonstrate the complementary benefits of these two paradigms, and emphasize\nthe importance of using a diverse suite of generalization tests to fully\ncharacterize model robustness to distribution shift.", "published": "2022-10-26 21:11:47", "link": "http://arxiv.org/abs/2210.15037v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Privately Fine-Tuning Large Language Models with Differential Privacy", "abstract": "Pre-trained Large Language Models (LLMs) are an integral part of modern AI\nthat have led to breakthrough performances in complex AI tasks. Major AI\ncompanies with expensive infrastructures are able to develop and train these\nlarge models with billions and millions of parameters from scratch. Third\nparties, researchers, and practitioners are increasingly adopting these\npre-trained models and fine-tuning them on their private data to accomplish\ntheir downstream AI tasks. However, it has been shown that an adversary can\nextract/reconstruct the exact training samples from these LLMs, which can lead\nto revealing personally identifiable information. The issue has raised deep\nconcerns about the privacy of LLMs. Differential privacy (DP) provides a\nrigorous framework that allows adding noise in the process of training or\nfine-tuning LLMs such that extracting the training data becomes infeasible\n(i.e., with a cryptographically small success probability). While the\ntheoretical privacy guarantees offered in most extant studies assume learning\nmodels from scratch through many training iterations in an asymptotic setting,\nthis assumption does not hold in fine-tuning scenarios in which the number of\ntraining iterations is significantly smaller. To address the gap, we present\n\\ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with\nfinite-sample privacy guarantees. Our results across four well-established\nnatural language understanding (NLU) tasks show that while \\ewtune~adds privacy\nguarantees to LLM fine-tuning process, it directly contributes to decreasing\nthe induced noise to up to 5.6\\% and improves the state-of-the-art LLMs\nperformance by up to 1.1\\% across all NLU tasks. We have open-sourced our\nimplementations for wide adoption and public testing purposes.", "published": "2022-10-26 21:18:31", "link": "http://arxiv.org/abs/2210.15042v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "DyREx: Dynamic Query Representation for Extractive Question Answering", "abstract": "Extractive question answering (ExQA) is an essential task for Natural\nLanguage Processing. The dominant approach to ExQA is one that represents the\ninput sequence tokens (question and passage) with a pre-trained transformer,\nthen uses two learned query vectors to compute distributions over the start and\nend answer span positions. These query vectors lack the context of the inputs,\nwhich can be a bottleneck for the model performance. To address this problem,\nwe propose \\textit{DyREx}, a generalization of the \\textit{vanilla} approach\nwhere we dynamically compute query vectors given the input, using an attention\nmechanism through transformer layers. Empirical observations demonstrate that\nour approach consistently improves the performance over the standard one. The\ncode and accompanying files for running the experiments are available at\n\\url{https://github.com/urchade/DyReX}.", "published": "2022-10-26 21:26:44", "link": "http://arxiv.org/abs/2210.15048v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards automatic generation of Piping and Instrumentation Diagrams\n  (P&IDs) with Artificial Intelligence", "abstract": "Developing Piping and Instrumentation Diagrams (P&IDs) is a crucial step\nduring the development of chemical processes. Currently, this is a tedious,\nmanual, and time-consuming task. We propose a novel, completely data-driven\nmethod for the prediction of control structures. Our methodology is inspired by\nend-to-end transformer-based human language translation models. We cast the\ncontrol structure prediction as a translation task where Process Flow Diagrams\n(PFDs) are translated to P&IDs. To use established transformer-based language\ntranslation models, we represent the P&IDs and PFDs as strings using our\nrecently proposed SFILES 2.0 notation. Model training is performed in a\ntransfer learning approach. Firstly, we pre-train our model using generated\nP&IDs to learn the grammatical structure of the process diagrams. Thereafter,\nthe model is fine-tuned leveraging transfer learning on real P&IDs. The model\nachieved a top-5 accuracy of 74.8% on 10,000 generated P&IDs and 89.2% on\n100,000 generated P&IDs. These promising results show great potential for\nAI-assisted process engineering. The tests on a dataset of 312 real P&IDs\nindicate the need of a larger P&IDs dataset for industry applications.", "published": "2022-10-26 10:03:15", "link": "http://arxiv.org/abs/2211.05583v1", "categories": ["cs.CL", "math.OC"], "primary_category": "cs.CL"}
{"title": "IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from\n  Egocentric Videos and Text", "abstract": "We present IMU2CLIP, a novel pre-training approach to align Inertial\nMeasurement Unit (IMU) motion sensor recordings with video and text, by\nprojecting them into the joint representation space of Contrastive\nLanguage-Image Pre-training (CLIP). The proposed approach allows IMU2CLIP to\ntranslate human motions (as measured by IMU sensors) into their corresponding\ntextual descriptions and videos -- while preserving the transitivity across\nthese modalities.\n  We explore several new IMU-based applications that IMU2CLIP enables, such as\nmotion-based media retrieval and natural language reasoning tasks with motion\ndata. In addition, we show that IMU2CLIP can significantly improve the\ndownstream performance when fine-tuned for each application (e.g. activity\nrecognition), demonstrating the universal usage of IMU2CLIP as a new\npre-trained resource. Our code will be made publicly available.", "published": "2022-10-26 00:22:41", "link": "http://arxiv.org/abs/2210.14395v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "RedPen: Region- and Reason-Annotated Dataset of Unnatural Speech", "abstract": "Even with recent advances in speech synthesis models, the evaluation of such\nmodels is based purely on human judgement as a single naturalness score, such\nas the Mean Opinion Score (MOS). The score-based metric does not give any\nfurther information about which parts of speech are unnatural or why human\njudges believe they are unnatural. We present a novel speech dataset, RedPen,\nwith human annotations on unnatural speech regions and their corresponding\nreasons. RedPen consists of 180 synthesized speeches with unnatural regions\nannotated by crowd workers; These regions are then reasoned and categorized by\nerror types, such as voice trembling and background noise. We find that our\ndataset shows a better explanation for unnatural speech regions than the\nmodel-driven unnaturalness prediction. Our analysis also shows that each model\nincludes different types of error types. Summing up, our dataset successfully\nshows the possibility that various error regions and types lie under the single\nnaturalness score. We believe that our dataset will shed light on the\nevaluation and development of more interpretable speech models in the future.\nOur dataset will be publicly available upon acceptance.", "published": "2022-10-26 01:16:35", "link": "http://arxiv.org/abs/2210.14406v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Smart Speech Segmentation using Acousto-Linguistic Features with\n  look-ahead", "abstract": "Segmentation for continuous Automatic Speech Recognition (ASR) has\ntraditionally used silence timeouts or voice activity detectors (VADs), which\nare both limited to acoustic features. This segmentation is often overly\naggressive, given that people naturally pause to think as they speak.\nConsequently, segmentation happens mid-sentence, hindering both punctuation and\ndownstream tasks like machine translation for which high-quality segmentation\nis critical. Model-based segmentation methods that leverage acoustic features\nare powerful, but without an understanding of the language itself, these\napproaches are limited. We present a hybrid approach that leverages both\nacoustic and language information to improve segmentation. Furthermore, we show\nthat including one word as a look-ahead boosts segmentation quality. On\naverage, our models improve segmentation-F0.5 score by 9.8% over baseline. We\nshow that this approach works for multiple languages. For the downstream task\nof machine translation, it improves the translation BLEU score by an average of\n1.05 points.", "published": "2022-10-26 03:36:31", "link": "http://arxiv.org/abs/2210.14446v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Speech-to-Speech Translation Through Unlabeled Text", "abstract": "Direct speech-to-speech translation (S2ST) is among the most challenging\nproblems in the translation paradigm due to the significant scarcity of S2ST\ndata. While effort has been made to increase the data size from unlabeled\nspeech by cascading pretrained speech recognition (ASR), machine translation\n(MT) and text-to-speech (TTS) models; unlabeled text has remained relatively\nunder-utilized to improve S2ST. We propose an effective way to utilize the\nmassive existing unlabeled text from different languages to create a large\namount of S2ST data to improve S2ST performance by applying various acoustic\neffects to the generated synthetic data. Empirically our method outperforms the\nstate of the art in Spanish-English translation by up to 2 BLEU. Significant\ngains by the proposed method are demonstrated in extremely low-resource\nsettings for both Spanish-English and Russian-English translations.", "published": "2022-10-26 06:52:19", "link": "http://arxiv.org/abs/2210.14514v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "OTSeq2Set: An Optimal Transport Enhanced Sequence-to-Set Model for\n  Extreme Multi-label Text Classification", "abstract": "Extreme multi-label text classification (XMTC) is the task of finding the\nmost relevant subset labels from an extremely large-scale label collection.\nRecently, some deep learning models have achieved state-of-the-art results in\nXMTC tasks. These models commonly predict scores for all labels by a fully\nconnected layer as the last layer of the model. However, such models can't\npredict a relatively complete and variable-length label subset for each\ndocument, because they select positive labels relevant to the document by a\nfixed threshold or take top k labels in descending order of scores. A less\npopular type of deep learning models called sequence-to-sequence (Seq2Seq)\nfocus on predicting variable-length positive labels in sequence style. However,\nthe labels in XMTC tasks are essentially an unordered set rather than an\nordered sequence, the default order of labels restrains Seq2Seq models in\ntraining. To address this limitation in Seq2Seq, we propose an autoregressive\nsequence-to-set model for XMTC tasks named OTSeq2Set. Our model generates\npredictions in student-forcing scheme and is trained by a loss function based\non bipartite matching which enables permutation-invariance. Meanwhile, we use\nthe optimal transport distance as a measurement to force the model to focus on\nthe closest labels in semantic label space. Experiments show that OTSeq2Set\noutperforms other competitive baselines on 4 benchmark datasets. Especially, on\nthe Wikipedia dataset with 31k labels, it outperforms the state-of-the-art\nSeq2Seq method by 16.34% in micro-F1 score. The code is available at\nhttps://github.com/caojie54/OTSeq2Set.", "published": "2022-10-26 07:25:18", "link": "http://arxiv.org/abs/2210.14523v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Knowledge Graphs for Automating AI of Digital Twins", "abstract": "Digital Twins are digital representations of systems in the Internet of\nThings (IoT) that are often based on AI models that are trained on data from\nthose systems. Semantic models are used increasingly to link these datasets\nfrom different stages of the IoT systems life-cycle together and to\nautomatically configure the AI modelling pipelines. This combination of\nsemantic models with AI pipelines running on external datasets raises unique\nchallenges particular if rolled out at scale. Within this paper we will discuss\nthe unique requirements of applying semantic graphs to automate Digital Twins\nin different practical use cases. We will introduce the benchmark dataset DTBM\nthat reflects these characteristics and look into the scaling challenges of\ndifferent knowledge graph technologies. Based on these insights we will propose\na reference architecture that is in-use in multiple products in IBM and derive\nlessons learned for scaling knowledge graphs for configuring AI models for\nDigital Twins.", "published": "2022-10-26 10:12:10", "link": "http://arxiv.org/abs/2210.14596v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "Pronunciation Generation for Foreign Language Words in Intra-Sentential\n  Code-Switching Speech Recognition", "abstract": "Code-Switching refers to the phenomenon of switching languages within a\nsentence or discourse. However, limited code-switching , different language\nphoneme-sets and high rebuilding costs throw a challenge to make the\nspecialized acoustic model for code-switching speech recognition. In this\npaper, we make use of limited code-switching data as driving materials and\nexplore a shortcut to quickly develop intra-sentential code-switching\nrecognition skill on the commissioned native language acoustic model, where we\npropose a data-driven method to make the seed lexicon which is used to train\ngrapheme-to-phoneme model to predict mapping pronunciations for foreign\nlanguage word in code-switching sentences. The core work of the data-driven\ntechnology in this paper consists of a phonetic decoding method and different\nselection methods. And for imbalanced word-level driving materials problem, we\nhave an internal assistance inspiration that learning the good pronunciation\nrules in the words that possess sufficient materials using the\ngrapheme-to-phoneme model to help the scarce. Our experiments show that the\nMixed Error Rate in intra-sentential Chinese-English code-switching recognition\nreduced from 29.15\\%, acquired on the pure Chinese recognizer, to 12.13\\% by\nadding foreign language words' pronunciation through our data-driven approach,\nand finally get the best result 11.14\\% with the combination of different\nselection methods and internal assistance tactic.", "published": "2022-10-26 13:19:35", "link": "http://arxiv.org/abs/2210.14691v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black\n  Magic?", "abstract": "Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently attracted attention in code\nassistants, with programs automatically written in a given programming language\nfrom a programming task description in natural language. They have the\npotential to save time and effort when writing code. However, these systems are\ncurrently poorly understood, preventing them from being used optimally. In this\npaper, we investigate the various input parameters of two language models, and\nconduct a study to understand if variations of these input parameters (e.g.\nprogramming task description and the surrounding context, creativity of the\nlanguage model, number of generated solutions) can have a significant impact on\nthe quality of the generated programs. We design specific operators for varying\ninput parameters and apply them over two code assistants (Copilot and Codex)\nand two benchmarks representing algorithmic problems (HumanEval and LeetCode).\nOur results showed that varying the input parameters can significantly improve\nthe performance of language models. However, there is a tight dependency when\nvarying the temperature, the prompt and the number of generated solutions,\nmaking potentially hard for developers to properly control the parameters to\nobtain an optimal result. This work opens opportunities to propose (automated)\nstrategies for improving performance.", "published": "2022-10-26 13:28:14", "link": "http://arxiv.org/abs/2210.14699v2", "categories": ["cs.SE", "cs.CL", "cs.PL", "68T50"], "primary_category": "cs.SE"}
{"title": "Monotonic segmental attention for automatic speech recognition", "abstract": "We introduce a novel segmental-attention model for automatic speech\nrecognition. We restrict the decoder attention to segments to avoid quadratic\nruntime of global attention, better generalize to long sequences, and\neventually enable streaming. We directly compare global-attention and different\nsegmental-attention modeling variants. We develop and compare two separate\ntime-synchronous decoders, one specifically taking the segmental nature into\naccount, yielding further improvements. Using time-synchronous decoding for\nsegmental models is novel and a step towards streaming applications. Our\nexperiments show the importance of a length model to predict the segment\nboundaries. The final best segmental-attention model using segmental decoding\nperforms better than global-attention, in contrast to other monotonic attention\napproaches in the literature. Further, we observe that the segmental model\ngeneralizes much better to long sequences of up to several minutes.", "published": "2022-10-26 14:21:23", "link": "http://arxiv.org/abs/2210.14742v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Don't Prompt, Search! Mining-based Zero-Shot Learning with Language\n  Models", "abstract": "Masked language models like BERT can perform text classification in a\nzero-shot fashion by reformulating downstream tasks as text infilling. However,\nthis approach is highly sensitive to the template used to prompt the model, yet\npractitioners are blind when designing them in strict zero-shot settings. In\nthis paper, we propose an alternative mining-based approach for zero-shot\nlearning. Instead of prompting language models, we use regular expressions to\nmine labeled examples from unlabeled corpora, which can optionally be filtered\nthrough prompting, and used to finetune a pretrained model. Our method is more\nflexible and interpretable than prompting, and outperforms it on a wide range\nof tasks when using comparable templates. Our results suggest that the success\nof prompting can partly be explained by the model being exposed to similar\nexamples during pretraining, which can be directly retrieved through regular\nexpressions.", "published": "2022-10-26 15:52:30", "link": "http://arxiv.org/abs/2210.14803v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic\n  Constraints for Adversarial Examples", "abstract": "Natural language inference (NLI) is critical for complex decision-making in\nbiomedical domain. One key question, for example, is whether a given biomedical\nmechanism is supported by experimental evidence. This can be seen as an NLI\nproblem but there are no directly usable datasets to address this. The main\nchallenge is that manually creating informative negative examples for this task\nis difficult and expensive. We introduce a novel semi-supervised procedure that\nbootstraps an NLI dataset from existing biomedical dataset that pairs\nmechanisms with experimental evidence in abstracts. We generate a range of\nnegative examples using nine strategies that manipulate the structure of the\nunderlying mechanisms both with rules, e.g., flip the roles of the entities in\nthe interaction, and, more importantly, as perturbations via logical\nconstraints in a neuro-logical decoding system. We use this procedure to create\na novel dataset for NLI in the biomedical domain, called BioNLI and benchmark\ntwo state-of-the-art biomedical classifiers. The best result we obtain is\naround mid 70s in F1, suggesting the difficulty of the task. Critically, the\nperformance on the different classes of negative examples varies widely, from\n97% F1 on the simple role change negative examples, to barely better than\nchance on the negative examples generated using neuro-logic decoding.", "published": "2022-10-26 16:02:49", "link": "http://arxiv.org/abs/2210.14814v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Semantic Parsing: From Images to Abstract Meaning Representation", "abstract": "The success of scene graphs for visual scene understanding has brought\nattention to the benefits of abstracting a visual input (e.g., image) into a\nstructured representation, where entities (people and objects) are nodes\nconnected by edges specifying their relations. Building these representations,\nhowever, requires expensive manual annotation in the form of images paired with\ntheir scene graphs or frames. These formalisms remain limited in the nature of\nentities and relations they can capture. In this paper, we propose to leverage\na widely-used meaning representation in the field of natural language\nprocessing, the Abstract Meaning Representation (AMR), to address these\nshortcomings. Compared to scene graphs, which largely emphasize spatial\nrelationships, our visual AMR graphs are more linguistically informed, with a\nfocus on higher-level semantic concepts extrapolated from visual input.\nMoreover, they allow us to generate meta-AMR graphs to unify information\ncontained in multiple image descriptions under one representation. Through\nextensive experimentation and analysis, we demonstrate that we can re-purpose\nan existing text-to-AMR parser to parse images into AMRs. Our findings point to\nimportant future research directions for improved scene understanding.", "published": "2022-10-26 17:06:42", "link": "http://arxiv.org/abs/2210.14862v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "What's Different between Visual Question Answering for Machine\n  \"Understanding\" Versus for Accessibility?", "abstract": "In visual question answering (VQA), a machine must answer a question given an\nassociated image. Recently, accessibility researchers have explored whether VQA\ncan be deployed in a real-world setting where users with visual impairments\nlearn about their environment by capturing their visual surroundings and asking\nquestions. However, most of the existing benchmarking datasets for VQA focus on\nmachine \"understanding\" and it remains unclear how progress on those datasets\ncorresponds to improvements in this real-world use case. We aim to answer this\nquestion by evaluating discrepancies between machine \"understanding\" datasets\n(VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA\nmodels. Based on our findings, we discuss opportunities and challenges in VQA\nfor accessibility and suggest directions for future work.", "published": "2022-10-26 18:23:53", "link": "http://arxiv.org/abs/2210.14966v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Efficient Utilization of Large Pre-Trained Models for Low Resource ASR", "abstract": "Unsupervised representation learning has recently helped automatic speech\nrecognition (ASR) to tackle tasks with limited labeled data. Following this,\nhardware limitations and applications give rise to the question how to take\nadvantage of large pre-trained models efficiently and reduce their complexity.\nIn this work, we study a challenging low resource conversational telephony\nspeech corpus from the medical domain in Vietnamese and German. We show the\nbenefits of using unsupervised techniques beyond simple fine-tuning of large\npre-trained models, discuss how to adapt them to a practical telephony task\nincluding bandwidth transfer and investigate different data conditions for\npre-training and fine-tuning. We outperform the project baselines by 22%\nrelative using pretraining techniques. Further gains of 29% can be achieved by\nrefinements of architecture and training and 6% by adding 0.8 h of in-domain\nadaptation data.", "published": "2022-10-26 17:34:30", "link": "http://arxiv.org/abs/2210.15445v3", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic extraction of materials and properties from superconductors\n  scientific literature", "abstract": "The automatic extraction of materials and related properties from the\nscientific literature is gaining attention in data-driven materials science\n(Materials Informatics). In this paper, we discuss Grobid-superconductors, our\nsolution for automatically extracting superconductor material names and\nrespective properties from text. Built as a Grobid module, it combines machine\nlearning and heuristic approaches in a multi-step architecture that supports\ninput data as raw text or PDF documents. Using Grobid-superconductors, we built\nSuperCon2, a database of 40324 materials and properties records from 37700\npapers. The material (or sample) information is represented by name, chemical\nformula, and material class, and is characterized by shape, doping,\nsubstitution variables for components, and substrate as adjoined information.\nThe properties include the Tc superconducting critical temperature and, when\navailable, applied pressure with the Tc measurement method.", "published": "2022-10-26 01:03:28", "link": "http://arxiv.org/abs/2210.15600v2", "categories": ["cs.CL", "cond-mat.supr-con", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Robustness of Prefix Tuning in Noisy Data: A Case Study in\n  Financial Sentiment Analysis", "abstract": "The invention of transformer-based models such as BERT, GPT, and RoBERTa has\nenabled researchers and financial companies to finetune these powerful models\nand use them in different downstream tasks to achieve state-of-the-art\nperformance. Recently, a lightweight alternative (approximately 0.1% - 3% of\nthe original model parameters) to fine-tuning, known as prefix tuning has been\nintroduced. This method freezes the model parameters and only updates the\nprefix to achieve performance comparable to full fine-tuning. Prefix tuning\nenables researchers and financial practitioners to achieve similar results with\nmuch fewer parameters. In this paper, we explore the robustness of prefix\ntuning when facing noisy data. Our experiments demonstrate that fine-tuning is\nmore robust to noise than prefix tuning -- the latter method faces a\nsignificant decrease in performance on most corrupted data sets with increasing\nnoise levels. Furthermore, prefix tuning has high variances in the F1 scores\ncompared to fine-tuning in many corruption methods. We strongly advocate that\ncaution should be carefully taken when applying the state-of-the-art prefix\ntuning method to noisy data.", "published": "2022-10-26 01:13:41", "link": "http://arxiv.org/abs/2211.05584v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incongruity Detection between Bangla News Headline and Body Content\n  through Graph Neural Network", "abstract": "Incongruity between news headlines and the body content is a common method of\ndeception used to attract readers. Profitable headlines pique readers' interest\nand encourage them to visit a specific website. This is usually done by adding\nan element of dishonesty, using enticements that do not precisely reflect the\ncontent being delivered. As a result, automatic detection of incongruent news\nbetween headline and body content using language analysis has gained the\nresearch community's attention. However, various solutions are primarily being\ndeveloped for English to address this problem, leaving low-resource languages\nout of the picture. Bangla is ranked 7th among the top 100 most widely spoken\nlanguages, which motivates us to pay special attention to the Bangla language.\nFurthermore, Bangla has a more complex syntactic structure and fewer natural\nlanguage processing resources, so it becomes challenging to perform NLP tasks\nlike incongruity detection and stance detection. To tackle this problem, for\nthe Bangla language, we offer a graph-based hierarchical dual encoder (BGHDE)\nmodel that learns the content similarity and contradiction between Bangla news\nheadlines and content paragraphs effectively. The experimental results show\nthat the proposed Bangla graph-based neural network model achieves above 90%\naccuracy on various Bangla news datasets.", "published": "2022-10-26 20:57:45", "link": "http://arxiv.org/abs/2211.07709v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilevel Transformer For Multimodal Emotion Recognition", "abstract": "Multimodal emotion recognition has attracted much attention recently. Fusing\nmultiple modalities effectively with limited labeled data is a challenging\ntask. Considering the success of pre-trained model and fine-grained nature of\nemotion expression, it is reasonable to take these two aspects into\nconsideration. Unlike previous methods that mainly focus on one aspect, we\nintroduce a novel multi-granularity framework, which combines fine-grained\nrepresentation with pre-trained utterance-level representation. Inspired by\nTransformer TTS, we propose a multilevel transformer model to perform\nfine-grained multimodal emotion recognition. Specifically, we explore different\nmethods to incorporate phoneme-level embedding with word-level embedding. To\nperform multi-granularity learning, we simply combine multilevel transformer\nmodel with Albert. Extensive experimental results show that both our multilevel\ntransformer model and multi-granularity model outperform previous\nstate-of-the-art approaches on IEMOCAP dataset with text transcripts and speech\nsignal.", "published": "2022-10-26 10:31:24", "link": "http://arxiv.org/abs/2211.07711v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Laws Beyond Backpropagation", "abstract": "Alternatives to backpropagation have long been studied to better understand\nhow biological brains may learn. Recently, they have also garnered interest as\na way to train neural networks more efficiently. By relaxing constraints\ninherent to backpropagation (e.g., symmetric feedforward and feedback weights,\nsequential updates), these methods enable promising prospects, such as local\nlearning. However, the tradeoffs between different methods in terms of final\ntask performance, convergence speed, and ultimately compute and data\nrequirements are rarely outlined. In this work, we use scaling laws to study\nthe ability of Direct Feedback Alignment~(DFA) to train causal decoder-only\nTransformers efficiently. Scaling laws provide an overview of the tradeoffs\nimplied by a modeling decision, up to extrapolating how it might transfer to\nincreasingly large models. We find that DFA fails to offer more efficient\nscaling than backpropagation: there is never a regime for which the degradation\nin loss incurred by using DFA is worth the potential reduction in compute\nbudget. Our finding comes at variance with previous beliefs in the alternative\ntraining methods community, and highlights the need for holistic empirical\napproaches to better understand modeling decisions.", "published": "2022-10-26 10:09:14", "link": "http://arxiv.org/abs/2210.14593v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "There is more than one kind of robustness: Fooling Whisper with\n  adversarial examples", "abstract": "Whisper is a recent Automatic Speech Recognition (ASR) model displaying\nimpressive robustness to both out-of-distribution inputs and random noise. In\nthis work, we show that this robustness does not carry over to adversarial\nnoise. We show that we can degrade Whisper performance dramatically, or even\ntranscribe a target sentence of our choice, by generating very small input\nperturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling\nthe Whisper language detector we can very easily degrade the performance of\nmultilingual models. These vulnerabilities of a widely popular open-source\nmodel have practical security implications and emphasize the need for\nadversarially robust ASR.", "published": "2022-10-26 21:03:17", "link": "http://arxiv.org/abs/2210.17316v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Will we run out of data? Limits of LLM scaling based on human-generated\n  data", "abstract": "We investigate the potential constraints on LLM scaling posed by the\navailability of public human-generated text data. We forecast the growing\ndemand for training data based on current trends and estimate the total stock\nof public human text data. Our findings indicate that if current LLM\ndevelopment trends continue, models will be trained on datasets roughly equal\nin size to the available stock of public human text data between 2026 and 2032,\nor slightly earlier if models are overtrained. We explore how progress in\nlanguage modeling can continue when human-generated text datasets cannot be\nscaled any further. We argue that synthetic data generation, transfer learning\nfrom data-rich domains, and data efficiency improvements might support further\nprogress.", "published": "2022-10-26 00:28:40", "link": "http://arxiv.org/abs/2211.04325v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.CY"], "primary_category": "cs.LG"}
{"title": "End-to-End Speech to Intent Prediction to improve E-commerce Customer\n  Support Voicebot in Hindi and English", "abstract": "Automation of on-call customer support relies heavily on accurate and\nefficient speech-to-intent (S2I) systems. Building such systems using\nmulti-component pipelines can pose various challenges because they require\nlarge annotated datasets, have higher latency, and have complex deployment.\nThese pipelines are also prone to compounding errors. To overcome these\nchallenges, we discuss an end-to-end (E2E) S2I model for customer support\nvoicebot task in a bilingual setting. We show how we can solve E2E intent\nclassification by leveraging a pre-trained automatic speech recognition (ASR)\nmodel with slight modification and fine-tuning on small annotated datasets.\nExperimental results show that our best E2E model outperforms a conventional\npipeline by a relative ~27% on the F1 score.", "published": "2022-10-26 18:29:44", "link": "http://arxiv.org/abs/2211.07710v1", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The NPU-ASLP System for The ISCSLP 2022 Magichub Code-Swiching ASR\n  Challenge", "abstract": "This paper describes our NPU-ASLP system submitted to the ISCSLP 2022\nMagichub Code-Switching ASR Challenge. In this challenge, we first explore\nseveral popular end-to-end ASR architectures and training strategies, including\nbi-encoder, language-aware encoder (LAE) and mixture of experts (MoE). To\nimprove our system's language modeling ability, we further attempt the internal\nlanguage model as well as the long context language model. Given the limited\ntraining data in the challenge, we further investigate the effects of data\naugmentation, including speed perturbation, pitch shifting, speech codec,\nSpecAugment and synthetic data from text-to-speech (TTS). Finally, we explore\nROVER-based score fusion to make full use of complementary hypotheses from\ndifferent models. Our submitted system achieves 16.87% on mix error rate (MER)\non the test set and comes to the 2nd place in the challenge ranking.", "published": "2022-10-26 03:42:55", "link": "http://arxiv.org/abs/2210.14448v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AVES: Animal Vocalization Encoder based on Self-Supervision", "abstract": "The lack of annotated training data in bioacoustics hinders the use of\nlarge-scale neural network models trained in a supervised way. In order to\nleverage a large amount of unannotated audio data, we propose AVES (Animal\nVocalization Encoder based on Self-Supervision), a self-supervised,\ntransformer-based audio representation model for encoding animal vocalizations.\nWe pretrain AVES on a diverse set of unannotated audio datasets and fine-tune\nthem for downstream bioacoustics tasks. Comprehensive experiments with a suite\nof classification and detection tasks have shown that AVES outperforms all the\nstrong baselines and even the supervised \"topline\" models trained on annotated\naudio classification datasets. The results also suggest that curating a small\ntraining subset related to downstream tasks is an efficient way to train\nhigh-quality audio representation models. We open-source our models at\n\\url{https://github.com/earthspecies/aves}.", "published": "2022-10-26 05:38:50", "link": "http://arxiv.org/abs/2210.14493v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Two-stage dimensional emotion recognition by fusing predictions of\n  acoustic and text networks using SVM", "abstract": "Automatic speech emotion recognition (SER) by a computer is a critical\ncomponent for more natural human-machine interaction. As in human-human\ninteraction, the capability to perceive emotion correctly is essential to take\nfurther steps in a particular situation. One issue in SER is whether it is\nnecessary to combine acoustic features with other data such as facial\nexpressions, text, and motion capture. This research proposes to combine\nacoustic and text information by applying a late-fusion approach consisting of\ntwo steps. First, acoustic and text features are trained separately in deep\nlearning systems. Second, the prediction results from the deep learning systems\nare fed into a support vector machine (SVM) to predict the final regression\nscore. Furthermore, the task in this research is dimensional emotion modeling\nbecause it can enable a deeper analysis of affective states. Experimental\nresults show that this two-stage, late-fusion approach, obtains higher\nperformance than that of any one-stage processing, with a linear correlation\nfrom one-stage to two-stage processing. This late-fusion approach improves\nprevious early fusion results measured in concordance correlation coefficients\nscore.", "published": "2022-10-26 05:49:13", "link": "http://arxiv.org/abs/2210.14495v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Effect of different splitting criteria on the performance of speech\n  emotion recognition", "abstract": "Traditional speech emotion recognition (SER) evaluations have been performed\nmerely on a speaker-independent condition; some of them even did not evaluate\ntheir result on this condition. This paper highlights the importance of\nsplitting training and test data for SER by script, known as sentence-open or\ntext-independent criteria. The results show that employing sentence-open\ncriteria degraded the performance of SER. This finding implies the difficulties\nof recognizing emotion from speech in different linguistic information embedded\nin acoustic information. Surprisingly, text-independent criteria consistently\nperformed worse than speaker+text-independent criteria. The full order of\ndifficulties for splitting criteria on SER performances from the most difficult\nto the easiest is text-independent, speaker+text-independent,\nspeaker-independent, and speaker+text-dependent. The gap between\nspeaker+text-independent and text-independent was smaller than other criteria,\nstrengthening the difficulties of recognizing emotion from speech in different\nsentences.", "published": "2022-10-26 06:16:09", "link": "http://arxiv.org/abs/2210.14501v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "UFO2: A unified pre-training framework for online and offline speech\n  recognition", "abstract": "In this paper, we propose a Unified pre-training Framework for Online and\nOffline (UFO2) Automatic Speech Recognition (ASR), which 1) simplifies the two\nseparate training workflows for online and offline modes into one process, and\n2) improves the Word Error Rate (WER) performance with limited utterance\nannotating. Specifically, we extend the conventional offline-mode\nSelf-Supervised Learning (SSL)-based ASR approach to a unified manner, where\nthe model training is conditioned on both the full-context and dynamic-chunked\ninputs. To enhance the pre-trained representation model, stop-gradient\noperation is applied to decouple the online-mode objectives to the quantizer.\nMoreover, in both the pre-training and the downstream fine-tuning stages, joint\nlosses are proposed to train the unified model with full-weight sharing for the\ntwo modes. Experimental results on the LibriSpeech dataset show that UFO2\noutperforms the SSL-based baseline method by 29.7% and 18.2% relative WER\nreduction in offline and online modes, respectively.", "published": "2022-10-26 06:59:02", "link": "http://arxiv.org/abs/2210.14515v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reducing Language confusion for Code-switching Speech Recognition with\n  Token-level Language Diarization", "abstract": "Code-switching (CS) refers to the phenomenon that languages switch within a\nspeech signal and leads to language confusion for automatic speech recognition\n(ASR). This paper aims to address language confusion for improving CS-ASR from\ntwo perspectives: incorporating and disentangling language information. We\nincorporate language information in the CS-ASR model by dynamically biasing the\nmodel with token-level language posteriors which are outputs of a\nsequence-to-sequence auxiliary language diarization module. In contrast, the\ndisentangling process reduces the difference between languages via adversarial\ntraining so as to normalize two languages. We conduct the experiments on the\nSEAME dataset. Compared to the baseline model, both the joint optimization with\nLD and the language posterior bias achieve performance improvement. The\ncomparison of the proposed methods indicates that incorporating language\ninformation is more effective than disentangling for reducing language\nconfusion in CS speech.", "published": "2022-10-26 08:55:25", "link": "http://arxiv.org/abs/2210.14567v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast Yet Effective Speech Emotion Recognition with Self-distillation", "abstract": "Speech emotion recognition (SER) is the task of recognising human's emotional\nstates from speech. SER is extremely prevalent in helping dialogue systems to\ntruly understand our emotions and become a trustworthy human conversational\npartner. Due to the lengthy nature of speech, SER also suffers from the lack of\nabundant labelled data for powerful models like deep neural networks.\nPre-trained complex models on large-scale speech datasets have been\nsuccessfully applied to SER via transfer learning. However, fine-tuning complex\nmodels still requires large memory space and results in low inference\nefficiency. In this paper, we argue achieving a fast yet effective SER is\npossible with self-distillation, a method of simultaneously fine-tuning a\npretrained model and training shallower versions of itself. The benefits of our\nself-distillation framework are threefold: (1) the adoption of\nself-distillation method upon the acoustic modality breaks through the limited\nground-truth of speech data, and outperforms the existing models' performance\non an SER dataset; (2) executing powerful models at different depth can achieve\nadaptive accuracy-efficiency trade-offs on resource-limited edge devices; (3) a\nnew fine-tuning process rather than training from scratch for self-distillation\nleads to faster learning time and the state-of-the-art accuracy on data with\nsmall quantities of label information.", "published": "2022-10-26 11:28:29", "link": "http://arxiv.org/abs/2210.14636v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Diarization Based on Multi-channel Microphone Array in\n  Small-scale Meeting", "abstract": "In the task of speaker diarization, the number of small-scale meetings\naccounts for a large proportion. When microphone arrays are employed as a\nrecording device, its spatial information is usually ignored by most\nresearchers. In this paper, inspired by the clustering method combining\nd-vector and microphone array spatial vector, we proposed a diarization method\nwhich using multi-channel microphone arrays for a meeting with no more than 4\nspeakers. We utilize speech enhancement to preprocess the audio from the\nmicrophone array. The Steered-Response Power Phase Transform (SRP-PHAT)\nalgorithm are employed to get more accurate speakers, and apply the number of\nspeakers to recluster the speech segments to achieve better performance.\nFinally, we fuse our system by DOVER-LAP to get the best result. We evaluated\nour system on the AMI corpus. Compared with the best experimental results so\nfar, our system has achieved largely improvement in the diarization error rate\n(DER).", "published": "2022-10-26 11:44:31", "link": "http://arxiv.org/abs/2210.14644v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on\n  Generative Adversarial Network", "abstract": "XiaoiceSing is a singing voice synthesis (SVS) system that aims at generating\n48kHz singing voices. However, the mel-spectrogram generated by it is\nover-smoothing in middle- and high-frequency areas due to no special design for\nmodeling the details of these parts. In this paper, we propose XiaoiceSing2,\nwhich can generate the details of middle- and high-frequency parts to better\nconstruct the full-band mel-spectrogram. Specifically, in order to alleviate\nthis problem, XiaoiceSing2 adopts a generative adversarial network (GAN), which\nconsists of a FastSpeech-based generator and a multi-band discriminator. We\nimprove the feed-forward Transformer (FFT) block by adding multiple residual\nconvolutional blocks in parallel with the self-attention block to balance the\nlocal and global features. The multi-band discriminator contains three\nsub-discriminators responsible for low-, middle-, and high-frequency parts of\nthe mel-spectrogram, respectively. Each sub-discriminator is composed of\nseveral segment discriminators (SD) and detail discriminators (DD) to\ndistinguish the audio from different aspects. The experiment on our internal\n48kHz singing voice dataset shows XiaoiceSing2 significantly improves the\nquality of the singing voice over XiaoiceSing.", "published": "2022-10-26 12:31:16", "link": "http://arxiv.org/abs/2210.14666v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Weighted Pressure Matching Based on Kernel Interpolation For Sound Field\n  Reproduction", "abstract": "A sound field reproduction method called weighted pressure matching is\nproposed. Sound field reproduction is aimed at synthesizing the desired sound\nfield using multiple loudspeakers inside a target region. Optimization-based\nmethods are derived from the minimization of errors between synthesized and\ndesired sound fields, which enable the use of an arbitrary array geometry in\ncontrast with integral-equation-based methods. Pressure matching is widely used\nin the optimization-based sound field reproduction methods because of its\nsimplicity of implementation. Its cost function is defined as the synthesis\nerrors at multiple control points inside the target region; then, the driving\nsignals of the loudspeakers are obtained by solving a least-squares problem.\nHowever, in pressure matching, the region between the control points is not\ntaken into consideration. We define the cost function as the regional\nintegration of the synthesis error over the target region. On the basis of the\nkernel interpolation of the sound field, this cost function is represented as\nthe weighted square error of the synthesized pressures at the control points.\nExperimental results indicate that the proposed weighted pressure matching\noutperforms conventional pressure matching.", "published": "2022-10-26 13:43:57", "link": "http://arxiv.org/abs/2210.14711v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Multitask Detection of Speaker Changes, Overlapping Speech and Voice\n  Activity Using wav2vec 2.0", "abstract": "Self-supervised learning approaches have lately achieved great success on a\nbroad spectrum of machine learning problems. In the field of speech processing,\none of the most successful recent self-supervised models is wav2vec 2.0. In\nthis paper, we explore the effectiveness of this model on three basic speech\nclassification tasks: speaker change detection, overlapped speech detection,\nand voice activity detection. First, we concentrate on only one task -- speaker\nchange detection -- where our proposed system surpasses the previously reported\nresults on four different corpora, and achieves comparable performance even\nwhen trained on out-of-domain data from an artificially designed dataset. Then\nwe expand our approach to tackle all three tasks in a single multitask system\nwith state-of-the-art performance on the AMI corpus. The implementation of the\nalgorithms in this paper is publicly available at\nhttps://github.com/mkunes/w2v2_audioFrameClassification.", "published": "2022-10-26 14:37:05", "link": "http://arxiv.org/abs/2210.14755v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Text-to-speech synthesis from dark data with evaluation-in-the-loop data\n  selection", "abstract": "This paper proposes a method for selecting training data for text-to-speech\n(TTS) synthesis from dark data. TTS models are typically trained on\nhigh-quality speech corpora that cost much time and money for data collection,\nwhich makes it very challenging to increase speaker variation. In contrast,\nthere is a large amount of data whose availability is unknown (a.k.a, \"dark\ndata\"), such as YouTube videos. To utilize data other than TTS corpora,\nprevious studies have selected speech data from the corpora on the basis of\nacoustic quality. However, considering that TTS models robust to data noise\nhave been proposed, we should select data on the basis of its importance as\ntraining data to the given TTS model, not the quality of speech itself. Our\nmethod with a loop of training and evaluation selects training data on the\nbasis of the automatically predicted quality of synthetic speech of a given TTS\nmodel. Results of evaluations using YouTube data reveal that our method\noutperforms the conventional acoustic-quality-based method.", "published": "2022-10-26 16:49:57", "link": "http://arxiv.org/abs/2210.14850v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustically-Driven Phoneme Removal That Preserves Vocal Affect Cues", "abstract": "In this paper, we propose a method for removing linguistic information from\nspeech for the purpose of isolating paralinguistic indicators of affect. The\nimmediate utility of this method lies in clinical tests of sensitivity to vocal\naffect that are not confounded by language, which is impaired in a variety of\nclinical populations. The method is based on simultaneous recordings of speech\naudio and electroglottographic (EGG) signals. The speech audio signal is used\nto estimate the average vocal tract filter response and amplitude envelop. The\nEGG signal supplies a direct correlate of voice source activity that is mostly\nindependent of phonetic articulation. The dynamic energy of the speech audio\nand the average vocal tract filter are applied to the EGG signal create a third\nsignal designed to capture as much paralinguistic information from the vocal\nproduction system as possible -- maximizing the retention of bioacoustic cues\nto affect -- while eliminating phonetic cues to verbal meaning. To evaluate the\nsuccess of this method, we studied the perception of corresponding speech audio\nand transformed EGG signals in an affect rating experiment with online\nlisteners. The results show a high degree of similarity in the perceived affect\nof matched signals, indicating that our method is effective.", "published": "2022-10-26 19:41:53", "link": "http://arxiv.org/abs/2210.15001v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SCP-GAN: Self-Correcting Discriminator Optimization for Training\n  Consistency Preserving Metric GAN on Speech Enhancement Tasks", "abstract": "In recent years, Generative Adversarial Networks (GANs) have produced\nsignificantly improved results in speech enhancement (SE) tasks. They are\ndifficult to train, however. In this work, we introduce several improvements to\nthe GAN training schemes, which can be applied to most GAN-based SE models. We\npropose using consistency loss functions, which target the inconsistency in\ntime and time-frequency domains caused by Fourier and Inverse Fourier\nTransforms. We also present self-correcting optimization for training a GAN\ndiscriminator on SE tasks, which helps avoid \"harmful\" training directions for\nparts of the discriminator loss function. We have tested our proposed methods\non several state-of-the-art GAN-based SE models and obtained consistent\nimprovements, including new state-of-the-art results for the Voice Bank+DEMAND\ndataset.", "published": "2022-10-26 04:48:40", "link": "http://arxiv.org/abs/2210.14474v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sinusoidal Frequency Estimation by Gradient Descent", "abstract": "Sinusoidal parameter estimation is a fundamental task in applications from\nspectral analysis to time-series forecasting. Estimating the sinusoidal\nfrequency parameter by gradient descent is, however, often impossible as the\nerror function is non-convex and densely populated with local minima. The\ngrowing family of differentiable signal processing methods has therefore been\nunable to tune the frequency of oscillatory components, preventing their use in\na broad range of applications. This work presents a technique for joint\nsinusoidal frequency and amplitude estimation using the Wirtinger derivatives\nof a complex exponential surrogate and any first order gradient-based\noptimizer, enabling end to-end training of neural network controllers for\nunconstrained sinusoidal models.", "published": "2022-10-26 04:55:04", "link": "http://arxiv.org/abs/2210.14476v2", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Parallel Gated Neural Network With Attention Mechanism For Speech\n  Enhancement", "abstract": "Deep learning algorithm are increasingly used for speech enhancement (SE). In\nsupervised methods, global and local information is required for accurate\nspectral mapping. A key restriction is often poor capture of key contextual\ninformation. To leverage long-term for target speakers and compensate\ndistortions of cleaned speech, this paper adopts a sequence-to-sequence (S2S)\nmapping structure and proposes a novel monaural speech enhancement system,\nconsisting of a Feature Extraction Block (FEB), a Compensation Enhancement\nBlock (ComEB) and a Mask Block (MB). In the FEB a U-net block is used to\nextract abstract features using complex-valued spectra with one path to\nsuppress the background noise in the magnitude domain using masking methods and\nthe MB takes magnitude features from the FEBand compensates the lost\ncomplex-domain features produced from ComEB to restore the final cleaned\nspeech. Experiments are conducted on the Librispeech dataset and results show\nthat the proposed model obtains better performance than recent models in terms\nof ESTOI and PESQ scores.", "published": "2022-10-26 06:42:19", "link": "http://arxiv.org/abs/2210.14509v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T10 (Primary) 68T07 (Secondary)"], "primary_category": "cs.SD"}
{"title": "Position tracking of a varying number of sound sources with sliding\n  permutation invariant training", "abstract": "Recent data- and learning-based sound source localization (SSL) methods have\nshown strong performance in challenging acoustic scenarios. However, little\nwork has been done on adapting such methods to track consistently multiple\nsources appearing and disappearing, as would occur in reality. In this paper,\nwe present a new training strategy for deep learning SSL models with a\nstraightforward implementation based on the mean squared error of the optimal\nassociation between estimated and reference positions in the preceding time\nframes. It optimizes the desired properties of a tracking system: handling a\ntime-varying number of sources and ordering localization estimates according to\ntheir trajectories, minimizing identity switches (IDSs). Evaluation on\nsimulated data of multiple reverberant moving sources and on two model\narchitectures proves its effectiveness on reducing identity switches without\ncompromising frame-wise localization accuracy.", "published": "2022-10-26 07:54:47", "link": "http://arxiv.org/abs/2210.14536v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "AdaMS: Deep Metric Learning with Adaptive Margin and Adaptive Scale for\n  Acoustic Word Discrimination", "abstract": "Many recent loss functions in deep metric learning are expressed with\nlogarithmic and exponential forms, and they involve margin and scale as\nessential hyper-parameters. Since each data class has an intrinsic\ncharacteristic, several previous works have tried to learn embedding space\nclose to the real distribution by introducing adaptive margins. However, there\nwas no work on adaptive scales at all. We argue that both margin and scale\nshould be adaptively adjustable during the training. In this paper, we propose\na method called Adaptive Margin and Scale (AdaMS), where hyper-parameters of\nmargin and scale are replaced with learnable parameters of adaptive margins and\nadaptive scales for each class. Our method is evaluated on Wall Street Journal\ndataset, and we achieve outperforming results for word discrimination tasks.", "published": "2022-10-26 08:53:31", "link": "http://arxiv.org/abs/2210.14564v2", "categories": ["eess.AS", "cs.IR", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Learning Based Audio-Visual Multi-Speaker DOA Estimation Using\n  Permutation-Free Loss Function", "abstract": "In this paper, we propose a deep learning based multi-speaker direction of\narrival (DOA) estimation with audio and visual signals by using\npermutation-free loss function. We first collect a data set for multi-modal\nsound source localization (SSL) where both audio and visual signals are\nrecorded in real-life home TV scenarios. Then we propose a novel spatial\nannotation method to produce the ground truth of DOA for each speaker with the\nvideo data by transformation between camera coordinate and pixel coordinate\naccording to the pin-hole camera model. With spatial location information\nserved as another input along with acoustic feature, multi-speaker DOA\nestimation could be solved as a classification task of active speaker\ndetection. Label permutation problem in multi-speaker related tasks will be\naddressed since the locations of each speaker are used as input. Experiments\nconducted on both simulated data and real data show that the proposed\naudio-visual DOA estimation model outperforms audio-only DOA estimation model\nby a large margin.", "published": "2022-10-26 09:33:10", "link": "http://arxiv.org/abs/2210.14581v1", "categories": ["eess.AS", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Efficient Data Mosaicing with Simulation-based Inference", "abstract": "We introduce an efficient algorithm for general data mosaicing, based on the\nsimulation-based inference paradigm. Our algorithm takes as input a target\ndatum, source data, and partitions of the target and source data into\nfragments, learning distributions over averages of fragments of the source data\nsuch that samples from those distributions approximate fragments of the target\ndatum. We utilize a model that can be trivially parallelized in conjunction\nwith the latest advances in efficient simulation-based inference in order to\nfind approximate posteriors fast enough for use in practical applications. We\ndemonstrate our technique is effective in both audio and image mosaicing\nproblems.", "published": "2022-10-26 10:20:50", "link": "http://arxiv.org/abs/2210.14602v2", "categories": ["cs.SD", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "Masked Modeling Duo: Learning Representations by Encouraging Both\n  Networks to Model the Input", "abstract": "Masked Autoencoders is a simple yet powerful self-supervised learning method.\nHowever, it learns representations indirectly by reconstructing masked input\npatches. Several methods learn representations directly by predicting\nrepresentations of masked patches; however, we think using all patches to\nencode training signal representations is suboptimal. We propose a new method,\nMasked Modeling Duo (M2D), that learns representations directly while obtaining\ntraining signals using only masked patches. In the M2D, the online network\nencodes visible patches and predicts masked patch representations, and the\ntarget network, a momentum encoder, encodes masked patches. To better predict\ntarget representations, the online network should model the input well, while\nthe target network should also model it well to agree with online predictions.\nThen the learned representations should better model the input. We validated\nthe M2D by learning general-purpose audio representations, and M2D set new\nstate-of-the-art performance on tasks such as UrbanSound8K, VoxCeleb1,\nAudioSet20K, GTZAN, and SpeechCommandsV2. We additionally validate the\neffectiveness of M2D for images using ImageNet-1K in the appendix.", "published": "2022-10-26 11:49:30", "link": "http://arxiv.org/abs/2210.14648v3", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD", "68T07"], "primary_category": "eess.AS"}
{"title": "TSUP Speaker Diarization System for Conversational Short-phrase Speaker\n  Diarization Challenge", "abstract": "This paper describes the TSUP team's submission to the ISCSLP 2022\nconversational short-phrase speaker diarization (CSSD) challenge which\nparticularly focuses on short-phrase conversations with a new evaluation metric\ncalled conversational diarization error rate (CDER). In this challenge, we\nexplore three kinds of typical speaker diarization systems, which are spectral\nclustering(SC) based diarization, target-speaker voice activity\ndetection(TS-VAD) and end-to-end neural diarization(EEND) respectively. Our\nmajor findings are summarized as follows. First, the SC approach is more\nfavored over the other two approaches under the new CDER metric. Second, tuning\non hyperparameters is essential to CDER for all three types of speaker\ndiarization systems. Specifically, CDER becomes smaller when the length of\nsub-segments setting longer. Finally, multi-system fusion through DOVER-LAP\nwill worsen the CDER metric on the challenge data. Our submitted SC system\neventually ranks the third place in the challenge.", "published": "2022-10-26 12:01:24", "link": "http://arxiv.org/abs/2210.14653v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Full-band General Audio Synthesis with Score-based Diffusion", "abstract": "Recent works have shown the capability of deep generative models to tackle\ngeneral audio synthesis from a single label, producing a variety of impulsive,\ntonal, and environmental sounds. Such models operate on band-limited signals\nand, as a result of an autoregressive approach, they are typically conformed by\npre-trained latent encoders and/or several cascaded modules. In this work, we\npropose a diffusion-based generative model for general audio synthesis, named\nDAG, which deals with full-band signals end-to-end in the waveform domain.\nResults show the superiority of DAG over existing label-conditioned generators\nin terms of both quality and diversity. More specifically, when compared to the\nstate of the art, the band-limited and full-band versions of DAG achieve\nrelative improvements that go up to 40 and 65%, respectively. We believe DAG is\nflexible enough to accommodate different conditioning schemas while providing\ngood quality synthesis.", "published": "2022-10-26 12:25:57", "link": "http://arxiv.org/abs/2210.14661v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "In search of strong embedding extractors for speaker diarisation", "abstract": "Speaker embedding extractors (EEs), which map input audio to a speaker\ndiscriminant latent space, are of paramount importance in speaker diarisation.\nHowever, there are several challenges when adopting EEs for diarisation, from\nwhich we tackle two key problems. First, the evaluation is not straightforward\nbecause the features required for better performance differ between speaker\nverification and diarisation. We show that better performance on widely adopted\nspeaker verification evaluation protocols does not lead to better diarisation\nperformance. Second, embedding extractors have not seen utterances in which\nmultiple speakers exist. These inputs are inevitably present in speaker\ndiarisation because of overlapped speech and speaker changes; they degrade the\nperformance. To mitigate the first problem, we generate speaker verification\nevaluation protocols that mimic the diarisation scenario better. We propose two\ndata augmentation techniques to alleviate the second problem, making embedding\nextractors aware of overlapped speech or speaker change input. One technique\ngenerates overlapped speech segments, and the other generates segments where\ntwo speakers utter sequentially. Extensive experimental results using three\nstate-of-the-art speaker embedding extractors demonstrate that both proposed\napproaches are effective.", "published": "2022-10-26 13:00:29", "link": "http://arxiv.org/abs/2210.14682v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pretrained audio neural networks for Speech emotion recognition in\n  Portuguese", "abstract": "The goal of speech emotion recognition (SER) is to identify the emotional\naspects of speech. The SER challenge for Brazilian Portuguese speech was\nproposed with short snippets of Portuguese which are classified as neutral,\nnon-neutral female and non-neutral male according to paralinguistic elements\n(laughing, crying, etc). This dataset contains about $50$ minutes of Brazilian\nPortuguese speech. As the dataset leans on the small side, we investigate\nwhether a combination of transfer learning and data augmentation techniques can\nproduce positive results. Thus, by combining a data augmentation technique\ncalled SpecAugment, with the use of Pretrained Audio Neural Networks (PANNs)\nfor transfer learning we are able to obtain interesting results. The PANNs\n(CNN6, CNN10 and CNN14) are pretrained on a large dataset called AudioSet\ncontaining more than $5000$ hours of audio. They were finetuned on the SER\ndataset and the best performing model (CNN10) on the validation set was\nsubmitted to the challenge, achieving an $F1$ score of $0.73$ up from $0.54$\nfrom the baselines provided by the challenge. Moreover, we also tested the use\nof Transformer neural architecture, pretrained on about $600$ hours of\nBrazilian Portuguese audio data. Transformers, as well as more complex models\nof PANNs (CNN14), fail to generalize to the test set in the SER dataset and do\nnot beat the baseline. Considering the limitation of the dataset sizes,\ncurrently the best approach for SER is using PANNs (specifically, CNN6 and\nCNN10).", "published": "2022-10-26 13:48:51", "link": "http://arxiv.org/abs/2210.14716v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Naturalistic Head Motion Generation from Speech", "abstract": "Synthesizing natural head motion to accompany speech for an embodied\nconversational agent is necessary for providing a rich interactive experience.\nMost prior works assess the quality of generated head motion by comparing them\nagainst a single ground-truth using an objective metric. Yet there are many\nplausible head motion sequences to accompany a speech utterance. In this work,\nwe study the variation in the perceptual quality of head motions sampled from a\ngenerative model. We show that, despite providing more diverse head motions,\nthe generative model produces motions with varying degrees of perceptual\nquality. We finally show that objective metrics commonly used in previous\nresearch do not accurately reflect the perceptual quality of generated head\nmotions. These results open an interesting avenue for future work to\ninvestigate better objective metrics that correlate with human perception of\nquality.", "published": "2022-10-26 15:49:38", "link": "http://arxiv.org/abs/2210.14800v1", "categories": ["eess.AS", "cs.HC", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Knowledge Transfer For On-Device Speech Emotion Recognition with Neural\n  Structured Learning", "abstract": "Speech emotion recognition (SER) has been a popular research topic in\nhuman-computer interaction (HCI). As edge devices are rapidly springing up,\napplying SER to edge devices is promising for a huge number of HCI\napplications. Although deep learning has been investigated to improve the\nperformance of SER by training complex models, the memory space and\ncomputational capability of edge devices represents a constraint for embedding\ndeep learning models. We propose a neural structured learning (NSL) framework\nthrough building synthesized graphs. An SER model is trained on a source\ndataset and used to build graphs on a target dataset. A relatively lightweight\nmodel is then trained with the speech samples and graphs together as the input.\nOur experiments demonstrate that training a lightweight SER model on the target\ndataset with speech samples and graphs can not only produce small SER models,\nbut also enhance the model performance compared to models with speech samples\nonly and those using classic transfer learning strategies.", "published": "2022-10-26 18:38:42", "link": "http://arxiv.org/abs/2210.14977v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Privacy-preserving Automatic Speaker Diarization", "abstract": "Automatic Speaker Diarization (ASD) is an enabling technology with numerous\napplications, which deals with recordings of multiple speakers, raising special\nconcerns in terms of privacy. In fact, in remote settings, where recordings are\nshared with a server, clients relinquish not only the privacy of their\nconversation, but also of all the information that can be inferred from their\nvoices. However, to the best of our knowledge, the development of\nprivacy-preserving ASD systems has been overlooked thus far. In this work, we\ntackle this problem using a combination of two cryptographic techniques, Secure\nMultiparty Computation (SMC) and Secure Modular Hashing, and apply them to the\ntwo main steps of a cascaded ASD system: speaker embedding extraction and\nagglomerative hierarchical clustering. Our system is able to achieve a\nreasonable trade-off between performance and efficiency, presenting real-time\nfactors of 1.1 and 1.6, for two different SMC security settings.", "published": "2022-10-26 19:34:47", "link": "http://arxiv.org/abs/2210.14995v2", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "HEiMDaL: Highly Efficient Method for Detection and Localization of\n  wake-words", "abstract": "Streaming keyword spotting is a widely used solution for activating voice\nassistants. Deep Neural Networks with Hidden Markov Model (DNN-HMM) based\nmethods have proven to be efficient and widely adopted in this space, primarily\nbecause of the ability to detect and identify the start and end of the wake-up\nword at low compute cost. However, such hybrid systems suffer from loss metric\nmismatch when the DNN and HMM are trained independently. Sequence\ndiscriminative training cannot fully mitigate the loss-metric mismatch due to\nthe inherent Markovian style of the operation. We propose an low footprint CNN\nmodel, called HEiMDaL, to detect and localize keywords in streaming conditions.\nWe introduce an alignment-based classification loss to detect the occurrence of\nthe keyword along with an offset loss to predict the start of the keyword.\nHEiMDaL shows 73% reduction in detection metrics along with equivalent\nlocalization accuracy and with the same memory footprint as existing DNN-HMM\nstyle models for a given wake-word.", "published": "2022-10-26 17:26:57", "link": "http://arxiv.org/abs/2210.15425v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
