{"title": "Embedded Translations for Low-resource Automated Glossing", "abstract": "We investigate automatic interlinear glossing in low-resource settings. We\naugment a hard-attentional neural model with embedded translation information\nextracted from interlinear glossed text. After encoding these translations\nusing large language models, specifically BERT and T5, we introduce a\ncharacter-level decoder for generating glossed output. Aided by these\nenhancements, our model demonstrates an average improvement of 3.97\\%-points\nover the previous state of the art on datasets from the SIGMORPHON 2023 Shared\nTask on Interlinear Glossing. In a simulated ultra low-resource setting,\ntrained on as few as 100 sentences, our system achieves an average 9.78\\%-point\nimprovement over the plain hard-attentional baseline. These results highlight\nthe critical role of translation information in boosting the system's\nperformance, especially in processing and interpreting modest data sources. Our\nfindings suggest a promising avenue for the documentation and preservation of\nlanguages, with our experiments on shared task datasets indicating significant\nadvancements over the existing state of the art.", "published": "2024-03-13 02:23:13", "link": "http://arxiv.org/abs/2403.08189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Validating and Exploring Large Geographic Corpora", "abstract": "This paper investigates the impact of corpus creation decisions on large\nmulti-lingual geographic web corpora. Beginning with a 427 billion word corpus\nderived from the Common Crawl, three methods are used to improve the quality of\nsub-corpora representing specific language-country pairs like New Zealand\nEnglish: (i) the agreement of independent language identification systems, (ii)\nhash-based deduplication, and (iii) location-specific outlier detection. The\nimpact of each of these steps is then evaluated at the language level and the\ncountry level by using corpus similarity measures to compare each resulting\ncorpus with baseline data sets. The goal is to understand the impact of\nupstream data cleaning decisions on downstream corpora with a specific focus on\nunder-represented languages and populations. The evaluation shows that the\nvalidity of sub-corpora is improved with each stage of cleaning but that this\nimprovement is unevenly distributed across languages and populations. This\nresult shows how standard corpus creation techniques can accidentally exclude\nunder-represented populations.", "published": "2024-03-13 02:46:17", "link": "http://arxiv.org/abs/2403.08198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Identify Authorship?", "abstract": "The ability to accurately identify authorship is crucial for verifying\ncontent authenticity and mitigating misinformation. Large Language Models\n(LLMs) have demonstrated an exceptional capacity for reasoning and\nproblem-solving. However, their potential in authorship analysis remains\nunder-explored. Traditional studies have depended on hand-crafted stylistic\nfeatures, whereas state-of-the-art approaches leverage text embeddings from\npre-trained language models. These methods, which typically require fine-tuning\non labeled data, often suffer from performance degradation in cross-domain\napplications and provide limited explainability. This work seeks to address\nthree research questions: (1) Can LLMs perform zero-shot, end-to-end authorship\nverification effectively? (2) Are LLMs capable of accurately attributing\nauthorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs\nprovide explainability in authorship analysis, particularly through the role of\nlinguistic features? Moreover, we investigate the integration of explicit\nlinguistic features to guide LLMs in their reasoning processes. Our assessment\ndemonstrates LLMs' proficiency in both tasks without the need for\ndomain-specific fine-tuning, providing explanations into their decision making\nvia a detailed analysis of linguistic features. This establishes a new\nbenchmark for future research on LLM-based authorship analysis.", "published": "2024-03-13 03:22:02", "link": "http://arxiv.org/abs/2403.08213v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Disfluency Detection with Large Language Model as Disfluency\n  Generator", "abstract": "Current disfluency detection methods heavily rely on costly and scarce\nhuman-annotated data. To tackle this issue, some approaches employ heuristic or\nstatistical features to generate disfluent sentences, partially improving\ndetection performance. However, these sentences often deviate from real-life\nscenarios, constraining overall model enhancement. In this study, we propose a\nlightweight data augmentation approach for disfluency detection, utilizing the\nsuperior generative and semantic understanding capabilities of large language\nmodel (LLM) to generate disfluent sentences as augmentation data. We leverage\nLLM to generate diverse and more realistic sentences guided by specific\nprompts, without the need for fine-tuning the LLM. Subsequently, we apply an\nuncertainty-aware data filtering approach to improve the quality of the\ngenerated sentences, utilized in training a small detection model for improved\nperformance. Experiments using enhanced data yielded state-of-the-art results.\nThe results showed that using a small amount of LLM-generated enhanced data can\nsignificantly improve performance, thereby further enhancing\ncost-effectiveness. Our code is available here.", "published": "2024-03-13 04:14:33", "link": "http://arxiv.org/abs/2403.08229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education", "abstract": "The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale and real-world interactions between students and AI\nsystems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE\nfor University), a dataset sourced from a semester-long experiment with 212\ncollege students in English as Foreign Language (EFL) writing courses. During\nthe study, students engaged in dialogues with ChatGPT to revise their essays.\nRECIPE4U includes comprehensive records of these interactions, including\nconversation logs, students' intent, students' self-rated satisfaction, and\nstudents' essay edit histories. In particular, we annotate the students'\nutterances in RECIPE4U with 13 intention labels based on our coding schemes. We\nestablish baseline results for two subtasks in task-oriented dialogue systems\nwithin educational contexts: intent detection and satisfaction estimation. As a\nfoundational step, we explore student-ChatGPT interaction patterns through\nRECIPE4U and analyze them by focusing on students' dialogue, essay data\nstatistics, and students' essay edits. We further illustrate potential\napplications of RECIPE4U dataset for enhancing the incorporation of LLMs in\neducational frameworks. RECIPE4U is publicly available at\nhttps://zeunie.github.io/RECIPE4U/.", "published": "2024-03-13 05:51:57", "link": "http://arxiv.org/abs/2403.08272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Personalized Evaluation of Large Language Models with An\n  Anonymous Crowd-Sourcing Platform", "abstract": "Large language model evaluation plays a pivotal role in the enhancement of\nits capacity. Previously, numerous methods for evaluating large language models\nhave been proposed in this area. Despite their effectiveness, these existing\nworks mainly focus on assessing objective questions, overlooking the capability\nto evaluate subjective questions which is extremely common for large language\nmodels. Additionally, these methods predominantly utilize centralized datasets\nfor evaluation, with question banks concentrated within the evaluation\nplatforms themselves. Moreover, the evaluation processes employed by these\nplatforms often overlook personalized factors, neglecting to consider the\nindividual characteristics of both the evaluators and the models being\nevaluated. To address these limitations, we propose a novel anonymous\ncrowd-sourcing evaluation platform, BingJian, for large language models that\nemploys a competitive scoring mechanism where users participate in ranking\nmodels based on their performance. This platform stands out not only for its\nsupport of centralized evaluations to assess the general capabilities of models\nbut also for offering an open evaluation gateway. Through this gateway, users\nhave the opportunity to submit their questions, testing the models on a\npersonalized and potentially broader range of capabilities. Furthermore, our\nplatform introduces personalized evaluation scenarios, leveraging various forms\nof human-computer interaction to assess large language models in a manner that\naccounts for individual user preferences and contexts. The demonstration of\nBingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.", "published": "2024-03-13 07:31:20", "link": "http://arxiv.org/abs/2403.08305v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Context Helpful for Chat Translation Evaluation?", "abstract": "Despite the recent success of automatic metrics for assessing translation\nquality, their application in evaluating the quality of machine-translated\nchats has been limited. Unlike more structured texts like news, chat\nconversations are often unstructured, short, and heavily reliant on contextual\ninformation. This poses questions about the reliability of existing\nsentence-level metrics in this domain as well as the role of context in\nassessing the translation quality. Motivated by this, we conduct a\nmeta-evaluation of existing sentence-level automatic metrics, primarily\ndesigned for structured domains such as news, to assess the quality of\nmachine-translated chats. We find that reference-free metrics lag behind\nreference-based ones, especially when evaluating translation quality in\nout-of-English settings. We then investigate how incorporating conversational\ncontextual information in these metrics affects their performance. Our findings\nshow that augmenting neural learned metrics with contextual information helps\nimprove correlation with human judgments in the reference-free scenario and\nwhen evaluating translations in out-of-English settings. Finally, we propose a\nnew evaluation metric, Context-MQM, that utilizes bilingual context with a\nlarge language model (LLM) and further validate that adding context helps even\nfor LLM-based evaluation metrics.", "published": "2024-03-13 07:49:50", "link": "http://arxiv.org/abs/2403.08314v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From human experts to machines: An LLM supported approach to ontology\n  and knowledge graph construction", "abstract": "The conventional process of building Ontologies and Knowledge Graphs (KGs)\nheavily relies on human domain experts to define entities and relationship\ntypes, establish hierarchies, maintain relevance to the domain, fill the ABox\n(or populate with instances), and ensure data quality (including amongst others\naccuracy and completeness). On the other hand, Large Language Models (LLMs)\nhave recently gained popularity for their ability to understand and generate\nhuman-like natural language, offering promising ways to automate aspects of\nthis process. This work explores the (semi-)automatic construction of KGs\nfacilitated by open-source LLMs. Our pipeline involves formulating competency\nquestions (CQs), developing an ontology (TBox) based on these CQs, constructing\nKGs using the developed ontology, and evaluating the resultant KG with minimal\nto no involvement of human experts. We showcase the feasibility of our\nsemi-automated pipeline by creating a KG on deep learning methodologies by\nexploiting scholarly publications. To evaluate the answers generated via\nRetrieval-Augmented-Generation (RAG) as well as the KG concepts automatically\nextracted using LLMs, we design a judge LLM, which rates the generated content\nbased on ground truth. Our findings suggest that employing LLMs could\npotentially reduce the human effort involved in the construction of KGs,\nalthough a human-in-the-loop approach is recommended to evaluate automatically\ngenerated KGs.", "published": "2024-03-13 08:50:15", "link": "http://arxiv.org/abs/2403.08345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Describe for Predicting Zero-shot Drug-Drug Interactions", "abstract": "Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of\nconcurrent drug administration, posing a significant challenge in healthcare.\nAs the development of new drugs continues, the potential for unknown adverse\neffects resulting from DDIs becomes a growing concern. Traditional\ncomputational methods for DDI prediction may fail to capture interactions for\nnew drugs due to the lack of knowledge. In this paper, we introduce a new\nproblem setup as zero-shot DDI prediction that deals with the case of new\ndrugs. Leveraging textual information from online databases like DrugBank and\nPubChem, we propose an innovative approach TextDDI with a language model-based\nDDI predictor and a reinforcement learning~(RL)-based information selector,\nenabling the selection of concise and pertinent text for accurate DDI\nprediction on new drugs. Empirical results show the benefits of the proposed\napproach on several settings including zero-shot and few-shot DDI prediction,\nand the selected texts are semantically relevant. Our code and data are\navailable at \\url{https://github.com/zhufq00/DDIs-Prediction}.", "published": "2024-03-13 09:42:46", "link": "http://arxiv.org/abs/2403.08377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Misinformation is not about Bad Facts: An Analysis of the Production and\n  Consumption of Fringe Content", "abstract": "What if misinformation is not an information problem at all? To understand\nthe role of news publishers in potentially unintentionally propagating\nmisinformation, we examine how far-right and fringe online groups share and\nleverage established legacy news media articles to advance their narratives.\nOur findings suggest that online fringe ideologies spread through the use of\ncontent that is consensus-based and \"factually correct\". We found that\nAustralian news publishers with both moderate and far-right political leanings\ncontain comparable levels of information completeness and quality; and\nfurthermore, that far-right Twitter users often share from moderate sources.\nHowever, a stark difference emerges when we consider two additional factors: 1)\nthe narrow topic selection of articles by far-right users, suggesting that they\ncherry pick only news articles that engage with their preexisting worldviews\nand specific topics of concern, and 2) the difference between moderate and\nfar-right publishers when we examine the writing style of their articles.\nFurthermore, we can identify users prone to sharing misinformation based on\ntheir communication style. These findings have important implications for\ncountering online misinformation, as they highlight the powerful role that\npersonal biases towards specific topics and publishers' writing styles have in\namplifying fringe ideologies online.", "published": "2024-03-13 10:10:07", "link": "http://arxiv.org/abs/2403.08391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Targeted Efficient Fine-tuning: Optimizing Parameter Updates with\n  Data-Driven Sample Selection", "abstract": "Fine-tuning all parameters of Large Language Models (LLMs) is computationally\nexpensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by\nselectively fine-tuning specific parameters. Most of the parameter efficient\nfine-tuning (PEFT) methods center on selecting or introducing a set of\nparameters to be fine-tuned. However, there are few methods that consider the\nimpact of data samples on parameter selecting. Representative data driven\nmethods include FISH Mask based method, which randomly selects a portion of\ndata samples as a basis when selecting parameters. However, this random data\nsample selection method cannot select optimal parameters for unstable data\ndistribution. In this work, we introduce a data-centric approach and propose\nthe Iterative Range Decreasing (IRD) algorithm to optimize the sample-parameter\npair selection in FISH Mask. IRD iteratively refines the selection by\nidentifying subsets of samples and parameters exhibiting higher Fisher\ninformation. We demonstrate the effectiveness and rationality of proposed\nstrategy by conducting experiments on GLUE benchmark. Experimental results show\nour strategy optimizes the parameter selection and achieves preferable\nperformance over some typical baseline methods.", "published": "2024-03-13 12:50:23", "link": "http://arxiv.org/abs/2403.08484v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking", "abstract": "Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework.", "published": "2024-03-13 12:55:43", "link": "http://arxiv.org/abs/2403.08492v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Interactive Evaluation for Large Language Models with State\n  Aware Patient Simulator", "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.", "published": "2024-03-13 13:04:58", "link": "http://arxiv.org/abs/2403.08495v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Models Care About Text Quality? Evaluating Web-Crawled\n  Corpora Across 11 Languages", "abstract": "Large, curated, web-crawled corpora play a vital role in training language\nmodels (LMs). They form the lion's share of the training data in virtually all\nrecent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However,\ndespite this importance, relatively little attention has been given to the\nquality of these corpora. In this paper, we compare four of the currently most\nrelevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across\neleven lower-resourced European languages. Our approach is two-fold: first, we\nperform an intrinsic evaluation by performing a human evaluation of the quality\nof samples taken from different corpora; then, we assess the practical impact\nof the qualitative differences by training specific LMs on each of the corpora\nand evaluating their performance on downstream tasks. We find that there are\nclear differences in quality of the corpora, with MaCoCu and OSCAR obtaining\nthe best results. However, during the extrinsic evaluation, we actually find\nthat the CC100 corpus achieves the highest scores. We conclude that, in our\nexperiments, the quality of the web-crawled corpora does not seem to play a\nsignificant role when training LMs.", "published": "2024-03-13 16:56:33", "link": "http://arxiv.org/abs/2403.08693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning", "abstract": "The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n5.73% of the strong baseline's total), along with enhanced capabilities of LLMs\nin crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL", "published": "2024-03-13 16:57:57", "link": "http://arxiv.org/abs/2403.08694v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SOTOPIA-$\u03c0$: Interactive Learning of Socially Intelligent Language\n  Agents", "abstract": "Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.", "published": "2024-03-13 17:17:48", "link": "http://arxiv.org/abs/2403.08715v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Hallucination and Coverage Errors in Retrieval Augmented\n  Generation for Controversial Topics", "abstract": "We explore a strategy to handle controversial topics in LLM-based chatbots\nbased on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the\nabsence of a single true answer and surface multiple perspectives. We frame\nthis as retrieval augmented generation, where perspectives are retrieved from a\nknowledge base and the LLM is tasked with generating a fluent and faithful\nresponse from the given perspectives. As a starting point, we use a\ndeterministic retrieval system and then focus on common LLM failure modes that\narise during this approach to text generation, namely hallucination and\ncoverage errors. We propose and evaluate three methods to detect such errors\nbased on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our\nresults demonstrate that LLM-based classifiers, even when trained only on\nsynthetic errors, achieve high error detection performance, with ROC AUC scores\nof 95.3% for hallucination and 90.5% for coverage error detection on\nunambiguous error cases. We show that when no training data is available, our\nother methods still yield good results on hallucination (84.0%) and coverage\nerror (85.2%) detection.", "published": "2024-03-13 18:47:00", "link": "http://arxiv.org/abs/2403.08904v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots", "abstract": "Since the breakthrough of ChatGPT, large language models (LLMs) have garnered\nsignificant attention in the research community. With the development of LLMs,\nthe question of text style transfer for conversational models has emerged as a\nnatural extension, where chatbots may possess their own styles or even\ncharacters. However, standard evaluation metrics have not yet been established\nfor this new settings. This paper aims to address this issue by proposing the\nLMStyle Benchmark, a novel evaluation framework applicable to chat-style text\nstyle transfer (C-TST), that can measure the quality of style transfer for LLMs\nin an automated and scalable manner. In addition to conventional style strength\nmetrics, LMStyle Benchmark further considers a novel aspect of metrics called\nappropriateness, a high-level metrics take account of coherence, fluency and\nother implicit factors without the aid of reference samples. Our experiments\ndemonstrate that the new evaluation methods introduced by LMStyle Benchmark\nhave a higher correlation with human judgments in terms of appropriateness.\nBased on LMStyle Benchmark, we present a comprehensive list of evaluation\nresults for popular LLMs, including LLaMA, Alpaca, and Vicuna, reflecting their\nstylistic properties, such as formality and sentiment strength, along with\ntheir appropriateness.", "published": "2024-03-13 20:19:30", "link": "http://arxiv.org/abs/2403.08943v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ethos: Rectifying Language Models in Orthogonal Parameter Space", "abstract": "Language models (LMs) have greatly propelled the research on natural language\nprocessing. However, LMs also raise concerns regarding the generation of biased\nor toxic content and the potential disclosure of private information from the\ntraining dataset. In this work, we present a new efficient approach, Ethos,\nthat rectifies LMs to mitigate toxicity and bias in outputs and avoid privacy\nleakage. Ethos is built on task arithmetic. However, unlike current task\narithmetic algorithms, Ethos distinguishes general beneficial and undesired\nknowledge when reconstructing task vectors. Specifically, Ethos first obtains a\nset of principal components from the pre-trained models using singular value\ndecomposition. Then, by projecting the task vector onto principal components,\nEthos identifies the principal components that encode general or undesired\nknowledge. Ethos performs negating using the task vector with undesired\nknowledge only, thereby minimizing collateral damage on general model utility.\nWe demonstrate the efficacy of our approach on three different tasks:\ndebiasing, detoxification, and memorization unlearning. Evaluations show Ethos\nis more effective in removing undesired knowledge and maintaining the overall\nmodel performance compared to current task arithmetic methods.", "published": "2024-03-13 23:25:30", "link": "http://arxiv.org/abs/2403.08994v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distilling Named Entity Recognition Models for Endangered Species from\n  Large Language Models", "abstract": "Natural language processing (NLP) practitioners are leveraging large language\nmodels (LLM) to create structured datasets from semi-structured and\nunstructured data sources such as patents, papers, and theses, without having\ndomain-specific knowledge. At the same time, ecological experts are searching\nfor a variety of means to preserve biodiversity. To contribute to these\nefforts, we focused on endangered species and through in-context learning, we\ndistilled knowledge from GPT-4. In effect, we created datasets for both named\nentity recognition (NER) and relation extraction (RE) via a two-stage process:\n1) we generated synthetic data from GPT-4 of four classes of endangered\nspecies, 2) humans verified the factual accuracy of the synthetic data,\nresulting in gold data. Eventually, our novel dataset contains a total of 3.6K\nsentences, evenly divided between 1.8K NER and 1.8K RE sentences. The\nconstructed dataset was then used to fine-tune both general BERT and\ndomain-specific BERT variants, completing the knowledge distillation process\nfrom GPT-4 to BERT, because GPT-4 is resource intensive. Experiments show that\nour knowledge transfer approach is effective at creating a NER model suitable\nfor detecting endangered species from texts.", "published": "2024-03-13 15:38:55", "link": "http://arxiv.org/abs/2403.15430v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Loss Functions for Fact Verification", "abstract": "We explore loss functions for fact verification in the FEVER shared task.\nWhile the cross-entropy loss is a standard objective for training verdict\npredictors, it fails to capture the heterogeneity among the FEVER verdict\nclasses. In this paper, we develop two task-specific objectives tailored to\nFEVER. Experimental results confirm that the proposed objective functions\noutperform the standard cross-entropy. Performance is further improved when\nthese objectives are combined with simple class weighting, which effectively\novercomes the imbalance in the training data. The souce code is available at\nhttps://github.com/yuta-mukobara/RLF-KGAT", "published": "2024-03-13 01:56:32", "link": "http://arxiv.org/abs/2403.08174v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular\n  Comprehension", "abstract": "Large language models are playing an increasingly significant role in\nmolecular research, yet existing models often generate erroneous information,\nposing challenges to accurate molecular comprehension. Traditional evaluation\nmetrics for generated content fail to assess a model's accuracy in molecular\nunderstanding. To rectify the absence of factual evaluation, we present\nMoleculeQA, a novel question answering (QA) dataset which possesses 62K QA\npairs over 23K molecules. Each QA pair, composed of a manual question, a\npositive option and three negative options, has consistent semantics with a\nmolecular description from authoritative molecular corpus. MoleculeQA is not\nonly the first benchmark for molecular factual bias evaluation but also the\nlargest QA dataset for molecular research. A comprehensive evaluation on\nMoleculeQA for existing molecular LLMs exposes their deficiencies in specific\nareas and pinpoints several particularly crucial factors for molecular\nunderstanding.", "published": "2024-03-13 02:26:16", "link": "http://arxiv.org/abs/2403.08192v1", "categories": ["cs.CL", "q-bio.BM"], "primary_category": "cs.CL"}
{"title": "SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech\n  Recognition Evaluation", "abstract": "In the wake of the surging tide of deep learning over the past decade,\nAutomatic Speech Recognition (ASR) has garnered substantial attention, leading\nto the emergence of numerous publicly accessible ASR systems that are actively\nbeing integrated into our daily lives. Nonetheless, the impartial and\nreplicable evaluation of these ASR systems encounters challenges due to various\ncrucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a\ngeneral-purpose, open-source platform designed for ASR evaluation. With this\nplatform: (i) We report a comprehensive benchmark, unveiling the current\nstate-of-the-art panorama for ASR systems, covering both open-source models and\nindustrial commercial services. (ii) We quantize how distinct nuances in the\nscoring pipeline influence the final benchmark outcomes. These include nuances\nrelated to capitalization, punctuation, interjection, contraction, synonym\nusage, compound words, etc. These issues have gained prominence in the context\nof the transition towards an End-to-End future. (iii) We propose a practical\nmodification to the conventional Token-Error-Rate (TER) evaluation metric, with\ninspirations from Kolmogorov complexity and Normalized Information Distance\n(NID). This adaptation, called modified-TER (mTER), achieves proper\nnormalization and symmetrical treatment of reference and hypothesis. By\nleveraging this platform as a large-scale testing ground, this study\ndemonstrates the robustness and backward compatibility of mTER when compared to\nTER. The SpeechColab Leaderboard is accessible at\nhttps://github.com/SpeechColab/Leaderboard", "published": "2024-03-13 02:41:53", "link": "http://arxiv.org/abs/2403.08196v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Contrastive Reasoners", "abstract": "Prompting methods play a crucial role in enhancing the capabilities of\npre-trained large language models (LLMs). We explore how contrastive prompting\n(CP) significantly improves the ability of large language models to perform\ncomplex reasoning. We demonstrate that LLMs are decent contrastive reasoners by\nsimply adding \"Let's give a correct and a wrong answer.\" before LLMs provide\nanswers. Experiments on various large language models show that zero-shot\ncontrastive prompting improves the performance of standard zero-shot prompting\non a range of arithmetic, commonsense, and symbolic reasoning tasks without any\nhand-crafted few-shot examples, such as increasing the accuracy on GSM8K from\n35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4\nmodel. Our method not only surpasses zero-shot CoT and few-shot CoT in most\narithmetic and commonsense reasoning tasks but also can seamlessly integrate\nwith existing prompting methods, resulting in improved or comparable results\nwhen compared to state-of-the-art methods. Our code is available at\nhttps://github.com/yao8839836/cp", "published": "2024-03-13 03:15:05", "link": "http://arxiv.org/abs/2403.08211v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Research on the Application of Deep Learning-based BERT Model in\n  Sentiment Analysis", "abstract": "This paper explores the application of deep learning techniques, particularly\nfocusing on BERT models, in sentiment analysis. It begins by introducing the\nfundamental concept of sentiment analysis and how deep learning methods are\nutilized in this domain. Subsequently, it delves into the architecture and\ncharacteristics of BERT models. Through detailed explanation, it elucidates the\napplication effects and optimization strategies of BERT models in sentiment\nanalysis, supported by experimental validation. The experimental findings\nindicate that BERT models exhibit robust performance in sentiment analysis\ntasks, with notable enhancements post fine-tuning. Lastly, the paper concludes\nby summarizing the potential applications of BERT models in sentiment analysis\nand suggests directions for future research and practical implementations.", "published": "2024-03-13 03:31:26", "link": "http://arxiv.org/abs/2403.08217v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition", "abstract": "Conformer-based attention models have become the de facto backbone model for\nAutomatic Speech Recognition tasks. A blank symbol is usually introduced to\nalign the input and output sequences for CTC or RNN-T models. Unfortunately,\nthe long input length overloads computational budget and memory consumption\nquadratically by attention mechanism. In this work, we propose a\n\"Skip-and-Recover\" Conformer architecture, named Skipformer, to squeeze\nsequence input length dynamically and inhomogeneously. Skipformer uses an\nintermediate CTC output as criteria to split frames into three groups: crucial,\nskipping and ignoring. The crucial group feeds into next conformer blocks and\nits output joint with skipping group by original temporal order as the final\nencoder output. Experiments show that our model reduces the input sequence\nlength by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile,\nthe model can achieve better recognition accuracy and faster inference speed\nthan recent baseline models. Our code is open-sourced and available online.", "published": "2024-03-13 05:20:45", "link": "http://arxiv.org/abs/2403.08258v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mastering Text, Code and Math Simultaneously via Fusing Highly\n  Specialized Language Models", "abstract": "Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.", "published": "2024-03-13 06:18:48", "link": "http://arxiv.org/abs/2403.08281v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic\n  Language Models at Scale", "abstract": "A syntactic language model (SLM) incrementally generates a sentence with its\nsyntactic tree in a left-to-right manner. We present Generative Pretrained\nStructured Transformers (GPST), an unsupervised SLM at scale capable of being\npre-trained from scratch on raw texts with high parallelism. GPST circumvents\nthe limitations of previous SLMs such as relying on gold trees and sequential\ntraining. It consists of two components, a usual SLM supervised by a\nuni-directional language modeling loss, and an additional composition model,\nwhich induces syntactic parse trees and computes constituent representations,\nsupervised by a bi-directional language modeling loss. We propose a\nrepresentation surrogate to enable joint parallel training of the two models in\na hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion\ntokens, and demonstrate the superiority of GPST over GPT-2 with a comparable\nsize in numerous tasks covering both language understanding and language\ngeneration. Meanwhile, GPST also significantly outperforms existing\nunsupervised SLMs on left-to-right grammar induction, while holding a\nsubstantial acceleration on training.", "published": "2024-03-13 06:54:47", "link": "http://arxiv.org/abs/2403.08293v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gemma: Open Models Based on Gemini Research and Technology", "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.", "published": "2024-03-13 06:59:16", "link": "http://arxiv.org/abs/2403.08295v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context\n  Compression with Minimal Losses", "abstract": "Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200K or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200K of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation.", "published": "2024-03-13 07:44:14", "link": "http://arxiv.org/abs/2403.08312v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Autoregressive Score Generation for Multi-trait Essay Scoring", "abstract": "Recently, encoder-only pre-trained models such as BERT have been successfully\napplied in automated essay scoring (AES) to predict a single overall score.\nHowever, studies have yet to explore these models in multi-trait AES, possibly\ndue to the inefficiency of replicating BERT-based models for each trait.\nBreaking away from the existing sole use of encoder, we propose an\nautoregressive prediction of multi-trait scores (ArTS), incorporating a\ndecoding process by leveraging the pre-trained T5. Unlike prior regression or\nclassification methods, we redefine AES as a score-generation task, allowing a\nsingle model to predict multiple scores. During decoding, the subsequent trait\nprediction can benefit by conditioning on the preceding trait scores.\nExperimental results proved the efficacy of ArTS, showing over 5% average\nimprovements in both prompts and traits.", "published": "2024-03-13 08:34:53", "link": "http://arxiv.org/abs/2403.08332v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Log Summarisation for Defect Evolution Analysis", "abstract": "Log analysis and monitoring are essential aspects in software maintenance and\nidentifying defects. In particular, the temporal nature and vast size of log\ndata leads to an interesting and important research question: How can logs be\nsummarised and monitored over time? While this has been a fundamental topic of\nresearch in the software engineering community, work has typically focused on\nheuristic-, syntax-, or static-based methods. In this work, we suggest an\nonline semantic-based clustering approach to error logs that dynamically\nupdates the log clusters to enable monitoring code error life-cycles. We also\nintroduce a novel metric to evaluate the performance of temporal log clusters.\nWe test our system and evaluation metric with an industrial dataset and find\nthat our solution outperforms similar systems. We hope that our work encourages\nfurther temporal exploration in defect datasets.", "published": "2024-03-13 09:18:46", "link": "http://arxiv.org/abs/2403.08358v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Grammar as a Behavioral Biometric: Using Cognitively Motivated Grammar\n  Models for Authorship Verification", "abstract": "Authorship Verification (AV) is a key area of research in digital text\nforensics, which addresses the fundamental question of whether two texts were\nwritten by the same person. Numerous computational approaches have been\nproposed over the last two decades in an attempt to address this challenge.\nHowever, existing AV methods often suffer from high complexity, low\nexplainability and especially from a lack of clear scientific justification. We\npropose a simpler method based on modeling the grammar of an author following\nCognitive Linguistics principles. These models are used to calculate\n$\\lambda_G$ (LambdaG): the ratio of the likelihoods of a document given the\ncandidate's grammar versus given a reference population's grammar. Our\nempirical evaluation, conducted on twelve datasets and compared against seven\nbaseline methods, demonstrates that LambdaG achieves superior performance,\nincluding against several neural network-based AV methods. LambdaG is also\nrobust to small variations in the composition of the reference population and\nprovides interpretable visualizations, enhancing its explainability. We argue\nthat its effectiveness is due to the method's compatibility with Cognitive\nLinguistics theories predicting that a person's grammar is a behavioral\nbiometric.", "published": "2024-03-13 12:25:47", "link": "http://arxiv.org/abs/2403.08462v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language models scale reliably with over-training and on downstream\n  tasks", "abstract": "Scaling laws are useful guides for derisking expensive training runs, as they\npredict performance of large models using cheaper, small-scale experiments.\nHowever, there remain gaps between current scaling studies and how language\nmodels are ultimately trained and evaluated. For instance, scaling is usually\nstudied in the compute-optimal training regime (i.e., \"Chinchilla optimal\"\nregime). In contrast, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but models\nare usually compared on downstream task performance. To address both\nshortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters\ntrained with various numbers of tokens on three data distributions. First, we\nfit scaling laws that extrapolate in both the amount of over-training and the\nnumber of model parameters. This enables us to predict the validation loss of a\n1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B\nparameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each\nfrom experiments that take 300$\\times$ less compute. Second, we relate the\nperplexity of a language model to its downstream task performance by proposing\na power law. We use this law to predict top-1 error averaged over downstream\ntasks for the two aforementioned models, using experiments that take 20$\\times$\nless compute. Our experiments are available at\nhttps://github.com/mlfoundations/scaling.", "published": "2024-03-13 13:54:00", "link": "http://arxiv.org/abs/2403.08540v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over\n  Structured Environments", "abstract": "Large Language Models (LLMs) have shown potential in reasoning over\nstructured environments, e.g., knowledge graph and table. Such tasks typically\nrequire multi-hop reasoning, i.e., match natural language utterance with\ninstances in the environment. Previous methods leverage LLMs to incrementally\nbuild a reasoning path, where the LLMs either invoke tools or pick up schemas\nby step-by-step interacting with the environment. We propose\nReasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently\nand faithfully reason over structured environments. In Readi, LLMs initially\ngenerate a reasoning path given a query, and edit the path only when necessary.\nWe instantiate the path on structured environments and provide feedback to edit\nthe path if anything goes wrong. Experimental results on three KGQA and two\nTableQA datasets show the effectiveness of Readi, significantly surpassing\nprevious LLM-based methods (by 9.1% Hit@1 on WebQSP, 12.4% on MQA-3H and 9.5%\non WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and\n74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).\nOur code will be available on https://aka.ms/readi.", "published": "2024-03-13 14:59:07", "link": "http://arxiv.org/abs/2403.08593v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompting Large Language Models to Tackle the Full Software Development\n  Lifecycle: A Case Study", "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced their coding capabilities. However, existing benchmarks predominantly\nfocused on simplified or isolated aspects of coding, such as single-file code\ngeneration or repository issue debugging, falling short of measuring the full\nspectrum of challenges raised by real-world programming activities. In this\ncase study, we explore the performance of LLMs across the entire software\ndevelopment lifecycle with DevEval, encompassing stages including software\ndesign, environment setup, implementation, acceptance testing, and unit\ntesting. DevEval features four programming languages, multiple domains,\nhigh-quality data collection, and carefully designed and verified metrics for\neach task. Empirical studies show that current LLMs, including GPT-4, fail to\nsolve the challenges presented within DevEval. Our findings offer actionable\ninsights for the future development of LLMs toward real-world programming\napplications.", "published": "2024-03-13 15:13:44", "link": "http://arxiv.org/abs/2403.08604v3", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "MedInsight: A Multi-Source Context Augmentation Framework for Generating\n  Patient-Centric Medical Responses using Large Language Models", "abstract": "Large Language Models (LLMs) have shown impressive capabilities in generating\nhuman-like responses. However, their lack of domain-specific knowledge limits\ntheir applicability in healthcare settings, where contextual and comprehensive\nresponses are vital. To address this challenge and enable the generation of\npatient-centric responses that are contextually relevant and comprehensive, we\npropose MedInsight:a novel retrieval augmented framework that augments LLM\ninputs (prompts) with relevant background information from multiple sources.\nMedInsight extracts pertinent details from the patient's medical record or\nconsultation transcript. It then integrates information from authoritative\nmedical textbooks and curated web resources based on the patient's health\nhistory and condition. By constructing an augmented context combining the\npatient's record with relevant medical knowledge, MedInsight generates\nenriched, patient-specific responses tailored for healthcare applications such\nas diagnosis, treatment recommendations, or patient education. Experiments on\nthe MTSamples dataset validate MedInsight's effectiveness in generating\ncontextually appropriate medical responses. Quantitative evaluation using the\nRagas metric and TruLens for answer similarity and answer correctness\ndemonstrates the model's efficacy. Furthermore, human evaluation studies\ninvolving Subject Matter Expert (SMEs) confirm MedInsight's utility, with\nmoderate inter-rater agreement on the relevance and correctness of the\ngenerated responses.", "published": "2024-03-13 15:20:30", "link": "http://arxiv.org/abs/2403.08607v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Zero-shot and Few-shot Generation Strategies for Artificial Clinical\n  Records", "abstract": "The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.", "published": "2024-03-13 16:17:09", "link": "http://arxiv.org/abs/2403.08664v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Token Alignment via Character Matching for Subword Completion", "abstract": "Generative models, widely utilized in various applications, can often\nstruggle with prompts corresponding to partial tokens. This struggle stems from\ntokenization, where partial tokens fall out of distribution during inference,\nleading to incorrect or nonsensical outputs. This paper examines a technique to\nalleviate the tokenization artifact on text completion in generative models,\nmaintaining performance even in regular non-subword cases. The method, termed\ntoken alignment, involves backtracking to the last complete tokens and ensuring\nthe model's generation aligns with the prompt. This approach showcases marked\nimprovement across many partial token scenarios, including nuanced cases like\nspace-prefix and partial indentation, with only a minor time increase. The\ntechnique and analysis detailed in this paper contribute to the continuous\nadvancement of generative models in handling partial inputs, bearing relevance\nfor applications like code completion and text autocompletion.", "published": "2024-03-13 16:44:39", "link": "http://arxiv.org/abs/2403.08688v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Strengthening Multimodal Large Language Model with Bootstrapped\n  Preference Optimization", "abstract": "Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.", "published": "2024-03-13 17:29:45", "link": "http://arxiv.org/abs/2403.08730v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation", "abstract": "Existing Machine Learning approaches for local citation recommendation\ndirectly map or translate a query, which is typically a claim or an entity\nmention, to citation-worthy research papers. Within such a formulation, it is\nchallenging to pinpoint why one should cite a specific research paper for a\nparticular query, leading to limited recommendation interpretability. To\nalleviate this, we introduce the evidence-grounded local citation\nrecommendation task, where the target latent space comprises evidence spans for\nrecommending specific papers. Using a distantly-supervised evidence retrieval\nand multi-step re-ranking framework, our proposed system, ILCiteR, recommends\npapers to cite for a query grounded on similar evidence spans extracted from\nthe existing research literature. Unlike past formulations that simply output\nrecommendations, ILCiteR retrieves ranked lists of evidence span and\nrecommended paper pairs. Secondly, previously proposed neural models for\ncitation recommendation require expensive training on massive labeled data,\nideally after every significant update to the pool of candidate papers. In\ncontrast, ILCiteR relies solely on distant supervision from a dynamic evidence\ndatabase and pre-trained Transformer-based Language Models without any model\ntraining. We contribute a novel dataset for the evidence-grounded local\ncitation recommendation task and demonstrate the efficacy of our proposed\nconditional neural rank-ensembling approach for re-ranking evidence spans.", "published": "2024-03-13 17:38:05", "link": "http://arxiv.org/abs/2403.08737v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "AutoGuide: Automated Generation and Selection of Context-Aware\n  Guidelines for Large Language Model Agents", "abstract": "Recent advances in large language models (LLMs) have empowered AI agents\ncapable of performing various sequential decision-making tasks. However,\neffectively guiding LLMs to perform well in unfamiliar domains like web\nnavigation, where they lack sufficient knowledge, has proven to be difficult\nwith the demonstration-based in-context learning paradigm. In this paper, we\nintroduce a novel framework, called AutoGuide, which addresses this limitation\nby automatically generating context-aware guidelines from offline experiences.\nImportantly, each context-aware guideline is expressed in concise natural\nlanguage and follows a conditional structure, clearly describing the context\nwhere it is applicable. As a result, our guidelines facilitate the provision of\nrelevant knowledge for the agent's current decision-making process, overcoming\nthe limitations of the conventional demonstration-based learning paradigm. Our\nevaluation demonstrates that AutoGuide significantly outperforms competitive\nbaselines in complex benchmark domains, including real-world web navigation.", "published": "2024-03-13 22:06:03", "link": "http://arxiv.org/abs/2403.08978v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PET-SQL: A Prompt-Enhanced Two-Round Refinement of Text-to-SQL with\n  Cross-consistency", "abstract": "Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large\nlanguage models (LLM) on in-context learning, achieving significant results.\nNevertheless, they face challenges when dealing with verbose database\ninformation and complex user intentions. This paper presents a two-stage\nframework to enhance the performance of current LLM-based natural language to\nSQL systems. We first introduce a novel prompt representation, called\nreference-enhanced representation, which includes schema information and\nrandomly sampled cell values from tables to instruct LLMs in generating SQL\nqueries. Then, in the first stage, question-SQL pairs are retrieved as few-shot\ndemonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After\nthat, the mentioned entities in PreSQL are parsed to conduct schema linking,\nwhich can significantly compact the useful information. In the second stage,\nwith the linked schema, we simplify the prompt's schema information and\ninstruct the LLM to produce the final SQL. Finally, as the post-refinement\nmodule, we propose using cross-consistency across different LLMs rather than\nself-consistency within a particular LLM. Our methods achieve new SOTA results\non the Spider benchmark, with an execution accuracy of 87.6%.", "published": "2024-03-13 02:32:41", "link": "http://arxiv.org/abs/2403.09732v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OverleafCopilot: Empowering Academic Writing in Overleaf with Large\n  Language Models", "abstract": "The rapid development of Large Language Models (LLMs) has facilitated a\nvariety of applications from different domains. In this technical report, we\nexplore the integration of LLMs and the popular academic writing tool,\nOverleaf, to enhance the efficiency and quality of academic writing. To achieve\nthe above goal, there are three challenges: i) including seamless interaction\nbetween Overleaf and LLMs, ii) establishing reliable communication with the LLM\nprovider, and iii) ensuring user privacy. To address these challenges, we\npresent OverleafCopilot, the first-ever tool (i.e., a browser extension) that\nseamlessly integrates LLMs and Overleaf, enabling researchers to leverage the\npower of LLMs while writing papers. Specifically, we first propose an effective\nframework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a\nwebsite for researchers to easily find and share high-quality up-to-date\nprompts. Thirdly, we propose an agent command system to help researchers\nquickly build their customizable agents. OverleafCopilot\n(https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb\n) has been on the Chrome Extension Store, which now serves thousands of\nresearchers. Additionally, the code of PromptGenius is released at\nhttps://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the\npotential to revolutionize academic writing practices, empowering researchers\nto produce higher-quality papers in less time.", "published": "2024-03-13 07:52:31", "link": "http://arxiv.org/abs/2403.09733v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Large Language Models Solve ARC Visual Analogies Like People Do?", "abstract": "The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test\ndesigned for humans and machines (Chollet, 2019). We compared human and large\nlanguage model (LLM) performance on a new child-friendly set of ARC items.\nResults show that both children and adults outperform most LLMs on these tasks.\nError analysis revealed a similar \"fallback\" solution strategy in LLMs and\nyoung children, where part of the analogy is simply copied. In addition, we\nfound two other error types, one based on seemingly grasping key concepts\n(e.g., Inside-Outside) and the other based on simple combinations of analogy\ninput matrices. On the whole, \"concept\" errors were more common in humans, and\n\"matrix\" errors were more common in LLMs. This study sheds new light on LLM\nreasoning ability and the extent to which we can use error analyses and\ncomparisons with human development to understand how LLMs solve visual\nanalogies.", "published": "2024-03-13 09:48:13", "link": "http://arxiv.org/abs/2403.09734v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MolBind: Multimodal Alignment of Language, Molecules, and Proteins", "abstract": "Recent advancements in biology and chemistry have leveraged multi-modal\nlearning, integrating molecules and their natural language descriptions to\nenhance drug discovery. However, current pre-training frameworks are limited to\ntwo modalities, and designing a unified network to process different modalities\n(e.g., natural language, 2D molecular graphs, 3D molecular conformations, and\n3D proteins) remains challenging due to inherent gaps among them. In this work,\nwe propose MolBind, a framework that trains encoders for multiple modalities\nthrough contrastive learning, mapping all modalities to a shared feature space\nfor multi-modal semantic alignment. To facilitate effective pre-training of\nMolBind on multiple modalities, we also build and collect a high-quality\ndataset with four modalities, MolBind-M4, including graph-language,\nconformation-language, graph-conformation, and conformation-protein paired\ndata. MolBind shows superior zero-shot learning performance across a wide range\nof tasks, demonstrating its strong capability of capturing the underlying\nsemantics of multiple modalities.", "published": "2024-03-13 01:38:42", "link": "http://arxiv.org/abs/2403.08167v2", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of\n  Speech Sound Disorders in Korean children", "abstract": "This study presents a model of automatic speech recognition (ASR) designed to\ndiagnose pronunciation issues in children with speech sound disorders (SSDs) to\nreplace manual transcriptions in clinical procedures. Since ASR models trained\nfor general purposes primarily predict input speech into real words, employing\na well-known high-performance ASR model for evaluating pronunciation in\nchildren with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to\nrecognize speech as pronounced rather than as existing words. The model was\nfine-tuned with a speech dataset from 137 children with inadequate speech\nproduction pronouncing 73 Korean words selected for actual clinical diagnosis.\nThe model's predictions of the pronunciations of the words matched the human\nannotations with about 90% accuracy. While the model still requires improvement\nin recognizing unclear pronunciation, this study demonstrates that ASR models\ncan streamline complex pronunciation error diagnostic procedures in clinical\nfields.", "published": "2024-03-13 02:20:05", "link": "http://arxiv.org/abs/2403.08187v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Knowledge Conflicts for LLMs: A Survey", "abstract": "This survey provides an in-depth analysis of knowledge conflicts for large\nlanguage models (LLMs), highlighting the complex challenges they encounter when\nblending contextual and parametric knowledge. Our focus is on three categories\nof knowledge conflicts: context-memory, inter-context, and intra-memory\nconflict. These conflicts can significantly impact the trustworthiness and\nperformance of LLMs, especially in real-world applications where noise and\nmisinformation are common. By categorizing these conflicts, exploring the\ncauses, examining the behaviors of LLMs under such conflicts, and reviewing\navailable solutions, this survey aims to shed light on strategies for improving\nthe robustness of LLMs, thereby serving as a valuable resource for advancing\nresearch in this evolving area.", "published": "2024-03-13 08:02:23", "link": "http://arxiv.org/abs/2403.08319v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SMART: Submodular Data Mixture Strategy for Instruction Tuning", "abstract": "Instruction Tuning involves finetuning a language model on a collection of\ninstruction-formatted datasets in order to enhance the generalizability of the\nmodel to unseen tasks. Studies have shown the importance of balancing different\ntask proportions during finetuning, but finding the right balance remains\nchallenging. Unfortunately, there's currently no systematic method beyond\nmanual tuning or relying on practitioners' intuition. In this paper, we\nintroduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a\nnovel data mixture strategy which makes use of a submodular function to assign\nimportance scores to tasks which are then used to determine the mixture\nweights. Given a fine-tuning budget, SMART redistributes the budget among tasks\nand selects non-redundant samples from each task. Experimental results\ndemonstrate that SMART significantly outperforms traditional methods such as\nexamples proportional mixing and equal mixing. Furthermore, SMART facilitates\nthe creation of data mixtures based on a few representative subsets of tasks\nalone and through task pruning analysis, we reveal that in a limited budget\nsetting, allocating budget among a subset of representative tasks yields\nsuperior performance compared to distributing the budget among all tasks. The\ncode for reproducing our results is open-sourced at\nhttps://github.com/kowndinya-renduchintala/SMART.", "published": "2024-03-13 09:31:50", "link": "http://arxiv.org/abs/2403.08370v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Translating between SQL Dialects for Cloud Migration", "abstract": "Migrations of systems from on-site premises to the cloud has been a\nfundamental endeavor by many industrial institutions. A crucial component of\nsuch cloud migrations is the transition of databases to be hosted online. In\nthis work, we consider the difficulties of this migration for SQL databases.\nWhile SQL is one of the prominent methods for storing database procedures,\nthere are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)\nwhich can complicate migrations when the on-premise SQL dialect differs to the\ndialect hosted on the cloud. Tools exist by common cloud provides such as AWS\nand Azure to aid in translating between dialects in order to mitigate the\nmajority of the difficulties. However, these tools do not successfully\ntranslate $100\\%$ of the code. Consequently, software engineers must manually\nconvert the remainder of the untranslated database. For large organizations,\nthis task quickly becomes intractable and so more innovative solutions are\nrequired. We consider this challenge a novel yet vital industrial research\nproblem for any large corporation that is considering cloud migrations.\nFurthermore, we introduce potential avenues of research to tackle this\nchallenge that have yielded promising preliminary results.", "published": "2024-03-13 09:38:39", "link": "http://arxiv.org/abs/2403.08375v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.DB"}
{"title": "Distract Large Language Models for Automatic Jailbreak Attack", "abstract": "Extensive efforts have been made before the public release of Large language\nmodels (LLMs) to align their behaviors with human values. However, even\nmeticulously aligned LLMs remain vulnerable to malicious manipulations such as\njailbreaking, leading to unintended behaviors. In this work, we propose a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.", "published": "2024-03-13 11:16:43", "link": "http://arxiv.org/abs/2403.08424v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Generalizing Fairness to Generative Language Models via Reformulation of\n  Non-discrimination Criteria", "abstract": "Generative AI, such as large language models, has undergone rapid development\nwithin recent years. As these models become increasingly available to the\npublic, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.", "published": "2024-03-13 14:19:08", "link": "http://arxiv.org/abs/2403.08564v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Improving Acoustic Word Embeddings through Correspondence Training of\n  Self-supervised Speech Representations", "abstract": "Acoustic word embeddings (AWEs) are vector representations of spoken words.\nAn effective method for obtaining AWEs is the Correspondence Auto-Encoder\n(CAE). In the past, the CAE method has been associated with traditional MFCC\nfeatures. Representations obtained from self-supervised learning (SSL)-based\nspeech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many\ndownstream tasks. However, they have not been well studied in the context of\nlearning AWEs. This work explores the effectiveness of CAE with SSL-based\nspeech representations to obtain improved AWEs. Additionally, the capabilities\nof SSL-based speech models are explored in cross-lingual scenarios for\nobtaining AWEs. Experiments are conducted on five languages: Polish,\nPortuguese, Spanish, French, and English. HuBERT-based CAE model achieves the\nbest results for word discrimination in all languages, despite Hu-BERT being\npre-trained on English only. Also, the HuBERT-based CAE model works well in\ncross-lingual settings. It outperforms MFCC-based CAE models trained on the\ntarget languages when trained on one source language and tested on target\nlanguages.", "published": "2024-03-13 17:42:03", "link": "http://arxiv.org/abs/2403.08738v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution\n  in Large Language Models", "abstract": "A substantial gap persists in understanding the reasons behind the\nexceptional performance of the Transformer architecture in NLP. A particularly\nunexplored area involves the mechanistic description of how the distribution of\nparameters evolves over time during training. In this work we suggest that\nlooking at the time evolution of the statistic distribution of model\nparameters, and specifically at bifurcation effects, can help understanding the\nmodel quality, potentially reducing training costs and evaluation efforts and\nempirically showing the reasons behind the effectiveness of weights\nsparsification.", "published": "2024-03-13 17:42:32", "link": "http://arxiv.org/abs/2403.08739v1", "categories": ["cs.CL", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompting Fairness: Integrating Causality to Debias Large Language\n  Models", "abstract": "Large language models (LLMs), despite their remarkable capabilities, are\nsusceptible to generating biased and discriminatory responses. As LLMs\nincreasingly influence high-stakes decision-making (e.g., hiring and\nhealthcare), mitigating these biases becomes critical. In this work, we propose\na causality-guided debiasing framework to tackle social biases, aiming to\nreduce the objectionable dependence between LLMs' decisions and the social\ninformation in the input. Our framework introduces a novel perspective to\nidentify how social information can affect an LLM's decision through different\ncausal pathways. Leveraging these causal insights, we outline principled\nprompting strategies that regulate these pathways through selection mechanisms.\nThis framework not only unifies existing prompting-based debiasing techniques,\nbut also opens up new directions for reducing bias by encouraging the model to\nprioritize fact-based reasoning over reliance on biased social cues. We\nvalidate our framework through extensive experiments on real-world datasets\nacross multiple domains, demonstrating its effectiveness in debiasing LLM\ndecisions, even with only black-box access to the model.", "published": "2024-03-13 17:46:28", "link": "http://arxiv.org/abs/2403.08743v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DAM: Dynamic Adapter Merging for Continual Video QA Learning", "abstract": "We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM", "published": "2024-03-13 17:53:47", "link": "http://arxiv.org/abs/2403.08755v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models", "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.", "published": "2024-03-13 17:58:57", "link": "http://arxiv.org/abs/2403.08763v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "From \"um\" to \"yeah\": Producing, predicting, and regulating information\n  flow in human conversation", "abstract": "Conversation demands attention. Speakers must call words to mind, listeners\nmust make sense of them, and both together must negotiate this flow of\ninformation, all in fractions of a second. We used large language models to\nstudy how this works in a large-scale dataset of English-language conversation,\nthe CANDOR corpus. We provide a new estimate of the information density of\nunstructured conversation, of approximately 13 bits/second, and find\nsignificant effects associated with the cognitive load of both retrieving, and\npresenting, that information. We also reveal a role for backchannels -- the\nbrief yeahs, uh-huhs, and mhmms that listeners provide -- in regulating the\nproduction of novelty: the lead-up to a backchannel is associated with\ndeclining information rate, while speech downstream rebounds to previous rates.\nOur results provide new insights into long-standing theories of how we respond\nto fluctuating demands on cognitive resources, and how we negotiate those\ndemands in partnership with others.", "published": "2024-03-13 18:20:24", "link": "http://arxiv.org/abs/2403.08890v1", "categories": ["cs.CL", "cs.IT", "math.IT", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM\n  Era", "abstract": "Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended towards Large Language Models (LLMs) which are often criticized for\ntheir lack of transparency. This extension calls for a significant\ntransformation in XAI methodologies because of two reasons. First, many\nexisting XAI methods cannot be directly applied to LLMs due to their complexity\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse\nindustry applications, the role of XAI shifts from merely opening the \"black\nbox\" to actively enhancing the productivity and applicability of LLMs in\nreal-world settings. Meanwhile, unlike traditional machine learning models that\nare passive recipients of XAI insights, the distinct abilities of LLMs can\nreciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in\nthe context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10\nstrategies, introducing the key techniques for each and discussing their\nassociated challenges. We also provide case studies to demonstrate how to\nobtain and leverage explanations. The code used in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.", "published": "2024-03-13 20:25:27", "link": "http://arxiv.org/abs/2403.08946v1", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Evaluating Large Language Models as Generative User Simulators for\n  Conversational Recommendation", "abstract": "Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.", "published": "2024-03-13 18:16:21", "link": "http://arxiv.org/abs/2403.09738v4", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Human Factor in Detecting Errors of Large Language Models: A\n  Systematic Literature Review and Future Research Directions", "abstract": "The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for\nArtificial Intelligence, introducing Large Language Models (LLMs) to the\nmainstream and setting new records in user adoption. LLMs, particularly\nChatGPT, trained on extensive internet data, demonstrate remarkable\nconversational capabilities across various domains, suggesting a significant\nimpact on the workforce. However, these models are susceptible to errors -\n\"hallucinations\" and omissions, generating incorrect or incomplete information.\nThis poses risks especially in contexts where accuracy is crucial, such as\nlegal compliance, medicine or fine-grained process frameworks.\n  There are both technical and human solutions to cope with this isse. This\npaper explores the human factors that enable users to detect errors in LLM\noutputs, a critical component in mitigating risks associated with their use in\nprofessional settings. Understanding these factors is essential for\norganizations aiming to leverage LLM technology efficiently, guiding targeted\ntraining and deployment strategies to enhance error detection by users. This\napproach not only aims to optimize the use of LLMs but also to prevent\npotential downstream issues stemming from reliance on inaccurate model\nresponses. The research emphasizes the balance between technological\nadvancement and human insight in maximizing the benefits of LLMs while\nminimizing the risks, particularly in areas where precision is paramount.\n  This paper performs a systematic literature research on this research topic,\nanalyses and synthesizes the findings, and outlines future research directions.\nLiterature selection cut-off date is January 11th 2024.", "published": "2024-03-13 21:39:39", "link": "http://arxiv.org/abs/2403.09743v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Application of Large Language Models to Generate Feedback\n  in Programming Education", "abstract": "This study investigates the application of large language models,\nspecifically GPT-4, to enhance programming education. The research outlines the\ndesign of a web application that uses GPT-4 to provide feedback on programming\ntasks, without giving away the solution. A web application for working on\nprogramming tasks was developed for the study and evaluated with 51 students\nover the course of one semester. The results show that most of the feedback\ngenerated by GPT-4 effectively addressed code errors. However, challenges with\nincorrect suggestions and hallucinated issues indicate the need for further\nimprovements.", "published": "2024-03-13 23:14:35", "link": "http://arxiv.org/abs/2403.09744v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Second-Order Information Matters: Revisiting Machine Unlearning for\n  Large Language Models", "abstract": "With the rapid development of Large Language Models (LLMs), we have witnessed\nintense competition among the major LLM products like ChatGPT, LLaMa, and\nGemini. However, various issues (e.g. privacy leakage and copyright violation)\nof the training corpus still remain underexplored. For example, the Times sued\nOpenAI and Microsoft for infringing on its copyrights by using millions of its\narticles for training. From the perspective of LLM practitioners, handling such\nunintended privacy violations can be challenging. Previous work addressed the\n``unlearning\" problem of LLMs using gradient information, while they mostly\nintroduced significant overheads like data preprocessing or lacked robustness.\nIn this paper, contrasting with the methods based on first-order information,\nwe revisit the unlearning problem via the perspective of second-order\ninformation (Hessian). Our unlearning algorithms, which are inspired by classic\nNewton update, are not only data-agnostic/model-agnostic but also proven to be\nrobust in terms of utility preservation or privacy guarantee. Through a\ncomprehensive evaluation with four NLP datasets as well as a case study on\nreal-world datasets, our methods consistently show superiority over the\nfirst-order methods.", "published": "2024-03-13 18:57:30", "link": "http://arxiv.org/abs/2403.10557v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AutoTRIZ: Automating Engineering Innovation with TRIZ and Large Language\n  Models", "abstract": "Various ideation methods, such as morphological analysis and\ndesign-by-analogy, have been developed to aid creative problem-solving and\ninnovation. Among them, the Theory of Inventive Problem Solving (TRIZ) stands\nout as one of the best-known methods. However, the complexity of TRIZ and its\nreliance on users' knowledge, experience, and reasoning capabilities limit its\npracticality. To address this, we introduce AutoTRIZ, an artificial ideation\nsystem that integrates Large Language Models (LLMs) to automate and enhance the\nTRIZ methodology. By leveraging LLMs' vast pre-trained knowledge and advanced\nreasoning capabilities, AutoTRIZ offers a novel, generative, and interpretable\napproach to engineering innovation. AutoTRIZ takes a problem statement from the\nuser as its initial input, automatically conduct the TRIZ reasoning process and\ngenerates a structured solution report. We demonstrate and evaluate the\neffectiveness of AutoTRIZ through comparative experiments with textbook cases\nand a real-world application in the design of a Battery Thermal Management\nSystem (BTMS). Moreover, the proposed LLM-based framework holds the potential\nfor extension to automate other knowledge-based ideation methods, such as\nSCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era\nof AI-driven innovation tools.", "published": "2024-03-13 02:53:36", "link": "http://arxiv.org/abs/2403.13002v4", "categories": ["cs.HC", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "primary_category": "cs.HC"}
{"title": "A Moral Imperative: The Need for Continual Superalignment of Large\n  Language Models", "abstract": "This paper examines the challenges associated with achieving life-long\nsuperalignment in AI systems, particularly large language models (LLMs).\nSuperalignment is a theoretical framework that aspires to ensure that\nsuperintelligent AI systems act in accordance with human values and goals.\nDespite its promising vision, we argue that achieving superalignment requires\nsubstantial changes in the current LLM architectures due to their inherent\nlimitations in comprehending and adapting to the dynamic nature of these human\nethics and evolving global scenarios. We dissect the challenges of encoding an\never-changing spectrum of human values into LLMs, highlighting the\ndiscrepancies between static AI models and the dynamic nature of human\nsocieties. To illustrate these challenges, we analyze two distinct examples:\none demonstrates a qualitative shift in human values, while the other presents\na quantifiable change. Through these examples, we illustrate how LLMs,\nconstrained by their training data, fail to align with contemporary human\nvalues and scenarios. The paper concludes by exploring potential strategies to\naddress and possibly mitigate these alignment discrepancies, suggesting a path\nforward in the pursuit of more adaptable and responsive AI systems.", "published": "2024-03-13 05:44:50", "link": "http://arxiv.org/abs/2403.14683v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "CodingTeachLLM: Empowering LLM's Coding Ability via AST Prior Knowledge", "abstract": "In this paper, we introduce CodingTeachLLM, a large language model (LLM)\ndesigned for coding teaching. Specially, we aim to enhance the coding ability\nof LLM and lead it to better teaching mode in education context. Thus, we\npropose an end-to-end prior-based three-phases supervised fine-tuned model,\nwhich is proved more competitive than traditional fine-tuning method. More\nspecifically, our model realizes the structural disassembly and incremental\nguided output of educational knowledge. To this end, we robustify data\nclassification of three types via a sampler and overlap estimation neural\nnetwork, and inject the preprocessing datasets into pre-trained model in three\nbatches for LORA fine-tuning. Then, we design a prior module couples system\nprompt, vector databases, and abstract syntax tree task segmentation. Finally,\nthe compression method and regularization constraint are applied to the\nprior-based fine-tuned model, followed by text filter at the output end to\nobtain incremental guided results. Our model represents the first research\neffort to truly embody the tutor role with the features of abundant educational\nknowledge, step-by-step incremental guided outputs and non-disclosure of\nanswers. Extensive experiments report that our model also achieves\nstate-of-the-art in code abilities compared to open-source models, reaching an\nimpressive 75.10% on the HumanEval (@pass 1) benchmark. Additionally, our model\nmaintains strong conversational capabilities, with the 13B quantized version\nachieving scores of 56.34, 50.60, and 45.27 respectively on the MMLU, C-Eval,\nand AGIEval (5 shot) dialogue evaluation benchmarks.", "published": "2024-03-13 05:38:39", "link": "http://arxiv.org/abs/2403.15426v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "primary_category": "cs.LG"}
{"title": "PAPERCLIP: Associating Astronomical Observations and Natural Language\n  with Multi-Modal Models", "abstract": "We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation\nfor Contrastive Language-Image Pre-training), a method which associates\nastronomical observations imaged by telescopes with natural language using a\nneural network model. The model is fine-tuned from a pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) model using successful observing proposal\nabstracts and corresponding downstream observations, with the abstracts\noptionally summarized via guided generation using large language models (LLMs).\nUsing observations from the Hubble Space Telescope (HST) as an example, we show\nthat the fine-tuned model embodies a meaningful joint representation between\nobservations and natural language through tests targeting image retrieval\n(i.e., finding the most relevant observations using natural language queries)\nand description retrieval (i.e., querying for astrophysical object classes and\nuse cases most relevant to a given observation). Our study demonstrates the\npotential for using generalist foundation models rather than task-specific\nmodels for interacting with astronomical data by leveraging text as an\ninterface.", "published": "2024-03-13 18:00:00", "link": "http://arxiv.org/abs/2403.08851v1", "categories": ["astro-ph.IM", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "astro-ph.IM"}
{"title": "End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier\n  Models", "abstract": "This paper describes a data-driven approach to creating real-time neural\nnetwork models of guitar amplifiers, recreating the amplifiers' sonic response\nto arbitrary inputs at the full range of controls present on the physical\ndevice. While the focus on the paper is on the data collection pipeline, we\ndemonstrate the effectiveness of this conditioned black-box approach by\ntraining an LSTM model to the task, and comparing its performance to an offline\nwhite-box SPICE circuit simulation. Our listening test results demonstrate that\nthe neural amplifier modeling approach can match the subjective performance of\na high-quality SPICE model, all while using an automated, non-intrusive data\ncollection process, and an end-to-end trainable, real-time feasible neural\nnetwork model.", "published": "2024-03-13 14:10:10", "link": "http://arxiv.org/abs/2403.08559v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Efficient End-to-End Approach to Noise Invariant Speech Features via\n  Multi-Task Learning", "abstract": "Self-supervised speech representation learning enables the extraction of\nmeaningful features from raw waveforms. These features can then be efficiently\nused across multiple downstream tasks. However, two significant issues arise\nwhen considering the deployment of such methods ``in-the-wild\": (i) Their large\nsize, which can be prohibitive for edge applications; and (ii) their robustness\nto detrimental factors, such as noise and/or reverberation, that can heavily\ndegrade the performance of such systems. In this work, we propose\nRobustDistiller, a novel knowledge distillation mechanism that tackles both\nproblems jointly. Simultaneously to the distillation recipe, we apply a\nmulti-task learning objective to encourage the network to learn noise-invariant\nrepresentations by denoising the input. The proposed mechanism is evaluated on\ntwelve different downstream tasks. It outperforms several benchmarks regardless\nof noise type, or noise and reverberation levels. Experimental results show\nthat the new Student model with 23M parameters can achieve results comparable\nto the Teacher model with 95M parameters. Lastly, we show that the proposed\nrecipe can be applied to other distillation methodologies, such as the recent\nDPWavLM. For reproducibility, code and model checkpoints will be made available\nat \\mbox{\\url{https://github.com/Hguimaraes/robustdistiller}}.", "published": "2024-03-13 16:08:59", "link": "http://arxiv.org/abs/2403.08654v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight\n  Text-to-Speech", "abstract": "Recently, deep learning-based Text-to-Speech (TTS) systems have achieved\nhigh-quality speech synthesis results. Recurrent neural networks have become a\nstandard modeling technique for sequential data in TTS systems and are widely\nused. However, training a TTS model which includes RNN components requires\npowerful GPU performance and takes a long time. In contrast, CNN-based sequence\nsynthesis techniques can significantly reduce the parameters and training time\nof a TTS model while guaranteeing a certain performance due to their high\nparallelism, which alleviate these economic costs of training. In this paper,\nwe propose a lightweight TTS system based on deep convolutional neural\nnetworks, which is a two-stage training end-to-end TTS model and does not\nemploy any recurrent units. Our model consists of two stages: Text2Spectrum and\nSSRN. The former is used to encode phonemes into a coarse mel spectrogram and\nthe latter is used to synthesize the complete spectrum from the coarse mel\nspectrogram. Meanwhile, we improve the robustness of our model by a series of\ndata augmentations, such as noise suppression, time warping, frequency masking\nand time masking, for solving the low resource mongolian problem. Experiments\nshow that our model can reduce the training time and parameters while ensuring\nthe quality and naturalness of the synthesized speech compared to using\nmainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for\nvalidation, which significantly reduces training time while maintaining a\ncertain accuracy.", "published": "2024-03-13 01:27:57", "link": "http://arxiv.org/abs/2403.08164v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Weak to Strong Sound Event Labels using Adaptive Change-Point\n  Detection and Active Learning", "abstract": "We propose an adaptive change point detection method (A-CPD) for machine\nguided weak label annotation of audio recording segments. The goal is to\nmaximize the amount of information gained about the temporal activations of the\ntarget sounds. For each unlabeled audio recording, we use a prediction model to\nderive a probability curve used to guide annotation. The prediction model is\ninitially pre-trained on available annotated sound event data with classes that\nare disjoint from the classes in the unlabeled dataset. The prediction model\nthen gradually adapts to the annotations provided by the annotator in an active\nlearning loop. We derive query segments to guide the weak label annotator\ntowards strong labels, using change point detection on these probabilities. We\nshow that it is possible to derive strong labels of high quality with a limited\nannotation budget, and show favorable results for A-CPD when compared to two\nbaseline query segment strategies.", "published": "2024-03-13 13:33:35", "link": "http://arxiv.org/abs/2403.08525v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
