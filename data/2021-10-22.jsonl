{"title": "Text Counterfactuals via Latent Optimization and Shapley-Guided Search", "abstract": "We study the problem of generating counterfactual text for a classifier as a\nmeans for understanding and debugging classification. Given a textual input and\na classification model, we aim to minimally alter the text to change the\nmodel's prediction. White-box approaches have been successfully applied to\nsimilar problems in vision where one can directly optimize the continuous\ninput. Optimization-based approaches become difficult in the language domain\ndue to the discrete nature of text. We bypass this issue by directly optimizing\nin the latent space and leveraging a language model to generate candidate\nmodifications from optimized latent representations. We additionally use\nShapley values to estimate the combinatoric effect of multiple changes. We then\nuse these estimates to guide a beam search for the final counterfactual text.\nWe achieve favorable performance compared to recent white-box and black-box\nbaselines using human and automatic evaluations. Ablation studies show that\nboth latent optimization and the use of Shapley values improve success rate and\nthe quality of the generated counterfactuals.", "published": "2021-10-22 05:04:40", "link": "http://arxiv.org/abs/2110.11589v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ListReader: Extracting List-form Answers for Opinion Questions", "abstract": "Question answering (QA) is a high-level ability of natural language\nprocessing. Most extractive ma-chine reading comprehension models focus on\nfactoid questions (e.g., who, when, where) and restrict the output answer as a\nshort and continuous span in the original passage. However, in real-world\nscenarios, many questions are non-factoid (e.g., how, why) and their answers\nare organized in the list format that contains multiple non-contiguous spans.\nNaturally, existing extractive models are by design unable to answer such\nquestions. To address this issue, this paper proposes ListReader, a neural\nex-tractive QA model for list-form answer. In addition to learning the\nalignment between the question and content, we introduce a heterogeneous graph\nneural network to explicitly capture the associations among candidate segments.\nMoreover, our model adopts a co-extraction setting that can extract either\nspan- or sentence-level answers, allowing better applicability. Two large-scale\ndatasets of different languages are constructed to support this study.\nExperimental results show that our model considerably outperforms various\nstrong baselines. Further discussions provide an intuitive understanding of how\nour model works and where the performance gain comes from.", "published": "2021-10-22 10:33:08", "link": "http://arxiv.org/abs/2110.11692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lightweight Decoding Strategies for Increasing Specificity", "abstract": "Language models are known to produce vague and generic outputs. We propose\ntwo unsupervised decoding strategies based on either word-frequency or\npoint-wise mutual information to increase the specificity of any model that\noutputs a probability distribution over its vocabulary at generation time. We\ntest the strategies in a prompt completion task; with human evaluations, we\nfind that both strategies increase the specificity of outputs with only modest\ndecreases in sensibility. We also briefly present a summarization use case,\nwhere these strategies can produce more specific summaries.", "published": "2021-10-22 15:32:25", "link": "http://arxiv.org/abs/2110.11850v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cleaning Dirty Books: Post-OCR Processing for Previously Scanned Texts", "abstract": "Substantial amounts of work are required to clean large collections of\ndigitized books for NLP analysis, both because of the presence of errors in the\nscanned text and the presence of duplicate volumes in the corpora. In this\npaper, we consider the issue of deduplication in the presence of optical\ncharacter recognition (OCR) errors. We present methods to handle these errors,\nevaluated on a collection of 19,347 texts from the Project Gutenberg dataset\nand 96,635 texts from the HathiTrust Library. We demonstrate that improvements\nin language models now enable the detection and correction of OCR errors\nwithout consideration of the scanning image itself. The inconsistencies found\nby aligning pairs of scans of the same underlying work provides training data\nto build models for detecting and correcting errors. We identify the canonical\nversion for each of 17,136 repeatedly-scanned books from 58,808 scans. Finally,\nwe investigate methods to detect and correct errors in single-copy texts. We\nshow that on average, our method corrects over six times as many errors as it\nintroduces. We also provide interesting analysis on the relation between\nscanning quality and other factors such as location and publication year.", "published": "2021-10-22 17:33:17", "link": "http://arxiv.org/abs/2110.11934v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Framework for Learning Assessment through Multimodal Analysis of\n  Reading Behaviour and Language Comprehension", "abstract": "Reading comprehension, which has been defined as gaining an understanding of\nwritten text through a process of translating grapheme into meaning, is an\nimportant academic skill. Other language learning skills - writing, speaking\nand listening, all are connected to reading comprehension. There have been\nseveral measures proposed by researchers to automate the assessment of\ncomprehension skills for second language (L2) learners, especially English as\nSecond Language (ESL) and English as Foreign Language (EFL) learners. However,\ncurrent methods measure particular skills without analysing the impact of\nreading frequency on comprehension skills. In this dissertation, we show how\ndifferent skills could be measured and scored automatically. We also\ndemonstrate, using example experiments on multiple forms of learners'\nresponses, how frequent reading practices could impact on the variables of\nmultimodal skills (reading pattern, writing, and oral fluency).\n  This thesis comprises of five studies. The first and second studies are based\non eye-tracking data collected from EFL readers in repeated reading (RR)\nsessions. The third and fourth studies are to evaluate free-text summary\nwritten by EFL readers in repeated reading sessions. The fifth and last study,\ndescribed in the sixth chapter of the thesis, is to evaluate recorded oral\nsummaries recited by EFL readers in repeated reading sessions.\n  In a nutshell, through this dissertation, we show that multimodal skills of\nlearners could be assessed to measure their comprehension skills as well as to\nmeasure the effect of repeated readings on these skills in the course of time,\nby finding significant features and by applying machine learning techniques\nwith a combination of statistical models such as LMER.", "published": "2021-10-22 17:48:03", "link": "http://arxiv.org/abs/2110.11938v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClimateBert: A Pretrained Language Model for Climate-Related Text", "abstract": "Over the recent years, large pretrained language models (LM) have\nrevolutionized the field of natural language processing (NLP). However, while\npretraining on general language has been shown to work very well for common\nlanguage, it has been observed that niche language poses problems. In\nparticular, climate-related texts include specific language that common LMs can\nnot represent accurately. We argue that this shortcoming of today's LMs limits\nthe applicability of modern NLP to the broad field of text processing of\nclimate-related texts. As a remedy, we propose CLIMATEBERT, a transformer-based\nlanguage model that is further pretrained on over 2 million paragraphs of\nclimate-related texts, crawled from various sources such as common news,\nresearch articles, and climate reporting of companies. We find that CLIMATEBERT\nleads to a 48% improvement on a masked language model objective which, in turn,\nleads to lowering error rates by 3.57% to 35.71% for various climate-related\ndownstream tasks like text classification, sentiment analysis, and\nfact-checking.", "published": "2021-10-22 18:47:34", "link": "http://arxiv.org/abs/2110.12010v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Bridge between Training and Inference for Dialogue", "abstract": "Although exposure bias has been widely studied in some NLP tasks, it faces\nits unique challenges in dialogue response generation, the representative\none-to-various generation scenario. In real human dialogue, there are many\nappropriate responses for the same context, not only with different\nexpressions, but also with different topics. Therefore, due to the much bigger\ngap between various ground-truth responses and the generated synthetic\nresponse, exposure bias is more challenging in dialogue generation task. What's\nmore, as MLE encourages the model to only learn the common words among\ndifferent ground-truth responses, but ignores the interesting and specific\nparts, exposure bias may further lead to the common response generation\nproblem, such as \"I don't know\" and \"HaHa?\" In this paper, we propose a novel\nadaptive switching mechanism, which learns to automatically transit between\nground-truth learning and generated learning regarding the word-level matching\nscore, such as the cosine similarity. Experimental results on both Chinese STC\ndataset and English Reddit dataset, show that our adaptive method achieves a\nsignificant improvement in terms of metric-based evaluation and human\nevaluation, as compared with the state-of-the-art exposure bias approaches.\nFurther analysis on NMT task also shows that our model can achieve a\nsignificant improvement.", "published": "2021-10-22 02:43:27", "link": "http://arxiv.org/abs/2110.11560v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Simple Dialogue System with AUDITED", "abstract": "We devise a multimodal conversation system for dialogue utterances composed\nof text, image or both modalities. We leverage Auxiliary UnsuperviseD vIsual\nand TExtual Data (AUDITED). To improve the performance of text-based task, we\nutilize translations of target sentences from English to French to form the\nassisted supervision. For the image-based task, we employ the DeepFashion\ndataset in which we seek nearest neighbor images of positive and negative\ntarget images of the MMD data. These nearest neighbors form the nearest\nneighbor embedding providing an external context for target images. We form two\nmethods to create neighbor embedding vectors, namely Neighbor Embedding by Hard\nAssignment (NEHA) and Neighbor Embedding by Soft Assignment (NESA) which\ngenerate context subspaces per target image. Subsequently, these subspaces are\nlearnt by our pipeline as a context for the target data. We also propose a\ndiscriminator which switches between the image- and text-based tasks. We show\nimprovements over baselines on the large-scale Multimodal Dialogue Dataset\n(MMD) and SIMMC.", "published": "2021-10-22 16:07:16", "link": "http://arxiv.org/abs/2110.11881v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Challenges in Procedural Multimodal Machine Comprehension:A Novel Way To\n  Benchmark", "abstract": "We focus on Multimodal Machine Reading Comprehension (M3C) where a model is\nexpected to answer questions based on given passage (or context), and the\ncontext and the questions can be in different modalities. Previous works such\nas RecipeQA have proposed datasets and cloze-style tasks for evaluation.\nHowever, we identify three critical biases stemming from the question-answer\ngeneration process and memorization capabilities of large deep models. These\nbiases makes it easier for a model to overfit by relying on spurious\ncorrelations or naive data patterns. We propose a systematic framework to\naddress these biases through three Control-Knobs that enable us to generate a\ntest bed of datasets of progressive difficulty levels. We believe that our\nbenchmark (referred to as Meta-RecipeQA) will provide, for the first time, a\nfine grained estimate of a model's generalization capabilities. We also propose\na general M3C model that is used to realize several prior SOTA models and\nmotivate a novel hierarchical transformer based reasoning network (HTRN). We\nperform a detailed evaluation of these models with different language and\nvisual features on our benchmark. We observe a consistent improvement with HTRN\nover SOTA (~18% in Visual Cloze task and ~13% in average over all the tasks).\nWe also observe a drop in performance across all the models when testing on\nRecipeQA and proposed Meta-RecipeQA (e.g. 83.6% versus 67.1% for HTRN), which\nshows that the proposed dataset is relatively less biased. We conclude by\nhighlighting the impact of the control knobs with some quantitative results.", "published": "2021-10-22 16:33:57", "link": "http://arxiv.org/abs/2110.11899v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Double Trouble: How to not explain a text classifier's decisions using\n  counterfactuals synthesized by masked language models?", "abstract": "A principle behind dozens of attribution methods is to take the prediction\ndifference between before-and-after an input feature (here, a token) is removed\nas its attribution. A popular Input Marginalization (IM) method (Kim et al.,\n2020) uses BERT to replace a token, yielding more plausible counterfactuals.\nWhile Kim et al. (2020) reported that IM is effective, we find this conclusion\nnot convincing as the DeletionBERT metric used in their paper is biased towards\nIM. Importantly, this bias exists in Deletion-based metrics, including\nInsertion, Sufficiency, and Comprehensiveness. Furthermore, our rigorous\nevaluation using 6 metrics and 3 datasets finds no evidence that IM is better\nthan a Leave-One-Out (LOO) baseline. We find two reasons why IM is not better\nthan LOO: (1) deleting a single word from the input only marginally reduces a\nclassifier's accuracy; and (2) a highly predictable word is always given\nnear-zero attribution, regardless of its true importance to the classifier. In\ncontrast, making LIME samples more natural via BERT consistently improves LIME\naccuracy under several ROAR metrics.", "published": "2021-10-22 17:22:05", "link": "http://arxiv.org/abs/2110.11929v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPU-Accelerated Forward-Backward algorithm with Application to\n  Lattice-Free MMI", "abstract": "We propose to express the forward-backward algorithm in terms of operations\nbetween sparse matrices in a specific semiring. This new perspective naturally\nleads to a GPU-friendly algorithm which is easy to implement in Julia or any\nprogramming languages with native support of semiring algebra. We use this new\nimplementation to train a TDNN with the LF-MMI objective function and we\ncompare the training time of our system with PyChain - a recently introduced\nC++/CUDA implementation of the LF-MMI loss. Our implementation is about two\ntimes faster while not having to use any approximation such as the \"leaky-HMM\".", "published": "2021-10-22 08:31:02", "link": "http://arxiv.org/abs/2112.00709v1", "categories": ["cs.DC", "cs.CL"], "primary_category": "cs.DC"}
{"title": "SciCap: Generating Captions for Scientific Figures", "abstract": "Researchers use figures to communicate rich, complex information in\nscientific papers. The captions of these figures are critical to conveying\neffective messages. However, low-quality figure captions commonly occur in\nscientific articles and may decrease understanding. In this paper, we propose\nan end-to-end neural framework to automatically generate informative,\nhigh-quality captions for scientific figures. To this end, we introduce SCICAP,\na large-scale figure-caption dataset based on computer science arXiv papers\npublished between 2010 and 2020. After pre-processing - including figure-type\nclassification, sub-figure identification, text normalization, and caption text\nselection - SCICAP contained more than two million figures extracted from over\n290,000 papers. We then established baseline models that caption graph plots,\nthe dominant (19.2%) figure type. The experimental results showed both\nopportunities and steep challenges of generating captions for scientific\nfigures.", "published": "2021-10-22 07:10:41", "link": "http://arxiv.org/abs/2110.11624v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Signal-Envelope: A C++ library with Python bindings for temporal\n  envelope estimation", "abstract": "Signals can be interpreted as composed of a rapidly varying component\nmodulated by a slower varying envelope. Identifying this envelope is an\nessential operation in signal processing, with applications in areas ranging\nfrom seismology to medicine. Conventional envelope detection approaches based\non classic methods tend to lack generality, however, and need to be tailored to\neach specific application in order to yield reasonable results. Taking\ninspiration from geometric concepts, most notably the theory of alpha-shapes,\nwe introduce a general-purpose library to efficiently extract the envelope of\narbitrary signals.", "published": "2021-10-22 14:27:02", "link": "http://arxiv.org/abs/2110.11807v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Time-domain Ad-hoc Array Speech Enhancement Using a Triple-path Network", "abstract": "Deep neural networks (DNNs) are very effective for multichannel speech\nenhancement with fixed array geometries. However, it is not trivial to use DNNs\nfor ad-hoc arrays with unknown order and placement of microphones. We propose a\nnovel triple-path network for ad-hoc array processing in the time domain. The\nkey idea in the network design is to divide the overall processing into spatial\nprocessing and temporal processing and use self-attention for spatial\nprocessing. Using self-attention for spatial processing makes the network\ninvariant to the order and the number of microphones. The temporal processing\nis done independently for all channels using a recently proposed dual-path\nattentive recurrent network. The proposed network is a multiple-input\nmultiple-output architecture that can simultaneously enhance signals at all\nmicrophones. Experimental results demonstrate the excellent performance of the\nproposed approach. Further, we present analysis to demonstrate the\neffectiveness of the proposed network in utilizing multichannel information\neven from microphones at far locations.", "published": "2021-10-22 15:24:05", "link": "http://arxiv.org/abs/2110.11844v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
