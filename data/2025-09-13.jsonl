{"title": "ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER", "abstract": "Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER\nby jointly detecting textual mentions and grounding them to visual regions.\nWhile existing supervised methods achieve strong performance, they rely on\ncostly multimodal annotations and often underperform in low-resource domains.\nMultimodal Large Language Models (MLLMs) show strong generalization but suffer\nfrom Domain Knowledge Conflict, producing redundant or incorrect mentions for\ndomain-specific entities. To address these challenges, we propose ReFineG, a\nthree-stage collaborative framework that integrates small supervised models\nwith frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware\nNER data synthesis strategy transfers LLM knowledge to small models with\nsupervised training while avoiding domain knowledge conflicts. In the\nRefinement Stage, an uncertainty-based mechanism retains confident predictions\nfrom supervised models and delegates uncertain ones to the MLLM. In the\nGrounding Stage, a multimodal context selection algorithm enhances visual\ngrounding through analogical reasoning. In the CCKS2025 GMNER Shared Task,\nReFineG ranked second with an F1 score of 0.6461 on the online leaderboard,\ndemonstrating its effectiveness with limited annotations.", "published": "2025-09-13 20:32:12", "link": "http://arxiv.org/abs/2509.10975v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "An Interpretable Benchmark for Clickbait Detection and Tactic Attribution", "abstract": "The proliferation of clickbait headlines poses significant challenges to the\ncredibility of information and user trust in digital media. While recent\nadvances in machine learning have improved the detection of manipulative\ncontent, the lack of explainability limits their practical adoption. This paper\npresents a model for explainable clickbait detection that not only identifies\nclickbait titles but also attributes them to specific linguistic manipulation\nstrategies. We introduce a synthetic dataset generated by systematically\naugmenting real news headlines using a predefined catalogue of clickbait\nstrategies. This dataset enables controlled experimentation and detailed\nanalysis of model behaviour. We present a two-stage framework for automatic\nclickbait analysis comprising detection and tactic attribution. In the first\nstage, we compare a fine-tuned BERT classifier with large language models\n(LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot\nprompting and few-shot prompting enriched with illustrative clickbait headlines\nand their associated persuasive tactics. In the second stage, a dedicated\nBERT-based classifier predicts the specific clickbait strategies present in\neach headline. This work advances the development of transparent and\ntrustworthy AI systems for combating manipulative media content. We share the\ndataset with the research community at\nhttps://github.com/LLM-HITCS25S/ClickbaitTacticsDetection", "published": "2025-09-13 18:26:49", "link": "http://arxiv.org/abs/2509.10937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents", "abstract": "In this paper, we introduce Spotlight, a novel paradigm for information\nextraction that produces concise, engaging narratives by highlighting the most\ncompelling aspects of a document. Unlike traditional summaries, which\nprioritize comprehensive coverage, spotlights selectively emphasize intriguing\ncontent to foster deeper reader engagement with the source material. We\nformally differentiate spotlights from related constructs and support our\nanalysis with a detailed benchmarking study using new datasets curated for this\nwork. To generate high-quality spotlights, we propose a two-stage approach:\nfine-tuning a large language model on our benchmark data, followed by alignment\nvia Direct Preference Optimization (DPO). Our comprehensive evaluation\ndemonstrates that the resulting model not only identifies key elements with\nprecision but also enhances readability and boosts the engagement value of the\noriginal document.", "published": "2025-09-13 18:18:37", "link": "http://arxiv.org/abs/2509.10935v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Public Data Assisted Differentially Private In-Context Learning", "abstract": "In-context learning (ICL) in Large Language Models (LLMs) has shown\nremarkable performance across various tasks without requiring fine-tuning.\nHowever, recent studies have highlighted the risk of private data leakage\nthrough the prompt in ICL, especially when LLMs are exposed to malicious\nattacks. While differential privacy (DP) provides strong privacy guarantees, it\noften significantly reduces the utility of in-context learning (ICL). To\naddress this challenge, we incorporate task-related public data into the ICL\nframework while maintaining the DP guarantee. Based on this approach, we\npropose a private in-context learning algorithm that effectively balances\nprivacy protection and model utility. Through experiments, we demonstrate that\nour approach significantly improves the utility of private ICL with the\nassistance of public data. Additionally, we show that our method is robust\nagainst membership inference attacks, demonstrating empirical privacy\nprotection.", "published": "2025-09-13 18:11:51", "link": "http://arxiv.org/abs/2509.10932v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Harmful Prompt Laundering: Jailbreaking LLMs with Abductive Styles and Symbolic Encoding", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but their potential misuse for harmful purposes remains a\nsignificant concern. To strengthen defenses against such vulnerabilities, it is\nessential to investigate universal jailbreak attacks that exploit intrinsic\nweaknesses in the architecture and learning paradigms of LLMs. In response, we\npropose \\textbf{H}armful \\textbf{P}rompt \\textbf{La}undering (HaPLa), a novel\nand broadly applicable jailbreaking technique that requires only black-box\naccess to target models. HaPLa incorporates two primary strategies: 1)\n\\textit{abductive framing}, which instructs LLMs to infer plausible\nintermediate steps toward harmful activities, rather than directly responding\nto explicit harmful queries; and 2) \\textit{symbolic encoding}, a lightweight\nand flexible approach designed to obfuscate harmful content, given that current\nLLMs remain sensitive primarily to explicit harmful keywords. Experimental\nresults show that HaPLa achieves over 95% attack success rate on GPT-series\nmodels and 70% across all targets. Further analysis with diverse symbolic\nencoding rules also reveals a fundamental challenge: it remains difficult to\nsafely tune LLMs without significantly diminishing their helpfulness in\nresponding to benign queries.", "published": "2025-09-13 18:07:56", "link": "http://arxiv.org/abs/2509.10931v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction", "abstract": "The growing importance of environmental, social, and governance data in\nregulatory and investment contexts has increased the need for accurate,\ninterpretable, and internationally aligned representations of non-financial\nrisks, particularly those reported in unstructured news sources. However,\naligning such controversy-related data with principle-based normative\nframeworks, such as the United Nations Global Compact or Sustainable\nDevelopment Goals, presents significant challenges. These frameworks are\ntypically expressed in abstract language, lack standardized taxonomies, and\ndiffer from the proprietary classification systems used by commercial data\nproviders. In this paper, we present a semi-automatic method for constructing\nstructured knowledge representations of environmental, social, and governance\nevents reported in the news. Our approach uses lightweight ontology design,\nformal pattern modeling, and large language models to convert normative\nprinciples into reusable templates expressed in the Resource Description\nFramework. These templates are used to extract relevant information from news\ncontent and populate a structured knowledge graph that links reported incidents\nto specific framework principles. The result is a scalable and transparent\nframework for identifying and interpreting non-compliance with international\nsustainability guidelines.", "published": "2025-09-13 17:49:59", "link": "http://arxiv.org/abs/2509.10922v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis", "abstract": "Cultural competence, defined as the ability to understand and adapt to\nmulticultural contexts, is increasingly vital for large language models (LLMs)\nin global environments. While several cultural benchmarks exist to assess LLMs'\ncultural competence, current evaluations suffer from fragmented taxonomies,\ndomain specificity, and heavy reliance on manual data annotation. To address\nthese limitations, we introduce CultureSynth, a novel framework comprising (1)\na comprehensive hierarchical multilingual cultural taxonomy covering 12 primary\nand 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based\nmethodology leveraging factual knowledge to synthesize culturally relevant\nquestion-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360\nentries and 4,149 manually verified entries across 7 languages. Evaluation of\n14 prevalent LLMs of different sizes reveals clear performance stratification\nled by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that\na 3B-parameter threshold is necessary for achieving basic cultural competence,\nmodels display varying architectural biases in knowledge processing, and\nsignificant geographic disparities exist across models. We believe that\nCultureSynth offers a scalable framework for developing culturally aware AI\nsystems while reducing reliance on manual annotation\\footnote{Benchmark is\navailable at https://github.com/Eyr3/CultureSynth.}.", "published": "2025-09-13 16:33:56", "link": "http://arxiv.org/abs/2509.10886v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms", "abstract": "Training data is fundamental to the success of modern machine learning\nmodels, yet in high-stakes domains such as healthcare, the use of real-world\ntraining data is severely constrained by concerns over privacy leakage. A\npromising solution to this challenge is the use of differentially private (DP)\nsynthetic data, which offers formal privacy guarantees while maintaining data\nutility. However, striking the right balance between privacy protection and\nutility remains challenging in clinical note synthesis, given its domain\nspecificity and the complexity of long-form text generation. In this paper, we\npresent Term2Note, a methodology to synthesise long clinical notes under strong\nDP constraints. By structurally separating content and form, Term2Note\ngenerates section-wise note content conditioned on DP medical terms, with each\ngoverned by separate DP constraints. A DP quality maximiser further enhances\nsynthetic notes by selecting high-quality outputs. Experimental results show\nthat Term2Note produces synthetic notes with statistical properties closely\naligned with real clinical notes, demonstrating strong fidelity. In addition,\nmulti-label classification models trained on these synthetic notes perform\ncomparably to those trained on real data, confirming their high utility.\nCompared to existing DP text generation baselines, Term2Note achieves\nsubstantial improvements in both fidelity and utility while operating under\nfewer assumptions, suggesting its potential as a viable privacy-preserving\nalternative to using sensitive clinical notes.", "published": "2025-09-13 16:26:38", "link": "http://arxiv.org/abs/2509.10882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifier Scope Interpretation in Language Learners and LLMs", "abstract": "Sentences with multiple quantifiers often lead to interpretive ambiguities,\nwhich can vary across languages. This study adopts a cross-linguistic approach\nto examine how large language models (LLMs) handle quantifier scope\ninterpretation in English and Chinese, using probabilities to assess\ninterpretive likelihood. Human similarity (HS) scores were used to quantify the\nextent to which LLMs emulate human performance across language groups. Results\nreveal that most LLMs prefer the surface scope interpretations, aligning with\nhuman tendencies, while only some differentiate between English and Chinese in\nthe inverse scope preferences, reflecting human-similar patterns. HS scores\nhighlight variability in LLMs' approximation of human behavior, but their\noverall potential to align with humans is notable. Differences in model\narchitecture, scale, and particularly models' pre-training data language\nbackground, significantly influence how closely LLMs approximate human\nquantifier scope interpretations.", "published": "2025-09-13 15:32:25", "link": "http://arxiv.org/abs/2509.10860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Vanishing Signatures, Orbit Closure, and the Converse of the Holant Theorem", "abstract": "Valiant's Holant theorem is a powerful tool for algorithms and reductions for\ncounting problems. It states that if two sets $\\mathcal{F}$ and $\\mathcal{G}$\nof tensors (a.k.a. constraint functions or signatures) are related by a\n\\emph{holographic transformation}, then $\\mathcal{F}$ and $\\mathcal{G}$ are\n\\emph{Holant-indistinguishable}, i.e., every tensor network using tensors from\n$\\mathcal{F}$, resp. from $\\mathcal{G}$, contracts to the same value. Xia\n(ICALP 2010) conjectured the converse of the Holant theorem, but a\ncounterexample was found based on \\emph{vanishing} signatures, those which are\nHolant-indistinguishable from 0.\n  We prove two near-converses of the Holant theorem using techniques from\ninvariant theory. (I) Holant-indistinguishable $\\mathcal{F}$ and $\\mathcal{G}$\nalways admit two sequences of holographic transformations mapping them\narbitrarily close to each other, i.e., their $\\text{GL}_q$-orbit closures\nintersect. (II) We show that vanishing signatures are the only true obstacle to\na converse of the Holant theorem. As corollaries of the two theorems we obtain\nthe first characterization of homomorphism-indistinguishability over graphs of\nbounded degree, a long standing open problem, and show that two graphs with\ninvertible adjacency matrices are isomorphic if and only if they are\nhomomorphism-indistinguishable over graphs with maximum degree at most three.\nWe also show that Holant-indistinguishability is complete for a complexity\nclass \\textbf{TOCI} introduced by Lysikov and Walter, and hence hard for graph\nisomorphism.", "published": "2025-09-13 21:58:59", "link": "http://arxiv.org/abs/2509.10991v1", "categories": ["cs.DM", "G.2.1"], "primary_category": "cs.DM"}
{"title": "Erasing Classical Memory with Quantum Fluctuations: Shannon Information Entropy of Reverse Quantum Annealing", "abstract": "Quantum annealers can provide non-local optimization by tunneling between\nstates in a process that ideally eliminates memory of the initial\nconfiguration. We study the crossover between memory loss and retention due to\nquantum fluctuations, in a transverse Ising model on odd numbered\nantiferromagnetic rings of thousands of spins with periodic boundary\nconditions, by performing reverse quantum annealing experiments on three\nprogrammable superconducting flux qubit quantum annealers. After initializing\nthe spins to contain a single domain wall, we then expose it to quantum\nfluctuations by turning on the transverse Zeeman energy. We characterize the\ncrossover between memory retention at low transverse field, and memory loss at\nhigh transverse field by extracting the Shannon information entropy of magnetic\ndomain wall distributions. We demonstrate a clear crossover in memory\nretention, and its dependence on hardware platform and simulation time. Our\napproach establishes a general probe of the interplay between quantum\nfluctuations and memory.", "published": "2025-09-13 18:03:07", "link": "http://arxiv.org/abs/2509.10927v1", "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "Detectability Thresholds for Network Attacks on Static Graphs and Temporal Networks: Information-Theoretic Limits and Nearly-Optimal Tests", "abstract": "We develop a consolidated theory for the detectability of network-borne\nattacks under two canonical observation models: (i) a static graph drawn from\nan Erdos-Renyi background with a planted anomalous community, and (ii) a\ntemporal interaction network modeled by multivariate point processes (Poisson\nor Hawkes). Our main contribution is to match, up to universal constants,\ninformation-theoretic lower and upper bounds that govern when reliable testing\nis possible. In the static case, the core quantity is the accumulated edgewise\nsignal k^2 * chi^2(Bern(p+Delta) || Bern(p)), where chi^2 ~ Delta^2 / [p(1-p)]\nfor small Delta; detection is impossible when this falls below c * log n, and a\nnon-backtracking spectral statistic succeeds above C * log n. In the temporal\ncase, detectability is controlled by the KL information rate I contributed by\ninternal edges over a window of length T, yielding a threshold T I >= log n; a\nlikelihood-based cumulative-sum (CUSUM) test achieves first-order optimal delay\napproximately abs(log alpha) / I at false-alarm level alpha. We also quantify\nrobustness to bounded edge perturbations and outline conditional\nstatistical-computational separations. A brief case study shows how to turn\nthese bounds into concrete design choices.", "published": "2025-09-13 17:58:23", "link": "http://arxiv.org/abs/2509.10925v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "A Broadcast Channel Framework for MIMO-OFDM Integrated Sensing and Communication", "abstract": "Integrated sensing and communication (ISAC) is expected to be one of the\nmajor features of 6G wireless networks. In an ISAC system, communications and\nsensing functionalities are jointly performed using the same waveform,\nfrequency band and hardware, thereby enabling various use cases such as in\ncyber physical systems, digital twin and smart cities. A major challenge to the\ndesign and analysis of ISAC is a unified framework that incorporates the two\ndistinct functions. By viewing ISAC as a type of broadcast channel, in this\npaper, we propose a unified ISAC framework in which communication and sensing\nsignals are broadcast to the actual communication users and virtual sensing\nusers. This framework allows the application of existing multiplexing schemes,\nsuch as dirty paper coding (DPC) and frequency division multiplexing (FDM) that\nhave been intensively studied in data communications and information theory.\nWithin this framework, we propose different superposition coding schemes, for\ncases when the sensing waveform is known or unknown to the communication\nreceiver. We propose the waveform optimization algorithms in a multiple-input\nmultiple-output (MIMO) setting accounting for the effects of clutter and\nDoppler shift. The proposed framework is numerically evaluated for different\nschemes under various sensing and communications performance metrics.", "published": "2025-09-13 16:13:41", "link": "http://arxiv.org/abs/2509.10878v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Landscape Analysis of Simultaneous Blind Deconvolution and Phase Retrieval via Structured Low-Rank Tensor Recovery", "abstract": "This paper presents a geometric analysis of the simultaneous blind\ndeconvolution and phase retrieval (BDPR) problem via a structured low-rank\ntensor recovery framework. Due to the highly complicated structure of the\nassociated sensing tensor, directly characterizing its optimization landscape\nis intractable. To address this, we introduce a tensor sensing problem as a\ntractable surrogate that preserves the essential structural features of the\ntarget low-rank tensor while enabling rigorous theoretical analysis. As a first\nstep toward understanding this surrogate model, we study the corresponding\npopulation risk, which captures key aspects of the underlying low-rank tensor\nstructure. We characterize the global landscape of the population risk on the\nunit sphere and show that Riemannian gradient descent (RGD) converges linearly\nunder mild conditions. We then extend the analysis to the tensor sensing\nproblem, establishing local geometric properties, proving convergence\nguarantees for RGD, and quantifying robustness under measurement noise. Our\ntheoretical results are further supported by extensive numerical experiments.\nThese findings offer foundational insights into the optimization landscape of\nthe structured low-rank tensor recovery problem, which equivalently\ncharacterizes the original BDPR problem, thereby providing principled guidance\nfor solving the original BDPR problem.", "published": "2025-09-13 14:54:13", "link": "http://arxiv.org/abs/2509.10834v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Uniquely-Decodable Coding for Zero-Error Network Function Computation", "abstract": "We consider uniquely-decodable coding for zero-error network function\ncomputation, where in a directed acyclic graph, the single sink node is\nrequired to compute with zero error a target function multiple times, whose\narguments are the information sources generated at a set of source nodes. We\nare interested in the computing capacity from the information theoretic point\nof view, which is defined as the infimum of the maximum expected number of bits\ntransmitted on all the edges for computing the target function once on average.\nWe first prove some new results on clique entropy, in particular, the\nsubstitution lemma of clique entropy for probabilistic graphs with a certain\ncondition. With them, we prove a lower bound on the computing capacity\nassociated with clique entropies of the induced characteristic graphs, where\nthe obtained lower bound is applicable to arbitrary network topologies,\narbitrary information sources, and arbitrary target functions. By refining the\nprobability distribution of information sources, we further strictly improve\nthe obtained lower bound. In addition, we compare uniquely-decodable network\nfunction-computing coding and fixed-length network function-computing coding,\nand show that the former indeed outperforms the latter in terms of the\ncomputing capacity. Therein, we provide a novel graph-theoretic explanation of\nthe key parameter in the best known bound on the computing capacity for\nfixed-length network function-computing codes, which would be helpful to\nimprove the existing results.", "published": "2025-09-13 01:43:22", "link": "http://arxiv.org/abs/2509.10775v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Statistical Model Checking of NetLogo Models", "abstract": "Agent-based models (ABMs) are gaining increasing traction in several domains,\ndue to their ability to represent complex systems that are not easily\nexpressible with classical mathematical models. This expressivity and richness\ncome at a cost: ABMs can typically be analyzed only through simulation, making\ntheir analysis challenging. Specifically, when studying the output of ABMs, the\nanalyst is often confronted with practical questions such as: (i) how many\nindependent replications should be run? (ii) how many initial time steps should\nbe discarded as a warm-up? (iii) after the warm-up, how long should the model\nrun? (iv) what are the right parameter values? Analysts usually resort to rules\nof thumb and experimentation, which lack statistical rigor. This is mainly\nbecause addressing these points takes time, and analysts prefer to spend their\nlimited time improving the model. In this paper, we propose a methodology,\ndrawing on the field of Statistical Model Checking, to automate the process and\nprovide guarantees of statistical rigor for ABMs written in NetLogo, one of the\nmost popular ABM platforms. We discuss MultiVeStA, a tool that dramatically\nreduces the time and human intervention needed to run statistically rigorous\nchecks on ABM outputs, and introduce its integration with NetLogo. Using two\nABMs from the NetLogo library, we showcase MultiVeStA's analysis capabilities\nfor NetLogo ABMs, as well as a novel application to statistically rigorous\ncalibration. Our tool-chain makes it immediate to perform statistical checks\nwith NetLogo models, promoting more rigorous and reliable analyses of ABM\noutputs.", "published": "2025-09-13 20:47:40", "link": "http://arxiv.org/abs/2509.10977v1", "categories": ["cs.MA", "econ.GN", "q-fin.EC"], "primary_category": "cs.MA"}
{"title": "Using utility graphs to search for Pareto-optimal outcomes in complex, interdependent issue negotiations", "abstract": "This paper studies how utility graphs decomposition algorithms can be used to\neffectively search for Pareto-efficient outcomes in complex automated\nnegotiation. We propose a number of algorithms that can efficiently handle\nhigh-dimensional utility graphs, and test them on a variety of utility graph\ntopologies, generated based on state of the art methods for analysing complex\ngraphs. We show that we can achieve exponential speed-up, for many structures,\neven for very large utility graphs. To our knowledge, our approach can handle\nthe largest utility spaces to date for complex interdependent negotiations, in\nterms of number of issues. Moreover, we examine the performance of our\nalgorithms across two different types of elicitation queries from the\nliterature: value and comparison queries, thus making a connection between\nautomated negotiation and the preference elicitation literature.", "published": "2025-09-13 16:31:32", "link": "http://arxiv.org/abs/2509.10885v1", "categories": ["cs.MA", "05C90", "I.2.11"], "primary_category": "cs.MA"}
{"title": "Agent-based Simulation for Drone Charging in an Internet of Things Environment System", "abstract": "This paper presents an agent-based simulation model for coordinating battery\nrecharging in drone swarms, focusing on applications in Internet of Things\n(IoT) and Industry 4.0 environments. The proposed model includes a detailed\ndescription of the simulation methodology, system architecture, and\nimplementation. One practical use case is explored: Smart Farming, highlighting\nhow autonomous coordination strategies can optimize battery usage and mission\nefficiency in large-scale drone deployments. This work uses a machine learning\ntechnique to analyze the agent-based simulation sensitivity analysis output\nresults.", "published": "2025-09-13 15:47:08", "link": "http://arxiv.org/abs/2509.10867v1", "categories": ["cs.MA", "cs.NI", "cs.RO"], "primary_category": "cs.MA"}
{"title": "AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise", "abstract": "While individual components of agentic architectures have been studied in\nisolation, there remains limited empirical understanding of how different\ndesign dimensions interact within complex multi-agent systems. This study aims\nto address these gaps by providing a comprehensive enterprise-specific\nbenchmark evaluating 18 distinct agentic configurations across state-of-the-art\nlarge language models. We examine four critical agentic system dimensions:\norchestration strategy, agent prompt implementation (ReAct versus function\ncalling), memory architecture, and thinking tool integration. Our benchmark\nreveals significant model-specific architectural preferences that challenge the\nprevalent one-size-fits-all paradigm in agentic AI systems. It also reveals\nsignificant weaknesses in overall agentic performance on enterprise tasks with\nthe highest scoring models achieving a maximum of only 35.3\\% success on the\nmore complex task and 70.8\\% on the simpler task. We hope these findings inform\nthe design of future agentic systems by enabling more empirically backed\ndecisions regarding architectural components and model selection.", "published": "2025-09-13 01:18:23", "link": "http://arxiv.org/abs/2509.10769v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Development and Analysis of Chien-Physics-Informed Neural Networks for Singular Perturbation Problems", "abstract": "In this article, we employ Chien-Physics Informed Neural Networks (C-PINNs)\nto obtain solutions for singularly perturbed convection-diffusion equations,\nreaction-diffusion equations, and their coupled forms in both one and\ntwo-dimensional settings. While PINNs have emerged as a powerful tool for\nsolving various types of differential equations, their application to singular\nperturbation problems (SPPs) presents significant challenges. These challenges\narise because a small perturbation parameter multiplies the highest-order\nderivatives, leading to sharp gradient changes near the boundary layer. To\novercome these difficulties, we apply C-PINNs, a modified version of the\nstandard PINNs framework, which is specifically designed to address singular\nperturbation problems. Our study shows that C-PINNs provide a more accurate\nsolution for SPPs, demonstrating better performance than conventional methods.", "published": "2025-09-13 18:59:35", "link": "http://arxiv.org/abs/2509.10945v1", "categories": ["math.NA", "cs.NA", "65L10, 65L11, 65L20, 65L60, 65L70, 68T07"], "primary_category": "math.NA"}
{"title": "Design and accuracy trade-offs in Computational Statistics", "abstract": "Statistical computations are becoming increasingly important. These\ncomputations often need to be performed in log-space because probabilities\nbecome extremely small due to repeated multiplications. While using logarithms\neffectively prevents numerical underflow, this paper shows that its cost is\nhigh in performance, resource utilization, and, notably, numerical accuracy.\nThis paper then argues that using posit, a recently proposed floating-point\nformat, is a better strategy for statistical computations operating on\nextremely small numbers because of its unique encoding mechanism. To that end,\nthis paper performs a comprehensive analysis comparing posit, binary64, and\nlogarithm representations, examining both individual arithmetic operations,\nstatistical bioinformatics applications, and their accelerators. FPGA\nimplementation results highlight that posit-based accelerators can achieve up\nto two orders of magnitude higher accuracy, up to 60\\% lower resource\nutilization, and up to $1.3\\times$ speedup, compared to log-space accelerators.\nSuch improvement translates to $2\\times$ performance per unit resource on the\nFPGA.", "published": "2025-09-13 18:18:17", "link": "http://arxiv.org/abs/2509.10934v1", "categories": ["math.NA", "cs.AR", "cs.NA"], "primary_category": "math.NA"}
{"title": "The coupling of mixed and primal finite element methods for the coupled body-plate problem", "abstract": "This paper considers the coupled problem of a three-dimensional elastic body\nand a two-dimensional plate, which are rigidly connected at their interface.\nThe plate consists of a plane elasticity model along the longitudinal direction\nand a plate bending model with Kirchhoff assumptions along the transverse\ndirection. The Hellinger-Reissner formulation is adopted for the body by\nintroducing the stress as an auxiliary variable, while the primal formulation\nis employed for the plate. The well-posedness of the weak formulation is\nestablished. This approach enables direct stress approximations and allows for\nnon-matching meshes at the interface since the continuity condition of\ndisplacements acts as a natural boundary condition for the body. Under certain\nassumptions, discrete stability and error estimates are derived for both\nconforming and nonconforming finite element methods. Two specific pairs of\nconforming and nonconforming finite elements are shown to satisfy the required\nassumptions, respectively. Furthermore, the problem is reduced to an interface\nproblem based on the domain decomposition, which can be solved effectively by a\nconjugate gradient iteration. Numerical experiments are conducted to validate\nthe theoretical results.", "published": "2025-09-13 14:46:50", "link": "http://arxiv.org/abs/2509.10827v1", "categories": ["math.NA", "cs.NA", "65N15, 65N30, 74S05"], "primary_category": "math.NA"}
{"title": "Mixed regularity and sparse grid approximations of $N$-body Schr\u00f6dinger evolution equation", "abstract": "In this paper, we present a mathematical analysis of time-dependent $N$-body\nelectronic systems and establish mixed regularity for the corresponding\nwavefunctions. Based on this, we develop sparse grid approximations to reduce\ncomputational complexity, including a sparse grid Gaussian-type orbital (GTO)\nscheme. We validate the approach on the Helium atom (${\\rm He}$) and Hydrogen\nmolecule (${\\rm H}_2$), showing that sparse grid GTOs offer an efficient\nalternative to full grid discretizations.", "published": "2025-09-13 03:56:56", "link": "http://arxiv.org/abs/2509.10805v1", "categories": ["math.NA", "cs.NA", "math-ph", "math.AP", "math.MP", "physics.atom-ph", "physics.comp-ph", "35J10, 35B65, 41A25, 41A63"], "primary_category": "math.NA"}
{"title": "Why Bonds Fail Differently? Explainable Multimodal Learning for Multi-Class Default Prediction", "abstract": "In recent years, China's bond market has seen a surge in defaults amid\nregulatory reforms and macroeconomic volatility. Traditional machine learning\nmodels struggle to capture financial data's irregularity and temporal\ndependencies, while most deep learning models lack interpretability-critical\nfor financial decision-making. To tackle these issues, we propose EMDLOT\n(Explainable Multimodal Deep Learning for Time-series), a novel framework for\nmulti-class bond default prediction. EMDLOT integrates numerical time-series\n(financial/macroeconomic indicators) and unstructured textual data (bond\nprospectuses), uses Time-Aware LSTM to handle irregular sequences, and adopts\nsoft clustering and multi-level attention to boost interpretability.\nExperiments on 1994 Chinese firms (2015-2024) show EMDLOT outperforms\ntraditional (e.g., XGBoost) and deep learning (e.g., LSTM) benchmarks in\nrecall, F1-score, and mAP, especially in identifying default/extended firms.\nAblation studies validate each component's value, and attention analyses reveal\neconomically intuitive default drivers. This work provides a practical tool and\na trustworthy framework for transparent financial risk modeling.", "published": "2025-09-13 03:42:34", "link": "http://arxiv.org/abs/2509.10802v1", "categories": ["q-fin.RM", "cs.CL", "cs.LG", "q-fin.CP"], "primary_category": "q-fin.RM"}
{"title": "Gradient Methods with Online Scaling Part II. Practical Aspects", "abstract": "Part I of this work [Gao25] establishes online scaled gradient methods\n(OSGM), a framework that utilizes online convex optimization to adapt stepsizes\nin gradient methods. This paper focuses on the practical aspects of OSGM. We\nleverage the OSGM framework to design new adaptive first-order methods and\nprovide insights into their empirical behavior. The resulting method,\nOSGM-Best, matches the performance of quasi-Newton variants while requiring\nless memory and cheaper iterations. We also extend OSGM to nonconvex\noptimization and outline directions that connect OSGM to existing branches of\noptimization theory and practice.", "published": "2025-09-13 23:14:27", "link": "http://arxiv.org/abs/2509.11007v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "primary_category": "math.OC"}
{"title": "Variable Selection Using Relative Importance Rankings", "abstract": "Although conceptually related, variable selection and relative importance\n(RI) analysis have been treated quite differently in the literature. While RI\nis typically used for post-hoc model explanation, this paper explores its\npotential for variable ranking and filter-based selection before model\ncreation. Specifically, we anticipate strong performance from the RI measures\nbecause they incorporate both direct and combined effects of predictors,\naddressing a key limitation of marginal correlation that ignores dependencies\namong predictors. We implement and evaluate the RI-based variable selection\nmethods using general dominance (GD), comprehensive relative importance (CRI),\nand a newly proposed, computationally efficient variant termed CRI.Z.\n  We first demonstrate how the RI measures more accurately rank the variables\nthan the marginal correlation, especially when there are suppressed or weak\npredictors. We then show that predictive models built on these rankings are\nhighly competitive, often outperforming state-of-the-art methods such as the\nlasso and relaxed lasso. The proposed RI-based methods are particularly\neffective in challenging cases involving clusters of highly correlated\npredictors, a setting known to cause failures in many benchmark methods.\nAlthough lasso methods have dominated the recent literature on variable\nselection, our study reveals that the RI-based method is a powerful and\ncompetitive alternative. We believe these underutilized tools deserve greater\nattention in statistics and machine learning communities. The code is available\nat: https://github.com/tien-endotchang/RI-variable-selection.", "published": "2025-09-13 15:21:39", "link": "http://arxiv.org/abs/2509.10853v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "FACTORS: Factorial Approximation for Complementary Two-factor Optimization with Risk-aware Scoring", "abstract": "We propose FACTORS, a framework that combines design of experiments with\nShapley decomposition to address performance and stability issues that are\nsensitive to combinations of training factors. Our approach consistently\nestimates main effects and two-factor interactions, then integrates them into a\nrisk-adjusted objective function that jointly accounts for uncertainty and\ncost, enabling reliable selection of configurations under a fixed budget.\nEffect estimation is implemented through two complementary paths: a plug-in\npath based on conditional means, and a least-squares path that reconstructs\nShapley contributions from samples. These paths are designed to work\ncomplementarily even when design density and bias levels differ. By\nincorporating standardization of estimates, bias correction, and uncertainty\nquantification, our procedure ensures comparability across heterogeneous factor\nspaces and designs, while a lightweight search routine yields configurations\nwithin practical time even for large factor spaces. On the theoretical side, we\nprovide error decompositions, sample complexity analysis, and upper bounds on\noptimality gaps. On the interpretive side, we summarize main effects and\ninteractions in map form, highlighting adjustment priorities and safe\nimprovement pathways. Across diverse datasets and design conditions, our\napproach improves rank preservation and optimal configuration identification,\nreduces decision-making risks, and offers a tuning foundation that delivers\ninterpretable justification alongside stable performance gains even under\nbudget constraints.", "published": "2025-09-13 14:44:45", "link": "http://arxiv.org/abs/2509.10825v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Local Density-Based Anomaly Score Normalization for Domain Generalization", "abstract": "State-of-the-art anomalous sound detection (ASD) systems in domain-shifted\nconditions rely on projecting audio signals into an embedding space and using\ndistance-based outlier detection to compute anomaly scores. One of the major\ndifficulties to overcome is the so-called domain mismatch between the anomaly\nscore distributions of a source domain and a target domain that differ\nacoustically and in terms of the amount of training data provided. A decision\nthreshold that is optimal for one domain may be highly sub-optimal for the\nother domain and vice versa. This significantly degrades the performance when\nonly using a single decision threshold, as is required when generalizing to\nmultiple data domains that are possibly unseen during training while still\nusing the same trained ASD system as in the source domain. To reduce this\nmismatch between the domains, we propose a simple local-density-based anomaly\nscore normalization scheme. In experiments conducted on several ASD datasets,\nwe show that the proposed normalization scheme consistently improves\nperformance for various types of embedding-based ASD systems and yields better\nresults than existing anomaly score normalization approaches.", "published": "2025-09-13 19:22:27", "link": "http://arxiv.org/abs/2509.10951v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-Time Defense Against Coordinated Cyber-Physical Attacks: A Robust Constrained Reinforcement Learning Approach", "abstract": "Modern power systems face increasing vulnerability to sophisticated\ncyber-physical attacks beyond traditional N-1 contingency frameworks. Existing\nsecurity paradigms face a critical bottleneck: efficiently identifying\nworst-case scenarios and rapidly coordinating defensive responses are hindered\nby intensive computation and time delays, during which cascading failures can\npropagate. This paper presents a novel tri-level robust constrained\nreinforcement learning (RCRL) framework for robust power system security. The\nframework generates diverse system states through AC-OPF formulations,\nidentifies worst-case N-K attack scenarios for each state, and trains policies\nto mitigate these scenarios across all operating conditions without requiring\npredefined attack patterns. The framework addresses constraint satisfaction\nthrough Beta-blending projection-based feasible action mapping techniques\nduring training and primal-dual augmented Lagrangian optimization for\ndeployment. Once trained, the RCRL policy learns how to control observed\ncyber-physical attacks in real time. Validation on IEEE benchmark systems\ndemonstrates effectiveness against coordinated N-K attacks, causing widespread\ncascading failures throughout the network. The learned policy can successfully\nrespond rapidly to recover system-wide constraints back to normal within 0.21\nms inference times, establishing superior resilience for critical\ninfrastructure protection.", "published": "2025-09-13 22:49:39", "link": "http://arxiv.org/abs/2509.10999v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Design and Validation of a MATLAB-based GUI for Coarray Domain Analysis of Sparse Linear Arrays", "abstract": "This work presents a first-of-its-kind graphical user interface (GUI)-based\nsimulator developed using MATLAB App designer for the comprehensive analysis of\nsparse linear arrays (SLAs) in the difference coarray (DCA) domain. Sparse\nsensor arrays have emerged as a critical solution in enhancing signal\ndetection, direction of arrival (DOA) estimation, and beamforming in fields\nsuch as wireless communication, radar, sonar, and integrated sensing systems.\nThey offer several advantages over traditional uniform arrays, including\nreduced system complexity, lower deployment costs, and improved mitigation of\nmutual coupling effects. The tool enables users to input array configurations,\ncompute DCAs, visualize weight function graphs, and assess the hole-free status\nof arrays, as applicable for coarray processing. Unlike conventional simulators\nthat focus on radiation pattern visualization (array pattern, main lobe and\nsidelobe characteristics, azimuth cut, rectangular view, polar view etc.), this\ntool addresses the behavior of SLAs from a coarray domain perspective.\nNumerical validations demonstrate the tool's correctness, effectiveness, and\nits potential to foster further research in sparse arrays. This simulator could\nalso be used as a teaching aid to drive home complicated topics and attract\nyoung minds towards the fascinating field of sparse array design.", "published": "2025-09-13 17:58:51", "link": "http://arxiv.org/abs/2509.10926v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Forecasting Self-Similar User Traffic Demand Using Transformers in LEO Satellite Networks", "abstract": "In this paper, we propose the use of a transformer-based model to address the\nneed for forecasting user traffic demand in the next generation Low Earth Orbit\n(LEO) satellite networks. Considering a LEO satellite constellation, we present\nthe need to forecast the demand for the satellites in-orbit to utilize dynamic\nbeam-hopping in high granularity. We adopt a traffic dataset with second-order\nself-similar characteristics. Given this traffic dataset, the Fractional\nAuto-regressive Integrated Moving Average (FARIMA) model is considered a\nbenchmark forecasting solution. However, the constrained on-board processing\ncapabilities of LEO satellites, combined with the need to fit a new model for\neach input sequence due to the nature of FARIMA, motivate the investigation of\nalternative solutions. As an alternative, a pretrained probabilistic time\nseries model that utilizes transformers with a Prob-Sparse self-attention\nmechanism is considered. The considered solution is investigated under\ndifferent time granularities with varying sequence and prediction lengths.\nConcluding this paper, we provide extensive simulation results where the\ntransformer-based solution achieved up to six percent better forecasting\naccuracy on certain traffic conditions using mean squared error as the\nperformance indicator.", "published": "2025-09-13 17:33:53", "link": "http://arxiv.org/abs/2509.10917v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "On the Impact of Downstream Tasks on Sampling and Reconstructing Noisy Graph Signals", "abstract": "We investigate graph signal reconstruction and sample selection for\nclassification tasks. We present general theoretical characterisations of\nclassification error applicable to multiple commonly used reconstruction\nmethods, and compare that to the classical reconstruction error. We demonstrate\nthe applicability of our results by using them to derive new optimal sampling\nmethods for linearized graph convolutional networks, and show improvement over\nother graph signal processing based methods.", "published": "2025-09-13 16:09:43", "link": "http://arxiv.org/abs/2509.10874v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Online simplex-structured matrix factorization", "abstract": "Simplex-structured matrix factorization (SSMF) is a common task encountered\nin signal processing and machine learning. Minimum-volume constrained unmixing\n(MVCU) algorithms are among the most widely used methods to perform this task.\nWhile MVCU algorithms generally perform well in an offline setting, their\ndirect application to online scenarios suffers from scalability limitations due\nto memory and computational demands. To overcome these limitations, this paper\nproposes an approach which can build upon any off-the-shelf MVCU algorithm to\noperate sequentially, i.e., to handle one observation at a time. The key idea\nof the proposed method consists in updating the solution of MVCU only when\nnecessary, guided by an online check of the corresponding optimization problem\nconstraints. It only stores and processes observations identified as\ninformative with respect to the geometrical constraints underlying SSMF. We\ndemonstrate the effectiveness of the approach when analyzing synthetic and real\ndatasets, showing that it achieves estimation accuracy comparable to the\noffline MVCU method upon which it relies, while significantly reducing the\ncomputational cost.", "published": "2025-09-13 15:27:02", "link": "http://arxiv.org/abs/2509.10857v1", "categories": ["eess.SP", "physics.chem-ph", "stat.ME"], "primary_category": "eess.SP"}
{"title": "Self-Calibrating Integrate-and-Fire Time Encoding Machine", "abstract": "In this paper, we introduce a novel self-calibrating integrate-and-fire time\nencoding machine (S-IF-TEM) that enables simultaneous parameter estimation and\nsignal reconstruction during sampling, thereby effectively mitigating mismatch\neffects. The proposed framework is developed over a new practical IF-TEM\n(P-IF-TEM) setting, which extends classical models by incorporating device\nmismatches and imperfections that can otherwise lead to significant\nreconstruction errors. Unlike existing IF-TEM settings, P-IF-TEM accounts for\nscenarios where (i) system parameters are inaccurately known and may vary over\ntime, (ii) the integrator discharge time after firings can vary, and (iii) the\nsampler may operate in its nonlinear region under large input dynamic ranges.\nFor this practical model, we derive sampling rate bounds and reconstruction\nconditions that ensure perfect recovery. Analytical results establish the\nconditions for perfect reconstruction under self-calibration, and evaluation\nstudies demonstrate substantial improvements - exceeding 59dB - highlighting\nthe effectiveness of the proposed approach.", "published": "2025-09-13 14:51:25", "link": "http://arxiv.org/abs/2509.10831v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Hybrid Atomic Norm Sparse/Diffuse Channel Estimation", "abstract": "In this paper, the hybrid sparse/diffuse (HSD) channel model in frequency\ndomain is proposed. Based on the structural analysis on the resolvable paths\nand diffuse scattering statistics in the channel, the Hybrid\nAtomic-Least-Squares (HALS) algorithm is designed to estimate sparse/diffuse\ncomponents with a combined atomic and l2 regularization. A theoretical analysis\nis conducted on the Lagrangian dual problem and the conditions needed to be\nsatisfied by primal and dual solutions are provided. This analysis, in turn,\nsuggests an algorithm for optimal frequency support estimation. Debiased\nmethods for improved channel estimation are provided. Given differing amounts\nof side information, performance bounds are derived in terms of a genie-aided\nestimator and constrained Cramer-Rao lower bounds (CRLB). Numerical results via\nsimulations on synthetic data as well as real experimental data validate the\nefficacy of the proposed method. There are clear tradeoffs with respect to the\nproperties of the channel with respect to performance: sparsity of specular\npaths and relative energy of diffuse components.", "published": "2025-09-13 01:18:58", "link": "http://arxiv.org/abs/2509.10770v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Real-Time Defense Against Coordinated Cyber-Physical Attacks: A Robust Constrained Reinforcement Learning Approach", "abstract": "Modern power systems face increasing vulnerability to sophisticated\ncyber-physical attacks beyond traditional N-1 contingency frameworks. Existing\nsecurity paradigms face a critical bottleneck: efficiently identifying\nworst-case scenarios and rapidly coordinating defensive responses are hindered\nby intensive computation and time delays, during which cascading failures can\npropagate. This paper presents a novel tri-level robust constrained\nreinforcement learning (RCRL) framework for robust power system security. The\nframework generates diverse system states through AC-OPF formulations,\nidentifies worst-case N-K attack scenarios for each state, and trains policies\nto mitigate these scenarios across all operating conditions without requiring\npredefined attack patterns. The framework addresses constraint satisfaction\nthrough Beta-blending projection-based feasible action mapping techniques\nduring training and primal-dual augmented Lagrangian optimization for\ndeployment. Once trained, the RCRL policy learns how to control observed\ncyber-physical attacks in real time. Validation on IEEE benchmark systems\ndemonstrates effectiveness against coordinated N-K attacks, causing widespread\ncascading failures throughout the network. The learned policy can successfully\nrespond rapidly to recover system-wide constraints back to normal within 0.21\nms inference times, establishing superior resilience for critical\ninfrastructure protection.", "published": "2025-09-13 22:49:39", "link": "http://arxiv.org/abs/2509.10999v2", "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning", "abstract": "This paper proposes the MT-DQN model, which integrates a Transformer,\nTemporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the\nchallenges of predicting user behavior and optimizing recommendation strategies\nin short-video environments. Experiments demonstrated that MT-DQN consistently\noutperforms traditional concatenated models, such as Concat-Modal, achieving an\naverage F1-score improvement of 10.97% and an average NDCG@5 improvement of\n8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN\nreduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize\nchallenges in deploying MT-DQN in real-world scenarios, such as its\ncomputational cost and latency sensitivity during online inference, which will\nbe addressed through future architectural optimization.", "published": "2025-09-13 16:28:14", "link": "http://arxiv.org/abs/2509.12269v1", "categories": ["cs.LG", "cs.IR"], "primary_category": "cs.LG"}
{"title": "A Variational Physics-Informed Neural Network Framework Using Petrov-Galerkin Method for Solving Singularly Perturbed Boundary Value Problems", "abstract": "This work proposes a Variational Physics-Informed Neural Network (VPINN)\nframework that integrates the Petrov-Galerkin formulation with deep neural\nnetworks (DNNs) for solving one-dimensional singularly perturbed boundary value\nproblems (BVPs) and parabolic partial differential equations (PDEs) involving\none or two small parameters. The method adopts a nonlinear approximation in\nwhich the trial space is defined by neural network functions, while the test\nspace is constructed from hat functions. The weak formulation is constructed\nusing localized test functions, with interface penalty terms introduced to\nenhance numerical stability and accurately capture boundary layers. Dirichlet\nboundary conditions are imposed via hard constraints, and source terms are\ncomputed using automatic differentiation. Numerical experiments on benchmark\nproblems demonstrate the effectiveness of the proposed method, showing\nsignificantly improved accuracy in both the $L_2$ and maximum norms compared to\nthe standard VPINN approach for one-dimensional singularly perturbed\ndifferential equations (SPDEs).", "published": "2025-09-13 18:25:00", "link": "http://arxiv.org/abs/2509.12271v1", "categories": ["math.NA", "cs.AI", "cs.NA", "34D15, 35B25, 65M12, 68T07"], "primary_category": "math.NA"}
{"title": "A Traditional Approach to Symbolic Piano Continuation", "abstract": "We present a traditional approach to symbolic piano music continuation for\nthe MIREX 2025 Symbolic Music Generation challenge. While computational music\ngeneration has recently focused on developing large foundation models with\nsophisticated architectural modifications, we argue that simpler approaches\nremain more effective for constrained, single-instrument tasks. We thus return\nto a simple, unaugmented next-token-prediction objective on tokenized raw MIDI,\naiming to outperform large foundation models by using better data and better\nfundamentals. We release model weights and code at\nhttps://github.com/christianazinn/mirex2025.", "published": "2025-09-13 14:22:11", "link": "http://arxiv.org/abs/2509.12267v1", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
