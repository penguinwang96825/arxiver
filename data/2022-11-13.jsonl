{"title": "Conversational Pattern Mining using Motif Detection", "abstract": "The subject of conversational mining has become of great interest recently\ndue to the explosion of social and other online media. Supplementing this\nexplosion of text is the advancement in pre-trained language models which have\nhelped us to leverage these sources of information. An interesting domain to\nanalyse is conversations in terms of complexity and value. Complexity arises\ndue to the fact that a conversation can be asynchronous and can involve\nmultiple parties. It is also computationally intensive to process. We use\nunsupervised methods in our work in order to develop a conversational pattern\nmining technique which does not require time consuming, knowledge demanding and\nresource intensive labelling exercises. The task of identifying repeating\npatterns in sequences is well researched in the Bioinformatics field. In our\nwork, we adapt this to the field of Natural Language Processing and make\nseveral extensions to a motif detection algorithm. In order to demonstrate the\napplication of the algorithm on a dynamic, real world data set; we extract\nmotifs from an open-source film script data source. We run an exploratory\ninvestigation into the types of motifs we are able to mine.", "published": "2022-11-13 08:30:43", "link": "http://arxiv.org/abs/2211.06846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Xu at SemEval-2022 Task 4: Pre-BERT Neural Network Methods vs Post-BERT\n  RoBERTa Approach for Patronizing and Condescending Language Detection", "abstract": "This paper describes my participation in the SemEval-2022 Task 4: Patronizing\nand Condescending Language Detection. I participate in both subtasks:\nPatronizing and Condescending Language (PCL) Identification and Patronizing and\nCondescending Language Categorization, with the main focus put on subtask 1.\nThe experiments compare pre-BERT neural network (NN) based systems against\npost-BERT pretrained language model RoBERTa. This research finds NN-based\nsystems in the experiments perform worse on the task compared to the pretrained\nlanguage models. The top-performing RoBERTa system is ranked 26 out of 78 teams\n(F1-score: 54.64) in subtask 1, and 23 out of 49 teams (F1-score: 30.03) in\nsubtask 2.", "published": "2022-11-13 10:59:45", "link": "http://arxiv.org/abs/2211.06874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mOKB6: A Multilingual Open Knowledge Base Completion Benchmark", "abstract": "Automated completion of open knowledge bases (Open KBs), which are\nconstructed from triples of the form (subject phrase, relation phrase, object\nphrase), obtained via open information extraction (Open IE) system, are useful\nfor discovering novel facts that may not be directly present in the text.\nHowever, research in Open KB completion (Open KBC) has so far been limited to\nresource-rich languages like English. Using the latest advances in multilingual\nOpen IE, we construct the first multilingual Open KBC dataset, called mOKB6,\ncontaining facts from Wikipedia in six languages (including English). Improving\nthe previous Open KB construction pipeline by doing multilingual coreference\nresolution and keeping only entity-linked triples, we create a dense Open KB.\nWe experiment with several models for the task and observe a consistent benefit\nof combining languages with the help of shared embedding space as well as\ntranslations of facts. We also observe that current multilingual models\nstruggle to remember facts seen in languages of different scripts.", "published": "2022-11-13 17:10:49", "link": "http://arxiv.org/abs/2211.06959v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BiFSMNv2: Pushing Binary Neural Networks for Keyword Spotting to\n  Real-Network Performance", "abstract": "Deep neural networks, such as the Deep-FSMN, have been widely studied for\nkeyword spotting (KWS) applications while suffering expensive computation and\nstorage. Therefore, network compression technologies like binarization are\nstudied to deploy KWS models on edge. In this paper, we present a strong yet\nefficient binary neural network for KWS, namely BiFSMNv2, pushing it to the\nreal-network accuracy performance. First, we present a Dual-scale Thinnable\n1-bit-Architecture to recover the representation capability of the binarized\ncomputation units by dual-scale activation binarization and liberate the\nspeedup potential from an overall architecture perspective. Second, we also\nconstruct a Frequency Independent Distillation scheme for KWS\nbinarization-aware training, which distills the high and low-frequency\ncomponents independently to mitigate the information mismatch between\nfull-precision and binarized representations. Moreover, we propose the Learning\nPropagation Binarizer, a general and efficient binarizer that enables the\nforward and backward propagation of binary KWS networks to be continuously\nimproved through learning. We implement and deploy the BiFSMNv2 on ARMv8\nreal-world hardware with a novel Fast Bitwise Computation Kernel, which is\nproposed to fully utilize registers and increase instruction throughput.\nComprehensive experiments show our BiFSMNv2 outperforms existing binary\nnetworks for KWS by convincing margins across different datasets and achieves\ncomparable accuracy with the full-precision networks (only a tiny 1.51% drop on\nSpeech Commands V1-12). We highlight that benefiting from the compact\narchitecture and optimized hardware kernel, BiFSMNv2 can achieve an impressive\n25.1x speedup and 20.2x storage-saving on edge hardware.", "published": "2022-11-13 18:31:45", "link": "http://arxiv.org/abs/2211.06987v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language\n  Models at Almost No Cost", "abstract": "Large pre-trained models have revolutionized natural language processing\n(NLP) research and applications, but high training costs and limited data\nresources have prevented their benefits from being shared equally amongst\nspeakers of all the world's languages. To address issues of cross-linguistic\naccess to such models and reduce energy consumption for sustainability during\nlarge-scale model training, this study proposes an effective and\nenergy-efficient framework called GreenPLM that uses bilingual lexicons to\ndirectly \"translate\" pre-trained language models of one language into another\nat almost no additional cost. We validate this approach in 18 languages' BERT\nmodels and show that this framework is comparable to, if not better than, other\nheuristics with high training costs. In addition, given lightweight continued\npre-training on limited data where available, this framework outperforms the\noriginal monolingual language models in six out of seven tested languages with\nup to 200x less pre-training efforts. Aiming at the Leave No One Behind\nPrinciple (LNOB), our approach manages to reduce inequalities between languages\nand energy consumption greatly. We make our codes and models publicly available\nhere: \\url{https://github.com/qcznlp/GreenPLMs}", "published": "2022-11-13 18:59:15", "link": "http://arxiv.org/abs/2211.06993v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Classifier Aligns Better with Physician Word Sensitivity\n  than XGBoost on Readmission Prediction", "abstract": "Traditional evaluation metrics for classification in natural language\nprocessing such as accuracy and area under the curve fail to differentiate\nbetween models with different predictive behaviors despite their similar\nperformance metrics. We introduce sensitivity score, a metric that scrutinizes\nmodels' behaviors at the vocabulary level to provide insights into disparities\nin their decision-making logic. We assess the sensitivity score on a set of\nrepresentative words in the test set using two classifiers trained for hospital\nreadmission classification with similar performance statistics. Our experiments\ncompare the decision-making logic of clinicians and classifiers based on rank\ncorrelations of sensitivity scores. The results indicate that the language\nmodel's sensitivity score aligns better with the professionals than the xgboost\nclassifier on tf-idf embeddings, which suggests that xgboost uses some spurious\nfeatures. Overall, this metric offers a novel perspective on assessing models'\nrobustness by quantifying their discrepancy with professional opinions. Our\ncode is available on GitHub (https://github.com/nyuolab/Model_Sensitivity).", "published": "2022-11-13 23:59:11", "link": "http://arxiv.org/abs/2211.07047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinTech for Social Good: A Research Agenda from NLP Perspective", "abstract": "Making our research results positively impact on society and environment is\none of the goals our community has been pursuing recently. Although financial\ntechnology (FinTech) is one of the popular application fields, we notice that\nthere is no discussion on how NLP can help in FinTech for the social good. When\nmentioning FinTech for social good, people are talking about financial\ninclusion and green finance. However, the role of NLP in these directions only\ngets limited discussions. To fill this gap, this paper shares our idea of how\nwe can use NLP in FinTech for social good. We hope readers can rethink the\nrelationship between finance and NLP based on our sharing, and further join us\nin improving the financial literacy of individual investors and improving the\nsupports for impact investment.", "published": "2022-11-13 22:29:41", "link": "http://arxiv.org/abs/2211.06431v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Large-Scale Bidirectional Training for Zero-Shot Image Captioning", "abstract": "When trained on large-scale datasets, image captioning models can understand\nthe content of images from a general domain but often fail to generate\naccurate, detailed captions. To improve performance, pretraining-and-finetuning\nhas been a key strategy for image captioning. However, we find that large-scale\nbidirectional training between image and text enables zero-shot image\ncaptioning. In this paper, we introduce Bidirectional Image Text Training in\nlargER Scale, BITTERS, an efficient training and inference framework for\nzero-shot image captioning. We also propose a new evaluation benchmark which\ncomprises of high quality datasets and an extensive set of metrics to properly\nevaluate zero-shot captioning accuracy and societal bias. We additionally\nprovide an efficient finetuning approach for keyword extraction. We show that\ncareful selection of large-scale training set and model architecture is the key\nto achieving zero-shot image captioning.", "published": "2022-11-13 00:09:36", "link": "http://arxiv.org/abs/2211.06774v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Textual Data Augmentation for Patient Outcomes Prediction", "abstract": "Deep learning models have demonstrated superior performance in various\nhealthcare applications. However, the major limitation of these deep models is\nusually the lack of high-quality training data due to the private and sensitive\nnature of this field. In this study, we propose a novel textual data\naugmentation method to generate artificial clinical notes in patients'\nElectronic Health Records (EHRs) that can be used as additional training data\nfor patient outcomes prediction. Essentially, we fine-tune the generative\nlanguage model GPT-2 to synthesize labeled text with the original training\ndata. More specifically, We propose a teacher-student framework where we first\npre-train a teacher model on the original data, and then train a student model\non the GPT-augmented data under the guidance of the teacher. We evaluate our\nmethod on the most common patient outcome, i.e., the 30-day readmission rate.\nThe experimental results show that deep models can improve their predictive\nperformance with the augmented data, indicating the effectiveness of the\nproposed architecture.", "published": "2022-11-13 01:07:23", "link": "http://arxiv.org/abs/2211.06778v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FPT: Improving Prompt Tuning Efficiency via Progressive Training", "abstract": "Recently, prompt tuning (PT) has gained increasing attention as a\nparameter-efficient way of tuning pre-trained language models (PLMs). Despite\nextensively reducing the number of tunable parameters and achieving satisfying\nperformance, PT is training-inefficient due to its slow convergence. To improve\nPT's training efficiency, we first make some novel observations about the\nprompt transferability of \"partial PLMs\", which are defined by compressing a\nPLM in depth or width. We observe that the soft prompts learned by different\npartial PLMs of various sizes are similar in the parameter space, implying that\nthese soft prompts could potentially be transferred among partial PLMs.\nInspired by these observations, we propose Fast Prompt Tuning (FPT), which\nstarts by conducting PT using a small-scale partial PLM, and then progressively\nexpands its depth and width until the full-model size. After each expansion, we\nrecycle the previously learned soft prompts as initialization for the enlarged\npartial PLM and then proceed PT. We demonstrate the feasibility of FPT on 5\ntasks and show that FPT could save over 30% training computations while\nachieving comparable performance.", "published": "2022-11-13 08:00:29", "link": "http://arxiv.org/abs/2211.06840v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WR-ONE2SET: Towards Well-Calibrated Keyphrase Generation", "abstract": "Keyphrase generation aims to automatically generate short phrases summarizing\nan input document. The recently emerged ONE2SET paradigm (Ye et al., 2021)\ngenerates keyphrases as a set and has achieved competitive performance.\nNevertheless, we observe serious calibration errors outputted by ONE2SET,\nespecially in the over-estimation of $\\varnothing$ token (means \"no\ncorresponding keyphrase\"). In this paper, we deeply analyze this limitation and\nidentify two main reasons behind: 1) the parallel generation has to introduce\nexcessive $\\varnothing$ as padding tokens into training instances; and 2) the\ntraining mechanism assigning target to each slot is unstable and further\naggravates the $\\varnothing$ token over-estimation. To make the model\nwell-calibrated, we propose WR-ONE2SET which extends ONE2SET with an adaptive\ninstance-level cost Weighting strategy and a target Re-assignment mechanism.\nThe former dynamically penalizes the over-estimated slots for different\ninstances thus smoothing the uneven training distribution. The latter refines\nthe original inappropriate assignment and reduces the supervisory signals of\nover-estimated slots. Experimental results on commonly-used datasets\ndemonstrate the effectiveness and generality of our proposed paradigm.", "published": "2022-11-13 09:56:24", "link": "http://arxiv.org/abs/2211.06862v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models Meet Harry Potter: A Bilingual Dataset for\n  Aligning Dialogue Agents with Characters", "abstract": "In recent years, Dialogue-style Large Language Models (LLMs) such as ChatGPT\nand GPT4 have demonstrated immense potential in constructing open-domain\ndialogue agents. However, aligning these agents with specific characters or\nindividuals remains a considerable challenge due to the complexities of\ncharacter representation and the lack of comprehensive annotations. In this\npaper, we introduce the Harry Potter Dialogue (HPD) dataset, designed to\nadvance the study of dialogue agents and character alignment. The dataset\nencompasses all dialogue sessions (in both English and Chinese) from the Harry\nPotter series and is annotated with vital background information, including\ndialogue scenes, speakers, character relationships, and attributes. These\nextensive annotations may empower LLMs to unlock character-driven dialogue\ncapabilities. Furthermore, it can serve as a universal benchmark for evaluating\nhow well can a LLM aligning with a specific character. We benchmark LLMs on HPD\nusing both fine-tuning and in-context learning settings. Evaluation results\nreveal that although there is substantial room for improvement in generating\nhigh-quality, character-aligned responses, the proposed dataset is valuable in\nguiding models toward responses that better align with the character of Harry\nPotter.", "published": "2022-11-13 10:16:39", "link": "http://arxiv.org/abs/2211.06869v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying syntax similarity with a polynomial representation of\n  dependency trees", "abstract": "We introduce a graph polynomial that distinguishes tree structures to\nrepresent dependency grammar and a measure based on the polynomial\nrepresentation to quantify syntax similarity. The polynomial encodes accurate\nand comprehensive information about the dependency structure and dependency\nrelations of words in a sentence. We apply the polynomial-based methods to\nanalyze sentences in the Parallel Universal Dependencies treebanks.\nSpecifically, we compare the syntax of sentences and their translations in\ndifferent languages, and we perform a syntactic typology study of available\nlanguages in the Parallel Universal Dependencies treebanks. We also demonstrate\nand discuss the potential of the methods in measuring syntax diversity of\ncorpora.", "published": "2022-11-13 19:55:08", "link": "http://arxiv.org/abs/2211.07005v1", "categories": ["cs.CL", "math.CO"], "primary_category": "cs.CL"}
{"title": "World Knowledge in Multiple Choice Reading Comprehension", "abstract": "Recently it has been shown that without any access to the contextual passage,\nmultiple choice reading comprehension (MCRC) systems are able to answer\nquestions significantly better than random on average. These systems use their\naccumulated \"world knowledge\" to directly answer questions, rather than using\ninformation from the passage. This paper examines the possibility of exploiting\nthis observation as a tool for test designers to ensure that the use of \"world\nknowledge\" is acceptable for a particular set of questions. We propose\ninformation-theory based metrics that enable the level of \"world knowledge\"\nexploited by systems to be assessed. Two metrics are described: the expected\nnumber of options, which measures whether a passage-free system can identify\nthe answer a question using world knowledge; and the contextual mutual\ninformation, which measures the importance of context for a given question. We\ndemonstrate that questions with low expected number of options, and hence\nanswerable by the shortcut system, are often similarly answerable by humans\nwithout context. This highlights that the general knowledge 'shortcuts' could\nbe equally used by exam candidates, and that our proposed metrics may be\nhelpful for future test designers to monitor the quality of questions.", "published": "2022-11-13 23:02:09", "link": "http://arxiv.org/abs/2211.07040v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TIER-A: Denoising Learning Framework for Information Extraction", "abstract": "With the development of deep neural language models, great progress has been\nmade in information extraction recently. However, deep learning models often\noverfit on noisy data points, leading to poor performance. In this work, we\nexamine the role of information entropy in the overfitting process and draw a\nkey insight that overfitting is a process of overconfidence and entropy\ndecreasing. Motivated by such properties, we propose a simple yet effective\nco-regularization joint-training framework TIER-A, Aggregation Joint-training\nFramework with Temperature Calibration and Information Entropy Regularization.\nOur framework consists of several neural models with identical structures.\nThese models are jointly trained and we avoid overfitting by introducing\ntemperature and information entropy regularization. Extensive experiments on\ntwo widely-used but noisy datasets, TACRED and CoNLL03, demonstrate the\ncorrectness of our assumption and the effectiveness of our framework.", "published": "2022-11-13 11:28:56", "link": "http://arxiv.org/abs/2211.11527v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Autovocoder: Fast Waveform Generation from a Learned Speech\n  Representation using Differentiable Digital Signal Processing", "abstract": "Most state-of-the-art Text-to-Speech systems use the mel-spectrogram as an\nintermediate representation, to decompose the task into acoustic modelling and\nwaveform generation.\n  A mel-spectrogram is extracted from the waveform by a simple, fast DSP\noperation, but generating a high-quality waveform from a mel-spectrogram\nrequires computationally expensive machine learning: a neural vocoder. Our\nproposed ``autovocoder'' reverses this arrangement. We use machine learning to\nobtain a representation that replaces the mel-spectrogram, and that can be\ninverted back to a waveform using simple, fast operations including a\ndifferentiable implementation of the inverse STFT.\n  The autovocoder generates a waveform 5 times faster than the DSP-based\nGriffin-Lim algorithm, and 14 times faster than the neural vocoder HiFi-GAN. We\nprovide perceptual listening test results to confirm that the speech is of\ncomparable quality to HiFi-GAN in the copy synthesis task.", "published": "2022-11-13 18:37:57", "link": "http://arxiv.org/abs/2211.06989v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "OverFlow: Putting flows on top of neural transducers for better TTS", "abstract": "Neural HMMs are a type of neural transducer recently proposed for\nsequence-to-sequence modelling in text-to-speech. They combine the best\nfeatures of classic statistical speech synthesis and modern neural TTS,\nrequiring less data and fewer training updates, and are less prone to gibberish\noutput caused by neural attention failures. In this paper, we combine neural\nHMM TTS with normalising flows for describing the highly non-Gaussian\ndistribution of speech acoustics. The result is a powerful, fully probabilistic\nmodel of durations and acoustics that can be trained using exact maximum\nlikelihood. Experiments show that a system based on our proposal needs fewer\nupdates than comparable methods to produce accurate pronunciations and a\nsubjective speech quality close to natural speech. Please see\nhttps://shivammehta25.github.io/OverFlow/ for audio examples and code.", "published": "2022-11-13 12:53:05", "link": "http://arxiv.org/abs/2211.06892v2", "categories": ["eess.AS", "cs.HC", "cs.LG", "cs.SD", "68T07", "I.2.7; I.2.6; G.3; H.5.5"], "primary_category": "eess.AS"}
{"title": "FullPack: Full Vector Utilization for Sub-Byte Quantized Inference on\n  General Purpose CPUs", "abstract": "Although prior art has demonstrated negligible accuracy drop in sub-byte\nquantization -- where weights and/or activations are represented by less than 8\nbits -- popular SIMD instructions of CPUs do not natively support these\ndatatypes. While recent methods, such as ULPPACK, are already using sub-byte\nquantization on general-purpose CPUs with vector units, they leave out several\nempty bits between the sub-byte values in memory and in vector registers to\navoid overflow to the neighbours during the operations. This results in memory\nfootprint and bandwidth-usage inefficiencies and suboptimal performance. In\nthis paper, we present memory layouts for storing, and mechanisms for\nprocessing sub-byte (4-, 2-, or 1-bit) models that utilize all the bits in the\nmemory as well as in the vector registers for the actual data. We provide\ncompute kernels for the proposed layout for the GEMV (GEneral Matrix-Vector\nmultiplication) operations between weights and activations of different\ndatatypes (e.g., 8-bit activations and 4-bit weights). For evaluation, we\nextended the TFLite package and added our methods to it, then ran the models on\nthe cycle-accurate gem5 simulator to compare detailed memory and CPU cycles of\neach method. We compare against nine other methods that are actively used in\nproduction including GEMLOWP, Ruy, XNNPack, and ULPPACK. Furthermore, we\nexplore the effect of different input and output sizes of deep learning layers\non the performance of our proposed method. Experimental results show 0.96-2.1x\nspeedup for small sizes and 1.2-6.7x speedup for mid to large sizes. Applying\nour proposal to a real-world speech recognition model, Mozilla DeepSpeech, we\nproved that our method achieves 1.56-2.11x end-to-end speedup compared to the\nstate-of-the-art, depending on the bit-width employed.", "published": "2022-11-13 18:13:31", "link": "http://arxiv.org/abs/2211.06982v2", "categories": ["cs.PF", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.PF"}
