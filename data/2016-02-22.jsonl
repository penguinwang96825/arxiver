{"title": "Semi-supervised Clustering for Short Text via Deep Representation\n  Learning", "abstract": "In this work, we propose a semi-supervised method for short text clustering,\nwhere we represent texts as distributed vectors with neural networks, and use a\nsmall amount of labeled data to specify our intention for clustering. We design\na novel objective to combine the representation learning process and the\nk-means clustering process together, and optimize the objective with both\nlabeled data and unlabeled data iteratively until convergence through three\nsteps: (1) assign each short text to its nearest centroid based on its\nrepresentation from the current neural networks; (2) re-estimate the cluster\ncentroids based on cluster assignments from step (1); (3) update neural\nnetworks according to the objective by keeping centroids and cluster\nassignments fixed. Experimental results on four datasets show that our method\nworks significantly better than several other text clustering methods.", "published": "2016-02-22 14:55:26", "link": "http://arxiv.org/abs/1602.06797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empath: Understanding Topic Signals in Large-Scale Text", "abstract": "Human language is colored by a broad range of topics, but existing text\nanalysis tools only focus on a small number of them. We present Empath, a tool\nthat can generate and validate new lexical categories on demand from a small\nset of seed terms (like \"bleed\" and \"punch\" to generate the category violence).\nEmpath draws connotations between words and phrases by deep learning a neural\nembedding across more than 1.8 billion words of modern fiction. Given a small\nset of seed words that characterize a category, Empath uses its neural\nembedding to discover new related terms, then validates the category with a\ncrowd-powered filter. Empath also analyzes text across 200 built-in,\npre-validated categories we have generated from common topics in our web\ndataset, like neglect, government, and social media. We show that Empath's\ndata-driven, human validated categories are highly correlated (r=0.906) with\nsimilar categories in LIWC.", "published": "2016-02-22 21:47:43", "link": "http://arxiv.org/abs/1602.06979v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Temporal Network Analysis of Literary Texts", "abstract": "We study temporal networks of characters in literature focusing on \"Alice's\nAdventures in Wonderland\" (1865) by Lewis Carroll and the anonymous \"La Chanson\nde Roland\" (around 1100). The former, one of the most influential pieces of\nnonsense literature ever written, describes the adventures of Alice in a\nfantasy world with logic plays interspersed along the narrative. The latter, a\nsong of heroic deeds, depicts the Battle of Roncevaux in 778 A.D. during\nCharlemagne's campaign on the Iberian Peninsula. We apply methods recently\ndeveloped by Taylor and coworkers \\cite{Taylor+2015} to find time-averaged\neigenvector centralities, Freeman indices and vitalities of characters. We show\nthat temporal networks are more appropriate than static ones for studying\nstories, as they capture features that the time-independent approaches fail to\nyield.", "published": "2016-02-22 17:55:04", "link": "http://arxiv.org/abs/1602.07275v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "From quantum foundations via natural language meaning to a theory of\n  everything", "abstract": "In this paper we argue for a paradigmatic shift from `reductionism' to\n`togetherness'. In particular, we show how interaction between systems in\nquantum theory naturally carries over to modelling how word meanings interact\nin natural language. Since meaning in natural language, depending on the\nsubject domain, encompasses discussions within any scientific discipline, we\nobtain a template for theories such as social interaction, animal behaviour,\nand many others.", "published": "2016-02-22 16:17:54", "link": "http://arxiv.org/abs/1602.07618v1", "categories": ["cs.CL", "quant-ph"], "primary_category": "cs.CL"}
{"title": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using\n  Stacked Bottleneck Features and Minimum Generation Error Training", "abstract": "We propose two novel techniques --- stacking bottleneck features and minimum\ngeneration error training criterion --- to improve the performance of deep\nneural network (DNN)-based speech synthesis. The techniques address the related\nissues of frame-by-frame independence and ignorance of the relationship between\nstatic and dynamic features, within current typical DNN-based synthesis\nframeworks. Stacking bottleneck features, which are an acoustically--informed\nlinguistic representation, provides an efficient way to include more detailed\nlinguistic context at the input. The minimum generation error training\ncriterion minimises overall output trajectory error across an utterance, rather\nthan minimising the error per frame independently, and thus takes into account\nthe interaction between static and dynamic features. The two techniques can be\neasily combined to further improve performance. We present both objective and\nsubjective results that demonstrate the effectiveness of the proposed\ntechniques. The subjective results show that combining the two techniques leads\nto significantly more natural synthetic speech than from conventional DNN or\nlong short-term memory (LSTM) recurrent neural network (RNN) systems.", "published": "2016-02-22 11:11:04", "link": "http://arxiv.org/abs/1602.06727v3", "categories": ["cs.SD", "cs.CL", "cs.NE"], "primary_category": "cs.SD"}
{"title": "Blind score normalization method for PLDA based speaker recognition", "abstract": "Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art\nmethod for modeling $i$-vector space in speaker recognition task. However the\nperformance degradation is observed if enrollment data size differs from one\nspeaker to another. This paper presents a solution to such problem by\nintroducing new PLDA scoring normalization technique. Normalization parameters\nare derived in a blind way, so that, unlike traditional \\textit{ZT-norm}, no\nextra development data is required. Moreover, proposed method has shown to be\noptimal in terms of detection cost function. The experiments conducted on NIST\nSRE 2014 database demonstrate an improved accuracy in a mixed enrollment number\ncondition.", "published": "2016-02-22 21:22:49", "link": "http://arxiv.org/abs/1602.06967v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "I.5.1; I.5.2; G.3"], "primary_category": "cs.CL"}
