{"title": "Deep Learning-based Sentiment Analysis in Persian Language", "abstract": "Recently, there has been a growing interest in the use of deep learning\ntechniques for tasks in natural language processing (NLP), with sentiment\nanalysis being one of the most challenging areas, particularly in the Persian\nlanguage. The vast amounts of content generated by Persian users on thousands\nof websites, blogs, and social networks such as Telegram, Instagram, and\nTwitter present a rich resource of information. Deep learning techniques have\nbecome increasingly favored for extracting insights from this extensive pool of\nraw data, although they face several challenges. In this study, we introduced\nand implemented a hybrid deep learning-based model for sentiment analysis,\nusing customer review data from the Digikala Online Retailer website. We\nemployed a variety of deep learning networks and regularization techniques as\nclassifiers. Ultimately, our hybrid approach yielded an impressive performance,\nachieving an F1 score of 78.3 across three sentiment categories: positive,\nnegative, and neutral.", "published": "2024-03-17 03:15:29", "link": "http://arxiv.org/abs/2403.11069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HarmPot: An Annotation Framework for Evaluating Offline Harm Potential\n  of Social Media Text", "abstract": "In this paper, we discuss the development of an annotation schema to build\ndatasets for evaluating the offline harm potential of social media texts. We\ndefine \"harm potential\" as the potential for an online public post to cause\nreal-world physical harm (i.e., violence). Understanding that real-world\nviolence is often spurred by a web of triggers, often combining several online\ntactics and pre-existing intersectional fissures in the social milieu, to\nresult in targeted physical violence, we do not focus on any single divisive\naspect (i.e., caste, gender, religion, or other identities of the victim and\nperpetrators) nor do we focus on just hate speech or mis/dis-information.\nRather, our understanding of the intersectional causes of such triggers focuses\nour attempt at measuring the harm potential of online content, irrespective of\nwhether it is hateful or not. In this paper, we discuss the development of a\nframework/annotation schema that allows annotating the data with different\naspects of the text including its socio-political grounding and intent of the\nspeaker (as expressed through mood and modality) that together contribute to it\nbeing a trigger for offline harm. We also give a comparative analysis and\nmapping of our framework with some of the existing frameworks.", "published": "2024-03-17 06:23:25", "link": "http://arxiv.org/abs/2403.11108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Granular Change Accuracy: A More Accurate Performance Metric for\n  Dialogue State Tracking", "abstract": "Current metrics for evaluating Dialogue State Tracking (DST) systems exhibit\nthree primary limitations. They: i) erroneously presume a uniform distribution\nof slots throughout the dialog, ii) neglect to assign partial scores for\nindividual turns, iii) frequently overestimate or underestimate performance by\nrepeatedly counting the models' successful or failed predictions. To address\nthese shortcomings, we introduce a novel metric: Granular Change Accuracy\n(GCA). GCA focuses on evaluating the predicted changes in dialogue state over\nthe entire dialogue history. Benchmarking reveals that GCA effectively reduces\nbiases arising from distribution uniformity and the positioning of errors\nacross turns, resulting in a more precise evaluation. Notably, we find that\nthese biases are particularly pronounced when evaluating few-shot or zero-shot\ntrained models, becoming even more evident as the model's error rate increases.\nHence, GCA offers significant promise, particularly for assessing models\ntrained with limited resources. Our GCA implementation is a useful addition to\nthe pool of DST metrics.", "published": "2024-03-17 07:07:44", "link": "http://arxiv.org/abs/2403.11123v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'\n  API Invocation Capabilities", "abstract": "With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.", "published": "2024-03-17 07:34:12", "link": "http://arxiv.org/abs/2403.11128v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Event Causality Identification with Rationale and\n  Structure-Aware Causal Question Answering", "abstract": "Document-level Event Causality Identification (DECI) aims to identify causal\nrelations between two events in documents. Recent research tends to use\npre-trained language models to generate the event causal relations. Whereas,\nthese methods are prone to the errors of sequential generation due to multiple\nevents in a document. Moreover, the potential structures such as event\ncoreference and related causal chain are neglected. In this paper, we propose a\nmulti-task learning framework to enhance event causality identification with\nrationale and structure-aware causal question answering. Specifically, the DECI\ntask is transformed into multiple-choice question answering, and the causes and\neffects of the questioned event are generated with large language models. In\naddition, we generate the rationales to explain why these events have causal\nrelations. Moreover, we construct an event structure graph, which models the\nmulti-hop potential relations for causal reasoning of the current event.\nExperiments on two benchmark datasets show the great advantages of our proposed\napproach compared to the state-of-the-art methods. Moreover, we conduct both\nquantitative and qualitative analyses, which shed light on why each component\nof our approach can lead to great improvements.", "published": "2024-03-17 07:41:58", "link": "http://arxiv.org/abs/2403.11129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced\n  Arabic Language Models", "abstract": "This paper presents a comprehensive examination of the impact of tokenization\nstrategies and vocabulary sizes on the performance of Arabic language models in\ndownstream natural language processing tasks. Our investigation focused on the\neffectiveness of four tokenizers across various tasks, including News\nClassification, Hate Speech Detection, Sentiment Analysis, and Natural Language\nInference. Leveraging a diverse set of vocabulary sizes, we scrutinize the\nintricate interplay between tokenization approaches and model performance. The\nresults reveal that Byte Pair Encoding (BPE) with Farasa outperforms other\nstrategies in multiple tasks, underscoring the significance of morphological\nanalysis in capturing the nuances of the Arabic language. However, challenges\narise in sentiment analysis, where dialect specific segmentation issues impact\nmodel efficiency. Computational efficiency analysis demonstrates the stability\nof BPE with Farasa, suggesting its practical viability. Our study uncovers\nlimited impacts of vocabulary size on model performance while keeping the model\nsize unchanged. This is challenging the established beliefs about the\nrelationship between vocabulary, model size, and downstream tasks, emphasizing\nthe need for the study of models' size and their corresponding vocabulary size\nto generalize across domains and mitigate biases, particularly in dialect based\ndatasets. Paper's recommendations include refining tokenization strategies to\naddress dialect challenges, enhancing model robustness across diverse\nlinguistic contexts, and expanding datasets to encompass the rich dialect based\nArabic. This work not only advances our understanding of Arabic language models\nbut also lays the foundation for responsible and ethical developments in\nnatural language processing technologies tailored to the intricacies of the\nArabic language.", "published": "2024-03-17 07:44:44", "link": "http://arxiv.org/abs/2403.11130v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Challenge Dataset and Effective Models for Conversational Stance\n  Detection", "abstract": "Previous stance detection studies typically concentrate on evaluating stances\nwithin individual instances, thereby exhibiting limitations in effectively\nmodeling multi-party discussions concerning the same specific topic, as\nnaturally transpire in authentic social media interactions. This constraint\narises primarily due to the scarcity of datasets that authentically replicate\nreal social media contexts, hindering the research progress of conversational\nstance detection. In this paper, we introduce a new multi-turn conversation\nstance detection dataset (called \\textbf{MT-CSD}), which encompasses multiple\ntargets for conversational stance detection. To derive stances from this\nchallenging dataset, we propose a global-local attention network\n(\\textbf{GLAN}) to address both long and short-range dependencies inherent in\nconversational data. Notably, even state-of-the-art stance detection methods,\nexemplified by GLAN, exhibit an accuracy of only 50.47\\%, highlighting the\npersistent challenges in conversational stance detection. Furthermore, our\nMT-CSD dataset serves as a valuable resource to catalyze advancements in\ncross-domain stance detection, where a classifier is adapted from a different\nyet related target. We believe that MT-CSD will contribute to advancing\nreal-world applications of stance detection research. Our source code, data,\nand models are available at \\url{https://github.com/nfq729/MT-CSD}.", "published": "2024-03-17 08:51:01", "link": "http://arxiv.org/abs/2403.11145v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoding Continuous Character-based Language from Non-invasive Brain\n  Recordings", "abstract": "Deciphering natural language from brain activity through non-invasive devices\nremains a formidable challenge. Previous non-invasive decoders either require\nmultiple experiments with identical stimuli to pinpoint cortical regions and\nenhance signal-to-noise ratios in brain activity, or they are limited to\ndiscerning basic linguistic elements such as letters and words. We propose a\nnovel approach to decoding continuous language from single-trial non-invasive\nfMRI recordings, in which a three-dimensional convolutional network augmented\nwith information bottleneck is developed to automatically identify responsive\nvoxels to stimuli, and a character-based decoder is designed for the semantic\nreconstruction of continuous language characterized by inherent character\nstructures. The resulting decoder can produce intelligible textual sequences\nthat faithfully capture the meaning of perceived speech both within and across\nsubjects, while existing decoders exhibit significantly inferior performance in\ncross-subject contexts. The ability to decode continuous language from single\ntrials across subjects demonstrates the promising applications of non-invasive\nlanguage brain-computer interfaces in both healthcare and neuroscience.", "published": "2024-03-17 12:12:33", "link": "http://arxiv.org/abs/2403.11183v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRELM: Towards Robust and Efficient Pre-training for Knowledge-Enhanced\n  Language Models", "abstract": "KEPLMs are pre-trained models that utilize external knowledge to enhance\nlanguage understanding. Previous language models facilitated knowledge\nacquisition by incorporating knowledge-related pre-training tasks learned from\nrelation triples in knowledge graphs. However, these models do not prioritize\nlearning embeddings for entity-related tokens. Moreover, updating the entire\nset of parameters in KEPLMs is computationally demanding. This paper introduces\nTRELM, a Robust and Efficient Pre-training framework for Knowledge-Enhanced\nLanguage Models. We observe that entities in text corpora usually follow the\nlong-tail distribution, where the representations of some entities are\nsuboptimally optimized and hinder the pre-training process for KEPLMs. To\ntackle this, we employ a robust approach to inject knowledge triples and employ\na knowledge-augmented memory bank to capture valuable information. Furthermore,\nupdating a small subset of neurons in the feed-forward networks (FFNs) that\nstore factual knowledge is both sufficient and efficient. Specifically, we\nutilize dynamic knowledge routing to identify knowledge paths in FFNs and\nselectively update parameters during pre-training. Experimental results show\nthat TRELM reduces pre-training time by at least 50% and outperforms other\nKEPLMs in knowledge probing tasks and multiple knowledge-aware language\nunderstanding tasks.", "published": "2024-03-17 13:04:35", "link": "http://arxiv.org/abs/2403.11203v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart\n  Summarization", "abstract": "Data visualization serves as a critical means for presenting data and mining\nits valuable insights. The task of chart summarization, through natural\nlanguage processing techniques, facilitates in-depth data analysis of charts.\nHowever, there still are notable deficiencies in terms of visual-language\nmatching and reasoning ability for existing approaches. To address these\nlimitations, this study constructs a large-scale dataset of comprehensive\nchart-caption pairs and fine-tuning instructions on each chart. Thanks to the\nbroad coverage of various topics and visual styles within this dataset, better\nmatching degree can be achieved from the view of training data. Moreover, we\npropose an innovative chart summarization method, ChartThinker, which\nsynthesizes deep analysis based on chains of thought and strategies of context\nretrieval, aiming to improve the logical coherence and accuracy of the\ngenerated summaries. Built upon the curated datasets, our trained model\nconsistently exhibits superior performance in chart summarization tasks,\nsurpassing 8 state-of-the-art models over 7 evaluation metrics. Our dataset and\ncodes are publicly accessible.", "published": "2024-03-17 14:49:09", "link": "http://arxiv.org/abs/2403.11236v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Makes Math Word Problems Challenging for LLMs?", "abstract": "This paper investigates the question of what makes math word problems (MWPs)\nin English challenging for large language models (LLMs). We conduct an in-depth\nanalysis of the key linguistic and mathematical characteristics of MWPs. In\naddition, we train feature-based classifiers to better understand the impact of\neach feature on the overall difficulty of MWPs for prominent LLMs and\ninvestigate whether this helps predict how well LLMs fare against specific\ncategories of MWPs.", "published": "2024-03-17 23:18:40", "link": "http://arxiv.org/abs/2403.11369v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Customizing Visual-Language Foundation Models for Multi-modal Anomaly\n  Detection and Reasoning", "abstract": "Anomaly detection is vital in various industrial scenarios, including the\nidentification of unusual patterns in production lines and the detection of\nmanufacturing defects for quality control. Existing techniques tend to be\nspecialized in individual scenarios and lack generalization capacities. In this\nstudy, we aim to develop a generic anomaly detection model applicable across\nmultiple scenarios. To achieve this, we customize generic visual-language\nfoundation models that possess extensive knowledge and robust reasoning\nabilities into anomaly detectors and reasoners. Specifically, we introduce a\nmulti-modal prompting strategy that incorporates domain knowledge from experts\nas conditions to guide the models. Our approach considers multi-modal prompt\ntypes, including task descriptions, class context, normality rules, and\nreference images. In addition, we unify the input representation of\nmulti-modality into a 2D image format, enabling multi-modal anomaly detection\nand reasoning. Our preliminary studies demonstrate that combining visual and\nlanguage prompts as conditions for customizing the models enhances anomaly\ndetection performance. The customized models showcase the ability to detect\nanomalies across different data modalities such as images and point clouds.\nQualitative case studies further highlight the anomaly detection and reasoning\ncapabilities, particularly for multi-object scenes and temporal data. Our code\nis available at https://github.com/Xiaohao-Xu/Customizable-VLM.", "published": "2024-03-17 04:30:57", "link": "http://arxiv.org/abs/2403.11083v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks", "abstract": "Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 10 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).", "published": "2024-03-17 04:36:18", "link": "http://arxiv.org/abs/2403.11085v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ProgGen: Generating Named Entity Recognition Datasets Step-by-step with\n  Self-Reflexive Large Language Models", "abstract": "Although Large Language Models (LLMs) exhibit remarkable adaptability across\ndomains, these models often fall short in structured knowledge extraction tasks\nsuch as named entity recognition (NER). This paper explores an innovative,\ncost-efficient strategy to harness LLMs with modest NER capabilities for\nproducing superior NER datasets. Our approach diverges from the basic\nclass-conditional prompts by instructing LLMs to self-reflect on the specific\ndomain, thereby generating domain-relevant attributes (such as category and\nemotions for movie reviews), which are utilized for creating attribute-rich\ntraining data. Furthermore, we preemptively generate entity terms and then\ndevelop NER context data around these entities, effectively bypassing the LLMs'\nchallenges with complex structures. Our experiments across both general and\nniche domains reveal significant performance enhancements over conventional\ndata generation methods while being more cost-effective than existing\nalternatives.", "published": "2024-03-17 06:12:43", "link": "http://arxiv.org/abs/2403.11103v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Data Diversity for Fine-Tuning Language Models in Human\n  Alignment", "abstract": "Alignment with human preference prevents large language models (LLMs) from\ngenerating misleading or toxic content while requiring high-cost human\nfeedback. Assuming resources of human annotation are limited, there are two\ndifferent ways of allocating considered: more diverse PROMPTS or more diverse\nRESPONSES to be labeled. Nonetheless, a straightforward comparison between\ntheir impact is absent. In this work, we first control the diversity of both\nsides according to the number of samples for fine-tuning, which can directly\nreflect their influence. We find that instead of numerous prompts, more\nresponses but fewer prompts better trigger LLMs for human alignment.\nAdditionally, the concept of diversity for prompts can be more complex than\nresponses that are typically quantified by single digits. Consequently, a new\nformulation of prompt diversity is proposed, further implying a linear\ncorrelation with the final performance of LLMs after fine-tuning. We also\nleverage it on data augmentation and conduct experiments to show its effect on\ndifferent algorithms.", "published": "2024-03-17 07:08:55", "link": "http://arxiv.org/abs/2403.11124v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluation Ethics of LLMs in Legal Domain", "abstract": "In recent years, the utilization of large language models for natural\nlanguage dialogue has gained momentum, leading to their widespread adoption\nacross various domains. However, their universal competence in addressing\nchallenges specific to specialized fields such as law remains a subject of\nscrutiny. The incorporation of legal ethics into the model has been overlooked\nby researchers. We asserts that rigorous ethic evaluation is essential to\nensure the effective integration of large language models in legal domains,\nemphasizing the need to assess domain-specific proficiency and domain-specific\nethic. To address this, we propose a novelty evaluation methodology, utilizing\nauthentic legal cases to evaluate the fundamental language abilities,\nspecialized legal knowledge and legal robustness of large language models\n(LLMs). The findings from our comprehensive evaluation contribute significantly\nto the academic discourse surrounding the suitability and performance of large\nlanguage models in legal domains.", "published": "2024-03-17 09:05:13", "link": "http://arxiv.org/abs/2403.11152v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Correcting misinformation on social media with a large language model", "abstract": "Real-world misinformation, often multimodal, can be partially or fully\nfactual but misleading using diverse tactics like conflating correlation with\ncausation. Such misinformation is severely understudied, challenging to\naddress, and harms various social domains, particularly on social media, where\nit can spread rapidly. High-quality and timely correction of misinformation\nthat identifies and explains its (in)accuracies effectively reduces false\nbeliefs. Despite the wide acceptance of manual correction, it is difficult to\nbe timely and scalable. While LLMs have versatile capabilities that could\naccelerate misinformation correction, they struggle due to a lack of recent\ninformation, a tendency to produce false content, and limitations in addressing\nmultimodal information. We propose MUSE, an LLM augmented with access to and\ncredibility evaluation of up-to-date information. By retrieving evidence as\nrefutations or supporting context, MUSE identifies and explains content\n(in)accuracies with references. It conducts multimodal retrieval and interprets\nvisual content to verify and correct multimodal content. Given the absence of a\ncomprehensive evaluation approach, we propose 13 dimensions of misinformation\ncorrection quality. Then, fact-checking experts evaluate responses to social\nmedia content that are not presupposed to be misinformation but broadly include\n(partially) incorrect and correct posts that may (not) be misleading. Results\ndemonstrate MUSE's ability to write high-quality responses to potential\nmisinformation--across modalities, tactics, domains, political leanings, and\nfor information that has not previously been fact-checked online--within\nminutes of its appearance on social media. Overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%. Our work provides a\ngeneral methodological and evaluative framework to correct misinformation at\nscale.", "published": "2024-03-17 10:59:09", "link": "http://arxiv.org/abs/2403.11169v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quality-Aware Image-Text Alignment for Opinion-Unaware Image Quality\n  Assessment", "abstract": "No-Reference Image Quality Assessment (NR-IQA) focuses on designing methods\nto measure image quality in alignment with human perception when a high-quality\nreference image is unavailable. Most state-of-the-art NR-IQA approaches are\nopinion-aware, i.e. they require human annotations for training. This\ndependency limits their scalability and broad applicability. To overcome this\nlimitation, we propose QualiCLIP (Quality-aware CLIP), a CLIP-based\nself-supervised opinion-unaware approach that does not require human opinions.\nIn particular, we introduce a quality-aware image-text alignment strategy to\nmake CLIP generate quality-aware image representations. Starting from pristine\nimages, we synthetically degrade them with increasing levels of intensity.\nThen, we train CLIP to rank these degraded images based on their similarity to\nquality-related antonym text prompts. At the same time, we force CLIP to\ngenerate consistent representations for images with similar content and the\nsame level of degradation. Our experiments show that the proposed method\nimproves over existing opinion-unaware approaches across multiple datasets with\ndiverse distortion types. Moreover, despite not requiring human annotations,\nQualiCLIP achieves excellent performance against supervised opinion-aware\nmethods in cross-dataset experiments, thus demonstrating remarkable\ngeneralization capabilities. The code and the model are publicly available at\nhttps://github.com/miccunifi/QualiCLIP.", "published": "2024-03-17 11:32:18", "link": "http://arxiv.org/abs/2403.11176v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Creating an African American-Sounding TTS: Guidelines, Technical\n  Challenges,and Surprising Evaluations", "abstract": "Representations of AI agents in user interfaces and robotics are\npredominantly White, not only in terms of facial and skin features, but also in\nthe synthetic voices they use. In this paper we explore some unexpected\nchallenges in the representation of race we found in the process of developing\nan U.S. English Text-to-Speech (TTS) system aimed to sound like an educated,\nprofessional, regional accent-free African American woman. The paper starts by\npresenting the results of focus groups with African American IT professionals\nwhere guidelines and challenges for the creation of a representative and\nappropriate TTS system were discussed and gathered, followed by a discussion\nabout some of the technical difficulties faced by the TTS system developers. We\nthen describe two studies with U.S. English speakers where the participants\nwere not able to attribute the correct race to the African American TTS voice\nwhile overwhelmingly correctly recognizing the race of a White TTS system of\nsimilar quality. A focus group with African American IT workers not only\nconfirmed the representativeness of the African American voice we built, but\nalso suggested that the surprising recognition results may have been caused by\nthe inability or the latent prejudice from non-African Americans to associate\neducated, non-vernacular, professionally-sounding voices to African American\npeople.", "published": "2024-03-17 13:21:33", "link": "http://arxiv.org/abs/2403.11209v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Cheap Ways of Extracting Clinical Markers from Texts", "abstract": "This paper describes the work of the UniBuc Archaeology team for CLPsych's\n2024 Shared Task, which involved finding evidence within the text supporting\nthe assigned suicide risk level. Two types of evidence were required:\nhighlights (extracting relevant spans within the text) and summaries\n(aggregating evidence into a synthesis). Our work focuses on evaluating Large\nLanguage Models (LLM) as opposed to an alternative method that is much more\nmemory and resource efficient. The first approach employs a good old-fashioned\nmachine learning (GOML) pipeline consisting of a tf-idf vectorizer with a\nlogistic regression classifier, whose representative features are used to\nextract relevant highlights. The second, more resource intensive, uses an LLM\nfor generating the summaries and is guided by chain-of-thought to provide\nsequences of text indicating clinical markers.", "published": "2024-03-17 14:21:42", "link": "http://arxiv.org/abs/2403.11227v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding", "abstract": "Deep multimodal semantic understanding that goes beyond the mere superficial\ncontent relation mining has received increasing attention in the realm of\nartificial intelligence. The challenges of collecting and annotating\nhigh-quality multi-modal data have underscored the significance of few-shot\nlearning. In this paper, we focus on two critical tasks under this context:\nfew-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis\n(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware\nPrompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on\nthe unified vision-language model (VLM). Specifically, we design three experts\nof soft prompts: a text prompt and an image prompt that extract\nmodality-specific features to enrich the single-modal representation, and a\nunified prompt to assist multi-modal interaction. Additionally, we reorganize\nTransformer layers into several blocks and introduce cross-modal prompt\nattention between adjacent blocks, which smoothens the transition from\nsingle-modal representation to multi-modal fusion. On both MSD and MSA datasets\nin few-shot setting, our proposed model not only surpasses the 8.2B model\nInstructBLIP with merely 2% parameters (150M), but also significantly\noutperforms other widely-used prompt methods on VLMs or task-specific methods.", "published": "2024-03-17 19:12:26", "link": "http://arxiv.org/abs/2403.11311v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Reasoning in Transformers -- Mitigating Spurious Correlations and\n  Reasoning Shortcuts", "abstract": "Transformer language models are neural networks used for a wide variety of\ntasks concerning natural language, including some that also require logical\nreasoning. However, a transformer model may easily learn spurious patterns in\nthe data, short-circuiting actual reasoning. In this paper we investigate to\nwhat extent transformers can be trained to a) approximate reasoning in\npropositional logic while b) avoiding known reasoning shortcuts via spurious\ncorrelations in the training data. To do so, we use a dataset with known\nspurious correlation between truth and e.g. the number of rules in the problem.\nWe augment the data with proofs, and train two models: a generative\ntransformer, WP-BART, trained on problems and their whole proofs, and a\nneuro-symbolic model, SIP-BART, trained on individual proof steps and combining\nthe generative transformer model BART with a symbolic proof checker. We find\nthat SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not.\nFor SIP-BART, we then identify a few remaining reasoning errors, not previously\ndescribed in the literature, arising from using a pre-trained language model.\nThese are qualitatively analysed to create a taxonomy of four different types\nof additional pitfalls.", "published": "2024-03-17 19:32:12", "link": "http://arxiv.org/abs/2403.11314v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches", "abstract": "Two approaches have emerged to input images into large language models\n(LLMs). The first is to caption images into natural language. The second is to\nmap image feature embeddings into the domain of the LLM and pass the mapped\nembeddings directly to the LLM. The majority of recent few-shot multimodal work\nreports performance using architectures that employ variations of one of these\ntwo approaches. But they overlook an important comparison between them. We\ndesign a controlled and focused experiment to compare these two approaches to\nfew-shot visual question answering (VQA) with LLMs. Our findings indicate that\nfor Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to\nthe LLM embedding space does not guarantee improved performance over using\nimage captions. In the zero-shot regime, we find using textual image captions\nis better. In the few-shot regimes, how the in-context examples are selected\ndetermines which is better.", "published": "2024-03-17 19:44:05", "link": "http://arxiv.org/abs/2403.11317v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows", "abstract": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.", "published": "2024-03-17 19:54:16", "link": "http://arxiv.org/abs/2403.11322v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ConvSDG: Session Data Generation for Conversational Search", "abstract": "Conversational search provides a more convenient interface for users to\nsearch by allowing multi-turn interaction with the search engine. However, the\neffectiveness of the conversational dense retrieval methods is limited by the\nscarcity of training data required for their fine-tuning. Thus, generating more\ntraining conversational sessions with relevant labels could potentially improve\nsearch performance. Based on the promising capabilities of large language\nmodels (LLMs) on text generation, we propose ConvSDG, a simple yet effective\nframework to explore the feasibility of boosting conversational search by using\nLLM for session data generation. Within this framework, we design\ndialogue/session-level and query-level data generation with unsupervised and\nsemi-supervised learning, according to the availability of relevance judgments.\nThe generated data are used to fine-tune the conversational dense retriever.\nExtensive experiments on four widely used datasets demonstrate the\neffectiveness and broad applicability of our ConvSDG framework compared with\nseveral strong baselines.", "published": "2024-03-17 20:34:40", "link": "http://arxiv.org/abs/2403.11335v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CantonMT: Cantonese to English NMT Platform with Fine-Tuned Models Using\n  Synthetic Back-Translation Data", "abstract": "Neural Machine Translation (NMT) for low-resource languages is still a\nchallenging task in front of NLP researchers. In this work, we deploy a\nstandard data augmentation methodology by back-translation to a new language\ntranslation direction Cantonese-to-English. We present the models we fine-tuned\nusing the limited amount of real data and the synthetic data we generated using\nback-translation including OpusMT, NLLB, and mBART. We carried out automatic\nevaluation using a range of different metrics including lexical-based and\nembedding-based. Furthermore. we create a user-friendly interface for the\nmodels we included in this\\textsc{ CantonMT} research project and make it\navailable to facilitate Cantonese-to-English MT research. Researchers can add\nmore models into this platform via our open-source\\textsc{ CantonMT} toolkit\n\\url{https://github.com/kenrickkung/CantoneseTranslation}.", "published": "2024-03-17 21:16:17", "link": "http://arxiv.org/abs/2403.11346v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Concept-Best-Matching: Evaluating Compositionality in Emergent\n  Communication", "abstract": "Artificial agents that learn to communicate in order to accomplish a given\ntask acquire communication protocols that are typically opaque to a human. A\nlarge body of work has attempted to evaluate the emergent communication via\nvarious evaluation measures, with \\emph{compositionality} featuring as a\nprominent desired trait. However, current evaluation procedures do not directly\nexpose the compositionality of the emergent communication. We propose a\nprocedure to assess the compositionality of emergent communication by finding\nthe best-match between emerged words and natural language concepts. The\nbest-match algorithm provides both a global score and a translation-map from\nemergent words to natural language concepts. To the best of our knowledge, it\nis the first time that such direct and interpretable mapping between emergent\nwords and human concepts is provided.", "published": "2024-03-17 12:47:02", "link": "http://arxiv.org/abs/2403.14705v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "FlowMind: Automatic Workflow Generation with LLMs", "abstract": "The rapidly evolving field of Robotic Process Automation (RPA) has made\nsignificant strides in automating repetitive processes, yet its effectiveness\ndiminishes in scenarios requiring spontaneous or unpredictable tasks demanded\nby users. This paper introduces a novel approach, FlowMind, leveraging the\ncapabilities of Large Language Models (LLMs) such as Generative Pretrained\nTransformer (GPT), to address this limitation and create an automatic workflow\ngeneration system. In FlowMind, we propose a generic prompt recipe for a\nlecture that helps ground LLM reasoning with reliable Application Programming\nInterfaces (APIs). With this, FlowMind not only mitigates the common issue of\nhallucinations in LLMs, but also eliminates direct interaction between LLMs and\nproprietary data or code, thus ensuring the integrity and confidentiality of\ninformation - a cornerstone in financial services. FlowMind further simplifies\nuser interaction by presenting high-level descriptions of auto-generated\nworkflows, enabling users to inspect and provide feedback effectively. We also\nintroduce NCEN-QA, a new dataset in finance for benchmarking question-answering\ntasks from N-CEN reports on funds. We used NCEN-QA to evaluate the performance\nof workflows generated by FlowMind against baseline and ablation variants of\nFlowMind. We demonstrate the success of FlowMind, the importance of each\ncomponent in the proposed lecture recipe, and the effectiveness of user\ninteraction and feedback in FlowMind.", "published": "2024-03-17 00:36:37", "link": "http://arxiv.org/abs/2404.13050v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RobustSentEmbed: Robust Sentence Embeddings Using Adversarial\n  Self-Supervised Contrastive Learning", "abstract": "Pre-trained language models (PLMs) have consistently demonstrated outstanding\nperformance across a diverse spectrum of natural language processing tasks.\nNevertheless, despite their success with unseen data, current PLM-based\nrepresentations often exhibit poor robustness in adversarial settings. In this\npaper, we introduce RobustSentEmbed, a self-supervised sentence embedding\nframework designed to improve both generalization and robustness in diverse\ntext representation tasks and against a diverse set of adversarial attacks.\nThrough the generation of high-risk adversarial perturbations and their\nutilization in a novel objective function, RobustSentEmbed adeptly learns\nhigh-quality and robust sentence embeddings. Our experiments confirm the\nsuperiority of RobustSentEmbed over state-of-the-art representations.\nSpecifically, Our framework achieves a significant reduction in the success\nrate of various adversarial attacks, notably reducing the BERTAttack success\nrate by almost half (from 75.51\\% to 38.81\\%). The framework also yields\nimprovements of 1.59\\% and 0.23\\% in semantic textual similarity tasks and\nvarious transfer tasks, respectively.", "published": "2024-03-17 04:29:45", "link": "http://arxiv.org/abs/2403.11082v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Forging the Forger: An Attempt to Improve Authorship Verification via\n  Data Augmentation", "abstract": "Authorship Verification (AV) is a text classification task concerned with\ninferring whether a candidate text has been written by one specific author or\nby someone else. It has been shown that many AV systems are vulnerable to\nadversarial attacks, where a malicious author actively tries to fool the\nclassifier by either concealing their writing style, or by imitating the style\nof another author. In this paper, we investigate the potential benefits of\naugmenting the classifier training set with (negative) synthetic examples.\nThese synthetic examples are generated to imitate the style of the author of\ninterest. We analyze the improvements in classifier prediction that this\naugmentation brings to bear in the task of AV in an adversarial setting. In\nparticular, we experiment with three different generator architectures (one\nbased on Recurrent Neural Networks, another based on small-scale transformers,\nand another based on the popular GPT model) and with two training strategies\n(one inspired by standard Language Models, and another inspired by Wasserstein\nGenerative Adversarial Networks). We evaluate our hypothesis on five datasets\n(three of which have been specifically collected to represent an adversarial\nsetting) and using two learning algorithms for the AV classifier (Support\nVector Machines and Convolutional Neural Networks). This experimentation has\nyielded negative results, revealing that, although our methodology proves\neffective in many adversarial settings, its benefits are too sporadic for a\npragmatical application.", "published": "2024-03-17 16:36:26", "link": "http://arxiv.org/abs/2403.11265v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Modified Word Saliency-Based Adversarial Attack on Text Classification\n  Models", "abstract": "This paper introduces a novel adversarial attack method targeting text\nclassification models, termed the Modified Word Saliency-based Adversarial\nAt-tack (MWSAA). The technique builds upon the concept of word saliency to\nstrategically perturb input texts, aiming to mislead classification models\nwhile preserving semantic coherence. By refining the traditional adversarial\nattack approach, MWSAA significantly enhances its efficacy in evading detection\nby classification systems. The methodology involves first identifying salient\nwords in the input text through a saliency estimation process, which\nprioritizes words most influential to the model's decision-making process.\nSubsequently, these salient words are subjected to carefully crafted\nmodifications, guided by semantic similarity metrics to ensure that the altered\ntext remains coherent and retains its original meaning. Empirical evaluations\nconducted on diverse text classification datasets demonstrate the effectiveness\nof the proposed method in generating adversarial examples capable of\nsuccessfully deceiving state-of-the-art classification models. Comparative\nanalyses with existing adversarial attack techniques further indicate the\nsuperiority of the proposed approach in terms of both attack success rate and\npreservation of text coherence.", "published": "2024-03-17 18:39:14", "link": "http://arxiv.org/abs/2403.11297v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant", "abstract": "Recent advances in vision-language models have shown notable generalization\nin broad tasks through visual instruction tuning. However, bridging the gap\nbetween the pre-trained vision encoder and the large language models (LLMs)\nbecomes the whole network's bottleneck. To improve cross-modality alignment,\nexisting works usually consider more visual instruction data covering a broader\nrange of vision tasks to fine-tune the model for question-answering, which,\nhowever, is costly to obtain and has not thoroughly explored the rich\ncontextual information contained in images. This paper first attempts to\nharness the overlooked context within visual instruction data, training the\nmodel to self-supervised \"learning\" how to ask high-quality questions. In this\nway, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large\nVision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible\nand meaningful image-related questions while analyzing the visual clue and\nprior language knowledge, signifying an advanced level of generalized visual\nunderstanding. Moreover, fine-tuning SQ-LLaVA on higher-quality instruction\ndata shows a performance improvement compared with traditional\nvisual-instruction tuning methods. This improvement highlights the efficacy of\nself-questioning techniques in achieving a deeper and more nuanced\ncomprehension of visual content across various contexts.", "published": "2024-03-17 18:42:38", "link": "http://arxiv.org/abs/2403.11299v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation\n  with Local Implicit Multimodal Feedback", "abstract": "We describe an approach for aligning an LLM-based dialogue agent based on\nglobal (i.e., dialogue-level) rewards, while also taking into account\nnaturally-occurring multimodal signals. At a high level, our approach (dubbed\nGELI) learns a local, turn-level reward model by decomposing the human-provided\nGlobal Explicit (GE) session-level reward, using Local Implicit (LI) multimodal\nreward signals to crossmodally shape the reward decomposition step. This\ndecomposed reward model is then used as part of the standard RHLF pipeline\nimprove an LLM-based dialog agent. We run quantitative and qualitative human\nstudies to evaluate the performance of our GELI approach, and find that it\nshows consistent improvements across various conversational metrics compared to\nbaseline methods.", "published": "2024-03-17 20:21:26", "link": "http://arxiv.org/abs/2403.11330v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented\n  Fine-Tuning", "abstract": "The scaling of Large Language Models (LLMs) for retrieval-based tasks,\nparticularly in Retrieval Augmented Generation (RAG), faces significant memory\nconstraints, especially when fine-tuning extensive prompt sequences. Current\nopen-source libraries support full-model inference and fine-tuning across\nmultiple GPUs but fall short of accommodating the efficient parameter\ndistribution required for retrieved context. Addressing this gap, we introduce\na novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging\ndistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)\ncompilation and tensor-sharding for efficient resource management, thereby\nenabling accelerated fine-tuning with reduced memory requirements. This\nadvancement significantly improves the scalability and feasibility of\nfine-tuning LLMs for complex RAG applications, even on systems with limited GPU\nresources. Our experiments show more than 12x improvement in runtime compared\nto Hugging Face/DeepSpeed implementation with four GPUs while consuming less\nthan half the VRAM per GPU.", "published": "2024-03-17 23:02:04", "link": "http://arxiv.org/abs/2403.11366v2", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "Psittacines of Innovation? Assessing the True Novelty of AI Creations", "abstract": "We examine whether Artificial Intelligence (AI) systems generate truly novel\nideas rather than merely regurgitating patterns learned during training.\nUtilizing a novel experimental design, we task an AI with generating project\ntitles for hypothetical crowdfunding campaigns. We compare within AI-generated\nproject titles, measuring repetition and complexity. We compare between the\nAI-generated titles and actual observed field data using an extension of\nmaximum mean discrepancy--a metric derived from the application of kernel mean\nembeddings of statistical distributions to high-dimensional machine learning\n(large language) embedding vectors--yielding a structured analysis of AI output\nnovelty. Results suggest that (1) the AI generates unique content even under\nincreasing task complexity, and at the limits of its computational\ncapabilities, (2) the generated content has face validity, being consistent\nwith both inputs to other generative AI and in qualitative comparison to field\ndata, and (3) exhibits divergence from field data, mitigating concerns relating\nto intellectual property rights. We discuss implications for copyright and\ntrademark law.", "published": "2024-03-17 13:08:11", "link": "http://arxiv.org/abs/2404.00017v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Lost in Translation? Translation Errors and Challenges for Fair\n  Assessment of Text-to-Image Models on Multilingual Concepts", "abstract": "Benchmarks of the multilingual capabilities of text-to-image (T2I) models\ncompare generated images prompted in a test language to an expected image\ndistribution over a concept set. One such benchmark, \"Conceptual Coverage\nAcross Languages\" (CoCo-CroLa), assesses the tangible noun inventory of T2I\nmodels by prompting them to generate pictures from a concept list translated to\nseven languages and comparing the output image populations. Unfortunately, we\nfind that this benchmark contains translation errors of varying severity in\nSpanish, Japanese, and Chinese. We provide corrections for these errors and\nanalyze how impactful they are on the utility and validity of CoCo-CroLa as a\nbenchmark. We reassess multiple baseline T2I models with the revisions, compare\nthe outputs elicited under the new translations to those conditioned on the\nold, and show that a correction's impactfulness on the image-domain benchmark\nresults can be predicted in the text domain with similarity scores. Our\nfindings will guide the future development of T2I multilinguality metrics by\nproviding analytical tools for practical translation decisions.", "published": "2024-03-17 05:05:11", "link": "http://arxiv.org/abs/2403.11092v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY", "eess.IV"], "primary_category": "cs.CL"}
{"title": "Multitask frame-level learning for few-shot sound event detection", "abstract": "This paper focuses on few-shot Sound Event Detection (SED), which aims to\nautomatically recognize and classify sound events with limited samples.\nHowever, prevailing methods methods in few-shot SED predominantly rely on\nsegment-level predictions, which often providing detailed, fine-grained\npredictions, particularly for events of brief duration. Although frame-level\nprediction strategies have been proposed to overcome these limitations, these\nstrategies commonly face difficulties with prediction truncation caused by\nbackground noise. To alleviate this issue, we introduces an innovative\nmultitask frame-level SED framework. In addition, we introduce TimeFilterAug, a\nlinear timing mask for data augmentation, to increase the model's robustness\nand adaptability to diverse acoustic environments. The proposed method achieves\na F-score of 63.8%, securing the 1st rank in the few-shot bioacoustic event\ndetection category of the Detection and Classification of Acoustic Scenes and\nEvents Challenge 2023.", "published": "2024-03-17 05:00:40", "link": "http://arxiv.org/abs/2403.11091v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Artificial Intelligence for Cochlear Implants: Review of Strategies,\n  Challenges, and Perspectives", "abstract": "Automatic speech recognition (ASR) plays a pivotal role in our daily lives,\noffering utility not only for interacting with machines but also for\nfacilitating communication for individuals with partial or profound hearing\nimpairments. The process involves receiving the speech signal in analog form,\nfollowed by various signal processing algorithms to make it compatible with\ndevices of limited capacities, such as cochlear implants (CIs). Unfortunately,\nthese implants, equipped with a finite number of electrodes, often result in\nspeech distortion during synthesis. Despite efforts by researchers to enhance\nreceived speech quality using various state-of-the-art (SOTA) signal processing\ntechniques, challenges persist, especially in scenarios involving multiple\nsources of speech, environmental noise, and other adverse conditions. The\nadvent of new artificial intelligence (AI) methods has ushered in cutting-edge\nstrategies to address the limitations and difficulties associated with\ntraditional signal processing techniques dedicated to CIs. This review aims to\ncomprehensively cover advancements in CI-based ASR and speech enhancement,\namong other related aspects. The primary objective is to provide a thorough\noverview of metrics and datasets, exploring the capabilities of AI algorithms\nin this biomedical field, and summarizing and commenting on the best results\nobtained. Additionally, the review will delve into potential applications and\nsuggest future directions to bridge existing research gaps in this domain.", "published": "2024-03-17 11:28:23", "link": "http://arxiv.org/abs/2403.15442v3", "categories": ["eess.AS", "cs.AI", "cs.CV", "eess.IV"], "primary_category": "eess.AS"}
{"title": "Audio-Visual Segmentation via Unlabeled Frame Exploitation", "abstract": "Audio-visual segmentation (AVS) aims to segment the sounding objects in video\nframes. Although great progress has been witnessed, we experimentally reveal\nthat current methods reach marginal performance gain within the use of the\nunlabeled frames, leading to the underutilization issue. To fully explore the\npotential of the unlabeled frames for AVS, we explicitly divide them into two\ncategories based on their temporal characteristics, i.e., neighboring frame\n(NF) and distant frame (DF). NFs, temporally adjacent to the labeled frame,\noften contain rich motion information that assists in the accurate localization\nof sounding objects. Contrary to NFs, DFs have long temporal distances from the\nlabeled frame, which share semantic-similar objects with appearance variations.\nConsidering their unique characteristics, we propose a versatile framework that\neffectively leverages them to tackle AVS. Specifically, for NFs, we exploit the\nmotion cues as the dynamic guidance to improve the objectness localization.\nBesides, we exploit the semantic cues in DFs by treating them as valid\naugmentations to the labeled frames, which are then used to enrich data\ndiversity in a self-training manner. Extensive experimental results demonstrate\nthe versatility and superiority of our method, unleashing the power of the\nabundant unlabeled frames.", "published": "2024-03-17 03:45:14", "link": "http://arxiv.org/abs/2403.11074v1", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
