{"title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts", "abstract": "Recent work has shown that feed-forward networks (FFNs) in pre-trained\nTransformers are a key component, storing various linguistic and factual\nknowledge. However, the computational patterns of FFNs are still unclear. In\nthis work, we study the computational patterns of FFNs and observe that most\ninputs only activate a tiny ratio of neurons of FFNs. This phenomenon is\nsimilar to the sparsity of the human brain, which drives research on functional\npartitions of the human brain. To verify whether functional partitions also\nemerge in FFNs, we propose to convert a model into its MoE version with the\nsame parameters, namely MoEfication. Specifically, MoEfication consists of two\nphases: (1) splitting the parameters of FFNs into multiple functional\npartitions as experts, and (2) building expert routers to decide which experts\nwill be used for each input. Experimental results show that MoEfication can\nconditionally use 10% to 30% of FFN parameters while maintaining over 95%\noriginal performance for different models on various downstream tasks. Besides,\nMoEfication brings two advantages: (1) it significantly reduces the FLOPS of\ninference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a\nfine-grained perspective to study the inner mechanism of FFNs. The source code\nof this paper can be obtained from https://github.com/thunlp/MoEfication.", "published": "2021-10-05 02:14:38", "link": "http://arxiv.org/abs/2110.01786v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating the Impact of Pre-trained Language Models on Dialog\n  Evaluation", "abstract": "Recently, there is a surge of interest in applying pre-trained language\nmodels (Pr-LM) in automatic open-domain dialog evaluation. Pr-LMs offer a\npromising direction for addressing the multi-domain evaluation challenge. Yet,\nthe impact of different Pr-LMs on the performance of automatic metrics is not\nwell-understood. This paper examines 8 different Pr-LMs and studies their\nimpact on three typical automatic dialog evaluation metrics across three\ndifferent dialog evaluation benchmarks. Specifically, we analyze how the choice\nof Pr-LMs affects the performance of automatic metrics. Extensive correlation\nanalyses on each of the metrics are performed to assess the effects of\ndifferent Pr-LMs along various axes, including pre-training objectives, dialog\nevaluation criteria, model size, and cross-dataset robustness. This study\nserves as the first comprehensive assessment of the effects of different Pr-LMs\non automatic dialog evaluation.", "published": "2021-10-05 09:30:59", "link": "http://arxiv.org/abs/2110.01895v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sicilian Translator: A Recipe for Low-Resource NMT", "abstract": "With 17,000 pairs of Sicilian-English translated sentences, Arba Sicula\ndeveloped the first neural machine translator for the Sicilian language. Using\nsmall subword vocabularies, we trained small Transformer models with high\ndropout parameters and achieved BLEU scores in the upper 20s. Then we\nsupplemented our dataset with backtranslation and multilingual translation and\npushed our scores into the mid 30s. We also attribute our success to\nincorporating theoretical information in our dataset. Prior to training, we\nbiased the subword vocabulary towards the desinences one finds in a textbook.\nAnd we included textbook exercises in our dataset.", "published": "2021-10-05 11:04:13", "link": "http://arxiv.org/abs/2110.01938v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Mastering the Explicit Opinion-role Interaction: Syntax-aided Neural\n  Transition System for Unified Opinion Role Labeling", "abstract": "Unified opinion role labeling (ORL) aims to detect all possible opinion\nstructures of 'opinion-holder-target' in one shot, given a text. The existing\ntransition-based unified method, unfortunately, is subject to longer opinion\nterms and fails to solve the term overlap issue. Current top performance has\nbeen achieved by employing the span-based graph model, which however still\nsuffers from both high model complexity and insufficient interaction among\nopinions and roles. In this work, we investigate a novel solution by revisiting\nthe transition architecture, and augmenting it with a pointer network\n(PointNet). The framework parses out all opinion structures in linear-time\ncomplexity, meanwhile breaks through the limitation of any length of terms with\nPointNet. To achieve the explicit opinion-role interactions, we further propose\na unified dependency-opinion graph (UDOG), co-modeling the syntactic dependency\nstructure and the partial opinion-role structure. We then devise a\nrelation-centered graph aggregator (RCGA) to encode the multi-relational UDOG,\nwhere the resulting high-order representations are used to promote the\npredictions in the vanilla transition system. Our model achieves new\nstate-of-the-art results on the MPQA benchmark. Analyses further demonstrate\nthe superiority of our methods on both efficacy and efficiency.", "published": "2021-10-05 12:45:59", "link": "http://arxiv.org/abs/2110.02001v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploiting Twitter as Source of Large Corpora of Weakly Similar Pairs\n  for Semantic Sentence Embeddings", "abstract": "Semantic sentence embeddings are usually supervisedly built minimizing\ndistances between pairs of embeddings of sentences labelled as semantically\nsimilar by annotators. Since big labelled datasets are rare, in particular for\nnon-English languages, and expensive, recent studies focus on unsupervised\napproaches that require not-paired input sentences. We instead propose a\nlanguage-independent approach to build large datasets of pairs of informal\ntexts weakly similar, without manual human effort, exploiting Twitter's\nintrinsic powerful signals of relatedness: replies and quotes of tweets. We use\nthe collected pairs to train a Transformer model with triplet-like structures,\nand we test the generated embeddings on Twitter NLP similarity tasks (PIT and\nTURL) and STSb. We also introduce four new sentence ranking evaluation\nbenchmarks of informal texts, carefully extracted from the initial collections\nof tweets, proving not only that our best model learns classical Semantic\nTextual Similarity, but also excels on tasks where pairs of sentences are not\nexact paraphrases. Ablation studies reveal how increasing the corpus size\ninfluences positively the results, even at 2M samples, suggesting that bigger\ncollections of Tweets still do not contain redundant information about semantic\nsimilarities.", "published": "2021-10-05 13:21:40", "link": "http://arxiv.org/abs/2110.02030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teach Me What to Say and I Will Learn What to Pick: Unsupervised\n  Knowledge Selection Through Response Generation with Pretrained Generative\n  Models", "abstract": "Knowledge Grounded Conversation Models (KGCM) are usually based on a\nselection/retrieval module and a generation module, trained separately or\nsimultaneously, with or without having access to a gold knowledge option. With\nthe introduction of large pre-trained generative models, the selection and\ngeneration part have become more and more entangled, shifting the focus towards\nenhancing knowledge incorporation (from multiple sources) instead of trying to\npick the best knowledge option. These approaches however depend on knowledge\nlabels and/or a separate dense retriever for their best performance. In this\nwork we study the unsupervised selection abilities of pre-trained generative\nmodels (e.g. BART) and show that by adding a score-and-aggregate module between\nencoder and decoder, they are capable of learning to pick the proper knowledge\nthrough minimising the language modelling loss (i.e. without having access to\nknowledge labels). Trained as such, our model - K-Mine - shows competitive\nselection and generation performance against models that benefit from knowledge\nlabels and/or separate dense retriever.", "published": "2021-10-05 14:12:47", "link": "http://arxiv.org/abs/2110.02067v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Sense-Specific Static Embeddings using Contextualised Word\n  Embeddings as a Proxy", "abstract": "Contextualised word embeddings generated from Neural Language Models (NLMs),\nsuch as BERT, represent a word with a vector that considers the semantics of\nthe target word as well its context. On the other hand, static word embeddings\nsuch as GloVe represent words by relatively low-dimensional, memory- and\ncompute-efficient vectors but are not sensitive to the different senses of the\nword. We propose Context Derived Embeddings of Senses (CDES), a method that\nextracts sense related information from contextualised embeddings and injects\nit into static embeddings to create sense-specific static embeddings.\nExperimental results on multiple benchmarks for word sense disambiguation and\nsense discrimination tasks show that CDES can accurately learn sense-specific\nstatic embeddings reporting comparable performance to the current\nstate-of-the-art sense embeddings.", "published": "2021-10-05 17:50:48", "link": "http://arxiv.org/abs/2110.02204v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Acquisition in Neural Language Models", "abstract": "We investigate how neural language models acquire individual words during\ntraining, extracting learning curves and ages of acquisition for over 600 words\non the MacArthur-Bates Communicative Development Inventory (Fenson et al.,\n2007). Drawing on studies of word acquisition in children, we evaluate multiple\npredictors for words' ages of acquisition in LSTMs, BERT, and GPT-2. We find\nthat the effects of concreteness, word length, and lexical class are pointedly\ndifferent in children and language models, reinforcing the importance of\ninteraction and sensorimotor experience in child language acquisition. Language\nmodels rely far more on word frequency than children, but like children, they\nexhibit slower learning of words in longer utterances. Interestingly, models\nfollow consistent patterns during training for both unidirectional and\nbidirectional models, and for both LSTM and Transformer architectures. Models\npredict based on unigram token frequencies early in training, before\ntransitioning loosely to bigram probabilities, eventually converging on more\nnuanced predictions. These results shed light on the role of distributional\nlearning mechanisms in children, while also providing insights for more\nhuman-like language acquisition in language models.", "published": "2021-10-05 23:26:16", "link": "http://arxiv.org/abs/2110.02406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building the Language Resource for a Cebuano-Filipino Neural Machine\n  Translation System", "abstract": "Parallel corpus is a critical resource in machine learning-based translation.\nThe task of collecting, extracting, and aligning texts in order to build an\nacceptable corpus for doing the translation is very tedious most especially for\nlow-resource languages. In this paper, we present the efforts made to build a\nparallel corpus for Cebuano and Filipino from two different domains: biblical\ntexts and the web. For the biblical resource, subword unit translation for\nverbs and copy-able approach for nouns were applied to correct inconsistencies\nin the translation. This correction mechanism was applied as a preprocessing\ntechnique. On the other hand, for Wikipedia being the main web resource,\ncommonly occurring topic segments were extracted from both the source and the\ntarget languages. These observed topic segments are unique in 4 different\ncategories. The identification of these topic segments may be used for the\nautomatic extraction of sentences. A Recurrent Neural Network was used to\nimplement the translation using OpenNMT sequence modeling tool in TensorFlow.\nThe two different corpora were then evaluated by using them as two separate\ninputs in the neural network. Results have shown a difference in BLEU scores in\nboth corpora.", "published": "2021-10-05 23:03:09", "link": "http://arxiv.org/abs/2110.15716v1", "categories": ["cs.CL", "A.2"], "primary_category": "cs.CL"}
{"title": "On the Complementarity between Pre-Training and Back-Translation for\n  Neural Machine Translation", "abstract": "Pre-training (PT) and back-translation (BT) are two simple and powerful\nmethods to utilize monolingual data for improving the model performance of\nneural machine translation (NMT). This paper takes the first step to\ninvestigate the complementarity between PT and BT. We introduce two probing\ntasks for PT and BT respectively and find that PT mainly contributes to the\nencoder module while BT brings more benefits to the decoder. Experimental\nresults show that PT and BT are nicely complementary to each other,\nestablishing state-of-the-art performances on the WMT16 English-Romanian and\nEnglish-Russian benchmarks. Through extensive analyses on sentence originality\nand word frequency, we also demonstrate that combining Tagged BT with PT is\nmore helpful to their complementarity, leading to better translation quality.\nSource code is freely available at https://github.com/SunbowLiu/PTvsBT.", "published": "2021-10-05 04:01:36", "link": "http://arxiv.org/abs/2110.01811v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Truth-Conditional Captioning of Time Series Data", "abstract": "In this paper, we explore the task of automatically generating natural\nlanguage descriptions of salient patterns in a time series, such as stock\nprices of a company over a week. A model for this task should be able to\nextract high-level patterns such as presence of a peak or a dip. While typical\ncontemporary neural models with attention mechanisms can generate fluent output\ndescriptions for this task, they often generate factually incorrect\ndescriptions. We propose a computational model with a truth-conditional\narchitecture which first runs small learned programs on the input time series,\nthen identifies the programs/patterns which hold true for the given input, and\nfinally conditions on only the chosen valid program (rather than the input time\nseries) to generate the output text description. A program in our model is\nconstructed from modules, which are small neural networks that are designed to\ncapture numerical patterns and temporal information. The modules are shared\nacross multiple programs, enabling compositionality as well as efficient\nlearning of module parameters. The modules, as well as the composition of the\nmodules, are unobserved in data, and we learn them in an end-to-end fashion\nwith the only training signal coming from the accompanying natural language\ntext descriptions. We find that the proposed model is able to generate\nhigh-precision captions even though we consider a small and simple space of\nmodule types.", "published": "2021-10-05 06:28:37", "link": "http://arxiv.org/abs/2110.01839v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ASR Rescoring and Confidence Estimation with ELECTRA", "abstract": "In automatic speech recognition (ASR) rescoring, the hypothesis with the\nfewest errors should be selected from the n-best list using a language model\n(LM). However, LMs are usually trained to maximize the likelihood of correct\nword sequences, not to detect ASR errors. We propose an ASR rescoring method\nfor directly detecting errors with ELECTRA, which is originally a pre-training\nmethod for NLP tasks. ELECTRA is pre-trained to predict whether each word is\nreplaced by BERT or not, which can simulate ASR error detection on large text\ncorpora. To make this pre-training closer to ASR error detection, we further\npropose an extended version of ELECTRA called phone-attentive ELECTRA\n(P-ELECTRA). In the pre-training of P-ELECTRA, each word is replaced by a\nphone-to-word conversion model, which leverages phone information to generate\nacoustically similar words. Since our rescoring method is optimized for\ndetecting errors, it can also be used for word-level confidence estimation.\nExperimental evaluations on the Librispeech and TED-LIUM2 corpora show that our\nrescoring method with ELECTRA is competitive with conventional rescoring\nmethods with faster inference. ELECTRA also performs better in confidence\nestimation than BERT because it can learn to detect inappropriate words not\nonly in fine-tuning but also in pre-training.", "published": "2021-10-05 07:45:55", "link": "http://arxiv.org/abs/2110.01857v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "DistilHuBERT: Speech Representation Learning by Layer-wise Distillation\n  of Hidden-unit BERT", "abstract": "Self-supervised speech representation learning methods like wav2vec 2.0 and\nHidden-unit BERT (HuBERT) leverage unlabeled speech data for pre-training and\noffer good representations for numerous speech processing tasks. Despite the\nsuccess of these methods, they require large memory and high pre-training\ncosts, making them inaccessible for researchers in academia and small\ncompanies. Therefore, this paper introduces DistilHuBERT, a novel multi-task\nlearning framework to distill hidden representations from a HuBERT model\ndirectly. This method reduces HuBERT's size by 75% and 73% faster while\nretaining most performance in ten different tasks. Moreover, DistilHuBERT\nrequired little training time and data, opening the possibilities of\npre-training personal and on-device SSL models for speech.", "published": "2021-10-05 09:34:44", "link": "http://arxiv.org/abs/2110.01900v4", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AraCOVID19-SSD: Arabic COVID-19 Sentiment and Sarcasm Detection Dataset", "abstract": "Coronavirus disease (COVID-19) is an infectious respiratory disease that was\nfirst discovered in late December 2019, in Wuhan, China, and then spread\nworldwide causing a lot of panic and death. Users of social networking sites\nsuch as Facebook and Twitter have been focused on reading, publishing, and\nsharing novelties, tweets, and articles regarding the newly emerging pandemic.\nA lot of these users often employ sarcasm to convey their intended meaning in a\nhumorous, funny, and indirect way making it hard for computer-based\napplications to automatically understand and identify their goal and the harm\nlevel that they can inflect. Motivated by the emerging need for annotated\ndatasets that tackle these kinds of problems in the context of COVID-19, this\npaper builds and releases AraCOVID19-SSD a manually annotated Arabic COVID-19\nsarcasm and sentiment detection dataset containing 5,162 tweets. To confirm the\npractical utility of the built dataset, it has been carefully analyzed and\ntested using several classification models.", "published": "2021-10-05 11:24:24", "link": "http://arxiv.org/abs/2110.01948v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Objective Few-shot Learning for Fair Classification", "abstract": "In this paper, we propose a general framework for mitigating the disparities\nof the predicted classes with respect to secondary attributes within the data\n(e.g., race, gender etc.). Our proposed method involves learning a\nmulti-objective function that in addition to learning the primary objective of\npredicting the primary class labels from the data, also employs a\nclustering-based heuristic to minimize the disparities of the class label\ndistribution with respect to the cluster memberships, with the assumption that\neach cluster should ideally map to a distinct combination of attribute values.\nExperiments demonstrate effective mitigation of cognitive biases on a benchmark\ndataset without the use of annotations of secondary attribute values (the\nzero-shot case) or with the use of a small number of attribute value\nannotations (the few-shot case).", "published": "2021-10-05 11:28:58", "link": "http://arxiv.org/abs/2110.01951v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FoodChem: A food-chemical relation extraction model", "abstract": "In this paper, we present FoodChem, a new Relation Extraction (RE) model for\nidentifying chemicals present in the composition of food entities, based on\ntextual information provided in biomedical peer-reviewed scientific literature.\nThe RE task is treated as a binary classification problem, aimed at identifying\nwhether the contains relation exists between a food-chemical entity pair. This\nis accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models.\nFor evaluation purposes, a novel dataset with annotated contains relations in\nfood-chemical entity pairs is generated, in a golden and silver version. The\nmodels are integrated into a voting scheme in order to produce the silver\nversion of the dataset which we use for augmenting the individual models, while\nthe manually annotated golden version is used for their evaluation. Out of the\nthree evaluated models, the BioBERT model achieves the best results, with a\nmacro averaged F1 score of 0.902 in the unbalanced augmentation setting.", "published": "2021-10-05 13:07:33", "link": "http://arxiv.org/abs/2110.02019v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT\n  Models", "abstract": "This paper describes our approach (ur-iw-hnt) for the Shared Task of\nGermEval2021 to identify toxic, engaging, and fact-claiming comments. We\nsubmitted three runs using an ensembling strategy by majority (hard) voting\nwith multiple different BERT models of three different types: German-based,\nTwitter-based, and multilingual models. All ensemble models outperform single\nmodels, while BERTweet is the winner of all individual models in every subtask.\nTwitter-based models perform better than GermanBERT models, and multilingual\nmodels perform worse but by a small margin.", "published": "2021-10-05 13:48:20", "link": "http://arxiv.org/abs/2110.02042v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical information matters: Text classification via tree based\n  graph neural network", "abstract": "Text classification is a primary task in natural language processing (NLP).\nRecently, graph neural networks (GNNs) have developed rapidly and been applied\nto text classification tasks. As a special kind of graph data, the tree has a\nsimpler data structure and can provide rich hierarchical information for text\nclassification. Inspired by the structural entropy, we construct the coding\ntree of the graph by minimizing the structural entropy and propose HINT, which\naims to make full use of the hierarchical information contained in the text for\nthe task of text classification. Specifically, we first establish a dependency\nparsing graph for each text. Then we designed a structural entropy minimization\nalgorithm to decode the key information in the graph and convert each graph to\nits corresponding coding tree. Based on the hierarchical structure of the\ncoding tree, the representation of the entire graph is obtained by updating the\nrepresentation of non-leaf nodes in the coding tree layer by layer. Finally, we\npresent the effectiveness of hierarchical information in text classification.\nExperimental results show that HINT outperforms the state-of-the-art methods on\npopular benchmarks while having a simple structure and few parameters.", "published": "2021-10-05 13:55:47", "link": "http://arxiv.org/abs/2110.02047v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NaRLE: Natural Language Models using Reinforcement Learning with Emotion\n  Feedback", "abstract": "Current research in dialogue systems is focused on conversational assistants\nworking on short conversations in either task-oriented or open domain settings.\nIn this paper, we focus on improving task-based conversational assistants\nonline, primarily those working on document-type conversations (e.g., emails)\nwhose contents may or may not be completely related to the assistant's task. We\npropose \"NARLE\" a deep reinforcement learning (RL) framework for improving the\nnatural language understanding (NLU) component of dialogue systems online\nwithout the need to collect human labels for customer data. The proposed\nsolution associates user emotion with the assistant's action and uses that to\nimprove NLU models using policy gradients. For two intent classification\nproblems, we empirically show that using reinforcement learning to fine tune\nthe pre-trained supervised learning models improves performance up to 43%.\nFurthermore, we demonstrate the robustness of the method to partial and noisy\nimplicit feedback.", "published": "2021-10-05 16:24:19", "link": "http://arxiv.org/abs/2110.02148v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Analyzing the Impact of COVID-19 on Economy from the Perspective of\n  Users Reviews", "abstract": "One of the most important incidents in the world in 2020 is the outbreak of\nthe Coronavirus. Users on social networks publish a large number of comments\nabout this event. These comments contain important hidden information of public\nopinion regarding this pandemic. In this research, a large number of\nCoronavirus-related tweets are considered and analyzed using natural language\nprocessing and information retrieval science. Initially, the location of the\ntweets is determined using a dictionary prepared through the Geo-Names\ngeographic database, which contains detailed and complete information of places\nsuch as city names, streets, and postal codes. Then, using a large dictionary\nprepared from the terms of economics, related tweets are extracted and\nsentiments corresponded to tweets are analyzed with the help of the RoBERTa\nlanguage-based model, which has high accuracy and good performance. Finally,\nthe frequency chart of tweets related to the economy and their sentiment scores\n(positive and negative tweets) is plotted over time for the entire world and\nthe top 10 economies. From the analysis of the charts, we learn that the reason\nfor publishing economic tweets is not only the increase in the number of people\ninfected with the Coronavirus but also imposed restrictions and lockdowns in\ncountries. The consequences of these restrictions include the loss of millions\nof jobs and the economic downturn.", "published": "2021-10-05 17:44:41", "link": "http://arxiv.org/abs/2110.02198v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Using Psuedolabels for training Sentiment Classifiers makes the model\n  generalize better across datasets", "abstract": "The problem statement addressed in this work is : For a public sentiment\nclassification API, how can we set up a classifier that works well on different\ntypes of data, having limited ability to annotate data from across domains. We\nshow that given a large amount of unannotated data from across different\ndomains and pseudolabels on this dataset generated by a classifier trained on a\nsmall annotated dataset from one domain, we can train a sentiment classifier\nthat generalizes better across different datasets.", "published": "2021-10-05 17:47:15", "link": "http://arxiv.org/abs/2110.02200v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EntQA: Entity Linking as Question Answering", "abstract": "A conventional approach to entity linking is to first find mentions in a\ngiven document and then infer their underlying entities in the knowledge base.\nA well-known limitation of this approach is that it requires finding mentions\nwithout knowing their entities, which is unnatural and difficult. We present a\nnew model that does not suffer from this limitation called EntQA, which stands\nfor Entity linking as Question Answering. EntQA first proposes candidate\nentities with a fast retrieval module, and then scrutinizes the document to\nfind mentions of each candidate with a powerful reader module. Our approach\ncombines progress in entity linking with that in open-domain question answering\nand capitalizes on pretrained models for dense entity retrieval and reading\ncomprehension. Unlike in previous works, we do not rely on a mention-candidates\ndictionary or large-scale weak supervision. EntQA achieves strong results on\nthe GERBIL benchmarking platform.", "published": "2021-10-05 21:39:57", "link": "http://arxiv.org/abs/2110.02369v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging the Inductive Bias of Large Language Models for Abstract\n  Textual Reasoning", "abstract": "Large natural language models (such as GPT-3 or T5) demonstrate impressive\nabilities across a range of general NLP tasks. Here, we show that the knowledge\nembedded in such models provides a useful inductive bias, not just on\ntraditional NLP tasks, but also in the nontraditional task of training a\nsymbolic reasoning engine. We observe that these engines learn quickly and\ngeneralize in a natural way that reflects human intuition. For example,\ntraining such a system to model block-stacking might naturally generalize to\nstacking other types of objects because of structure in the real world that has\nbeen partially captured by the language describing it. We study several\nabstract textual reasoning tasks, such as object manipulation and navigation,\nand demonstrate multiple types of generalization to novel scenarios and the\nsymbols that comprise them. We also demonstrate the surprising utility of\n\\textit{compositional learning}, where a learner dedicated to mastering a\ncomplicated task gains an advantage by training on relevant simpler tasks\ninstead of jumping straight to the complicated task.", "published": "2021-10-05 21:40:46", "link": "http://arxiv.org/abs/2110.02370v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analyzing the Effects of Reasoning Types on Cross-Lingual Transfer\n  Performance", "abstract": "Multilingual language models achieve impressive zero-shot accuracies in many\nlanguages in complex tasks such as Natural Language Inference (NLI). Examples\nin NLI (and equivalent complex tasks) often pertain to various types of\nsub-tasks, requiring different kinds of reasoning. Certain types of reasoning\nhave proven to be more difficult to learn in a monolingual context, and in the\ncrosslingual context, similar observations may shed light on zero-shot transfer\nefficiency and few-shot sample selection. Hence, to investigate the effects of\ntypes of reasoning on transfer performance, we propose a category-annotated\nmultilingual NLI dataset and discuss the challenges to scale monolingual\nannotations to multiple languages. We statistically observe interesting effects\nthat the confluence of reasoning types and language similarities have on\ntransfer performance.", "published": "2021-10-05 22:36:46", "link": "http://arxiv.org/abs/2110.02386v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Modeling using LMUs: 10x Better Data Efficiency or Improved\n  Scaling Compared to Transformers", "abstract": "Recent studies have demonstrated that the performance of transformers on the\ntask of language modeling obeys a power-law relationship with model size over\nsix orders of magnitude. While transformers exhibit impressive scaling, their\nperformance hinges on processing large amounts of data, and their computational\nand memory requirements grow quadratically with sequence length. Motivated by\nthese considerations, we construct a Legendre Memory Unit based model that\nintroduces a general prior for sequence processing and exhibits an $O(n)$ and\n$O(n \\ln n)$ (or better) dependency for memory and computation respectively.\nOver three orders of magnitude, we show that our new architecture attains the\nsame accuracy as transformers with 10x fewer tokens. We also show that for the\nsame amount of training our model improves the loss over transformers about as\nmuch as transformers improve over LSTMs. Additionally, we demonstrate that\nadding global self-attention complements our architecture and the augmented\nmodel improves performance even further.", "published": "2021-10-05 23:20:37", "link": "http://arxiv.org/abs/2110.02402v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "COVIDRead: A Large-scale Question Answering Dataset on COVID-19", "abstract": "During this pandemic situation, extracting any relevant information related\nto COVID-19 will be immensely beneficial to the community at large. In this\npaper, we present a very important resource, COVIDRead, a Stanford Question\nAnswering Dataset (SQuAD) like dataset over more than 100k question-answer\npairs. The dataset consists of Context-Answer-Question triples. Primarily the\nquestions from the context are constructed in an automated way. After that, the\nsystem-generated questions are manually checked by hu-mans annotators. This is\na precious resource that could serve many purposes, ranging from common people\nqueries regarding this very uncommon disease to managing articles by\neditors/associate editors of a journal. We establish several end-to-end neural\nnetwork based baseline models that attain the lowest F1 of 32.03% and the\nhighest F1 of 37.19%. To the best of our knowledge, we are the first to provide\nthis kind of QA dataset in such a large volume on COVID-19. This dataset\ncreates a new avenue of carrying out research on COVID-19 by providing a\nbenchmark dataset and a baseline model.", "published": "2021-10-05 07:38:06", "link": "http://arxiv.org/abs/2110.09321v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LegalNLP -- Natural Language Processing methods for the Brazilian Legal\n  Language", "abstract": "We present and make available pre-trained language models (Phraser, Word2Vec,\nDoc2Vec, FastText, and BERT) for the Brazilian legal language, a Python package\nwith functions to facilitate their use, and a set of demonstrations/tutorials\ncontaining some applications involving them. Given that our material is built\nupon legal texts coming from several Brazilian courts, this initiative is\nextremely helpful for the Brazilian legal field, which lacks other open and\nspecific tools and language models. Our main objective is to catalyze the use\nof natural language processing tools for legal texts analysis by the Brazilian\nindustry, government, and academia, providing the necessary tools and\naccessible material.", "published": "2021-10-05 04:44:37", "link": "http://arxiv.org/abs/2110.15709v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ContractNLI: A Dataset for Document-level Natural Language Inference for\n  Contracts", "abstract": "Reviewing contracts is a time-consuming procedure that incurs large expenses\nto companies and social inequality to those who cannot afford it. In this work,\nwe propose \"document-level natural language inference (NLI) for contracts\", a\nnovel, real-world application of NLI that addresses such problems. In this\ntask, a system is given a set of hypotheses (such as \"Some obligations of\nAgreement may survive termination.\") and a contract, and it is asked to\nclassify whether each hypothesis is \"entailed by\", \"contradicting to\" or \"not\nmentioned by\" (neutral to) the contract as well as identifying \"evidence\" for\nthe decision as spans in the contract. We annotated and release the largest\ncorpus to date consisting of 607 annotated contracts. We then show that\nexisting models fail badly on our task and introduce a strong baseline, which\n(1) models evidence identification as multi-label classification over spans\ninstead of trying to predict start and end tokens, and (2) employs more\nsophisticated context segmentation for dealing with long documents. We also\nshow that linguistic characteristics of contracts, such as negations by\nexceptions, are contributing to the difficulty of this task and that there is\nmuch room for improvement.", "published": "2021-10-05 03:22:31", "link": "http://arxiv.org/abs/2110.01799v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey On Neural Word Embeddings", "abstract": "Understanding human language has been a sub-challenge on the way of\nintelligent machines. The study of meaning in natural language processing (NLP)\nrelies on the distributional hypothesis where language elements get meaning\nfrom the words that co-occur within contexts. The revolutionary idea of\ndistributed representation for a concept is close to the working of a human\nmind in that the meaning of a word is spread across several neurons, and a loss\nof activation will only slightly affect the memory retrieval process.\n  Neural word embeddings transformed the whole field of NLP by introducing\nsubstantial improvements in all NLP tasks. In this survey, we provide a\ncomprehensive literature review on neural word embeddings. We give theoretical\nfoundations and describe existing work by an interplay between word embeddings\nand language modelling. We provide broad coverage on neural word embeddings,\nincluding early word embeddings, embeddings targeting specific semantic\nrelations, sense embeddings, morpheme embeddings, and finally, contextual\nrepresentations. Finally, we describe benchmark datasets in word embeddings'\nperformance evaluation and downstream tasks along with the performance results\nof/due to word embeddings.", "published": "2021-10-05 03:37:57", "link": "http://arxiv.org/abs/2110.01804v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Data Augmentation Approaches in Natural Language Processing: A Survey", "abstract": "As an effective strategy, data augmentation (DA) alleviates data scarcity\nscenarios where deep learning techniques may fail. It is widely applied in\ncomputer vision then introduced to natural language processing and achieves\nimprovements in many tasks. One of the main focuses of the DA methods is to\nimprove the diversity of training data, thereby helping the model to better\ngeneralize to unseen testing data. In this survey, we frame DA methods into\nthree categories based on the diversity of augmented data, including\nparaphrasing, noising, and sampling. Our paper sets out to analyze DA methods\nin detail according to the above categories. Further, we also introduce their\napplications in NLP tasks as well as the challenges. Some helpful resources are\nprovided in the appendix.", "published": "2021-10-05 07:35:32", "link": "http://arxiv.org/abs/2110.01852v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FooDI-ML: a large multi-language dataset of food, drinks and groceries\n  images and descriptions", "abstract": "In this paper we introduce the FooDI-ML dataset. This dataset contains over\n1.5M unique images and over 9.5M store names, product names descriptions, and\ncollection sections gathered from the Glovo application. The data made\navailable corresponds to food, drinks and groceries products from 37 countries\nin Europe, the Middle East, Africa and Latin America. The dataset comprehends\n33 languages, including 870K samples of languages of countries from Eastern\nEurope and Western Asia such as Ukrainian and Kazakh, which have been so far\nunderrepresented in publicly available visio-linguistic datasets. The dataset\nalso includes widely spoken languages such as Spanish and English. To assist\nfurther research, we include benchmarks over two tasks: text-image retrieval\nand conditional image generation.", "published": "2021-10-05 13:33:08", "link": "http://arxiv.org/abs/2110.02035v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Waypoint Models for Instruction-guided Navigation in Continuous\n  Environments", "abstract": "Little inquiry has explicitly addressed the role of action spaces in\nlanguage-guided visual navigation -- either in terms of its effect on\nnavigation success or the efficiency with which a robotic agent could execute\nthe resulting trajectory. Building on the recently released VLN-CE setting for\ninstruction following in continuous environments, we develop a class of\nlanguage-conditioned waypoint prediction networks to examine this question. We\nvary the expressivity of these models to explore a spectrum between low-level\nactions and continuous waypoint prediction. We measure task performance and\nestimated execution time on a profiled LoCoBot robot. We find more expressive\nmodels result in simpler, faster to execute trajectories, but lower-level\nactions can achieve better navigation metrics by approximating shortest paths\nbetter. Further, our models outperform prior work in VLN-CE and set a new\nstate-of-the-art on the public leaderboard -- increasing success rate by 4%\nwith our best model on this challenging task.", "published": "2021-10-05 17:55:49", "link": "http://arxiv.org/abs/2110.02207v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "BERT Attends the Conversation: Improving Low-Resource Conversational ASR", "abstract": "We propose new, data-efficient training tasks for BERT models that improve\nperformance of automatic speech recognition (ASR) systems on conversational\nspeech. We include past conversational context and fine-tune BERT on transcript\ndisambiguation without external data to rescore ASR candidates. Our results\nshow word error rate recoveries up to 37.2%. We test our methods in\nlow-resource data domains, both in language (Norwegian), tone (spontaneous,\nconversational), and topics (parliament proceedings and customer service phone\ncalls). These techniques are applicable to any ASR system and do not require\nany additional data, provided a pre-trained BERT model. We also show how the\nperformance of our context-augmented rescoring methods strongly depends on the\ndegree of spontaneity and nature of the conversation.", "published": "2021-10-05 18:15:15", "link": "http://arxiv.org/abs/2110.02267v2", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Co-training an Unsupervised Constituency Parser with Weak Supervision", "abstract": "We introduce a method for unsupervised parsing that relies on bootstrapping\nclassifiers to identify if a node dominates a specific span in a sentence.\nThere are two types of classifiers, an inside classifier that acts on a span,\nand an outside classifier that acts on everything outside of a given span.\nThrough self-training and co-training with the two classifiers, we show that\nthe interplay between them helps improve the accuracy of both, and as a result,\neffectively parse. A seed bootstrapping technique prepares the data to train\nthese classifiers. Our analyses further validate that such an approach in\nconjunction with weak supervision using prior branching knowledge of a known\nlanguage (left/right-branching) and minimal heuristics injects strong inductive\nbias into the parser, achieving 63.1 F$_1$ on the English (PTB) test set. In\naddition, we show the effectiveness of our architecture by evaluating on\ntreebanks for Chinese (CTB) and Japanese (KTB) and achieve new state-of-the-art\nresults. Our code and pre-trained models are available at\nhttps://github.com/Nickil21/weakly-supervised-parsing.", "published": "2021-10-05 18:45:06", "link": "http://arxiv.org/abs/2110.02283v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Conditional Text Generation for Aspect-Based Sentiment\n  Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is an NLP task that entails processing\nuser-generated reviews to determine (i) the target being evaluated, (ii) the\naspect category to which it belongs, and (iii) the sentiment expressed towards\nthe target and aspect pair. In this article, we propose transforming ABSA into\nan abstract summary-like conditional text generation task that uses targets,\naspects, and polarities to generate auxiliary statements. To demonstrate the\nefficacy of our task formulation and a proposed system, we fine-tune a\npre-trained model for conditional text generation tasks to get new\nstate-of-the-art results on a few restaurant domains and urban neighborhoods\ndomain benchmark datasets.", "published": "2021-10-05 20:08:25", "link": "http://arxiv.org/abs/2110.02334v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Interpreting intermediate convolutional layers in unsupervised acoustic\n  word classification", "abstract": "Understanding how deep convolutional neural networks classify data has been\nsubject to extensive research. This paper proposes a technique to visualize and\ninterpret intermediate layers of unsupervised deep convolutional networks by\naveraging over individual feature maps in each convolutional layer and\ninferring underlying distributions of words with non-linear regression\ntechniques. A GAN-based architecture (ciwGAN arXiv:2006.02951) that includes a\nGenerator, a Discriminator, and a classifier was trained on unlabeled sliced\nlexical items from TIMIT. The training process results in a deep convolutional\nnetwork that learns to classify words into discrete classes only from the\nrequirement of the Generator to output informative data. This classifier\nnetwork has no access to the training data -- only to the generated data. We\npropose a technique to visualize individual convolutional layers in the\nclassifier that yields highly informative time-series data for each\nconvolutional layer and apply it to unobserved test data. Using non-linear\nregression, we infer underlying distributions for each word which allows us to\nanalyze both absolute values and shapes of individual words at different\nconvolutional layers, as well as perform hypothesis testing on their acoustic\nproperties. The technique also allows us to test individual phone contrasts and\nhow they are represented at each layer.", "published": "2021-10-05 21:53:32", "link": "http://arxiv.org/abs/2110.02375v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Aging with Audio-Visual Style Transfer", "abstract": "Face aging techniques have used generative adversarial networks (GANs) and\nstyle transfer learning to transform one's appearance to look younger/older.\nIdentity is maintained by conditioning these generative networks on a learned\nvector representation of the source content. In this work, we apply a similar\napproach to age a speaker's voice, referred to as voice aging. We first analyze\nthe classification of a speaker's age by training a convolutional neural\nnetwork (CNN) on the speaker's voice and face data from Common Voice and\nVoxCeleb datasets. We generate aged voices from style transfer to transform an\ninput spectrogram to various ages and demonstrate our method on a mobile app.", "published": "2021-10-05 23:33:28", "link": "http://arxiv.org/abs/2110.02411v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Fast Contextual Adaptation with Neural Associative Memory for On-Device\n  Personalized Speech Recognition", "abstract": "Fast contextual adaptation has shown to be effective in improving Automatic\nSpeech Recognition (ASR) of rare words and when combined with an on-device\npersonalized training, it can yield an even better recognition result. However,\nthe traditional re-scoring approaches based on an external language model is\nprone to diverge during the personalized training. In this work, we introduce a\nmodel-based end-to-end contextual adaptation approach that is decoder-agnostic\nand amenable to on-device personalization. Our on-device simulation experiments\ndemonstrate that the proposed approach outperforms the traditional re-scoring\ntechnique by 12% relative WER and 15.7% entity mention specific F1-score in a\ncontinues personalization scenario.", "published": "2021-10-05 00:33:09", "link": "http://arxiv.org/abs/2110.02220v2", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "eess.AS"}
{"title": "Is Attention always needed? A Case Study on Language Identification from\n  Speech", "abstract": "Language Identification (LID) is a crucial preliminary process in the field\nof Automatic Speech Recognition (ASR) that involves the identification of a\nspoken language from audio samples. Contemporary systems that can process\nspeech in multiple languages require users to expressly designate one or more\nlanguages prior to utilization. The LID task assumes a significant role in\nscenarios where ASR systems are unable to comprehend the spoken language in\nmultilingual settings, leading to unsuccessful speech recognition outcomes. The\npresent study introduces convolutional recurrent neural network (CRNN) based\nLID, designed to operate on the Mel-frequency Cepstral Coefficient (MFCC)\ncharacteristics of audio samples. Furthermore, we replicate certain\nstate-of-the-art methodologies, specifically the Convolutional Neural Network\n(CNN) and Attention-based Convolutional Recurrent Neural Network (CRNN with\nattention), and conduct a comparative analysis with our CRNN-based approach. We\nconducted comprehensive evaluations on thirteen distinct Indian languages and\nour model resulted in over 98\\% classification accuracy. The LID model exhibits\nhigh-performance levels ranging from 97% to 100% for languages that are\nlinguistically similar. The proposed LID model exhibits a high degree of\nextensibility to additional languages and demonstrates a strong resistance to\nnoise, achieving 91.2% accuracy in a noisy setting when applied to a European\nLanguage (EU) dataset.", "published": "2021-10-05 16:38:57", "link": "http://arxiv.org/abs/2110.03427v3", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.LG"}
{"title": "DNSMOS P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric\n  to Evaluate Noise Suppressors", "abstract": "Human subjective evaluation is the gold standard to evaluate speech quality\noptimized for human perception. Perceptual objective metrics serve as a proxy\nfor subjective scores. We have recently developed a non-intrusive speech\nquality metric called Deep Noise Suppression Mean Opinion Score (DNSMOS) using\nthe scores from ITU-T Rec. P.808 subjective evaluation. The P.808 scores\nreflect the overall quality of the audio clip. ITU-T Rec. P.835 subjective\nevaluation framework gives the standalone quality scores of speech and\nbackground noise in addition to the overall quality. In this work, we train an\nobjective metric based on P.835 human ratings that outputs 3 scores: i) speech\nquality (SIG), ii) background noise quality (BAK), and iii) the overall quality\n(OVRL) of the audio. The developed metric is highly correlated with human\nratings, with a Pearson's Correlation Coefficient (PCC)=0.94 for SIG and\nPCC=0.98 for BAK and OVRL. This is the first non-intrusive P.835 predictor we\nare aware of. DNSMOS P.835 is made publicly available as an Azure service.", "published": "2021-10-05 00:42:13", "link": "http://arxiv.org/abs/2110.01763v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Deep Optimization of Parametric IIR Filters for Audio Equalization", "abstract": "This paper describes a novel Deep Learning method for the design of IIR\nparametric filters for automatic audio equalization. A simple and effective\nneural architecture, named BiasNet, is proposed to determine the IIR equalizer\nparameters. An output denormalization technique is used to obtain accurate\ntuning of the IIR filters center frequency, quality factor and gain. All layers\ninvolved in the proposed method are shown to be differentiable, allowing\nbackpropagation to optimize the network weights and achieve, after a number of\ntraining iterations, the optimal output. The parameters are optimized with\nrespect to a loss function based on a spectral distance between the measured\nand desired magnitude response, and a regularization term used to achieve a\nspatialization of the acoustc scene. Two scenarios with different\ncharacteristics were considered for the experimental evaluation: a room and a\ncar cabin. The performance of the proposed method improves over the baseline\ntechniques and achieves an almost flat band. Moreover IIR filters provide a\nconsistently lower computational cost during runtime with respect to FIR\nfilters.", "published": "2021-10-05 14:19:30", "link": "http://arxiv.org/abs/2110.02077v1", "categories": ["eess.AS", "cs.SD", "68T07 (Primary) 14C20 (Secondary)", "I.2.0; F.2.1"], "primary_category": "eess.AS"}
{"title": "Late reverberation suppression using U-nets", "abstract": "In real-world settings, speech signals are almost always affected by\nreverberation produced by the working environment; these corrupted signals need\nto be \\emph{dereverberated} prior to performing, e.g., speech recognition,\nspeech-to-text conversion, compression, or general audio enhancement. In this\npaper, we propose a supervised dereverberation technique using \\emph{U-nets\nwith skip connections}, which are fully-convolutional encoder-decoder networks\nwith layers arranged in the form of an \"U\" and connections that \"skip\" some\nlayers. Building on this architecture, we address speech dereverberation\nthrough the lens of Late Reverberation Suppression (LS). Via experiments on\nsynthetic and real-world data with different noise levels and reverberation\nsettings, we show that our proposed method termed \"LS U-net\" improves quality,\nintelligibility and other performance metrics compared to the original U-net\nmethod and it is on par with the state-of-the-art GAN-based approaches.", "published": "2021-10-05 16:18:09", "link": "http://arxiv.org/abs/2110.02144v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Detection of blue whale vocalisations using a temporal-domain\n  convolutional neural network", "abstract": "We present a framework for detecting blue whale vocalisations from acoustic\nsubmarine recordings. The proposed methodology comprises three stages: i) a\npreprocessing step where the audio recordings are conditioned through\nnormalisation, filtering, and denoising; ii) a label-propagation mechanism to\nensure the consistency of the annotations of the whale vocalisations, and iii)\na convolutional neural network that receives audio samples. Based on 34\nreal-world submarine recordings (28 for training and 6 for testing) we obtained\npromising performance indicators including an Accuracy of 85.4\\% and a Recall\nof 93.5\\%. Furthermore, even for the cases where our detector did not match the\nground-truth labels, a visual inspection validates the ability of our approach\nto detect possible parts of whale calls unlabelled as such due to not being\ncomplete calls.", "published": "2021-10-05 16:32:16", "link": "http://arxiv.org/abs/2110.02151v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Manifold learning-supported estimation of relative transfer functions\n  for spatial filtering", "abstract": "Many spatial filtering algorithms used for voice capture in, e.g.,\nteleconferencing applications, can benefit from or even rely on knowledge of\nRelative Transfer Functions (RTFs). Accordingly, many RTF estimators have been\nproposed which, however, suffer from performance degradation under acoustically\nadverse conditions or need prior knowledge on the properties of the interfering\nsources. While state-of-the-art RTF estimators ignore prior knowledge about the\nacoustic enclosure, audio signal processing algorithms for teleconferencing\nequipment are often operating in the same or at least a similar acoustic\nenclosure, e.g., a car or an office, such that training data can be collected.\nIn this contribution, we use such data to train Variational Autoencoders (VAEs)\nin an unsupervised manner and apply the trained VAEs to enhance imprecise RTF\nestimates. Furthermore, a hybrid between classic RTF estimation and the trained\nVAE is investigated. Comprehensive experiments with real-world data confirm the\nefficacy for the proposed method.", "published": "2021-10-05 17:28:38", "link": "http://arxiv.org/abs/2110.02189v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modelling of the Fender Bassman 5F6-A Tone Stack", "abstract": "This paper outlines the procedure for the effective modelling of a complex\nanalogue filter circuit. The Fender Bassman 5F6-A is a circuit commonly\nemployed in guitar amplifiers to shape the tonal characteristics of the\namplifier output. On first inspection this circuit may look rather simple,\nhowever the controls are not orthogonal, resulting in complicated filter\ncoefficients as the controls are varied. This in turn can make the circuit\ndifficult to analyse without the use of mathematical emulation tools such as\nPSPICE or MATLAB. First the circuit is described, a method of analysis is\nproposed and general expressions for continuous-time coefficients are given. A\nMATLAB model is then produced and the frequency responses of which are shown.", "published": "2021-10-05 18:47:17", "link": "http://arxiv.org/abs/2110.02285v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unsupervised Speech Segmentation and Variable Rate Representation\n  Learning using Segmental Contrastive Predictive Coding", "abstract": "Typically, unsupervised segmentation of speech into the phone and word-like\nunits are treated as separate tasks and are often done via different methods\nwhich do not fully leverage the inter-dependence of the two tasks. Here, we\nunify them and propose a technique that can jointly perform both, showing that\nthese two tasks indeed benefit from each other. Recent attempts employ\nself-supervised learning, such as contrastive predictive coding (CPC), where\nthe next frame is predicted given past context. However, CPC only looks at the\naudio signal's frame-level structure. We overcome this limitation with a\nsegmental contrastive predictive coding (SCPC) framework to model the signal\nstructure at a higher level, e.g., phone level. A convolutional neural network\nlearns frame-level representation from the raw waveform via noise-contrastive\nestimation (NCE). A differentiable boundary detector finds variable-length\nsegments, which are then used to optimize a segment encoder via NCE to learn\nsegment representations. The differentiable boundary detector allows us to\ntrain frame-level and segment-level encoders jointly. Experiments show that our\nsingle model outperforms existing phone and word segmentation methods on TIMIT\nand Buckeye datasets. We discover that phone class impacts the boundary\ndetection performance, and the boundaries between successive vowels or\nsemivowels are the most difficult to identify. Finally, we use SCPC to extract\nspeech features at the segment level rather than at uniformly spaced frame\nlevel (e.g., 10 ms) and produce variable rate representations that change\naccording to the contents of the utterance. We can lower the feature extraction\nrate from the typical 100 Hz to as low as 14.5 Hz on average while still\noutperforming the MFCC features on the linear phone classification task.", "published": "2021-10-05 20:25:29", "link": "http://arxiv.org/abs/2110.02345v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Neural Pitch-Shifting and Time-Stretching with Controllable LPCNet", "abstract": "Modifying the pitch and timing of an audio signal are fundamental audio\nediting operations with applications in speech manipulation, audio-visual\nsynchronization, and singing voice editing and synthesis. Thus far, methods for\npitch-shifting and time-stretching that use digital signal processing (DSP)\nhave been favored over deep learning approaches due to their speed and\nrelatively higher quality. However, even existing DSP-based methods for\npitch-shifting and time-stretching induce artifacts that degrade audio quality.\nIn this paper, we propose Controllable LPCNet (CLPCNet), an improved LPCNet\nvocoder capable of pitch-shifting and time-stretching of speech. For objective\nevaluation, we show that CLPCNet performs pitch-shifting of speech on unseen\ndatasets with high accuracy relative to prior neural methods. For subjective\nevaluation, we demonstrate that the quality and naturalness of pitch-shifting\nand time-stretching with CLPCNet on unseen datasets meets or exceeds\ncompetitive neural- or DSP-based approaches.", "published": "2021-10-05 21:04:32", "link": "http://arxiv.org/abs/2110.02360v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sound Event Detection Transformer: An Event-based End-to-End Model for\n  Sound Event Detection", "abstract": "Sound event detection (SED) has gained increasing attention with its wide\napplication in surveillance, video indexing, etc. Existing models in SED mainly\ngenerate frame-level prediction, converting it into a sequence multi-label\nclassification problem. A critical issue with the frame-based model is that it\npursues the best frame-level prediction rather than the best event-level\nprediction. Besides, it needs post-processing and cannot be trained in an\nend-to-end way. This paper firstly presents the one-dimensional Detection\nTransformer (1D-DETR), inspired by Detection Transformer for image object\ndetection. Furthermore, given the characteristics of SED, the audio query\nbranch and a one-to-many matching strategy for fine-tuning the model are added\nto 1D-DETR to form Sound Event Detection Transformer (SEDT). To our knowledge,\nSEDT is the first event-based and end-to-end SED model. Experiments are\nconducted on the URBAN-SED dataset and the DCASE2019 Task4 dataset, and both\nshow that SEDT can achieve competitive performance.", "published": "2021-10-05 12:56:23", "link": "http://arxiv.org/abs/2110.02011v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "3D-MOV: Audio-Visual LSTM Autoencoder for 3D Reconstruction of Multiple\n  Objects from Video", "abstract": "3D object reconstructions of transparent and concave structured objects, with\ninferred material properties, remains an open research problem for robot\nnavigation in unstructured environments. In this paper, we propose a multimodal\nsingle- and multi-frame neural network for 3D reconstructions using\naudio-visual inputs. Our trained reconstruction LSTM autoencoder 3D-MOV accepts\nmultiple inputs to account for a variety of surface types and views. Our neural\nnetwork produces high-quality 3D reconstructions using voxel representation.\nBased on Intersection-over-Union (IoU), we evaluate against other baseline\nmethods using synthetic audio-visual datasets ShapeNet and Sound20K with impact\nsounds and bounding box annotations. To the best of our knowledge, our single-\nand multi-frame model is the first audio-visual reconstruction neural network\nfor 3D geometry and material representation.", "published": "2021-10-05 23:23:19", "link": "http://arxiv.org/abs/2110.02404v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Echo-Reconstruction: Audio-Augmented 3D Scene Reconstruction", "abstract": "Reflective and textureless surfaces such as windows, mirrors, and walls can\nbe a challenge for object and scene reconstruction. These surfaces are often\npoorly reconstructed and filled with depth discontinuities and holes, making it\ndifficult to cohesively reconstruct scenes that contain these planar\ndiscontinuities. We propose Echoreconstruction, an audio-visual method that\nuses the reflections of sound to aid in geometry and audio reconstruction for\nvirtual conferencing, teleimmersion, and other AR/VR experience. The mobile\nphone prototype emits pulsed audio, while recording video for RGB-based 3D\nreconstruction and audio-visual classification. Reflected sound and images from\nthe video are input into our audio (EchoCNN-A) and audio-visual (EchoCNN-AV)\nconvolutional neural networks for surface and sound source detection, depth\nestimation, and material classification. The inferences from these\nclassifications enhance scene 3D reconstructions containing open spaces and\nreflective surfaces by depth filtering, inpainting, and placement of unmixed\nsound sources in the scene. Our prototype, VR demo, and experimental results\nfrom real-world and virtual scenes with challenging surfaces and sound indicate\nhigh success rates on classification of material, depth estimation, and\nclosed/open surfaces, leading to considerable visual and audio improvement in\n3D scenes (see Figure 1).", "published": "2021-10-05 23:23:51", "link": "http://arxiv.org/abs/2110.02405v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
