{"title": "Uncertainty-aware Self-training for Low-resource Neural Sequence\n  Labeling", "abstract": "Neural sequence labeling (NSL) aims at assigning labels for input language\ntokens, which covers a broad range of applications, such as named entity\nrecognition (NER) and slot filling, etc. However, the satisfying results\nachieved by traditional supervised-based approaches heavily depend on the large\namounts of human annotation data, which may not be feasible in real-world\nscenarios due to data privacy and computation efficiency issues. This paper\npresents SeqUST, a novel uncertain-aware self-training framework for NSL to\naddress the labeled data scarcity issue and to effectively utilize unlabeled\ndata. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural\nnetwork (BNN) to perform uncertainty estimation at the token level and then\nselect reliable language tokens from unlabeled data based on the model\nconfidence and certainty. A well-designed masked sequence labeling task with a\nnoise-robust loss supports robust training, which aims to suppress the problem\nof noisy pseudo labels. In addition, we develop a Gaussian-based consistency\nregularization technique to further improve the model robustness on\nGaussian-distributed perturbed representations. This effectively alleviates the\nover-fitting dilemma originating from pseudo-labeled augmented data. Extensive\nexperiments over six benchmarks demonstrate that our SeqUST framework\neffectively improves the performance of self-training, and consistently\noutperforms strong baselines by a large margin in low-resource scenarios", "published": "2023-02-17 02:40:04", "link": "http://arxiv.org/abs/2302.08659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DREEAM: Guiding Attention with Evidence for Improving Document-Level\n  Relation Extraction", "abstract": "Document-level relation extraction (DocRE) is the task of identifying all\nrelations between each entity pair in a document. Evidence, defined as\nsentences containing clues for the relationship between an entity pair, has\nbeen shown to help DocRE systems focus on relevant texts, thus improving\nrelation extraction. However, evidence retrieval (ER) in DocRE faces two major\nissues: high memory consumption and limited availability of annotations. This\nwork aims at addressing these issues to improve the usage of ER in DocRE.\nFirst, we propose DREEAM, a memory-efficient approach that adopts evidence\ninformation as the supervisory signal, thereby guiding the attention modules of\nthe DocRE system to assign high weights to evidence. Second, we propose a\nself-training strategy for DREEAM to learn ER from automatically-generated\nevidence on massive data without evidence annotations. Experimental results\nreveal that our approach exhibits state-of-the-art performance on the DocRED\nbenchmark for both DocRE and ER. To the best of our knowledge, DREEAM is the\nfirst approach to employ ER self-training.", "published": "2023-02-17 03:54:31", "link": "http://arxiv.org/abs/2302.08675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "False perspectives on human language: why statistics needs linguistics", "abstract": "A sharp tension exists about the nature of human language between two\nopposite parties: those who believe that statistical surface distributions, in\nparticular using measures like surprisal, provide a better understanding of\nlanguage processing, vs. those who believe that discrete hierarchical\nstructures implementing linguistic information such as syntactic ones are a\nbetter tool. In this paper, we show that this dichotomy is a false one. Relying\non the fact that statistical measures can be defined on the basis of either\nstructural or non-structural models, we provide empirical evidence that only\nmodels of surprisal that reflect syntactic structure are able to account for\nlanguage regularities.", "published": "2023-02-17 11:40:32", "link": "http://arxiv.org/abs/2302.08822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages", "abstract": "Africa is home to over 2,000 languages from more than six language families\nand has the highest linguistic diversity among all continents. These include 75\nlanguages with at least one million speakers each. Yet, there is little NLP\nresearch conducted on African languages. Crucial to enabling such research is\nthe availability of high-quality annotated datasets. In this paper, we\nintroduce AfriSenti, a sentiment analysis benchmark that contains a total of\n>110,000 tweets in 14 African languages (Amharic, Algerian Arabic, Hausa, Igbo,\nKinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo,\nSwahili, Tigrinya, Twi, Xitsonga, and Yor\\`ub\\'a) from four language families.\nThe tweets were annotated by native speakers and used in the AfriSenti-SemEval\nshared task (The AfriSenti Shared Task had over 200 participants. See website\nat https://afrisenti-semeval.github.io). We describe the data collection\nmethodology, annotation process, and the challenges we dealt with when curating\neach dataset. We further report baseline experiments conducted on the different\ndatasets and discuss their usefulness.", "published": "2023-02-17 15:40:12", "link": "http://arxiv.org/abs/2302.08956v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Like a Good Nearest Neighbor: Practical Content Moderation and Text\n  Classification", "abstract": "Few-shot text classification systems have impressive capabilities but are\ninfeasible to deploy and use reliably due to their dependence on prompting and\nbillion-parameter language models. SetFit (Tunstall et al., 2022) is a recent,\npractical approach that fine-tunes a Sentence Transformer under a contrastive\nlearning paradigm and achieves similar results to more unwieldy systems.\nInexpensive text classification is important for addressing the problem of\ndomain drift in all classification tasks, and especially in detecting harmful\ncontent, which plagues social media platforms. Here, we propose Like a Good\nNearest Neighbor (LaGoNN), a modification to SetFit that introduces no\nlearnable parameters but alters input text with information from its nearest\nneighbor, for example, the label and text, in the training data, making novel\ndata appear similar to an instance on which the model was optimized. LaGoNN is\neffective at flagging undesirable content and text classification, and improves\nthe performance of SetFit. To demonstrate the value of LaGoNN, we conduct a\nthorough study of text classification systems in the context of content\nmoderation under four label distributions, and in general and multilingual\nclassification settings.", "published": "2023-02-17 15:43:29", "link": "http://arxiv.org/abs/2302.08957v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fine-Grained Information: Identifying the Type and Location of\n  Translation Errors", "abstract": "Fine-grained information on translation errors is helpful for the translation\nevaluation community. Existing approaches can not synchronously consider error\nposition and type, failing to integrate the error information of both. In this\npaper, we propose Fine-Grained Translation Error Detection (FG-TED) task,\naiming at identifying both the position and the type of translation errors on\ngiven source-hypothesis sentence pairs. Besides, we build an FG-TED model to\npredict the \\textbf{addition} and \\textbf{omission} errors -- two typical\ntranslation accuracy errors. First, we use a word-level classification paradigm\nto form our model and use the shortcut learning reduction to relieve the\ninfluence of monolingual features. Besides, we construct synthetic datasets for\nmodel training, and relieve the disagreement of data labeling in authoritative\ndatasets, making the experimental benchmark concordant. Experiments show that\nour model can identify both error type and position concurrently, and gives\nstate-of-the-art results on the restored dataset. Our model also delivers more\nreliable predictions on low-resource and transfer scenarios than existing\nbaselines. The related datasets and the source code will be released in the\nfuture.", "published": "2023-02-17 16:20:33", "link": "http://arxiv.org/abs/2302.08975v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Med-EASi: Finely Annotated Dataset and Models for Controllable\n  Simplification of Medical Texts", "abstract": "Automatic medical text simplification can assist providers with\npatient-friendly communication and make medical texts more accessible, thereby\nimproving health literacy. But curating a quality corpus for this task requires\nthe supervision of medical experts. In this work, we present\n$\\textbf{Med-EASi}$ ($\\underline{\\textbf{Med}}$ical dataset for\n$\\underline{\\textbf{E}}$laborative and $\\underline{\\textbf{A}}$bstractive\n$\\underline{\\textbf{Si}}$mplification), a uniquely crowdsourced and finely\nannotated dataset for supervised simplification of short medical texts. Its\n$\\textit{expert-layman-AI collaborative}$ annotations facilitate\n$\\textit{controllability}$ over text simplification by marking four kinds of\ntextual transformations: elaboration, replacement, deletion, and insertion. To\nlearn medical text simplification, we fine-tune T5-large with four different\nstyles of input-output combinations, leading to two control-free and two\ncontrollable versions of the model. We add two types of\n$\\textit{controllability}$ into text simplification, by using a multi-angle\ntraining approach: $\\textit{position-aware}$, which uses in-place annotated\ninputs and outputs, and $\\textit{position-agnostic}$, where the model only\nknows the contents to be edited, but not their positions. Our results show that\nour fine-grained annotations improve learning compared to the unannotated\nbaseline. Furthermore, $\\textit{position-aware}$ control generates better\nsimplification than the $\\textit{position-agnostic}$ one. The data and code are\navailable at https://github.com/Chandrayee/CTRL-SIMP.", "published": "2023-02-17 21:50:13", "link": "http://arxiv.org/abs/2302.09155v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extraction of Constituent Factors of Digestion Efficiency in Information\n  Transfer by Media Composed of Texts and Images", "abstract": "The development and spread of information and communication technologies have\nincreased and diversified information. However, the increase in the volume and\nthe selection of information does not necessarily promote understanding. In\naddition, conventional evaluations of information transfer have focused only on\nthe arrival of information to the receivers. They need to sufficiently take\ninto account the receivers' understanding of the information after it has been\nacquired, which is the original purpose of the evaluation. In this study, we\npropose the concept of \"information digestion,\" which refers to the receivers'\ncorrect understanding of the acquired information, its contents, and its\npurpose. In the experiment, we proposed an evaluation model of information\ndigestibility using hierarchical factor analysis and extracted factors that\nconstitute digestibility by four types of media.", "published": "2023-02-17 23:45:02", "link": "http://arxiv.org/abs/2302.09189v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Propaganda Processing", "abstract": "Propaganda campaigns have long been used to influence public opinion via\ndisseminating biased and/or misleading information. Despite the increasing\nprevalence of propaganda content on the Internet, few attempts have been made\nby AI researchers to analyze such content. We introduce the task of multimodal\npropaganda processing, where the goal is to automatically analyze propaganda\ncontent. We believe that this task presents a long-term challenge to AI\nresearchers and that successful processing of propaganda could bring machine\nunderstanding one important step closer to human understanding. We discuss the\ntechnical challenges associated with this task and outline the steps that need\nto be taken to address it.", "published": "2023-02-17 05:49:55", "link": "http://arxiv.org/abs/2302.08709v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Natural Response Generation for Chinese Reading Comprehension", "abstract": "Machine reading comprehension (MRC) is an important area of conversation\nagents and draws a lot of attention. However, there is a notable limitation to\ncurrent MRC benchmarks: The labeled answers are mostly either spans extracted\nfrom the target corpus or the choices of the given candidates, ignoring the\nnatural aspect of high-quality responses. As a result, MRC models trained on\nthese datasets can not generate human-like responses in real QA scenarios. To\nthis end, we construct a new dataset called Penguin to promote the research of\nMRC, providing a training and test bed for natural response generation to real\nscenarios. Concretely, Penguin consists of 200k training data with high-quality\nfluent, and well-informed responses. Penguin is the first benchmark towards\nnatural response generation in Chinese MRC on a relatively large scale. To\naddress the challenges in Penguin, we develop two strong baselines: end-to-end\nand two-stage frameworks. Following that, we further design Prompt-BART:\nfine-tuning the pre-trained generative language models with a mixture of prefix\nprompts in Penguin. Extensive experiments validated the effectiveness of this\ndesign.", "published": "2023-02-17 11:31:05", "link": "http://arxiv.org/abs/2302.08817v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Massively Multilingual Shallow Fusion with Large Language Models", "abstract": "While large language models (LLM) have made impressive progress in natural\nlanguage processing, it remains unclear how to utilize them in improving\nautomatic speech recognition (ASR). In this work, we propose to train a single\nmultilingual language model (LM) for shallow fusion in multiple languages. We\npush the limits of the multilingual LM to cover up to 84 languages by scaling\nup using a mixture-of-experts LLM, i.e., generalist language model (GLaM). When\nthe number of experts increases, GLaM dynamically selects only two at each\ndecoding step to keep the inference computation roughly constant. We then apply\nGLaM to a multilingual shallow fusion task based on a state-of-the-art\nend-to-end model. Compared to a dense LM of similar computation during\ninference, GLaM reduces the WER of an English long-tail test set by 4.4%\nrelative. In a multilingual shallow fusion task, GLaM improves 41 out of 50\nlanguages with an average relative WER reduction of 3.85%, and a maximum\nreduction of 10%. Compared to the baseline model, GLaM achieves an average WER\nreduction of 5.53% over 43 languages.", "published": "2023-02-17 14:46:38", "link": "http://arxiv.org/abs/2302.08917v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "More Data Types More Problems: A Temporal Analysis of Complexity,\n  Stability, and Sensitivity in Privacy Policies", "abstract": "Collecting personally identifiable information (PII) on data subjects has\nbecome big business. Data brokers and data processors are part of a\nmulti-billion-dollar industry that profits from collecting, buying, and selling\nconsumer data. Yet there is little transparency in the data collection industry\nwhich makes it difficult to understand what types of data are being collected,\nused, and sold, and thus the risk to individual data subjects. In this study,\nwe examine a large textual dataset of privacy policies from 1997-2019 in order\nto investigate the data collection activities of data brokers and data\nprocessors. We also develop an original lexicon of PII-related terms\nrepresenting PII data types curated from legislative texts. This mesoscale\nanalysis looks at privacy policies overtime on the word, topic, and network\nlevels to understand the stability, complexity, and sensitivity of privacy\npolicies over time. We find that (1) privacy legislation correlates with\nchanges in stability and turbulence of PII data types in privacy policies; (2)\nthe complexity of privacy policies decreases over time and becomes more\nregularized; (3) sensitivity rises over time and shows spikes that are\ncorrelated with events when new privacy legislation is introduced.", "published": "2023-02-17 15:21:24", "link": "http://arxiv.org/abs/2302.08936v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Entry Separation using a Mixed Visual and Textual Language Model:\n  Application to 19th century French Trade Directories", "abstract": "When extracting structured data from repetitively organized documents, such\nas dictionaries, directories, or even newspapers, a key challenge is to\ncorrectly segment what constitutes the basic text regions for the target\ndatabase. Traditionally, such a problem was tackled as part of the layout\nanalysis and was mostly based on visual clues for dividing (top-down)\napproaches. Some agglomerating (bottom-up) approaches started to consider\ntextual information to link similar contents, but they required a proper\nover-segmentation of fine-grained units. In this work, we propose a new\npragmatic approach whose efficiency is demonstrated on 19th century French\nTrade Directories. We propose to consider two sub-problems: coarse layout\ndetection (text columns and reading order), which is assumed to be effective\nand not detailed here, and a fine-grained entry separation stage for which we\npropose to adapt a state-of-the-art Named Entity Recognition (NER) approach. By\ninjecting special visual tokens, coding, for instance, indentation or breaks,\ninto the token stream of the language model used for NER purpose, we can\nleverage both textual and visual knowledge simultaneously. Code, data, results\nand models are available at\nhttps://github.com/soduco/paper-entryseg-icdar23-code,\nhttps://huggingface.co/HueyNemud/ (icdar23-entrydetector* variants)", "published": "2023-02-17 15:30:44", "link": "http://arxiv.org/abs/2302.08948v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Designing and Evaluating Interfaces that Highlight News Coverage\n  Diversity Using Discord Questions", "abstract": "Modern news aggregators do the hard work of organizing a large news stream,\ncreating collections for a given news story with tens of source options. This\npaper shows that navigating large source collections for a news story can be\nchallenging without further guidance. In this work, we design three interfaces\n-- the Annotated Article, the Recomposed Article, and the Question Grid --\naimed at accompanying news readers in discovering coverage diversity while they\nread. A first usability study with 10 journalism experts confirms the designed\ninterfaces all reveal coverage diversity and determine each interface's\npotential use cases and audiences. In a second usability study, we developed\nand implemented a reading exercise with 95 novice news readers to measure\nexposure to coverage diversity. Results show that Annotated Article users are\nable to answer questions 34% more completely than with two existing interfaces\nwhile finding the interface equally easy to use.", "published": "2023-02-17 16:59:31", "link": "http://arxiv.org/abs/2302.08997v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Conveying the Predicted Future to Users: A Case Study of Story Plot\n  Prediction", "abstract": "Creative writing is hard: Novelists struggle with writer's block daily. While\nautomatic story generation has advanced recently, it is treated as a \"toy task\"\nfor advancing artificial intelligence rather than helping people. In this\npaper, we create a system that produces a short description that narrates a\npredicted plot using existing story generation approaches. Our goal is to\nassist writers in crafting a consistent and compelling story arc. We conducted\nexperiments on Amazon Mechanical Turk (AMT) to examine the quality of the\ngenerated story plots in terms of consistency and storiability. The results\nshow that short descriptions produced by our frame-enhanced GPT-2 (FGPT-2) were\nrated as the most consistent and storiable among all models; FGPT-2's outputs\neven beat some random story snippets written by humans. Next, we conducted a\npreliminary user study using a story continuation task where AMT workers were\ngiven access to machine-generated story plots and asked to write a follow-up\nstory. FGPT-2 could positively affect the writing process, though people favor\nother baselines more. Our study shed some light on the possibilities of future\ncreative writing support systems beyond the scope of completing sentences. Our\ncode is available at: https://github.com/appleternity/Story-Plot-Generation.", "published": "2023-02-17 20:10:55", "link": "http://arxiv.org/abs/2302.09122v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Cluster-Guided Label Generation in Extreme Multi-Label Classification", "abstract": "For extreme multi-label classification (XMC), existing classification-based\nmodels poorly perform for tail labels and often ignore the semantic relations\namong labels, like treating \"Wikipedia\" and \"Wiki\" as independent and separate\nlabels. In this paper, we cast XMC as a generation task (XLGen), where we\nbenefit from pre-trained text-to-text models. However, generating labels from\nthe extremely large label space is challenging without any constraints or\nguidance. We, therefore, propose to guide label generation using label cluster\ninformation to hierarchically generate lower-level labels. We also find that\nfrequency-based label ordering and using decoding ensemble methods are critical\nfactors for the improvements in XLGen. XLGen with cluster guidance\nsignificantly outperforms the classification and generation baselines on tail\nlabels, and also generally improves the overall performance in four popular XMC\nbenchmarks. In human evaluation, we also find XLGen generates unseen but\nplausible labels. Our code is now available at\nhttps://github.com/alexa/xlgen-eacl-2023.", "published": "2023-02-17 21:20:36", "link": "http://arxiv.org/abs/2302.09150v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "KILM: Knowledge Injection into Encoder-Decoder Language Models", "abstract": "Large pre-trained language models (PLMs) have been shown to retain implicit\nknowledge within their parameters. To enhance this implicit knowledge, we\npropose Knowledge Injection into Language Models (KILM), a novel approach that\ninjects entity-related knowledge into encoder-decoder PLMs, via a generative\nknowledge infilling objective through continued pre-training. This is done\nwithout architectural modifications to the PLMs or adding additional\nparameters. Experimental results over a suite of knowledge-intensive tasks\nspanning numerous datasets show that KILM enables models to retain more\nknowledge and hallucinate less, while preserving their original performance on\ngeneral NLU and NLG tasks. KILM also demonstrates improved zero-shot\nperformances on tasks such as entity disambiguation, outperforming\nstate-of-the-art models having 30x more parameters.", "published": "2023-02-17 22:48:07", "link": "http://arxiv.org/abs/2302.09170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Subtask Graph Generation from Instructional Videos", "abstract": "Real-world tasks consist of multiple inter-dependent subtasks (e.g., a dirty\npan needs to be washed before it can be used for cooking). In this work, we aim\nto model the causal dependencies between such subtasks from instructional\nvideos describing the task. This is a challenging problem since complete\ninformation about the world is often inaccessible from videos, which demands\nrobust learning mechanisms to understand the causal structure of events. We\npresent Multimodal Subtask Graph Generation (MSG2), an approach that constructs\na Subtask Graph defining the dependency between a task's subtasks relevant to a\ntask from noisy web videos. Graphs generated by our multimodal approach are\ncloser to human-annotated graphs compared to prior approaches. MSG2 further\nperforms the downstream task of next subtask prediction 85% and 30% more\naccurately than recent video transformer models in the ProceL and CrossTask\ndatasets, respectively.", "published": "2023-02-17 03:41:38", "link": "http://arxiv.org/abs/2302.08672v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Deep Implicit Distribution Alignment Networks for Cross-Corpus Speech\n  Emotion Recognition", "abstract": "In this paper, we propose a novel deep transfer learning method called deep\nimplicit distribution alignment networks (DIDAN) to deal with cross-corpus\nspeech emotion recognition (SER) problem, in which the labeled training\n(source) and unlabeled testing (target) speech signals come from different\ncorpora. Specifically, DIDAN first adopts a simple deep regression network\nconsisting of a set of convolutional and fully connected layers to directly\nregress the source speech spectrums into the emotional labels such that the\nproposed DIDAN can own the emotion discriminative ability. Then, such ability\nis transferred to be also applicable to the target speech samples regardless of\ncorpus variance by resorting to a well-designed regularization term called\nimplicit distribution alignment (IDA). Unlike widely-used maximum mean\ndiscrepancy (MMD) and its variants, the proposed IDA absorbs the idea of sample\nreconstruction to implicitly align the distribution gap, which enables DIDAN to\nlearn both emotion discriminative and corpus invariant features from speech\nspectrums. To evaluate the proposed DIDAN, extensive cross-corpus SER\nexperiments on widely-used speech emotion corpora are carried out. Experimental\nresults show that the proposed DIDAN can outperform lots of recent\nstate-of-the-art methods in coping with the cross-corpus SER tasks.", "published": "2023-02-17 14:51:37", "link": "http://arxiv.org/abs/2302.08921v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Handling the Alignment for Wake Word Detection: A Comparison Between\n  Alignment-Based, Alignment-Free and Hybrid Approaches", "abstract": "Wake word detection exists in most intelligent homes and portable devices. It\noffers these devices the ability to \"wake up\" when summoned at a low cost of\npower and computing. This paper focuses on understanding alignment's role in\ndeveloping a wake-word system that answers a generic phrase. We discuss three\napproaches. The first is alignment-based, where the model is trained with\nframe-wise cross-entropy. The second is alignment-free, where the model is\ntrained with CTC. The third, proposed by us, is a hybrid solution in which the\nmodel is trained with a small set of aligned data and then tuned with a\nsizeable unaligned dataset. We compare the three approaches and evaluate the\nimpact of the different aligned-to-unaligned ratios for hybrid training. Our\nresults show that the alignment-free system performs better than the\nalignment-based for the target operating point, and with a small fraction of\nthe data (20%), we can train a model that complies with our initial\nconstraints.", "published": "2023-02-17 15:33:47", "link": "http://arxiv.org/abs/2302.08950v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate\n  Fairytales", "abstract": "The quality of text-to-image generation is continuously improving, yet the\nboundaries of its applicability are still unclear. In particular, refinement of\nthe text input with the objective of achieving better results - commonly called\nprompt engineering - so far seems to have not been geared towards work with\npre-existing texts. We investigate whether text-to-image generation and prompt\nengineering could be used to generate basic illustrations of popular\nfairytales. Using Midjourney v4, we engage in action research with a dual aim:\nto attempt to generate 5 believable illustrations for each of 5 popular\nfairytales, and to define a prompt engineering process that starts from a\npre-existing text and arrives at an illustration of it. We arrive at a\ntentative 4-stage process: i) initial prompt, ii) composition adjustment, iii)\nstyle refinement, and iv) variation selection. We also discuss three reasons\nwhy the generation model struggles with certain illustrations: difficulties\nwith counts, bias from stereotypical configurations and inability to depict\noverly fantastic situations. Our findings are not limited to the specific\ngeneration model and are intended to be generalisable to future ones.", "published": "2023-02-17 15:49:19", "link": "http://arxiv.org/abs/2302.08961v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2"], "primary_category": "cs.CL"}
{"title": "CK-Transformer: Commonsense Knowledge Enhanced Transformers for\n  Referring Expression Comprehension", "abstract": "The task of multimodal referring expression comprehension (REC), aiming at\nlocalizing an image region described by a natural language expression, has\nrecently received increasing attention within the research comminity. In this\npaper, we specifically focus on referring expression comprehension with\ncommonsense knowledge (KB-Ref), a task which typically requires reasoning\nbeyond spatial, visual or semantic information. We propose a novel framework\nfor Commonsense Knowledge Enhanced Transformers (CK-Transformer) which\neffectively integrates commonsense knowledge into the representations of\nobjects in an image, facilitating identification of the target objects referred\nto by the expressions. We conduct extensive experiments on several benchmarks\nfor the task of KB-Ref. Our results show that the proposed CK-Transformer\nachieves a new state of the art, with an absolute improvement of 3.14% accuracy\nover the existing state of the art.", "published": "2023-02-17 17:49:26", "link": "http://arxiv.org/abs/2302.09027v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Complex QA and language models hybrid architectures, Survey", "abstract": "This paper reviews the state-of-the-art of language models architectures and\nstrategies for \"complex\" question-answering (QA, CQA, CPS) with a focus on\nhybridization. Large Language Models (LLM) are good at leveraging public data\non standard problems but once you want to tackle more specific complex\nquestions or problems (e.g. How does the concept of personal freedom vary\nbetween different cultures ? What is the best mix of power generation methods\nto reduce climate change ?) you may need specific architecture, knowledge,\nskills, methods, sensitive data protection, explainability, human approval and\nversatile feedback... Recent projects like ChatGPT and GALACTICA have allowed\nnon-specialists to grasp the great potential as well as the equally strong\nlimitations of LLM in complex QA. In this paper, we start by reviewing required\nskills and evaluation techniques. We integrate findings from the robust\ncommunity edited research papers BIG, BLOOM and HELM which open source,\nbenchmark and analyze limits and challenges of LLM in terms of tasks complexity\nand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as\na baseline. We discuss some challenges associated with complex QA, including\ndomain adaptation, decomposition and efficient multi-step QA, long form and\nnon-factoid QA, safety and multi-sensitivity data protection, multimodal\nsearch, hallucinations, explainability and truthfulness, temporal reasoning. We\nanalyze current solutions and promising research trends, using elements such\nas: hybrid LLM architectural patterns, training and prompting strategies,\nactive human reinforcement learning supervised with AI, neuro-symbolic and\nstructured knowledge grounding, program synthesis, iterated decomposition and\nothers.", "published": "2023-02-17 18:31:31", "link": "http://arxiv.org/abs/2302.09051v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Task Graph Generation from Instructional Video Transcripts", "abstract": "This work explores the problem of generating task graphs of real-world\nactivities. Different from prior formulations, we consider a setting where text\ntranscripts of instructional videos performing a real-world activity (e.g.,\nmaking coffee) are provided and the goal is to identify the key steps relevant\nto the task as well as the dependency relationship between these key steps. We\npropose a novel task graph generation approach that combines the reasoning\ncapabilities of instruction-tuned language models along with clustering and\nranking components to generate accurate task graphs in a completely\nunsupervised manner. We show that the proposed approach generates more accurate\ntask graphs compared to a supervised learning approach on tasks from the ProceL\nand CrossTask datasets.", "published": "2023-02-17 22:50:08", "link": "http://arxiv.org/abs/2302.09173v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Bounding the Capabilities of Large Language Models in Open Text\n  Generation with Prompt Constraints", "abstract": "The limits of open-ended generative models are unclear, yet increasingly\nimportant. What causes them to succeed and what causes them to fail? In this\npaper, we take a prompt-centric approach to analyzing and bounding the\nabilities of open-ended generative models. We present a generic methodology of\nanalysis with two challenging prompt constraint types: structural and\nstylistic. These constraint types are categorized into a set of well-defined\nconstraints that are analyzable by a single prompt. We then systematically\ncreate a diverse set of simple, natural, and useful prompts to robustly analyze\neach individual constraint. Using the GPT-3 text-davinci-002 model as a case\nstudy, we generate outputs from our collection of prompts and analyze the\nmodel's generative failures. We also show the generalizability of our proposed\nmethod on other large models like BLOOM and OPT. Our results and our in-context\nmitigation strategies reveal open challenges for future research. We have\npublicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM.", "published": "2023-02-17 23:30:28", "link": "http://arxiv.org/abs/2302.09185v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hate Speech and Offensive Language Detection using an Emotion-aware\n  Shared Encoder", "abstract": "The rise of emergence of social media platforms has fundamentally altered how\npeople communicate, and among the results of these developments is an increase\nin online use of abusive content. Therefore, automatically detecting this\ncontent is essential for banning inappropriate information, and reducing\ntoxicity and violence on social media platforms. The existing works on hate\nspeech and offensive language detection produce promising results based on\npre-trained transformer models, however, they considered only the analysis of\nabusive content features generated through annotated datasets. This paper\naddresses a multi-task joint learning approach which combines external\nemotional features extracted from another corpora in dealing with the\nimbalanced and scarcity of labeled datasets. Our analysis are using two\nwell-known Transformer-based models, BERT and mBERT, where the later is used to\naddress abusive content detection in multi-lingual scenarios. Our model jointly\nlearns abusive content detection with emotional features by sharing\nrepresentations through transformers' shared encoder. This approach increases\ndata efficiency, reduce overfitting via shared representations, and ensure fast\nlearning by leveraging auxiliary information. Our findings demonstrate that\nemotional knowledge helps to more reliably identify hate speech and offensive\nlanguage across datasets. Our hate speech detection Multi-task model exhibited\n3% performance improvement over baseline models, but the performance of\nmulti-task models were not significant for offensive language detection task.\nMore interestingly, in both tasks, multi-task models exhibits less false\npositive errors compared to single task scenario.", "published": "2023-02-17 09:31:06", "link": "http://arxiv.org/abs/2302.08777v1", "categories": ["cs.CL", "cs.IT", "cs.LG", "cs.SI", "math.IT"], "primary_category": "cs.CL"}
{"title": "Conformers are All You Need for Visual Speech Recognition", "abstract": "Visual speech recognition models extract visual features in a hierarchical\nmanner. At the lower level, there is a visual front-end with a limited temporal\nreceptive field that processes the raw pixels depicting the lips or faces. At\nthe higher level, there is an encoder that attends to the embeddings produced\nby the front-end over a large temporal receptive field. Previous work has\nfocused on improving the visual front-end of the model to extract more useful\nfeatures for speech recognition. Surprisingly, our work shows that complex\nvisual front-ends are not necessary. Instead of allocating resources to a\nsophisticated visual front-end, we find that a linear visual front-end paired\nwith a larger Conformer encoder results in lower latency, more efficient memory\nusage, and improved WER performance. We achieve a new state-of-the-art of 12.8%\nWER for visual speech recognition on the TED LRS3 dataset, which rivals the\nperformance of audio-only models from just four years ago.", "published": "2023-02-17 01:31:55", "link": "http://arxiv.org/abs/2302.10915v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "jazznet: A Dataset of Fundamental Piano Patterns for Music Audio Machine\n  Learning Research", "abstract": "This paper introduces the jazznet Dataset, a dataset of fundamental jazz\npiano music patterns for developing machine learning (ML) algorithms in music\ninformation retrieval (MIR). The dataset contains 162520 labeled piano\npatterns, including chords, arpeggios, scales, and chord progressions with\ntheir inversions, resulting in more than 26k hours of audio and a total size of\n95GB. The paper explains the dataset's composition, creation, and generation,\nand presents an open-source Pattern Generator using a method called\nDistance-Based Pattern Structures (DBPS), which allows researchers to easily\ngenerate new piano patterns simply by defining the distances between pitches\nwithin the musical patterns. We demonstrate that the dataset can help\nresearchers benchmark new models for challenging MIR tasks, using a\nconvolutional recurrent neural network (CRNN) and a deep convolutional neural\nnetwork. The dataset and code are available via:\nhttps://github.com/tosiron/jazznet.", "published": "2023-02-17 00:13:22", "link": "http://arxiv.org/abs/2302.08632v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Transformer-based Networks With Locality For Automatic Speaker\n  Verification", "abstract": "Recently, Transformer-based architectures have been explored for speaker\nembedding extraction. Although the Transformer employs the self-attention\nmechanism to efficiently model the global interaction between token embeddings,\nit is inadequate for capturing short-range local context, which is essential\nfor the accurate extraction of speaker information. In this study, we enhance\nthe Transformer with the enhanced locality modeling in two directions. First,\nwe propose the Locality-Enhanced Conformer (LE-Confomer) by introducing\ndepth-wise convolution and channel-wise attention into the Conformer blocks.\nSecond, we present the Speaker Swin Transformer (SST) by adapting the Swin\nTransformer, originally proposed for vision tasks, into speaker embedding\nnetwork. We evaluate the proposed approaches on the VoxCeleb datasets and a\nlarge-scale Microsoft internal multilingual (MS-internal) dataset. The proposed\nmodels achieve 0.75% EER on VoxCeleb 1 test set, outperforming the previously\nproposed Transformer-based models and CNN-based models, such as ResNet34 and\nECAPA-TDNN. When trained on the MS-internal dataset, the proposed models\nachieve promising results with 14.6% relative reduction in EER over the\nRes2Net50 model.", "published": "2023-02-17 01:04:51", "link": "http://arxiv.org/abs/2302.08639v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gaussian-smoothed Imbalance Data Improves Speech Emotion Recognition", "abstract": "In speech emotion recognition tasks, models learn emotional representations\nfrom datasets. We find the data distribution in the IEMOCAP dataset is very\nimbalanced, which may harm models to learn a better representation. To address\nthis issue, we propose a novel Pairwise-emotion Data Distribution Smoothing\n(PDDS) method. PDDS considers that the distribution of emotional data should be\nsmooth in reality, then applies Gaussian smoothing to emotion-pairs for\nconstructing a new training set with a smoother distribution. The required new\ndata are complemented using the mixup augmentation. As PDDS is model and\nmodality agnostic, it is evaluated with three SOTA models on the IEMOCAP\ndataset. The experimental results show that these models are improved by 0.2\\%\n- 4.8\\% and 1.5\\% - 5.9\\% in terms of WA and UA. In addition, an ablation study\ndemonstrates that the key advantage of PDDS is the reasonable data distribution\nrather than a simple data augmentation.", "published": "2023-02-17 01:50:46", "link": "http://arxiv.org/abs/2302.08650v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Build a training interface to install the bat's echolocation skills in\n  humans", "abstract": "Bats use a sophisticated ultrasonic sensing method called echolocation to\nrecognize the environment. Recently, it has been reported that sighted human\nparticipants with no prior experience in echolocation can improve their ability\nto perceive the spatial layout of various environments through training to\nlisten to echoes (Norman, et al., 2021). In this study, we developed the new\ntraining system for human echolocation using the eye-tracker. Binaural echoes\nof consecutive downward linear FM pulses that were inspired by feeding\nstrategies of echolocating bats were simulated using the wave equation finite\ndifference time domain method. The virtual echoes were presented to the sighted\nsubject in response to his or her eye movements on the monitor. The latency\nfrom eye gazing to the echo presentation wasn't audible delay to perceive. In a\npreliminary experiment in which the participants were asked to identify the\nshapes of the hidden target, the participants were found to concentrate their\ngaze on the edges of the hidden target on the monitor. We will conduct a\npsycho-acoustical experiment to examine the learning process of human\necholocation in a shape-identification task, which will lead to device\ndevelopment in the field of welfare engineering.", "published": "2023-02-17 10:31:49", "link": "http://arxiv.org/abs/2302.08794v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Lip-to-Speech Synthesis in the Wild with Multi-task Learning", "abstract": "Recent studies have shown impressive performance in Lip-to-speech synthesis\nthat aims to reconstruct speech from visual information alone. However, they\nhave been suffering from synthesizing accurate speech in the wild, due to\ninsufficient supervision for guiding the model to infer the correct content.\nDistinct from the previous methods, in this paper, we develop a powerful\nLip2Speech method that can reconstruct speech with correct contents from the\ninput lip movements, even in a wild environment. To this end, we design\nmulti-task learning that guides the model using multimodal supervision, i.e.,\ntext and audio, to complement the insufficient word representations of acoustic\nfeature reconstruction loss. Thus, the proposed framework brings the advantage\nof synthesizing speech containing the right content of multiple speakers with\nunconstrained sentences. We verify the effectiveness of the proposed method\nusing LRS2, LRS3, and LRW datasets.", "published": "2023-02-17 12:31:26", "link": "http://arxiv.org/abs/2302.08841v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
