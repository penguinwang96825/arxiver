{"title": "A Machine Learning Analysis of the Features in Deceptive and Credible\n  News", "abstract": "Fake news is a type of pervasive propaganda that spreads misinformation\nonline, taking advantage of social media's extensive reach to manipulate public\nperception. Over the past three years, fake news has become a focal discussion\npoint in the media due to its impact on the 2016 U.S. presidential election.\nFake news can have severe real-world implications: in 2016, a man walked into a\npizzeria carrying a rifle because he read that Hillary Clinton was harboring\nchildren as sex slaves. This project presents a high accuracy (87%) machine\nlearning classifier that determines the validity of news based on the word\ndistributions and specific linguistic and stylistic differences in the first\nfew sentences of an article. This can help readers identify the validity of an\narticle by looking for specific features in the opening lines aiding them in\nmaking informed decisions. Using a dataset of 2,107 articles from 30 different\nwebsites, this project establishes an understanding of the variations between\nfake and credible news by examining the model, dataset, and features. This\nclassifier appears to use the differences in word distribution, levels of tone\nauthenticity, and frequency of adverbs, adjectives, and nouns. The\ndifferentiation in the features of these articles can be used to improve future\nclassifiers. This classifier can also be further applied directly to browsers\nas a Google Chrome extension or as a filter for social media outlets or news\nwebsites to reduce the spread of misinformation.", "published": "2019-10-05 06:48:23", "link": "http://arxiv.org/abs/1910.02223v1", "categories": ["cs.CL", "J.5.5"], "primary_category": "cs.CL"}
{"title": "On the Limits of Learning to Actively Learn Semantic Representations", "abstract": "One of the goals of natural language understanding is to develop models that\nmap sentences into meaning representations. However, training such models\nrequires expensive annotation of complex structures, which hinders their\nadoption. Learning to actively-learn (LTAL) is a recent paradigm for reducing\nthe amount of labeled data by learning a policy that selects which samples\nshould be labeled. In this work, we examine LTAL for learning semantic\nrepresentations, such as QA-SRL. We show that even an oracle policy that is\nallowed to pick examples that maximize performance on the test set (and\nconstitutes an upper bound on the potential of LTAL), does not substantially\nimprove performance compared to a random policy. We investigate factors that\ncould explain this finding and show that a distinguishing characteristic of\nsuccessful applications of LTAL is the interaction between optimization and the\noracle policy selection process. In successful applications of LTAL, the\nexamples selected by the oracle policy do not substantially depend on the\noptimization procedure, while in our setup the stochastic nature of\noptimization strongly affects the examples selected by the oracle. We conclude\nthat the current applicability of LTAL for improving data efficiency in\nlearning semantic meaning representations is limited.", "published": "2019-10-05 07:49:12", "link": "http://arxiv.org/abs/1910.02228v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Transformer Revitalizes Character-based Neural Machine Translation:\n  An Investigation on Japanese-Vietnamese Translation Systems", "abstract": "While translating between East Asian languages, many works have discovered\nclear advantages of using characters as the translation unit. Unfortunately,\ntraditional recurrent neural machine translation systems hinder the practical\nusage of those character-based systems due to their architectural limitations.\nThey are unfavorable in handling extremely long sequences as well as highly\nrestricted in parallelizing the computations. In this paper, we demonstrate\nthat the new transformer architecture can perform character-based translation\nbetter than the recurrent one. We conduct experiments on a low-resource\nlanguage pair: Japanese-Vietnamese. Our models considerably outperform the\nstate-of-the-art systems which employ word-based recurrent architectures.", "published": "2019-10-05 09:16:44", "link": "http://arxiv.org/abs/1910.02238v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Diacritization, Lemmatization, Normalization, and Fine-Grained\n  Morphological Tagging", "abstract": "Semitic languages can be highly ambiguous, having several interpretations of\nthe same surface forms, and morphologically rich, having many morphemes that\nrealize several morphological features. This is further exacerbated for\ndialectal content, which is more prone to noise and lacks a standard\northography. The morphological features can be lexicalized, like lemmas and\ndiacritized forms, or non-lexicalized, like gender, number, and part-of-speech\ntags, among others. Joint modeling of the lexicalized and non-lexicalized\nfeatures can identify more intricate morphological patterns, which provide\nbetter context modeling, and further disambiguate ambiguous lexical choices.\nHowever, the different modeling granularity can make joint modeling more\ndifficult. Our approach models the different features jointly, whether\nlexicalized (on the character-level), where we also model surface form\nnormalization, or non-lexicalized (on the word-level). We use Arabic as a test\ncase, and achieve state-of-the-art results for Modern Standard Arabic, with 20%\nrelative error reduction, and Egyptian Arabic (a dialectal variant of Arabic),\nwith 11% reduction.", "published": "2019-10-05 13:31:39", "link": "http://arxiv.org/abs/1910.02267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Dimensional Linguistic Properties of the Word Embedding Space", "abstract": "Word embeddings have become a staple of several natural language processing\ntasks, yet much remains to be understood about their properties. In this work,\nwe analyze word embeddings in terms of their principal components and arrive at\na number of novel and counterintuitive observations. In particular, we\ncharacterize the utility of variance explained by the principal components as a\nproxy for downstream performance. Furthermore, through syntactic probing of the\nprincipal embedding space, we show that the syntactic information captured by a\nprincipal component does not correlate with the amount of variance it explains.\nConsequently, we investigate the limitations of variance based embedding\npost-processing and demonstrate that such post-processing is counter-productive\nin sentence classification and machine translation tasks. Finally, we offer a\nfew precautionary guidelines on applying variance based embedding\npost-processing and explain why non-isotropic geometry might be integral to\nword embedding performance.", "published": "2019-10-05 05:03:30", "link": "http://arxiv.org/abs/1910.02211v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Keyword Spotter Model for Crop Pest and Disease Monitoring from\n  Community Radio Data", "abstract": "In societies with well developed internet infrastructure, social media is the\nleading medium of communication for various social issues especially for\nbreaking news situations. In rural Uganda however, public community radio is\nstill a dominant means for news dissemination. Community radio gives audience\nto the general public especially to individuals living in rural areas, and thus\nplays an important role in giving a voice to those living in the broadcast\narea. It is an avenue for participatory communication and a tool relevant in\nboth economic and social development.This is supported by the rise to ubiquity\nof mobile phones providing access to phone-in or text-in talk shows. In this\npaper, we describe an approach to analysing the readily available community\nradio data with machine learning-based speech keyword spotting techniques. We\nidentify the keywords of interest related to agriculture and build models to\nautomatically identify these keywords from audio streams. Our contribution\nthrough these techniques is a cost-efficient and effective way to monitor food\nsecurity concerns particularly in rural areas. Through keyword spotting and\nradio talk show analysis, issues such as crop diseases, pests, drought and\nfamine can be captured and fed into an early warning system for stakeholders\nand policy makers.", "published": "2019-10-05 16:30:49", "link": "http://arxiv.org/abs/1910.02292v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Mapping Natural-language Problems to Formal-language Solutions Using\n  Structured Neural Representations", "abstract": "Generating formal-language programs represented by relational tuples, such as\nLisp programs or mathematical operations, to solve problems stated in natural\nlanguage is a challenging task because it requires explicitly capturing\ndiscrete symbolic structural information implicit in the input. However, most\ngeneral neural sequence models do not explicitly capture such structural\ninformation, limiting their performance on these tasks. In this paper, we\npropose a new encoder-decoder model based on a structured neural\nrepresentation, Tensor Product Representations (TPRs), for mapping\nNatural-language problems to Formal-language solutions, called TP-N2F. The\nencoder of TP-N2F employs TPR `binding' to encode natural-language symbolic\nstructure in vector space and the decoder uses TPR `unbinding' to generate, in\nsymbolic space, a sequential program represented by relational tuples, each\nconsisting of a relation (or operation) and a number of arguments. TP-N2F\nconsiderably outperforms LSTM-based seq2seq models on two benchmarks and\ncreates new state-of-the-art results. Ablation studies show that improvements\ncan be attributed to the use of structured TPRs explicitly in both the encoder\nand decoder. Analysis of the learned structures shows how TPRs enhance the\ninterpretability of TP-N2F.", "published": "2019-10-05 22:57:04", "link": "http://arxiv.org/abs/1910.02339v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamic Embedding on Textual Networks via a Gaussian Process", "abstract": "Textual network embedding aims to learn low-dimensional representations of\ntext-annotated nodes in a graph. Prior work in this area has typically focused\non fixed graph structures; however, real-world networks are often dynamic. We\naddress this challenge with a novel end-to-end node-embedding model, called\nDynamic Embedding for Textual Networks with a Gaussian Process (DetGP). After\ntraining, DetGP can be applied efficiently to dynamic graphs without\nre-training or backpropagation. The learned representation of each node is a\ncombination of textual and structural embeddings. Because the structure is\nallowed to be dynamic, our method uses the Gaussian process to take advantage\nof its non-parametric properties. To use both local and global graph\nstructures, diffusion is used to model multiple hops between neighbors. The\nrelative importance of global versus local structure for the embeddings is\nlearned automatically. With the non-parametric nature of the Gaussian process,\nupdating the embeddings for a changed graph structure requires only a forward\npass through the learned model. Considering link prediction and node\nclassification, experiments demonstrate the empirical effectiveness of our\nmethod compared to baseline approaches. We further show that DetGP can be\nstraightforwardly and efficiently applied to dynamic textual networks.", "published": "2019-10-05 01:16:33", "link": "http://arxiv.org/abs/1910.02187v3", "categories": ["cs.LG", "cs.CL", "cs.SI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning from Fact-checkers: Analysis and Generation of Fact-checking\n  Language", "abstract": "In fighting against fake news, many fact-checking systems comprised of\nhuman-based fact-checking sites (e.g., snopes.com and politifact.com) and\nautomatic detection systems have been developed in recent years. However,\nonline users still keep sharing fake news even when it has been debunked. It\nmeans that early fake news detection may be insufficient and we need another\ncomplementary approach to mitigate the spread of misinformation. In this paper,\nwe introduce a novel application of text generation for combating fake news. In\nparticular, we (1) leverage online users named \\emph{fact-checkers}, who cite\nfact-checking sites as credible evidences to fact-check information in public\ndiscourse; (2) analyze linguistic characteristics of fact-checking tweets; and\n(3) propose and build a deep learning framework to generate responses with\nfact-checking intention to increase the fact-checkers' engagement in\nfact-checking activities. Our analysis reveals that the fact-checkers tend to\nrefute misinformation and use formal language (e.g. few swear words and\nInternet slangs). Our framework successfully generates relevant responses, and\noutperforms competing models by achieving up to 30\\% improvements. Our\nqualitative study also confirms that the superiority of our generated responses\ncompared with responses generated from the existing models.", "published": "2019-10-05 03:23:45", "link": "http://arxiv.org/abs/1910.02202v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "JuICe: A Large Scale Distantly Supervised Dataset for Open Domain\n  Context-based Code Generation", "abstract": "Interactive programming with interleaved code snippet cells and natural\nlanguage markdown is recently gaining popularity in the form of Jupyter\nnotebooks, which accelerate prototyping and collaboration. To study code\ngeneration conditioned on a long context history, we present JuICe, a corpus of\n1.5 million examples with a curated test set of 3.7K instances based on online\nprogramming assignments. Compared with existing contextual code generation\ndatasets, JuICe provides refined human-curated data, open-domain code, and an\norder of magnitude more training data. Using JuICe, we train models for two\ntasks: (1) generation of the API call sequence in a code cell, and (2) full\ncode cell generation, both conditioned on the NL-Code history up to a\nparticular code cell. Experiments using current baseline code generation models\nshow that both context and distant supervision aid in generation, and that the\ndataset is challenging for current systems.", "published": "2019-10-05 05:51:45", "link": "http://arxiv.org/abs/1910.02216v2", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Hate Speech in Pixels: Detection of Offensive Memes towards Automatic\n  Moderation", "abstract": "This work addresses the challenge of hate speech detection in Internet memes,\nand attempts using visual information to automatically detect hate speech,\nunlike any previous work of our knowledge. Memes are pixel-based multimedia\ndocuments that contain photos or illustrations together with phrases which,\nwhen combined, usually adopt a funny meaning. However, hate memes are also used\nto spread hate through social networks, so their automatic detection would help\nreduce their harmful societal impact. Our results indicate that the model can\nlearn to detect some of the memes, but that the task is far from being solved\nwith this simple architecture. While previous work focuses on linguistic hate\nspeech, our experiments indicate how the visual modality can be much more\ninformative for hate speech detection than the linguistic one in memes. In our\nexperiments, we built a dataset of 5,020 memes to train and evaluate a\nmulti-layer perceptron over the visual and language representations, whether\nindependently or fused. The source code and mode and models are available\nhttps://github.com/imatge-upc/hate-speech-detection .", "published": "2019-10-05 22:05:43", "link": "http://arxiv.org/abs/1910.02334v1", "categories": ["cs.MM", "cs.CL", "cs.CV"], "primary_category": "cs.MM"}
{"title": "Few-shot tweet detection in emerging disaster events", "abstract": "Social media sources can provide crucial information in crisis situations,\nbut discovering relevant messages is not trivial. Methods have so far focused\non universal detection models for all kinds of crises or for certain crisis\ntypes (e.g. floods). Event-specific models could implement a more focused\nsearch area, but collecting data and training new models for a crisis that is\nalready in progress is costly and may take too much time for a prompt response.\nAs a compromise, manually collecting a small amount of example messages is\nfeasible. Few-shot models can generalize to unseen classes with such a small\nhandful of examples, and do not need be trained anew for each event. We compare\nhow few-shot approaches (matching networks and prototypical networks) perform\nfor this task. Since this is essentially a one-class problem, we also\ndemonstrate how a modified one-class version of prototypical models can be used\nfor this application.", "published": "2019-10-05 16:25:56", "link": "http://arxiv.org/abs/1910.02290v1", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "stat.ML"], "primary_category": "cs.LG"}
