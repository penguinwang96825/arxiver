{"title": "Few-Shot Structured Policy Learning for Multi-Domain and Multi-Task\n  Dialogues", "abstract": "Reinforcement learning has been widely adopted to model dialogue managers in\ntask-oriented dialogues. However, the user simulator provided by\nstate-of-the-art dialogue frameworks are only rough approximations of human\nbehaviour. The ability to learn from a small number of human interactions is\nhence crucial, especially on multi-domain and multi-task environments where the\naction space is large. We therefore propose to use structured policies to\nimprove sample efficiency when learning on these kinds of environments. We also\nevaluate the impact of learning from human vs simulated experts. Among the\ndifferent levels of structure that we tested, the graph neural networks (GNNs)\nshow a remarkable superiority by reaching a success rate above 80% with only 50\ndialogues, when learning from simulated experts. They also show superiority\nwhen learning from human experts, although a performance drop was observed,\nindicating a possible difficulty in capturing the variability of human\nstrategies. We therefore suggest to concentrate future research efforts on\nbridging the gap between human data, simulators and automatic evaluators in\ndialogue frameworks.", "published": "2023-02-22 08:18:49", "link": "http://arxiv.org/abs/2302.11199v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guiding Large Language Models via Directional Stimulus Prompting", "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding\nblack-box large language models (LLMs) toward specific desired outputs. Instead\nof directly adjusting LLMs, our method employs a small tunable policy model\n(e.g., T5) to generate an auxiliary directional stimulus prompt for each input\ninstance. These directional stimulus prompts act as nuanced, instance-specific\nhints and clues to guide LLMs in generating desired outcomes, such as including\nspecific keywords in the generated summary. Our approach sidesteps the\nchallenges of direct LLM tuning by optimizing the policy model to explore\ndirectional stimulus prompts that align LLMs with desired behaviors. The policy\nmodel can be optimized through 1) supervised fine-tuning using labeled data and\n2) reinforcement learning from offline or online rewards based on the LLM's\noutput. We assess our method across summarization, dialogue response\ngeneration, and chain-of-thought reasoning tasks. Our experiments demonstrate\nthat the framework consistently improves LLMs' (e.g., ChatGPT, Codex,\nInstructGPT) performance on these supervised tasks using minimal labeled data.\nNotably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances\nChatGPT's performance by an impressive 41.4%, matching or surpassing some fully\nsupervised start-of-the-art models. Additionally, the instance-specific\nchain-of-thought prompt generated by our approach improves InstructGPT's\nreasoning accuracy compared to human-crafted or automatically generated\nprompts. The code and data are publicly available at\n\\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.", "published": "2023-02-22 17:44:15", "link": "http://arxiv.org/abs/2302.11520v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Does In-Context Learning Help Prompt Tuning?", "abstract": "Fine-tuning large language models is becoming ever more impractical due to\ntheir rapidly-growing scale. This motivates the use of parameter-efficient\nadaptation methods such as prompt tuning (PT), which adds a small number of\ntunable embeddings to an otherwise frozen model, and in-context learning (ICL),\nin which demonstrations of the task are provided to the model in natural\nlanguage without any additional training. Recently, Singhal et al. (2022)\npropose ``instruction prompt tuning'' (IPT), which combines PT with ICL by\nconcatenating a natural language demonstration with learned prompt embeddings.\nWhile all of these methods have proven effective on different tasks, how they\ninteract with each other remains unexplored. In this paper, we empirically\nstudy when and how in-context examples improve prompt tuning by measuring the\neffectiveness of ICL, PT, and IPT on five text generation tasks with multiple\nbase language models. We observe that (1) IPT does \\emph{not} always outperform\nPT, and in fact requires the in-context demonstration to be semantically\nsimilar to the test input to yield improvements; (2) PT is unstable and\nexhibits high variance, but combining PT and ICL (into IPT) consistently\nreduces variance across all five tasks; and (3) prompts learned for a specific\nsource task via PT exhibit positive transfer when paired with in-context\nexamples of a different target task. Our results offer actionable insights on\nchoosing a suitable parameter-efficient adaptation method for a given task.", "published": "2023-02-22 17:45:12", "link": "http://arxiv.org/abs/2302.11521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FiNER-ORD: Financial Named Entity Recognition Open Research Dataset", "abstract": "Over the last two decades, the development of the CoNLL-2003 named entity\nrecognition (NER) dataset has helped enhance the capabilities of deep learning\nand natural language processing (NLP). The finance domain, characterized by its\nunique semantic and lexical variations for the same entities, presents specific\nchallenges to the NER task; thus, a domain-specific customized dataset is\ncrucial for advancing research in this field. In our work, we develop the first\nhigh-quality English Financial NER Open Research Dataset (FiNER-ORD). We\nbenchmark multiple pre-trained language models (PLMs) and large-language models\n(LLMs) on FiNER-ORD. We believe our proposed FiNER-ORD dataset will open future\nopportunities to use FiNER-ORD as a benchmark for financial domain-specific NER\nand NLP tasks. Our dataset, models, and code are publicly available on GitHub\nand Hugging Face under CC BY-NC 4.0 license.", "published": "2023-02-22 05:41:27", "link": "http://arxiv.org/abs/2302.11157v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Topic-switch adapted Japanese Dialogue System based on PLATO-2", "abstract": "Large-scale open-domain dialogue systems such as PLATO-2 have achieved\nstate-of-the-art scores in both English and Chinese. However, little work\nexplores whether such dialogue systems also work well in the Japanese language.\nIn this work, we create a large-scale Japanese dialogue dataset,\nDialogue-Graph, which contains 1.656 million dialogue data in a tree structure\nfrom News, TV subtitles, and Wikipedia corpus. Then, we train PLATO-2 using\nDialogue-Graph to build a large-scale Japanese dialogue system, PLATO-JDS. In\naddition, to improve the PLATO-JDS in the topic switch issue, we introduce a\ntopic-switch algorithm composed of a topic discriminator to switch to a new\ntopic when user input differs from the previous topic. We evaluate the user\nexperience by using our model with respect to four metrics, namely, coherence,\ninformativeness, engagingness, and humanness. As a result, our proposed\nPLATO-JDS achieves an average score of 1.500 for the human evaluation with\nhuman-bot chat strategy, which is close to the maximum score of 2.000 and\nsuggests the high-quality dialogue generation capability of PLATO-2 in\nJapanese. Furthermore, our proposed topic-switch algorithm achieves an average\nscore of 1.767 and outperforms PLATO-JDS by 0.267, indicating its effectiveness\nin improving the user experience of our system.", "published": "2023-02-22 10:57:59", "link": "http://arxiv.org/abs/2302.11280v1", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Impact of Subword Pooling Strategy on Cross-lingual Event Detection", "abstract": "Pre-trained multilingual language models (e.g., mBERT, XLM-RoBERTa) have\nsignificantly advanced the state-of-the-art for zero-shot cross-lingual\ninformation extraction. These language models ubiquitously rely on word\nsegmentation techniques that break a word into smaller constituent subwords.\nTherefore, all word labeling tasks (e.g. named entity recognition, event\ndetection, etc.), necessitate a pooling strategy that takes the subword\nrepresentations as input and outputs a representation for the entire word.\nTaking the task of cross-lingual event detection as a motivating example, we\nshow that the choice of pooling strategy can have a significant impact on the\ntarget language performance. For example, the performance varies by up to 16\nabsolute $f_{1}$ points depending on the pooling strategy when training in\nEnglish and testing in Arabic on the ACE task. We carry out our analysis with\nfive different pooling strategies across nine languages in diverse\nmulti-lingual datasets. Across configurations, we find that the canonical\nstrategy of taking just the first subword to represent the entire word is\nusually sub-optimal. On the other hand, we show that attention pooling is\nrobust to language and dataset variations by being either the best or close to\nthe optimal strategy. For reproducibility, we make our code available at\nhttps://github.com/isi-boston/ed-pooling.", "published": "2023-02-22 13:33:21", "link": "http://arxiv.org/abs/2302.11365v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Augmentation for Neural NLP", "abstract": "Data scarcity is a problem that occurs in languages and tasks where we do not\nhave large amounts of labeled data but want to use state-of-the-art models.\nSuch models are often deep learning models that require a significant amount of\ndata to train. Acquiring data for various machine learning problems is\naccompanied by high labeling costs. Data augmentation is a low-cost approach\nfor tackling data scarcity. This paper gives an overview of current\nstate-of-the-art data augmentation methods used for natural language\nprocessing, with an emphasis on methods for neural and transformer-based\nmodels. Furthermore, it discusses the practical challenges of data\naugmentation, possible mitigations, and directions for future research.", "published": "2023-02-22 14:47:15", "link": "http://arxiv.org/abs/2302.11412v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Advancements in Federated Learning: Models, Methods, and Privacy", "abstract": "Federated learning (FL) is a promising technique for addressing the rising\nprivacy and security issues. Its main ingredient is to cooperatively learn the\nmodel among the distributed clients without uploading any sensitive data. In\nthis paper, we conducted a thorough review of the related works, following the\ndevelopment context and deeply mining the key technologies behind FL from both\ntheoretical and practical perspectives. Specifically, we first classify the\nexisting works in FL architecture based on the network topology of FL systems\nwith detailed analysis and summarization. Next, we abstract the current\napplication problems, summarize the general techniques and frame the\napplication problems into the general paradigm of FL base models. Moreover, we\nprovide our proposed solutions for model training via FL. We have summarized\nand analyzed the existing FedOpt algorithms, and deeply revealed the\nalgorithmic development principles of many first-order algorithms in depth,\nproposing a more generalized algorithm design framework. Based on these\nframeworks, we have instantiated FedOpt algorithms. As privacy and security is\nthe fundamental requirement in FL, we provide the existing attack scenarios and\nthe defense methods. To the best of our knowledge, we are among the first tier\nto review the theoretical methodology and propose our strategies since there\nare very few works surveying the theoretical approaches. Our survey targets\nmotivating the development of high-performance, privacy-preserving, and secure\nmethods to integrate FL into real-world applications.", "published": "2023-02-22 16:00:27", "link": "http://arxiv.org/abs/2302.11466v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Solution for the EPO CodeFest on Green Plastics: Hierarchical\n  multi-label classification of patents relating to green plastics using deep\n  learning", "abstract": "This work aims at hierarchical multi-label patents classification for patents\ndisclosing technologies related to green plastics. This is an emerging field\nfor which there is currently no classification scheme, and hence, no labeled\ndata is available, making this task particularly challenging. We first propose\na classification scheme for this technology and a way to learn a machine\nlearning model to classify patents into the proposed classification scheme. To\nachieve this, we come up with a strategy to automatically assign labels to\npatents in order to create a labeled training dataset that can be used to learn\na classification model in a supervised learning setting. Using said training\ndataset, we come up with two classification models, a SciBERT Neural Network\n(SBNN) model and a SciBERT Hierarchical Neural Network (SBHNN) model. Both\nmodels use a BERT model as a feature extractor and on top of it, a neural\nnetwork as a classifier. We carry out extensive experiments and report commonly\nevaluation metrics for this challenging classification problem. The experiment\nresults verify the validity of our approach and show that our model sets a very\nstrong benchmark for this problem. We also interpret our models by visualizing\nthe word importance given by the trained model, which indicates the model is\ncapable to extract high-level semantic information of input documents. Finally,\nwe highlight how our solution fulfills the evaluation criteria for the EPO\nCodeFest and we also outline possible directions for future work. Our code has\nbeen made available at https://github.com/epo/CF22-Green-Hands", "published": "2023-02-22 19:06:58", "link": "http://arxiv.org/abs/2302.13784v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Role of Reviewer Expertise in Temporal Review Helpfulness\n  Prediction", "abstract": "Helpful reviews have been essential for the success of e-commerce services,\nas they help customers make quick purchase decisions and benefit the merchants\nin their sales. While many reviews are informative, others provide little value\nand may contain spam, excessive appraisal, or unexpected biases. With the large\nvolume of reviews and their uneven quality, the problem of detecting helpful\nreviews has drawn much attention lately. Existing methods for identifying\nhelpful reviews primarily focus on review text and ignore the two key factors\nof (1) who post the reviews and (2) when the reviews are posted. Moreover, the\nhelpfulness votes suffer from scarcity for less popular products and recently\nsubmitted (a.k.a., cold-start) reviews. To address these challenges, we\nintroduce a dataset and develop a model that integrates the reviewer's\nexpertise, derived from the past review history of the reviewers, and the\ntemporal dynamics of the reviews to automatically assess review helpfulness. We\nconduct experiments on our dataset to demonstrate the effectiveness of\nincorporating these factors and report improved results compared to several\nwell-established baselines.", "published": "2023-02-22 23:41:22", "link": "http://arxiv.org/abs/2303.00923v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Novel Intent Detection and Active Learning Based Classification (Student\n  Abstract)", "abstract": "Novel intent class detection is an important problem in real world scenario\nfor conversational agents for continuous interaction. Several research works\nhave been done to detect novel intents in a mono-lingual (primarily English)\ntexts and images. But, current systems lack an end-to-end universal framework\nto detect novel intents across various different languages with less human\nannotation effort for mis-classified and system rejected samples. This paper\nproposes NIDAL (Novel Intent Detection and Active Learning based\nclassification), a semi-supervised framework to detect novel intents while\nreducing human annotation cost. Empirical results on various benchmark datasets\ndemonstrate that this system outperforms the baseline methods by more than 10%\nmargin for accuracy and macro-F1. The system achieves this while maintaining\noverall annotation cost to be just ~6-10% of the unlabeled data available to\nthe system.", "published": "2023-02-22 13:04:17", "link": "http://arxiv.org/abs/2304.11058v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Preventing Catastrophic Forgetting in Continual Learning of New Natural\n  Language Tasks", "abstract": "Multi-Task Learning (MTL) is widely-accepted in Natural Language Processing\nas a standard technique for learning multiple related tasks in one model.\nTraining an MTL model requires having the training data for all tasks available\nat the same time. As systems usually evolve over time, (e.g., to support new\nfunctionalities), adding a new task to an existing MTL model usually requires\nretraining the model from scratch on all the tasks and this can be\ntime-consuming and computationally expensive. Moreover, in some scenarios, the\ndata used to train the original training may be no longer available, for\nexample, due to storage or privacy concerns. In this paper, we approach the\nproblem of incrementally expanding MTL models' capability to solve new tasks\nover time by distilling the knowledge of an already trained model on n tasks\ninto a new one for solving n+1 tasks. To avoid catastrophic forgetting, we\npropose to exploit unlabeled data from the same distributions of the old tasks.\nOur experiments on publicly available benchmarks show that such a technique\ndramatically benefits the distillation by preserving the already acquired\nknowledge (i.e., preventing up to 20% performance drops on old tasks) while\nobtaining good performance on the incrementally added tasks. Further, we also\nshow that our approach is beneficial in practical settings by using data from a\nleading voice assistant.", "published": "2023-02-22 00:18:25", "link": "http://arxiv.org/abs/2302.11074v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Test-Time Distribution Normalization for Contrastively Learned\n  Vision-language Models", "abstract": "Advances in the field of vision-language contrastive learning have made it\npossible for many downstream applications to be carried out efficiently and\naccurately by simply taking the dot product between image and text\nrepresentations. One of the most representative approaches proposed recently\nknown as CLIP has garnered widespread adoption due to its effectiveness. CLIP\nis trained with an InfoNCE loss that takes into account both positive and\nnegative samples to help learn a much more robust representation space. This\npaper reveals that the common downstream practice of taking a dot product is\nonly a zeroth-order approximation of the optimization goal, resulting in a loss\nof information during test-time. Intuitively, since the model has been\noptimized based on the InfoNCE loss, test-time procedures should also be in\nalignment. The question lies in how one can retrieve any semblance of negative\nsamples information during inference in a computationally efficient way. To\nthis end, we propose Distribution Normalization (DN), where we approximate the\nmean representation of a batch of test samples and use such a mean to represent\nwhat would be analogous to negative samples in the InfoNCE loss. DN requires no\nretraining or fine-tuning and can be effortlessly applied during inference.\nExtensive experiments on a wide variety of downstream tasks exhibit a clear\nadvantage of DN over the dot product on top of other existing test-time\naugmentation methods.", "published": "2023-02-22 01:14:30", "link": "http://arxiv.org/abs/2302.11084v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "GTRL: An Entity Group-Aware Temporal Knowledge Graph Representation\n  Learning Method", "abstract": "Temporal Knowledge Graph (TKG) representation learning embeds entities and\nevent types into a continuous low-dimensional vector space by integrating the\ntemporal information, which is essential for downstream tasks, e.g., event\nprediction and question answering. Existing methods stack multiple graph\nconvolution layers to model the influence of distant entities, leading to the\nover-smoothing problem. To alleviate the problem, recent studies infuse\nreinforcement learning to obtain paths that contribute to modeling the\ninfluence of distant entities. However, due to the limited number of hops,\nthese studies fail to capture the correlation between entities that are far\napart and even unreachable. To this end, we propose GTRL, an entity Group-aware\nTemporal knowledge graph Representation Learning method. GTRL is the first work\nthat incorporates the entity group modeling to capture the correlation between\nentities by stacking only a finite number of layers. Specifically, the entity\ngroup mapper is proposed to generate entity groups from entities in a learning\nway. Based on entity groups, the implicit correlation encoder is introduced to\ncapture implicit correlations between any pairwise entity groups. In addition,\nthe hierarchical GCNs are exploited to accomplish the message aggregation and\nrepresentation updating on the entity group graph and the entity graph.\nFinally, GRUs are employed to capture the temporal dependency in TKGs.\nExtensive experiments on three real-world datasets demonstrate that GTRL\nachieves the state-of-the-art performances on the event prediction task,\noutperforming the best baseline by an average of 13.44%, 9.65%, 12.15%, and\n15.12% in MRR, Hits@1, Hits@3, and Hits@10, respectively.", "published": "2023-02-22 01:57:42", "link": "http://arxiv.org/abs/2302.11091v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Open-domain Visual Entity Recognition: Towards Recognizing Millions of\n  Wikipedia Entities", "abstract": "Large-scale multi-modal pre-training models such as CLIP and PaLI exhibit\nstrong generalization on various visual domains and tasks. However, existing\nimage classification benchmarks often evaluate recognition on a specific domain\n(e.g., outdoor images) or a specific task (e.g., classifying plant species),\nwhich falls short of evaluating whether pre-trained foundational models are\nuniversal visual recognizers. To address this, we formally present the task of\nOpen-domain Visual Entity recognitioN (OVEN), where a model need to link an\nimage onto a Wikipedia entity with respect to a text query. We construct\nOVEN-Wiki by re-purposing 14 existing datasets with all labels grounded onto\none single label space: Wikipedia entities. OVEN challenges models to select\namong six million possible Wikipedia entities, making it a general visual\nrecognition benchmark with the largest number of labels. Our study on\nstate-of-the-art pre-trained models reveals large headroom in generalizing to\nthe massive-scale label space. We show that a PaLI-based auto-regressive visual\nrecognition model performs surprisingly well, even on Wikipedia entities that\nhave never been seen during fine-tuning. We also find existing pretrained\nmodels yield different strengths: while PaLI-based models obtain higher overall\nperformance, CLIP-based models are better at recognizing tail entities.", "published": "2023-02-22 05:31:26", "link": "http://arxiv.org/abs/2302.11154v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "UML: A Universal Monolingual Output Layer for Multilingual ASR", "abstract": "Word-piece models (WPMs) are commonly used subword units in state-of-the-art\nend-to-end automatic speech recognition (ASR) systems. For multilingual ASR,\ndue to the differences in written scripts across languages, multilingual WPMs\nbring the challenges of having overly large output layers and scaling to more\nlanguages. In this work, we propose a universal monolingual output layer (UML)\nto address such problems. Instead of one output node for only one WPM, UML\nre-associates each output node with multiple WPMs, one for each language, and\nresults in a smaller monolingual output layer shared across languages.\nConsequently, the UML enables to switch in the interpretation of each output\nnode depending on the language of the input speech. Experimental results on an\n11-language voice search task demonstrated the feasibility of using UML for\nhigh-quality and high-efficiency multilingual streaming ASR.", "published": "2023-02-22 07:40:01", "link": "http://arxiv.org/abs/2302.11186v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Contextual Spelling Correction by External Acoustics Attention\n  and Semantic Aware Data Augmentation", "abstract": "We previously proposed contextual spelling correction (CSC) to correct the\noutput of end-to-end (E2E) automatic speech recognition (ASR) models with\ncontextual information such as name, place, etc. Although CSC has achieved\nreasonable improvement in the biasing problem, there are still two drawbacks\nfor further accuracy improvement. First, due to information limitation in text\nonly hypothesis or weak performance of ASR model on rare domains, the CSC model\nmay fail to correct phrases with similar pronunciation or anti-context cases\nwhere all biasing phrases are not present in the utterance. Second, there is a\ndiscrepancy between the training and inference of CSC. The bias list in\ntraining is randomly selected but in inference there may be more similarity\nbetween ground truth phrase and other phrases. To solve above limitations, in\nthis paper we propose an improved non-autoregressive (NAR) spelling correction\nmodel for contextual biasing in E2E neural transducer-based ASR systems to\nimprove the previous CSC model from two perspectives: Firstly, we incorporate\nacoustics information with an external attention as well as text hypotheses\ninto CSC to better distinguish target phrase from dissimilar or irrelevant\nphrases. Secondly, we design a semantic aware data augmentation schema in\ntraining phrase to reduce the mismatch between training and inference to\nfurther boost the biasing accuracy. Experiments show that the improved method\noutperforms the baseline ASR+Biasing system by as much as 20.3% relative name\nrecall gain and achieves stable improvement compared to the previous CSC method\nover different bias list name coverage ratio.", "published": "2023-02-22 08:00:08", "link": "http://arxiv.org/abs/2302.11192v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MADI: Inter-domain Matching and Intra-domain Discrimination for\n  Cross-domain Speech Recognition", "abstract": "End-to-end automatic speech recognition (ASR) usually suffers from\nperformance degradation when applied to a new domain due to domain shift.\nUnsupervised domain adaptation (UDA) aims to improve the performance on the\nunlabeled target domain by transferring knowledge from the source to the target\ndomain. To improve transferability, existing UDA approaches mainly focus on\nmatching the distributions of the source and target domains globally and/or\nlocally, while ignoring the model discriminability. In this paper, we propose a\nnovel UDA approach for ASR via inter-domain MAtching and intra-domain\nDIscrimination (MADI), which improves the model transferability by fine-grained\ninter-domain matching and discriminability by intra-domain contrastive\ndiscrimination simultaneously. Evaluations on the Libri-Adapt dataset\ndemonstrate the effectiveness of our approach. MADI reduces the relative word\nerror rate (WER) on cross-device and cross-environment ASR by 17.7% and 22.8%,\nrespectively.", "published": "2023-02-22 09:11:06", "link": "http://arxiv.org/abs/2302.11224v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Learning from Multiple Sources for Data-to-Text and Text-to-Data", "abstract": "Data-to-text (D2T) and text-to-data (T2D) are dual tasks that convert\nstructured data, such as graphs or tables into fluent text, and vice versa.\nThese tasks are usually handled separately and use corpora extracted from a\nsingle source. Current systems leverage pre-trained language models fine-tuned\non D2T or T2D tasks. This approach has two main limitations: first, a separate\nsystem has to be tuned for each task and source; second, learning is limited by\nthe scarcity of available corpora. This paper considers a more general scenario\nwhere data are available from multiple heterogeneous sources. Each source, with\nits specific data format and semantic domain, provides a non-parallel corpus of\ntext and structured data. We introduce a variational auto-encoder model with\ndisentangled style and content variables that allows us to represent the\ndiversity that stems from multiple sources of text and data. Our model is\ndesigned to handle the tasks of D2T and T2D jointly. We evaluate our model on\nseveral datasets, and show that by learning from multiple sources, our model\ncloses the performance gap with its supervised single-source counterpart and\noutperforms it in some cases.", "published": "2023-02-22 10:39:33", "link": "http://arxiv.org/abs/2302.11269v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Grounding Complex Natural Language Commands for Temporal Tasks in Unseen\n  Environments", "abstract": "Grounding navigational commands to linear temporal logic (LTL) leverages its\nunambiguous semantics for reasoning about long-horizon tasks and verifying the\nsatisfaction of temporal constraints. Existing approaches require training data\nfrom the specific environment and landmarks that will be used in natural\nlanguage to understand commands in those environments. We propose Lang2LTL, a\nmodular system and a software package that leverages large language models\n(LLMs) to ground temporal navigational commands to LTL specifications in\nenvironments without prior language data. We comprehensively evaluate Lang2LTL\nfor five well-defined generalization behaviors. Lang2LTL demonstrates the\nstate-of-the-art ability of a single model to ground navigational commands to\ndiverse temporal specifications in 21 city-scaled environments. Finally, we\ndemonstrate a physical robot using Lang2LTL can follow 52 semantically diverse\nnavigational commands in two indoor environments.", "published": "2023-02-22 20:56:40", "link": "http://arxiv.org/abs/2302.11649v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.FL"], "primary_category": "cs.RO"}
{"title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution\n  Perspective", "abstract": "ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.", "published": "2023-02-22 11:01:20", "link": "http://arxiv.org/abs/2302.12095v5", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Scaling Robot Learning with Semantically Imagined Experience", "abstract": "Recent advances in robot learning have shown promise in enabling robots to\nperform a variety of manipulation tasks and generalize to novel scenarios. One\nof the key contributing factors to this progress is the scale of robot data\nused to train the models. To obtain large-scale datasets, prior approaches have\nrelied on either demonstrations requiring high human involvement or\nengineering-heavy autonomous data collection schemes, both of which are\nchallenging to scale. To mitigate this issue, we propose an alternative route\nand leverage text-to-image foundation models widely used in computer vision and\nnatural language processing to obtain meaningful data for robot learning\nwithout requiring additional robot data. We term our method Robot Learning with\nSemantically Imagened Experience (ROSIE). Specifically, we make use of the\nstate of the art text-to-image diffusion models and perform aggressive data\naugmentation on top of our existing robotic manipulation datasets via\ninpainting various unseen objects for manipulation, backgrounds, and\ndistractors with text guidance. Through extensive real-world experiments, we\nshow that manipulation policies trained on data augmented this way are able to\nsolve completely unseen tasks with new objects and can behave more robustly\nw.r.t. novel distractors. In addition, we find that we can improve the\nrobustness and generalization of high-level robot learning tasks such as\nsuccess detection through training with the diffusion-based data augmentation.\nThe project's website and videos can be found at diffusion-rosie.github.io", "published": "2023-02-22 18:47:51", "link": "http://arxiv.org/abs/2302.11550v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Contrastive Representation Learning for Acoustic Parameter Estimation", "abstract": "A study is presented in which a contrastive learning approach is used to\nextract low-dimensional representations of the acoustic environment from\nsingle-channel, reverberant speech signals. Convolution of room impulse\nresponses (RIRs) with anechoic source signals is leveraged as a data\naugmentation technique that offers considerable flexibility in the design of\nthe upstream task. We evaluate the embeddings across three different downstream\ntasks, which include the regression of acoustic parameters reverberation time\nRT60 and clarity index C50, and the classification into small and large rooms.\nWe demonstrate that the learned representations generalize well to unseen data\nand perform similarly to a fully-supervised baseline.", "published": "2023-02-22 08:37:43", "link": "http://arxiv.org/abs/2302.11205v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Unifying Speech Enhancement and Separation with Gradient Modulation for\n  End-to-End Noise-Robust Speech Separation", "abstract": "Recent studies in neural network-based monaural speech separation (SS) have\nachieved a remarkable success thanks to increasing ability of long sequence\nmodeling. However, they would degrade significantly when put under realistic\nnoisy conditions, as the background noise could be mistaken for speaker's\nspeech and thus interfere with the separated sources. To alleviate this\nproblem, we propose a novel network to unify speech enhancement and separation\nwith gradient modulation to improve noise-robustness. Specifically, we first\nbuild a unified network by combining speech enhancement (SE) and separation\nmodules, with multi-task learning for optimization, where SE is supervised by\nparallel clean mixture to reduce noise for downstream speech separation.\nFurthermore, in order to avoid suppressing valid speaker information when\nreducing noise, we propose a gradient modulation (GM) strategy to harmonize the\nSE and SS tasks from optimization view. Experimental results show that our\napproach achieves the state-of-the-art on large-scale Libri2Mix- and\nLibri3Mix-noisy datasets, with SI-SNRi results of 16.0 dB and 15.8 dB\nrespectively. Our code is available at GitHub.", "published": "2023-02-22 03:54:50", "link": "http://arxiv.org/abs/2302.11131v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Gradient Remedy for Multi-Task Learning in End-to-End Noise-Robust\n  Speech Recognition", "abstract": "Speech enhancement (SE) is proved effective in reducing noise from noisy\nspeech signals for downstream automatic speech recognition (ASR), where\nmulti-task learning strategy is employed to jointly optimize these two tasks.\nHowever, the enhanced speech learned by SE objective may not always yield good\nASR results. From the optimization view, there sometimes exists interference\nbetween the gradients of SE and ASR tasks, which could hinder the multi-task\nlearning and finally lead to sub-optimal ASR performance. In this paper, we\npropose a simple yet effective approach called gradient remedy (GR) to solve\ninterference between task gradients in noise-robust speech recognition, from\nperspectives of both angle and magnitude. Specifically, we first project the SE\ntask's gradient onto a dynamic surface that is at acute angle to ASR gradient,\nin order to remove the conflict between them and assist in ASR optimization.\nFurthermore, we adaptively rescale the magnitude of two gradients to prevent\nthe dominant ASR task from being misled by SE gradient. Experimental results\nshow that the proposed approach well resolves the gradient interference and\nachieves relative word error rate (WER) reductions of 9.3% and 11.1% over\nmulti-task learning baseline, on RATS and CHiME-4 datasets, respectively. Our\ncode is available at GitHub.", "published": "2023-02-22 13:31:13", "link": "http://arxiv.org/abs/2302.11362v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Cross-modal Audio-visual Co-learning for Text-independent Speaker\n  Verification", "abstract": "Visual speech (i.e., lip motion) is highly related to auditory speech due to\nthe co-occurrence and synchronization in speech production. This paper\ninvestigates this correlation and proposes a cross-modal speech co-learning\nparadigm. The primary motivation of our cross-modal co-learning method is\nmodeling one modality aided by exploiting knowledge from another modality.\nSpecifically, two cross-modal boosters are introduced based on an audio-visual\npseudo-siamese structure to learn the modality-transformed correlation. Inside\neach booster, a max-feature-map embedded Transformer variant is proposed for\nmodality alignment and enhanced feature generation. The network is co-learned\nboth from scratch and with pretrained models. Experimental results on the\nLRSLip3, GridLip, LomGridLip, and VoxLip datasets demonstrate that our proposed\nmethod achieves 60% and 20% average relative performance improvement over\nindependently trained audio-only/visual-only and baseline fusion systems,\nrespectively.", "published": "2023-02-22 10:06:37", "link": "http://arxiv.org/abs/2302.11254v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
