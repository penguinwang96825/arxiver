{"title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic", "abstract": "Pre-trained language models (LMs) are currently integral to many natural\nlanguage processing systems. Although multilingual LMs were also introduced to\nserve many languages, these have limitations such as being costly at inference\ntime and the size and diversity of non-English data involved in their\npre-training. We remedy these issues for a collection of diverse Arabic\nvarieties by introducing two powerful deep bidirectional transformer-based\nmodels, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a\nnew benchmark for multi-dialectal Arabic language understanding evaluation.\nARLUE is built using 42 datasets targeting six different task clusters,\nallowing us to offer a series of standardized experiments under rich\nconditions. When fine-tuned on ARLUE, our models collectively achieve new\nstate-of-the-art results across the majority of tasks (37 out of 48\nclassification tasks, on the 42 datasets). Our best model acquires the highest\nARLUE score (77.40) across all six task clusters, outperforming all other\nmodels including XLM-R Large (~ 3.4 x larger size). Our models are publicly\navailable at https://github.com/UBC-NLP/marbert and ARLUE will be released\nthrough the same repository.", "published": "2020-12-27 06:32:55", "link": "http://arxiv.org/abs/2101.01785v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Light-Weight Translation Models from Deep Transformer", "abstract": "Recently, deep models have shown tremendous improvements in neural machine\ntranslation (NMT). However, systems of this kind are computationally expensive\nand memory intensive. In this paper, we take a natural step towards learning\nstrong but light-weight NMT systems. We proposed a novel group-permutation\nbased knowledge distillation approach to compressing the deep Transformer model\ninto a shallow model. The experimental results on several benchmarks validate\nthe effectiveness of our method. Our compressed model is 8X shallower than the\ndeep model, with almost no loss in BLEU. To further enhance the teacher model,\nwe present a Skipping Sub-Layer method to randomly omit sub-layers to introduce\nperturbation into training, which achieves a BLEU score of 30.63 on\nEnglish-German newstest2014. The code is publicly available at\nhttps://github.com/libeineu/GPKD.", "published": "2020-12-27 05:33:21", "link": "http://arxiv.org/abs/2012.13866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Embarrassingly Simple Model for Dialogue Relation Extraction", "abstract": "Dialogue relation extraction (RE) is to predict the relation type of two\nentities mentioned in a dialogue. In this paper, we propose a simple yet\neffective model named SimpleRE for the RE task. SimpleRE captures the\ninterrelations among multiple relations in a dialogue through a novel input\nformat named BERT Relation Token Sequence (BRS). In BRS, multiple [CLS] tokens\nare used to capture possible relations between different pairs of entities\nmentioned in the dialogue. A Relation Refinement Gate (RRG) is then designed to\nextract relation-specific semantic representation in an adaptive manner.\nExperiments on the DialogRE dataset show that SimpleRE achieves the best\nperformance, with much shorter training time. Further, SimpleRE outperforms all\ndirect baselines on sentence-level RE without using external resources.", "published": "2020-12-27 06:22:23", "link": "http://arxiv.org/abs/2012.13873v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Convolution for Semantic Role Labeling", "abstract": "Semantic role labeling (SRL) aims at elaborating the meaning of a sentence by\nforming a predicate-argument structure. Recent researches depicted that the\neffective use of syntax can improve SRL performance. However, syntax is a\ncomplicated linguistic clue and is hard to be effectively applied in a\ndownstream task like SRL. This work effectively encodes syntax using adaptive\nconvolution which endows strong flexibility to existing convolutional networks.\nThe existing CNNs may help in encoding a complicated structure like syntax for\nSRL, but it still has shortcomings. Contrary to traditional convolutional\nnetworks that use same filters for different inputs, adaptive convolution uses\nadaptively generated filters conditioned on syntactically informed inputs. We\nachieve this with the integration of a filter generation network which\ngenerates the input specific filters. This helps the model to focus on\nimportant syntactic features present inside the input, thus enlarging the gap\nbetween syntax-aware and syntax-agnostic SRL systems. We further study a\nhashing technique to compress the size of the filter generation network for SRL\nin terms of trainable parameters. Experiments on CoNLL-2009 dataset confirm\nthat the proposed model substantially outperforms most previous SRL systems for\nboth English and Chinese languages", "published": "2020-12-27 13:26:11", "link": "http://arxiv.org/abs/2012.13939v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation", "abstract": "Knowledge distillation is considered as a training and compression strategy\nin which two neural networks, namely a teacher and a student, are coupled\ntogether during training. The teacher network is supposed to be a trustworthy\npredictor and the student tries to mimic its predictions. Usually, a student\nwith a lighter architecture is selected so we can achieve compression and yet\ndeliver high-quality results. In such a setting, distillation only happens for\nfinal predictions whereas the student could also benefit from teacher's\nsupervision for internal components.\n  Motivated by this, we studied the problem of distillation for intermediate\nlayers. Since there might not be a one-to-one alignment between student and\nteacher layers, existing techniques skip some teacher layers and only distill\nfrom a subset of them. This shortcoming directly impacts quality, so we instead\npropose a combinatorial technique which relies on attention. Our model fuses\nteacher-side information and takes each layer's significance into\nconsideration, then performs distillation between combined teacher layers and\nthose of the student. Using our technique, we distilled a 12-layer BERT (Devlin\net al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE\ntasks (Wang et al. 2018). Experimental results show that our combinatorial\napproach is able to outperform other existing techniques.", "published": "2020-12-27 22:30:13", "link": "http://arxiv.org/abs/2012.14022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inserting Information Bottlenecks for Attribution in Transformers", "abstract": "Pretrained transformers achieve the state of the art across tasks in natural\nlanguage processing, motivating researchers to investigate their inner\nmechanisms. One common direction is to understand what features are important\nfor prediction. In this paper, we apply information bottlenecks to analyze the\nattribution of each feature for prediction on a black-box model. We use BERT as\nthe example and evaluate our approach both quantitatively and qualitatively. We\nshow the effectiveness of our method in terms of attribution and the ability to\nprovide insight into how information flows through layers. We demonstrate that\nour technique outperforms two competitive methods in degradation tests on four\ndatasets. Code is available at https://github.com/bazingagin/IBA.", "published": "2020-12-27 00:35:43", "link": "http://arxiv.org/abs/2012.13838v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Explaining NLP Models via Minimal Contrastive Editing (MiCE)", "abstract": "Humans have been shown to give contrastive explanations, which explain why an\nobserved event happened rather than some other counterfactual event (the\ncontrast case). Despite the influential role that contrastivity plays in how\nhumans explain, this property is largely missing from current methods for\nexplaining NLP models. We present Minimal Contrastive Editing (MiCE), a method\nfor producing contrastive explanations of model predictions in the form of\nedits to inputs that change model outputs to the contrast case. Our experiments\nacross three tasks--binary sentiment classification, topic classification, and\nmultiple-choice question answering--show that MiCE is able to produce edits\nthat are not only contrastive, but also minimal and fluent, consistent with\nhuman contrastive edits. We demonstrate how MiCE edits can be used for two use\ncases in NLP system development--debugging incorrect model outputs and\nuncovering dataset artifacts--and thereby illustrate that producing contrastive\nexplanations is a promising research direction for model interpretability.", "published": "2020-12-27 18:06:26", "link": "http://arxiv.org/abs/2012.13985v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SMART: A Situation Model for Algebra Story Problems via Attributed\n  Grammar", "abstract": "Solving algebra story problems remains a challenging task in artificial\nintelligence, which requires a detailed understanding of real-world situations\nand a strong mathematical reasoning capability. Previous neural solvers of math\nword problems directly translate problem texts into equations, lacking an\nexplicit interpretation of the situations, and often fail to handle more\nsophisticated situations. To address such limits of neural solvers, we\nintroduce the concept of a \\emph{situation model}, which originates from\npsychology studies to represent the mental states of humans in problem-solving,\nand propose \\emph{SMART}, which adopts attributed grammar as the representation\nof situation models for algebra story problems. Specifically, we first train an\ninformation extraction module to extract nodes, attributes, and relations from\nproblem texts and then generate a parse graph based on a pre-defined attributed\ngrammar. An iterative learning strategy is also proposed to improve the\nperformance of SMART further. To rigorously study this task, we carefully\ncurate a new dataset named \\emph{ASP6.6k}. Experimental results on ASP6.6k show\nthat the proposed model outperforms all previous neural solvers by a large\nmargin while preserving much better interpretability. To test these models'\ngeneralization capability, we also design an out-of-distribution (OOD)\nevaluation, in which problems are more complex than those in the training set.\nOur model exceeds state-of-the-art models by 17\\% in the OOD evaluation,\ndemonstrating its superior generalization ability.", "published": "2020-12-27 21:03:40", "link": "http://arxiv.org/abs/2012.14011v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "My Teacher Thinks The World Is Flat! Interpreting Automatic Essay\n  Scoring Mechanism", "abstract": "Significant progress has been made in deep-learning based Automatic Essay\nScoring (AES) systems in the past two decades. However, little research has\nbeen put to understand and interpret the black-box nature of these\ndeep-learning based scoring models. Recent work shows that automated scoring\nsystems are prone to even common-sense adversarial samples. Their lack of\nnatural language understanding capability raises questions on the models being\nactively used by millions of candidates for life-changing decisions. With\nscoring being a highly multi-modal task, it becomes imperative for scoring\nmodels to be validated and tested on all these modalities. We utilize recent\nadvances in interpretability to find the extent to which features such as\ncoherence, content and relevance are important for automated scoring mechanisms\nand why they are susceptible to adversarial samples. We find that the systems\ntested consider essays not as a piece of prose having the characteristics of\nnatural flow of speech and grammatical structure, but as `word-soups' where a\nfew words are much more important than the other words. Removing the context\nsurrounding those few important words causes the prose to lose the flow of\nspeech and grammar, however has little impact on the predicted score. We also\nfind that since the models are not semantically grounded with world-knowledge\nand common sense, adding false facts such as ``the world is flat'' actually\nincreases the score instead of decreasing it.", "published": "2020-12-27 06:19:20", "link": "http://arxiv.org/abs/2012.13872v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Improving Opinion Spam Detection by Cumulative Relative Frequency\n  Distribution", "abstract": "Over the last years, online reviews became very important since they can\ninfluence the purchase decision of consumers and the reputation of businesses,\ntherefore, the practice of writing fake reviews can have severe consequences on\ncustomers and service providers. Various approaches have been proposed for\ndetecting opinion spam in online reviews, especially based on supervised\nclassifiers. In this contribution, we start from a set of effective features\nused for classifying opinion spam and we re-engineered them, by considering the\nCumulative Relative Frequency Distribution of each feature. By an experimental\nevaluation carried out on real data from Yelp.com, we show that the use of the\ndistributional features is able to improve the performances of classifiers.", "published": "2020-12-27 10:23:44", "link": "http://arxiv.org/abs/2012.13905v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "SG-Net: Syntax Guided Transformer for Language Representation", "abstract": "Understanding human language is one of the key themes of artificial\nintelligence. For language representation, the capacity of effectively modeling\nthe linguistic knowledge from the detail-riddled and lengthy texts and getting\nrid of the noises is essential to improve its performance. Traditional\nattentive models attend to all words without explicit constraint, which results\nin inaccurate concentration on some dispensable words. In this work, we propose\nusing syntax to guide the text modeling by incorporating explicit syntactic\nconstraints into attention mechanisms for better linguistically motivated word\nrepresentations. In detail, for self-attention network (SAN) sponsored\nTransformer-based encoder, we introduce syntactic dependency of interest (SDOI)\ndesign into the SAN to form an SDOI-SAN with syntax-guided self-attention.\nSyntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the\nSAN from the original Transformer encoder through a dual contextual\narchitecture for better linguistics inspired representation. The proposed\nSG-Net is applied to typical Transformer encoders. Extensive experiments on\npopular benchmark tasks, including machine reading comprehension, natural\nlanguage inference, and neural machine translation show the effectiveness of\nthe proposed SG-Net design.", "published": "2020-12-27 11:09:35", "link": "http://arxiv.org/abs/2012.13915v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language\n  Understanding Pretraining", "abstract": "One of the biggest challenges that prohibit the use of many current NLP\nmethods in clinical settings is the availability of public datasets. In this\nwork, we present MeDAL, a large medical text dataset curated for abbreviation\ndisambiguation, designed for natural language understanding pre-training in the\nmedical domain. We pre-trained several models of common architectures on this\ndataset and empirically showed that such pre-training leads to improved\nperformance and convergence speed when fine-tuning on downstream medical tasks.", "published": "2020-12-27 17:17:39", "link": "http://arxiv.org/abs/2012.13978v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring University Impact: Wikipedia approach", "abstract": "The impact of Universities on the social, economic and political landscape is\none of the key directions in contemporary educational evaluation. In this\npaper, we discuss the new methodological technique that evaluates the impact of\nuniversity based on popularity (number of page-views) of their alumni's pages\non Wikipedia. It allows revealing the alumni popularity dynamics and tracking\nits state. Preliminary analysis shows that the number of page-views is higher\nfor the contemporary persons that prove the perspectives of this approach.\nThen, universities were ranked based on the methodology and compared to the\nfamous international university rankings ARWU and QS based only on alumni\nscales: for the top 10 universities, there is an intersection of two\nuniversities (Columbia University, Stanford University). The correlation\ncoefficients between different university rankings are provided in the paper.\nFinally, the ranking based on the alumni popularity was compared with the\nranking of universities based on the popularity of their webpages on Wikipedia:\nthere is a strong connection between these indicators.", "published": "2020-12-27 17:41:56", "link": "http://arxiv.org/abs/2012.13980v1", "categories": ["cs.SI", "cs.CL", "cs.IR"], "primary_category": "cs.SI"}
{"title": "Neural document expansion for ad-hoc information retrieval", "abstract": "Recently, Nogueira et al. [2019] proposed a new approach to document\nexpansion based on a neural Seq2Seq model, showing significant improvement on\nshort text retrieval task. However, this approach needs a large amount of\nin-domain training data. In this paper, we show that this neural document\nexpansion approach can be effectively adapted to standard IR tasks, where\nlabels are scarce and many long documents are present.", "published": "2020-12-27 20:00:08", "link": "http://arxiv.org/abs/2012.14005v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
