{"title": "Abstractive Multi-Document Summarization via Phrase Selection and\n  Merging", "abstract": "We propose an abstraction-based multi-document summarization framework that\ncan construct new sentences by exploring more fine-grained syntactic units than\nsentences, namely, noun/verb phrases. Different from existing abstraction-based\napproaches, our method first constructs a pool of concepts and facts\nrepresented by phrases from the input documents. Then new sentences are\ngenerated by selecting and merging informative phrases to maximize the salience\nof phrases and meanwhile satisfy the sentence construction constraints. We\nemploy integer linear optimization for conducting phrase selection and merging\nsimultaneously in order to achieve the global optimal solution for a summary.\nExperimental results on the benchmark data set TAC 2011 show that our framework\noutperforms the state-of-the-art models under automated pyramid evaluation\nmetric, and achieves reasonably well results on manual linguistic quality\nevaluation.", "published": "2015-06-04 14:04:10", "link": "http://arxiv.org/abs/1506.01597v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Long-Short Story of Movie Description", "abstract": "Generating descriptions for videos has many applications including assisting\nblind people and human-robot interaction. The recent advances in image\ncaptioning as well as the release of large-scale movie description datasets\nsuch as MPII Movie Description allow to study this task in more depth. Many of\nthe proposed methods for image captioning rely on pre-trained object classifier\nCNNs and Long-Short Term Memory recurrent networks (LSTMs) for generating\ndescriptions. While image description focuses on objects, we argue that it is\nimportant to distinguish verbs, objects, and places in the challenging setting\nof movie description. In this work we show how to learn robust visual\nclassifiers from the weak annotations of the sentence descriptions. Based on\nthese visual classifiers we learn how to generate a description using an LSTM.\nWe explore different design choices to build and train the LSTM and achieve the\nbest performance to date on the challenging MPII-MD dataset. We compare and\nanalyze our approach and prior work along various dimensions to better\nunderstand the key challenges of the movie description task.", "published": "2015-06-04 19:45:36", "link": "http://arxiv.org/abs/1506.01698v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
