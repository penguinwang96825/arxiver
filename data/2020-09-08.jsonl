{"title": "kk2018 at SemEval-2020 Task 9: Adversarial Training for Code-Mixing\n  Sentiment Classification", "abstract": "Code switching is a linguistic phenomenon that may occur within a\nmultilingual setting where speakers share more than one language. With the\nincreasing communication between groups with different languages, this\nphenomenon is more and more popular. However, there are little research and\ndata in this area, especially in code-mixing sentiment classification. In this\nwork, the domain transfer learning from state-of-the-art uni-language model\nERNIE is tested on the code-mixing dataset, and surprisingly, a strong baseline\nis achieved. Furthermore, the adversarial training with a multi-lingual model\nis used to achieve 1st place of SemEval-2020 Task 9 Hindi-English sentiment\nclassification competition.", "published": "2020-09-08 12:20:04", "link": "http://arxiv.org/abs/2009.03673v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simple is Better! Lightweight Data Augmentation for Low Resource Slot\n  Filling and Intent Classification", "abstract": "Neural-based models have achieved outstanding performance on slot filling and\nintent classification, when fairly large in-domain training data are available.\nHowever, as new domains are frequently added, creating sizeable data is\nexpensive. We show that lightweight augmentation, a set of augmentation methods\ninvolving word span and sentence level operations, alleviates data scarcity\nproblems. Our experiments on limited data settings show that lightweight\naugmentation yields significant performance improvement on slot filling on the\nATIS and SNIPS datasets, and achieves competitive performance with respect to\nmore complex, state-of-the-art, augmentation approaches. Furthermore,\nlightweight augmentation is also beneficial when combined with pre-trained\nLM-based models, as it improves BERT-based joint intent and slot filling\nmodels.", "published": "2020-09-08 12:39:47", "link": "http://arxiv.org/abs/2009.03695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ERNIE at SemEval-2020 Task 10: Learning Word Emphasis Selection by\n  Pre-trained Language Model", "abstract": "This paper describes the system designed by ERNIE Team which achieved the\nfirst place in SemEval-2020 Task 10: Emphasis Selection For Written Text in\nVisual Media. Given a sentence, we are asked to find out the most important\nwords as the suggestion for automated design. We leverage the unsupervised\npre-training model and finetune these models on our task. After our\ninvestigation, we found that the following models achieved an excellent\nperformance in this task: ERNIE 2.0, XLM-ROBERTA, ROBERTA and ALBERT. We\ncombine a pointwise regression loss and a pairwise ranking loss which is more\nclose to the final M atchm metric to finetune our models. And we also find that\nadditional feature engineering and data augmentation can help improve the\nperformance. Our best model achieves the highest score of 0.823 and ranks first\nfor all kinds of metrics", "published": "2020-09-08 12:51:22", "link": "http://arxiv.org/abs/2009.03706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying the Causal Effects of Conversational Tendencies", "abstract": "Understanding what leads to effective conversations can aid the design of\nbetter computer-mediated communication platforms. In particular, prior\nobservational work has sought to identify behaviors of individuals that\ncorrelate to their conversational efficiency. However, translating such\ncorrelations to causal interpretations is a necessary step in using them in a\nprescriptive fashion to guide better designs and policies.\n  In this work, we formally describe the problem of drawing causal links\nbetween conversational behaviors and outcomes. We focus on the task of\ndetermining a particular type of policy for a text-based crisis counseling\nplatform: how best to allocate counselors based on their behavioral tendencies\nexhibited in their past conversations. We apply arguments derived from causal\ninference to underline key challenges that arise in conversational settings\nwhere randomized trials are hard to implement. Finally, we show how to\ncircumvent these inference challenges in our particular domain, and illustrate\nthe potential benefits of an allocation policy informed by the resulting\nprescriptive information.", "published": "2020-09-08 18:00:00", "link": "http://arxiv.org/abs/2009.03897v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting LSTM Networks for Semi-Supervised Text Classification via\n  Mixed Objective Function", "abstract": "In this paper, we study bidirectional LSTM network for the task of text\nclassification using both supervised and semi-supervised approaches. Several\nprior works have suggested that either complex pretraining schemes using\nunsupervised methods such as language modeling (Dai and Le 2015; Miyato, Dai,\nand Goodfellow 2016) or complicated models (Johnson and Zhang 2017) are\nnecessary to achieve a high classification accuracy. However, we develop a\ntraining strategy that allows even a simple BiLSTM model, when trained with\ncross-entropy loss, to achieve competitive results compared with more complex\napproaches. Furthermore, in addition to cross-entropy loss, by using a\ncombination of entropy minimization, adversarial, and virtual adversarial\nlosses for both labeled and unlabeled data, we report state-of-the-art results\nfor text classification task on several benchmark datasets. In particular, on\nthe ACL-IMDB sentiment analysis and AG-News topic classification datasets, our\nmethod outperforms current approaches by a substantial margin. We also show the\ngenerality of the mixed objective function by improving the performance on\nrelation extraction task.", "published": "2020-09-08 21:55:22", "link": "http://arxiv.org/abs/2009.04007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LynyrdSkynyrd at WNUT-2020 Task 2: Semi-Supervised Learning for\n  Identification of Informative COVID-19 English Tweets", "abstract": "We describe our system for WNUT-2020 shared task on the identification of\ninformative COVID-19 English tweets. Our system is an ensemble of various\nmachine learning methods, leveraging both traditional feature-based classifiers\nas well as recent advances in pre-trained language models that help in\ncapturing the syntactic, semantic, and contextual features from the tweets. We\nfurther employ pseudo-labelling to incorporate the unlabelled Twitter data\nreleased on the pandemic. Our best performing model achieves an F1-score of\n0.9179 on the provided validation set and 0.8805 on the blind test-set.", "published": "2020-09-08 16:29:25", "link": "http://arxiv.org/abs/2009.03849v1", "categories": ["cs.CL", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Covid-Transformer: Detecting COVID-19 Trending Topics on Twitter Using\n  Universal Sentence Encoder", "abstract": "The novel corona-virus disease (also known as COVID-19) has led to a\npandemic, impacting more than 200 countries across the globe. With its global\nimpact, COVID-19 has become a major concern of people almost everywhere, and\ntherefore there are a large number of tweets coming out from every corner of\nthe world, about COVID-19 related topics. In this work, we try to analyze the\ntweets and detect the trending topics and major concerns of people on Twitter,\nwhich can enable us to better understand the situation, and devise better\nplanning. More specifically we propose a model based on the universal sentence\nencoder to detect the main topics of Tweets in recent months. We used universal\nsentence encoder in order to derive the semantic representation and the\nsimilarity of tweets. We then used the sentence similarity and their\nembeddings, and feed them to K-means clustering algorithm to group similar\ntweets (in semantic sense). After that, the cluster summary is obtained using a\ntext summarization algorithm based on deep learning, which can uncover the\nunderlying topics of each cluster. Through experimental results, we show that\nour model can detect very informative topics, by processing a large number of\ntweets on sentence level (which can preserve the overall meaning of the\ntweets). Since this framework has no restriction on specific data distribution,\nit can be used to detect trending topics from any other social media and any\nother context rather than COVID-19. Experimental results show superiority of\nour proposed approach to other baselines, including TF-IDF, and latent\nDirichlet allocation (LDA).", "published": "2020-09-08 19:00:38", "link": "http://arxiv.org/abs/2009.03947v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probabilistic Predictions of People Perusing: Evaluating Metrics of\n  Language Model Performance for Psycholinguistic Modeling", "abstract": "By positing a relationship between naturalistic reading times and\ninformation-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008)\nprovides a natural interface between language models and psycholinguistic\nmodels. This paper re-evaluates a claim due to Goodkind and Bicknell (2018)\nthat a language model's ability to model reading times is a linear function of\nits perplexity. By extending Goodkind and Bicknell's analysis to modern neural\narchitectures, we show that the proposed relation does not always hold for Long\nShort-Term Memory networks, Transformers, and pre-trained models. We introduce\nan alternate measure of language modeling performance called predictability\nnorm correlation based on Cloze probabilities measured from human subjects. Our\nnew metric yields a more robust relationship between language model quality and\npsycholinguistic modeling performance that allows for comparison between models\nwith different training configurations.", "published": "2020-09-08 19:12:06", "link": "http://arxiv.org/abs/2009.03954v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Quantifying the Effects of COVID-19 on Mental Health Support Forums", "abstract": "The COVID-19 pandemic, like many of the disease outbreaks that have preceded\nit, is likely to have a profound effect on mental health. Understanding its\nimpact can inform strategies for mitigating negative consequences. In this\nwork, we seek to better understand the effects of COVID-19 on mental health by\nexamining discussions within mental health support communities on Reddit.\nFirst, we quantify the rate at which COVID-19 is discussed in each community,\nor subreddit, in order to understand levels of preoccupation with the pandemic.\nNext, we examine the volume of activity in order to determine whether the\nquantity of people seeking online mental health support has risen. Finally, we\nanalyze how COVID-19 has influenced language use and topics of discussion\nwithin each subreddit.", "published": "2020-09-08 21:59:08", "link": "http://arxiv.org/abs/2009.04008v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "QED: A Framework and Dataset for Explanations in Question Answering", "abstract": "A question answering system that in addition to providing an answer provides\nan explanation of the reasoning that leads to that answer has potential\nadvantages in terms of debuggability, extensibility and trust. To this end, we\npropose QED, a linguistically informed, extensible framework for explanations\nin question answering. A QED explanation specifies the relationship between a\nquestion and answer according to formal semantic notions such as referential\nequality, sentencehood, and entailment. We describe and publicly release an\nexpert-annotated dataset of QED explanations built upon a subset of the Google\nNatural Questions dataset, and report baseline models on two tasks -- post-hoc\nexplanation generation given an answer, and joint question answering and\nexplanation generation. In the joint setting, a promising result suggests that\ntraining on a relatively small amount of QED data can improve question\nanswering. In addition to describing the formal, language-theoretic motivations\nfor the QED approach, we describe a large user study showing that the presence\nof QED explanations significantly improves the ability of untrained raters to\nspot errors made by a strong neural QA baseline.", "published": "2020-09-08 23:34:18", "link": "http://arxiv.org/abs/2009.06354v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leam: An Interactive System for In-situ Visual Text Analysis", "abstract": "With the increase in scale and availability of digital text generated on the\nweb, enterprises such as online retailers and aggregators often use text\nanalytics to mine and analyze the data to improve their services and products\nalike. Text data analysis is an iterative, non-linear process with diverse\nworkflows spanning multiple stages, from data cleaning to visualization.\nExisting text analytics systems usually accommodate a subset of these stages\nand often fail to address challenges related to data heterogeneity, provenance,\nworkflow reusability and reproducibility, and compatibility with established\npractices. Based on a set of design considerations we derive from these\nchallenges, we propose Leam, a system that treats the text analysis process as\na single continuum by combining advantages of computational notebooks,\nspreadsheets, and visualization tools. Leam features an interactive user\ninterface for running text analysis workflows, a new data model for managing\nmultiple atomic and composite data types, and an expressive algebra that\ncaptures diverse sets of operations representing various stages of text\nanalysis and enables coordination among different components of the system,\nincluding data, code, and visualizations. We report our current progress in\nLeam development while demonstrating its usefulness with usage examples.\nFinally, we outline a number of enhancements to Leam and identify several\nresearch directions for developing an interactive visual text analysis system.", "published": "2020-09-08 05:18:29", "link": "http://arxiv.org/abs/2009.03520v1", "categories": ["cs.DB", "cs.CL", "cs.HC"], "primary_category": "cs.DB"}
{"title": "Brown University at TREC Deep Learning 2019", "abstract": "This paper describes Brown University's submission to the TREC 2019 Deep\nLearning track. We followed a 2-phase method for producing a ranking of\npassages for a given input query: In the the first phase, the user's query is\nexpanded by appending 3 queries generated by a transformer model which was\ntrained to rephrase an input query into semantically similar queries. The\nexpanded query can exhibit greater similarity in surface form and vocabulary\noverlap with the passages of interest and can therefore serve as enriched input\nto any downstream information retrieval method. In the second phase, we use a\nBERT-based model pre-trained for language modeling but fine-tuned for query -\ndocument relevance prediction to compute relevance scores for a set of 1000\ncandidate passages per query and subsequently obtain a ranking of passages by\nsorting them based on the predicted relevance scores. According to the results\npublished in the official Overview of the TREC Deep Learning Track 2019, our\nteam ranked 3rd in the passage retrieval task (including full ranking and\nre-ranking), and 2nd when considering only re-ranking submissions.", "published": "2020-09-08 22:54:03", "link": "http://arxiv.org/abs/2009.04016v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Predictions of Subjective Ratings and Spoofing Assessments of Voice\n  Conversion Challenge 2020 Submissions", "abstract": "The Voice Conversion Challenge 2020 is the third edition under its flagship\nthat promotes intra-lingual semiparallel and cross-lingual voice conversion\n(VC). While the primary evaluation of the challenge submissions was done\nthrough crowd-sourced listening tests, we also performed an objective\nassessment of the submitted systems. The aim of the objective assessment is to\nprovide complementary performance analysis that may be more beneficial than the\ntime-consuming listening tests. In this study, we examined five types of\nobjective assessments using automatic speaker verification (ASV), neural\nspeaker embeddings, spoofing countermeasures, predicted mean opinion scores\n(MOS), and automatic speech recognition (ASR). Each of these objective measures\nassesses the VC output along different aspects. We observed that the\ncorrelations of these objective assessments with the subjective results were\nhigh for ASV, neural speaker embedding, and ASR, which makes them more\ninfluential for predicting subjective test results. In addition, we performed\nspoofing assessments on the submitted systems and identified some of the VC\nmethods showing a potentially high security risk.", "published": "2020-09-08 07:17:58", "link": "http://arxiv.org/abs/2009.03554v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "AutoKWS: Keyword Spotting with Differentiable Architecture Search", "abstract": "Smart audio devices are gated by an always-on lightweight keyword spotting\nprogram to reduce power consumption. It is however challenging to design models\nthat have both high accuracy and low latency for accurate and fast\nresponsiveness. Many efforts have been made to develop end-to-end neural\nnetworks, in which depthwise separable convolutions, temporal convolutions, and\nLSTMs are adopted as building units. Nonetheless, these networks designed with\nhuman expertise may not achieve an optimal trade-off in an expansive search\nspace. In this paper, we propose to leverage recent advances in differentiable\nneural architecture search to discover more efficient networks. Our searched\nmodel attains 97.2% top-1 accuracy on Google Speech Command Dataset v1 with\nonly nearly 100K parameters.", "published": "2020-09-08 12:01:55", "link": "http://arxiv.org/abs/2009.03658v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
