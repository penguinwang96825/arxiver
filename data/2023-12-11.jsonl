{"title": "SECNN: Squeeze-and-Excitation Convolutional Neural Network for Sentence\n  Classification", "abstract": "Sentence classification is one of the basic tasks of natural language\nprocessing. Convolution neural network (CNN) has the ability to extract n-grams\nfeatures through convolutional filters and capture local correlations between\nconsecutive words in parallel, so CNN is a popular neural network architecture\nto dealing with the task. But restricted by the width of convolutional filters,\nit is difficult for CNN to capture long term contextual dependencies. Attention\nis a mechanism that considers global information and pays more attention to\nkeywords in sentences, thus attention mechanism is cooperated with CNN network\nto improve performance in sentence classification task. In our work, we don't\nfocus on keyword in a sentence, but on which CNN's output feature map is more\nimportant. We propose a Squeeze-and-Excitation Convolutional neural Network\n(SECNN) for sentence classification. SECNN takes the feature maps from multiple\nCNN as different channels of sentence representation, and then, we can utilize\nchannel attention mechanism, that is SE attention mechanism, to enable the\nmodel to learn the attention weights of different channel features. The results\nshow that our model achieves advanced performance in the sentence\nclassification task.", "published": "2023-12-11 03:26:36", "link": "http://arxiv.org/abs/2312.06088v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generative Large Language Models Are All-purpose Text Analytics Engines:\n  Text-to-text Learning Is All Your Need", "abstract": "Objective To solve major clinical natural language processing (NLP) tasks\nusing a unified text-to-text learning architecture based on a generative large\nlanguage model (LLM) via prompt tuning. Methods We formulated 7 key clinical\nNLP tasks as text-to-text learning and solved them using one unified generative\nclinical LLM, GatorTronGPT, developed using GPT-3 architecture and trained with\nup to 20 billion parameters. We adopted soft prompts (i.e., trainable vectors)\nwith frozen LLM, where the LLM parameters were not updated (i.e., frozen) and\nonly the vectors of soft prompts were updated, known as prompt tuning. We added\nadditional soft prompts as a prefix to the input layer, which were optimized\nduring the prompt tuning. We evaluated the proposed method using 7 clinical NLP\ntasks and compared them with previous task-specific solutions based on\nTransformer models. Results and Conclusion The proposed approach achieved\nstate-of-the-art performance for 5 out of 7 major clinical NLP tasks using one\nunified generative LLM. Our approach outperformed previous task-specific\ntransformer models by ~3% for concept extraction and 7% for relation extraction\napplied to social determinants of health, 3.4% for clinical concept\nnormalization, 3.4~10% for clinical abbreviation disambiguation, and 5.5~9% for\nnatural language inference. Our approach also outperformed a previously\ndeveloped prompt-based machine reading comprehension (MRC) model,\nGatorTron-MRC, for clinical concept and relation extraction. The proposed\napproach can deliver the ``one model for all`` promise from training to\ndeployment using a unified generative LLM.", "published": "2023-12-11 04:00:26", "link": "http://arxiv.org/abs/2312.06099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decoupling SQL Query Hardness Parsing for Text-to-SQL", "abstract": "The fundamental goal of the Text-to-SQL task is to translate natural language\nquestion into SQL query. Current research primarily emphasizes the information\ncoupling between natural language questions and schemas, and significant\nprogress has been made in this area. The natural language questions as the\nprimary task requirements source determines the hardness of correspond SQL\nqueries, the correlation between the two always be ignored. However, when the\ncorrelation between questions and queries was decoupled, it may simplify the\ntask. In this paper, we introduce an innovative framework for Text-to-SQL based\non decoupling SQL query hardness parsing. This framework decouples the\nText-to-SQL task based on query hardness by analyzing questions and schemas,\nsimplifying the multi-hardness task into a single-hardness challenge. This\ngreatly reduces the parsing pressure on the language model. We evaluate our\nproposed framework and achieve a new state-of-the-art performance of\nfine-turning methods on Spider dev.", "published": "2023-12-11 07:20:46", "link": "http://arxiv.org/abs/2312.06172v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to\n  Fine-grained", "abstract": "For the task of fine-grained entity typing (FET), due to the use of a large\nnumber of entity types, it is usually considered too costly to manually\nannotating a training dataset that contains an ample number of examples for\neach type. A common way to address this problem is to use distantly annotated\ntraining data that contains incorrect labels. However, the performance of\nmodels trained solely with such data can be limited by the errors in the\nautomatic annotation. Recently, there are a few approaches that no longer\nfollow this conventional way. But without using sufficient direct entity typing\nsupervision may also cause them to yield inferior performance. In this paper,\nwe propose a new approach that can avoid the need of creating distantly labeled\ndata whenever there is a new type schema. We first train an entity typing model\nthat have an extremely board type coverage by using the ultra-fine entity\ntyping data. Then, when there is a need to produce a model for a newly designed\nfine-grained entity type schema. We can simply fine-tune the previously trained\nmodel with a small number of examples annotated under this schema. Experimental\nresults show that our approach achieves outstanding performance for FET under\nthe few-shot setting. It can also outperform state-of-the-art weak supervision\nbased methods after fine-tuning the model with only a small size manually\nannotated training set.", "published": "2023-12-11 08:12:01", "link": "http://arxiv.org/abs/2312.06188v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UstanceBR: a social media language resource for stance prediction", "abstract": "This work introduces UstanceBR, a multimodal corpus in the Brazilian\nPortuguese Twitter domain for target-based stance prediction. The corpus\ncomprises 86.8 k labelled stances towards selected target topics, and extensive\nnetwork information about the users who published these stances on social\nmedia. In this article we describe the corpus multimodal data, and a number of\nusage examples in both in-domain and zero-shot stance prediction based on text-\nand network-related information, which are intended to provide initial baseline\nresults for future studies in the field.", "published": "2023-12-11 13:32:11", "link": "http://arxiv.org/abs/2312.06374v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Get an A in Math: Progressive Rectification Prompting", "abstract": "Chain-of-Thought (CoT) prompting methods have enabled large language models\n(LLMs) to generate reasoning paths and solve math word problems (MWPs).\nHowever, they are sensitive to mistakes in the paths, as any mistake can result\nin an incorrect answer. We propose a novel method named Progressive\nRectification Prompting (PRP) to improve average accuracy on eight MWP datasets\nfrom 77.3 to 90.5. Given an initial answer from CoT, PRP iterates a\nverify-then-rectify process to progressively identify incorrect answers and\nrectify the reasoning paths. With the most likely correct answer, the LLM\npredicts a masked numerical value in the question; if the prediction does not\nmatch the masked value, the answer is likely incorrect. Then the LLM is\nprompted to re-generate the reasoning path hinted with a set of incorrect\nanswers to prevent itself from repeating previous mistakes. PRP achieves the\nbest performance compared against the CoT methods. Our implementation is made\npublicly available at https://wzy6642.github.io/prp.github.io/.", "published": "2023-12-11 22:25:57", "link": "http://arxiv.org/abs/2312.06867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IEKG: A Commonsense Knowledge Graph for Idiomatic Expressions", "abstract": "Idiomatic expression (IE) processing and comprehension have challenged\npre-trained language models (PTLMs) because their meanings are\nnon-compositional. Unlike prior works that enable IE comprehension through\nfine-tuning PTLMs with sentences containing IEs, in this work, we construct\nIEKG, a commonsense knowledge graph for figurative interpretations of IEs. This\nextends the established ATOMIC2020 graph, converting PTLMs into knowledge\nmodels (KMs) that encode and infer commonsense knowledge related to IE use.\nExperiments show that various PTLMs can be converted into KMs with IEKG. We\nverify the quality of IEKG and the ability of the trained KMs with automatic\nand human evaluation. Through applications in natural language understanding,\nwe show that a PTLM injected with knowledge from IEKG exhibits improved IE\ncomprehension ability and can generalize to IEs unseen during training.", "published": "2023-12-11 00:57:11", "link": "http://arxiv.org/abs/2312.06053v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GTA: Gated Toxicity Avoidance for LM Performance Preservation", "abstract": "Caution: This paper includes offensive words that could potentially cause\nunpleasantness. The fast-paced evolution of generative language models such as\nGPT-4 has demonstrated outstanding results in various NLP generation tasks.\nHowever, due to the potential generation of offensive words related to race or\ngender, various Controllable Text Generation (CTG) methods have been proposed\nto mitigate the occurrence of harmful words. However, existing CTG methods not\nonly reduce toxicity but also negatively impact several aspects of the language\nmodel's generation performance, including topic consistency, grammar, and\nperplexity. This paper explores the limitations of previous methods and\nintroduces a novel solution in the form of a simple Gated Toxicity Avoidance\n(GTA) that can be applied to any CTG method. We also evaluate the effectiveness\nof the proposed GTA by comparing it with state-of-the-art CTG methods across\nvarious datasets. Our findings reveal that gated toxicity avoidance efficiently\nachieves comparable levels of toxicity reduction to the original CTG methods\nwhile preserving the generation performance of the language model.", "published": "2023-12-11 05:04:17", "link": "http://arxiv.org/abs/2312.06122v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Order Matters in the Presence of Dataset Imbalance for Multilingual\n  Learning", "abstract": "In this paper, we empirically study the optimization dynamics of multi-task\nlearning, particularly focusing on those that govern a collection of tasks with\nsignificant data imbalance. We present a simple yet effective method of\npre-training on high-resource tasks, followed by fine-tuning on a mixture of\nhigh/low-resource tasks. We provide a thorough empirical study and analysis of\nthis method's benefits showing that it achieves consistent improvements\nrelative to the performance trade-off profile of standard static weighting. We\nanalyze under what data regimes this method is applicable and show its\nimprovements empirically in neural machine translation (NMT) and multi-lingual\nlanguage modeling.", "published": "2023-12-11 05:46:57", "link": "http://arxiv.org/abs/2312.06134v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"What's important here?\": Opportunities and Challenges of Using LLMs in\n  Retrieving Information from Web Interfaces", "abstract": "Large language models (LLMs) that have been trained on a corpus that includes\nlarge amount of code exhibit a remarkable ability to understand HTML code. As\nweb interfaces are primarily constructed using HTML, we design an in-depth\nstudy to see how LLMs can be used to retrieve and locate important elements for\na user given query (i.e. task description) in a web interface. In contrast with\nprior works, which primarily focused on autonomous web navigation, we decompose\nthe problem as an even atomic operation - Can LLMs identify the important\ninformation in the web page for a user given query? This decomposition enables\nus to scrutinize the current capabilities of LLMs and uncover the opportunities\nand challenges they present. Our empirical experiments show that while LLMs\nexhibit a reasonable level of performance in retrieving important UI elements,\nthere is still a substantial room for improvement. We hope our investigation\nwill inspire follow-up works in overcoming the current challenges in this\ndomain.", "published": "2023-12-11 06:26:38", "link": "http://arxiv.org/abs/2312.06147v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large\n  Language Models Decoding", "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text\ngeneration. However, achieving optimal results with a given prompt or\ninstruction can be challenging, especially for billion-sized models.\nAdditionally, undesired behaviors such as toxicity or hallucinations can\nmanifest. While much larger models (e.g., ChatGPT) may demonstrate strength in\nmitigating these issues, there is still no guarantee of complete prevention. In\nthis work, we propose formalizing text generation as a future-constrained\ngeneration problem to minimize undesirable behaviors and enforce faithfulness\nto instructions. The estimation of future constraint satisfaction, accomplished\nusing LLMs, guides the text generation process. Our extensive experiments\ndemonstrate the effectiveness of the proposed approach across three distinct\ntext generation tasks: keyword-constrained generation (Lin et al., 2020),\ntoxicity reduction (Gehman et al., 2020), and factual correctness in\nquestion-answering (Gao et al., 2023).", "published": "2023-12-11 06:35:33", "link": "http://arxiv.org/abs/2312.06149v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KnowGPT: Knowledge Graph based Prompting for Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nmany real-world applications. Nonetheless, LLMs are often criticized for their\ntendency to produce hallucinations, wherein the models fabricate incorrect\nstatements on tasks beyond their knowledge and perception. To alleviate this\nissue, researchers have explored leveraging the factual knowledge in knowledge\ngraphs (KGs) to ground the LLM's responses in established facts and principles.\nHowever, most state-of-the-art LLMs are closed-source, making it challenging to\ndevelop a prompting framework that can efficiently and effectively integrate\nKGs into LLMs with hard prompts only. Generally, existing KG-enhanced LLMs\nusually suffer from three critical issues, including huge search space, high\nAPI costs, and laborious prompt engineering, that impede their widespread\napplication in practice. To this end, we introduce a novel Knowledge Graph\nbased PrompTing framework, namely KnowGPT, to enhance LLMs with domain\nknowledge. KnowGPT contains a knowledge extraction module to extract the most\ninformative knowledge from KGs, and a context-aware prompt construction module\nto automatically convert extracted knowledge into effective prompts.\nExperiments on three benchmarks demonstrate that KnowGPT significantly\noutperforms all competitors. Notably, KnowGPT achieves a 92.6% accuracy on\nOpenbookQA leaderboard, comparable to human-level performance.", "published": "2023-12-11 07:56:25", "link": "http://arxiv.org/abs/2312.06185v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Medical Vision Language Pretraining: A survey", "abstract": "Medical Vision Language Pretraining (VLP) has recently emerged as a promising\nsolution to the scarcity of labeled data in the medical domain. By leveraging\npaired/unpaired vision and text datasets through self-supervised learning,\nmodels can be trained to acquire vast knowledge and learn robust feature\nrepresentations. Such pretrained models have the potential to enhance multiple\ndownstream medical tasks simultaneously, reducing the dependency on labeled\ndata. However, despite recent progress and its potential, there is no such\ncomprehensive survey paper that has explored the various aspects and\nadvancements in medical VLP. In this paper, we specifically review existing\nworks through the lens of different pretraining objectives, architectures,\ndownstream evaluation tasks, and datasets utilized for pretraining and\ndownstream tasks. Subsequently, we delve into current challenges in medical\nVLP, discussing existing and potential solutions, and conclude by highlighting\nfuture directions. To the best of our knowledge, this is the first survey\nfocused on medical VLP.", "published": "2023-12-11 09:14:13", "link": "http://arxiv.org/abs/2312.06224v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models", "abstract": "We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of\nemotional intelligence in Large Language Models (LLMs). We assess the ability\nof LLMs to understand complex emotions and social interactions by asking them\nto predict the intensity of emotional states of characters in a dialogue. The\nbenchmark is able to discriminate effectively between a wide range of models.\nWe find that EQ-Bench correlates strongly with comprehensive multi-domain\nbenchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may\nbe capturing similar aspects of broad intelligence. Our benchmark produces\nhighly repeatable results using a set of 60 English-language questions. We also\nprovide open-source code for an automated benchmarking pipeline at\nhttps://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com", "published": "2023-12-11 10:35:32", "link": "http://arxiv.org/abs/2312.06281v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "BoschAI @ Causal News Corpus 2023: Robust Cause-Effect Span Extraction\n  using Multi-Layer Sequence Tagging and Data Augmentation", "abstract": "Understanding causality is a core aspect of intelligence. The Event Causality\nIdentification with Causal News Corpus Shared Task addresses two aspects of\nthis challenge: Subtask 1 aims at detecting causal relationships in texts, and\nSubtask 2 requires identifying signal words and the spans that refer to the\ncause or effect, respectively. Our system, which is based on pre-trained\ntransformers, stacked sequence tagging, and synthetic data augmentation, ranks\nthird in Subtask 1 and wins Subtask 2 with an F1 score of 72.8, corresponding\nto a margin of 13 pp. to the second-best system.", "published": "2023-12-11 12:35:35", "link": "http://arxiv.org/abs/2312.06338v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous\n  Driving Datasets using Markup Annotations", "abstract": "Visual Question Answering (VQA) is one of the most important tasks in\nautonomous driving, which requires accurate recognition and complex situation\nevaluations. However, datasets annotated in a QA format, which guarantees\nprecise language generation and scene recognition from driving scenes, have not\nbeen established yet. In this work, we introduce Markup-QA, a novel dataset\nannotation technique in which QAs are enclosed within markups. This approach\nfacilitates the simultaneous evaluation of a model's capabilities in sentence\ngeneration and VQA. Moreover, using this annotation methodology, we designed\nthe NuScenes-MQA dataset. This dataset empowers the development of vision\nlanguage models, especially for autonomous driving tasks, by focusing on both\ndescriptive capabilities and precise QA. The dataset is available at\nhttps://github.com/turingmotors/NuScenes-MQA.", "published": "2023-12-11 12:58:54", "link": "http://arxiv.org/abs/2312.06352v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "TaCo: Targeted Concept Erasure Prevents Non-Linear Classifiers From\n  Detecting Protected Attributes", "abstract": "Ensuring fairness in NLP models is crucial, as they often encode sensitive\nattributes like gender and ethnicity, leading to biased outcomes. Current\nconcept erasure methods attempt to mitigate this by modifying final latent\nrepresentations to remove sensitive information without retraining the entire\nmodel. However, these methods typically rely on linear classifiers, which leave\nmodels vulnerable to non-linear adversaries capable of recovering sensitive\ninformation.\n  We introduce Targeted Concept Erasure (TaCo), a novel approach that removes\nsensitive information from final latent representations, ensuring fairness even\nagainst non-linear classifiers. Our experiments show that TaCo outperforms\nstate-of-the-art methods, achieving greater reductions in the prediction\naccuracy of sensitive attributes by non-linear classifier while preserving\noverall task performance. Code is available on\nhttps://github.com/fanny-jourdan/TaCo.", "published": "2023-12-11 16:22:37", "link": "http://arxiv.org/abs/2312.06499v4", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Where exactly does contextualization in a PLM happen?", "abstract": "Pre-trained Language Models (PLMs) have shown to be consistently successful\nin a plethora of NLP tasks due to their ability to learn contextualized\nrepresentations of words (Ethayarajh, 2019). BERT (Devlin et al., 2018), ELMo\n(Peters et al., 2018) and other PLMs encode word meaning via textual context,\nas opposed to static word embeddings, which encode all meanings of a word in a\nsingle vector representation. In this work, we present a study that aims to\nlocalize where exactly in a PLM word contextualization happens. In order to\nfind the location of this word meaning transformation, we investigate\nrepresentations of polysemous words in the basic BERT uncased 12 layer\narchitecture (Devlin et al., 2018), a masked language model trained on an\nadditional sentence adjacency objective, using qualitative and quantitative\nmeasures.", "published": "2023-12-11 16:39:52", "link": "http://arxiv.org/abs/2312.06514v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Gated Linear Attention Transformers with Hardware-Efficient Training", "abstract": "Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.", "published": "2023-12-11 18:51:59", "link": "http://arxiv.org/abs/2312.06635v6", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Building Domain-Specific LLMs Faithful To The Islamic Worldview: Mirage\n  or Technical Possibility?", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across\nnumerous natural language understanding use cases. However, this impressive\nperformance comes with inherent limitations, such as the tendency to perpetuate\nstereotypical biases or fabricate non-existent facts. In the context of Islam\nand its representation, accurate and factual representation of its beliefs and\nteachings rooted in the Quran and Sunnah is key. This work focuses on the\nchallenge of building domain-specific LLMs faithful to the Islamic worldview\nand proposes ways to build and evaluate such systems. Firstly, we define this\nopen-ended goal as a technical problem and propose various solutions.\nSubsequently, we critically examine known challenges inherent to each approach\nand highlight evaluation methodologies that can be used to assess such systems.\nThis work highlights the need for high-quality datasets, evaluations, and\ninterdisciplinary work blending machine learning with Islamic scholarship.", "published": "2023-12-11 18:59:09", "link": "http://arxiv.org/abs/2312.06652v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Utilization of Non-verbal Behaviour and Social Gaze in Classroom\n  Human-Robot Interaction Communications", "abstract": "This abstract explores classroom Human-Robot Interaction (HRI) scenarios with\nan emphasis on the adaptation of human-inspired social gaze models in robot\ncognitive architecture to facilitate a more seamless social interaction. First,\nwe detail the HRI scenarios explored by us in our studies followed by a\ndescription of the social gaze model utilized for our research. We highlight\nthe advantages of utilizing such an attentional model in classroom HRI\nscenarios. We also detail the intended goals of our upcoming study involving\nthis social gaze model.", "published": "2023-12-11 20:34:16", "link": "http://arxiv.org/abs/2312.06825v2", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Multimodal Pretraining of Medical Time Series and Notes", "abstract": "Within the intensive care unit (ICU), a wealth of patient data, including\nclinical measurements and clinical notes, is readily available. This data is a\nvaluable resource for comprehending patient health and informing medical\ndecisions, but it also contains many challenges in analysis. Deep learning\nmodels show promise in extracting meaningful patterns, but they require\nextensive labeled data, a challenge in critical care. To address this, we\npropose a novel approach employing self-supervised pretraining, focusing on the\nalignment of clinical measurements and notes. Our approach combines contrastive\nand masked token prediction tasks during pretraining. Semi-supervised\nexperiments on the MIMIC-III dataset demonstrate the effectiveness of our\nself-supervised pretraining. In downstream tasks, including in-hospital\nmortality prediction and phenotyping, our pretrained model outperforms\nbaselines in settings where only a fraction of the data is labeled, emphasizing\nits ability to enhance ICU data analysis. Notably, our method excels in\nsituations where very few labels are available, as evidenced by an increase in\nthe AUC-ROC for in-hospital mortality by 0.17 and in AUC-PR for phenotyping by\n0.1 when only 1% of labels are accessible. This work advances self-supervised\nlearning in the healthcare domain, optimizing clinical insights from abundant\nyet challenging ICU data.", "published": "2023-12-11 21:53:40", "link": "http://arxiv.org/abs/2312.06855v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Disentangling Perceptions of Offensiveness: Cultural and Moral\n  Correlates", "abstract": "Perception of offensiveness is inherently subjective, shaped by the lived\nexperiences and socio-cultural values of the perceivers. Recent years have seen\nsubstantial efforts to build AI-based tools that can detect offensive language\nat scale, as a means to moderate social media platforms, and to ensure safety\nof conversational AI technologies such as ChatGPT and Bard. However, existing\napproaches treat this task as a technical endeavor, built on top of data\nannotated for offensiveness by a global crowd workforce without any attention\nto the crowd workers' provenance or the values their perceptions reflect. We\nargue that cultural and psychological factors play a vital role in the\ncognitive processing of offensiveness, which is critical to consider in this\ncontext. We re-frame the task of determining offensiveness as essentially a\nmatter of moral judgment -- deciding the boundaries of ethically wrong vs.\nright language within an implied set of socio-cultural norms. Through a\nlarge-scale cross-cultural study based on 4309 participants from 21 countries\nacross 8 cultural regions, we demonstrate substantial cross-cultural\ndifferences in perceptions of offensiveness. More importantly, we find that\nindividual moral values play a crucial role in shaping these variations: moral\nconcerns about Care and Purity are significant mediating factors driving\ncross-cultural differences. These insights are of crucial importance as we\nbuild AI models for the pluralistic world, where the values they espouse should\naim to respect and account for moral values in diverse geo-cultural contexts.", "published": "2023-12-11 22:12:20", "link": "http://arxiv.org/abs/2312.06861v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Sparse Transformer with Local and Seasonal Adaptation for Multivariate\n  Time Series Forecasting", "abstract": "Transformers have achieved remarkable performance in multivariate time\nseries(MTS) forecasting due to their capability to capture long-term\ndependencies. However, the canonical attention mechanism has two key\nlimitations: (1) its quadratic time complexity limits the sequence length, and\n(2) it generates future values from the entire historical sequence. To address\nthis, we propose a Dozer Attention mechanism consisting of three sparse\ncomponents: (1) Local, each query exclusively attends to keys within a\nlocalized window of neighboring time steps. (2) Stride, enables each query to\nattend to keys at predefined intervals. (3) Vary, allows queries to selectively\nattend to keys from a subset of the historical sequence. Notably, the size of\nthis subset dynamically expands as forecasting horizons extend. Those three\ncomponents are designed to capture essential attributes of MTS data, including\nlocality, seasonality, and global temporal dependencies. Additionally, we\npresent the Dozerformer Framework, incorporating the Dozer Attention mechanism\nfor the MTS forecasting task. We evaluated the proposed Dozerformer framework\nwith recent state-of-the-art methods on nine benchmark datasets and confirmed\nits superior performance. The experimental results indicate that excluding a\nsubset of historical time steps from the time series forecasting process does\nnot compromise accuracy while significantly improving efficiency. Code is\navailable at https://github.com/GRYGY1215/Dozerformer.", "published": "2023-12-11 22:49:02", "link": "http://arxiv.org/abs/2312.06874v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DYAD: A Descriptive Yet Abjuring Density efficient approximation to\n  linear neural network layers", "abstract": "We devise, implement and performance-asses DYAD, a layer which can serve as a\nfaster and more memory-efficient approximate replacement for linear layers,\n(nn.Linear() in Pytorch). These layers appear in common subcomponents, such as\nin the ff module of Transformers. DYAD is based on a bespoke near-sparse matrix\nstructure which approximates the dense \"weight\" matrix W that matrix-multiplies\nthe input in the typical realization of such a layer, a.k.a DENSE. Our\nalternative near-sparse matrix structure is decomposable to a sum of 2 matrices\npermutable to a block-sparse counterpart. These can be represented as 3D\ntensors, which in unison allow a faster execution of matrix multiplication with\nthe mini-batched input matrix X compared to DENSE (O(rows(W ) x cols(W )) -->\nO( rows(W ) x cols(W ) # of blocks )). As the crux of our experiments, we\npretrain both DYAD and DENSE variants of 2 sizes of the OPT arch and 1 size of\nthe Pythia arch, including at different token scales of the babyLM benchmark.\nWe find DYAD to be competitive (>= 90%) of DENSE performance on zero-shot (e.g.\nBLIMP), few-shot (OPENLM) and finetuning (GLUE) benchmarks, while being >=7-15%\nfaster to train on-GPU even at 125m scale, besides surfacing larger speedups at\nincreasing scale and model width.", "published": "2023-12-11 23:04:48", "link": "http://arxiv.org/abs/2312.06881v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge\n  Graph Completion", "abstract": "Knowledge graphs generally suffer from incompleteness, which can be\nalleviated by completing the missing information. Deep knowledge convolutional\nembedding models based on neural networks are currently popular methods for\nknowledge graph completion. However, most existing methods use external\nconvolution kernels and traditional plain convolution processes, which limits\nthe feature interaction capability of the model. In this paper, we propose a\nnovel dynamic convolutional embedding model ConvD for knowledge graph\ncompletion, which directly reshapes the relation embeddings into multiple\ninternal convolution kernels to improve the external convolution kernels of the\ntraditional convolutional embedding model. The internal convolution kernels can\neffectively augment the feature interaction between the relation embeddings and\nentity embeddings, thus enhancing the model embedding performance. Moreover, we\ndesign a priori knowledge-optimized attention mechanism, which can assign\ndifferent contribution weight coefficients to multiple relation convolution\nkernels for dynamic convolution to improve the expressiveness of the model\nfurther. Extensive experiments on various datasets show that our proposed model\nconsistently outperforms the state-of-the-art baseline methods, with average\nimprovements ranging from 11.30\\% to 16.92\\% across all model evaluation\nmetrics. Ablation experiments verify the effectiveness of each component module\nof the ConvD model.", "published": "2023-12-11 07:37:58", "link": "http://arxiv.org/abs/2312.07589v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating ChatGPT as a Question Answering System: A Comprehensive\n  Analysis and Comparison with Existing Models", "abstract": "In the current era, a multitude of language models has emerged to cater to\nuser inquiries. Notably, the GPT-3.5 Turbo language model has gained\nsubstantial attention as the underlying technology for ChatGPT. Leveraging\nextensive parameters, this model adeptly responds to a wide range of questions.\nHowever, due to its reliance on internal knowledge, the accuracy of responses\nmay not be absolute. This article scrutinizes ChatGPT as a Question Answering\nSystem (QAS), comparing its performance to other existing QASs. The primary\nfocus is on evaluating ChatGPT's proficiency in extracting responses from\nprovided paragraphs, a core QAS capability. Additionally, performance\ncomparisons are made in scenarios without a surrounding passage. Multiple\nexperiments, exploring response hallucination and considering question\ncomplexity, were conducted on ChatGPT. Evaluation employed well-known Question\nAnswering (QA) datasets, including SQuAD, NewsQA, and PersianQuAD, across\nEnglish and Persian languages. Metrics such as F-score, exact match, and\naccuracy were employed in the assessment. The study reveals that, while ChatGPT\ndemonstrates competence as a generative model, it is less effective in question\nanswering compared to task-specific models. Providing context improves its\nperformance, and prompt engineering enhances precision, particularly for\nquestions lacking explicit answers in provided paragraphs. ChatGPT excels at\nsimpler factual questions compared to \"how\" and \"why\" question types. The\nevaluation highlights occurrences of hallucinations, where ChatGPT provides\nresponses to questions without available answers in the provided context.", "published": "2023-12-11 08:49:18", "link": "http://arxiv.org/abs/2312.07592v1", "categories": ["cs.CL", "cs.AI", "I.2, I.7"], "primary_category": "cs.CL"}
{"title": "Contrastive News and Social Media Linking using BERT for Articles and\n  Tweets across Dual Platforms", "abstract": "X (formerly Twitter) has evolved into a contemporary agora, offering a\nplatform for individuals to express opinions and viewpoints on current events.\nThe majority of the topics discussed on Twitter are directly related to ongoing\nevents, making it an important source for monitoring public discourse. However,\nlinking tweets to specific news presents a significant challenge due to their\nconcise and informal nature. Previous approaches, including topic models,\ngraph-based models, and supervised classifiers, have fallen short in\neffectively capturing the unique characteristics of tweets and articles.\n  Inspired by the success of the CLIP model in computer vision, which employs\ncontrastive learning to model similarities between images and captions, this\npaper introduces a contrastive learning approach for training a representation\nspace where linked articles and tweets exhibit proximity. We present our\ncontrastive learning approach, CATBERT (Contrastive Articles Tweets BERT),\nleveraging pre-trained BERT models. The model is trained and tested on a\ndataset containing manually labeled English and Polish tweets and articles\nrelated to the Russian-Ukrainian war. We evaluate CATBERT's performance against\ntraditional approaches like LDA, and the novel method based on OpenAI\nembeddings, which has not been previously applied to this task. Our findings\nindicate that CATBERT demonstrates superior performance in associating tweets\nwith relevant news articles. Furthermore, we demonstrate the performance of the\nmodels when applied to finding the main topic -- represented by an article --\nof the whole cascade of tweets. In this new task, we report the performance of\nthe different models in dependence on the cascade size.", "published": "2023-12-11 13:38:16", "link": "http://arxiv.org/abs/2312.07599v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "User Modeling in the Era of Large Language Models: Current Research and\n  Future Directions", "abstract": "User modeling (UM) aims to discover patterns or learn representations from\nuser data about the characteristics of a specific user, such as profile,\npreference, and personality. The user models enable personalization and\nsuspiciousness detection in many online applications such as recommendation,\neducation, and healthcare. Two common types of user data are text and graph, as\nthe data usually contain a large amount of user-generated content (UGC) and\nonline interactions. The research of text and graph mining is developing\nrapidly, contributing many notable solutions in the past two decades. Recently,\nlarge language models (LLMs) have shown superior performance on generating,\nunderstanding, and even reasoning over text data. The approaches of user\nmodeling have been equipped with LLMs and soon become outstanding. This article\nsummarizes existing research about how and why LLMs are great tools of modeling\nand understanding UGC. Then it reviews a few categories of large language\nmodels for user modeling (LLM-UM) approaches that integrate the LLMs with text\nand graph-based methods in different ways. Then it introduces specific LLM-UM\ntechniques for a variety of UM applications. Finally, it presents remaining\nchallenges and future directions in the LLM-UM research. We maintain the\nreading list at: https://github.com/TamSiuhin/LLM-UM-Reading", "published": "2023-12-11 03:59:36", "link": "http://arxiv.org/abs/2312.11518v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "METAL: Metamorphic Testing Framework for Analyzing Large-Language Model\n  Qualities", "abstract": "Large-Language Models (LLMs) have shifted the paradigm of natural language\ndata processing. However, their black-boxed and probabilistic characteristics\ncan lead to potential risks in the quality of outputs in diverse LLM\napplications. Recent studies have tested Quality Attributes (QAs), such as\nrobustness or fairness, of LLMs by generating adversarial input texts. However,\nexisting studies have limited their coverage of QAs and tasks in LLMs and are\ndifficult to extend. Additionally, these studies have only used one evaluation\nmetric, Attack Success Rate (ASR), to assess the effectiveness of their\napproaches. We propose a MEtamorphic Testing for Analyzing LLMs (METAL)\nframework to address these issues by applying Metamorphic Testing (MT)\ntechniques. This approach facilitates the systematic testing of LLM qualities\nby defining Metamorphic Relations (MRs), which serve as modularized evaluation\nmetrics. The METAL framework can automatically generate hundreds of MRs from\ntemplates that cover various QAs and tasks. In addition, we introduced novel\nmetrics that integrate the ASR method into the semantic qualities of text to\nassess the effectiveness of MRs accurately. Through the experiments conducted\nwith three prominent LLMs, we have confirmed that the METAL framework\neffectively evaluates essential QAs on primary LLM tasks and reveals the\nquality risks in LLMs. Moreover, the newly proposed metrics can guide the\noptimal MRs for testing each task and suggest the most effective method for\ngenerating MRs.", "published": "2023-12-11 01:29:19", "link": "http://arxiv.org/abs/2312.06056v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "PromptMTopic: Unsupervised Multimodal Topic Modeling of Memes using\n  Large Language Models", "abstract": "The proliferation of social media has given rise to a new form of\ncommunication: memes. Memes are multimodal and often contain a combination of\ntext and visual elements that convey meaning, humor, and cultural significance.\nWhile meme analysis has been an active area of research, little work has been\ndone on unsupervised multimodal topic modeling of memes, which is important for\ncontent moderation, social media analysis, and cultural studies. We propose\n\\textsf{PromptMTopic}, a novel multimodal prompt-based model designed to learn\ntopics from both text and visual modalities by leveraging the language modeling\ncapabilities of large language models. Our model effectively extracts and\nclusters topics learned from memes, considering the semantic interaction\nbetween the text and visual modalities. We evaluate our proposed model through\nextensive experiments on three real-world meme datasets, which demonstrate its\nsuperiority over state-of-the-art topic modeling baselines in learning\ndescriptive topics in memes. Additionally, our qualitative analysis shows that\n\\textsf{PromptMTopic} can identify meaningful and culturally relevant topics\nfrom memes. Our work contributes to the understanding of the topics and themes\nof memes, a crucial form of communication in today's society.\\\\\n\\red{\\textbf{Disclaimer: This paper contains sensitive content that may be\ndisturbing to some readers.}}", "published": "2023-12-11 03:36:50", "link": "http://arxiv.org/abs/2312.06093v1", "categories": ["cs.CL", "cs.CV", "cs.MM", "I.1.4; I.1.7"], "primary_category": "cs.CL"}
{"title": "MATK: The Meme Analytical Tool Kit", "abstract": "The rise of social media platforms has brought about a new digital culture\ncalled memes. Memes, which combine visuals and text, can strongly influence\npublic opinions on social and cultural issues. As a result, people have become\ninterested in categorizing memes, leading to the development of various\ndatasets and multimodal models that show promising results in this field.\nHowever, there is currently a lack of a single library that allows for the\nreproduction, evaluation, and comparison of these models using fair benchmarks\nand settings. To fill this gap, we introduce the Meme Analytical Tool Kit\n(MATK), an open-source toolkit specifically designed to support existing memes\ndatasets and cutting-edge multimodal models. MATK aims to assist researchers\nand engineers in training and reproducing these multimodal models for meme\nclassification tasks, while also providing analysis techniques to gain insights\ninto their strengths and weaknesses. To access MATK, please visit\n\\url{https://github.com/Social-AI-Studio/MATK}.", "published": "2023-12-11 03:36:59", "link": "http://arxiv.org/abs/2312.06094v1", "categories": ["cs.CL", "cs.CV", "cs.MM", "I.1.4"], "primary_category": "cs.CL"}
{"title": "Improving Startup Success with Text Analysis", "abstract": "Investors are interested in predicting future success of startup companies,\npreferably using publicly available data which can be gathered using free\nonline sources. Using public-only data has been shown to work, but there is\nstill much room for improvement. Two of the best performing prediction\nexperiments use 17 and 49 features respectively, mostly numeric and categorical\nin nature. In this paper, we significantly expand and diversify both the\nsources and the number of features (to 171) to achieve better prediction. Data\ncollected from Crunchbase, the Google Search API, and Twitter (now X) are used\nto predict whether a company will raise a round of funding within a fixed time\nhorizon. Much of the new features are textual and the Twitter subset include\nlinguistic metrics such as measures of passive voice and parts-of-speech. A\ntotal of ten machine learning models are also evaluated for best performance.\nThe adaptable model can be used to predict funding 1-5 years into the future,\nwith a variable cutoff threshold to favor either precision or recall.\nPrediction with comparable assumptions generally achieves F scores above 0.730\nwhich outperforms previous attempts in the literature (0.531), and does so with\nfewer examples. Furthermore, we find that the vast majority of the performance\nimpact comes from the top 18 of 171 features which are mostly generic company\nobservations, including the best performing individual feature which is the\nfree-form text description of the company.", "published": "2023-12-11 09:22:18", "link": "http://arxiv.org/abs/2312.06236v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Creating Spoken Dialog Systems in Ultra-Low Resourced Settings", "abstract": "Automatic Speech Recognition (ASR) systems are a crucial technology that is\nused today to design a wide variety of applications, most notably, smart\nassistants, such as Alexa. ASR systems are essentially dialogue systems that\nemploy Spoken Language Understanding (SLU) to extract meaningful information\nfrom speech. The main challenge with designing such systems is that they\nrequire a huge amount of labeled clean data to perform competitively, such data\nis extremely hard to collect and annotate to respective SLU tasks, furthermore,\nwhen designing such systems for low resource languages, where data is extremely\nlimited, the severity of the problem intensifies. In this paper, we focus on a\nfairly popular SLU task, that is, Intent Classification while working with a\nlow resource language, namely, Flemish. Intent Classification is a task\nconcerned with understanding the intents of the user interacting with the\nsystem. We build on existing light models for intent classification in Flemish,\nand our main contribution is applying different augmentation techniques on two\nlevels -- the voice level, and the phonetic transcripts level -- to the\nexisting models to counter the problem of scarce labeled data in low-resource\nlanguages. We find that our data augmentation techniques, on both levels, have\nimproved the model performance on a number of tasks.", "published": "2023-12-11 10:04:05", "link": "http://arxiv.org/abs/2312.06266v1", "categories": ["cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language\n  Models", "abstract": "Warning: This paper contains content that may be offensive or upsetting.\nThere has been a significant increase in the usage of large language models\n(LLMs) in various applications, both in their original form and through\nfine-tuned adaptations. As a result, LLMs have gained popularity and are being\nwidely adopted by a large user community. However, one of the concerns with\nLLMs is the potential generation of socially biased content. The existing\nevaluation methods have many constraints, and their results exhibit a limited\ndegree of interpretability. In this work, we propose a bias evaluation\nframework named GPTBIAS that leverages the high performance of LLMs (e.g.,\nGPT-4 \\cite{openai2023gpt4}) to assess bias in models. We also introduce\nprompts called Bias Attack Instructions, which are specifically designed for\nevaluating model bias. To enhance the credibility and interpretability of bias\nevaluation, our framework not only provides a bias score but also offers\ndetailed information, including bias types, affected demographics, keywords,\nreasons behind the biases, and suggestions for improvement. We conduct\nextensive experiments to demonstrate the effectiveness and usability of our\nbias evaluation framework.", "published": "2023-12-11 12:02:14", "link": "http://arxiv.org/abs/2312.06315v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Imbalanced Learning for Multimodal Emotion Recognition in\n  Conversations", "abstract": "The main task of Multimodal Emotion Recognition in Conversations (MERC) is to\nidentify the emotions in modalities, e.g., text, audio, image and video, which\nis a significant development direction for realizing machine intelligence.\nHowever, many data in MERC naturally exhibit an imbalanced distribution of\nemotion categories, and researchers ignore the negative impact of imbalanced\ndata on emotion recognition. To tackle this problem, we systematically analyze\nit from three aspects: data augmentation, loss sensitivity, and sampling\nstrategy, and propose the Class Boundary Enhanced Representation Learning\n(CBERL) model. Concretely, we first design a multimodal generative adversarial\nnetwork to address the imbalanced distribution of {emotion} categories in raw\ndata. Secondly, a deep joint variational autoencoder is proposed to fuse\ncomplementary semantic information across modalities and obtain discriminative\nfeature representations. Finally, we implement a multi-task graph neural\nnetwork with mask reconstruction and classification optimization to solve the\nproblem of overfitting and underfitting in class boundary learning, and achieve\ncross-modal emotion recognition. We have conducted extensive experiments on the\nIEMOCAP and MELD benchmark datasets, and the results show that CBERL has\nachieved a certain performance improvement in the effectiveness of emotion\nrecognition. Especially on the minority class fear and disgust emotion labels,\nour model improves the accuracy and F1 value by 10% to 20%.", "published": "2023-12-11 12:35:17", "link": "http://arxiv.org/abs/2312.06337v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluation of Large Language Models for Decision Making in Autonomous\n  Driving", "abstract": "Various methods have been proposed for utilizing Large Language Models (LLMs)\nin autonomous driving. One strategy of using LLMs for autonomous driving\ninvolves inputting surrounding objects as text prompts to the LLMs, along with\ntheir coordinate and velocity information, and then outputting the subsequent\nmovements of the vehicle. When using LLMs for such purposes, capabilities such\nas spatial recognition and planning are essential. In particular, two\nfoundational capabilities are required: (1) spatial-aware decision making,\nwhich is the ability to recognize space from coordinate information and make\ndecisions to avoid collisions, and (2) the ability to adhere to traffic rules.\nHowever, quantitative research has not been conducted on how accurately\ndifferent types of LLMs can handle these problems. In this study, we\nquantitatively evaluated these two abilities of LLMs in the context of\nautonomous driving. Furthermore, to conduct a Proof of Concept (POC) for the\nfeasibility of implementing these abilities in actual vehicles, we developed a\nsystem that uses LLMs to drive a vehicle.", "published": "2023-12-11 12:56:40", "link": "http://arxiv.org/abs/2312.06351v1", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Linguistic and Structural Basis of Engineering Design Knowledge", "abstract": "Natural language artefact descriptions are primary carriers of engineering\ndesign knowledge, whose retrieval, representation, and reuse are fundamental to\nsupporting knowledge-intensive tasks in the design process. In this paper, we\nexplicate design knowledge from patented artefact descriptions as knowledge\ngraphs and examine these to understand the linguistic and structural basis. The\npurpose of our work is to advance the traditional and ontological perspectives\nof design knowledge and to guide Large-Language Models (LLMs) on how to\narticulate natural language responses that reflect knowledge that is valuable\nin a design environment. We populate 33,881 knowledge graphs from a sample of\npatents stratified according to technology classes. For linguistic basis, we\nconduct Zipf distribution analyses on the frequencies of unique entities and\nrelationships to identify 64 and 37 generalisable linguistic syntaxes\nrespectively. The relationships largely represent attributes ('of'), structure\n('in', 'with'), purpose ('to', 'for'), hierarchy ('include'), exemplification\n('such as'), and behaviour ('to', 'from'). For structural basis, we draw\ninspiration from various studies on biological/ecological networks and discover\nmotifs from patent knowledge graphs. We identify four 3-node and four 4-node\nsubgraph patterns that could be converged and simplified into sequence\n[->...->], aggregation [->...<-], and hierarchy [<-...->]. Based on these\nresults, we suggest concretisation strategies for entities and relationships\nand explicating hierarchical structures, potentially aiding the construction\nand modularisation of design knowledge.", "published": "2023-12-11 13:03:39", "link": "http://arxiv.org/abs/2312.06355v3", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples", "abstract": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input. Our implementation is available at:\nhttps://github.com/KDEGroup/MMICT.", "published": "2023-12-11 13:11:04", "link": "http://arxiv.org/abs/2312.06363v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot\n  Disease Phenotyping", "abstract": "Identifying disease phenotypes from electronic health records (EHRs) is\ncritical for numerous secondary uses. Manually encoding physician knowledge\ninto rules is particularly challenging for rare diseases due to inadequate EHR\ncoding, necessitating review of clinical notes. Large language models (LLMs)\noffer promise in text understanding but may not efficiently handle real-world\nclinical documentation. We propose a zero-shot LLM-based method enriched by\nretrieval-augmented generation and MapReduce, which pre-identifies\ndisease-related text snippets to be used in parallel as queries for the LLM to\nestablish diagnosis. We show that this method as applied to pulmonary\nhypertension (PH), a rare disease characterized by elevated arterial pressures\nin the lungs, significantly outperforms physician logic rules ($F_1$ score of\n0.62 vs. 0.75). This method has the potential to enhance rare disease cohort\nidentification, expanding the scope of robust clinical research and care gap\nidentification.", "published": "2023-12-11 15:45:27", "link": "http://arxiv.org/abs/2312.06457v1", "categories": ["cs.AI", "cs.CL", "cs.IR", "I.2.7"], "primary_category": "cs.AI"}
{"title": "Revisiting the Role of Label Smoothing in Enhanced Text Sentiment\n  Classification", "abstract": "Label smoothing is a widely used technique in various domains, such as text\nclassification, image classification and speech recognition, known for\neffectively combating model overfitting. However, there is little fine-grained\nanalysis on how label smoothing enhances text sentiment classification. To fill\nin the gap, this article performs a set of in-depth analyses on eight datasets\nfor text sentiment classification and three deep learning architectures:\nTextCNN, BERT, and RoBERTa, under two learning schemes: training from scratch\nand fine-tuning. By tuning the smoothing parameters, we can achieve improved\nperformance on almost all datasets for each model architecture. We further\ninvestigate the benefits of label smoothing, finding that label smoothing can\naccelerate the convergence of deep models and make samples of different labels\neasily distinguishable.", "published": "2023-12-11 17:00:35", "link": "http://arxiv.org/abs/2312.06522v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM360: Towards Fully Transparent Open-Source LLMs", "abstract": "The recent surge in open-source Large Language Models (LLMs), such as LLaMA,\nFalcon, and Mistral, provides diverse options for AI practitioners and\nresearchers. However, most LLMs have only released partial artifacts, such as\nthe final model weights or inference code, and technical reports increasingly\nlimit their scope to high-level design choices and surface statistics. These\nchoices hinder progress in the field by degrading transparency into the\ntraining of LLMs and forcing teams to rediscover many details in the training\nprocess. We present LLM360, an initiative to fully open-source LLMs, which\nadvocates for all training code and data, model checkpoints, and intermediate\nresults to be made available to the community. The goal of LLM360 is to support\nopen and collaborative AI research by making the end-to-end LLM training\nprocess transparent and reproducible by everyone. As a first step of LLM360, we\nrelease two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder,\nincluding their training code, data, intermediate checkpoints, and analyses (at\nhttps://www.llm360.ai). We are committed to continually pushing the boundaries\nof LLMs through this open-source effort. More large-scale and stronger models\nare underway and will be released in the future.", "published": "2023-12-11 17:39:00", "link": "http://arxiv.org/abs/2312.06550v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Meta-Prompting", "abstract": "Modern generative language models are capable of interpreting input strings\nas instructions, or prompts, and carry out tasks based on them. Many approaches\nto prompting and pre-training these models involve the automated generation of\nthese prompts: meta-prompting, or prompting to obtain prompts. We propose a\ntheoretical framework based on category theory to generalize and describe them.\nThis framework is flexible enough to account for stochasticity, and allows us\nto obtain formal results around task agnosticity and equivalence of various\nmeta-prompting approaches. Experimentally, we test our framework in two active\nareas of model research: creativity and ideation. We find that user preference\nstrongly favors (p < 0.01) the prompts generated under meta-prompting, as well\nas their corresponding outputs, over a series of hardcoded baseline prompts\nthat include the original task definition. Using our framework, we argue that\nmeta-prompting is more effective than basic prompting at generating desirable\noutputs.", "published": "2023-12-11 17:46:44", "link": "http://arxiv.org/abs/2312.06562v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.CT"], "primary_category": "cs.CL"}
{"title": "Dense X Retrieval: What Retrieval Granularity Should We Use?", "abstract": "Dense retrieval has become a prominent method to obtain relevant context or\nworld knowledge in open-domain NLP tasks. When we use a learned dense retriever\non a retrieval corpus at inference time, an often-overlooked design choice is\nthe retrieval unit in which the corpus is indexed, e.g. document, passage, or\nsentence. We discover that the retrieval unit choice significantly impacts the\nperformance of both retrieval and downstream tasks. Distinct from the typical\napproach of using passages or sentences, we introduce a novel retrieval unit,\nproposition, for dense retrieval. Propositions are defined as atomic\nexpressions within text, each encapsulating a distinct factoid and presented in\na concise, self-contained natural language format. We conduct an empirical\ncomparison of different retrieval granularity. Our experiments reveal that\nindexing a corpus by fine-grained units such as propositions significantly\noutperforms passage-level units in retrieval tasks. Moreover, constructing\nprompts with fine-grained retrieved units for retrieval-augmented language\nmodels improves the performance of downstream QA tasks given a specific\ncomputation budget.", "published": "2023-12-11 18:57:35", "link": "http://arxiv.org/abs/2312.06648v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "EgoPlan-Bench: Benchmarking Multimodal Large Language Models for\n  Human-Level Planning", "abstract": "The pursuit of artificial general intelligence (AGI) has been accelerated by\nMultimodal Large Language Models (MLLMs), which exhibit superior reasoning,\ngeneralization capabilities, and proficiency in processing multimodal inputs. A\ncrucial milestone in the evolution of AGI is the attainment of human-level\nplanning, a fundamental ability for making informed decisions in complex\nenvironments, and solving a wide range of real-world problems. Despite the\nimpressive advancements in MLLMs, a question remains: How far are current MLLMs\nfrom achieving human-level planning? To shed light on this question, we\nintroduce EgoPlan-Bench, a comprehensive benchmark to evaluate the planning\nabilities of MLLMs in real-world scenarios from an egocentric perspective,\nmirroring human perception. EgoPlan-Bench emphasizes the evaluation of planning\ncapabilities of MLLMs, featuring realistic tasks, diverse action plans, and\nintricate visual observations. Our rigorous evaluation of a wide range of MLLMs\nreveals that EgoPlan-Bench poses significant challenges, highlighting a\nsubstantial scope for improvement in MLLMs to achieve human-level task\nplanning. To facilitate this advancement, we further present EgoPlan-IT, a\nspecialized instruction-tuning dataset that effectively enhances model\nperformance on EgoPlan-Bench. We have made all codes, data, and a maintained\nbenchmark leaderboard available to advance future research.", "published": "2023-12-11 03:35:58", "link": "http://arxiv.org/abs/2312.06722v3", "categories": ["cs.CV", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Honeybee: Locality-enhanced Projector for Multimodal LLM", "abstract": "In Multimodal Large Language Models (MLLMs), a visual projector plays a\ncrucial role in bridging pre-trained vision encoders with LLMs, enabling\nprofound visual understanding while harnessing the LLMs' robust capabilities.\nDespite the importance of the visual projector, it has been relatively less\nexplored. In this study, we first identify two essential projector properties:\n(i) flexibility in managing the number of visual tokens, crucial for MLLMs'\noverall efficiency, and (ii) preservation of local context from visual\nfeatures, vital for spatial understanding. Based on these findings, we propose\na novel projector design that is both flexible and locality-enhanced,\neffectively satisfying the two desirable properties. Additionally, we present\ncomprehensive strategies to effectively utilize multiple and multifaceted\ninstruction datasets. Through extensive experiments, we examine the impact of\nindividual design choices. Finally, our proposed MLLM, Honeybee, remarkably\noutperforms previous state-of-the-art methods across various benchmarks,\nincluding MME, MMBench, SEED-Bench, and LLaVA-Bench, achieving significantly\nhigher efficiency. Code and models are available at\nhttps://github.com/kakaobrain/honeybee.", "published": "2023-12-11 18:59:06", "link": "http://arxiv.org/abs/2312.06742v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Extracting Self-Consistent Causal Insights from Users Feedback with LLMs\n  and In-context Learning", "abstract": "Microsoft Windows Feedback Hub is designed to receive customer feedback on a\nwide variety of subjects including critical topics such as power and battery.\nFeedback is one of the most effective ways to have a grasp of users' experience\nwith Windows and its ecosystem. However, the sheer volume of feedback received\nby Feedback Hub makes it immensely challenging to diagnose the actual cause of\nreported issues. To better understand and triage issues, we leverage Double\nMachine Learning (DML) to associate users' feedback with telemetry signals. One\nof the main challenges we face in the DML pipeline is the necessity of domain\nknowledge for model design (e.g., causal graph), which sometimes is either not\navailable or hard to obtain. In this work, we take advantage of reasoning\ncapabilities in Large Language Models (LLMs) to generate a prior model that\nwhich to some extent compensates for the lack of domain knowledge and could be\nused as a heuristic for measuring feedback informativeness. Our LLM-based\napproach is able to extract previously known issues, uncover new bugs, and\nidentify sequences of events that lead to a bug, while minimizing out-of-domain\noutputs.", "published": "2023-12-11 20:12:46", "link": "http://arxiv.org/abs/2312.06820v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.AI"}
{"title": "Speaker-Text Retrieval via Contrastive Learning", "abstract": "In this study, we introduce a novel cross-modal retrieval task involving\nspeaker descriptions and their corresponding audio samples. Utilizing\npre-trained speaker and text encoders, we present a simple learning framework\nbased on contrastive learning. Additionally, we explore the impact of\nincorporating speaker labels into the training process. Our findings establish\nthe effectiveness of linking speaker and text information for the task for both\nEnglish and Japanese languages, across diverse data configurations. Additional\nvisual analysis unveils potential nuanced associations between speaker\nclustering and retrieval performance.", "published": "2023-12-11 01:23:50", "link": "http://arxiv.org/abs/2312.06055v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "EEND-DEMUX: End-to-End Neural Speaker Diarization via Demultiplexed\n  Speaker Embeddings", "abstract": "In recent years, there have been studies to further improve the end-to-end\nneural speaker diarization (EEND) systems. This letter proposes the EEND-DEMUX\nmodel, a novel framework utilizing demultiplexed speaker embeddings. In this\nwork, we focus on disentangling speaker-relevant information in the latent\nspace and then transform each separated latent variable into its corresponding\nspeech activity. EEND-DEMUX can directly obtain separated speaker embeddings\nthrough the demultiplexing operation in the inference phase without an external\nspeaker diarization system, an embedding extractor, or a heuristic decoding\ntechnique. Furthermore, we employ a multi-head cross-attention mechanism to\ncapture the correlation between mixture and separated speaker embeddings\neffectively. We formulate three loss functions based on matching,\northogonality, and sparsity constraints to learn robust demultiplexed speaker\nembeddings. The experimental results on the LibriMix dataset show consistently\nimproved performance in both a fixed and flexible number of speakers scenarios.", "published": "2023-12-11 02:14:55", "link": "http://arxiv.org/abs/2312.06065v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transformer Attractors for Robust and Efficient End-to-End Neural\n  Diarization", "abstract": "End-to-end neural diarization with encoder-decoder based attractors\n(EEND-EDA) is a method to perform diarization in a single neural network. EDA\nhandles the diarization of a flexible number of speakers by using an LSTM-based\nencoder-decoder that generates a set of speaker-wise attractors in an\nautoregressive manner. In this paper, we propose to replace EDA with a\ntransformer-based attractor calculation (TA) module. TA is composed of a\nCombiner block and a Transformer decoder. The main function of the combiner\nblock is to generate conversational dependent (CD) embeddings by incorporating\nlearned conversational information into a global set of embeddings. These CD\nembeddings will then serve as the input for the transformer decoder. Results on\npublic datasets show that EEND-TA achieves 2.68% absolute DER improvement over\nEEND-EDA. EEND-TA inference is 1.28 times faster than that of EEND-EDA.", "published": "2023-12-11 09:49:47", "link": "http://arxiv.org/abs/2312.06253v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Testing Correctness, Fairness, and Robustness of Speech Emotion\n  Recognition Models", "abstract": "Machine learning models for speech emotion recognition (SER) can be trained\nfor different tasks and are usually evaluated based on a few available datasets\nper task. Tasks could include arousal, valence, dominance, emotional\ncategories, or tone of voice. Those models are mainly evaluated in terms of\ncorrelation or recall, and always show some errors in their predictions. The\nerrors manifest themselves in model behaviour, which can be very different\nalong different dimensions even if the same recall or correlation is achieved\nby the model. This paper introduces a testing framework to investigate\nbehaviour of speech emotion recognition models, by requiring different metrics\nto reach a certain threshold in order to pass a test. The test metrics can be\ngrouped in terms of correctness, fairness, and robustness. It also provides a\nmethod for automatically specifying test thresholds for fairness tests, based\non the datasets used, and recommendations on how to select the remaining test\nthresholds. We evaluated a xLSTM-based and nine transformer-based acoustic\nfoundation models against a convolutional baseline model, testing their\nperformance on arousal, valence, dominance, and emotional category\nclassification. The test results highlight, that models with high correlation\nor recall might rely on shortcuts -- such as text sentiment --, and differ in\nterms of fairness.", "published": "2023-12-11 10:15:35", "link": "http://arxiv.org/abs/2312.06270v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Towards Domain-Specific Cross-Corpus Speech Emotion Recognition Approach", "abstract": "Cross-corpus speech emotion recognition (SER) poses a challenge due to\nfeature distribution mismatch, potentially degrading the performance of\nestablished SER methods. In this paper, we tackle this challenge by proposing a\nnovel transfer subspace learning method called acoustic knowledgeguided\ntransfer linear regression (AKTLR). Unlike existing approaches, which often\noverlook domain-specific knowledge related to SER and simply treat cross-corpus\nSER as a generic transfer learning task, our AKTLR method is built upon a\nwell-designed acoustic knowledge-guided dual sparsity constraint mechanism.\nThis mechanism emphasizes the potential of minimalistic acoustic parameter\nfeature sets to alleviate classifier overadaptation, which is empirically\nvalidated acoustic knowledge in SER, enabling superior generalization in\ncross-corpus SER tasks compared to using large feature sets. Through this\nmechanism, we extend a simple transfer linear regression model to AKTLR. This\nextension harnesses its full capability to seek emotiondiscriminative and\ncorpus-invariant features from established acoustic parameter feature sets used\nfor describing speech signals across two scales: contributive acoustic\nparameter groups and constituent elements within each contributive group. Our\nproposed method is evaluated through extensive cross-corpus SER experiments on\nthree widely-used speech emotion corpora: EmoDB, eNTERFACE, and CASIA. The\nresults confirm the effectiveness and superior performance of our method,\noutperforming recent state-of-the-art transfer subspace learning and deep\ntransfer learning-based cross-corpus SER methods. Furthermore, our work\nprovides experimental evidence supporting the feasibility and superiority of\nincorporating domain-specific knowledge into the transfer learning model to\naddress cross-corpus SER tasks.", "published": "2023-12-11 15:53:57", "link": "http://arxiv.org/abs/2312.06466v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MART: Learning Hierarchical Music Audio Representations with Part-Whole\n  Transformer", "abstract": "Recent research in self-supervised contrastive learning of music\nrepresentations has demonstrated remarkable results across diverse downstream\ntasks. However, a prevailing trend in existing methods involves representing\nequally-sized music clips in either waveform or spectrogram formats, often\noverlooking the intrinsic part-whole hierarchies within music. In our quest to\ncomprehend the bottom-up structure of music, we introduce MART, a hierarchical\nmusic representation learning approach that facilitates feature interactions\namong cropped music clips while considering their part-whole hierarchies.\nSpecifically, we propose a hierarchical part-whole transformer to capture the\nstructural relationships between music clips in a part-whole hierarchy.\nFurthermore, a hierarchical contrastive learning objective is crafted to align\npart-whole music representations at adjacent levels, progressively establishing\na multi-hierarchy representation space. The effectiveness of our music\nrepresentation learning from part-whole hierarchies has been empirically\nvalidated across multiple downstream tasks, including music classification and\ncover song identification.", "published": "2023-12-11 08:17:58", "link": "http://arxiv.org/abs/2312.06197v3", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for\n  Audio-Visual Segmentation", "abstract": "Recently, an audio-visual segmentation (AVS) task has been introduced, aiming\nto group pixels with sounding objects within a given video. This task\nnecessitates a first-ever audio-driven pixel-level understanding of the scene,\nposing significant challenges. In this paper, we propose an innovative\naudio-visual transformer framework, termed COMBO, an acronym for COoperation of\nMulti-order Bilateral relatiOns. For the first time, our framework explores\nthree types of bilateral entanglements within AVS: pixel entanglement, modality\nentanglement, and temporal entanglement. Regarding pixel entanglement, we\nemploy a Siam-Encoder Module (SEM) that leverages prior knowledge to generate\nmore precise visual features from the foundational model. For modality\nentanglement, we design a Bilateral-Fusion Module (BFM), enabling COMBO to\nalign corresponding visual and auditory signals bi-directionally. As for\ntemporal entanglement, we introduce an innovative adaptive inter-frame\nconsistency loss according to the inherent rules of temporal. Comprehensive\nexperiments and ablation studies on AVSBench-object (84.7 mIoU on S4, 59.2 mIou\non MS3) and AVSBench-semantic (42.1 mIoU on AVSS) datasets demonstrate that\nCOMBO surpasses previous state-of-the-art methods. Code and more results will\nbe publicly available at https://yannqi.github.io/AVS-COMBO/.", "published": "2023-12-11 15:51:38", "link": "http://arxiv.org/abs/2312.06462v2", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Deep Photonic Reservoir Computer for Speech Recognition", "abstract": "Speech recognition is a critical task in the field of artificial intelligence\nand has witnessed remarkable advancements thanks to large and complex neural\nnetworks, whose training process typically requires massive amounts of labeled\ndata and computationally intensive operations. An alternative paradigm,\nreservoir computing, is energy efficient and is well adapted to implementation\nin physical substrates, but exhibits limitations in performance when compared\nto more resource-intensive machine learning algorithms. In this work we address\nthis challenge by investigating different architectures of interconnected\nreservoirs, all falling under the umbrella of deep reservoir computing. We\npropose a photonic-based deep reservoir computer and evaluate its effectiveness\non different speech recognition tasks. We show specific design choices that aim\nto simplify the practical implementation of a reservoir computer while\nsimultaneously achieving high-speed processing of high-dimensional audio\nsignals. Overall, with the present work we hope to help the advancement of\nlow-power and high-performance neuromorphic hardware.", "published": "2023-12-11 17:43:58", "link": "http://arxiv.org/abs/2312.06558v1", "categories": ["cs.NE", "cs.SD", "eess.AS", "physics.optics"], "primary_category": "cs.NE"}
{"title": "Neural Text to Articulate Talk: Deep Text to Audiovisual Speech\n  Synthesis achieving both Auditory and Photo-realism", "abstract": "Recent advances in deep learning for sequential data have given rise to fast\nand powerful models that produce realistic videos of talking humans. The state\nof the art in talking face generation focuses mainly on lip-syncing, being\nconditioned on audio clips. However, having the ability to synthesize talking\nhumans from text transcriptions rather than audio is particularly beneficial\nfor many applications and is expected to receive more and more attention,\nfollowing the recent breakthroughs in large language models. For that, most\nmethods implement a cascaded 2-stage architecture of a text-to-speech module\nfollowed by an audio-driven talking face generator, but this ignores the highly\ncomplex interplay between audio and visual streams that occurs during speaking.\nIn this paper, we propose the first, to the best of our knowledge, text-driven\naudiovisual speech synthesizer that uses Transformers and does not follow a\ncascaded approach. Our method, which we call NEUral Text to ARticulate Talk\n(NEUTART), is a talking face generator that uses a joint audiovisual feature\nspace, as well as speech-informed 3D facial reconstructions and a lip-reading\nloss for visual supervision. The proposed model produces photorealistic talking\nface videos with human-like articulation and well-synced audiovisual streams.\nOur experiments on audiovisual datasets as well as in-the-wild videos reveal\nstate-of-the-art generation quality both in terms of objective metrics and\nhuman evaluation.", "published": "2023-12-11 18:41:55", "link": "http://arxiv.org/abs/2312.06613v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
