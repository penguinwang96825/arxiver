{"title": "Identifying and interpreting non-aligned human conceptual\n  representations using language modeling", "abstract": "The question of whether people's experience in the world shapes conceptual\nrepresentation and lexical semantics is longstanding. Word-association,\nfeature-listing and similarity rating tasks aim to address this question but\nrequire a subjective interpretation of the latent dimensions identified. In\nthis study, we introduce a supervised representational-alignment method that\n(i) determines whether two groups of individuals share the same basis of a\ncertain category, and (ii) explains in what respects they differ. In applying\nthis method, we show that congenital blindness induces conceptual\nreorganization in both a-modal and sensory-related verbal domains, and we\nidentify the associated semantic shifts. We first apply supervised\nfeature-pruning to a language model (GloVe) to optimize prediction accuracy of\nhuman similarity judgments from word embeddings. Pruning identifies one subset\nof retained GloVe features that optimizes prediction of judgments made by\nsighted individuals and another subset that optimizes judgments made by blind.\nA linear probing analysis then interprets the latent semantics of these\nfeature-subsets by learning a mapping from the retained GloVe features to 65\ninterpretable semantic dimensions. We applied this approach to seven semantic\ndomains, including verbs related to motion, sight, touch, and amodal verbs\nrelated to knowledge acquisition. We find that blind individuals more strongly\nassociate social and cognitive meanings to verbs related to motion or those\ncommunicating non-speech vocal utterances (e.g., whimper, moan). Conversely,\nfor amodal verbs, they demonstrate much sparser information. Finally, for some\nverbs, representations of blind and sighted are highly similar. The study\npresents a formal approach for studying interindividual differences in word\nmeaning, and the first demonstration of how blindness impacts conceptual\nrepresentation of everyday verbs.", "published": "2024-03-10 13:02:27", "link": "http://arxiv.org/abs/2403.06204v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized LoRA for Human-Centered Text Understanding", "abstract": "Effectively and efficiently adapting a pre-trained language model (PLM) for\nhuman-centered text understanding (HCTU) is challenging since user tokens are\nmillion-level in most personalized applications and do not have concrete\nexplicit semantics. A standard and parameter-efficient approach (e.g., LoRA)\nnecessitates memorizing numerous suits of adapters for each user. In this work,\nwe introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework\nfor the HCTU task. PLoRA is effective, parameter-efficient, and dynamically\ndeploying in PLMs. Moreover, a personalized dropout and a mutual information\nmaximizing strategies are adopted and hence the proposed PLoRA can be well\nadapted to few/zero-shot learning scenarios for the cold-start issue.\nExperiments conducted on four benchmark datasets show that the proposed method\noutperforms existing methods in full/few/zero-shot learning scenarios for the\nHCTU task, even though it has fewer trainable parameters. For reproducibility,\nthe code for this paper is available at: https://github.com/yoyo-yun/PLoRA.", "published": "2024-03-10 13:04:54", "link": "http://arxiv.org/abs/2403.06208v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LIEDER: Linguistically-Informed Evaluation for Discourse Entity\n  Recognition", "abstract": "Discourse Entity (DE) recognition is the task of identifying novel and known\nentities introduced within a text. While previous work has found that large\nlanguage models have basic, if imperfect, DE recognition abilities (Schuster\nand Linzen, 2022), it remains largely unassessed which of the fundamental\nsemantic properties that govern the introduction and subsequent reference to\nDEs they have knowledge of. We propose the Linguistically-Informed Evaluation\nfor Discourse Entity Recognition (LIEDER) dataset that allows for a detailed\nexamination of language models' knowledge of four crucial semantic properties:\nexistence, uniqueness, plurality, and novelty. We find evidence that\nstate-of-the-art large language models exhibit sensitivity to all of these\nproperties except novelty, which demonstrates that they have yet to reach\nhuman-level language understanding abilities.", "published": "2024-03-10 20:20:16", "link": "http://arxiv.org/abs/2403.06301v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Language Models for Multilingual Sentiment Analysis", "abstract": "The rapid advancement of social media enables us to analyze user opinions. In\nrecent times, sentiment analysis has shown a prominent research gap in\nunderstanding human sentiment based on the content shared on social media.\nAlthough sentiment analysis for commonly spoken languages has advanced\nsignificantly, low-resource languages like Arabic continue to get little\nresearch due to resource limitations. In this study, we explore sentiment\nanalysis on tweet texts from SemEval-17 and the Arabic Sentiment Tweet dataset.\nMoreover, We investigated four pretrained language models and proposed two\nensemble language models. Our findings include monolingual models exhibiting\nsuperior performance and ensemble models outperforming the baseline while the\nmajority voting ensemble outperforms the English language.", "published": "2024-03-10 01:39:10", "link": "http://arxiv.org/abs/2403.06060v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Target-constrained Bidirectional Planning for Generation of\n  Target-oriented Proactive Dialogue", "abstract": "Target-oriented proactive dialogue systems aim to lead conversations from a\ndialogue context toward a pre-determined target, such as making recommendations\non designated items or introducing new specific topics. To this end, it is\ncritical for such dialogue systems to plan reasonable actions to drive the\nconversation proactively, and meanwhile, to plan appropriate topics to move the\nconversation forward to the target topic smoothly. In this work, we mainly\nfocus on effective dialogue planning for target-oriented dialogue generation.\nInspired by decision-making theories in cognitive science, we propose a novel\ntarget-constrained bidirectional planning (TRIP) approach, which plans an\nappropriate dialogue path by looking ahead and looking back. By formulating the\nplanning as a generation task, our TRIP bidirectionally generates a dialogue\npath consisting of a sequence of <action, topic> pairs using two Transformer\ndecoders. They are expected to supervise each other and converge on consistent\nactions and topics by minimizing the decision gap and contrastive generation of\ntargets. Moreover, we propose a target-constrained decoding algorithm with a\nbidirectional agreement to better control the planning process. Subsequently,\nwe adopt the planned dialogue paths to guide dialogue generation in a pipeline\nmanner, where we explore two variants: prompt-based generation and\nplan-controlled generation. Extensive experiments are conducted on two\nchallenging dialogue datasets, which are re-purposed for exploring\ntarget-oriented dialogue. Our automatic and human evaluations demonstrate that\nthe proposed methods significantly outperform various baseline models.", "published": "2024-03-10 02:14:24", "link": "http://arxiv.org/abs/2403.06063v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FrameQuant: Flexible Low-Bit Quantization for Transformers", "abstract": "Transformers are the backbone of powerful foundation models for many Vision\nand Natural Language Processing tasks. But their compute and memory/storage\nfootprint is large, and so, serving such models is expensive often requiring\nhigh-end hardware. To mitigate this difficulty, Post-Training Quantization\nseeks to modify a pre-trained model and quantize it to eight bits or lower,\nsignificantly boosting compute/memory/latency efficiency. Such models have been\nsuccessfully quantized to four bits with some performance loss. In this work,\nwe outline a simple scheme to quantize Transformer-based models to just two\nbits (plus some overhead) with only a small drop in accuracy. Key to our\nformulation is a concept borrowed from Harmonic analysis called Fusion Frames.\nOur main finding is that the quantization must take place not in the original\nweight space, but instead in the Fusion Frame representations. If quantization\nis interpreted as the addition of noise, our casting of the problem allows\ninvoking an extensive body of known consistent recovery and noise robustness\nguarantees. Further, if desired, de-noising filters are known in closed form.\nWe show empirically, via a variety of experiments, that (almost) two-bit\nquantization for Transformer models promises sizable efficiency gains. The code\nis available at https://github.com/vsingh-group/FrameQuant", "published": "2024-03-10 04:01:49", "link": "http://arxiv.org/abs/2403.06082v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video\n  Diffusion Models", "abstract": "The arrival of Sora marks a new era for text-to-video diffusion models,\nbringing significant advancements in video generation and potential\napplications. However, Sora, along with other text-to-video diffusion models,\nis highly reliant on prompts, and there is no publicly available dataset that\nfeatures a study of text-to-video prompts. In this paper, we introduce VidProM,\nthe first large-scale dataset comprising 1.67 Million unique text-to-Video\nPrompts from real users. Additionally, this dataset includes 6.69 million\nvideos generated by four state-of-the-art diffusion models, alongside some\nrelated data. We initially discuss the curation of this large-scale dataset, a\nprocess that is both time-consuming and costly. Subsequently, we underscore the\nneed for a new prompt dataset specifically designed for text-to-video\ngeneration by illustrating how VidProM differs from DiffusionDB, a large-scale\nprompt-gallery dataset for image generation. Our extensive and diverse dataset\nalso opens up many exciting new research areas. For instance, we suggest\nexploring text-to-video prompt engineering, efficient video generation, and\nvideo copy detection for diffusion models to develop better, more efficient,\nand safer models. The project (including the collected dataset VidProM and\nrelated code) is publicly available at https://vidprom.github.io under the\nCC-BY-NC 4.0 License.", "published": "2024-03-10 05:40:12", "link": "http://arxiv.org/abs/2403.06098v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large Language Models on Fine-grained Emotion Detection Dataset with\n  Data Augmentation and Transfer Learning", "abstract": "This paper delves into enhancing the classification performance on the\nGoEmotions dataset, a large, manually annotated dataset for emotion detection\nin text. The primary goal of this paper is to address the challenges of\ndetecting subtle emotions in text, a complex issue in Natural Language\nProcessing (NLP) with significant practical applications. The findings offer\nvaluable insights into addressing the challenges of emotion detection in text\nand suggest directions for future research, including the potential for a\nsurvey paper that synthesizes methods and performances across various datasets\nin this domain.", "published": "2024-03-10 06:30:54", "link": "http://arxiv.org/abs/2403.06108v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-grainedly Synthesize Streaming Data Based On Large Language Models\n  With Graph Structure Understanding For Data Sparsity", "abstract": "Due to the sparsity of user data, sentiment analysis on user reviews in\ne-commerce platforms often suffers from poor performance, especially when faced\nwith extremely sparse user data or long-tail labels. Recently, the emergence of\nLLMs has introduced new solutions to such problems by leveraging graph\nstructures to generate supplementary user profiles. However, previous\napproaches have not fully utilized the graph understanding capabilities of LLMs\nand have struggled to adapt to complex streaming data environments. In this\nwork, we propose a fine-grained streaming data synthesis framework that\ncategorizes sparse users into three categories: Mid-tail, Long-tail, and\nExtreme. Specifically, we design LLMs to comprehensively understand three key\ngraph elements in streaming data, including Local-global Graph Understanding,\nSecond-Order Relationship Extraction, and Product Attribute Understanding,\nwhich enables the generation of high-quality synthetic data to effectively\naddress sparsity across different categories. Experimental results on three\nreal datasets demonstrate significant performance improvements, with\nsynthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%,\nrespectively.", "published": "2024-03-10 08:59:04", "link": "http://arxiv.org/abs/2403.06139v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Automatically Score Proficiency of Written\n  Essays?", "abstract": "Although several methods were proposed to address the problem of automated\nessay scoring (AES) in the last 50 years, there is still much to desire in\nterms of effectiveness. Large Language Models (LLMs) are transformer-based\nmodels that demonstrate extraordinary capabilities on various tasks. In this\npaper, we test the ability of LLMs, given their powerful linguistic knowledge,\nto analyze and effectively score written essays. We experimented with two\npopular LLMs, namely ChatGPT and Llama. We aim to check if these models can do\nthis task and, if so, how their performance is positioned among the\nstate-of-the-art (SOTA) models across two levels, holistically and per\nindividual writing trait. We utilized prompt-engineering tactics in designing\nfour different prompts to bring their maximum potential to this task. Our\nexperiments conducted on the ASAP dataset revealed several interesting\nobservations. First, choosing the right prompt depends highly on the model and\nnature of the task. Second, the two LLMs exhibited comparable average\nperformance in AES, with a slight advantage for ChatGPT. Finally, despite the\nperformance gap between the two LLMs and SOTA models in terms of predictions,\nthey provide feedback to enhance the quality of the essays, which can\npotentially help both teachers and students.", "published": "2024-03-10 09:39:00", "link": "http://arxiv.org/abs/2403.06149v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models", "abstract": "Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.", "published": "2024-03-10 12:43:27", "link": "http://arxiv.org/abs/2403.06199v4", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "No Language is an Island: Unifying Chinese and English in Financial\n  Large Language Models, Instruction Data, and Benchmarks", "abstract": "While the progression of Large Language Models (LLMs) has notably propelled\nfinancial analysis, their application has largely been confined to singular\nlanguage realms, leaving untapped the potential of bilingual Chinese-English\ncapacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating\nthe ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.\nICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated\nand original English datasets, enriching the breadth and depth of bilingual\nfinancial modeling. It provides unrestricted access to diverse model variants,\na substantial compilation of diverse cross-lingual and multi-modal instruction\ndata, and an evaluation benchmark with expert annotations, comprising 10 NLP\ntasks, 20 bilingual specific tasks, totaling 95k datasets. Our thorough\nevaluation emphasizes the advantages of incorporating these bilingual datasets,\nespecially in translation tasks and utilizing original English data, enhancing\nboth linguistic flexibility and analytical acuity in financial contexts.\nNotably, ICE-INTENT distinguishes itself by showcasing significant enhancements\nover conventional LLMs and existing financial LLMs in bilingual milieus,\nunderscoring the profound impact of robust bilingual data on the accuracy and\nefficacy of financial NLP.", "published": "2024-03-10 16:22:20", "link": "http://arxiv.org/abs/2403.06249v3", "categories": ["cs.CE", "cs.CL"], "primary_category": "cs.CE"}
{"title": "Transformer based Multitask Learning for Image Captioning and Object\n  Detection", "abstract": "In several real-world scenarios like autonomous navigation and mobility, to\nobtain a better visual understanding of the surroundings, image captioning and\nobject detection play a crucial role. This work introduces a novel multitask\nlearning framework that combines image captioning and object detection into a\njoint model. We propose TICOD, Transformer-based Image Captioning and Object\ndetection model for jointly training both tasks by combining the losses\nobtained from image captioning and object detection networks. By leveraging\njoint training, the model benefits from the complementary information shared\nbetween the two tasks, leading to improved performance for image captioning.\nOur approach utilizes a transformer-based architecture that enables end-to-end\nnetwork integration for image captioning and object detection and performs both\ntasks jointly. We evaluate the effectiveness of our approach through\ncomprehensive experiments on the MS-COCO dataset. Our model outperforms the\nbaselines from image captioning literature by achieving a 3.65% improvement in\nBERTScore.", "published": "2024-03-10 19:31:13", "link": "http://arxiv.org/abs/2403.06292v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "L^2GC:Lorentzian Linear Graph Convolutional Networks for Node\n  Classification", "abstract": "Linear Graph Convolutional Networks (GCNs) are used to classify the node in\nthe graph data. However, we note that most existing linear GCN models perform\nneural network operations in Euclidean space, which do not explicitly capture\nthe tree-like hierarchical structure exhibited in real-world datasets that\nmodeled as graphs. In this paper, we attempt to introduce hyperbolic space into\nlinear GCN and propose a novel framework for Lorentzian linear GCN.\nSpecifically, we map the learned features of graph nodes into hyperbolic space,\nand then perform a Lorentzian linear feature transformation to capture the\nunderlying tree-like structure of data. Experimental results on standard\ncitation networks datasets with semi-supervised learning show that our approach\nyields new state-of-the-art results of accuracy 74.7$\\%$ on Citeseer and\n81.3$\\%$ on PubMed datasets. Furthermore, we observe that our approach can be\ntrained up to two orders of magnitude faster than other nonlinear GCN models on\nPubMed dataset. Our code is publicly available at\nhttps://github.com/llqy123/LLGC-master.", "published": "2024-03-10 02:16:13", "link": "http://arxiv.org/abs/2403.06064v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese\n  Address Entity Recognition Dataset for UAV Delivery", "abstract": "We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.", "published": "2024-03-10 05:12:16", "link": "http://arxiv.org/abs/2403.06097v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained\n  Monetary Policy Analysis Framework on Their Language", "abstract": "The effectiveness of central bank communication is a crucial aspect of\nmonetary policy transmission. While recent research has examined the influence\nof policy communication by the chairs of the Federal Reserve on various\nfinancial variables, much of the literature relies on rule-based or\ndictionary-based methods in parsing the language of the chairs, leaving nuanced\ninformation about policy stance contained in nonverbal emotion out of the\nanalysis. In the current study, we propose the Fine-Grained Monetary Policy\nAnalysis Framework (FMPAF), a novel approach that integrates large language\nmodels (LLMs) with regression analysis to provide a comprehensive analysis of\nthe impact of the press-conference communications of chairs of the Federal\nReserve on financial markets. We conduct extensive comparisons of model\nperformance under different levels of granularity, modalities, and\ncommunication scenarios. Based on our preferred specification, a one-unit\nincrease in the sentiment score is associated with an increase of the price of\nS\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a\n15-basis-point decrease in the policy interest rate, while not leading to a\nsignificant response in exchange rates.", "published": "2024-03-10 07:21:31", "link": "http://arxiv.org/abs/2403.06115v1", "categories": ["cs.CL", "cs.AI", "cs.CE"], "primary_category": "cs.CL"}
{"title": "Are You Being Tracked? Discover the Power of Zero-Shot Trajectory\n  Tracing with LLMs!", "abstract": "There is a burgeoning discussion around the capabilities of Large Language\nModels (LLMs) in acting as fundamental components that can be seamlessly\nincorporated into Artificial Intelligence of Things (AIoT) to interpret complex\ntrajectories. This study introduces LLMTrack, a model that illustrates how LLMs\ncan be leveraged for Zero-Shot Trajectory Recognition by employing a novel\nsingle-prompt technique that combines role-play and think step-by-step\nmethodologies with unprocessed Inertial Measurement Unit (IMU) data. We\nevaluate the model using real-world datasets designed to challenge it with\ndistinct trajectories characterized by indoor and outdoor scenarios. In both\ntest scenarios, LLMTrack not only meets but exceeds the performance benchmarks\nset by traditional machine learning approaches and even contemporary\nstate-of-the-art deep learning models, all without the requirement of training\non specialized datasets. The results of our research suggest that, with\nstrategically designed prompts, LLMs can tap into their extensive knowledge\nbase and are well-equipped to analyze raw sensor data with remarkable\neffectiveness.", "published": "2024-03-10 12:50:35", "link": "http://arxiv.org/abs/2403.06201v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned\n  Decision", "abstract": "Numerous large language model (LLM) agents have been built for different\ntasks like web navigation and online shopping due to LLM's wide knowledge and\ntext-understanding ability. Among these works, many of them utilize in-context\nexamples to achieve generalization without the need for fine-tuning, while few\nof them have considered the problem of how to select and effectively utilize\nthese examples. Recently, methods based on trajectory-level retrieval with task\nmeta-data and using trajectories as in-context examples have been proposed to\nimprove the agent's overall performance in some sequential decision making\ntasks. However, these methods can be problematic due to plausible examples\nretrieved without task-specific state transition dynamics and long input with\nplenty of irrelevant context. In this paper, we propose a novel framework\n(TRAD) to address these issues. TRAD first conducts Thought Retrieval,\nachieving step-level demonstration selection via thought matching, leading to\nmore helpful demonstrations and less irrelevant input noise. Then, TRAD\nintroduces Aligned Decision, complementing retrieved demonstration steps with\ntheir previous or subsequent steps, which enables tolerance for imperfect\nthought and provides a choice for balance between more context and less noise.\nExtensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not\nonly outperforms state-of-the-art models but also effectively helps in reducing\nnoise and promoting generalization. Furthermore, TRAD has been deployed in\nreal-world scenarios of a global business insurance company and improves the\nsuccess rate of robotic process automation.", "published": "2024-03-10 13:58:38", "link": "http://arxiv.org/abs/2403.06221v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "SCORE: Self-supervised Correspondence Fine-tuning for Improved Content\n  Representations", "abstract": "There is a growing interest in cost-effective self-supervised fine-tuning\n(SSFT) of self-supervised learning (SSL)-based speech models to obtain\ntask-specific representations. These task-specific representations are used for\nrobust performance on various downstream tasks by fine-tuning on the labelled\ndata. This work presents a cost-effective SSFT method named Self-supervised\nCorrespondence (SCORE) fine-tuning to adapt the SSL speech representations for\ncontent-related tasks. The proposed method uses a correspondence training\nstrategy, aiming to learn similar representations from perturbed speech and\noriginal speech. Commonly used data augmentation techniques for content-related\ntasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT\noutperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of\nfine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme\nrecognition, and query-by-example tasks, with relative improvements of 1.09%,\n3.58%, and 12.65%, respectively. SCORE provides competitive results with the\nrecently proposed SSFT method SPIN, using only 1/3 of the processed speech\ncompared to SPIN.", "published": "2024-03-10 16:57:51", "link": "http://arxiv.org/abs/2403.06260v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Unpacking Tokenization: Evaluating Text Compression and its Correlation\n  with Model Performance", "abstract": "Despite it being the cornerstone of BPE, the most common tokenization\nalgorithm, the importance of compression in the tokenization process is still\nunclear. In this paper, we argue for the theoretical importance of compression,\nthat can be viewed as 0-gram language modeling where equal probability is\nassigned to all tokens. We also demonstrate the empirical importance of\ncompression for downstream success of pre-trained language models. We control\nthe compression ability of several BPE tokenizers by varying the amount of\ndocuments available during their training: from 1 million documents to a\ncharacter-based tokenizer equivalent to no training data at all. We then\npre-train English language models based on those tokenizers and fine-tune them\nover several tasks. We show that there is a correlation between tokenizers'\ncompression and models' downstream performance, suggesting that compression is\na reliable intrinsic indicator of tokenization quality. These correlations are\nmore pronounced for generation tasks (over classification) or for smaller\nmodels (over large ones). We replicated a representative part of our\nexperiments on Turkish and found similar results, confirming that our results\nhold for languages with typological characteristics dissimilar to English. We\nconclude that building better compressing tokenizers is a fruitful avenue for\nfurther research and for improving overall model performance.", "published": "2024-03-10 17:02:53", "link": "http://arxiv.org/abs/2403.06265v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "From Instructions to Constraints: Language Model Alignment with\n  Automatic Constraint Verification", "abstract": "User alignment is crucial for adapting general-purpose language models (LMs)\nto downstream tasks, but human annotations are often not available for all\ntypes of instructions, especially those with customized constraints. We observe\nthat user instructions typically contain constraints. While assessing response\nquality in terms of the whole instruction is often costly, efficiently\nevaluating the satisfaction rate of constraints is feasible. We investigate\ncommon constraints in NLP tasks, categorize them into three classes based on\nthe types of their arguments, and propose a unified framework, ACT (Aligning to\nConsTraints), to automatically produce supervision signals for user alignment\nwith constraints. Specifically, ACT uses constraint verifiers, which are\ntypically easy to implement in practice, to compute constraint satisfaction\nrate (CSR) of each response. It samples multiple responses for each prompt and\ncollect preference labels based on their CSR automatically. Subsequently, ACT\nadapts the LM to the target task through a ranking-based learning process.\nExperiments on fine-grained entity typing, abstractive summarization, and\ntemporal question answering show that ACT is able to enhance LMs' capability to\nadhere to different classes of constraints, thereby improving task performance.\nFurther experiments show that the constraint-following capabilities are\ntransferable.", "published": "2024-03-10 22:14:54", "link": "http://arxiv.org/abs/2403.06326v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic design optimization of preference-based subjective evaluation\n  with online learning in crowdsourcing environment", "abstract": "A preference-based subjective evaluation is a key method for evaluating\ngenerative media reliably. However, its huge combinations of pairs prohibit it\nfrom being applied to large-scale evaluation using crowdsourcing. To address\nthis issue, we propose an automatic optimization method for preference-based\nsubjective evaluation in terms of pair combination selections and allocation of\nevaluation volumes with online learning in a crowdsourcing environment. We use\na preference-based online learning method based on a sorting algorithm to\nidentify the total order of evaluation targets with minimum sample volumes. Our\nonline learning algorithm supports parallel and asynchronous execution under\nfixed-budget conditions required for crowdsourcing. Our experiment on\npreference-based subjective evaluation of synthetic speech shows that our\nmethod successfully optimizes the test by reducing pair combinations from 351\nto 83 and allocating optimal evaluation volumes for each pair ranging from 30\nto 663 without compromising evaluation accuracies and wasting budget\nallocations.", "published": "2024-03-10 05:55:00", "link": "http://arxiv.org/abs/2403.06100v1", "categories": ["cs.HC", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.HC"}
{"title": "Editing Conceptual Knowledge for Large Language Models", "abstract": "Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.", "published": "2024-03-10 16:57:10", "link": "http://arxiv.org/abs/2403.06259v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
