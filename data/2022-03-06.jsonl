{"title": "A Multi-Document Coverage Reward for RELAXed Multi-Document\n  Summarization", "abstract": "Multi-document summarization (MDS) has made significant progress in recent\nyears, in part facilitated by the availability of new, dedicated datasets and\ncapacious language models. However, a standing limitation of these models is\nthat they are trained against limited references and with plain\nmaximum-likelihood objectives. As for many other generative tasks,\nreinforcement learning (RL) offers the potential to improve the training of MDS\nmodels; yet, it requires a carefully-designed reward that can ensure\nappropriate leverage of both the reference summaries and the input documents.\nFor this reason, in this paper we propose fine-tuning an MDS baseline with a\nreward that balances a reference-based metric such as ROUGE with coverage of\nthe input documents. To implement the approach, we utilize RELAX (Grathwohl et\nal., 2018), a contemporary gradient estimator which is both low-variance and\nunbiased, and we fine-tune the baseline in a few-shot style for both stability\nand computational efficiency. Experimental results over the Multi-News and WCEP\nMDS datasets show significant improvements of up to +0.95 pp average ROUGE\nscore and +3.17 pp METEOR score over the baseline, and competitive results with\nthe literature. In addition, they show that the coverage of the input documents\nis increased, and evenly across all documents.", "published": "2022-03-06 07:33:01", "link": "http://arxiv.org/abs/2203.02894v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Divide and Conquer: Text Semantic Matching with Disentangled Keywords\n  and Intents", "abstract": "Text semantic matching is a fundamental task that has been widely used in\nvarious scenarios, such as community question answering, information retrieval,\nand recommendation. Most state-of-the-art matching models, e.g., BERT, directly\nperform text comparison by processing each word uniformly. However, a query\nsentence generally comprises content that calls for different levels of\nmatching granularity. Specifically, keywords represent factual information such\nas action, entity, and event that should be strictly matched, while intents\nconvey abstract concepts and ideas that can be paraphrased into various\nexpressions. In this work, we propose a simple yet effective training strategy\nfor text semantic matching in a divide-and-conquer manner by disentangling\nkeywords from intents. Our approach can be easily combined with pre-trained\nlanguage models (PLM) without influencing their inference efficiency, achieving\nstable performance improvements against a wide range of PLMs on three\nbenchmarks.", "published": "2022-03-06 07:48:24", "link": "http://arxiv.org/abs/2203.02898v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph Neural Network Enhanced Language Models for Efficient Multilingual\n  Text Classification", "abstract": "Online social media works as a source of various valuable and actionable\ninformation during disasters. These information might be available in multiple\nlanguages due to the nature of user generated content. An effective system to\nautomatically identify and categorize these actionable information should be\ncapable to handle multiple languages and under limited supervision. However,\nexisting works mostly focus on English language only with the assumption that\nsufficient labeled data is available. To overcome these challenges, we propose\na multilingual disaster related text classification system which is capable to\nwork under \\{mono, cross and multi\\} lingual scenarios and under limited\nsupervision. Our end-to-end trainable framework combines the versatility of\ngraph neural networks, by applying over the corpus, with the power of\ntransformer based large language models, over examples, with the help of\ncross-attention between the two. We evaluate our framework over total nine\nEnglish, Non-English and monolingual datasets in \\{mono, cross and multi\\}\nlingual classification scenarios. Our framework outperforms state-of-the-art\nmodels in disaster domain and multilingual BERT baseline in terms of Weighted\nF$_1$ score. We also show the generalizability of the proposed model under\nlimited supervision.", "published": "2022-03-06 09:05:42", "link": "http://arxiv.org/abs/2203.02912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conditional Bilingual Mutual Information Based Adaptive Training for\n  Neural Machine Translation", "abstract": "Token-level adaptive training approaches can alleviate the token imbalance\nproblem and thus improve neural machine translation, through re-weighting the\nlosses of different target tokens based on specific statistical metrics (e.g.,\ntoken frequency or mutual information). Given that standard translation models\nmake predictions on the condition of previous target contexts, we argue that\nthe above statistical metrics ignore target context information and may assign\ninappropriate weights to target tokens. While one possible solution is to\ndirectly take target contexts into these statistical metrics, the\ntarget-context-aware statistical computing is extremely expensive, and the\ncorresponding storage overhead is unrealistic. To solve the above issues, we\npropose a target-context-aware metric, named conditional bilingual mutual\ninformation (CBMI), which makes it feasible to supplement target context\ninformation for statistical metrics. Particularly, our CBMI can be formalized\nas the log quotient of the translation model probability and language model\nprobability by decomposing the conditional joint distribution. Thus CBMI can be\nefficiently calculated during model training without any pre-specific\nstatistical calculations and large storage overhead. Furthermore, we propose an\neffective adaptive training approach based on both the token- and\nsentence-level CBMI. Experimental results on WMT14 English-German and WMT19\nChinese-English tasks show our approach can significantly outperform the\nTransformer baseline and other related methods.", "published": "2022-03-06 12:34:10", "link": "http://arxiv.org/abs/2203.02951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Implicit Discourse Relation Recognition", "abstract": "A discourse containing one or more sentences describes daily issues and\nevents for people to communicate their thoughts and opinions. As sentences are\nnormally consist of multiple text segments, correct understanding of the theme\nof a discourse should take into consideration of the relations in between text\nsegments. Although sometimes a connective exists in raw texts for conveying\nrelations, it is more often the cases that no connective exists in between two\ntext segments but some implicit relation does exist in between them. The task\nof implicit discourse relation recognition (IDRR) is to detect implicit\nrelation and classify its sense between two text segments without a connective.\nIndeed, the IDRR task is important to diverse downstream natural language\nprocessing tasks, such as text summarization, machine translation and so on.\nThis article provides a comprehensive and up-to-date survey for the IDRR task.\nWe first summarize the task definition and data sources widely used in the\nfield. We categorize the main solution approaches for the IDRR task from the\nviewpoint of its development history. In each solution category, we present and\nanalyze the most representative methods, including their origins, ideas,\nstrengths and weaknesses. We also present performance comparisons for those\nsolutions experimented on a public corpus with standard data processing\nprocedures. Finally, we discuss future research directions for discourse\nrelation analysis.", "published": "2022-03-06 15:12:53", "link": "http://arxiv.org/abs/2203.02982v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Focus on the Target's Vocabulary: Masked Label Smoothing for Machine\n  Translation", "abstract": "Label smoothing and vocabulary sharing are two widely used techniques in\nneural machine translation models. However, we argue that simply applying both\ntechniques can be conflicting and even leads to sub-optimal performance. When\nallocating smoothed probability, original label smoothing treats the\nsource-side words that would never appear in the target language equally to the\nreal target-side words, which could bias the translation model. To address this\nissue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the\nsoft label probability of source-side words to zero. Simple yet effective, MLS\nmanages to better integrate label smoothing with vocabulary sharing. Our\nextensive experiments show that MLS consistently yields improvement over\noriginal label smoothing on different datasets, including bilingual and\nmultilingual translation from both translation quality and model's calibration.\nOur code is released at https://github.com/PKUnlp-icler/MLS", "published": "2022-03-06 07:01:39", "link": "http://arxiv.org/abs/2203.02889v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Doctor Recommendation in Online Health Forums via Expertise Learning", "abstract": "Huge volumes of patient queries are daily generated on online health forums,\nrendering manual doctor allocation a labor-intensive task. To better help\npatients, this paper studies a novel task of doctor recommendation to enable\nautomatic pairing of a patient to a doctor with relevant expertise. While most\nprior work in recommendation focuses on modeling target users from their past\nbehavior, we can only rely on the limited words in a query to infer a patient's\nneeds for privacy reasons. For doctor modeling, we study the joint effects of\ntheir profiles and previous dialogues with other patients and explore their\ninteractions via self-learning. The learned doctor embeddings are further\nemployed to estimate their capabilities of handling a patient query with a\nmulti-head attention mechanism. For experiments, a large-scale dataset is\ncollected from Chunyu Yisheng, a Chinese online health forum, where our model\nexhibits the state-of-the-art results, outperforming baselines only consider\nprofiles and past dialogues to characterize a doctor.", "published": "2022-03-06 10:39:48", "link": "http://arxiv.org/abs/2203.02932v4", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Twitter Dataset for 2022 Russo-Ukrainian Crisis", "abstract": "Online Social Networks (OSNs) play a significant role in information sharing\nduring a crisis. The data collected during such a crisis can reflect the large\nscale public opinions and sentiment. In addition, OSN data can also be used to\nstudy different campaigns that are employed by various entities to engineer\npublic opinions. Such information sharing campaigns can range from spreading\nfactual information to propaganda and misinformation. We provide a Twitter\ndataset of the 2022 Russo-Ukrainian conflict. In the first release, we share\nover 1.6 million tweets shared during the 1st week of the crisis.", "published": "2022-03-06 12:49:40", "link": "http://arxiv.org/abs/2203.02955v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for\n  Temporal Sentence Grounding", "abstract": "Temporal sentence grounding aims to localize a target segment in an untrimmed\nvideo semantically according to a given sentence query. Most previous works\nfocus on learning frame-level features of each whole frame in the entire video,\nand directly match them with the textual information. Such frame-level feature\nextraction leads to the obstacles of these methods in distinguishing ambiguous\nvideo frames with complicated contents and subtle appearance differences, thus\nlimiting their performance. In order to differentiate fine-grained appearance\nsimilarities among consecutive frames, some state-of-the-art methods\nadditionally employ a detection model like Faster R-CNN to obtain detailed\nobject-level features in each frame for filtering out the redundant background\ncontents. However, these methods suffer from missing motion analysis since the\nobject detection module in Faster R-CNN lacks temporal modeling. To alleviate\nthe above limitations, in this paper, we propose a novel Motion- and\nAppearance-guided 3D Semantic Reasoning Network (MA3SRN), which incorporates\noptical-flow-guided motion-aware, detection-based appearance-aware, and\n3D-aware object-level features to better reason the spatial-temporal object\nrelations for accurately modelling the activity among consecutive frames.\nSpecifically, we first develop three individual branches for motion,\nappearance, and 3D encoding separately to learn fine-grained motion-guided,\nappearance-guided, and 3D-aware object features, respectively. Then, both\nmotion and appearance information from corresponding branches are associated to\nenhance the 3D-aware features for the final precise grounding. Extensive\nexperiments on three challenging datasets (ActivityNet Caption, Charades-STA\nand TACoS) demonstrate that the proposed MA3SRN model achieves a new\nstate-of-the-art.", "published": "2022-03-06 13:57:09", "link": "http://arxiv.org/abs/2203.02966v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Recent Advances in Neural Text Generation: A Task-Agnostic Survey", "abstract": "In recent years, considerable research has been dedicated to the application\nof neural models in the field of natural language generation (NLG). The primary\nobjective is to generate text that is both linguistically natural and\nhuman-like, while also exerting control over the generation process. This paper\noffers a comprehensive and task-agnostic survey of the recent advancements in\nneural text generation. These advancements have been facilitated through a\nmultitude of developments, which we categorize into four key areas: data\nconstruction, neural frameworks, training and inference strategies, and\nevaluation metrics. By examining these different aspects, we aim to provide a\nholistic overview of the progress made in the field. Furthermore, we explore\nthe future directions for the advancement of neural text generation, which\nencompass the utilization of neural pipelines and the incorporation of\nbackground knowledge. These avenues present promising opportunities to further\nenhance the capabilities of NLG systems. Overall, this survey serves to\nconsolidate the current state of the art in neural text generation and\nhighlights potential avenues for future research and development in this\ndynamic field.", "published": "2022-03-06 20:47:49", "link": "http://arxiv.org/abs/2203.03047v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey on Bias and Fairness in Natural Language Processing", "abstract": "As NLP models become more integrated with the everyday lives of people, it\nbecomes important to examine the social effect that the usage of these systems\nhas. While these models understand language and have increased accuracy on\ndifficult downstream tasks, there is evidence that these models amplify gender,\nracial and cultural stereotypes and lead to a vicious cycle in many settings.\nIn this survey, we analyze the origins of biases, the definitions of fairness,\nand how different subfields of NLP mitigate bias. We finally discuss how future\nstudies can work towards eradicating pernicious biases from NLP algorithms.", "published": "2022-03-06 18:12:30", "link": "http://arxiv.org/abs/2204.09591v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic Key-value Memory Enhanced Multi-step Graph Reasoning for\n  Knowledge-based Visual Question Answering", "abstract": "Knowledge-based visual question answering (VQA) is a vision-language task\nthat requires an agent to correctly answer image-related questions using\nknowledge that is not presented in the given image. It is not only a more\nchallenging task than regular VQA but also a vital step towards building a\ngeneral VQA system. Most existing knowledge-based VQA systems process knowledge\nand image information similarly and ignore the fact that the knowledge base\n(KB) contains complete information about a triplet, while the extracted image\ninformation might be incomplete as the relations between two objects are\nmissing or wrongly detected. In this paper, we propose a novel model named\ndynamic knowledge memory enhanced multi-step graph reasoning (DMMGR), which\nperforms explicit and implicit reasoning over a key-value knowledge memory\nmodule and a spatial-aware image graph, respectively. Specifically, the memory\nmodule learns a dynamic knowledge representation and generates a\nknowledge-aware question representation at each reasoning step. Then, this\nrepresentation is used to guide a graph attention operator over the\nspatial-aware image graph. Our model achieves new state-of-the-art accuracy on\nthe KRVQR and FVQA datasets. We also conduct ablation experiments to prove the\neffectiveness of each component of the proposed model.", "published": "2022-03-06 15:19:39", "link": "http://arxiv.org/abs/2203.02985v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Modeling Coreference Relations in Visual Dialog", "abstract": "Visual dialog is a vision-language task where an agent needs to answer a\nseries of questions grounded in an image based on the understanding of the\ndialog history and the image. The occurrences of coreference relations in the\ndialog makes it a more challenging task than visual question-answering. Most\nprevious works have focused on learning better multi-modal representations or\non exploring different ways of fusing visual and language features, while the\ncoreferences in the dialog are mainly ignored. In this paper, based on\nlinguistic knowledge and discourse features of human dialog we propose two soft\nconstraints that can improve the model's ability of resolving coreferences in\ndialog in an unsupervised way. Experimental results on the VisDial v1.0 dataset\nshows that our model, which integrates two novel and linguistically inspired\nsoft constraints in a deep transformer neural architecture, obtains new\nstate-of-the-art performance in terms of recall at 1 and other evaluation\nmetrics compared to current existing models and this without pretraining on\nother vision-language datasets. Our qualitative results also demonstrate the\neffectiveness of the method that we propose.", "published": "2022-03-06 15:22:24", "link": "http://arxiv.org/abs/2203.02986v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leashing the Inner Demons: Self-Detoxification for Language Models", "abstract": "Language models (LMs) can reproduce (or amplify) toxic language seen during\ntraining, which poses a risk to their practical application. In this paper, we\nconduct extensive experiments to study this phenomenon. We analyze the impact\nof prompts, decoding strategies and training corpora on the output toxicity.\nBased on our findings, we propose a simple yet effective method for language\nmodels to \"detoxify\" themselves without an additional large corpus or external\ndiscriminator. Compared to a supervised baseline, our proposed method shows\nbetter toxicity reduction with good generation quality in the generated content\nunder multiple settings. Warning: some examples shown in the paper may contain\nuncensored offensive content.", "published": "2022-03-06 23:55:12", "link": "http://arxiv.org/abs/2203.03072v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "C-P Map: A Novel Evaluation Toolkit for Speaker Verification", "abstract": "Evaluation trials are used to probe performance of automatic speaker\nverification (ASV) systems. In spite of the clear importance and impact,\nevaluation trials have not been seriously treated in research and engineering\npractice. This paper firstly presents a theoretical analysis on evaluation\ntrials and highlights potential bias with the most popular cross-pairing\napproach used in trials design. To interpret and settle this problem, we define\nthe concept of trial config and C-P map derived from it. The C-P map measures\nthe performance of an ASV system on various trial configs in a 2-dimensional\nmap. On the map, each location represents a particular trial config and its\ncorresponding color represents the system performance. Experiments conducted on\nrepresentative ASV systems show that the proposed C-P map offers a powerful\nevaluation toolkit for ASV performance analysis and comparison. The source code\nfor C-P map has been release at https://gitlab.com/csltstu/sunine.", "published": "2022-03-06 11:47:04", "link": "http://arxiv.org/abs/2203.02942v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "CNN self-attention voice activity detector", "abstract": "In this work we present a novel single-channel Voice Activity Detector (VAD)\napproach. We utilize a Convolutional Neural Network (CNN) which exploits the\nspatial information of the noisy input spectrum to extract frame-wise embedding\nsequence, followed by a Self Attention (SA) Encoder with a goal of finding\ncontextual information from the embedding sequence. Different from previous\nworks which were employed on each frame (with context frames) separately, our\nmethod is capable of processing the entire signal at once, and thus enabling\nlong receptive field. We show that the fusion of CNN and SA architectures\noutperforms methods based solely on CNN and SA. Extensive experimental-study\nshows that our model outperforms previous models on real-life benchmarks, and\nprovides State Of The Art (SOTA) results with relatively small and lightweight\nmodel.", "published": "2022-03-06 11:52:00", "link": "http://arxiv.org/abs/2203.02944v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Variational Auto-Encoder based Mandarin Speech Cloning", "abstract": "Speech cloning technology is becoming more sophisticated thanks to the\nadvances in machine learning. Researchers have successfully implemented\nnatural-sounding English speech synthesis and good English speech cloning by\nsome effective models. However, because of prosodic phrasing and large\ncharacter set of Mandarin, Chinese utilization of these models is not yet\ncomplete. By creating a new dataset and replacing Tacotron synthesizer with\nVAENAR-TTS, we improved the existing speech cloning technique CV2TTS to almost\nreal-time speech cloning while guaranteeing synthesis quality. In the process,\nwe customized the subjective tests of synthesis quality assessment by attaching\nvarious scenarios, so that subjects focus on the differences between voice and\nour improvements maybe were more advantageous to practical applications. The\nresults of the A/B test, real-time factor (RTF) and 2.74 mean opinion score\n(MOS) in terms of naturalness and similarity, reflect the real-time\nhigh-quality Mandarin speech cloning we achieved.", "published": "2022-03-06 14:01:39", "link": "http://arxiv.org/abs/2203.02967v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Pre-trained BERT for Audio Captioning", "abstract": "Audio captioning aims at using natural language to describe the content of an\naudio clip. Existing audio captioning systems are generally based on an\nencoder-decoder architecture, in which acoustic information is extracted by an\naudio encoder and then a language decoder is used to generate the captions.\nTraining an audio captioning system often encounters the problem of data\nscarcity. Transferring knowledge from pre-trained audio models such as\nPre-trained Audio Neural Networks (PANNs) have recently emerged as a useful\nmethod to mitigate this issue. However, there is less attention on exploiting\npre-trained language models for the decoder, compared with the encoder. BERT is\na pre-trained language model that has been extensively used in Natural Language\nProcessing (NLP) tasks. Nevertheless, the potential of BERT as the language\ndecoder for audio captioning has not been investigated. In this study, we\ndemonstrate the efficacy of the pre-trained BERT model for audio captioning.\nSpecifically, we apply PANNs as the encoder and initialize the decoder from the\npublic pre-trained BERT models. We conduct an empirical study on the use of\nthese BERT models for the decoder in the audio captioning model. Our models\nachieve competitive results with the existing audio captioning methods on the\nAudioCaps dataset.", "published": "2022-03-06 00:05:58", "link": "http://arxiv.org/abs/2203.02838v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Single microphone speaker extraction using unified time-frequency\n  Siamese-Unet", "abstract": "In this paper we present a unified time-frequency method for speaker\nextraction in clean and noisy conditions. Given a mixed signal, along with a\nreference signal, the common approaches for extracting the desired speaker are\neither applied in the time-domain or in the frequency-domain. In our approach,\nwe propose a Siamese-Unet architecture that uses both representations. The\nSiamese encoders are applied in the frequency-domain to infer the embedding of\nthe noisy and reference spectra, respectively. The concatenated representations\nare then fed into the decoder to estimate the real and imaginary components of\nthe desired speaker, which are then inverse-transformed to the time-domain. The\nmodel is trained with the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR)\nloss to exploit the time-domain information. The time-domain loss is also\nregularized with frequency-domain loss to preserve the speech patterns.\nExperimental results demonstrate that the unified approach is not only very\neasy to train, but also provides superior results as compared with\nstate-of-the-art (SOTA) Blind Source Separation (BSS) methods, as well as\ncommonly used speaker extraction approach.", "published": "2022-03-06 11:45:30", "link": "http://arxiv.org/abs/2203.02941v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HEAR: Holistic Evaluation of Audio Representations", "abstract": "What audio embedding approach generalizes best to a wide range of downstream\ntasks across a variety of everyday domains without fine-tuning? The aim of the\nHEAR benchmark is to develop a general-purpose audio representation that\nprovides a strong basis for learning in a wide variety of tasks and scenarios.\nHEAR evaluates audio representations using a benchmark suite across a variety\nof domains, including speech, environmental sound, and music. HEAR was launched\nas a NeurIPS 2021 shared challenge. In the spirit of shared exchange, each\nparticipant submitted an audio embedding model following a common API that is\ngeneral-purpose, open-source, and freely available to use. Twenty-nine models\nby thirteen external teams were evaluated on nineteen diverse downstream tasks\nderived from sixteen datasets. Open evaluation code, submitted models and\ndatasets are key contributions, enabling comprehensive and reproducible\nevaluation, as well as previously impossible longitudinal studies. It still\nremains an open question whether one single general-purpose audio\nrepresentation can perform as holistically as the human ear.", "published": "2022-03-06 18:13:09", "link": "http://arxiv.org/abs/2203.03022v3", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
