{"title": "Tensor Fusion Network for Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis is an increasingly popular research area, which\nextends the conventional language-based definition of sentiment analysis to a\nmultimodal setup where other relevant modalities accompany language. In this\npaper, we pose the problem of multimodal sentiment analysis as modeling\nintra-modality and inter-modality dynamics. We introduce a novel model, termed\nTensor Fusion Network, which learns both such dynamics end-to-end. The proposed\napproach is tailored for the volatile nature of spoken language in online\nvideos as well as accompanying gestures and voice. In the experiments, our\nmodel outperforms state-of-the-art approaches for both multimodal and unimodal\nsentiment analysis.", "published": "2017-07-23 05:54:20", "link": "http://arxiv.org/abs/1707.07250v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composing Distributed Representations of Relational Patterns", "abstract": "Learning distributed representations for relation instances is a central\ntechnique in downstream NLP applications. In order to address semantic modeling\nof relational patterns, this paper constructs a new dataset that provides\nmultiple similarity ratings for every pair of relational patterns on the\nexisting dataset. In addition, we conduct a comparative study of different\nencoders including additive composition, RNN, LSTM, and GRU for composing\ndistributed representations of relational patterns. We also present Gated\nAdditive Composition, which is an enhancement of additive composition with the\ngating mechanism. Experiments show that the new dataset does not only enable\ndetailed analyses of the different encoders, but also provides a gauge to\npredict successes of distributed representations of relational patterns in the\nrelation classification task.", "published": "2017-07-23 08:12:59", "link": "http://arxiv.org/abs/1707.07265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Embeddings for Hypernymy Detection and Directionality", "abstract": "We present a novel neural model HyperVec to learn hierarchical embeddings for\nhypernymy detection and directionality. While previous embeddings have shown\nlimitations on prototypical hypernyms, HyperVec represents an unsupervised\nmeasure where embeddings are learned in a specific order and capture the\nhypernym$-$hyponym distributional hierarchy. Moreover, our model is able to\ngeneralize over unseen hypernymy pairs, when using only small sets of training\ndata, and by mapping to other languages. Results on benchmark datasets show\nthat HyperVec outperforms both state$-$of$-$the$-$art unsupervised measures and\nembedding models on hypernymy detection and directionality, and on predicting\ngraded lexical entailment.", "published": "2017-07-23 09:55:48", "link": "http://arxiv.org/abs/1707.07273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine Grained Citation Span for References in Wikipedia", "abstract": "\\emph{Verifiability} is one of the core editing principles in Wikipedia,\neditors being encouraged to provide citations for the added content. For a\nWikipedia article, determining the \\emph{citation span} of a citation, i.e.\nwhat content is covered by a citation, is important as it helps decide for\nwhich content citations are still missing.\n  We are the first to address the problem of determining the \\emph{citation\nspan} in Wikipedia articles. We approach this problem by classifying which\ntextual fragments in an article are covered by a citation. We propose a\nsequence classification approach where for a paragraph and a citation, we\ndetermine the citation span at a fine-grained level.\n  We provide a thorough experimental evaluation and compare our approach\nagainst baselines adopted from the scientific domain, where we show improvement\nfor all evaluation metrics.", "published": "2017-07-23 10:43:26", "link": "http://arxiv.org/abs/1707.07278v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Argument-based Features to Predict and Analyse Review Helpfulness", "abstract": "We study the helpful product reviews identification problem in this paper. We\nobserve that the evidence-conclusion discourse relations, also known as\narguments, often appear in product reviews, and we hypothesise that some\nargument-based features, e.g. the percentage of argumentative sentences, the\nevidences-conclusions ratios, are good indicators of helpful reviews. To\nvalidate this hypothesis, we manually annotate arguments in 110 hotel reviews,\nand investigate the effectiveness of several combinations of argument-based\nfeatures. Experiments suggest that, when being used together with the\nargument-based features, the state-of-the-art baseline features can enjoy a\nperformance boost (in terms of F1) of 11.01\\% in average.", "published": "2017-07-23 10:51:30", "link": "http://arxiv.org/abs/1707.07279v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rule-Based Spanish Morphological Analyzer Built From Spell Checking\n  Lexicon", "abstract": "Preprocessing tools for automated text analysis have become more widely\navailable in major languages, but non-English tools are often still limited in\ntheir functionality. When working with Spanish-language text, researchers can\neasily find tools for tokenization and stemming, but may not have the means to\nextract more complex word features like verb tense or mood. Yet Spanish is a\nmorphologically rich language in which such features are often identifiable\nfrom word form. Conjugation rules are consistent, but many special verbs and\nnouns take on different rules. While building a complete dictionary of known\nwords and their morphological rules would be labor intensive, resources to do\nso already exist, in spell checkers designed to generate valid forms of known\nwords. This paper introduces a set of tools for Spanish-language morphological\nanalysis, built using the COES spell checking tools, to label person, mood,\ntense, gender and number, derive a word's root noun or verb infinitive, and\nconvert verbs to their nominal form.", "published": "2017-07-23 18:42:24", "link": "http://arxiv.org/abs/1707.07331v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sequential Model for Classifying Temporal Relations between\n  Intra-Sentence Events", "abstract": "We present a sequential model for temporal relation classification between\nintra-sentence events. The key observation is that the overall syntactic\nstructure and compositional meanings of the multi-word context between events\nare important for distinguishing among fine-grained temporal relations.\nSpecifically, our approach first extracts a sequence of context words that\nindicates the temporal relation between two events, which well align with the\ndependency path between two event mentions. The context word sequence, together\nwith a parts-of-speech tag sequence and a dependency relation sequence that are\ngenerated corresponding to the word sequence, are then provided as input to\nbidirectional recurrent neural network (LSTM) models. The neural nets learn\ncompositional syntactic and semantic representations of contexts surrounding\nthe two events and predict the temporal relation between them. Evaluation of\nthe proposed approach on TimeBank corpus shows that sequential modeling is\ncapable of accurately recognizing temporal relations between events, which\noutperforms a neural net model using various discrete features as input that\nimitates previous feature based models.", "published": "2017-07-23 20:47:02", "link": "http://arxiv.org/abs/1707.07343v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Coreference Resolution by Iteratively Unfolding Inter-dependencies\n  among Events", "abstract": "We introduce a novel iterative approach for event coreference resolution that\ngradually builds event clusters by exploiting inter-dependencies among event\nmentions within the same chain as well as across event chains. Among event\nmentions in the same chain, we distinguish within- and cross-document event\ncoreference links by using two distinct pairwise classifiers, trained\nseparately to capture differences in feature distributions of within- and\ncross-document event clusters. Our event coreference approach alternates\nbetween WD and CD clustering and combines arguments from both event clusters\nafter every merge, continuing till no more merge can be made. And then it\nperforms further merging between event chains that are both closely related to\na set of other chains of events. Experiments on the ECB+ corpus show that our\nmodel outperforms state-of-the-art methods in joint task of WD and CD event\ncoreference resolution.", "published": "2017-07-23 20:49:18", "link": "http://arxiv.org/abs/1707.07344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MatchZoo: A Toolkit for Deep Text Matching", "abstract": "In recent years, deep neural models have been widely adopted for text\nmatching tasks, such as question answering and information retrieval, showing\nimproved performance as compared with previous methods. In this paper, we\nintroduce the MatchZoo toolkit that aims to facilitate the designing, comparing\nand sharing of deep text matching models. Specifically, the toolkit provides a\nunified data preparation module for different text matching problems, a\nflexible layer-based model construction process, and a variety of training\nobjectives and evaluation metrics. In addition, the toolkit has implemented two\nschools of representative deep text matching models, namely\nrepresentation-focused models and interaction-focused models. Finally, users\ncan easily modify existing models, create and share their own models for text\nmatching in MatchZoo.", "published": "2017-07-23 09:36:50", "link": "http://arxiv.org/abs/1707.07270v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Adversarial Examples for Evaluating Reading Comprehension Systems", "abstract": "Standard accuracy metrics indicate that reading comprehension systems are\nmaking rapid progress, but the extent to which these systems truly understand\nlanguage remains unclear. To reward systems with real language understanding\nabilities, we propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). Our method tests whether systems can answer\nquestions about paragraphs that contain adversarially inserted sentences, which\nare automatically generated to distract computer systems without changing the\ncorrect answer or misleading humans. In this adversarial setting, the accuracy\nof sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$;\nwhen the adversary is allowed to add ungrammatical sequences of words, average\naccuracy on four models decreases further to $7\\%$. We hope our insights will\nmotivate the development of new models that understand language more precisely.", "published": "2017-07-23 18:26:29", "link": "http://arxiv.org/abs/1707.07328v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language modeling with Neural trans-dimensional random fields", "abstract": "Trans-dimensional random field language models (TRF LMs) have recently been\nintroduced, where sentences are modeled as a collection of random fields. The\nTRF approach has been shown to have the advantages of being computationally\nmore efficient in inference than LSTM LMs with close performance and being able\nto flexibly integrating rich features. In this paper we propose neural TRFs,\nbeyond of the previous discrete TRFs that only use linear potentials with\ndiscrete features. The idea is to use nonlinear potentials with continuous\nfeatures, implemented by neural networks (NNs), in the TRF framework. Neural\nTRFs combine the advantages of both NNs and TRFs. The benefits of word\nembedding, nonlinear feature learning and larger context modeling are inherited\nfrom the use of NNs. At the same time, the strength of efficient inference by\navoiding expensive softmax is preserved. A number of technical contributions,\nincluding employing deep convolutional neural networks (CNNs) to define the\npotentials and incorporating the joint stochastic approximation (JSA) strategy\nin the training algorithm, are developed in this work, which enable us to\nsuccessfully train neural TRF LMs. Various LMs are evaluated in terms of speech\nrecognition WERs by rescoring the 1000-best lists of WSJ'92 test data. The\nresults show that neural TRF LMs not only improve over discrete TRF LMs, but\nalso perform slightly better than LSTM LMs with only one fifth of parameters\nand 16x faster inference efficiency.", "published": "2017-07-23 03:06:47", "link": "http://arxiv.org/abs/1707.07240v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
