{"title": "Better Language Model with Hypernym Class Prediction", "abstract": "Class-based language models (LMs) have been long devised to address context\nsparsity in $n$-gram LMs. In this study, we revisit this approach in the\ncontext of neural LMs. We hypothesize that class-based prediction leads to an\nimplicit context aggregation for similar words and thus can improve\ngeneralization for rare words. We map words that have a common WordNet hypernym\nto the same class and train large neural LMs by gradually annealing from\npredicting the class to token prediction during training. Empirically, this\ncurriculum learning strategy consistently improves perplexity over various\nlarge, highly-performant state-of-the-art Transformer-based models on two\ndatasets, WikiText-103 and Arxiv. Our analysis shows that the performance\nimprovement is achieved without sacrificing performance on rare words. Finally,\nwe document other attempts that failed to yield empirical gains, and discuss\nfuture directions for the adoption of class-based LMs on a larger scale.", "published": "2022-03-21 01:16:44", "link": "http://arxiv.org/abs/2203.10692v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Prompting-based Approach for Adversarial Example Generation and\n  Robustness Enhancement", "abstract": "Recent years have seen the wide application of NLP models in crucial areas\nsuch as finance, medical treatment, and news media, raising concerns of the\nmodel robustness and vulnerabilities. In this paper, we propose a novel\nprompt-based adversarial attack to compromise NLP models and robustness\nenhancement technique. We first construct malicious prompts for each instance\nand generate adversarial examples via mask-and-filling under the effect of a\nmalicious purpose. Our attack technique targets the inherent vulnerabilities of\nNLP models, allowing us to generate samples even without interacting with the\nvictim NLP model, as long as it is based on pre-trained language models (PLMs).\nFurthermore, we design a prompt-based adversarial training method to improve\nthe robustness of PLMs. As our training method does not actually generate\nadversarial samples, it can be applied to large-scale training sets\nefficiently. The experimental results show that our attack method can achieve a\nhigh attack success rate with more diverse, fluent and natural adversarial\nexamples. In addition, our robustness enhancement method can significantly\nimprove the robustness of models to resist adversarial attacks. Our work\nindicates that prompting paradigm has great potential in probing some\nfundamental flaws of PLMs and fine-tuning them for downstream tasks.", "published": "2022-03-21 03:21:32", "link": "http://arxiv.org/abs/2203.10714v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long\n  Document Summarization", "abstract": "Document structure is critical for efficient information consumption.\nHowever, it is challenging to encode it efficiently into the modern Transformer\narchitecture. In this work, we present HIBRIDS, which injects Hierarchical\nBiases foR Incorporating Document Structure into the calculation of attention\nscores. We further present a new task, hierarchical question-summary\ngeneration, for summarizing salient content in the source document into a\nhierarchy of questions and summaries, where each follow-up question inquires\nabout the content of its parent question-summary pair. We also annotate a new\ndataset with 6,153 question-summary hierarchies labeled on long government\nreports. Experiment results show that our model produces better\nquestion-summary hierarchies than comparisons on both hierarchy quality and\ncontent coverage, a finding also echoed by human judges. Additionally, our\nmodel improves the generation of long-form summaries from lengthy government\nreports and Wikipedia articles, as measured by ROUGE scores.", "published": "2022-03-21 05:27:35", "link": "http://arxiv.org/abs/2203.10741v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XTREME-S: Evaluating Cross-lingual Speech Representations", "abstract": "We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual\nspeech representations in many languages. XTREME-S covers four task families:\nspeech recognition, classification, speech-to-text translation and retrieval.\nCovering 102 languages from 10+ language families, 3 different domains and 4\ntask families, XTREME-S aims to simplify multilingual speech representation\nevaluation, as well as catalyze research in \"universal\" speech representation\nlearning. This paper describes the new benchmark and establishes the first\nspeech-only and speech-text baselines using XLS-R and mSLAM on all downstream\ntasks. We motivate the design choices and detail how to use the benchmark.\nDatasets and fine-tuning scripts are made easily accessible at\nhttps://hf.co/datasets/google/xtreme_s.", "published": "2022-03-21 06:50:21", "link": "http://arxiv.org/abs/2203.10752v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Match the Script, Adapt if Multilingual: Analyzing the Effect of\n  Multilingual Pretraining on Cross-lingual Transferability", "abstract": "Pretrained multilingual models enable zero-shot learning even for unseen\nlanguages, and that performance can be further improved via adaptation prior to\nfinetuning. However, it is unclear how the number of pretraining languages\ninfluences a model's zero-shot learning for languages unseen during\npretraining. To fill this gap, we ask the following research questions: (1) How\ndoes the number of pretraining languages influence zero-shot performance on\nunseen target languages? (2) Does the answer to that question change with model\nadaptation? (3) Do the findings for our first question change if the languages\nused for pretraining are all related? Our experiments on pretraining with\nrelated languages indicate that choosing a diverse set of languages is crucial.\nWithout model adaptation, surprisingly, increasing the number of pretraining\nlanguages yields better results up to adding related languages, after which\nperformance plateaus. In contrast, with model adaptation via continued\npretraining, pretraining on a larger number of languages often gives further\nimprovement, suggesting that model adaptation is crucial to exploit additional\npretraining languages.", "published": "2022-03-21 06:52:38", "link": "http://arxiv.org/abs/2203.10753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Token Graph Modeling using a Novel Labeling Strategy for\n  Structured Sentiment Analysis", "abstract": "The state-of-the-art model for structured sentiment analysis casts the task\nas a dependency parsing problem, which has some limitations: (1) The label\nproportions for span prediction and span relation prediction are imbalanced.\n(2) The span lengths of sentiment tuple components may be very large in this\ntask, which will further exacerbate the imbalance problem. (3) Two nodes in a\ndependency graph cannot have multiple arcs, therefore some overlapped sentiment\ntuples cannot be recognized. In this work, we propose nichetargeting solutions\nfor these issues. First, we introduce a novel labeling strategy, which contains\ntwo sets of token pair labels, namely essential label set and whole label set.\nThe essential label set consists of the basic labels for this task, which are\nrelatively balanced and applied in the prediction layer. The whole label set\nincludes rich labels to help our model capture various token relations, which\nare applied in the hidden layer to softly influence our model. Moreover, we\nalso propose an effective model to well collaborate with our labeling strategy,\nwhich is equipped with the graph attention networks to iteratively refine token\nrepresentations, and the adaptive multi-label classifier to dynamically predict\nmultiple relations between token pairs. We perform extensive experiments on 5\nbenchmark datasets in four languages. Experimental results show that our model\noutperforms previous SOTA models by a large margin.", "published": "2022-03-21 08:23:03", "link": "http://arxiv.org/abs/2203.10796v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Token Segmentation for High Token-Internal Complexity", "abstract": "Tokenizing raw texts into word units is an essential pre-processing step for\ncritical tasks in the NLP pipeline such as tagging, parsing, named entity\nrecognition, and more. For most languages, this tokenization step\nstraightforward. However, for languages with high token-internal complexity,\nfurther token-to-word segmentation is required. Previous canonical segmentation\nstudies were based on character-level frameworks, with no contextualised\nrepresentation involved. Contextualized vectors a la BERT show remarkable\nresults in many applications, but were not shown to improve performance on\nlinguistic segmentation per se. Here we propose a novel neural segmentation\nmodel which combines the best of both worlds, contextualised token\nrepresentation and char-level decoding, which is particularly effective for\nlanguages with high token-internal complexity and extreme morphological\nambiguity. Our model shows substantial improvements in segmentation accuracy on\nHebrew and Arabic compared to the state-of-the-art, and leads to further\nimprovements on downstream tasks such as Part-of-Speech Tagging, Dependency\nParsing and Named-Entity Recognition, over existing pipelines. When comparing\nour segmentation-first pipeline with joint segmentation and labeling in the\nsame settings, we show that, contrary to pre-neural studies, the pipeline\nperformance is superior.", "published": "2022-03-21 10:07:17", "link": "http://arxiv.org/abs/2203.10845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Paraphrasing Techniques for Maritime QA system", "abstract": "There has been an increasing interest in incorporating Artificial\nIntelligence (AI) into Defence and military systems to complement and augment\nhuman intelligence and capabilities. However, much work still needs to be done\ntoward achieving an effective human-machine partnership. This work is aimed at\nenhancing human-machine communications by developing a capability for\nautomatically translating human natural language into a machine-understandable\nlanguage (e.g., SQL queries). Techniques toward achieving this goal typically\ninvolve building a semantic parser trained on a very large amount of\nhigh-quality manually-annotated data. However, in many real-world Defence\nscenarios, it is not feasible to obtain such a large amount of training data.\nTo the best of our knowledge, there are few works trying to explore the\npossibility of training a semantic parser with limited manually-paraphrased\ndata, in other words, zero-shot. In this paper, we investigate how to exploit\nparaphrasing methods for the automated generation of large-scale training\ndatasets (in the form of paraphrased utterances and their corresponding logical\nforms in SQL format) and present our experimental results using real-world data\nin the maritime domain.", "published": "2022-03-21 10:20:30", "link": "http://arxiv.org/abs/2203.10854v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-Level Relation Extraction with Adaptive Focal Loss and\n  Knowledge Distillation", "abstract": "Document-level Relation Extraction (DocRE) is a more challenging task\ncompared to its sentence-level counterpart. It aims to extract relations from\nmultiple sentences at once. In this paper, we propose a semi-supervised\nframework for DocRE with three novel components. Firstly, we use an axial\nattention module for learning the interdependency among entity-pairs, which\nimproves the performance on two-hop relations. Secondly, we propose an adaptive\nfocal loss to tackle the class imbalance problem of DocRE. Lastly, we use\nknowledge distillation to overcome the differences between human annotated data\nand distantly supervised data. We conducted experiments on two DocRE datasets.\nOur model consistently outperforms strong baselines and its performance exceeds\nthe previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard.\nOur code and data will be released at https://github.com/tonytan48/KD-DocRE.", "published": "2022-03-21 11:48:40", "link": "http://arxiv.org/abs/2203.10900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "x-enVENT: A Corpus of Event Descriptions with Experiencer-specific\n  Emotion and Appraisal Annotations", "abstract": "Emotion classification is often formulated as the task to categorize texts\ninto a predefined set of emotion classes. So far, this task has been the\nrecognition of the emotion of writers and readers, as well as that of entities\nmentioned in the text. We argue that a classification setup for emotion\nanalysis should be performed in an integrated manner, including the different\nsemantic roles that participate in an emotion episode. Based on appraisal\ntheories in psychology, which treat emotions as reactions to events, we compile\nan English corpus of written event descriptions. The descriptions depict\nemotion-eliciting circumstances, and they contain mentions of people who\nresponded emotionally. We annotate all experiencers, including the original\nauthor, with the emotions they likely felt. In addition, we link them to the\nevent they found salient (which can be different for different experiencers in\na text) by annotating event properties, or appraisals (e.g., the perceived\nevent undesirability, the uncertainty of its outcome). Our analysis reveals\npatterns in the co-occurrence of people's emotions in interaction. Hence, this\nrichly-annotated resource provides useful data to study emotions and event\nevaluations from the perspective of different roles, and it enables the\ndevelopment of experiencer-specific emotion and appraisal classification\nsystems.", "published": "2022-03-21 12:02:06", "link": "http://arxiv.org/abs/2203.10909v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "General and Domain Adaptive Chinese Spelling Check with Error Consistent\n  Pretraining", "abstract": "The lack of label data is one of the significant bottlenecks for Chinese\nSpelling Check (CSC). Existing researches use the method of automatic\ngeneration by exploiting unlabeled data to expand the supervised corpus.\nHowever, there is a big gap between the real input scenario and automatic\ngenerated corpus. Thus, we develop a competitive general speller ECSpell which\nadopts the Error Consistent masking strategy to create data for pretraining.\nThis error consistency masking strategy is used to specify the error types of\nautomatically generated sentences which is consistent with real scene. The\nexperimental result indicates our model outperforms previous state-of-the-art\nmodels on the general benchmark. Moreover, spellers often work within a\nparticular domain in real life. Due to lots of uncommon domain terms,\nexperiments on our built domain specific datasets show that general models\nperform terribly. Inspired by the common practice of input methods, we propose\nto add an alterable user dictionary to handle the zero-shot domain adaption\nproblem. Specifically, we attach a User Dictionary guided inference module (UD)\nto a general token classification based speller. Our experiments demonstrate\nthat ECSpell$^{UD}$, namely ECSpell combined with UD, surpasses all the other\nbaselines largely, even approaching the performance on the general benchmark.", "published": "2022-03-21 12:49:44", "link": "http://arxiv.org/abs/2203.10929v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quality Controlled Paraphrase Generation", "abstract": "Paraphrase generation has been widely used in various downstream tasks. Most\ntasks benefit mainly from high quality paraphrases, namely those that are\nsemantically similar to, yet linguistically diverse from, the original\nsentence. Generating high-quality paraphrases is challenging as it becomes\nincreasingly hard to preserve meaning as linguistic diversity increases. Recent\nworks achieve nice results by controlling specific aspects of the paraphrase,\nsuch as its syntactic tree. However, they do not allow to directly control the\nquality of the generated paraphrase, and suffer from low flexibility and\nscalability. Here we propose $QCPG$, a quality-guided controlled paraphrase\ngeneration model, that allows directly controlling the quality dimensions.\nFurthermore, we suggest a method that given a sentence, identifies points in\nthe quality control space that are expected to yield optimal generated\nparaphrases. We show that our method is able to generate paraphrases which\nmaintain the original meaning while achieving higher diversity than the\nuncontrolled baseline. The models, the code, and the data can be found in\nhttps://github.com/IBM/quality-controlled-paraphrase-generation.", "published": "2022-03-21 13:09:59", "link": "http://arxiv.org/abs/2203.10940v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraBART: a Pretrained Arabic Sequence-to-Sequence Model for Abstractive\n  Summarization", "abstract": "Like most natural language understanding and generation tasks,\nstate-of-the-art models for summarization are transformer-based\nsequence-to-sequence architectures that are pretrained on large corpora. While\nmost existing models focused on English, Arabic remained understudied. In this\npaper we propose AraBART, the first Arabic model in which the encoder and the\ndecoder are pretrained end-to-end, based on BART. We show that AraBART achieves\nthe best performance on multiple abstractive summarization datasets,\noutperforming strong baselines including a pretrained Arabic BERT-based model\nand multilingual mBART and mT5 models.", "published": "2022-03-21 13:11:41", "link": "http://arxiv.org/abs/2203.10945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Order Does Matter (And Shuffled Language Models Know It)", "abstract": "Recent studies have shown that language models pretrained and/or fine-tuned\non randomly permuted sentences exhibit competitive performance on GLUE, putting\ninto question the importance of word order information. Somewhat\ncounter-intuitively, some of these studies also report that position embeddings\nappear to be crucial for models' good performance with shuffled text. We probe\nthese language models for word order information and investigate what position\nembeddings learned from shuffled text encode, showing that these models retain\ninformation pertaining to the original, naturalistic word order. We show this\nis in part due to a subtlety in how shuffling is implemented in previous work\n-- before rather than after subword segmentation. Surprisingly, we find even\nLanguage models trained on text shuffled after subword segmentation retain some\nsemblance of information about word order because of the statistical\ndependencies between sentence length and unigram probabilities. Finally, we\nshow that beyond GLUE, a variety of language understanding tasks do require\nword order information, often to an extent that cannot be learned through\nfine-tuning.", "published": "2022-03-21 14:10:15", "link": "http://arxiv.org/abs/2203.10995v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Do We Answer Complex Questions: Discourse Structure of Long-form\n  Answers", "abstract": "Long-form answers, consisting of multiple sentences, can provide nuanced and\ncomprehensive answers to a broader set of questions. To better understand this\ncomplex and understudied task, we study the functional structure of long-form\nanswers collected from three datasets, ELI5, WebGPT and Natural Questions. Our\nmain goal is to understand how humans organize information to craft complex\nanswers. We develop an ontology of six sentence-level functional roles for\nlong-form answers, and annotate 3.9k sentences in 640 answer paragraphs.\nDifferent answer collection methods manifest in different discourse structures.\nWe further analyze model-generated answers -- finding that annotators agree\nless with each other when annotating model-generated answers compared to\nannotating human-written answers. Our annotated data enables training a strong\nclassifier that can be used for automatic analysis. We hope our work can\ninspire future research on discourse-level modeling and evaluation of long-form\nQA systems.", "published": "2022-03-21 15:14:10", "link": "http://arxiv.org/abs/2203.11048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Relevant CommonSense Subgraphs for \"What if...\" Procedural Reasoning", "abstract": "We study the challenge of learning causal reasoning over procedural text to\nanswer \"What if...\" questions when external commonsense knowledge is required.\nWe propose a novel multi-hop graph reasoning model to 1) efficiently extract a\ncommonsense subgraph with the most relevant information from a large knowledge\ngraph; 2) predict the causal answer by reasoning over the representations\nobtained from the commonsense subgraph and the contextual interactions between\nthe questions and context. We evaluate our model on WIQA benchmark and achieve\nstate-of-the-art performance compared to the recent models.", "published": "2022-03-21 17:57:58", "link": "http://arxiv.org/abs/2203.11187v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and\n  Quantization", "abstract": "Large-scale pre-trained sequence-to-sequence models like BART and T5 achieve\nstate-of-the-art performance on many generative NLP tasks. However, such models\npose a great challenge in resource-constrained scenarios owing to their large\nmemory requirements and high latency. To alleviate this issue, we propose to\njointly distill and quantize the model, where knowledge is transferred from the\nfull-precision teacher model to the quantized and distilled low-precision\nstudent model. Empirical analyses show that, despite the challenging nature of\ngenerative tasks, we were able to achieve a 16.5x model footprint compression\nratio with little performance drop relative to the full-precision counterparts\non multiple summarization and QA datasets. We further pushed the limit of\ncompression ratio to 27.7x and presented the performance-efficiency trade-off\nfor generative tasks using pre-trained models. To the best of our knowledge,\nthis is the first work aiming to effectively distill and quantize\nsequence-to-sequence pre-trained models for language generation tasks.", "published": "2022-03-21 18:04:25", "link": "http://arxiv.org/abs/2203.11239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Classification of Long Documents Using Transformers", "abstract": "Several methods have been proposed for classifying long textual documents\nusing Transformers. However, there is a lack of consensus on a benchmark to\nenable a fair comparison among different approaches. In this paper, we provide\na comprehensive evaluation of the relative efficacy measured against various\nbaselines and diverse datasets -- both in terms of accuracy as well as time and\nspace overheads. Our datasets cover binary, multi-class, and multi-label\nclassification tasks and represent various ways information is organized in a\nlong text (e.g. information that is critical to making the classification\ndecision is at the beginning or towards the end of the document). Our results\nshow that more complex models often fail to outperform simple baselines and\nyield inconsistent performance across datasets. These findings emphasize the\nneed for future studies to consider comprehensive baselines and datasets that\nbetter represent the task of long document classification to develop robust\nmodels.", "published": "2022-03-21 18:36:18", "link": "http://arxiv.org/abs/2203.11258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hate Speech Classification Using SVM and Naive BAYES", "abstract": "The spread of hatred that was formerly limited to verbal communications has\nrapidly moved over the Internet. Social media and community forums that allow\npeople to discuss and express their opinions are becoming platforms for the\nspreading of hate messages. Many countries have developed laws to avoid online\nhate speech. They hold the companies that run the social media responsible for\ntheir failure to eliminate hate speech. But as online content continues to\ngrow, so does the spread of hate speech However, manual analysis of hate speech\non online platforms is infeasible due to the huge amount of data as it is\nexpensive and time consuming. Thus, it is important to automatically process\nthe online user contents to detect and remove hate speech from online media.\nMany recent approaches suffer from interpretability problem which means that it\ncan be difficult to understand why the systems make the decisions they do.\nThrough this work, some solutions for the problem of automatic detection of\nhate messages were proposed using Support Vector Machine (SVM) and Na\\\"ive\nBayes algorithms. This achieved near state-of-the-art performance while being\nsimpler and producing more easily interpretable decisions than other methods.\nEmpirical evaluation of this technique has resulted in a classification\naccuracy of approximately 99% and 50% for SVM and NB respectively over the test\nset.\n  Keywords: classification; hate speech; feature extraction, algorithm,\nsupervised learning", "published": "2022-03-21 17:15:38", "link": "http://arxiv.org/abs/2204.07057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Expert Guided Adversarial Augmentation For Improving\n  Generalization in Named Entity Recognition", "abstract": "Named Entity Recognition (NER) systems often demonstrate great performance on\nin-distribution data, but perform poorly on examples drawn from a shifted\ndistribution. One way to evaluate the generalization ability of NER models is\nto use adversarial examples, on which the specific variations associated with\nnamed entities are rarely considered. To this end, we propose leveraging\nexpert-guided heuristics to change the entity tokens and their surrounding\ncontexts thereby altering their entity types as adversarial attacks. Using\nexpert-guided heuristics, we augmented the CoNLL 2003 test set and manually\nannotated it to construct a high-quality challenging set. We found that\nstate-of-the-art NER systems trained on CoNLL 2003 training data drop\nperformance dramatically on our challenging set. By training on adversarial\naugmented training examples and using mixup for regularization, we were able to\nsignificantly improve the performance on the challenging set as well as improve\nout-of-domain generalization which we evaluated by using OntoNotes data. We\nhave publicly released our dataset and code at\nhttps://github.com/GT-SALT/Guided-Adversarial-Augmentation.", "published": "2022-03-21 01:21:12", "link": "http://arxiv.org/abs/2203.10693v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Compression of Generative Pre-trained Language Models via Quantization", "abstract": "The increasing size of generative Pre-trained Language Models (PLMs) has\ngreatly increased the demand for model compression. Despite various methods to\ncompress BERT or its variants, there are few attempts to compress generative\nPLMs, and the underlying difficulty remains unclear. In this paper, we compress\ngenerative PLMs by quantization. We find that previous quantization methods\nfail on generative tasks due to the \\textit{homogeneous word embeddings} caused\nby reduced capacity, and \\textit{varied distribution of weights}.\nCorrespondingly, we propose a token-level contrastive distillation to learn\ndistinguishable word embeddings, and a module-wise dynamic scaling to make\nquantizers adaptive to different modules. Empirical results on various tasks\nshow that our proposed method outperforms the state-of-the-art compression\nmethods on generative PLMs by a clear margin. With comparable performance with\nthe full-precision models, we achieve 14.4x and 13.4x compression rates on\nGPT-2 and BART, respectively.", "published": "2022-03-21 02:11:35", "link": "http://arxiv.org/abs/2203.10705v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "An Intellectual Property Entity Recognition Method Based on Transformer\n  and Technological Word Information", "abstract": "Patent texts contain a large amount of entity information. Through named\nentity recognition, intellectual property entity information containing key\ninformation can be extracted from it, helping researchers to understand the\npatent content faster. Therefore, it is difficult for existing named entity\nextraction methods to make full use of the semantic information at the word\nlevel brought about by professional vocabulary changes. This paper proposes a\nmethod for extracting intellectual property entities based on Transformer and\ntechnical word information , and provides accurate word vector representation\nin combination with the BERT language method. In the process of word vector\ngeneration, the technical word information extracted by IDCNN is added to\nimprove the understanding of intellectual property entities Representation\nability. Finally, the Transformer encoder that introduces relative position\nencoding is used to learn the deep semantic information of the text from the\nsequence of word vectors, and realize entity label prediction. Experimental\nresults on public datasets and annotated patent datasets show that the method\nimproves the accuracy of entity recognition.", "published": "2022-03-21 03:28:37", "link": "http://arxiv.org/abs/2203.10717v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Academic Resource Text Level Multi-label Classification based on\n  Attention", "abstract": "Hierarchical multi-label academic text classification (HMTC) is to assign\nacademic texts into a hierarchically structured labeling system. We propose an\nattention-based hierarchical multi-label classification algorithm of academic\ntexts (AHMCA) by integrating features such as text, keywords, and hierarchical\nstructure, the academic documents are classified into the most relevant\ncategories. We utilize word2vec and BiLSTM to obtain embedding and latent\nvector representations of text, keywords, and hierarchies. We use hierarchical\nattention mechanism to capture the associations between keywords, label\nhierarchies, and text word vectors to generate hierarchical-specific document\nembedding vectors to replace the original text embeddings in HMCN-F. The\nexperimental results on the academic text dataset demonstrate the effectiveness\nof the AHMCA algorithm.", "published": "2022-03-21 05:32:35", "link": "http://arxiv.org/abs/2203.10743v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Programming Language Agnostic Mining of Code and Language Pairs with\n  Sequence Labeling Based Question Answering", "abstract": "Mining aligned natural language (NL) and programming language (PL) pairs is a\ncritical task to NL-PL understanding. Existing methods applied specialized\nhand-crafted features or separately-trained models for each PL. However, they\nusually suffered from low transferability across multiple PLs, especially for\nniche PLs with less annotated data. Fortunately, a Stack Overflow answer post\nis essentially a sequence of text and code blocks and its global textual\ncontext can provide PL-agnostic supplementary information. In this paper, we\npropose a Sequence Labeling based Question Answering (SLQA) method to mine\nNL-PL pairs in a PL-agnostic manner. In particular, we propose to apply the BIO\ntagging scheme instead of the conventional binary scheme to mine the code\nsolutions which are often composed of multiple blocks of a post. Experiments on\ncurrent single-PL single-block benchmarks and a manually-labeled cross-PL\nmulti-block benchmark prove the effectiveness and transferability of SLQA. We\nfurther present a parallel NL-PL corpus named Lang2Code automatically mined\nwith SLQA, which contains about 1.4M pairs on 6 PLs. Under statistical analysis\nand downstream evaluation, we demonstrate that Lang2Code is a large-scale\nhigh-quality data resource for further NL-PL research.", "published": "2022-03-21 05:33:59", "link": "http://arxiv.org/abs/2203.10744v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Transformer-based HTR for Historical Documents", "abstract": "We apply the TrOCR framework to real-world, historical manuscripts and show\nthat TrOCR per se is a strong model, ideal for transfer learning. TrOCR has\nbeen trained on English only, but it can adapt to other languages that use the\nLatin alphabet fairly easily and with little training material. We compare\nTrOCR against a SOTA HTR framework (Transkribus) and show that it can beat such\nsystems. This finding is essential since Transkribus performs best when it has\naccess to baseline information, which is not needed at all to fine-tune TrOCR.", "published": "2022-03-21 14:23:10", "link": "http://arxiv.org/abs/2203.11008v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Targeted Extraction of Temporal Facts from Textual Resources for\n  Improved Temporal Question Answering over Knowledge Bases", "abstract": "Knowledge Base Question Answering (KBQA) systems have the goal of answering\ncomplex natural language questions by reasoning over relevant facts retrieved\nfrom Knowledge Bases (KB). One of the major challenges faced by these systems\nis their inability to retrieve all relevant facts due to factors such as\nincomplete KB and entity/relation linking errors. In this paper, we address\nthis particular challenge for systems handling a specific category of questions\ncalled temporal questions, where answer derivation involve reasoning over facts\nasserting point/intervals of time for various events. We propose a novel\napproach where a targeted temporal fact extraction technique is used to assist\nKBQA whenever it fails to retrieve temporal facts from the KB. We use\n$\\lambda$-expressions of the questions to logically represent the component\nfacts and the reasoning steps needed to derive the answer. This allows us to\nspot those facts that failed to get retrieved from the KB and generate textual\nqueries to extract them from the textual resources in an open-domain question\nanswering fashion. We evaluated our approach on a benchmark temporal question\nanswering dataset considering Wikidata and Wikipedia respectively as the KB and\ntextual resource. Experimental results show a significant $\\sim$30\\% relative\nimprovement in answer accuracy, demonstrating the effectiveness of our\napproach.", "published": "2022-03-21 15:26:35", "link": "http://arxiv.org/abs/2203.11054v1", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "Automated Clinical Coding: What, Why, and Where We Are?", "abstract": "Clinical coding is the task of transforming medical information in a\npatient's health records into structured codes so that they can be used for\nstatistical analysis. This is a cognitive and time-consuming task that follows\na standard process in order to achieve a high level of consistency. Clinical\ncoding could potentially be supported by an automated system to improve the\nefficiency and accuracy of the process. We introduce the idea of automated\nclinical coding and summarise its challenges from the perspective of Artificial\nIntelligence (AI) and Natural Language Processing (NLP), based on the\nliterature, our project experience over the past two and half years (late 2019\n- early 2022), and discussions with clinical coding experts in Scotland and the\nUK. Our research reveals the gaps between the current deep learning-based\napproach applied to clinical coding and the need for explainability and\nconsistency in real-world practice. Knowledge-based methods that represent and\nreason the standard, explainable process of a task may need to be incorporated\ninto deep learning-based methods for clinical coding. Automated clinical coding\nis a promising task for AI, despite the technical and organisational\nchallenges. Coders are needed to be involved in the development process. There\nis much to achieve to develop and deploy an AI-based automated system to\nsupport coding in the next five years and beyond.", "published": "2022-03-21 16:17:38", "link": "http://arxiv.org/abs/2203.11092v3", "categories": ["cs.CL", "cs.AI", "68T07 (Primary), 68T50 (Secondary)", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Towards Explainable Evaluation Metrics for Natural Language Generation", "abstract": "Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics (such as BERTScore or MoverScore) are based on black-box\nlanguage models such as BERT or XLM-R. They often achieve strong correlations\nwith human judgments, but recent research indicates that the lower-quality\nclassical metrics remain dominant, one of the potential reasons being that\ntheir decision processes are transparent. To foster more widespread acceptance\nof the novel high-quality metrics, explainability thus becomes crucial. In this\nconcept paper, we identify key properties and propose key goals of explainable\nmachine translation evaluation metrics. We also provide a synthesizing overview\nover recent approaches for explainable machine translation metrics and discuss\nhow they relate to those goals and properties. Further, we conduct own novel\nexperiments, which (among others) find that current adversarial NLP techniques\nare unsuitable for automatically identifying limitations of high-quality\nblack-box evaluation metrics, as they are not meaning-preserving. Finally, we\nprovide a vision of future approaches to explainable evaluation metrics and\ntheir evaluation. We hope that our work can help catalyze and guide future\nresearch on explainable evaluation metrics and, mediately, also contribute to\nbetter and more transparent text generation systems.", "published": "2022-03-21 17:05:54", "link": "http://arxiv.org/abs/2203.11131v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Teaching language models to support answers with verified quotes", "abstract": "Recent large language models often answer factual questions correctly. But\nusers can't trust any given claim a model makes without fact-checking, because\nlanguage models can hallucinate convincing nonsense. In this work we use\nreinforcement learning from human preferences (RLHP) to train \"open-book\" QA\nmodels that generate answers whilst also citing specific evidence for their\nclaims, which aids in the appraisal of correctness. Supporting evidence is\ndrawn from multiple documents found via a search engine, or from a single\nuser-provided document. Our 280 billion parameter model, GopherCite, is able to\nproduce answers with high quality supporting evidence and abstain from\nanswering when unsure. We measure the performance of GopherCite by conducting\nhuman evaluation of answers to questions in a subset of the NaturalQuestions\nand ELI5 datasets. The model's response is found to be high-quality 80\\% of the\ntime on this Natural Questions subset, and 67\\% of the time on the ELI5 subset.\nAbstaining from the third of questions for which it is most unsure improves\nperformance to 90\\% and 80\\% respectively, approaching human baselines.\nHowever, analysis on the adversarial TruthfulQA dataset shows why citation is\nonly one part of an overall strategy for safety and trustworthiness: not all\nclaims supported by evidence are true.", "published": "2022-03-21 17:26:29", "link": "http://arxiv.org/abs/2203.11147v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "abstract": "Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).", "published": "2022-03-21 17:48:52", "link": "http://arxiv.org/abs/2203.11171v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Change that Matters in Discourse Parsing: Estimating the Impact of\n  Domain Shift on Parser Error", "abstract": "Discourse analysis allows us to attain inferences of a text document that\nextend beyond the sentence-level. The current performance of discourse models\nis very low on texts outside of the training distribution's coverage,\ndiminishing the practical utility of existing models. There is need for a\nmeasure that can inform us to what extent our model generalizes from the\ntraining to the test sample when these samples may be drawn from distinct\ndistributions. While this can be estimated via distribution shift, we argue\nthat this does not directly correlate with change in the observed error of a\nclassifier (i.e. error-gap). Thus, we propose to use a statistic from the\ntheoretical domain adaptation literature which can be directly tied to\nerror-gap. We study the bias of this statistic as an estimator of error-gap\nboth theoretically and through a large-scale empirical study of over 2400\nexperiments on 6 discourse datasets from domains including, but not limited to:\nnews, biomedical texts, TED talks, Reddit posts, and fiction. Our results not\nonly motivate our proposal and help us to understand its limitations, but also\nprovide insight on the properties of discourse models and datasets which\nimprove performance in domain adaptation. For instance, we find that non-news\ndatasets are slightly easier to transfer to than news datasets when the\ntraining and test sets are very different. Our code and an associated Python\npackage are available to allow practitioners to make more informed model and\ndataset choices.", "published": "2022-03-21 20:04:23", "link": "http://arxiv.org/abs/2203.11317v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On The Robustness of Offensive Language Classifiers", "abstract": "Social media platforms are deploying machine learning based offensive\nlanguage classification systems to combat hateful, racist, and other forms of\noffensive speech at scale. However, despite their real-world deployment, we do\nnot yet comprehensively understand the extent to which offensive language\nclassifiers are robust against adversarial attacks. Prior work in this space is\nlimited to studying robustness of offensive language classifiers against\nprimitive attacks such as misspellings and extraneous spaces. To address this\ngap, we systematically analyze the robustness of state-of-the-art offensive\nlanguage classifiers against more crafty adversarial attacks that leverage\ngreedy- and attention-based word selection and context-aware embeddings for\nword replacement. Our results on multiple datasets show that these crafty\nadversarial attacks can degrade the accuracy of offensive language classifiers\nby more than 50% while also being able to preserve the readability and meaning\nof the modified text.", "published": "2022-03-21 20:44:30", "link": "http://arxiv.org/abs/2203.11331v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Information-theoretic Approach to Prompt Engineering Without Ground\n  Truth Labels", "abstract": "Pre-trained language models derive substantial linguistic and factual\nknowledge from the massive corpora on which they are trained, and prompt\nengineering seeks to align these models to specific tasks. Unfortunately,\nexisting prompt engineering methods require significant amounts of labeled\ndata, access to model parameters, or both. We introduce a new method for\nselecting prompt templates \\textit{without labeled examples} and\n\\textit{without direct access to the model}. Specifically, over a set of\ncandidate templates, we choose the template that maximizes the mutual\ninformation between the input and the corresponding model output. Across 8\ndatasets representing 7 distinct NLP tasks, we show that when a template has\nhigh mutual information, it also has high accuracy on the task. On the largest\nmodel, selecting prompts with our method gets 90\\% of the way from the average\nprompt accuracy to the best prompt accuracy and requires no ground truth\nlabels.", "published": "2022-03-21 21:51:43", "link": "http://arxiv.org/abs/2203.11364v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language modeling via stochastic processes", "abstract": "Modern language models can generate high-quality short texts. However, they\noften meander or are incoherent when generating longer texts. These issues\narise from the next-token-only language modeling objective. Recent work in\nself-supervised learning suggests that models can learn good latent\nrepresentations via contrastive learning, which can be effective for\ndiscriminative tasks. Our work analyzes the application of contrastive\nrepresentations for generative tasks, like long text generation. We propose one\napproach for leveraging constrastive representations, which we call Time\nControl (TC). TC first learns a contrastive representation of the target text\ndomain, then generates text by decoding from these representations. Compared to\ndomain-specific methods and fine-tuning GPT2 across a variety of text domains,\nTC performs competitively to methods specific for learning sentence\nrepresentations on discourse coherence. On long text generation settings, TC\npreserves the text structure both in terms of ordering (up to $+15\\%$ better)\nand text length consistency (up to $+90\\%$ better).", "published": "2022-03-21 22:13:53", "link": "http://arxiv.org/abs/2203.11370v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WeSinger: Data-augmented Singing Voice Synthesis with Auxiliary Losses", "abstract": "In this paper, we develop a new multi-singer Chinese neural singing voice\nsynthesis (SVS) system named WeSinger. To improve the accuracy and naturalness\nof synthesized singing voice, we design several specifical modules and\ntechniques: 1) A deep bi-directional LSTM-based duration model with multi-scale\nrhythm loss and post-processing step; 2) A Transformer-alike acoustic model\nwith progressive pitch-weighted decoder loss; 3) a 24 kHz pitch-aware LPCNet\nneural vocoder to produce high-quality singing waveforms; 4) A novel data\naugmentation method with multi-singer pre-training for stronger robustness and\nnaturalness. To our knowledge, WeSinger is the first SVS system to adopt 24 kHz\nLPCNet and multi-singer pre-training simultaneously. Both quantitative and\nqualitative evaluation results demonstrate the effectiveness of WeSinger in\nterms of accuracy and naturalness, and WeSinger achieves state-of-the-art\nperformance on the recent public Chinese singing corpus\nOpencpop\\footnote{https://wenet.org.cn/opencpop/}. Some synthesized singing\nsamples are available online\\footnote{https://zzw922cn.github.io/wesinger/}.", "published": "2022-03-21 06:42:44", "link": "http://arxiv.org/abs/2203.10750v5", "categories": ["cs.SD", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "A Slot Is Not Built in One Utterance: Spoken Language Dialogs with\n  Sub-Slots", "abstract": "A slot value might be provided segment by segment over multiple-turn\ninteractions in a dialog, especially for some important information such as\nphone numbers and names. It is a common phenomenon in daily life, but little\nattention has been paid to it in previous work. To fill the gap, this paper\ndefines a new task named Sub-Slot based Task-Oriented Dialog (SSTOD) and builds\na Chinese dialog dataset SSD for boosting research on SSTOD. The dataset\nincludes a total of 40K dialogs and 500K utterances from four different\ndomains: Chinese names, phone numbers, ID numbers and license plate numbers.\nThe data is well annotated with sub-slot values, slot values, dialog states and\nactions. We find some new linguistic phenomena and interactive manners in SSTOD\nwhich raise critical challenges of building dialog agents for the task. We test\nthree state-of-the-art dialog models on SSTOD and find they cannot handle the\ntask well on any of the four domains. We also investigate an improved model by\ninvolving slot knowledge in a plug-in manner. More work should be done to meet\nthe new challenges raised from SSTOD which widely exists in real-life\napplications. The dataset and code are publicly available via\nhttps://github.com/shunjiu/SSTOD.", "published": "2022-03-21 07:10:19", "link": "http://arxiv.org/abs/2203.10759v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TCM-SD: A Benchmark for Probing Syndrome Differentiation via Natural\n  Language Processing", "abstract": "Traditional Chinese Medicine (TCM) is a natural, safe, and effective therapy\nthat has spread and been applied worldwide. The unique TCM diagnosis and\ntreatment system requires a comprehensive analysis of a patient's symptoms\nhidden in the clinical record written in free text. Prior studies have shown\nthat this system can be informationized and intelligentized with the aid of\nartificial intelligence (AI) technology, such as natural language processing\n(NLP). However, existing datasets are not of sufficient quality nor quantity to\nsupport the further development of data-driven AI technology in TCM. Therefore,\nin this paper, we focus on the core task of the TCM diagnosis and treatment\nsystem -- syndrome differentiation (SD) -- and we introduce the first public\nlarge-scale dataset for SD, called TCM-SD. Our dataset contains 54,152\nreal-world clinical records covering 148 syndromes. Furthermore, we collect a\nlarge-scale unlabelled textual corpus in the field of TCM and propose a\ndomain-specific pre-trained language model, called ZY-BERT. We conducted\nexperiments using deep neural networks to establish a strong performance\nbaseline, reveal various challenges in SD, and prove the potential of\ndomain-specific pre-trained language model. Our study and analysis reveal\nopportunities for incorporating computer science and linguistics knowledge to\nexplore the empirical validity of TCM theories.", "published": "2022-03-21 09:59:54", "link": "http://arxiv.org/abs/2203.10839v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Zoom Out and Observe: News Environment Perception for Fake News\n  Detection", "abstract": "Fake news detection is crucial for preventing the dissemination of\nmisinformation on social media. To differentiate fake news from real ones,\nexisting methods observe the language patterns of the news post and \"zoom in\"\nto verify its content with knowledge sources or check its readers' replies.\nHowever, these methods neglect the information in the external news environment\nwhere a fake news post is created and disseminated. The news environment\nrepresents recent mainstream media opinion and public attention, which is an\nimportant inspiration of fake news fabrication because fake news is often\ndesigned to ride the wave of popular events and catch public attention with\nunexpected novel content for greater exposure and spread. To capture the\nenvironmental signals of news posts, we \"zoom out\" to observe the news\nenvironment and propose the News Environment Perception Framework (NEP). For\neach post, we construct its macro and micro news environment from recent\nmainstream news. Then we design a popularity-oriented and a novelty-oriented\nmodule to perceive useful signals and further assist final prediction.\nExperiments on our newly built datasets show that the NEP can efficiently\nimprove the performance of basic fake news detectors.", "published": "2022-03-21 11:10:46", "link": "http://arxiv.org/abs/2203.10885v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Enhancing Speech Recognition Decoding via Layer Aggregation", "abstract": "Recently proposed speech recognition systems are designed to predict using\nrepresentations generated by their top layers, employing greedy decoding which\nisolates each timestep from the rest of the sequence. Aiming for improved\nperformance, a beam search algorithm is frequently utilized and a language\nmodel is incorporated to assist with ranking the top candidates. In this work,\nwe experiment with several speech recognition models and find that logits\npredicted using the top layers may hamper beam search from achieving optimal\nresults. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield\nhighly confident predictions, and hypothesize that the predictions are based on\nlocal information and may not take full advantage of the information encoded in\nintermediate layers. To this end, we perform a layer analysis to reveal and\nvisualize how predictions evolve throughout the inference flow. We then propose\na prediction method that aggregates the top M layers, potentially leveraging\nuseful information encoded in intermediate layers and relaxing model\nconfidence. We showcase the effectiveness of our approach via beam search\ndecoding, conducting our experiments on Librispeech test and dev sets and\nachieving WER, and CER reduction of up to 10% and 22%, respectively.", "published": "2022-03-21 20:28:06", "link": "http://arxiv.org/abs/2203.11325v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PACS: A Dataset for Physical Audiovisual CommonSense Reasoning", "abstract": "In order for AI to be safely deployed in real-world scenarios such as\nhospitals, schools, and the workplace, it must be able to robustly reason about\nthe physical world. Fundamental to this reasoning is physical common sense:\nunderstanding the physical properties and affordances of available objects, how\nthey can be manipulated, and how they interact with other objects. Physical\ncommonsense reasoning is fundamentally a multi-sensory task, since physical\nproperties are manifested through multiple modalities - two of them being\nvision and acoustics. Our paper takes a step towards real-world physical\ncommonsense reasoning by contributing PACS: the first audiovisual benchmark\nannotated for physical commonsense attributes. PACS contains 13,400\nquestion-answer pairs, involving 1,377 unique physical commonsense questions\nand 1,526 videos. Our dataset provides new opportunities to advance the\nresearch field of physical reasoning by bringing audio as a core component of\nthis multimodal problem. Using PACS, we evaluate multiple state-of-the-art\nmodels on our new challenging task. While some models show promising results\n(70% accuracy), they all fall short of human performance (95% accuracy). We\nconclude the paper by demonstrating the importance of multimodal reasoning and\nproviding possible avenues for future research.", "published": "2022-03-21 17:05:23", "link": "http://arxiv.org/abs/2203.11130v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Separating Content from Speaker Identity in Speech for the Assessment of\n  Cognitive Impairments", "abstract": "Deep speaker embeddings have been shown effective for assessing cognitive\nimpairments aside from their original purpose of speaker verification. However,\nthe research found that speaker embeddings encode speaker identity and an array\nof information, including speaker demographics, such as sex and age, and speech\ncontents to an extent, which are known confounders in the assessment of\ncognitive impairments. In this paper, we hypothesize that content information\nseparated from speaker identity using a framework for voice conversion is more\neffective for assessing cognitive impairments and train simple classifiers for\nthe comparative analysis on the DementiaBank Pitt Corpus. Our results show that\nwhile content embeddings have an advantage over speaker embeddings for the\ndefined problem, further experiments show their effectiveness depends on\ninformation encoded in speaker embeddings due to the inherent design of the\narchitecture used for extracting contents.", "published": "2022-03-21 09:29:28", "link": "http://arxiv.org/abs/2203.10827v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Individualizing Head-Related Transfer Functions for Binaural Acoustic\n  Applications", "abstract": "A Head Related Transfer Function (HRTF) characterizes how a human ear\nreceives sounds from a point in space, and depends on the shapes of one's head,\npinna, and torso. Accurate estimations of HRTFs for human subjects are crucial\nin enabling binaural acoustic applications such as sound localization and 3D\nsound spatialization. Unfortunately, conventional approaches for HRTF\nestimation rely on specialized devices or lengthy measurement processes. This\nwork proposes a novel lightweight method for HRTF individualization that can be\nimplemented using commercial-off-the-shelf components and performed by average\nusers in home settings. The proposed method has two key components: a\ngenerative neural network model that can be individualized to predict HRTFs of\nnew subjects from sparse measurements, and a lightweight measurement procedure\nthat collects HRTF data from spatial locations. Extensive experiments using a\npublic dataset and in house measurement data from 10 subjects of different ages\nand genders, show that the individualized models significantly outperform a\nbaseline model in the accuracy of predicted HRTFs. To further demonstrate the\nadvantages of individualized HRTFs, we implement two prototype applications for\nbinaural localization and acoustic spatialization. We find that the performance\nof a localization model is improved by 15 degree after trained with\nindividualized HRTFs. Furthermore, in hearing tests, the success rate of\ncorrectly identifying the azimuth direction of incoming sounds increases by\n183% after individualization.", "published": "2022-03-21 17:13:55", "link": "http://arxiv.org/abs/2203.11138v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The VoiceMOS Challenge 2022", "abstract": "We present the first edition of the VoiceMOS Challenge, a scientific event\nthat aims to promote the study of automatic prediction of the mean opinion\nscore (MOS) of synthetic speech. This challenge drew 22 participating teams\nfrom academia and industry who tried a variety of approaches to tackle the\nproblem of predicting human ratings of synthesized speech. The listening test\ndata for the main track of the challenge consisted of samples from 187\ndifferent text-to-speech and voice conversion systems spanning over a decade of\nresearch, and the out-of-domain track consisted of data from more recent\nsystems rated in a separate listening test. Results of the challenge show the\neffectiveness of fine-tuning self-supervised speech models for the MOS\nprediction task, as well as the difficulty of predicting MOS ratings for unseen\nspeakers and listeners, and for unseen systems in the out-of-domain setting.", "published": "2022-03-21 23:32:08", "link": "http://arxiv.org/abs/2203.11389v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phase-Aware Spoof Speech Detection Based on Res2Net with Phase Network", "abstract": "The spoof speech detection (SSD) is the essential countermeasure for\nautomatic speaker verification systems. Although SSD with magnitude features in\nthe frequency domain has shown promising results, the phase information also\ncan be important to capture the artefacts of certain types of spoofing attacks.\nThus, both magnitude and phase features must be considered to ensure the\ngeneralization ability to diverse types of spoofing attacks. In this paper, we\ninvestigate the failure reason of feature-level fusion of the previous works\nthrough the entropy analysis from which we found that the randomness difference\nbetween magnitude and phase features is large, which can interrupt the\nfeature-level fusion via backend neural network; thus, we propose a phase\nnetwork to reduce that difference. Our SSD system: phase network equipped\nRes2Net achieved significant performance improvement, specifically in the\nspoofing attack for which the phase information is considered to be important.\nAlso, we demonstrate our SSD system in both known- and unknown-kind SSD\nscenarios for practical applications.", "published": "2022-03-21 08:15:51", "link": "http://arxiv.org/abs/2203.10793v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Perceptual Features as Markers of Parkinson's Disease: The Issue of\n  Clinical Interpretability", "abstract": "Up to 90% of patients with Parkinson's disease (PD) suffer from hypokinetic\ndysathria (HD) which is also manifested in the field of phonation. Clinical\nsigns of HD like monoloudness, monopitch or hoarse voice are usually quantified\nby conventional clinical interpretable features (jitter, shimmer,\nharmonic-to-noise ratio, etc.). This paper provides large and robust insight\ninto perceptual analysis of 5 Czech vowels of 84 PD patients and proves that\ndespite the clinical inexplicability the perceptual features outperform the\nconventional ones, especially in terms of discrimination power (classification\naccuracy ACC = 92 %, sensitivity SEN = 93 %, specificity SPE = 92 %) and\npartial correlation with clinical scores like UPDRS (Unified Parkinson's\ndisease rating scale), MMSE (Mini-mental state examination) or FOG (Freezing of\ngait questionnaire), where p < 0.0001.", "published": "2022-03-21 09:46:48", "link": "http://arxiv.org/abs/2203.10830v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-class versus One-class classifier in spontaneous speech analysis\n  oriented to Alzheimer Disease diagnosis", "abstract": "Most of medical developments require the ability to identify samples that are\nanomalous with respect to a target group or control group, in the sense they\ncould belong to a new, previously unseen class or are not class data. In this\ncase when there are not enough data to train two-class One-class classification\nappear like an available solution. On the other hand non-linear approaches\ncould give very useful information. The aim of our project is to contribute to\nearlier diagnosis of AD and better estimates of its severity by using automatic\nanalysis performed through new biomarkers extracted from speech signal. The\nmethods selected in this case are speech biomarkers oriented to Spontaneous\nSpeech and Emotional Response Analysis. In this approach One-class classifiers\nand two-class classifiers are analyzed. The use of information about outlier\nand Fractal Dimension features improves the system performance.", "published": "2022-03-21 09:57:20", "link": "http://arxiv.org/abs/2203.10837v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spoofing-Aware Speaker Verification with Unsupervised Domain Adaptation", "abstract": "In this paper, we initiate the concern of enhancing the spoofing robustness\nof the automatic speaker verification (ASV) system, without the primary\npresence of a separate countermeasure module. We start from the standard ASV\nframework of the ASVspoof 2019 baseline and approach the problem from the\nback-end classifier based on probabilistic linear discriminant analysis. We\nemploy three unsupervised domain adaptation techniques to optimize the back-end\nusing the audio data in the training partition of the ASVspoof 2019 dataset. We\ndemonstrate notable improvements on both logical and physical access scenarios,\nespecially on the latter where the system is attacked by replayed audios, with\na maximum of 36.1% and 5.3% relative improvement on bonafide and spoofed cases,\nrespectively. We perform additional studies such as per-attack breakdown\nanalysis, data composition, and integration with a countermeasure system at\nscore-level with Gaussian back-end.", "published": "2022-03-21 14:02:06", "link": "http://arxiv.org/abs/2203.10992v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AutoTTS: End-to-End Text-to-Speech Synthesis through Differentiable\n  Duration Modeling", "abstract": "Parallel text-to-speech (TTS) models have recently enabled fast and\nhighly-natural speech synthesis. However, they typically require external\nalignment models, which are not necessarily optimized for the decoder as they\nare not jointly trained. In this paper, we propose a differentiable duration\nmethod for learning monotonic alignments between input and output sequences.\nOur method is based on a soft-duration mechanism that optimizes a stochastic\nprocess in expectation. Using this differentiable duration method, we introduce\nAutoTTS, a direct text-to-waveform speech synthesis model. AutoTTS enables\nhigh-fidelity speech synthesis through a combination of adversarial training\nand matching the total ground-truth duration. Experimental results show that\nour model obtains competitive results while enjoying a much simpler training\npipeline. Audio samples are available online.", "published": "2022-03-21 15:14:44", "link": "http://arxiv.org/abs/2203.11049v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
