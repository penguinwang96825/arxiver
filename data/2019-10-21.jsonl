{"title": "Diamonds in the Rough: Generating Fluent Sentences from Early-Stage\n  Drafts for Academic Writing Assistance", "abstract": "The writing process consists of several stages such as drafting, revising,\nediting, and proofreading. Studies on writing assistance, such as grammatical\nerror correction (GEC), have mainly focused on sentence editing and\nproofreading, where surface-level issues such as typographical, spelling, or\ngrammatical errors should be corrected. We broaden this focus to include the\nearlier revising stage, where sentences require adjustment to the information\nincluded or major rewriting and propose Sentence-level Revision (SentRev) as a\nnew writing assistance task. Well-performing systems in this task can help\ninexperienced authors by producing fluent, complete sentences given their\nrough, incomplete drafts. We build a new freely available crowdsourced\nevaluation dataset consisting of incomplete sentences authored by non-native\nwriters paired with their final versions extracted from published academic\npapers for developing and evaluating SentRev models. We also establish baseline\nperformance on SentRev using our newly built evaluation dataset.", "published": "2019-10-21 07:26:07", "link": "http://arxiv.org/abs/1910.09180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Graph Convolutional Network for Implicit Discourse Relation\n  Classification", "abstract": "Implicit discourse relation classification is of great importance for\ndiscourse parsing, but remains a challenging problem due to the absence of\nexplicit discourse connectives communicating these relations. Modeling the\nsemantic interactions between the two arguments of a relation has proven useful\nfor detecting implicit discourse relations. However, most previous approaches\nmodel such semantic interactions from a shallow interactive level, which is\ninadequate on capturing enough semantic information. In this paper, we propose\na novel and effective Semantic Graph Convolutional Network (SGCN) to enhance\nthe modeling of inter-argument semantics on a deeper interaction level for\nimplicit discourse relation classification. We first build an interaction graph\nover representations of the two arguments, and then automatically extract\nin-depth semantic interactive information through graph convolution.\nExperimental results on the English corpus PDTB and the Chinese corpus CDTB\nboth demonstrate the superiority of our model to previous state-of-the-art\nsystems.", "published": "2019-10-21 07:35:46", "link": "http://arxiv.org/abs/1910.09183v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Localization of Fake News Detection via Multitask Transfer Learning", "abstract": "The use of the internet as a fast medium of spreading fake news reinforces\nthe need for computational tools that combat it. Techniques that train fake\nnews classifiers exist, but they all assume an abundance of resources including\nlarge labeled datasets and expert-curated corpora, which low-resource languages\nmay not have. In this work, we make two main contributions: First, we alleviate\nresource scarcity by constructing the first expertly-curated benchmark dataset\nfor fake news detection in Filipino, which we call \"Fake News Filipino.\"\nSecond, we benchmark Transfer Learning (TL) techniques and show that they can\nbe used to train robust fake news classifiers from little data, achieving 91%\naccuracy on our fake news dataset, reducing the error by 14% compared to\nestablished few-shot baselines. Furthermore, lifting ideas from multitask\nlearning, we show that augmenting transformer-based transfer techniques with\nauxiliary language modeling losses improves their performance by adapting to\nwriting style. Using this, we improve TL performance by 4-6%, achieving an\naccuracy of 96% on our best model. Lastly, we show that our method generalizes\nwell to different types of news articles, including political news,\nentertainment news, and opinion articles.", "published": "2019-10-21 12:28:00", "link": "http://arxiv.org/abs/1910.09295v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diversify Your Datasets: Analyzing Generalization via Controlled\n  Variance in Adversarial Datasets", "abstract": "Phenomenon-specific \"adversarial\" datasets have been recently designed to\nperform targeted stress-tests for particular inference types. Recent work (Liu\net al., 2019a) proposed that such datasets can be utilized for training NLI and\nother types of models, often allowing to learn the phenomenon in focus and\nimprove on the challenge dataset, indicating a \"blind spot\" in the original\ntraining data. Yet, although a model can improve in such a training process, it\nmight still be vulnerable to other challenge datasets targeting the same\nphenomenon but drawn from a different distribution, such as having a different\nsyntactic complexity level. In this work, we extend this method to drive\nconclusions about a model's ability to learn and generalize a target phenomenon\nrather than to \"learn\" a dataset, by controlling additional aspects in the\nadversarial datasets. We demonstrate our approach on two inference phenomena -\ndative alternation and numerical reasoning, elaborating, and in some cases\ncontradicting, the results of Liu et al.. Our methodology enables building\nbetter challenge datasets for creating more robust models, and may yield better\nmodel understanding and subsequent overarching improvements.", "published": "2019-10-21 12:34:53", "link": "http://arxiv.org/abs/1910.09302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-agnostic Question-Answering with Adversarial Training", "abstract": "Adapting models to new domain without finetuning is a challenging problem in\ndeep learning. In this paper, we utilize an adversarial training framework for\ndomain generalization in Question Answering (QA) task. Our model consists of a\nconventional QA model and a discriminator. The training is performed in the\nadversarial manner, where the two models constantly compete, so that QA model\ncan learn domain-invariant features. We apply this approach in MRQA Shared Task\n2019 and show better performance compared to the baseline model.", "published": "2019-10-21 13:11:21", "link": "http://arxiv.org/abs/1910.09342v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Word Representations: A Sub-sampled Unigram Distribution for\n  Negative Sampling", "abstract": "Word2Vec is the most popular model for word representation and has been\nwidely investigated in literature. However, its noise distribution for negative\nsampling is decided by empirical trials and the optimality has always been\nignored. We suggest that the distribution is a sub-optimal choice, and propose\nto use a sub-sampled unigram distribution for better negative sampling. Our\ncontributions include: (1) proposing the concept of semantics quantification\nand deriving a suitable sub-sampling rate for the proposed distribution\nadaptive to different training corpora; (2) demonstrating the advantages of our\napproach in both negative sampling and noise contrastive estimation by\nextensive evaluation tasks; and (3) proposing a semantics weighted model for\nthe MSR sentence completion task, resulting in considerable improvements. Our\nwork not only improves the quality of word vectors but also benefits current\nunderstanding of Word2Vec.", "published": "2019-10-21 13:34:12", "link": "http://arxiv.org/abs/1910.09362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trouble with the Curve: Predicting Future MLB Players Using Scouting\n  Reports", "abstract": "In baseball, a scouting report profiles a player's characteristics and\ntraits, usually intended for use in player valuation. This work presents a\nfirst-of-its-kind dataset of almost 10,000 scouting reports for minor league,\ninternational, and draft prospects. Compiled from articles posted to MLB.com\nand Fangraphs.com, each report consists of a written description of the player,\nnumerical grades for several skills, and unique IDs to reference their profiles\non popular resources like MLB.com, FanGraphs, and Baseball-Reference. With this\ndataset, we employ several deep neural networks to predict if minor league\nplayers will make the MLB given their scouting report. We open-source this data\nto share with the community, and present a web application demonstrating\nlanguage variations in the reports of successful and unsuccessful prospects.", "published": "2019-10-21 02:02:59", "link": "http://arxiv.org/abs/1910.12622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Good, Better, Best: Textual Distractors Generation for Multiple-Choice\n  Visual Question Answering via Reinforcement Learning", "abstract": "Multiple-choice VQA has drawn increasing attention from researchers and\nend-users recently. As the demand for automatically constructing large-scale\nmultiple-choice VQA data grows, we introduce a novel task called textual\nDistractors Generation for VQA (DG-VQA) focusing on generating challenging yet\nmeaningful distractors given the context image, question, and correct answer.\nThe DG-VQA task aims at generating distractors without ground-truth training\nsamples since such resources are rarely available. To tackle the DG-VQA\nunsupervisedly, we propose Gobbet, a reinforcement learning(RL) based framework\nthat utilizes pre-trained VQA models as an alternative knowledge base to guide\nthe distractor generation process. In Gobbet, a pre-trained VQA model serves as\nthe environment in RL setting to provide feedback for the input multi-modal\nquery, while a neural distractor generator serves as the agent to take actions\naccordingly. We propose to use existing VQA models' performance degradation as\nindicators of the quality of generated distractors. On the other hand, we show\nthe utility of generated distractors through data augmentation experiments,\nsince robustness is more and more important when AI models apply to\nunpredictable open-domain scenarios or security-sensitive applications. We\nfurther conduct a manual case study on the factors why distractors generated by\nGobbet can fool existing models.", "published": "2019-10-21 03:32:17", "link": "http://arxiv.org/abs/1910.09134v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Human-Like Decision Making: Document-level Aspect Sentiment\n  Classification via Hierarchical Reinforcement Learning", "abstract": "Recently, neural networks have shown promising results on Document-level\nAspect Sentiment Classification (DASC). However, these approaches often offer\nlittle transparency w.r.t. their inner working mechanisms and lack\ninterpretability. In this paper, to simulating the steps of analyzing aspect\nsentiment in a document by human beings, we propose a new Hierarchical\nReinforcement Learning (HRL) approach to DASC. This approach incorporates\nclause selection and word selection strategies to tackle the data noise problem\nin the task of DASC. First, a high-level policy is proposed to select\naspect-relevant clauses and discard noisy clauses. Then, a low-level policy is\nproposed to select sentiment-relevant words and discard noisy words inside the\nselected clauses. Finally, a sentiment rating predictor is designed to provide\nreward signals to guide both clause and word selection. Experimental results\ndemonstrate the impressive effectiveness of the proposed approach to DASC over\nthe state-of-the-art baselines.", "published": "2019-10-21 10:55:46", "link": "http://arxiv.org/abs/1910.09260v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text Matters but Speech Influences: A Computational Analysis of\n  Syntactic Ambiguity Resolution", "abstract": "Analyzing how human beings resolve syntactic ambiguity has long been an issue\nof interest in the field of linguistics. It is, at the same time, one of the\nmost challenging issues for spoken language understanding (SLU) systems as\nwell. As syntactic ambiguity is intertwined with issues regarding prosody and\nsemantics, the computational approach toward speech intention identification is\nexpected to benefit from the observations of the human language processing\nmechanism. In this regard, we address the task with attentive recurrent neural\nnetworks that exploit acoustic and textual features simultaneously and reveal\nhow the modalities interact with each other to derive sentence meaning.\nUtilizing a speech corpus recorded on Korean scripts of syntactically ambiguous\nutterances, we revealed that co-attention frameworks, namely multi-hop\nattention and cross-attention, show significantly superior performance in\ndisambiguating speech intention. With further analysis, we demonstrate that the\ncomputational models reflect the internal relationship between auditory and\nlinguistic processes.", "published": "2019-10-21 11:53:07", "link": "http://arxiv.org/abs/1910.09275v3", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "On Semi-Supervised Multiple Representation Behavior Learning", "abstract": "We propose a novel paradigm of semi-supervised learning (SSL)--the\nsemi-supervised multiple representation behavior learning (SSMRBL). SSMRBL aims\nto tackle the difficulty of learning a grammar for natural language parsing\nwhere the data are natural language texts and the 'labels' for marking data are\nparsing trees and/or grammar rule pieces. We call such 'labels' as compound\nstructured labels which require a hard work for training. SSMRBL is an\nincremental learning process that can learn more than one representation, which\nis an appropriate solution for dealing with the scarce of labeled training data\nin the age of big data and with the heavy workload of learning compound\nstructured labels. We also present a typical example of SSMRBL, regarding\nbehavior learning in form of a grammatical approach towards domain-based\nmultiple text summarization (DBMTS). DBMTS works under the framework of\nrhetorical structure theory (RST). SSMRBL includes two representations: text\nembedding (for representing information contained in the texts) and grammar\nmodel (for representing parsing as a behavior). The first representation was\nlearned as embedded digital vectors called impacts in a low dimensional space.\nThe grammar model was learned in an iterative way. Then an automatic\ndomain-oriented multi-text summarization approach was proposed based on the two\nrepresentations discussed above. Experimental results on large-scale Chinese\ndataset SogouCA indicate that the proposed method brings a good performance\neven if only few labeled texts are used for training with respect to our\ndefined automated metrics.", "published": "2019-10-21 12:23:48", "link": "http://arxiv.org/abs/1910.09292v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Neural Entity Coreference Resolution Review", "abstract": "Entity Coreference Resolution is the task of resolving all mentions in a\ndocument that refer to the same real world entity and is considered as one of\nthe most difficult tasks in natural language understanding. It is of great\nimportance for downstream natural language processing tasks such as entity\nlinking, machine translation, summarization, chatbots, etc. This work aims to\ngive a detailed review of current progress on solving Coreference Resolution\nusing neural-based approaches. It also provides a detailed appraisal of the\ndatasets and evaluation metrics in the field, as well as the subtask of Pronoun\nResolution that has seen various improvements in the recent years. We highlight\nthe advantages and disadvantages of the approaches, the challenges of the task,\nthe lack of agreed-upon standards in the task and propose a way to further\nexpand the boundaries of the field.", "published": "2019-10-21 12:59:32", "link": "http://arxiv.org/abs/1910.09329v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Czech Court Decisions Corpus (CzCDC): Availability as the First Step", "abstract": "In this paper, we describe the Czech Court Decision Corpus (CzCDC). CzCDC is\na dataset of 237,723 decisions published by the Czech apex (or top-tier)\ncourts, namely the Supreme Court, the Supreme Administrative Court and the\nConstitutional Court. All the decisions were published between 1st January 1993\nand 30th September 2018.\n  Court decisions are available on the webpages of the respective courts or via\ncommercial databases of legal information. This often leads researchers\ninterested in these decisions to reach either to respective court or to\ncommercial provider. This leads to delays and additional costs. These are\nfurther exacerbated by a lack of inter-court standard in the terms of the data\nformat in which courts provide their decisions. Additionally, courts' databases\noften lack proper documentation.\n  Our goal is to make the dataset of court decisions freely available online in\nconsistent (plain) format to lower the cost associated with obtaining data for\nfuture research. We believe that simplified access to court decisions through\nthe CzCDC could benefit other researchers.\n  In this paper, we describe the processing of decisions before their inclusion\ninto CzCDC and basic statistics of the dataset. This dataset contains plain\ntexts of court decisions and these texts are not annotated for any grammatical\nor syntactical features.", "published": "2019-10-21 17:06:38", "link": "http://arxiv.org/abs/1910.09513v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Building Dynamic Knowledge Graphs from Text-based Games", "abstract": "We are interested in learning how to update Knowledge Graphs (KG) from text.\nIn this preliminary work, we propose a novel Sequence-to-Sequence (Seq2Seq)\narchitecture to generate elementary KG operations. Furthermore, we introduce a\nnew dataset for KG extraction built upon text-based game transitions (over 300k\ndata points). We conduct experiments and discuss the results.", "published": "2019-10-21 17:55:25", "link": "http://arxiv.org/abs/1910.09532v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Automating Conversations", "abstract": "From 2016 to 2018, we developed and deployed Chorus, a system that blends\nreal-time human computation with artificial intelligence (AI) and has\nreal-world, open conversations with users. We took a top-down approach that\nstarted with a working crowd-powered system, Chorus, and then created a\nframework, Evorus, that enables Chorus to automate itself over time. Over our\ntwo-year deployment, more than 420 users talked with Chorus, having over 2,200\nconversation sessions. This line of work demonstrated how a crowd-powered\nconversational assistant can be automated over time, and more importantly, how\nsuch a system can be deployed to talk with real users to help them with their\neveryday tasks. This position paper discusses two sets of challenges that we\nexplored during the development and deployment of Chorus and Evorus: the\nchallenges that come from being an \"agent\" and those that arise from the subset\nof conversations that are more difficult to automate.", "published": "2019-10-21 19:29:06", "link": "http://arxiv.org/abs/1910.09621v3", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Opinion aspect extraction in Dutch childrens diary entries", "abstract": "Aspect extraction can be used in dialogue systems to understand the topic of\nopinionated text. Expressing an empathetic reaction to an opinion can\nstrengthen the bond between a human and, for example, a robot. The aim of this\nstudy is three-fold: 1. create a new annotated dataset for both aspect\nextraction and opinion words for Dutch childrens language, 2. acquire aspect\nextraction results for this task and 3. improve current results for aspect\nextraction in Dutch reviews. This was done by training a deep learning Gated\nRecurrent Unit (GRU) model, originally developed for an English review dataset,\non Dutch restaurant review data to classify both opinion words and their\nrespective aspects. We obtained state-of-the-art performance on the Dutch\nrestaurant review dataset. Additionally, we acquired aspect extraction results\nfor the Dutch childrens dataset. Since the model was trained on standardised\nlanguage, these results are quite promising.", "published": "2019-10-21 09:33:09", "link": "http://arxiv.org/abs/1910.10502v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Textual analysis of artificial intelligence manuscripts reveals features\n  associated with peer review outcome", "abstract": "We analysed a dataset of scientific manuscripts that were submitted to\nvarious conferences in artificial intelligence. We performed a combination of\nsemantic, lexical and psycholinguistic analyses of the full text of the\nmanuscripts and compared them with the outcome of the peer review process. We\nfound that accepted manuscripts scored lower than rejected manuscripts on two\nindicators of readability, and that they also used more scientific and\nartificial intelligence jargon. We also found that accepted manuscripts were\nwritten with words that are less frequent, that are acquired at an older age,\nand that are more abstract than rejected manuscripts. The analysis of\nreferences included in the manuscripts revealed that the subset of accepted\nsubmissions were more likely to cite the same publications. This finding was\nechoed by pairwise comparisons of the word content of the manuscripts (i.e. an\nindicator or semantic similarity), which were more similar in the subset of\naccepted manuscripts. Finally, we predicted the peer review outcome of\nmanuscripts with their word content, with words related to machine learning and\nneural networks positively related with acceptance, whereas words related to\nlogic, symbolic processing and knowledge-based systems negatively related with\nacceptance.", "published": "2019-10-21 16:36:51", "link": "http://arxiv.org/abs/1911.02648v2", "categories": ["cs.DL", "cs.AI", "cs.CL"], "primary_category": "cs.DL"}
{"title": "Transformer-CNN: Fast and Reliable tool for QSAR", "abstract": "We present SMILES-embeddings derived from the internal encoder state of a\nTransformer [1] model trained to canonize SMILES as a Seq2Seq problem. Using a\nCharNN [2] architecture upon the embeddings results in higher quality\ninterpretable QSAR/QSPR models on diverse benchmark datasets including\nregression and classification tasks. The proposed Transformer-CNN method uses\nSMILES augmentation for training and inference, and thus the prognosis is based\non an internal consensus. That both the augmentation and transfer learning are\nbased on embeddings allows the method to provide good results for small\ndatasets. We discuss the reasons for such effectiveness and draft future\ndirections for the development of the method. The source code and the\nembeddings needed to train a QSAR model are available on\nhttps://github.com/bigchem/transformer-cnn. The repository also has a\nstandalone program for QSAR prognosis which calculates individual atoms\ncontributions, thus interpreting the model's result. OCHEM [3] environment\n(https://ochem.eu) hosts the on-line implementation of the method proposed.", "published": "2019-10-21 12:49:55", "link": "http://arxiv.org/abs/1911.06603v3", "categories": ["q-bio.QM", "cs.CL", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "Discovering the Compositional Structure of Vector Representations with\n  Role Learning Networks", "abstract": "How can neural networks perform so well on compositional tasks even though\nthey lack explicit compositional representations? We use a novel analysis\ntechnique called ROLE to show that recurrent neural networks perform well on\nsuch tasks by converging to solutions which implicitly represent symbolic\nstructure. This method uncovers a symbolic structure which, when properly\nembedded in vector space, closely approximates the encodings of a standard\nseq2seq network trained to perform the compositional SCAN task. We verify the\ncausal importance of the discovered symbolic structure by showing that, when we\nsystematically manipulate hidden embeddings based on this symbolic structure,\nthe model's output is changed in the way predicted by our analysis.", "published": "2019-10-21 02:12:51", "link": "http://arxiv.org/abs/1910.09113v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Comparison of Semantic Similarity Methods for Maximum Human\n  Interpretability", "abstract": "The inclusion of semantic information in any similarity measures improves the\nefficiency of the similarity measure and provides human interpretable results\nfor further analysis. The similarity calculation method that focuses on\nfeatures related to the text's words only, will give less accurate results.\nThis paper presents three different methods that not only focus on the text's\nwords but also incorporates semantic information of texts in their feature\nvector and computes semantic similarities. These methods are based on\ncorpus-based and knowledge-based methods, which are: cosine similarity using\ntf-idf vectors, cosine similarity using word embedding and soft cosine\nsimilarity using word embedding. Among these three, cosine similarity using\ntf-idf vectors performed best in finding similarities between short news texts.\nThe similar texts given by the method are easy to interpret and can be used\ndirectly in other information retrieval applications.", "published": "2019-10-21 03:09:02", "link": "http://arxiv.org/abs/1910.09129v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Constructing Artificial Data for Fine-tuning for Low-Resource Biomedical\n  Text Tagging with Applications in PICO Annotation", "abstract": "Biomedical text tagging systems are plagued by the dearth of labeled training\ndata. There have been recent attempts at using pre-trained encoders to deal\nwith this issue. Pre-trained encoder provides representation of the input text\nwhich is then fed to task-specific layers for classification. The entire\nnetwork is fine-tuned on the labeled data from the target task. Unfortunately,\na low-resource biomedical task often has too few labeled instances for\nsatisfactory fine-tuning. Also, if the label space is large, it contains few or\nno labeled instances for majority of the labels. Most biomedical tagging\nsystems treat labels as indexes, ignoring the fact that these labels are often\nconcepts expressed in natural language e.g. `Appearance of lesion on brain\nimaging'. To address these issues, we propose constructing extra labeled\ninstances using label-text (i.e. label's name) as input for the corresponding\nlabel-index (i.e. label's index). In fact, we propose a number of strategies\nfor manufacturing multiple artificial labeled instances from a single label.\nThe network is then fine-tuned on a combination of real and these newly\nconstructed artificial labeled instances. We evaluate the proposed approach on\nan important low-resource biomedical task called \\textit{PICO annotation},\nwhich requires tagging raw text describing clinical trials with labels\ncorresponding to different aspects of the trial i.e. PICO (Population,\nIntervention/Control, Outcome) characteristics of the trial. Our empirical\nresults show that the proposed method achieves a new state-of-the-art\nperformance for PICO annotation with very significant improvements over\ncompetitive baselines.", "published": "2019-10-21 10:19:53", "link": "http://arxiv.org/abs/1910.09255v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Clotho: An Audio Captioning Dataset", "abstract": "Audio captioning is the novel task of general audio content description using\nfree text. It is an intermodal translation task (not speech-to-text), where a\nsystem accepts as an input an audio signal and outputs the textual description\n(i.e. the caption) of that signal. In this paper we present Clotho, a dataset\nfor audio captioning consisting of 4981 audio samples of 15 to 30 seconds\nduration and 24 905 captions of eight to 20 words length, and a baseline method\nto provide initial results. Clotho is built with focus on audio content and\ncaption diversity, and the splits of the data are not hampering the training or\nevaluation of methods. All sounds are from the Freesound platform, and captions\nare crowdsourced using Amazon Mechanical Turk and annotators from English\nspeaking countries. Unique words, named entities, and speech transcription are\nremoved with post-processing. Clotho is freely available online\n(https://zenodo.org/record/3490684).", "published": "2019-10-21 14:06:01", "link": "http://arxiv.org/abs/1910.09387v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image\n  Synthesis", "abstract": "Text-to-image synthesis refers to computational methods which translate human\nwritten textual descriptions, in the form of keywords or sentences, into images\nwith similar semantic meaning to the text. In earlier research, image synthesis\nrelied mainly on word to image correlation analysis combined with supervised\nmethods to find best alignment of the visual content matching to the text.\nRecent progress in deep learning (DL) has brought a new set of unsupervised\ndeep learning methods, particularly deep generative models which are able to\ngenerate realistic visual images using suitably trained neural network models.\nIn this paper, we review the most recent development in the text-to-image\nsynthesis research domain. Our survey first introduces image synthesis and its\nchallenges, and then reviews key concepts such as generative adversarial\nnetworks (GANs) and deep convolutional encoder-decoder neural networks (DCNN).\nAfter that, we propose a taxonomy to summarize GAN based text-to-image\nsynthesis into four major categories: Semantic Enhancement GANs, Resolution\nEnhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We\nelaborate the main objective of each group, and further review typical GAN\narchitectures in each group. The taxonomy and the review outline the techniques\nand the evolution of different approaches, and eventually provide a clear\nroadmap to summarize the list of contemporaneous solutions that utilize GANs\nand DCNNs to generate enthralling results in categories such as human faces,\nbirds, flowers, room interiors, object reconstruction from edge maps (games)\netc. The survey will conclude with a comparison of the proposed solutions,\nchallenges that remain unresolved, and future developments in the text-to-image\nsynthesis domain.", "published": "2019-10-21 14:23:14", "link": "http://arxiv.org/abs/1910.09399v1", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "HIGhER : Improving instruction following with Hindsight Generation for\n  Experience Replay", "abstract": "Language creates a compact representation of the world and allows the\ndescription of unlimited situations and objectives through compositionality.\nWhile these characterizations may foster instructing, conditioning or\nstructuring interactive agent behavior, it remains an open-problem to correctly\nrelate language understanding and reinforcement learning in even simple\ninstruction following scenarios. This joint learning problem is alleviated\nthrough expert demonstrations, auxiliary losses, or neural inductive biases. In\nthis paper, we propose an orthogonal approach called Hindsight Generation for\nExperience Replay (HIGhER) that extends the Hindsight Experience Replay (HER)\napproach to the language-conditioned policy setting. Whenever the agent does\nnot fulfill its instruction, HIGhER learns to output a new directive that\nmatches the agent trajectory, and it relabels the episode with a positive\nreward. To do so, HIGhER learns to map a state into an instruction by using\npast successful trajectories, which removes the need to have external expert\ninterventions to relabel episodes as in vanilla HER. We show the efficiency of\nour approach in the BabyAI environment, and demonstrate how it complements\nother instruction following methods.", "published": "2019-10-21 15:31:29", "link": "http://arxiv.org/abs/1910.09451v3", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Content Removal as a Moderation Strategy: Compliance and Other Outcomes\n  in the ChangeMyView Community", "abstract": "Moderators of online communities often employ comment deletion as a tool. We\nask here whether, beyond the positive effects of shielding a community from\nundesirable content, does comment removal actually cause the behavior of the\ncomment's author to improve? We examine this question in a particularly\nwell-moderated community, the ChangeMyView subreddit.\n  The standard analytic approach of interrupted time-series analysis\nunfortunately cannot answer this question of causality because it fails to\ndistinguish the effect of having made a non-compliant comment from the effect\nof being subjected to moderator removal of that comment. We therefore leverage\na \"delayed feedback\" approach based on the observation that some users may\nremain active between the time when they posted the non-compliant comment and\nthe time when that comment is deleted. Applying this approach to such users, we\nreveal the causal role of comment deletion in reducing immediate noncompliance\nrates, although we do not find evidence of it having a causal role in inducing\nother behavior improvements. Our work thus empirically demonstrates both the\npromise and some potential limits of content removal as a positive moderation\nstrategy, and points to future directions for identifying causal effects from\nobservational data.", "published": "2019-10-21 18:00:04", "link": "http://arxiv.org/abs/1910.09563v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Designovel's system description for Fashion-IQ challenge 2019", "abstract": "This paper describes Designovel's systems which are submitted to the Fashion\nIQ Challenge 2019. Goal of the challenge is building an image retrieval system\nwhere input query is a candidate image plus two text phrases describe user's\nfeedback about visual differences between the candidate image and the search\ntarget. We built the systems by combining methods from recent work on deep\nmetric learning, multi-modal retrieval and natual language processing. First,\nwe encode both candidate and target images with CNNs into high-level\nrepresentations, and encode text descriptions to a single text vector using\nTransformer-based encoder. Then we compose candidate image vector and text\nrepresentation into a single vector which is exptected to be biased toward\ntarget image vector. Finally, we compute cosine similarities between composed\nvector and encoded vectors of whole dataset, and rank them in desceding order\nto get ranked list. We experimented with Fashion IQ 2019 dataset in various\nsettings of hyperparameters, achieved 39.12% average recall by a single model\nand 43.67% average recall by an ensemble of 16 models on test dataset.", "published": "2019-10-21 18:06:26", "link": "http://arxiv.org/abs/1910.11119v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Enforcing Reasoning in Visual Commonsense Reasoning", "abstract": "The task of Visual Commonsense Reasoning is extremely challenging in the\nsense that the model has to not only be able to answer a question given an\nimage, but also be able to learn to reason. The baselines introduced in this\ntask are quite limiting because two networks are trained for predicting answers\nand rationales separately. Question and image is used as input to train answer\nprediction network while question, image and correct answer are used as input\nin the rationale prediction network. As rationale is conditioned on the correct\nanswer, it is based on the assumption that we can solve Visual Question\nAnswering task without any error - which is over ambitious. Moreover, such an\napproach makes both answer and rationale prediction two completely independent\nVQA tasks rendering cognition task meaningless. In this paper, we seek to\naddress these issues by proposing an end-to-end trainable model which considers\nboth answers and their reasons jointly. Specifically, we first predict the\nanswer for the question and then use the chosen answer to predict the\nrationale. However, a trivial design of such a model becomes non-differentiable\nwhich makes it difficult to train. We solve this issue by proposing four\napproaches - softmax, gumbel-softmax, reinforcement learning based sampling and\ndirect cross entropy against all pairs of answers and rationales. We\ndemonstrate through experiments that our model performs competitively against\ncurrent state-of-the-art. We conclude with an analysis of presented approaches\nand discuss avenues for further work.", "published": "2019-10-21 02:33:18", "link": "http://arxiv.org/abs/1910.11124v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Learning to Map Natural Language Instructions to Physical Quadcopter\n  Control using Simulated Flight", "abstract": "We propose a joint simulation and real-world learning framework for mapping\nnavigation instructions and raw first-person observations to continuous\ncontrol. Our model estimates the need for environment exploration, predicts the\nlikelihood of visiting environment positions during execution, and controls the\nagent to both explore and visit high-likelihood positions. We introduce\nSupervised Reinforcement Asynchronous Learning (SuReAL). Learning uses both\nsimulation and real environments without requiring autonomous flight in the\nphysical environment during training, and combines supervised learning for\npredicting positions to visit and reinforcement learning for continuous\ncontrol. We evaluate our approach on a natural language instruction-following\ntask with a physical quadcopter, and demonstrate effective execution and\nexploration behavior.", "published": "2019-10-21 21:19:33", "link": "http://arxiv.org/abs/1910.09664v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Using Speech Synthesis to Train End-to-End Spoken Language Understanding\n  Models", "abstract": "End-to-end models are an attractive new approach to spoken language\nunderstanding (SLU) in which the meaning of an utterance is inferred directly\nfrom the raw audio without employing the standard pipeline composed of a\nseparately trained speech recognizer and natural language understanding module.\nThe downside of end-to-end SLU is that in-domain speech data must be recorded\nto train the model. In this paper, we propose a strategy for overcoming this\nrequirement in which speech synthesis is used to generate a large synthetic\ntraining dataset from several artificial speakers. Experiments on two\nopen-source SLU datasets confirm the effectiveness of our approach, both as a\nsole source of training data and as a form of data augmentation.", "published": "2019-10-21 15:47:52", "link": "http://arxiv.org/abs/1910.09463v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Sound texture synthesis using RI spectrograms", "abstract": "This article introduces a new parametric synthesis method for sound textures\nbased on existing works in visual and sound texture synthesis. Starting from a\nbase sound signal, an optimization process is performed until the\ncross-correlations between the feature-maps of several untrained 2D\nConvolutional Neural Networks (CNN) resemble those of an original sound\ntexture. We use compressed RI spectrograms as input to the CNN: this\ntime-frequency representation is the stacking of the real and imaginary part of\nthe Short Time Fourier Transform (STFT) and thus implicitly contains both the\nmagnitude and phase information, allowing for convincing syntheses of various\naudio events. The optimization is however performed directly on the time signal\nto avoid any STFT consistency issue. The results of an online perceptual\nevaluation are also detailed, and show that this method achieves results that\nare more realistic-sounding than existing parametric methods on a wide array of\ntextures.", "published": "2019-10-21 16:40:21", "link": "http://arxiv.org/abs/1910.09497v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Comparative Study between Adversarial Networks and Classical Techniques\n  for Speech Enhancement", "abstract": "Speech enhancement is a crucial task for several applications. Among the most\nexplored techniques are the Wiener filter and the LogMMSE, but approaches\nexploring deep learning adapted to this task, such as SEGAN, have presented\nrelevant results. This study compared the performance of the mentioned\ntechniques in 85 noise conditions regarding quality, intelligibility, and\ndistortion; and concluded that classical techniques continue to exhibit\nsuperior results for most scenarios, but, in severe noise scenarios, SEGAN\nperformed better and with lower variance.", "published": "2019-10-21 17:28:12", "link": "http://arxiv.org/abs/1910.09522v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Modeling of Individual HRTFs based on Spatial Principal Component\n  Analysis", "abstract": "Head-related transfer function (HRTF) plays an important role in the\nconstruction of 3D auditory display. This paper presents an individual HRTF\nmodeling method using deep neural networks based on spatial principal component\nanalysis. The HRTFs are represented by a small set of spatial principal\ncomponents combined with frequency and individual-dependent weights. By\nestimating the spatial principal components using deep neural networks and\nmapping the corresponding weights to a quantity of anthropometric parameters,\nwe predict individual HRTFs in arbitrary spatial directions. The objective and\nsubjective experiments evaluate the HRTFs generated by the proposed method, the\nprincipal component analysis (PCA) method, and the generic method. The results\nshow that the HRTFs generated by the proposed method and PCA method perform\nbetter than the generic method. For most frequencies the spectral distortion of\nthe proposed method is significantly smaller than the PCA method in the high\nfrequencies but significantly larger in the low frequencies. The evaluation of\nthe localization model shows the PCA method is better than the proposed method.\nThe subjective localization experiments show that the PCA and the proposed\nmethods have similar performances in most conditions. Both the objective and\nsubjective experiments show that the proposed method can predict HRTFs in\narbitrary spatial directions.", "published": "2019-10-21 16:20:41", "link": "http://arxiv.org/abs/1910.09484v6", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Signal Combination for Language Identification", "abstract": "Google's multilingual speech recognition system combines low-level acoustic\nsignals with language-specific recognizer signals to better predict the\nlanguage of an utterance. This paper presents our experience with different\nsignal combination methods to improve overall language identification accuracy.\nWe compare the performance of a lattice-based ensemble model and a deep neural\nnetwork model to combine signals from recognizers with that of a baseline that\nonly uses low-level acoustic signals. Experimental results show that the deep\nneural network model outperforms the lattice-based ensemble model, and it\nreduced the error rate from 5.5% in the baseline to 4.3%, which is a 21.8%\nrelative reduction.", "published": "2019-10-21 23:00:47", "link": "http://arxiv.org/abs/1910.09687v2", "categories": ["cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-Band Multi-Resolution Fully Convolutional Neural Networks for\n  Singing Voice Separation", "abstract": "Deep neural networks with convolutional layers usually process the entire\nspectrogram of an audio signal with the same time-frequency resolutions, number\nof filters, and dimensionality reduction scale. According to the constant-Q\ntransform, good features can be extracted from audio signals if the low\nfrequency bands are processed with high frequency resolution filters and the\nhigh frequency bands with high time resolution filters. In the spectrogram of a\nmixture of singing voices and music signals, there is usually more information\nabout the voice in the low frequency bands than the high frequency bands. These\nraise the need for processing each part of the spectrogram differently. In this\npaper, we propose a multi-band multi-resolution fully convolutional neural\nnetwork (MBR-FCN) for singing voice separation. The MBR-FCN processes the\nfrequency bands that have more information about the target signals with more\nfilters and smaller dimentionality reduction scale than the bands with less\ninformation. Furthermore, the MBR-FCN processes the low frequency bands with\nhigh frequency resolution filters and the high frequency bands with high time\nresolution filters. Our experimental results show that the proposed MBR-FCN\nwith very few parameters achieves better singing voice separation performance\nthan other deep neural networks.", "published": "2019-10-21 11:29:29", "link": "http://arxiv.org/abs/1910.09266v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP", "stat.ML", "68T01, 68T10, 68T45, 62H25", "H.5.5; I.5; I.2.6; I.4.3; I.4; I.2"], "primary_category": "cs.SD"}
{"title": "AeGAN: Time-Frequency Speech Denoising via Generative Adversarial\n  Networks", "abstract": "Automatic speech recognition (ASR) systems are of vital importance nowadays\nin commonplace tasks such as speech-to-text processing and language\ntranslation. This created the need for an ASR system that can operate in\nrealistic crowded environments. Thus, speech enhancement is a valuable building\nblock in ASR systems and other applications such as hearing aids, smartphones\nand teleconferencing systems. In this paper, a generative adversarial network\n(GAN) based framework is investigated for the task of speech enhancement, more\nspecifically speech denoising of audio tracks. A new architecture based on\nCasNet generator and an additional feature-based loss are incorporated to get\nrealistically denoised speech phonetics. Finally, the proposed framework is\nshown to outperform other learning and traditional model-based speech\nenhancement approaches.", "published": "2019-10-21 13:27:22", "link": "http://arxiv.org/abs/1910.12620v3", "categories": ["eess.AS", "cs.LG", "cs.NE", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
