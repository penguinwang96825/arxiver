{"title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse\n  Heterogeneous Computing", "abstract": "The scaling of large language models has greatly improved natural language\nunderstanding, generation, and reasoning. In this work, we develop a system\nthat trained a trillion-parameter language model on a cluster of Ascend 910 AI\nprocessors and MindSpore framework, and present the language model with 1.085T\nparameters named PanGu-{\\Sigma}. With parameter inherent from PanGu-{\\alpha},\nwe extend the dense Transformer model to sparse one with Random Routed Experts\n(RRE), and efficiently train the model over 329B tokens by using Expert\nComputation and Storage Separation(ECSS). This resulted in a 6.3x increase in\ntraining throughput through heterogeneous computing. Our experimental findings\nshow that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot\nlearning of various Chinese NLP downstream tasks. Moreover, it demonstrates\nstrong abilities when fine-tuned in application data of open-domain dialogue,\nquestion answering, machine translation and code generation.", "published": "2023-03-20 03:39:27", "link": "http://arxiv.org/abs/2303.10845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieving Multimodal Information for Augmented Generation: A Survey", "abstract": "As Large Language Models (LLMs) become popular, there emerged an important\ntrend of using multimodality to augment the LLMs' generation ability, which\nenables LLMs to better interact with the world. However, there lacks a unified\nperception of at which stage and how to incorporate different modalities. In\nthis survey, we review methods that assist and augment generative models by\nretrieving multimodal knowledge, whose formats range from images, codes,\ntables, graphs, to audio. Such methods offer a promising solution to important\nconcerns such as factuality, reasoning, interpretability, and robustness. By\nproviding an in-depth review, this survey is expected to provide scholars with\na deeper understanding of the methods' applications and encourage them to adapt\nexisting techniques to the fast-growing field of LLMs.", "published": "2023-03-20 05:07:41", "link": "http://arxiv.org/abs/2303.10868v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-task Transformer with Relation-attention and Type-attention for\n  Named Entity Recognition", "abstract": "Named entity recognition (NER) is an important research problem in natural\nlanguage processing. There are three types of NER tasks, including flat, nested\nand discontinuous entity recognition. Most previous sequential labeling models\nare task-specific, while recent years have witnessed the rising of generative\nmodels due to the advantage of unifying all NER tasks into the seq2seq model\nframework. Although achieving promising performance, our pilot studies\ndemonstrate that existing generative models are ineffective at detecting entity\nboundaries and estimating entity types. This paper proposes a multi-task\nTransformer, which incorporates an entity boundary detection task into the\nnamed entity recognition task. More concretely, we achieve entity boundary\ndetection by classifying the relations between tokens within the sentence. To\nimprove the accuracy of entity-type mapping during decoding, we adopt an\nexternal knowledge base to calculate the prior entity-type distributions and\nthen incorporate the information into the model via the self and\ncross-attention mechanisms. We perform experiments on an extensive set of NER\nbenchmarks, including two flat, three nested, and three discontinuous NER\ndatasets. Experimental results show that our approach considerably improves the\ngenerative NER model's performance.", "published": "2023-03-20 05:11:22", "link": "http://arxiv.org/abs/2303.10870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Character, Word, or Both? Revisiting the Segmentation Granularity for\n  Chinese Pre-trained Language Models", "abstract": "Pretrained language models (PLMs) have shown marvelous improvements across\nvarious NLP tasks. Most Chinese PLMs simply treat an input text as a sequence\nof characters, and completely ignore word information. Although Whole Word\nMasking can alleviate this, the semantics in words is still not well\nrepresented. In this paper, we revisit the segmentation granularity of Chinese\nPLMs. We propose a mixed-granularity Chinese BERT (MigBERT) by considering both\ncharacters and words. To achieve this, we design objective functions for\nlearning both character and word-level representations. We conduct extensive\nexperiments on various Chinese NLP tasks to evaluate existing PLMs as well as\nthe proposed MigBERT. Experimental results show that MigBERT achieves new SOTA\nperformance on all these tasks. Further analysis demonstrates that words are\nsemantically richer than characters. More interestingly, we show that MigBERT\nalso works with Japanese. Our code and model have been released\nhere~\\footnote{https://github.com/xnliang98/MigBERT}.", "published": "2023-03-20 06:20:03", "link": "http://arxiv.org/abs/2303.10893v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Reliable Neural Machine Translation with Consistency-Aware\n  Meta-Learning", "abstract": "Neural machine translation (NMT) has achieved remarkable success in producing\nhigh-quality translations. However, current NMT systems suffer from a lack of\nreliability, as their outputs that are often affected by lexical or syntactic\nchanges in inputs, resulting in large variations in quality. This limitation\nhinders the practicality and trustworthiness of NMT. A contributing factor to\nthis problem is that NMT models trained with the one-to-one paradigm struggle\nto handle the source diversity phenomenon, where inputs with the same meaning\ncan be expressed differently. In this work, we treat this problem as a bilevel\noptimization problem and present a consistency-aware meta-learning (CAML)\nframework derived from the model-agnostic meta-learning (MAML) algorithm to\naddress it. Specifically, the NMT model with CAML (named CoNMT) first learns a\nconsistent meta representation of semantically equivalent sentences in the\nouter loop. Subsequently, a mapping from the meta representation to the output\nsentence is learned in the inner loop, allowing the NMT model to translate\nsemantically equivalent sentences to the same target sentence. We conduct\nexperiments on the NIST Chinese to English task, three WMT translation tasks,\nand the TED M2O task. The results demonstrate that CoNMT effectively improves\noverall translation quality and reliably handles diverse inputs.", "published": "2023-03-20 09:41:28", "link": "http://arxiv.org/abs/2303.10966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EmotionIC: emotional inertia and contagion-driven dependency modeling\n  for emotion recognition in conversation", "abstract": "Emotion Recognition in Conversation (ERC) has attracted growing attention in\nrecent years as a result of the advancement and implementation of\nhuman-computer interface technologies. In this paper, we propose an emotional\ninertia and contagion-driven dependency modeling approach (EmotionIC) for ERC\ntask. Our EmotionIC consists of three main components, i.e., Identity Masked\nMulti-Head Attention (IMMHA), Dialogue-based Gated Recurrent Unit (DiaGRU), and\nSkip-chain Conditional Random Field (SkipCRF). Compared to previous ERC models,\nEmotionIC can model a conversation more thoroughly at both the\nfeature-extraction and classification levels. The proposed model attempts to\nintegrate the advantages of attention- and recurrence-based methods at the\nfeature-extraction level. Specifically, IMMHA is applied to capture\nidentity-based global contextual dependencies, while DiaGRU is utilized to\nextract speaker- and temporal-aware local contextual information. At the\nclassification level, SkipCRF can explicitly mine complex emotional flows from\nhigher-order neighboring utterances in the conversation. Experimental results\nshow that our method can significantly outperform the state-of-the-art models\non four benchmark datasets. The ablation studies confirm that our modules can\neffectively model emotional inertia and contagion.", "published": "2023-03-20 13:58:35", "link": "http://arxiv.org/abs/2303.11117v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocRED-FE: A Document-Level Fine-Grained Entity And Relation Extraction\n  Dataset", "abstract": "Joint entity and relation extraction (JERE) is one of the most important\ntasks in information extraction. However, most existing works focus on\nsentence-level coarse-grained JERE, which have limitations in real-world\nscenarios. In this paper, we construct a large-scale document-level\nfine-grained JERE dataset DocRED-FE, which improves DocRED with Fine-Grained\nEntity Type. Specifically, we redesign a hierarchical entity type schema\nincluding 11 coarse-grained types and 119 fine-grained types, and then\nre-annotate DocRED manually according to this schema. Through comprehensive\nexperiments we find that: (1) DocRED-FE is challenging to existing JERE models;\n(2) Our fine-grained entity types promote relation classification. We make\nDocRED-FE with instruction and the code for our baselines publicly available at\nhttps://github.com/PKU-TANGENT/DOCRED-FE.", "published": "2023-03-20 14:19:58", "link": "http://arxiv.org/abs/2303.11141v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversation Modeling to Predict Derailment", "abstract": "Conversations among online users sometimes derail, i.e., break down into\npersonal attacks. Such derailment has a negative impact on the healthy growth\nof cyberspace communities. The ability to predict whether ongoing conversations\nare likely to derail could provide valuable real-time insight to interlocutors\nand moderators. Prior approaches predict conversation derailment\nretrospectively without the ability to forestall the derailment proactively.\nSome works attempt to make dynamic prediction as the conversation develops, but\nfail to incorporate multisource information, such as conversation structure and\ndistance to derailment.\n  We propose a hierarchical transformer-based framework that combines\nutterance-level and conversation-level information to capture fine-grained\ncontextual semantics. We propose a domain-adaptive pretraining objective to\nintegrate conversational structure information and a multitask learning scheme\nto leverage the distance from each utterance to derailment. An evaluation of\nour framework on two conversation derailment datasets yields improvement over\nF1 score for the prediction of derailment. These results demonstrate the\neffectiveness of incorporating multisource information.", "published": "2023-03-20 15:10:45", "link": "http://arxiv.org/abs/2303.11184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Shannon Game with Images", "abstract": "The Shannon game has long been used as a thought experiment in linguistics\nand NLP, asking participants to guess the next letter in a sentence based on\nits preceding context. We extend the game by introducing an optional extra\nmodality in the form of image information. To investigate the impact of\nmultimodal information in this game, we use human participants and a language\nmodel (LM, GPT-2).\n  We show that the addition of image information improves both self-reported\nconfidence and accuracy for both humans and LM. Certain word classes, such as\nnouns and determiners, benefit more from the additional modality information.\nThe priming effect in both humans and the LM becomes more apparent as the\ncontext size (extra modality information + sentence context) increases. These\nfindings highlight the potential of multimodal information in improving\nlanguage understanding and modeling.", "published": "2023-03-20 15:22:11", "link": "http://arxiv.org/abs/2303.11192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-faithful Prompting for Large Language Models", "abstract": "Large language models (LLMs) encode parametric knowledge about world facts\nand have shown remarkable performance in knowledge-driven NLP tasks. However,\ntheir reliance on parametric knowledge may cause them to overlook contextual\ncues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,\nknowledge acquisition tasks). In this paper, we seek to assess and enhance\nLLMs' contextual faithfulness in two aspects: knowledge conflict and prediction\nwith abstention. We demonstrate that LLMs' faithfulness can be significantly\nimproved using carefully designed prompting strategies. In particular, we\nidentify opinion-based prompts and counterfactual demonstrations as the most\neffective methods. Opinion-based prompts reframe the context as a narrator's\nstatement and inquire about the narrator's opinions, while counterfactual\ndemonstrations use instances containing false facts to improve faithfulness in\nknowledge conflict situations. Neither technique requires additional training.\nWe conduct experiments on three datasets of two standard NLP tasks, machine\nreading comprehension and relation extraction, and the results demonstrate\nsignificant improvement in faithfulness to contexts. Code and data are released\nat https://github.com/wzhouad/context-faithful-llm.", "published": "2023-03-20 17:54:58", "link": "http://arxiv.org/abs/2303.11315v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Model Behavior: A Comprehensive Survey", "abstract": "Transformer language models have received widespread public attention, yet\ntheir generated text is often surprising even to NLP researchers. In this\nsurvey, we discuss over 250 recent studies of English language model behavior\nbefore task-specific fine-tuning. Language models possess basic capabilities in\nsyntax, semantics, pragmatics, world knowledge, and reasoning, but these\ncapabilities are sensitive to specific inputs and surface features. Despite\ndramatic increases in generated text quality as models scale to hundreds of\nbillions of parameters, the models are still prone to unfactual responses,\ncommonsense errors, memorized text, and social biases. Many of these weaknesses\ncan be framed as over-generalizations or under-generalizations of learned\npatterns in text. We synthesize recent results to highlight what is currently\nknown about large language model capabilities, thus providing a resource for\napplied work and for research in adjacent fields that use language models.", "published": "2023-03-20 23:54:26", "link": "http://arxiv.org/abs/2303.11504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Semantic Text Similarity to rank Hypernyms of Financial Terms", "abstract": "Over the years, there has been a paradigm shift in how users access financial\nservices. With the advancement of digitalization more users have been\npreferring the online mode of performing financial activities. This has led to\nthe generation of a huge volume of financial content. Most investors prefer to\ngo through these contents before making decisions. Every industry has terms\nthat are specific to the domain it operates in. Banking and Financial Services\nare not an exception to this. In order to fully comprehend these contents, one\nneeds to have a thorough understanding of the financial terms. Getting a basic\nidea about a term becomes easy when it is explained with the help of the broad\ncategory to which it belongs. This broad category is referred to as hypernym.\nFor example, \"bond\" is a hypernym of the financial term \"alternative\ndebenture\". In this paper, we propose a system capable of extracting and\nranking hypernyms for a given financial term. The system has been trained with\nfinancial text corpora obtained from various sources like DBpedia [4],\nInvestopedia, Financial Industry Business Ontology (FIBO), prospectus and so\non. Embeddings of these terms have been extracted using FinBERT [3], FinISH [1]\nand fine-tuned using SentenceBERT [54]. A novel approach has been used to\naugment the training set with negative samples. It uses the hierarchy present\nin FIBO. Finally, we benchmark the system performance with that of the existing\nones. We establish that it performs better than the existing ones and is also\nscalable.", "published": "2023-03-20 16:53:36", "link": "http://arxiv.org/abs/2303.13475v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Improving-Leaderboard(SIL): A Call for Real-World Centric Natural\n  Language Processing Leaderboards", "abstract": "Leaderboard systems allow researchers to objectively evaluate Natural\nLanguage Processing (NLP) models and are typically used to identify models that\nexhibit superior performance on a given task in a predetermined setting.\nHowever, we argue that evaluation on a given test dataset is just one of many\nperformance indications of the model. In this paper, we claim leaderboard\ncompetitions should also aim to identify models that exhibit the best\nperformance in a real-world setting. We highlight three issues with current\nleaderboard systems: (1) the use of a single, static test set, (2) discrepancy\nbetween testing and real-world application (3) the tendency for\nleaderboard-centric competition to be biased towards the test set. As a\nsolution, we propose a new paradigm of leaderboard systems that addresses these\nissues of current leaderboard system. Through this study, we hope to induce a\nparadigm shift towards more real -world-centric leaderboard competitions.", "published": "2023-03-20 06:13:03", "link": "http://arxiv.org/abs/2303.10888v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Translate your gibberish: black-box adversarial attack on machine\n  translation systems", "abstract": "Neural networks are deployed widely in natural language processing tasks on\nthe industrial scale, and perhaps the most often they are used as compounds of\nautomatic machine translation systems. In this work, we present a simple\napproach to fool state-of-the-art machine translation tools in the task of\ntranslation from Russian to English and vice versa. Using a novel black-box\ngradient-free tensor-based optimizer, we show that many online translation\ntools, such as Google, DeepL, and Yandex, may both produce wrong or offensive\ntranslations for nonsensical adversarial input queries and refuse to translate\nseemingly benign input phrases. This vulnerability may interfere with\nunderstanding a new language and simply worsen the user's experience while\nusing machine translation systems, and, hence, additional improvements of these\ntools are required to establish better translation.", "published": "2023-03-20 09:52:52", "link": "http://arxiv.org/abs/2303.10974v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4", "abstract": "The digitization of healthcare has facilitated the sharing and re-using of\nmedical data but has also raised concerns about confidentiality and privacy.\nHIPAA (Health Insurance Portability and Accountability Act) mandates removing\nre-identifying information before the dissemination of medical records. Thus,\neffective and efficient solutions for de-identifying medical data, especially\nthose in free-text forms, are highly needed. While various computer-assisted\nde-identification methods, including both rule-based and learning-based, have\nbeen developed and used in prior practice, such solutions still lack\ngeneralizability or need to be fine-tuned according to different scenarios,\nsignificantly imposing restrictions in wider use. The advancement of large\nlanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential in\nprocessing text data in the medical domain with zero-shot in-context learning,\nespecially in the task of privacy protection, as these models can identify\nconfidential information by their powerful named entity recognition (NER)\ncapability. In this work, we developed a novel GPT4-enabled de-identification\nframework (``DeID-GPT\") to automatically identify and remove the identifying\ninformation. Compared to existing commonly used medical text data\nde-identification methods, our developed DeID-GPT showed the highest accuracy\nand remarkable reliability in masking private information from the unstructured\nmedical text while preserving the original structure and meaning of the text.\nThis study is one of the earliest to utilize ChatGPT and GPT-4 for medical text\ndata processing and de-identification, which provides insights for further\nresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 in\nhealthcare. Codes and benchmarking data information are available at\nhttps://github.com/yhydhx/ChatGPT-API.", "published": "2023-03-20 11:34:37", "link": "http://arxiv.org/abs/2303.11032v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Evaluating Language Models for Knowledge Base Completion", "abstract": "Structured knowledge bases (KBs) are a foundation of many intelligent\napplications, yet are notoriously incomplete. Language models (LMs) have\nrecently been proposed for unsupervised knowledge base completion (KBC), yet,\ndespite encouraging initial results, questions regarding their suitability\nremain open. Existing evaluations often fall short because they only evaluate\non popular subjects, or sample already existing facts from KBs. In this work,\nwe introduce a novel, more challenging benchmark dataset, and a methodology\ntailored for a realistic assessment of the KBC potential of LMs. For automated\nassessment, we curate a dataset called WD-KNOWN, which provides an unbiased\nrandom sample of Wikidata, containing over 3.9 million facts. In a second step,\nwe perform a human evaluation on predictions that are not yet in the KB, as\nonly this provides real insights into the added value over existing KBs. Our\nkey finding is that biases in dataset conception of previous benchmarks lead to\na systematic overestimate of LM performance for KBC. However, our results also\nreveal strong areas of LMs. We could, for example, perform a significant\ncompletion of Wikidata on the relations nativeLanguage, by a factor of ~21\n(from 260k to 5.8M) at 82% precision, usedLanguage, by a factor of ~2.1 (from\n2.1M to 6.6M) at 82% precision, and citizenOf by a factor of ~0.3 (from 4.2M to\n5.3M) at 90% precision. Moreover, we find that LMs possess surprisingly strong\ngeneralization capabilities: even on relations where most facts were not\ndirectly observed in LM training, prediction quality can be high.", "published": "2023-03-20 13:14:59", "link": "http://arxiv.org/abs/2303.11082v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EVA-02: A Visual Representation for Neon Genesis", "abstract": "We launch EVA-02, a next-generation Transformer-based visual representation\npre-trained to reconstruct strong and robust language-aligned vision features\nvia masked image modeling. With an updated plain Transformer architecture as\nwell as extensive pre-training from an open & accessible giant CLIP vision\nencoder, EVA-02 demonstrates superior performance compared to prior\nstate-of-the-art approaches across various representative vision tasks, while\nutilizing significantly fewer parameters and compute budgets. Notably, using\nexclusively publicly accessible training data, EVA-02 with only 304M parameters\nachieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set.\nAdditionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on\nImageNet-1K, outperforming the previous largest & best open-sourced CLIP with\nonly ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02\nvariants in various model sizes, ranging from 6M to 304M parameters, all with\nimpressive performance. To facilitate open access and open research, we release\nthe complete suite of EVA-02 to the community at\nhttps://github.com/baaivision/EVA/tree/master/EVA-02.", "published": "2023-03-20 17:59:59", "link": "http://arxiv.org/abs/2303.11331v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Mind meets machine: Unravelling GPT-4's cognitive psychology", "abstract": "Cognitive psychology delves on understanding perception, attention, memory,\nlanguage, problem-solving, decision-making, and reasoning. Large language\nmodels (LLMs) are emerging as potent tools increasingly capable of performing\nhuman-level tasks. The recent development in the form of GPT-4 and its\ndemonstrated success in tasks complex to humans exam and complex problems has\nled to an increased confidence in the LLMs to become perfect instruments of\nintelligence. Although GPT-4 report has shown performance on some cognitive\npsychology tasks, a comprehensive assessment of GPT-4, via the existing\nwell-established datasets is required. In this study, we focus on the\nevaluation of GPT-4's performance on a set of cognitive psychology datasets\nsuch as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how\nGPT-4 processes and integrates cognitive psychology with contextual\ninformation, providing insight into the underlying cognitive processes that\nenable its ability to generate the responses. We show that GPT-4 exhibits a\nhigh level of accuracy in cognitive psychology tasks relative to the prior\nstate-of-the-art models. Our results strengthen the already available\nassessments and confidence on GPT-4's cognitive psychology abilities. It has\nsignificant potential to revolutionize the field of AI, by enabling machines to\nbridge the gap between human and machine reasoning.", "published": "2023-03-20 20:28:26", "link": "http://arxiv.org/abs/2303.11436v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Leveraging Foundation Models for Clinical Text Analysis", "abstract": "Infectious diseases are a significant public health concern globally, and\nextracting relevant information from scientific literature can facilitate the\ndevelopment of effective prevention and treatment strategies. However, the\nlarge amount of clinical data available presents a challenge for information\nextraction. To address this challenge, this study proposes a natural language\nprocessing (NLP) framework that uses a pre-trained transformer model fine-tuned\non task-specific data to extract key information related to infectious diseases\nfrom free-text clinical data. The proposed framework includes three components:\na data layer for preparing datasets from clinical texts, a foundation model\nlayer for entity extraction, and an assessment layer for performance analysis.\nThe results of the evaluation indicate that the proposed method outperforms\nstandard methods, and leveraging prior knowledge through the pre-trained\ntransformer model makes it useful for investigating other infectious diseases\nin the future.", "published": "2023-03-20 17:05:13", "link": "http://arxiv.org/abs/2303.13314v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Capabilities of GPT-4 on Medical Challenge Problems", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.", "published": "2023-03-20 16:18:38", "link": "http://arxiv.org/abs/2303.13375v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relate auditory speech to EEG by shallow-deep attention-based network", "abstract": "Electroencephalography (EEG) plays a vital role in detecting how brain\nresponses to different stimulus. In this paper, we propose a novel Shallow-Deep\nAttention-based Network (SDANet) to classify the correct auditory stimulus\nevoking the EEG signal. It adopts the Attention-based Correlation Module (ACM)\nto discover the connection between auditory speech and EEG from global aspect,\nand the Shallow-Deep Similarity Classification Module (SDSCM) to decide the\nclassification result via the embeddings learned from the shallow and deep\nlayers. Moreover, various training strategies and data augmentation are used to\nboost the model robustness. Experiments are conducted on the dataset provided\nby Auditory EEG challenge (ICASSP Signal Processing Grand Challenge 2023).\nResults show that the proposed model has a significant gain over the baseline\non the match-mismatch track.", "published": "2023-03-20 06:34:22", "link": "http://arxiv.org/abs/2303.10897v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Exploring Representation Learning for Small-Footprint Keyword Spotting", "abstract": "In this paper, we investigate representation learning for low-resource\nkeyword spotting (KWS). The main challenges of KWS are limited labeled data and\nlimited available device resources. To address those challenges, we explore\nrepresentation learning for KWS by self-supervised contrastive learning and\nself-training with pretrained model. First, local-global contrastive siamese\nnetworks (LGCSiam) are designed to learn similar utterance-level\nrepresentations for similar audio samplers by proposed local-global contrastive\nloss without requiring ground-truth. Second, a self-supervised pretrained\nWav2Vec 2.0 model is applied as a constraint module (WVC) to force the KWS\nmodel to learn frame-level acoustic representations. By the LGCSiam and WVC\nmodules, the proposed small-footprint KWS model can be pretrained with\nunlabeled data. Experiments on speech commands dataset show that the\nself-training WVC module and the self-supervised LGCSiam module significantly\nimprove accuracy, especially in the case of training on a small labeled\ndataset.", "published": "2023-03-20 07:09:26", "link": "http://arxiv.org/abs/2303.10912v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On-the-fly Text Retrieval for End-to-End ASR Adaptation", "abstract": "End-to-end speech recognition models are improved by incorporating external\ntext sources, typically by fusion with an external language model. Such\nlanguage models have to be retrained whenever the corpus of interest changes.\nFurthermore, since they store the entire corpus in their parameters, rare words\ncan be challenging to recall. In this work, we propose augmenting a\ntransducer-based ASR model with a retrieval language model, which directly\nretrieves from an external text corpus plausible completions for a partial ASR\nhypothesis. These completions are then integrated into subsequent predictions\nby an adapter, which is trained once, so that the corpus of interest can be\nswitched without incurring the computational overhead of retraining. Our\nexperiments show that the proposed model significantly improves the performance\nof a transducer baseline on a pair of question-answering datasets. Further, it\noutperforms shallow fusion on recognition of named entities by about 7\nrelative; when the two are combined, the relative improvement increases to 13%.", "published": "2023-03-20 08:54:40", "link": "http://arxiv.org/abs/2303.10942v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Code-Switching Text Generation and Injection in Mandarin-English ASR", "abstract": "Code-switching speech refers to a means of expression by mixing two or more\nlanguages within a single utterance. Automatic Speech Recognition (ASR) with\nEnd-to-End (E2E) modeling for such speech can be a challenging task due to the\nlack of data. In this study, we investigate text generation and injection for\nimproving the performance of an industry commonly-used streaming model,\nTransformer-Transducer (T-T), in Mandarin-English code-switching speech\nrecognition. We first propose a strategy to generate code-switching text data\nand then investigate injecting generated text into T-T model explicitly by\nText-To-Speech (TTS) conversion or implicitly by tying speech and text latent\nspaces. Experimental results on the T-T model trained with a dataset containing\n1,800 hours of real Mandarin-English code-switched speech show that our\napproaches to inject generated code-switching text significantly boost the\nperformance of T-T models, i.e., 16% relative Token-based Error Rate (TER)\nreduction averaged on three evaluation sets, and the approach of tying speech\nand text latent spaces is superior to that of TTS conversion on the evaluation\nset which contains more homogeneous data with the training set.", "published": "2023-03-20 09:13:27", "link": "http://arxiv.org/abs/2303.10949v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Controllable Ancient Chinese Lyrics Generation Based on Phrase Prototype\n  Retrieving", "abstract": "Generating lyrics and poems is one of the essential downstream tasks in the\nNatural Language Processing (NLP) field. Current methods have performed well in\nsome lyrics generation scenarios but need further improvements in tasks\nrequiring fine control. We propose a novel method for generating ancient\nChinese lyrics (Song Ci), a type of ancient lyrics that involves precise\ncontrol of song structure. The system is equipped with a phrase retriever and a\nphrase connector. Based on an input prompt, the phrase retriever picks phrases\nfrom a database to construct a phrase pool. The phrase connector then selects a\nseries of phrases from the phrase pool that minimizes a multi-term loss\nfunction that considers rhyme, song structure, and fluency. Experimental\nresults show that our method can generate high-quality ancient Chinese lyrics\nwhile performing well on topic and song structure control. We also expect our\napproach to be generalized to other lyrics-generating tasks.", "published": "2023-03-20 10:33:06", "link": "http://arxiv.org/abs/2303.11005v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cocktail HuBERT: Generalized Self-Supervised Pre-training for Mixture\n  and Single-Source Speech", "abstract": "Self-supervised learning leverages unlabeled data effectively, improving\nlabel efficiency and generalization to domains without labeled data. While\nrecent work has studied generalization to more acoustic/linguistic domains,\nlanguages, and modalities, these investigations are limited to single-source\nspeech with one primary speaker in the recording. This paper presents Cocktail\nHuBERT, a self-supervised learning framework that generalizes to mixture speech\nusing a masked pseudo source separation objective. This objective encourages\nthe model to identify the number of sources, separate and understand the\ncontext, and infer the content of masked regions represented as discovered\nunits. Cocktail HuBERT outperforms state-of-the-art results with 69% lower WER\non multi-speaker ASR, 31% lower DER on diarization, and is competitive on\nsingle- and multi-speaker tasks from SUPERB.", "published": "2023-03-20 14:07:13", "link": "http://arxiv.org/abs/2303.11131v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Reflexion: Language Agents with Verbal Reinforcement Learning", "abstract": "Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.", "published": "2023-03-20 18:08:50", "link": "http://arxiv.org/abs/2303.11366v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", "abstract": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\nvision experts to achieve multimodal reasoning and action. In this paper, we\ndefine and explore a comprehensive list of advanced vision tasks that are\nintriguing to solve, but may exceed the capabilities of existing vision and\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\nintroduces a textual prompt design that can represent text descriptions,\ntextualized spatial coordinates, and aligned file names for dense visual\nsignals such as images and videos. MM-REACT's prompt design allows language\nmodels to accept, associate, and process multimodal information, thereby\nfacilitating the synergetic combination of ChatGPT and various vision experts.\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\nspecified capabilities of interests and its wide application in different\nscenarios that require advanced visual understanding. Furthermore, we discuss\nand compare MM-REACT's system paradigm with an alternative approach that\nextends language models for multimodal scenarios through joint finetuning.\nCode, demo, video, and visualization are available at\nhttps://multimodal-react.github.io/", "published": "2023-03-20 18:31:47", "link": "http://arxiv.org/abs/2303.11381v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "eP-ALM: Efficient Perceptual Augmentation of Language Models", "abstract": "Large Language Models (LLMs) have so far impressed the world, with\nunprecedented capabilities that emerge in models at large scales. On the vision\nside, transformer models (i.e., ViT) are following the same trend, achieving\nthe best performance on challenging benchmarks. With the abundance of such\nunimodal models, a natural question arises; do we need also to follow this\ntrend to tackle multimodal tasks? In this work, we propose to rather direct\neffort to efficient adaptations of existing models, and propose to augment\nLanguage Models with perception. Existing approaches for adapting pretrained\nmodels for vision-language tasks still rely on several key components that\nhinder their efficiency. In particular, they still train a large number of\nparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)\ntrained on huge image-text datasets, and add significant inference overhead. In\naddition, most of these approaches have focused on Zero-Shot and In Context\nLearning, with little to no effort on direct finetuning. We investigate the\nminimal computational effort needed to adapt unimodal models for multimodal\ntasks and propose a new challenging setup, alongside different approaches, that\nefficiently adapts unimodal pretrained models. We show that by freezing more\nthan 99% of total parameters, training only one linear projection layer, and\nprepending only one trainable token, our approach (dubbed eP-ALM) significantly\noutperforms other baselines on VQA and Captioning across Image, Video, and\nAudio modalities, following the proposed setup. The code is available here:\nhttps://github.com/mshukor/eP-ALM.", "published": "2023-03-20 19:20:34", "link": "http://arxiv.org/abs/2303.11403v4", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Large Language Models and Simple, Stupid Bugs", "abstract": "With the advent of powerful neural language models, AI-based systems to\nassist developers in coding tasks are becoming widely available; Copilot is one\nsuch system. Copilot uses Codex, a large language model (LLM), to complete code\nconditioned on a preceding \"prompt\". Codex, however, is trained on public\nGitHub repositories, viz., on code that may include bugs and vulnerabilities.\nPrevious studies [1], [2] show Codex reproduces vulnerabilities seen in\ntraining. In this study, we examine how prone Codex is to generate an\ninteresting bug category, single statement bugs, commonly referred to as\nsimple, stupid bugs or SStuBs in the MSR community. We find that Codex and\nsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs\nas much as 2x as likely than known, verbatim correct code. We explore the\nconsequences of the Codex generated SStuBs and propose avoidance strategies\nthat suggest the possibility of reducing the production of known, verbatim\nSStubs, and increase the possibility of producing known, verbatim fixes.", "published": "2023-03-20 21:14:06", "link": "http://arxiv.org/abs/2303.11455v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and\n  Cross-Attention", "abstract": "Massive rumors usually appear along with breaking news or trending topics,\nseriously hindering the truth. Existing rumor detection methods are mostly\nfocused on the same domain, and thus have poor performance in cross-domain\nscenarios due to domain shift. In this work, we propose an end-to-end\ninstance-wise and prototype-wise contrastive learning model with a\ncross-attention mechanism for cross-domain rumor detection. The model not only\nperforms cross-domain feature alignment but also enforces target samples to\nalign with the corresponding prototypes of a given source domain. Since target\nlabels in a target domain are unavailable, we use a clustering-based approach\nwith carefully initialized centers by a batch of source domain samples to\nproduce pseudo labels. Moreover, we use a cross-attention mechanism on a pair\nof source data and target data with the same labels to learn domain-invariant\nrepresentations. Because the samples in a domain pair tend to express similar\nsemantic patterns, especially on the people's attitudes (e.g., supporting or\ndenying) towards the same category of rumors, the discrepancy between a pair of\nthe source domain and target domain will be decreased. We conduct experiments\non four groups of cross-domain datasets and show that our proposed model\nachieves state-of-the-art performance.", "published": "2023-03-20 06:19:49", "link": "http://arxiv.org/abs/2303.11945v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "The Multimodal And Modular Ai Chef: Complex Recipe Generation From\n  Imagery", "abstract": "The AI community has embraced multi-sensory or multi-modal approaches to\nadvance this generation of AI models to resemble expected intelligent\nunderstanding. Combining language and imagery represents a familiar method for\nspecific tasks like image captioning or generation from descriptions. This\npaper compares these monolithic approaches to a lightweight and specialized\nmethod based on employing image models to label objects, then serially\nsubmitting this resulting object list to a large language model (LLM). This use\nof multiple Application Programming Interfaces (APIs) enables better than 95%\nmean average precision for correct object lists, which serve as input to the\nlatest Open AI text generator (GPT-4). To demonstrate the API as a modular\nalternative, we solve the problem of a user taking a picture of ingredients\navailable in a refrigerator, then generating novel recipe cards tailored to\ncomplex constraints on cost, preparation time, dietary restrictions, portion\nsizes, and multiple meal plans. The research concludes that monolithic\nmultimodal models currently lack the coherent memory to maintain context and\nformat for this task and that until recently, the language models like GPT-2/3\nstruggled to format similar problems without degenerating into repetitive or\nnon-sensical combinations of ingredients. For the first time, an AI chef or\ncook seems not only possible but offers some enhanced capabilities to augment\nhuman recipe libraries in pragmatic ways. The work generates a 100-page recipe\nbook featuring the thirty top ingredients using over 2000 refrigerator images\nas initializing lists.", "published": "2023-03-20 01:57:52", "link": "http://arxiv.org/abs/2304.02016v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation from Multiple Foundation Models for End-to-End\n  Speech Recognition", "abstract": "Although large foundation models pre-trained by self-supervised learning have\nachieved state-of-the-art performance in many tasks including automatic speech\nrecognition (ASR), knowledge distillation (KD) is often required in practice to\ntransfer the knowledge learned by large teacher models into much smaller\nstudent models with affordable computation and memory costs. This paper\nproposes a novel two-stage KD framework to distil the knowledge from multiple\nspeech foundation models as teachers into a single student neural transducer\nmodel for ASR. In the first stage, the student model encoder is pre-trained\nusing the embeddings extracted from multiple teacher models. In the second\nstage, the student encoder is fine-tuned with the audio-text pairs based on the\nASR task. Experiments on the LibriSpeech 100-hour subset show that the proposed\nKD framework improves the performance of both streaming and non-streaming\nstudent models when using only one teacher. The performance of the student\nmodel can be further enhanced when multiple teachers are used jointly,\nachieving word error rate reductions (WERRs) of 17.5% and 10.6%. Our proposed\nframework can be combined with other existing KD methods to achieve further\nimprovements. Further WERRs were obtained by incorporating extra unlabelled\ndata during encoder pre-training, leading to a total relative WERR of 55.0% on\nthe non-streaming student model.", "published": "2023-03-20 07:18:18", "link": "http://arxiv.org/abs/2303.10917v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Approaching an unknown communication system by latent space exploration\n  and causal inference", "abstract": "This paper proposes a methodology for discovering meaningful properties in\ndata by exploring the latent space of unsupervised deep generative models. We\ncombine manipulation of individual latent variables to extreme values with\nmethods inspired by causal inference into an approach we call causal\ndisentanglement with extreme values (CDEV) and show that this method yields\ninsights for model interpretability. With this, we can test for what properties\nof unknown data the model encodes as meaningful, using it to glean insight into\nthe communication system of sperm whales (Physeter macrocephalus), one of the\nmost intriguing and understudied animal communication systems. The network\narchitecture used has been shown to learn meaningful representations of speech;\nhere, it is used as a learning mechanism to decipher the properties of another\nvocal communication system in which case we have no ground truth. The proposed\nmethodology suggests that sperm whales encode information using the number of\nclicks in a sequence, the regularity of their timing, and audio properties such\nas the spectral mean and the acoustic regularity of the sequences. Some of\nthese findings are consistent with existing hypotheses, while others are\nproposed for the first time. We also argue that our models uncover rules that\ngovern the structure of units in the communication system and apply them while\ngenerating innovative data not shown during training. This paper suggests that\nan interpretation of the outputs of deep neural networks with causal inference\nmethodology can be a viable strategy for approaching data about which little is\nknown and presents another case of how deep learning can limit the hypothesis\nspace. Finally, the proposed approach can be extended to other architectures\nand datasets.", "published": "2023-03-20 08:09:13", "link": "http://arxiv.org/abs/2303.10931v2", "categories": ["stat.ML", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "stat.ML"}
{"title": "DS-TDNN: Dual-stream Time-delay Neural Network with Global-aware Filter\n  for Speaker Verification", "abstract": "Conventional time-delay neural networks (TDNNs) struggle to handle long-range\ncontext, their ability to represent speaker information is therefore limited in\nlong utterances. Existing solutions either depend on increasing model\ncomplexity or try to balance between local features and global context to\naddress this issue. To effectively leverage the long-term dependencies of audio\nsignals and constrain model complexity, we introduce a novel module called\nGlobal-aware Filter layer (GF layer) in this work, which employs a set of\nlearnable transform-domain filters between a 1D discrete Fourier transform and\nits inverse transform to capture global context. Additionally, we develop a\ndynamic filtering strategy and a sparse regularization method to enhance the\nperformance of the GF layer and prevent overfitting. Based on the GF layer, we\npresent a dual-stream TDNN architecture called DS-TDNN for automatic speaker\nverification (ASV), which utilizes two unique branches to extract both local\nand global features in parallel and employs an efficient strategy to fuse\ndifferent-scale information. Experiments on the Voxceleb and SITW databases\ndemonstrate that the DS-TDNN achieves a relative improvement of 10\\% together\nwith a relative decline of 20\\% in computational cost over the ECAPA-TDNN in\nspeaker verification task. This improvement will become more evident as the\nutterance's duration grows. Furthermore, the DS-TDNN also beats popular deep\nresidual models and attention-based systems on utterances of arbitrary length.", "published": "2023-03-20 10:58:12", "link": "http://arxiv.org/abs/2303.11020v3", "categories": ["cs.SD", "cs.AI", "eess.AS", "68", "I.2.1"], "primary_category": "cs.SD"}
{"title": "EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation", "abstract": "Speech-driven 3D face animation aims to generate realistic facial expressions\nthat match the speech content and emotion. However, existing methods often\nneglect emotional facial expressions or fail to disentangle them from speech\ncontent. To address this issue, this paper proposes an end-to-end neural\nnetwork to disentangle different emotions in speech so as to generate rich 3D\nfacial expressions. Specifically, we introduce the emotion disentangling\nencoder (EDE) to disentangle the emotion and content in the speech by\ncross-reconstructed speech signals with different emotion labels. Then an\nemotion-guided feature fusion decoder is employed to generate a 3D talking face\nwith enhanced emotion. The decoder is driven by the disentangled identity,\nemotional, and content embeddings so as to generate controllable personal and\nemotional styles. Finally, considering the scarcity of the 3D emotional talking\nface data, we resort to the supervision of facial blendshapes, which enables\nthe reconstruction of plausible 3D faces from 2D emotional data, and contribute\na large-scale 3D emotional talking face dataset (3D-ETF) to train the network.\nOur experiments and user studies demonstrate that our approach outperforms\nstate-of-the-art methods and exhibits more diverse facial movements. We\nrecommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/emotalk", "published": "2023-03-20 13:22:04", "link": "http://arxiv.org/abs/2303.11089v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Sound Localization from Motion: Jointly Learning Sound Direction and\n  Camera Rotation", "abstract": "The images and sounds that we perceive undergo subtle but geometrically\nconsistent changes as we rotate our heads. In this paper, we use these cues to\nsolve a problem we call Sound Localization from Motion (SLfM): jointly\nestimating camera rotation and localizing sound sources. We learn to solve\nthese tasks solely through self-supervision. A visual model predicts camera\nrotation from a pair of images, while an audio model predicts the direction of\nsound sources from binaural sounds. We train these models to generate\npredictions that agree with one another. At test time, the models can be\ndeployed independently. To obtain a feature representation that is well-suited\nto solving this challenging problem, we also propose a method for learning an\naudio-visual representation through cross-view binauralization: estimating\nbinaural sound from one view, given images and sound from another. Our model\ncan successfully estimate accurate rotations on both real and synthetic scenes,\nand localize sound sources with accuracy competitive with state-of-the-art\nself-supervised approaches. Project site: https://ificl.github.io/SLfM/", "published": "2023-03-20 17:59:55", "link": "http://arxiv.org/abs/2303.11329v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
