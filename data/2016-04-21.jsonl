{"title": "Chinese Song Iambics Generation with Neural Attention-based Model", "abstract": "Learning and generating Chinese poems is a charming yet challenging task.\nTraditional approaches involve various language modeling and machine\ntranslation techniques, however, they perform not as well when generating poems\nwith complex pattern constraints, for example Song iambics, a famous type of\npoems that involve variable-length sentences and strict rhythmic patterns. This\npaper applies the attention-based sequence-to-sequence model to generate\nChinese Song iambics. Specifically, we encode the cue sentences by a\nbi-directional Long-Short Term Memory (LSTM) model and then predict the entire\niambic with the information provided by the encoder, in the form of an\nattention-based LSTM that can regularize the generation process by the fine\nstructure of the input cues. Several techniques are investigated to improve the\nmodel, including global context integration, hybrid style training, character\nvector initialization and adaptation. Both the automatic and subjective\nevaluation results show that our model indeed can learn the complex structural\nand rhythmic patterns of Song iambics, and the generation is rather successful.", "published": "2016-04-21 12:25:04", "link": "http://arxiv.org/abs/1604.06274v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Approach to Dropped Pronoun Translation", "abstract": "Dropped Pronouns (DP) in which pronouns are frequently dropped in the source\nlanguage but should be retained in the target language are challenge in machine\ntranslation. In response to this problem, we propose a semi-supervised approach\nto recall possibly missing pronouns in the translation. Firstly, we build\ntraining data for DP generation in which the DPs are automatically labelled\naccording to the alignment information from a parallel corpus. Secondly, we\nbuild a deep learning-based DP generator for input sentences in decoding when\nno corresponding references exist. More specifically, the generation is\ntwo-phase: (1) DP position detection, which is modeled as a sequential\nlabelling task with recurrent neural networks; and (2) DP prediction, which\nemploys a multilayer perceptron with rich features. Finally, we integrate the\nabove outputs into our translation system to recall missing pronouns by both\nextracting rules from the DP-labelled training data and translating the\nDP-generated input sentences. Experimental results show that our approach\nachieves a significant improvement of 1.58 BLEU points in translation\nperformance with 66% F-score for DP generation accuracy.", "published": "2016-04-21 12:55:29", "link": "http://arxiv.org/abs/1604.06285v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Row-less Universal Schema", "abstract": "Universal schema jointly embeds knowledge bases and textual patterns to\nreason about entities and relations for automatic knowledge base construction\nand information extraction. In the past, entity pairs and relations were\nrepresented as learned vectors with compatibility determined by a scoring\nfunction, limiting generalization to unseen text patterns and entities.\nRecently, 'column-less' versions of Universal Schema have used compositional\npattern encoders to generalize to all text patterns. In this work we take the\nnext step and propose a 'row-less' model of universal schema, removing explicit\nentity pair representations. Instead of learning vector representations for\neach entity pair in our training set, we treat an entity pair as a function of\nits relation types. In experimental results on the FB15k-237 benchmark we\ndemonstrate that we can match the performance of a comparable model with\nexplicit entity pair representations using a model of attention over relation\ntypes. We further demonstrate that the model per- forms with nearly the same\naccuracy on entity pairs never seen during training.", "published": "2016-04-21 15:39:01", "link": "http://arxiv.org/abs/1604.06361v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OCR Error Correction Using Character Correction and Feature-Based Word\n  Classification", "abstract": "This paper explores the use of a learned classifier for post-OCR text\ncorrection. Experiments with the Arabic language show that this approach, which\nintegrates a weighted confusion matrix and a shallow language model, improves\nthe vast majority of segmentation and recognition errors, the most frequent\ntypes of error on our dataset.", "published": "2016-04-21 09:25:11", "link": "http://arxiv.org/abs/1604.06225v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
