{"title": "Pretrained LLM Adapted with LoRA as a Decision Transformer for Offline RL in Quantitative Trading", "abstract": "Developing effective quantitative trading strategies using reinforcement\nlearning (RL) is challenging due to the high risks associated with online\ninteraction with live financial markets. Consequently, offline RL, which\nleverages historical market data without additional exploration, becomes\nessential. However, existing offline RL methods often struggle to capture the\ncomplex temporal dependencies inherent in financial time series and may overfit\nto historical patterns. To address these challenges, we introduce a Decision\nTransformer (DT) initialized with pre-trained GPT-2 weights and fine-tuned\nusing Low-Rank Adaptation (LoRA). This architecture leverages the\ngeneralization capabilities of pre-trained language models and the efficiency\nof LoRA to learn effective trading policies from expert trajectories solely\nfrom historical data. Our model performs competitively with established offline\nRL algorithms, including Conservative Q-Learning (CQL), Implicit Q-Learning\n(IQL), and Behavior Cloning (BC), as well as a baseline Decision Transformer\nwith randomly initialized GPT-2 weights and LoRA. Empirical results demonstrate\nthat our approach effectively learns from expert trajectories and secures\nsuperior rewards in certain trading scenarios, highlighting the effectiveness\nof integrating pre-trained language models and parameter-efficient fine-tuning\nin offline RL for quantitative trading. Replication code for our experiments is\npublicly available at https://github.com/syyunn/finrl-dt", "published": "2024-11-26 21:31:58", "link": "http://arxiv.org/abs/2411.17900v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Joint Combinatorial Node Selection and Resource Allocations in the Lightning Network using Attention-based Reinforcement Learning", "abstract": "The Lightning Network (LN) has emerged as a second-layer solution to\nBitcoin's scalability challenges. The rise of Payment Channel Networks (PCNs)\nand their specific mechanisms incentivize individuals to join the network for\nprofit-making opportunities. According to the latest statistics, the total\nvalue locked within the Lightning Network is approximately \\$500 million.\nMeanwhile, joining the LN with the profit-making incentives presents several\nobstacles, as it involves solving a complex combinatorial problem that\nencompasses both discrete and continuous control variables related to node\nselection and resource allocation, respectively. Current research inadequately\ncaptures the critical role of resource allocation and lacks realistic\nsimulations of the LN routing mechanism. In this paper, we propose a Deep\nReinforcement Learning (DRL) framework, enhanced by the power of transformers,\nto address the Joint Combinatorial Node Selection and Resource Allocation\n(JCNSRA) problem. We have improved upon an existing environment by introducing\nmodules that enhance its routing mechanism, thereby narrowing the gap with the\nactual LN routing system and ensuring compatibility with the JCNSRA problem. We\ncompare our model against several baselines and heuristics, demonstrating its\nsuperior performance across various settings. Additionally, we address concerns\nregarding centralization in the LN by deploying our agent within the network\nand monitoring the centrality measures of the evolved graph. Our findings\nsuggest not only an absence of conflict between LN's decentralization goals and\nindividuals' revenue-maximization incentives but also a positive association\nbetween the two.", "published": "2024-11-26 11:56:19", "link": "http://arxiv.org/abs/2411.17353v1", "categories": ["cs.LG", "q-fin.CP", "I.2.6; I.2.8"], "primary_category": "cs.LG"}
