{"title": "Pretrained LLM Adapted with LoRA as a Decision Transformer for Offline RL in Quantitative Trading", "abstract": "Developing effective quantitative trading strategies using reinforcement\nlearning (RL) is challenging due to the high risks associated with online\ninteraction with live financial markets. Consequently, offline RL, which\nleverages historical market data without additional exploration, becomes\nessential. However, existing offline RL methods often struggle to capture the\ncomplex temporal dependencies inherent in financial time series and may overfit\nto historical patterns. To address these challenges, we introduce a Decision\nTransformer (DT) initialized with pre-trained GPT-2 weights and fine-tuned\nusing Low-Rank Adaptation (LoRA). This architecture leverages the\ngeneralization capabilities of pre-trained language models and the efficiency\nof LoRA to learn effective trading policies from expert trajectories solely\nfrom historical data. Our model performs competitively with established offline\nRL algorithms, including Conservative Q-Learning (CQL), Implicit Q-Learning\n(IQL), and Behavior Cloning (BC), as well as a baseline Decision Transformer\nwith randomly initialized GPT-2 weights and LoRA. Empirical results demonstrate\nthat our approach effectively learns from expert trajectories and secures\nsuperior rewards in certain trading scenarios, highlighting the effectiveness\nof integrating pre-trained language models and parameter-efficient fine-tuning\nin offline RL for quantitative trading. Replication code for our experiments is\npublicly available at https://github.com/syyunn/finrl-dt", "published": "2024-11-26 21:31:58", "link": "http://arxiv.org/abs/2411.17900v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Joint Combinatorial Node Selection and Resource Allocations in the Lightning Network using Attention-based Reinforcement Learning", "abstract": "The Lightning Network (LN) has emerged as a second-layer solution to\nBitcoin's scalability challenges. The rise of Payment Channel Networks (PCNs)\nand their specific mechanisms incentivize individuals to join the network for\nprofit-making opportunities. According to the latest statistics, the total\nvalue locked within the Lightning Network is approximately \\$500 million.\nMeanwhile, joining the LN with the profit-making incentives presents several\nobstacles, as it involves solving a complex combinatorial problem that\nencompasses both discrete and continuous control variables related to node\nselection and resource allocation, respectively. Current research inadequately\ncaptures the critical role of resource allocation and lacks realistic\nsimulations of the LN routing mechanism. In this paper, we propose a Deep\nReinforcement Learning (DRL) framework, enhanced by the power of transformers,\nto address the Joint Combinatorial Node Selection and Resource Allocation\n(JCNSRA) problem. We have improved upon an existing environment by introducing\nmodules that enhance its routing mechanism, thereby narrowing the gap with the\nactual LN routing system and ensuring compatibility with the JCNSRA problem. We\ncompare our model against several baselines and heuristics, demonstrating its\nsuperior performance across various settings. Additionally, we address concerns\nregarding centralization in the LN by deploying our agent within the network\nand monitoring the centrality measures of the evolved graph. Our findings\nsuggest not only an absence of conflict between LN's decentralization goals and\nindividuals' revenue-maximization incentives but also a positive association\nbetween the two.", "published": "2024-11-26 11:56:19", "link": "http://arxiv.org/abs/2411.17353v1", "categories": ["cs.LG", "q-fin.CP", "I.2.6; I.2.8"], "primary_category": "cs.LG"}
{"title": "Don't Command, Cultivate: An Exploratory Study of System-2 Alignment", "abstract": "The o1 system card identifies the o1 models as the most robust within OpenAI,\nwith their defining characteristic being the progression from rapid, intuitive\nthinking to slower, more deliberate reasoning. This observation motivated us to\ninvestigate the influence of System-2 thinking patterns on model safety. In our\npreliminary research, we conducted safety evaluations of the o1 model,\nincluding complex jailbreak attack scenarios using adversarial natural language\nprompts and mathematical encoding prompts. Our findings indicate that the o1\nmodel demonstrates relatively improved safety performance; however, it still\nexhibits vulnerabilities, particularly against jailbreak attacks employing\nmathematical encoding. Through detailed case analysis, we identified specific\npatterns in the o1 model's responses. We also explored the alignment of\nSystem-2 safety in open-source models using prompt engineering and supervised\nfine-tuning techniques. Experimental results show that some simple methods to\nencourage the model to carefully scrutinize user requests are beneficial for\nmodel safety. Additionally, we proposed a implementation plan for process\nsupervision to enhance safety alignment. The implementation details and\nexperimental results will be provided in future versions.", "published": "2024-11-26 03:27:43", "link": "http://arxiv.org/abs/2411.17075v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Novel Word Pair-based Gaussian Sentence Similarity Algorithm For\n  Bengali Extractive Text Summarization", "abstract": "Extractive Text Summarization is the process of selecting the most\nrepresentative parts of a larger text without losing any key information.\nRecent attempts at extractive text summarization in Bengali, either relied on\nstatistical techniques like TF-IDF or used naive sentence similarity measures\nlike the word averaging technique. All of these strategies suffer from\nexpressing semantic relationships correctly. Here, we propose a novel Word\npair-based Gaussian Sentence Similarity (WGSS) algorithm for calculating the\nsemantic relation between two sentences. WGSS takes the geometric means of\nindividual Gaussian similarity values of word embedding vectors to get the\nsemantic relationship between sentences. It compares two sentences on a\nword-to-word basis which rectifies the sentence representation problem faced by\nthe word averaging method. The summarization process extracts key sentences by\ngrouping semantically similar sentences into clusters using the Spectral\nClustering algorithm. After clustering, we use TF-IDF ranking to pick the best\nsentence from each cluster. The proposed method is validated using four\ndifferent datasets, and it outperformed other recent models by 43.2% on average\nROUGE scores (ranging from 2.5% to 95.4%). It is also experimented on other\nlow-resource languages i.e. Turkish, Marathi, and Hindi language, where we find\nthat the proposed method performs as similar as Bengali for these languages. In\naddition, a new high-quality Bengali dataset is curated which contains 250\narticles and a pair of summaries for each of them. We believe this research is\na crucial addition to Bengali Natural Language Processing (NLP) research and it\ncan easily be extended into other low-resource languages. We made the\nimplementation of the proposed model and data public on\nhttps://github.com/FMOpee/WGSS.", "published": "2024-11-26 07:42:16", "link": "http://arxiv.org/abs/2411.17181v2", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Attempt to Develop a Neural Parser based on Simplified Head-Driven\n  Phrase Structure Grammar on Vietnamese", "abstract": "In this paper, we aimed to develop a neural parser for Vietnamese based on\nsimplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora,\nVietTreebank and VnDT, had around 15% of constituency and dependency tree pairs\nthat did not adhere to simplified HPSG rules. To attempt to address the issue\nof the corpora not adhering to simplified HPSG rules, we randomly permuted\nsamples from the training and development sets to make them compliant with\nsimplified HPSG. We then modified the first simplified HPSG Neural Parser for\nthe Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which\ncan encode Vietnamese texts. We conducted experiments on our modified\nVietTreebank and VnDT corpora. Our extensive experiments showed that the\nsimplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82%\nfor constituency parsing when using the same predicted part-of-speech (POS)\ntags as the self-attentive constituency parser. Additionally, it outperformed\nprevious studies in dependency parsing with a higher Unlabeled Attachment Score\n(UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores\nlikely due to our focus on arc permutation without changing the original\nlabels, as we did not consult with a linguistic expert. Lastly, the research\nfindings of this paper suggest that simplified HPSG should be given more\nattention to linguistic expert when developing treebanks for Vietnamese natural\nlanguage processing.", "published": "2024-11-26 09:46:32", "link": "http://arxiv.org/abs/2411.17270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Extractive-Abstractive Spectrum: Uncovering Verifiability Trade-offs\n  in LLM Generations", "abstract": "Across all fields of academic study, experts cite their sources when sharing\ninformation. While large language models (LLMs) excel at synthesizing\ninformation, they do not provide reliable citation to sources, making it\ndifficult to trace and verify the origins of the information they present. In\ncontrast, search engines make sources readily accessible to users and place the\nburden of synthesizing information on the user. Through a survey, we find that\nusers prefer search engines over LLMs for high-stakes queries, where concerns\nregarding information provenance outweigh the perceived utility of LLM\nresponses. To examine the interplay between verifiability and utility of\ninformation-sharing tools, we introduce the extractive-abstractive spectrum, in\nwhich search engines and LLMs are extreme endpoints encapsulating multiple\nunexplored intermediate operating points. Search engines are extractive because\nthey respond to queries with snippets of sources with links (citations) to the\noriginal webpages. LLMs are abstractive because they address queries with\nanswers that synthesize and logically transform relevant information from\ntraining and in-context sources without reliable citation. We define five\noperating points that span the extractive-abstractive spectrum and conduct\nhuman evaluations on seven systems across four diverse query distributions that\nreflect real-world QA settings: web search, language simplification, multi-step\nreasoning, and medical advice. As outputs become more abstractive, we find that\nperceived utility improves by as much as 200%, while the proportion of properly\ncited sentences decreases by as much as 50% and users take up to 3 times as\nlong to verify cited information. Our findings recommend distinct operating\npoints for domain-specific LLM systems and our failure analysis informs\napproaches to high-utility LLM systems that empower users to verify\ninformation.", "published": "2024-11-26 12:34:52", "link": "http://arxiv.org/abs/2411.17375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Mind, Many Tongues: A Deep Dive into Language-Agnostic Knowledge\n  Neurons in Large Language Models", "abstract": "Large language models (LLMs) have learned vast amounts of factual knowledge\nthrough self-supervised pre-training on large-scale corpora. Meanwhile, LLMs\nhave also demonstrated excellent multilingual capabilities, which can express\nthe learned knowledge in multiple languages. However, the knowledge storage\nmechanism in LLMs still remains mysterious. Some researchers attempt to\ndemystify the factual knowledge in LLMs from the perspective of knowledge\nneurons, and subsequently discover language-agnostic knowledge neurons that\nstore factual knowledge in a form that transcends language barriers. However,\nthe preliminary finding suffers from two limitations: 1) High Uncertainty in\nLocalization Results. Existing study only uses a prompt-based probe to localize\nknowledge neurons for each fact, while LLMs cannot provide consistent answers\nfor semantically equivalent queries. Thus, it leads to inaccurate localization\nresults with high uncertainty. 2) Lack of Analysis in More Languages. The study\nonly analyzes language-agnostic knowledge neurons on English and Chinese data,\nwithout exploring more language families and languages. Naturally, it limits\nthe generalizability of the findings. To address aforementioned problems, we\nfirst construct a new benchmark called Rephrased Multilingual LAMA (RML-LAMA),\nwhich contains high-quality cloze-style multilingual parallel queries for each\nfact. Then, we propose a novel method named Multilingual Integrated Gradients\nwith Uncertainty Estimation (MATRICE), which quantifies the uncertainty across\nqueries and languages during knowledge localization. Extensive experiments show\nthat our method can accurately localize language-agnostic knowledge neurons. We\nalso further investigate the role of language-agnostic knowledge neurons in\ncross-lingual knowledge editing, knowledge enhancement and new knowledge\ninjection.", "published": "2024-11-26 13:03:49", "link": "http://arxiv.org/abs/2411.17401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Stupid robot, I want to speak to a human!\" User Frustration Detection\n  in Task-Oriented Dialog Systems", "abstract": "Detecting user frustration in modern-day task-oriented dialog (TOD) systems\nis imperative for maintaining overall user satisfaction, engagement, and\nretention. However, most recent research is focused on sentiment and emotion\ndetection in academic settings, thus failing to fully encapsulate implications\nof real-world user data. To mitigate this gap, in this work, we focus on user\nfrustration in a deployed TOD system, assessing the feasibility of\nout-of-the-box solutions for user frustration detection. Specifically, we\ncompare the performance of our deployed keyword-based approach, open-source\napproaches to sentiment analysis, dialog breakdown detection methods, and\nemerging in-context learning LLM-based detection. Our analysis highlights the\nlimitations of open-source methods for real-world frustration detection, while\ndemonstrating the superior performance of the LLM-based approach, achieving a\n16\\% relative improvement in F1 score on an internal benchmark. Finally, we\nanalyze advantages and limitations of our methods and provide an insight into\nuser frustration detection task for industry practitioners.", "published": "2024-11-26 13:51:48", "link": "http://arxiv.org/abs/2411.17437v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code\n  Search", "abstract": "Low isotropy in an embedding space impairs performance on tasks involving\nsemantic inference. Our study investigates the impact of isotropy on semantic\ncode search performance and explores post-processing techniques to mitigate\nthis issue. We analyze various code language models, examine isotropy in their\nembedding spaces, and its influence on search effectiveness. We propose a\nmodified ZCA whitening technique to control isotropy levels in embeddings. Our\nresults demonstrate that Soft-ZCA whitening improves the performance of\npre-trained code language models and can complement contrastive fine-tuning.", "published": "2024-11-26 15:53:28", "link": "http://arxiv.org/abs/2411.17538v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How do Multimodal Foundation Models Encode Text and Speech? An Analysis\n  of Cross-Lingual and Cross-Modal Representations", "abstract": "Multimodal foundation models aim to create a unified representation space\nthat abstracts away from surface features like language syntax or modality\ndifferences. To investigate this, we study the internal representations of\nthree recent models, analyzing the model activations from semantically\nequivalent sentences across languages in the text and speech modalities. Our\nfindings reveal that: 1) Cross-modal representations converge over model\nlayers, except in the initial layers specialized at text and speech processing.\n2) Length adaptation is crucial for reducing the cross-modal gap between text\nand speech, although current approaches' effectiveness is primarily limited to\nhigh-resource languages. 3) Speech exhibits larger cross-lingual differences\nthan text. 4) For models not explicitly trained for modality-agnostic\nrepresentations, the modality gap is more prominent than the language gap.", "published": "2024-11-26 18:29:11", "link": "http://arxiv.org/abs/2411.17666v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with\n  Receptive-Field-Aware Attention Weighting", "abstract": "Understanding the emotions in a dialogue usually requires external knowledge\nto accurately understand the contents. As the LLMs become more and more\npowerful, we do not want to settle on the limited ability of the pre-trained\nlanguage model. However, the LLMs either can only process text modality or are\ntoo expensive to process the multimedia information. We aim to utilize both the\npower of LLMs and the supplementary features from the multimedia modalities. In\nthis paper, we present a framework, Lantern, that can improve the performance\nof a certain vanilla model by prompting large language models with\nreceptive-field-aware attention weighting. This framework trained a multi-task\nvanilla model to produce probabilities of emotion classes and dimension scores.\nThese predictions are fed into the LLMs as references to adjust the predicted\nprobabilities of each emotion class with its external knowledge and contextual\nunderstanding. We slice the dialogue into different receptive fields, and each\nsample is included in exactly t receptive fields. Finally, the predictions of\nLLMs are merged with a receptive-field-aware attention-driven weighting module.\nIn the experiments, vanilla models CORECT and SDT are deployed in Lantern with\nGPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way\nsettings demonstrated that the Lantern can significantly improve the\nperformance of current vanilla models by up to 1.23% and 1.80%.", "published": "2024-11-26 18:35:24", "link": "http://arxiv.org/abs/2411.17674v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Character-Level Understanding in LLMs through Token Internal\n  Structure Learning", "abstract": "Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness.", "published": "2024-11-26 18:44:39", "link": "http://arxiv.org/abs/2411.17679v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats", "abstract": "As large language models (LLMs) become increasingly capable, it is prudent to\nassess whether safety measures remain effective even if LLMs intentionally try\nto bypass them. Previous work introduced control evaluations, an adversarial\nframework for testing deployment strategies of untrusted models (i.e., models\nwhich might be trying to bypass safety measures). While prior work treats a\nsingle failure as unacceptable, we perform control evaluations in a\n\"distributed threat setting\" -- a setting where no single action is\ncatastrophic and no single action provides overwhelming evidence of\nmisalignment. We approach this problem with a two-level deployment framework\nthat uses an adaptive macro-protocol to choose between micro-protocols.\nMicro-protocols operate on a single task, using a less capable, but extensively\ntested (trusted) model to harness and monitor the untrusted model. Meanwhile,\nthe macro-protocol maintains an adaptive credence on the untrusted model's\nalignment based on its past actions, using it to pick between safer and riskier\nmicro-protocols. We evaluate our method in a code generation testbed where a\nred team attempts to generate subtly backdoored code with an LLM whose\ndeployment is safeguarded by a blue team. We plot Pareto frontiers of safety (#\nof non-backdoored solutions) and usefulness (# of correct solutions). At a\ngiven level of usefulness, our adaptive deployment strategy reduces the number\nof backdoors by 80% compared to non-adaptive baselines.", "published": "2024-11-26 18:58:20", "link": "http://arxiv.org/abs/2411.17693v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Safe to Serve: Aligning Instruction-Tuned Models for Safety and\n  Helpfulness", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning and text generation. However, these models can inadvertently\ngenerate unsafe or biased responses when prompted with problematic inputs,\nraising significant ethical and practical concerns for real-world deployment.\nThis research addresses the critical challenge of developing language models\nthat generate both helpful and harmless content, navigating the delicate\nbalance between model performance and safety. We demonstrate that incorporating\nsafety-related instructions during the instruction-tuning of pre-trained models\nsignificantly reduces toxic responses to unsafe prompts without compromising\nperformance on helpfulness datasets. We found Direct Preference Optimization\n(DPO) to be particularly effective, outperforming both SIT and RAFT by\nleveraging both chosen and rejected responses for learning. Our approach\nincreased safe responses from 40$\\%$ to over 90$\\%$ across various harmfulness\nbenchmarks. In addition, we discuss a rigorous evaluation framework\nencompassing specialized metrics and diverse datasets for safety and\nhelpfulness tasks ensuring a comprehensive assessment of the model's\ncapabilities.", "published": "2024-11-26 06:52:22", "link": "http://arxiv.org/abs/2412.00074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-Based Offline Learning for Embodied Agents via Consistency-Guided\n  Reward Ensemble", "abstract": "Employing large language models (LLMs) to enable embodied agents has become\npopular, yet it presents several limitations in practice. In this work, rather\nthan using LLMs directly as agents, we explore their use as tools for embodied\nagent learning. Specifically, to train separate agents via offline\nreinforcement learning (RL), an LLM is used to provide dense reward feedback on\nindividual actions in training datasets. In doing so, we present a\nconsistency-guided reward ensemble framework (CoREN), designed for tackling\ndifficulties in grounding LLM-generated estimates to the target environment\ndomain. The framework employs an adaptive ensemble of spatio-temporally\nconsistent rewards to derive domain-grounded rewards in the training datasets,\nthus enabling effective offline learning of embodied agents in different\nenvironment domains. Experiments with the VirtualHome benchmark demonstrate\nthat CoREN significantly outperforms other offline RL agents, and it also\nachieves comparable performance to state-of-the-art LLM-based agents with 8B\nparameters, despite CoREN having only 117M parameters for the agent policy\nnetwork and using LLMs only for training.", "published": "2024-11-26 06:04:10", "link": "http://arxiv.org/abs/2411.17135v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Learning Monotonic Attention in Transducer for Streaming Generation", "abstract": "Streaming generation models are increasingly utilized across various fields,\nwith the Transducer architecture being particularly popular in industrial\napplications. However, its input-synchronous decoding mechanism presents\nchallenges in tasks requiring non-monotonic alignments, such as simultaneous\ntranslation, leading to suboptimal performance in these contexts. In this\nresearch, we address this issue by tightly integrating Transducer's decoding\nwith the history of input stream via a learnable monotonic attention mechanism.\nOur approach leverages the forward-backward algorithm to infer the posterior\nprobability of alignments between the predictor states and input timestamps,\nwhich is then used to estimate the context representations of monotonic\nattention in training. This allows Transducer models to adaptively adjust the\nscope of attention based on their predictions, avoiding the need to enumerate\nthe exponentially large alignment space. Extensive experiments demonstrate that\nour MonoAttn-Transducer significantly enhances the handling of non-monotonic\nalignments in streaming generation, offering a robust solution for\nTransducer-based frameworks to tackle more complex streaming generation tasks.", "published": "2024-11-26 07:19:26", "link": "http://arxiv.org/abs/2411.17170v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interleaved Scene Graphs for Interleaved Text-and-Image Generation\n  Assessment", "abstract": "Many real-world user queries (e.g. \"How do to make egg fried rice?\") could\nbenefit from systems capable of generating responses with both textual steps\nwith accompanying images, similar to a cookbook. Models designed to generate\ninterleaved text and images face challenges in ensuring consistency within and\nacross these modalities. To address these challenges, we present ISG, a\ncomprehensive evaluation framework for interleaved text-and-image generation.\nISG leverages a scene graph structure to capture relationships between text and\nimage blocks, evaluating responses on four levels of granularity: holistic,\nstructural, block-level, and image-specific. This multi-tiered evaluation\nallows for a nuanced assessment of consistency, coherence, and accuracy, and\nprovides interpretable question-answer feedback. In conjunction with ISG, we\nintroduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8\ncategories and 21 subcategories. This benchmark dataset includes complex\nlanguage-vision dependencies and golden answers to evaluate models effectively\non vision-centric tasks such as style transfer, a challenging area for current\nmodels. Using ISG-Bench, we demonstrate that recent unified vision-language\nmodels perform poorly on generating interleaved content. While compositional\napproaches that combine separate language and image models show a 111%\nimprovement over unified models at the holistic level, their performance\nremains suboptimal at both block and image levels. To facilitate future work,\nwe develop ISG-Agent, a baseline agent employing a \"plan-execute-refine\"\npipeline to invoke tools, achieving a 122% performance improvement.", "published": "2024-11-26 07:55:57", "link": "http://arxiv.org/abs/2411.17188v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Strategic Prompting for Conversational Tasks: A Comparative Analysis of\n  Large Language Models Across Diverse Conversational Tasks", "abstract": "Given the advancements in conversational artificial intelligence, the\nevaluation and assessment of Large Language Models (LLMs) play a crucial role\nin ensuring optimal performance across various conversational tasks. In this\npaper, we present a comprehensive study that thoroughly evaluates the\ncapabilities and limitations of five prevalent LLMs: Llama, OPT, Falcon,\nAlpaca, and MPT. The study encompasses various conversational tasks, including\nreservation, empathetic response generation, mental health and legal\ncounseling, persuasion, and negotiation. To conduct the evaluation, an\nextensive test setup is employed, utilizing multiple evaluation criteria that\nspan from automatic to human evaluation. This includes using generic and\ntask-specific metrics to gauge the LMs' performance accurately. From our\nevaluation, no single model emerges as universally optimal for all tasks.\nInstead, their performance varies significantly depending on the specific\nrequirements of each task. While some models excel in certain tasks, they may\ndemonstrate comparatively poorer performance in others. These findings\nemphasize the importance of considering task-specific requirements and\ncharacteristics when selecting the most suitable LM for conversational\napplications.", "published": "2024-11-26 08:21:24", "link": "http://arxiv.org/abs/2411.17204v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Topic-level Self-Correctional Approach to Mitigate Hallucinations in\n  MLLMs", "abstract": "Aligning the behaviors of Multimodal Large Language Models (MLLMs) with human\npreferences is crucial for developing robust and trustworthy AI systems. While\nrecent attempts have employed human experts or powerful auxiliary AI systems to\nprovide more accurate preference feedback, such as determining the preferable\nresponses from MLLMs or directly rewriting hallucination-free responses,\nextensive resource overhead compromise the scalability of the feedback\ncollection. In this work, we introduce Topic-level Preference Overwriting\n(TPO), a self-correctional approach that guide the model itself to mitigate its\nown hallucination at the topic level. Through a deconfounded strategy that\nreplaces each topic within the response with the best or worst alternatives\ngenerated by the model itself, TPO creates more contrasting pairwise preference\nfeedback, enhancing the feedback quality without human or proprietary model\nintervention. Notably, the experimental results demonstrate proposed TPO\nachieves state-of-the-art performance in trustworthiness, significantly\nreducing the object hallucinations by 92% and overall hallucinations by 38%.\nCode, model and dataset are available now.", "published": "2024-11-26 09:42:07", "link": "http://arxiv.org/abs/2411.17265v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "2D Matryoshka Training for Information Retrieval", "abstract": "2D Matryoshka Training is an advanced embedding representation training\napproach designed to train an encoder model simultaneously across various\nlayer-dimension setups. This method has demonstrated higher effectiveness in\nSemantic Text Similarity (STS) tasks over traditional training approaches when\nusing sub-layers for embeddings. Despite its success, discrepancies exist\nbetween two published implementations, leading to varied comparative results\nwith baseline models. In this reproducibility study, we implement and evaluate\nboth versions of 2D Matryoshka Training on STS tasks and extend our analysis to\nretrieval tasks. Our findings indicate that while both versions achieve higher\neffectiveness than traditional Matryoshka training on sub-dimensions, and\ntraditional full-sized model training approaches, they do not outperform models\ntrained separately on specific sub-layer and sub-dimension setups. Moreover,\nthese results generalize well to retrieval tasks, both in supervised (MSMARCO)\nand zero-shot (BEIR) settings. Further explorations of different loss\ncomputations reveals more suitable implementations for retrieval tasks, such as\nincorporating full-dimension loss and training on a broader range of target\ndimensions. Conversely, some intuitive approaches, such as fixing document\nencoders to full model outputs, do not yield improvements. Our reproduction\ncode is available at https://github.com/ielab/2DMSE-Reproduce.", "published": "2024-11-26 10:47:35", "link": "http://arxiv.org/abs/2411.17299v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ReFINE: A Reward-Based Framework for Interpretable and Nuanced\n  Evaluation of Radiology Report Generation", "abstract": "Automated radiology report generation (R2Gen) has advanced significantly,\nintroducing challenges in accurate evaluation due to its complexity.\nTraditional metrics often fall short by relying on rigid word-matching or\nfocusing only on pathological entities, leading to inconsistencies with human\nassessments. To bridge this gap, we introduce ReFINE, an automatic evaluation\nmetric designed specifically for R2Gen. Our metric utilizes a reward model,\nguided by our margin-based reward enforcement loss, along with a tailored\ntraining data design that enables customization of evaluation criteria to suit\nuser-defined needs. It not only scores reports according to user-specified\ncriteria but also provides detailed sub-scores, enhancing interpretability and\nallowing users to adjust the criteria between different aspects of reports.\nLeveraging GPT-4, we designed an easy-to-use data generation pipeline, enabling\nus to produce extensive training data based on two distinct scoring systems,\neach containing reports of varying quality along with corresponding scores.\nThese GPT-generated reports are then paired as accepted and rejected samples\nthrough our pairing rule to train an LLM towards our fine-grained reward model,\nwhich assigns higher rewards to the report with high quality. Our\nreward-control loss enables this model to simultaneously output multiple\nindividual rewards corresponding to the number of evaluation criteria, with\ntheir summation as our final ReFINE. Our experiments demonstrate ReFINE's\nheightened correlation with human judgments and superior performance in model\nselection compared to traditional metrics. Notably, our model provides both an\noverall score and individual scores for each evaluation item, enhancing\ninterpretability. We also demonstrate its flexible training across various\nevaluation systems.", "published": "2024-11-26 10:48:55", "link": "http://arxiv.org/abs/2411.17301v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meaningless is better: hashing bias-inducing words in LLM prompts\n  improves performance in logical reasoning and statistical learning", "abstract": "This paper introduces a novel method, referred to as \"hashing\", which\ninvolves masking potentially bias-inducing words in large language models\n(LLMs) with hash-like meaningless identifiers to reduce cognitive biases and\nreliance on external knowledge. The method was tested across three sets of\nexperiments involving a total of 490 prompts. Statistical analysis using\nchi-square tests showed significant improvements in all tested scenarios, which\ncovered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first\nexperiment, hashing decreased the fallacy rate in a modified version of the\n\"Linda\" problem aimed at evaluating susceptibility to cognitive biases. In the\nsecond experiment, it improved LLM results on the frequent itemset extraction\ntask. In the third experiment, we found hashing is also effective when the\nLinda problem is presented in a tabular format rather than text, indicating\nthat the technique works across various input representations. Overall, the\nmethod was shown to improve bias reduction and incorporation of external\nknowledge. Despite bias reduction, hallucination rates were inconsistently\nreduced across types of LLM models. These findings suggest that masking\nbias-inducing terms can improve LLM performance, although its effectiveness is\nmodel- and task-dependent.", "published": "2024-11-26 10:52:08", "link": "http://arxiv.org/abs/2411.17304v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can LLMs be Good Graph Judger for Knowledge Graph Construction?", "abstract": "In real-world scenarios, most of the data obtained from information retrieval\n(IR) system is unstructured. Converting natural language sentences into\nstructured Knowledge Graphs (KGs) remains a critical challenge. The quality of\nconstructed KGs may also impact the performance of some KG-dependent domains\nlike GraphRAG systems and recommendation systems. Recently, Large Language\nModels (LLMs) have demonstrated impressive capabilities in addressing a wide\nrange of natural language processing tasks. However, there are still challenges\nwhen utilizing LLMs to address the task of generating structured KGs. And we\nhave identified three limitations with respect to existing KG construction\nmethods. (1)There is a large amount of information and excessive noise in\nreal-world documents, which could result in extracting messy information.\n(2)Native LLMs struggle to effectively extract accuracy knowledge from some\ndomain-specific documents. (3)Hallucinations phenomenon cannot be overlooked\nwhen utilizing LLMs directly as an unsupervised method for constructing KGs.\n  In this paper, we propose GraphJudger, a knowledge graph construction\nframework to address the aforementioned challenges. We introduce three\ninnovative modules in our method, which are entity-centric iterative text\ndenoising, knowledge aware instruction tuning and graph judgement,\nrespectively. We seek to utilize the capacity of LLMs to function as a graph\njudger, a capability superior to their role only as a predictor for KG\nconstruction problems. Experiments conducted on two general text-graph pair\ndatasets and one domain-specific text-graph pair dataset show superior\nperformances compared to baseline methods. The code of our proposed method is\navailable at https://github.com/hhy-huang/GraphJudger.", "published": "2024-11-26 12:46:57", "link": "http://arxiv.org/abs/2411.17388v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical\n  Modeling Problem Solving", "abstract": "LLMs exhibit advanced reasoning capabilities, offering the potential to\ntransform natural language questions into mathematical models. However,\nexisting open-source datasets in operations research domain lack detailed\nannotations of the modeling process, such as variable definitions, focusing\nsolely on objective values, which hinders reinforcement learning applications.\nTo address this, we release the StructuredOR dataset, annotated with\ncomprehensive labels that capture the complete mathematical modeling process.\nWe further propose BPP-Search, a algorithm that integrates reinforcement\nlearning into a tree-of-thought structure using Beam search, a Process reward\nmodel, and a pairwise Preference algorithm. This approach enables efficient\nexploration of tree structures, avoiding exhaustive search while improving\naccuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP\ndatasets show that BPP-Search significantly outperforms state-of-the-art\nmethods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency,\nenabling faster retrieval of correct solutions.", "published": "2024-11-26 13:05:53", "link": "http://arxiv.org/abs/2411.17404v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "VLRewardBench: A Challenging Benchmark for Vision-Language Generative\n  Reward Models", "abstract": "Vision-language generative reward models (VL-GenRMs) play a crucial role in\naligning and evaluating multimodal AI systems, yet their own evaluation remains\nunder-explored. Current assessment methods primarily rely on AI-annotated\npreference labels from traditional VL tasks, which can introduce biases and\noften fail to effectively challenge state-of-the-art models. To address these\nlimitations, we introduce VL-RewardBench, a comprehensive benchmark spanning\ngeneral multimodal queries, visual hallucination detection, and complex\nreasoning tasks. Through our AI-assisted annotation pipeline combining sample\nselection with human verification, we curate 1,250 high-quality examples\nspecifically designed to probe model limitations. Comprehensive evaluation\nacross 16 leading large vision-language models, demonstrates VL-RewardBench's\neffectiveness as a challenging testbed, where even GPT-4o achieves only 65.4%\naccuracy, and state-of-the-art open-source models such as Qwen2-VL-72B,\nstruggle to surpass random-guessing. Importantly, performance on VL-RewardBench\nstrongly correlates (Pearson's r > 0.9) with MMMU-Pro accuracy using Best-of-N\nsampling with VL-GenRMs. Analysis experiments uncover three critical insights\nfor improving VL-GenRMs: (i) models predominantly fail at basic visual\nperception tasks rather than reasoning tasks; (ii) inference-time scaling\nbenefits vary dramatically by model capacity; and (iii) training VL-GenRMs to\nlearn to judge substantially boosts judgment capability (+14.7% accuracy for a\n7B VL-GenRM). We believe VL-RewardBench along with the experimental insights\nwill become a valuable resource for advancing VL-GenRMs.", "published": "2024-11-26 14:08:34", "link": "http://arxiv.org/abs/2411.17451v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FLEX-CLIP: Feature-Level GEneration Network Enhanced CLIP for X-shot\n  Cross-modal Retrieval", "abstract": "Given a query from one modality, few-shot cross-modal retrieval (CMR)\nretrieves semantically similar instances in another modality with the target\ndomain including classes that are disjoint from the source domain. Compared\nwith classical few-shot CMR methods, vision-language pretraining methods like\nCLIP have shown great few-shot or zero-shot learning performance. However, they\nstill suffer challenges due to (1) the feature degradation encountered in the\ntarget domain and (2) the extreme data imbalance. To tackle these issues, we\npropose FLEX-CLIP, a novel Feature-level Generation Network Enhanced CLIP.\nFLEX-CLIP includes two training stages. In multimodal feature generation, we\npropose a composite multimodal VAE-GAN network to capture real feature\ndistribution patterns and generate pseudo samples based on CLIP features,\naddressing data imbalance. For common space projection, we develop a gate\nresidual network to fuse CLIP features with projected features, reducing\nfeature degradation in X-shot scenarios. Experimental results on four benchmark\ndatasets show a 7%-15% improvement over state-of-the-art methods, with ablation\nstudies demonstrating enhancement of CLIP features.", "published": "2024-11-26 14:12:14", "link": "http://arxiv.org/abs/2411.17454v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Natural Language Understanding and Inference with MLLM in Visual\n  Question Answering: A Survey", "abstract": "Visual Question Answering (VQA) is a challenge task that combines natural\nlanguage processing and computer vision techniques and gradually becomes a\nbenchmark test task in multimodal large language models (MLLMs). The goal of\nour survey is to provide an overview of the development of VQA and a detailed\ndescription of the latest models with high timeliness. This survey gives an\nup-to-date synthesis of natural language understanding of images and text, as\nwell as the knowledge reasoning module based on image-question information on\nthe core VQA tasks. In addition, we elaborate on recent advances in extracting\nand fusing modal information with vision-language pretraining models and\nmultimodal large language models in VQA. We also exhaustively review the\nprogress of knowledge reasoning in VQA by detailing the extraction of internal\nknowledge and the introduction of external knowledge. Finally, we present the\ndatasets of VQA and different evaluation metrics and discuss possible\ndirections for future work.", "published": "2024-11-26 16:21:03", "link": "http://arxiv.org/abs/2411.17558v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "On Limitations of LLM as Annotator for Low Resource Languages", "abstract": "Low-resource languages face significant challenges due to the lack of\nsufficient linguistic data, resources, and tools for tasks such as supervised\nlearning, annotation, and classification. This shortage hinders the development\nof accurate models and datasets, making it difficult to perform critical NLP\ntasks like sentiment analysis or hate speech detection. To bridge this gap,\nLarge Language Models (LLMs) present an opportunity for potential annotators,\ncapable of generating datasets and resources for these underrepresented\nlanguages. In this paper, we focus on Marathi, a low-resource language, and\nevaluate the performance of both closed-source and open-source LLMs as\nannotators, while also comparing these results with fine-tuned BERT models. We\nassess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama\n3.1 (8B and 405B) on classification tasks including sentiment analysis, news\nclassification, and hate speech detection. Our findings reveal that while LLMs\nexcel in annotation tasks for high-resource languages like English, they still\nfall short when applied to Marathi. Even advanced models like GPT-4o and Llama\n3.1 405B underperform compared to fine-tuned BERT-based baselines, with GPT-4o\nand Llama 3.1 405B trailing fine-tuned BERT by accuracy margins of 10.2% and\n14.1%, respectively. This highlights the limitations of LLMs as annotators for\nlow-resource languages.", "published": "2024-11-26 17:55:37", "link": "http://arxiv.org/abs/2411.17637v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Non-Contextual BERT or FastText? A Comparative Analysis", "abstract": "Natural Language Processing (NLP) for low-resource languages, which lack\nlarge annotated datasets, faces significant challenges due to limited\nhigh-quality data and linguistic resources. The selection of embeddings plays a\ncritical role in achieving strong performance in NLP tasks. While contextual\nBERT embeddings require a full forward pass, non-contextual BERT embeddings\nrely only on table lookup. Existing research has primarily focused on\ncontextual BERT embeddings, leaving non-contextual embeddings largely\nunexplored. In this study, we analyze the effectiveness of non-contextual\nembeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT\nand MahaFT) for tasks such as news classification, sentiment analysis, and hate\nspeech detection in one such low-resource language Marathi. We compare these\nembeddings with their contextual and compressed variants. Our findings indicate\nthat non-contextual BERT embeddings extracted from the model's first embedding\nlayer outperform FastText embeddings, presenting a promising alternative for\nlow-resource NLP.", "published": "2024-11-26 18:25:57", "link": "http://arxiv.org/abs/2411.17661v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Linguistic Laws Meet Protein Sequences: A Comparative Analysis of\n  Subword Tokenization Methods", "abstract": "Tokenization is a crucial step in processing protein sequences for machine\nlearning models, as proteins are complex sequences of amino acids that require\nmeaningful segmentation to capture their functional and structural properties.\nHowever, existing subword tokenization methods, developed primarily for human\nlanguage, may be inadequate for protein sequences, which have unique patterns\nand constraints. This study evaluates three prominent tokenization approaches,\nByte-Pair Encoding (BPE), WordPiece, and SentencePiece, across varying\nvocabulary sizes (400-6400), analyzing their effectiveness in protein sequence\nrepresentation, domain boundary preservation, and adherence to established\nlinguistic laws. Our comprehensive analysis reveals distinct behavioral\npatterns among these tokenizers, with vocabulary size significantly influencing\ntheir performance. BPE demonstrates better contextual specialization and\nmarginally better domain boundary preservation at smaller vocabularies, while\nSentencePiece achieves better encoding efficiency, leading to lower fertility\nscores. WordPiece offers a balanced compromise between these characteristics.\nHowever, all tokenizers show limitations in maintaining protein domain\nintegrity, particularly as vocabulary size increases. Analysis of linguistic\nlaw adherence shows partial compliance with Zipf's and Brevity laws but notable\ndeviations from Menzerath's law, suggesting that protein sequences may follow\ndistinct organizational principles from natural languages. These findings\nhighlight the limitations of applying traditional NLP tokenization methods to\nprotein sequences and emphasize the need for developing specialized\ntokenization strategies that better account for the unique characteristics of\nproteins.", "published": "2024-11-26 18:30:20", "link": "http://arxiv.org/abs/2411.17669v1", "categories": ["cs.CL", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Attamba: Attending To Multi-Token States", "abstract": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.", "published": "2024-11-26 18:52:06", "link": "http://arxiv.org/abs/2411.17685v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens", "abstract": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang.", "published": "2024-11-26 18:57:58", "link": "http://arxiv.org/abs/2411.17691v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language\n  Generator", "abstract": "Sign language is a visual language that encompasses all linguistic features\nof natural languages and serves as the primary communication method for the\ndeaf and hard-of-hearing communities. Although many studies have successfully\nadapted pretrained language models (LMs) for sign language translation\n(sign-to-text), the reverse task-sign language generation\n(text-to-sign)-remains largely unexplored. In this work, we introduce a\nmultilingual sign language model, Signs as Tokens (SOKE), which can generate 3D\nsign avatars autoregressively from text inputs using a pretrained LM. To align\nsign language with the LM, we leverage a decoupled tokenizer that discretizes\ncontinuous signs into token sequences representing various body parts. During\ndecoding, unlike existing approaches that flatten all part-wise tokens into a\nsingle sequence and predict one token at a time, we propose a multi-head\ndecoding method capable of predicting multiple tokens simultaneously. This\napproach improves inference efficiency while maintaining effective information\nfusion across different body parts. To further ease the generation process, we\npropose a retrieval-enhanced SLG approach, which incorporates external sign\ndictionaries to provide accurate word-level signs as auxiliary conditions,\nsignificantly improving the precision of generated signs. Extensive qualitative\nand quantitative evaluations demonstrate the effectiveness of SOKE. Code,\nmodels, and data will be made publicly available.", "published": "2024-11-26 18:28:09", "link": "http://arxiv.org/abs/2411.17799v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Leveraging Large Language Models and Topic Modeling for Toxicity\n  Classification", "abstract": "Content moderation and toxicity classification represent critical tasks with\nsignificant social implications. However, studies have shown that major\nclassification models exhibit tendencies to magnify or reduce biases and\npotentially overlook or disadvantage certain marginalized groups within their\nclassification processes. Researchers suggest that the positionality of\nannotators influences the gold standard labels in which the models learned from\npropagate annotators' bias. To further investigate the impact of annotator\npositionality, we delve into fine-tuning BERTweet and HateBERT on the dataset\nwhile using topic-modeling strategies for content moderation. The results\nindicate that fine-tuning the models on specific topics results in a notable\nimprovement in the F1 score of the models when compared to the predictions\ngenerated by other prominent classification models such as GPT-4,\nPerspectiveAPI, and RewireAPI. These findings further reveal that the\nstate-of-the-art large language models exhibit significant limitations in\naccurately detecting and interpreting text toxicity contrasted with earlier\nmethodologies. Code is available at\nhttps://github.com/aheldis/Toxicity-Classification.git.", "published": "2024-11-26 20:47:24", "link": "http://arxiv.org/abs/2411.17876v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Generative AI-Enhanced Content: A Conceptual Framework Using\n  Qualitative, Quantitative, and Mixed-Methods Approaches", "abstract": "Generative AI (GenAI) has revolutionized content generation, offering\ntransformative capabilities for improving language coherence, readability, and\noverall quality. This manuscript explores the application of qualitative,\nquantitative, and mixed-methods research approaches to evaluate the performance\nof GenAI models in enhancing scientific writing. Using a hypothetical use case\ninvolving a collaborative medical imaging manuscript, we demonstrate how each\nmethod provides unique insights into the impact of GenAI. Qualitative methods\ngather in-depth feedback from expert reviewers, analyzing their responses using\nthematic analysis tools to capture nuanced improvements and identify\nlimitations. Quantitative approaches employ automated metrics such as BLEU,\nROUGE, and readability scores, as well as user surveys, to objectively measure\nimprovements in coherence, fluency, and structure. Mixed-methods research\nintegrates these strengths, combining statistical evaluations with detailed\nqualitative insights to provide a comprehensive assessment. These research\nmethods enable quantifying improvement levels in GenAI-generated content,\naddressing critical aspects of linguistic quality and technical accuracy. They\nalso offer a robust framework for benchmarking GenAI tools against traditional\nediting processes, ensuring the reliability and effectiveness of these\ntechnologies. By leveraging these methodologies, researchers can evaluate the\nperformance boost driven by GenAI, refine its applications, and guide its\nresponsible adoption in high-stakes domains like healthcare and scientific\nresearch. This work underscores the importance of rigorous evaluation\nframeworks for advancing trust and innovation in GenAI.", "published": "2024-11-26 23:34:07", "link": "http://arxiv.org/abs/2411.17943v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Condense, Don't Just Prune: Enhancing Efficiency and Performance in MoE\n  Layer Pruning", "abstract": "Mixture-of-Experts (MoE) has garnered significant attention for its ability\nto scale up neural networks while utilizing the same or even fewer active\nparameters. However, MoE does not alleviate the massive memory requirements of\nnetworks, which limits their practicality in real-world applications,\nespecially in the era of large language models (LLMs). While recent work\nexplores the possibility of removing entire layers of MoE to reduce memory, the\nperformance degradation is still notable. In this paper, we propose\nConDense-MoE (CD-MoE), which, instead of dropping the entire MoE layer,\ncondenses the large, sparse MoE layer into a smaller, denser layer with only a\nfew experts activated for all tokens, while maintaining hardware friendliness.\nOur approach is specifically designed for fine-grained MoE with shared experts,\nwhere Feed-Forward Networks are split into many small experts, with certain\nexperts isolated to serve as shared experts that are always activated, such as\nDeepSeekMoE and QwenMoE. We demonstrate the effectiveness of our method.\nSpecifically, for the DeepSeekMoE-16B model, our approach maintains 90% of the\naverage accuracy while reducing memory usage by 27.5% and increasing inference\nspeed by 1.26 times. Moreover, we show that by applying lightweight expert\nfine-tuning -- only to the condensed layers -- and using 5 hours on a single\n80G A100 GPU, we can successfully recover 98% of the original performance. Our\ncode is available at: https://github.com/duterscmy/CD-MoE/tree/main.", "published": "2024-11-26 00:56:18", "link": "http://arxiv.org/abs/2412.00069v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Relations, Negations, and Numbers: Looking for Logic in Generative\n  Text-to-Image Models", "abstract": "Despite remarkable progress in multi-modal AI research, there is a salient\ndomain in which modern AI continues to lag considerably behind even human\nchildren: the reliable deployment of logical operators. Here, we examine three\nforms of logical operators: relations, negations, and discrete numbers. We\nasked human respondents (N=178 in total) to evaluate images generated by a\nstate-of-the-art image-generating AI (DALL-E 3) prompted with these `logical\nprobes', and find that none reliably produce human agreement scores greater\nthan 50\\%. The negation probes and numbers (beyond 3) fail most frequently. In\na 4th experiment, we assess a `grounded diffusion' pipeline that leverages\ntargeted prompt engineering and structured intermediate representations for\ngreater compositional control, but find its performance is judged even worse\nthan that of DALL-E 3 across prompts. To provide further clarity on potential\nsources of success and failure in these text-to-image systems, we supplement\nour 4 core experiments with multiple auxiliary analyses and schematic diagrams,\ndirectly quantifying, for example, the relationship between the N-gram\nfrequency of relational prompts and the average match to generated images; the\nsuccess rates for 3 different prompt modification strategies in the rendering\nof negation prompts; and the scalar variability / ratio dependence\n(`approximate numeracy') of prompts involving integers. We conclude by\ndiscussing the limitations inherent to `grounded' multimodal learning systems\nwhose grounding relies heavily on vector-based semantics (e.g. DALL-E 3), or\nunder-specified syntactical constraints (e.g. `grounded diffusion'), and\npropose minimal modifications (inspired by development, based in imagery) that\ncould help to bridge the lingering compositional gap between scale and\nstructure. All data and code is available at\nhttps://github.com/ColinConwell/T2I-Probology", "published": "2024-11-26 03:06:52", "link": "http://arxiv.org/abs/2411.17066v1", "categories": ["cs.CV", "cs.CL", "cs.SC"], "primary_category": "cs.CV"}
{"title": "Star Attention: Efficient LLM Inference over Long Sequences", "abstract": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.", "published": "2024-11-26 05:10:04", "link": "http://arxiv.org/abs/2411.17116v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AutoElicit: Using Large Language Models for Expert Prior Elicitation in\n  Predictive Modelling", "abstract": "Large language models (LLMs) acquire a breadth of information across various\ndomains. However, their computational complexity, cost, and lack of\ntransparency often hinder their direct application for predictive tasks where\nprivacy and interpretability are paramount. In fields such as healthcare,\nbiology, and finance, specialised and interpretable linear models still hold\nconsiderable value. In such domains, labelled data may be scarce or expensive\nto obtain. Well-specified prior distributions over model parameters can reduce\nthe sample complexity of learning through Bayesian inference; however,\neliciting expert priors can be time-consuming. We therefore introduce\nAutoElicit to extract knowledge from LLMs and construct priors for predictive\nmodels. We show these priors are informative and can be refined using natural\nlanguage. We perform a careful study contrasting AutoElicit with in-context\nlearning and demonstrate how to perform model selection between the two\nmethods. We find that AutoElicit yields priors that can substantially reduce\nerror over uninformative priors, using fewer labels, and consistently\noutperform in-context learning. We show that AutoElicit saves over 6 months of\nlabelling effort when building a new predictive model for urinary tract\ninfections from sensor recordings of people living with dementia.", "published": "2024-11-26 10:13:39", "link": "http://arxiv.org/abs/2411.17284v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a\n  Fact-Based Approach", "abstract": "Large language models (LLMs) often reflect real-world biases, leading to\nefforts to mitigate these effects and make the models unbiased. Achieving this\ngoal requires defining clear criteria for an unbiased state, with any deviation\nfrom these criteria considered biased. Some studies define an unbiased state as\nequal treatment across diverse demographic groups, aiming for balanced outputs\nfrom LLMs. However, differing perspectives on equality and the importance of\npluralism make it challenging to establish a universal standard. Alternatively,\nother approaches propose using fact-based criteria for more consistent and\nobjective evaluations, though these methods have not yet been fully applied to\nLLM bias assessments. Thus, there is a need for a metric with objective\ncriteria that offers a distinct perspective from equality-based approaches.\nMotivated by this need, we introduce a novel metric to assess bias using\nfact-based criteria and real-world statistics. In this paper, we conducted a\nhuman survey demonstrating that humans tend to perceive LLM outputs more\npositively when they align closely with real-world demographic distributions.\nEvaluating various LLMs with our proposed metric reveals that model bias varies\ndepending on the criteria used, highlighting the need for multi-perspective\nassessment.", "published": "2024-11-26 11:32:43", "link": "http://arxiv.org/abs/2411.17338v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Fairness And Performance In Harmony: Data Debiasing Is All You Need", "abstract": "Fairness in both machine learning (ML) predictions and human decisions is\ncritical, with ML models prone to algorithmic and data bias, and human\ndecisions affected by subjectivity and cognitive bias. This study investigates\nfairness using a real-world university admission dataset with 870 profiles,\nleveraging three ML models, namely XGB, Bi-LSTM, and KNN. Textual features are\nencoded with BERT embeddings. For individual fairness, we assess decision\nconsistency among experts with varied backgrounds and ML models, using a\nconsistency score. Results show ML models outperform humans in fairness by\n14.08% to 18.79%. For group fairness, we propose a gender-debiasing pipeline\nand demonstrate its efficacy in removing gender-specific language without\ncompromising prediction performance. Post-debiasing, all models maintain or\nimprove their classification accuracy, validating the hypothesis that fairness\nand performance can coexist. Our findings highlight ML's potential to enhance\nfairness in admissions while maintaining high accuracy, advocating a hybrid\napproach combining human judgement and ML models.", "published": "2024-11-26 12:31:10", "link": "http://arxiv.org/abs/2411.17374v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent", "abstract": "Building Graphical User Interface (GUI) assistants holds significant promise\nfor enhancing human workflow productivity. While most agents are\nlanguage-based, relying on closed-source API with text-rich meta-information\n(e.g., HTML or accessibility tree), they show limitations in perceiving UI\nvisuals as humans do, highlighting the need for GUI visual agents. In this\nwork, we develop a vision-language-action model in digital world, namely\nShowUI, which features the following innovations: (i) UI-Guided Visual Token\nSelection to reduce computational costs by formulating screenshots as an UI\nconnected graph, adaptively identifying their redundant relationship and serve\nas the criteria for token selection during self-attention blocks; (ii)\nInterleaved Vision-Language-Action Streaming that flexibly unifies diverse\nneeds within GUI tasks, enabling effective management of visual-action history\nin navigation or pairing multi-turn query-action sequences per screenshot to\nenhance training efficiency; (iii) Small-scale High-quality GUI\nInstruction-following Datasets by careful data curation and employing a\nresampling strategy to address significant data type imbalances. With above\ncomponents, ShowUI, a lightweight 2B model using 256K data, achieves a strong\n75.1% accuracy in zero-shot screenshot grounding. Its UI-guided token selection\nfurther reduces 33% of redundant visual tokens during training and speeds up\nthe performance by 1.4x. Navigation experiments across web Mind2Web, mobile\nAITW, and online MiniWob environments further underscore the effectiveness and\npotential of our model in advancing GUI visual agents. The models are available\nat https://github.com/showlab/ShowUI.", "published": "2024-11-26 14:29:47", "link": "http://arxiv.org/abs/2411.17465v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CV"}
{"title": "What Differentiates Educational Literature? A Multimodal Fusion Approach\n  of Transformers and Computational Linguistics", "abstract": "The integration of new literature into the English curriculum remains a\nchallenge since educators often lack scalable tools to rapidly evaluate\nreadability and adapt texts for diverse classroom needs. This study proposes to\naddress this gap through a multimodal approach that combines transformer-based\ntext classification with linguistic feature analysis to align texts with UK Key\nStages. Eight state-of-the-art Transformers were fine-tuned on segmented text\ndata, with BERT achieving the highest unimodal F1 score of 0.75. In parallel,\n500 deep neural network topologies were searched for the classification of\nlinguistic characteristics, achieving an F1 score of 0.392. The fusion of these\nmodalities shows a significant improvement, with every multimodal approach\noutperforming all unimodal models. In particular, the ELECTRA Transformer fused\nwith the neural network achieved an F1 score of 0.996. Unimodal and multimodal\napproaches are shown to have statistically significant differences in all\nvalidation metrics (accuracy, precision, recall, F1 score) except for inference\ntime. The proposed approach is finally encapsulated in a stakeholder-facing web\napplication, providing non-technical stakeholder access to real-time insights\non text complexity, reading difficulty, curriculum alignment, and\nrecommendations for learning age range. The application empowers data-driven\ndecision making and reduces manual workload by integrating AI-based\nrecommendations into lesson planning for English literature.", "published": "2024-11-26 17:01:27", "link": "http://arxiv.org/abs/2411.17593v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data", "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech\noutput, allowing for more natural human-computer interaction compared to\ntext-based large language models (LLMs). Traditional approaches for developing\nSpeechLMs are constrained by the limited availability of unsupervised speech\ndata and parallel speech-text data, which are significantly less abundant than\ntext pre-training data, thereby limiting their scalability as LLMs. We propose\na novel approach to scaling speech-text pre-training by leveraging large-scale\nsynthetic interleaved data derived from text corpora, eliminating the need for\nparallel speech-text datasets. Our method efficiently constructs speech-text\ninterleaved data by sampling text spans from existing text corpora and\nsynthesizing corresponding speech spans using a text-to-token model, bypassing\nthe need to generate actual speech. We also employ a supervised speech\ntokenizer derived from an automatic speech recognition (ASR) model by\nincorporating a vector-quantized bottleneck into the encoder. This supervised\ntraining approach results in discrete speech tokens with strong semantic\npreservation even at lower frame rates (e.g. 12.5Hz), while still maintaining\nspeech reconstruction quality. Starting from a pre-trained language model and\nscaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved\nspeech-text data), we achieve state-of-the-art performance in speech language\nmodeling and spoken question answering, improving performance on spoken\nquestions tasks from the previous SOTA of 13% (Moshi) to 31%. We further\ndemonstrate that by fine-tuning the pre-trained model with speech dialogue\ndata, we can develop an end-to-end spoken chatbot that achieves competitive\nperformance comparable to existing baselines in both conversational abilities\nand speech quality, even operating exclusively in the speech domain.", "published": "2024-11-26 17:19:09", "link": "http://arxiv.org/abs/2411.17607v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Efficient Self-Improvement in Multimodal Large Language Models: A\n  Model-Level Judge-Free Approach", "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for\nenhancing their reliability and robustness. However, current methods often rely\nheavily on MLLMs themselves as judges, leading to high computational costs and\npotential pitfalls like reward hacking and model collapse. This paper\nintroduces a novel, model-level judge-free self-improvement framework. Our\napproach employs a controlled feedback mechanism while eliminating the need for\nMLLMs in the verification loop. We generate preference learning pairs using a\ncontrollable hallucination mechanism and optimize data quality by leveraging\nlightweight, contrastive language-image encoders to evaluate and reverse pairs\nwhen necessary. Evaluations across public benchmarks and our newly introduced\nIC dataset designed to challenge hallucination control demonstrate that our\nmodel outperforms conventional techniques. We achieve superior precision and\nrecall with significantly lower computational demands. This method offers an\nefficient pathway to scalable self-improvement in MLLMs, balancing performance\ngains with reduced resource requirements.", "published": "2024-11-26 00:44:37", "link": "http://arxiv.org/abs/2411.17760v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "$H^3$Fusion: Helpful, Harmless, Honest Fusion of Aligned LLMs", "abstract": "Alignment of pretrained LLMs using instruction-based datasets is critical for\ncreating fine-tuned models that reflect human preference. A growing number of\nalignment-based fine-tuning algorithms and benchmarks emerged recently, fueling\nthe efforts on effective alignments of pre-trained LLMs to ensure helpful,\nharmless, and honest answers from both open-source and closed-source LLMs. This\npaper tackles this problem by developing an alignment fusion approach, coined\nas $H^3$Fusion, with three unique characteristics. First, $H^3$Fusion ensembles\nmultiple individually aligned LLMs to create a final fine-tuned alignment model\nwith enhanced capabilities beyond those of individual models, delivering robust\nalignment through promoting helpful, harmless, honest fusion. Second,\n$H^3$Fusion leverages the mixture-of-experts (MoE) methodology in two steps. We\nfirst freeze the multi-head attention weights of each individual model while\ntuning the FFN layer during alignment fusion. Then we merge the aligned model\nweights with an expert router according to the type of input instruction and\ndynamically select a subset of experts that are best suited for producing the\noutput response. Finally, we boost the performance of the resulting\n$H^3$3Fusion model by introducing gating loss and regularization terms. The\nformer penalizes the selection errors of the expert-router, and the latter\nmediates the expert weights drifting during fine-tuning and dynamically adjusts\nthe fusion behavior of the resulting model by canalizing the activations on the\nexperts. Extensive evaluations on three benchmark datasets show that\n$H^3$3Fusion is more helpful, less harmful, and more honest from two aspects:\nit outperforms each individually aligned model by $11.37\\%$, and it provides\nstronger robustness compared to the state-of-the-art LLM ensemble approaches by\n$13.77\\%$. Code is available at github.com/sftekin/h3fusion.", "published": "2024-11-26 17:42:38", "link": "http://arxiv.org/abs/2411.17792v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LongKey: Keyphrase Extraction for Long Documents", "abstract": "In an era of information overload, manually annotating the vast and growing\ncorpus of documents and scholarly papers is increasingly impractical. Automated\nkeyphrase extraction addresses this challenge by identifying representative\nterms within texts. However, most existing methods focus on short documents (up\nto 512 tokens), leaving a gap in processing long-context documents. In this\npaper, we introduce LongKey, a novel framework for extracting keyphrases from\nlengthy documents, which uses an encoder-based language model to capture\nextended text intricacies. LongKey uses a max-pooling embedder to enhance\nkeyphrase candidate representation. Validated on the comprehensive LDKP\ndatasets and six diverse, unseen datasets, LongKey consistently outperforms\nexisting unsupervised and language model-based keyphrase extraction methods.\nOur findings demonstrate LongKey's versatility and superior performance,\nmarking an advancement in keyphrase extraction for varied text lengths and\ndomains.", "published": "2024-11-26 20:26:47", "link": "http://arxiv.org/abs/2411.17863v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HOPPR Medical-Grade Platform for Medical Imaging AI", "abstract": "Technological advances in artificial intelligence (AI) have enabled the\ndevelopment of large vision language models (LVLMs) that are trained on\nmillions of paired image and text samples. Subsequent research efforts have\ndemonstrated great potential of LVLMs to achieve high performance in medical\nimaging use cases (e.g., radiology report generation), but there remain\nbarriers that hinder the ability to deploy these solutions broadly. These\ninclude the cost of extensive computational requirements for developing large\nscale models, expertise in the development of sophisticated AI models, and the\ndifficulty in accessing substantially large, high-quality datasets that\nadequately represent the population in which the LVLM solution is to be\ndeployed. The HOPPR Medical-Grade Platform addresses these barriers by\nproviding powerful computational infrastructure, a suite of foundation models\non top of which developers can fine-tune for their specific use cases, and a\nrobust quality management system that sets a standard for evaluating fine-tuned\nmodels for deployment in clinical settings. The HOPPR Platform has access to\nmillions of imaging studies and text reports sourced from hundreds of imaging\ncenters from diverse populations to pretrain foundation models and enable use\ncase-specific cohorts for fine-tuning. All data are deidentified and securely\nstored for HIPAA compliance. Additionally, developers can securely host models\non the HOPPR platform and access them via an API to make inferences using these\nmodels within established clinical workflows. With the Medical-Grade Platform,\nHOPPR's mission is to expedite the deployment of LVLM solutions for medical\nimaging and ultimately optimize radiologist's workflows and meet the growing\ndemands of the field.", "published": "2024-11-26 21:21:45", "link": "http://arxiv.org/abs/2411.17891v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "COAP: Memory-Efficient Training with Correlation-Aware Gradient\n  Projection", "abstract": "Training large-scale neural networks in vision, and multimodal domains\ndemands substantial memory resources, primarily due to the storage of optimizer\nstates. While LoRA, a popular parameter-efficient method, reduces memory usage,\nit often suffers from suboptimal performance due to the constraints of low-rank\nupdates. Low-rank gradient projection methods (e.g., GaLore, Flora) reduce\noptimizer memory by projecting gradients and moment estimates into low-rank\nspaces via singular value decomposition or random projection. However, they\nfail to account for inter-projection correlation, causing performance\ndegradation, and their projection strategies often incur high computational\ncosts. In this paper, we present COAP (Correlation-Aware Gradient Projection),\na memory-efficient method that minimizes computational overhead while\nmaintaining training performance. Evaluated across various vision, language,\nand multimodal tasks, COAP outperforms existing methods in both training speed\nand model performance. For LLaMA-1B, it reduces optimizer memory by 61% with\nonly 2% additional time cost, achieving the same PPL as AdamW. With 8-bit\nquantization, COAP cuts optimizer memory by 81% and achieves 4x speedup over\nGaLore for LLaVA-v1.5-7B fine-tuning, while delivering higher accuracy.", "published": "2024-11-26 03:50:52", "link": "http://arxiv.org/abs/2412.00071v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient\n  Fine-Tuning of Language Models", "abstract": "Transformer-based large-scale pre-trained models achieve great success, and\nfine-tuning, which tunes a pre-trained model on a task-specific dataset, is the\nstandard practice to utilize these models for downstream tasks. Recent work has\ndeveloped adapter-tuning, but these approaches either still require a\nrelatively high resource usage. Through our investigation, we show that each\nadapter in adapter-tuning does not have the same impact on task performance and\nresource usage. Based on our findings, we propose SAFE, which gradually freezes\nless-important adapters that do not contribute to adaptation during the early\ntraining steps. In our experiments, SAFE reduces memory usage, computation\namount, and training time by 42.85\\%, 34.59\\%, and 11.82\\%, respectively, while\nachieving comparable or better performance compared to the baseline. We also\ndemonstrate that SAFE induces regularization effect, thereby smoothing the loss\nlandscape.", "published": "2024-11-26 08:41:45", "link": "http://arxiv.org/abs/2412.03587v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Socio-Emotional Response Generation: A Human Evaluation Protocol for\n  LLM-Based Conversational Systems", "abstract": "Conversational systems are now capable of producing impressive and generally\nrelevant responses. However, we have no visibility nor control of the\nsocio-emotional strategies behind state-of-the-art Large Language Models\n(LLMs), which poses a problem in terms of their transparency and thus their\ntrustworthiness for critical applications. Another issue is that current\nautomated metrics are not able to properly evaluate the quality of generated\nresponses beyond the dataset's ground truth. In this paper, we propose a neural\narchitecture that includes an intermediate step in planning socio-emotional\nstrategies before response generation. We compare the performance of\nopen-source baseline LLMs to the outputs of these same models augmented with\nour planning module. We also contrast the outputs obtained from automated\nmetrics and evaluation results provided by human annotators. We describe a\nnovel evaluation protocol that includes a coarse-grained consistency\nevaluation, as well as a finer-grained annotation of the responses on various\nsocial and emotional criteria. Our study shows that predicting a sequence of\nexpected strategy labels and using this sequence to generate a response yields\nbetter results than a direct end-to-end generation scheme. It also highlights\nthe divergences and the limits of current evaluation metrics for generated\ncontent. The code for the annotation platform and the annotated data are made\npublicly available for the evaluation of future models.", "published": "2024-11-26 08:15:36", "link": "http://arxiv.org/abs/2412.04492v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Enhancing Code-Switching ASR Leveraging Non-Peaky CTC Loss and Deep\n  Language Posterior Injection", "abstract": "Code-switching-where multilingual speakers alternately switch between\nlanguages during conversations-still poses significant challenges to end-to-end\n(E2E) automatic speech recognition (ASR) systems due to phenomena of both\nacoustic and semantic confusion. This issue arises because ASR systems struggle\nto handle the rapid alternation of languages effectively, which often leads to\nsignificant performance degradation. Our main contributions are at least\nthreefold: First, we incorporate language identification (LID) information into\nseveral intermediate layers of the encoder, aiming to enrich output embeddings\nwith more detailed language information. Secondly, through the novel\napplication of language boundary alignment loss, the subsequent ASR modules are\nenabled to more effectively utilize the knowledge of internal language\nposteriors. Third, we explore the feasibility of using language posteriors to\nfacilitate deep interaction between shared encoder and language-specific\nencoders. Through comprehensive experiments on the SEAME corpus, we have\nverified that our proposed method outperforms the prior-art method, disentangle\nbased mixture-of-experts (D-MoE), further enhancing the acuity of the encoder\nto languages.", "published": "2024-11-26 06:49:05", "link": "http://arxiv.org/abs/2412.08651v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech\n  Representation Learning", "abstract": "Self-supervised learning (SSL) has achieved great success in speech-related\ntasks. While Transformer and Conformer architectures have dominated SSL\nbackbones, encoders like Zipformer, which excel in automatic speech recognition\n(ASR), remain unexplored in SSL. Concurrently, inefficiencies in data\nprocessing within existing SSL training frameworks, such as fairseq, pose\nchallenges in managing the growing volumes of training data. To address these\nissues, we propose k2SSL, an open-source framework that offers faster, more\nmemory-efficient, and better-performing self-supervised speech representation\nlearning, focusing on downstream ASR tasks. The optimized HuBERT and proposed\nZipformer-based SSL systems exhibit substantial reductions in both training\ntime and memory usage during SSL training. Experiments on LibriSpeech\ndemonstrate that Zipformer Base significantly outperforms HuBERT and WavLM,\nachieving up to a 34.8% relative WER reduction compared to HuBERT Base after\nfine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled\nto 60k hours of LibriLight data, Zipformer Large exhibits remarkable\nefficiency, matching HuBERT Large's performance while requiring only 5/8\npre-training steps.", "published": "2024-11-26 04:37:11", "link": "http://arxiv.org/abs/2411.17100v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Typical vs. Atypical Disfluency Classification: Introducing the\n  IIITH-TISA Corpus and Temporal Context-Based Feature Representations", "abstract": "Speech disfluencies in spontaneous communication can be categorized as either\ntypical or atypical. Typical disfluencies, such as hesitations and repetitions,\nare natural occurrences in everyday speech, while atypical disfluencies are\nindicative of pathological disorders like stuttering. Distinguishing between\nthese categories is crucial for improving voice assistants (VAs) for Persons\nWho Stutter (PWS), who often face premature cutoffs due to misidentification of\nspeech termination. Accurate classification also aids in detecting stuttering\nearly in children, preventing misdiagnosis as language development disfluency.\nThis research introduces the IIITH-TISA dataset, the first Indian English\nstammer corpus, capturing atypical disfluencies. Additionally, we extend the\nIIITH-IED dataset with detailed annotations for typical disfluencies. We\npropose Perceptually Enhanced Zero-Time Windowed Cepstral Coefficients\n(PE-ZTWCC) combined with Shifted Delta Cepstra (SDC) as input features to a\nshallow Time Delay Neural Network (TDNN) classifier, capturing both local and\nwider temporal contexts. Our method achieves an average F1 score of 85.01% for\ndisfluency classification, outperforming traditional features.", "published": "2024-11-26 06:32:41", "link": "http://arxiv.org/abs/2411.17149v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Disentangled-Transformer: An Explainable End-to-End Automatic Speech\n  Recognition Model with Speech Content-Context Separation", "abstract": "End-to-end transformer-based automatic speech recognition (ASR) systems often\ncapture multiple speech traits in their learned representations that are highly\nentangled, leading to a lack of interpretability. In this study, we propose the\nexplainable Disentangled-Transformer, which disentangles the internal\nrepresentations into sub-embeddings with explicit content and speaker traits\nbased on varying temporal resolutions. Experimental results show that the\nproposed Disentangled-Transformer produces a clear speaker identity, separated\nfrom the speech content, for speaker diarization while improving ASR\nperformance.", "published": "2024-11-26 19:57:40", "link": "http://arxiv.org/abs/2411.17846v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Comparative Analysis of ASR Methods for Speech Deepfake Detection", "abstract": "Recent techniques for speech deepfake detection often rely on pre-trained\nself-supervised models. These systems, initially developed for Automatic Speech\nRecognition (ASR), have proved their ability to offer a meaningful\nrepresentation of speech signals, which can benefit various tasks, including\ndeepfake detection. In this context, pre-trained models serve as feature\nextractors and are used to extract embeddings from input speech, which are then\nfed to a binary speech deepfake detector. The remarkable accuracy achieved\nthrough this approach underscores a potential relationship between ASR and\nspeech deepfake detection. However, this connection is not yet entirely clear,\nand we do not know whether improved performance in ASR corresponds to higher\nspeech deepfake detection capabilities. In this paper, we address this question\nthrough a systematic analysis. We consider two different pre-trained\nself-supervised ASR models, Whisper and Wav2Vec 2.0, and adapt them for the\nspeech deepfake detection task. These models have been released in multiple\nversions, with increasing number of parameters and enhanced ASR performance. We\ninvestigate whether performance improvements in ASR correlate with improvements\nin speech deepfake detection. Our results provide insights into the\nrelationship between these two tasks and offer valuable guidance for the\ndevelopment of more effective speech deepfake detectors.", "published": "2024-11-26 11:51:10", "link": "http://arxiv.org/abs/2411.17349v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Towards Maximum Likelihood Training for Transducer-based Streaming\n  Speech Recognition", "abstract": "Transducer neural networks have emerged as the mainstream approach for\nstreaming automatic speech recognition (ASR), offering state-of-the-art\nperformance in balancing accuracy and latency. In the conventional framework,\nstreaming transducer models are trained to maximize the likelihood function\nbased on non-streaming recursion rules. However, this approach leads to a\nmismatch between training and inference, resulting in the issue of deformed\nlikelihood and consequently suboptimal ASR accuracy. We introduce a\nmathematical quantification of the gap between the actual likelihood and the\ndeformed likelihood, namely forward variable causal compensation (FoCC). We\nalso present its estimator, FoCCE, as a solution to estimate the exact\nlikelihood. Through experiments on the LibriSpeech dataset, we show that FoCCE\ntraining improves the accuracy of the streaming transducers.", "published": "2024-11-26 15:53:13", "link": "http://arxiv.org/abs/2411.17537v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis", "abstract": "In this paper, we propose a new task -- generating speech from videos of\npeople and their transcripts (VTTS) -- to motivate new techniques for\nmultimodal speech generation. This task generalizes the task of generating\nspeech from cropped lip videos, and is also more complicated than the task of\ngenerating generic audio clips (e.g., dog barking) from videos and text.\nMultilingual versions of the task could lead to new techniques for\ncross-lingual dubbing. We also present a decoder-only multimodal model for this\ntask, which we call Visatronic. This model embeds vision, text and speech\ndirectly into the common subspace of a transformer model and uses an\nautoregressive loss to learn a generative model of discretized mel-spectrograms\nconditioned on speaker videos and transcripts of their speech. By embedding all\nmodalities into a common subspace, Visatronic can achieve improved results over\nmodels that use only text or video as input. Further, it presents a much\nsimpler approach for multimodal speech generation compared to prevailing\napproaches which rely on lip-detectors and complicated architectures to fuse\nmodalities while producing better results. Since the model is flexible enough\nto accommodate different ways of ordering inputs as a sequence, we carefully\nexplore different strategies to better understand the best way to propagate\ninformation to the generative steps. To facilitate further research on VTTS, we\nwill release (i) our code, (ii) clean transcriptions for the large-scale\nVoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS\nincorporating both objective and subjective metrics.", "published": "2024-11-26 18:57:29", "link": "http://arxiv.org/abs/2411.17690v1", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound\neffects that diverge significantly from real-life sources and flexible control\nin the sound design. To address this problem, we introduce MultiFoley, a model\ndesigned for video-guided sound generation that supports multimodal\nconditioning through text, audio, and video. Given a silent video and a text\nprompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels\nspinning without wind noise) or more whimsical sounds (e.g., making a lion's\nroar sound like a cat's meow). MultiFoley also allows users to choose reference\naudio from sound effects (SFX) libraries or partial videos for conditioning. A\nkey novelty of our model lies in its joint training on both internet video\ndatasets with low-quality audio and professional SFX recordings, enabling\nhigh-quality, full-bandwidth (48kHz) audio generation. Through automated\nevaluations and human studies, we demonstrate that MultiFoley successfully\ngenerates synchronized high-quality sounds across varied conditional inputs and\noutperforms existing methods. Please see our project page for video results:\nhttps://ificl.github.io/MultiFoley/", "published": "2024-11-26 18:59:58", "link": "http://arxiv.org/abs/2411.17698v4", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
