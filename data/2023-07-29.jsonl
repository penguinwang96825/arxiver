{"title": "GeneMask: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning", "abstract": "Large-scale language models such as DNABert and LOGO aim to learn optimal\ngene representations and are trained on the entire Human Reference Genome.\nHowever, standard tokenization schemes involve a simple sliding window of\ntokens like k-mers that do not leverage any gene-based semantics and thus may\nlead to (trivial) masking of easily predictable sequences and subsequently\ninefficient Masked Language Modeling (MLM) training. Therefore, we propose a\nnovel masking algorithm, GeneMask, for MLM training of gene sequences, where we\nrandomly identify positions in a gene sequence as mask centers and locally\nselect the span around the mask center with the highest Normalized Pointwise\nMutual Information (NPMI) to mask. We observe that in the absence of\nhuman-understandable semantics in the genomics domain (in contrast, semantic\nunits like words and phrases are inherently available in NLP), GeneMask-based\nmodels substantially outperform the SOTA models (DNABert and LOGO) over four\nbenchmark gene sequence classification datasets in five few-shot settings (10\nto 1000-shot). More significantly, the GeneMask-based DNABert model is trained\nfor less than one-tenth of the number of epochs of the original SOTA model. We\nalso observe a strong correlation between top-ranked PMI tokens and conserved\nDNA sequence motifs, which may indicate the incorporation of latent genomic\ninformation. The codes (including trained models) and datasets are made\npublicly available at https://github.com/roysoumya/GeneMask.", "published": "2023-07-29 09:17:16", "link": "http://arxiv.org/abs/2307.15933v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Codable Watermarking for Injecting Multi-bits Information to\n  LLMs", "abstract": "As large language models (LLMs) generate texts with increasing fluency and\nrealism, there is a growing need to identify the source of texts to prevent the\nabuse of LLMs. Text watermarking techniques have proven reliable in\ndistinguishing whether a text is generated by LLMs by injecting hidden\npatterns. However, we argue that existing LLM watermarking methods are\nencoding-inefficient and cannot flexibly meet the diverse information encoding\nneeds (such as encoding model version, generation time, user id, etc.). In this\nwork, we conduct the first systematic study on the topic of Codable Text\nWatermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit\ncustomizable information. First of all, we study the taxonomy of LLM\nwatermarking technologies and give a mathematical formulation for CTWL.\nAdditionally, we provide a comprehensive evaluation system for CTWL: (1)\nwatermarking success rate, (2) robustness against various corruptions, (3)\ncoding rate of payload information, (4) encoding and decoding efficiency, (5)\nimpacts on the quality of the generated text. To meet the requirements of these\nnon-Pareto-improving metrics, we follow the most prominent vocabulary\npartition-based watermarking direction, and devise an advanced CTWL method\nnamed Balance-Marking. The core idea of our method is to use a proxy language\nmodel to split the vocabulary into probability-balanced parts, thereby\neffectively maintaining the quality of the watermarked text. Our code is\navailable at https://github.com/lancopku/codable-watermarking-for-llm.", "published": "2023-07-29 14:11:15", "link": "http://arxiv.org/abs/2307.15992v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Extraction of the Romanian Academic Word List: Data and\n  Methods", "abstract": "This paper presents the methodology and data used for the automatic\nextraction of the Romanian Academic Word List (Ro-AWL). Academic Word Lists are\nuseful in both L2 and L1 teaching contexts. For the Romanian language, no such\nresource exists so far. Ro-AWL has been generated by combining methods from\ncorpus and computational linguistics with L2 academic writing approaches. We\nuse two types of data: (a) existing data, such as the Romanian Frequency List\nbased on the ROMBAC corpus, and (b) self-compiled data, such as the expert\nacademic writing corpus EXPRES. For constructing the academic word list, we\nfollow the methodology for building the Academic Vocabulary List for the\nEnglish language. The distribution of Ro-AWL features (general distribution,\nPOS distribution) into four disciplinary datasets is in line with previous\nresearch. Ro-AWL is freely available and can be used for teaching, research and\nNLP applications.", "published": "2023-07-29 18:21:38", "link": "http://arxiv.org/abs/2307.16045v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Roll Up Your Sleeves: Working with a Collaborative and Engaging\n  Task-Oriented Dialogue System", "abstract": "We introduce TacoBot, a user-centered task-oriented digital assistant\ndesigned to guide users through complex real-world tasks with multiple steps.\nCovering a wide range of cooking and how-to tasks, we aim to deliver a\ncollaborative and engaging dialogue experience. Equipped with language\nunderstanding, dialogue management, and response generation components\nsupported by a robust search engine, TacoBot ensures efficient task assistance.\nTo enhance the dialogue experience, we explore a series of data augmentation\nstrategies using LLMs to train advanced neural models continuously. TacoBot\nbuilds upon our successful participation in the inaugural Alexa Prize TaskBot\nChallenge, where our team secured third place among ten competing teams. We\noffer TacoBot as an open-source framework that serves as a practical example\nfor deploying task-oriented dialogue systems.", "published": "2023-07-29 21:37:24", "link": "http://arxiv.org/abs/2307.16081v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ATESA-B\u00c6RT: A Heterogeneous Ensemble Learning Model for Aspect-Based\n  Sentiment Analysis", "abstract": "The increasing volume of online reviews has made possible the development of\nsentiment analysis models for determining the opinion of customers regarding\ndifferent products and services. Until now, sentiment analysis has proven to be\nan effective tool for determining the overall polarity of reviews. To improve\nthe granularity at the aspect level for a better understanding of the service\nor product, the task of aspect-based sentiment analysis aims to first identify\naspects and then determine the user's opinion about them. The complexity of\nthis task lies in the fact that the same review can present multiple aspects,\neach with its own polarity. Current solutions have poor performance on such\ndata. We address this problem by proposing ATESA-B{\\AE}RT, a heterogeneous\nensemble learning model for Aspect-Based Sentiment Analysis. Firstly, we divide\nour problem into two sub-tasks, i.e., Aspect Term Extraction and Aspect Term\nSentiment Analysis. Secondly, we use the \\textit{argmax} multi-class\nclassification on six transformers-based learners for each sub-task. Initial\nexperiments on two datasets prove that ATESA-B{\\AE}RT outperforms current\nstate-of-the-art solutions while solving the many aspects problem.", "published": "2023-07-29 07:35:19", "link": "http://arxiv.org/abs/2307.15920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RoCar: A Relationship Network-based Evaluation Method for Large Language\n  Models", "abstract": "Large language models (LLMs) have received increasing attention. However, due\nto the complexity of its capabilities, how to rationally evaluate the\ncapabilities of LLMs is still a task to be solved. We propose the RoCar method,\nwhich utilizes the defined basic schemas to randomly construct a task graph and\ngenerates natural language evaluation tasks based on the task graph to evaluate\nthe reasoning and memory abilities of LLMs respectively. Due to the very large\nrandomness of the task construction process, it is possible to ensure that none\nof the LLMs to be tested has directly learned the evaluation tasks,\nguaranteeing the fairness of the evaluation method.", "published": "2023-07-29 14:47:07", "link": "http://arxiv.org/abs/2307.15997v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Okapi: Instruction-tuned Large Language Models in Multiple Languages\n  with Reinforcement Learning from Human Feedback", "abstract": "A key technology for the development of large language models (LLMs) involves\ninstruction tuning that helps align the models' responses with human\nexpectations to realize impressive learning abilities. Two major approaches for\ninstruction tuning characterize supervised fine-tuning (SFT) and reinforcement\nlearning from human feedback (RLHF), which are currently applied to produce the\nbest commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for\nresearch and development efforts, various instruction-tuned open-source LLMs\nhave also been introduced recently, e.g., Alpaca, Vicuna, to name a few.\nHowever, existing open-source LLMs have only been instruction-tuned for English\nand a few popular languages, thus hindering their impacts and accessibility to\nmany other languages in the world. Among a few very recent work to explore\ninstruction tuning for LLMs in multiple languages, SFT has been used as the\nonly approach to instruction-tune LLMs for multiple languages. This has left a\nsignificant gap for fine-tuned LLMs based on RLHF in diverse languages and\nraised important questions on how RLHF can boost the performance of\nmultilingual instruction tuning. To overcome this issue, we present Okapi, the\nfirst system with instruction-tuned LLMs based on RLHF for multiple languages.\nOkapi introduces instruction and response-ranked data in 26 diverse languages\nto facilitate the experiments and development of future multilingual LLM\nresearch. We also present benchmark datasets to enable the evaluation of\ngenerative LLMs in multiple languages. Our experiments demonstrate the\nadvantages of RLHF for multilingual instruction over SFT for different base\nmodels and datasets. Our framework and resources are released at\nhttps://github.com/nlp-uoregon/Okapi.", "published": "2023-07-29 18:01:46", "link": "http://arxiv.org/abs/2307.16039v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "EnrichEvent: Enriching Social Data with Contextual Information for\n  Emerging Event Extraction", "abstract": "Social platforms have emerged as crucial platforms for distributing\ninformation and discussing social events, offering researchers an excellent\nopportunity to design and implement novel event detection frameworks.\nIdentifying unspecified events and detecting events without prior knowledge\nenables governments, aid agencies, and experts to respond swiftly and\neffectively to unfolding situations, such as natural disasters, by assessing\nseverity and optimizing aid delivery. Social data is characterized by\nmisspellings, incompleteness, word sense ambiguation, and irregular language.\nWhile discussing an ongoing event, users share different opinions and\nperspectives based on their prior experience, background, and knowledge. Prior\nworks primarily leverage tweets' lexical and structural patterns to capture\nusers' opinions and views about events. In this study, we propose an end-to-end\nnovel framework, EnrichEvent, to identify unspecified events from streaming\nsocial data. In addition to lexical and structural patterns, we leverage\ncontextual knowledge of the tweets to enrich their representation and gain a\nbetter perspective on users' opinions about events. Compared to our baselines,\nthe EnrichEvent framework achieves the highest values for Consolidation outcome\nwith an average of 87% vs. 67% and the lowest for Discrimination outcome with\nan average of 10% vs. 16%. Moreover, the Trending Data Extraction module in the\nEnrichEvent framework improves efficiency by reducing Runtime by up to 50% by\nidentifying and discarding irrelevant tweets within message blocks, making the\nframework highly scalable for processing streaming data. Our source code and\ndataset are available in our official replication package.", "published": "2023-07-29 21:37:55", "link": "http://arxiv.org/abs/2307.16082v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analysing the Resourcefulness of the Paragraph for Precedence Retrieval", "abstract": "Developing methods for extracting relevant legal information to aid legal\npractitioners is an active research area. In this regard, research efforts are\nbeing made by leveraging different kinds of information, such as meta-data,\ncitations, keywords, sentences, paragraphs, etc. Similar to any text document,\nlegal documents are composed of paragraphs. In this paper, we have analyzed the\nresourcefulness of paragraph-level information in capturing similarity among\njudgments for improving the performance of precedence retrieval. We found that\nthe paragraph-level methods could capture the similarity among the judgments\nwith only a few paragraph interactions and exhibit more discriminating power\nover the baseline document-level method. Moreover, the comparison results on\ntwo benchmark datasets for the precedence retrieval on the Indian supreme court\njudgments task show that the paragraph-level methods exhibit comparable\nperformance with the state-of-the-art methods", "published": "2023-07-29 08:55:38", "link": "http://arxiv.org/abs/2308.01203v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Theory for Emergence of Complex Skills in Language Models", "abstract": "A major driver of AI products today is the fact that new skills emerge in\nlanguage models when their parameter set and training corpora are scaled up.\nThis phenomenon is poorly understood, and a mechanistic explanation via\nmathematical analysis of gradient-based training seems difficult. The current\npaper takes a different approach, analysing emergence using the famous (and\nempirical) Scaling Laws of LLMs and a simple statistical framework.\nContributions include: (a) A statistical framework that relates cross-entropy\nloss of LLMs to competence on the basic skills that underlie language tasks.\n(b) Mathematical analysis showing that the Scaling Laws imply a strong form of\ninductive bias that allows the pre-trained model to learn very efficiently. We\ninformally call this {\\em slingshot generalization} since naively viewed it\nappears to give competence levels at skills that violate usual generalization\ntheory. (c) A key example of slingshot generalization, that competence at\nexecuting tasks involving $k$-tuples of skills emerges essentially at the same\nscaling and same rate as competence on the elementary skills themselves.", "published": "2023-07-29 09:22:54", "link": "http://arxiv.org/abs/2307.15936v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Marrying Dialogue Systems with Data Visualization: Interactive Data\n  Visualization Generation from Natural Language Conversations", "abstract": "Data visualization (DV) has become the prevailing tool in the market due to\nits effectiveness into illustrating insights in vast amounts of data. To lower\nthe barrier of using DVs, automatic DV tasks, such as natural language question\n(NLQ) to visualization translation (formally called text-to-vis), have been\ninvestigated in the research community. However, text-to-vis assumes the NLQ to\nbe well-organized and expressed in a single sentence. However, in real-world\nsettings, complex DV is needed through consecutive exchanges between the DV\nsystem and the users. In this paper, we propose a new task named CoVis, short\nfor Conversational text-to-Visualization, aiming at constructing DVs through a\nseries of interactions between users and the system. Since it is the task which\nhas not been studied in the literature, we first build a benchmark dataset\nnamed Dial-NVBench, including dialogue sessions with a sequence of queries from\na user and responses from the system. Then, we propose a multi-modal neural\nnetwork named MMCoVisNet to answer these DV-related queries. In particular,\nMMCoVisNet first fully understands the dialogue context and determines the\ncorresponding responses. Then, it uses adaptive decoders to provide the\nappropriate replies: (i) a straightforward text decoder is used to produce\ngeneral responses, (ii) an SQL-form decoder is applied to synthesize data\nquerying responses, and (iii) a DV-form decoder tries to construct the\nappropriate DVs. We comparatively evaluate MMCoVisNet with other baselines over\nour proposed benchmark dataset. Experimental results validate that MMCoVisNet\nperforms better than existing baselines and achieves a state-of-the-art\nperformance.", "published": "2023-07-29 15:50:07", "link": "http://arxiv.org/abs/2307.16013v1", "categories": ["cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.AI"}
{"title": "\u00ccr\u00f2y\u00ecnSpeech: A multi-purpose Yor\u00f9b\u00e1 Speech Corpus", "abstract": "We introduce \\`{I}r\\`{o}y\\`{i}nSpeech, a new corpus influenced by the desire\nto increase the amount of high quality, contemporary Yor\\`{u}b\\'{a} speech\ndata, which can be used for both Text-to-Speech (TTS) and Automatic Speech\nRecognition (ASR) tasks. We curated about 23000 text sentences from news and\ncreative writing domains with the open license CC-BY-4.0. To encourage a\nparticipatory approach to data creation, we provide 5000 curated sentences to\nthe Mozilla Common Voice platform to crowd-source the recording and validation\nof Yor\\`{u}b\\'{a} speech data. In total, we created about 42 hours of speech\ndata recorded by 80 volunteers in-house, and 6 hours of validated recordings on\nMozilla Common Voice platform. Our TTS evaluation suggests that a\nhigh-fidelity, general domain, single-speaker Yor\\`{u}b\\'{a} voice is possible\nwith as little as 5 hours of speech. Similarly, for ASR we obtained a baseline\nword error rate (WER) of 23.8.", "published": "2023-07-29 20:42:50", "link": "http://arxiv.org/abs/2307.16071v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "METTS: Multilingual Emotional Text-to-Speech by Cross-speaker and\n  Cross-lingual Emotion Transfer", "abstract": "Previous multilingual text-to-speech (TTS) approaches have considered\nleveraging monolingual speaker data to enable cross-lingual speech synthesis.\nHowever, such data-efficient approaches have ignored synthesizing emotional\naspects of speech due to the challenges of cross-speaker cross-lingual emotion\ntransfer - the heavy entanglement of speaker timbre, emotion, and language\nfactors in the speech signal will make a system produce cross-lingual synthetic\nspeech with an undesired foreign accent and weak emotion expressiveness. This\npaper proposes the Multilingual Emotional TTS (METTS) model to mitigate these\nproblems, realizing both cross-speaker and cross-lingual emotion transfer.\nSpecifically, METTS takes DelightfulTTS as the backbone model and proposes the\nfollowing designs. First, to alleviate the foreign accent problem, METTS\nintroduces multi-scale emotion modeling to disentangle speech prosody into\ncoarse-grained and fine-grained scales, producing language-agnostic and\nlanguage-specific emotion representations, respectively. Second, as a\npre-processing step, formant shift-based information perturbation is applied to\nthe reference signal for better disentanglement of speaker timbre in the\nspeech. Third, a vector quantization-based emotion matcher is designed for\nreference selection, leading to decent naturalness and emotion diversity in\ncross-lingual synthetic speech. Experiments demonstrate the good design of\nMETTS.", "published": "2023-07-29 10:46:11", "link": "http://arxiv.org/abs/2307.15951v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MSStyleTTS: Multi-Scale Style Modeling with Hierarchical Context\n  Information for Expressive Speech Synthesis", "abstract": "Expressive speech synthesis is crucial for many human-computer interaction\nscenarios, such as audiobooks, podcasts, and voice assistants. Previous works\nfocus on predicting the style embeddings at one single scale from the\ninformation within the current sentence. Whereas, context information in\nneighboring sentences and multi-scale nature of style in human speech are\nneglected, making it challenging to convert multi-sentence text into natural\nand expressive speech. In this paper, we propose MSStyleTTS, a style modeling\nmethod for expressive speech synthesis, to capture and predict styles at\ndifferent levels from a wider range of context rather than a sentence. Two\nsub-modules, including multi-scale style extractor and multi-scale style\npredictor, are trained together with a FastSpeech 2 based acoustic model. The\npredictor is designed to explore the hierarchical context information by\nconsidering structural relationships in context and predict style embeddings at\nglobal-level, sentence-level and subword-level. The extractor extracts\nmulti-scale style embedding from the ground-truth speech and explicitly guides\nthe style prediction. Evaluations on both in-domain and out-of-domain audiobook\ndatasets demonstrate that the proposed method significantly outperforms the\nthree baselines. In addition, we conduct the analysis of the context\ninformation and multi-scale style representations that have never been\ndiscussed before.", "published": "2023-07-29 15:48:20", "link": "http://arxiv.org/abs/2307.16012v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "UniBriVL: Robust Universal Representation and Generation of Audio Driven\n  Diffusion Models", "abstract": "Multimodal large models have been recognized for their advantages in various\nperformance and downstream tasks. The development of these models is crucial\ntowards achieving general artificial intelligence in the future. In this paper,\nwe propose a novel universal language representation learning method called\nUniBriVL, which is based on Bridging-Vision-and-Language (BriVL). Universal\nBriVL embeds audio, image, and text into a shared space, enabling the\nrealization of various multimodal applications. Our approach addresses major\nchallenges in robust language (both text and audio) representation learning and\neffectively captures the correlation between audio and image. Additionally, we\ndemonstrate the qualitative evaluation of the generated images from UniBriVL,\nwhich serves to highlight the potential of our approach in creating images from\naudio. Overall, our experimental results demonstrate the efficacy of UniBriVL\nin downstream tasks and its ability to choose appropriate images from audio.\nThe proposed approach has the potential for various applications such as speech\nrecognition, music signal processing, and captioning systems.", "published": "2023-07-29 05:55:25", "link": "http://arxiv.org/abs/2307.15898v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Moisesdb: A dataset for source separation beyond 4-stems", "abstract": "In this paper, we introduce the MoisesDB dataset for musical source\nseparation. It consists of 240 tracks from 45 artists, covering twelve musical\ngenres. For each song, we provide its individual audio sources, organized in a\ntwo-level hierarchical taxonomy of stems. This will facilitate building and\nevaluating fine-grained source separation systems that go beyond the limitation\nof using four stems (drums, bass, other, and vocals) due to lack of data. To\nfacilitate the adoption of this dataset, we publish an easy-to-use Python\nlibrary to download, process and use MoisesDB. Alongside a thorough\ndocumentation and analysis of the dataset contents, this work provides baseline\nresults for open-source separation models for varying separation granularities\n(four, five, and six stems), and discuss their results.", "published": "2023-07-29 06:59:37", "link": "http://arxiv.org/abs/2307.15913v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Monaural Multi-Speaker Speech Separation Using Efficient Transformer\n  Model", "abstract": "Cocktail party problem is the scenario where it is difficult to separate or\ndistinguish individual speaker from a mixed speech from several speakers. There\nhave been several researches going on in this field but the size and complexity\nof the model is being traded off with the accuracy and robustness of speech\nseparation. \"Monaural multi-speaker speech separation\" presents a\nspeech-separation model based on the Transformer architecture and its efficient\nforms. The model has been trained with the LibriMix dataset containing diverse\nspeakers' utterances. The model separates 2 distinct speaker sources from a\nmixed audio input. The developed model approaches the reduction in\ncomputational complexity of the speech separation model, with minimum tradeoff\nwith the performance of prevalent speech separation model and it has shown\nsignificant movement towards that goal. This project foresees, a rise in\ncontribution towards the ongoing research in the field of speech separation\nwith computational efficiency at its core.", "published": "2023-07-29 15:10:46", "link": "http://arxiv.org/abs/2308.00010v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "68T10", "I.2.m"], "primary_category": "cs.SD"}
