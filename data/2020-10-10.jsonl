{"title": "Self-play for Data Efficient Language Acquisition", "abstract": "When communicating, people behave consistently across conversational roles:\nPeople understand the words they say and are able to produce the words they\nhear. To date, artificial agents developed for language tasks have lacked such\nsymmetry, meaning agents trained to produce language are unable to understand\nit and vice-versa. In this work, we exploit the symmetric nature of\ncommunication in order to improve both the efficiency and quality of language\nacquisition in learning agents. Specifically, we consider the setting in which\nan agent must learn to both understand and generate words in an existing\nlanguage, but with the assumption that access to interaction with \"oracle\"\nspeakers of the language is very limited. We show that using self-play as a\nsubstitute for direct supervision enables the agent to transfer its knowledge\nacross roles (e.g. training as a listener but testing as a speaker) and make\nbetter inferences about the ground truth lexicon using only a handful of\ninteractions with the oracle.", "published": "2020-10-10 02:09:19", "link": "http://arxiv.org/abs/2010.04872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discourse structure interacts with reference but not syntax in neural\n  language models", "abstract": "Language models (LMs) trained on large quantities of text have been claimed\nto acquire abstract linguistic representations. Our work tests the robustness\nof these abstractions by focusing on the ability of LMs to learn interactions\nbetween different linguistic representations. In particular, we utilized\nstimuli from psycholinguistic studies showing that humans can condition\nreference (i.e. coreference resolution) and syntactic processing on the same\ndiscourse structure (implicit causality). We compared both transformer and long\nshort-term memory LMs to find that, contrary to humans, implicit causality only\ninfluences LM behavior for reference, not syntax, despite model representations\nthat encode the necessary discourse information. Our results further suggest\nthat LM behavior can contradict not only learned representations of discourse\nbut also syntactic agreement, pointing to shortcomings of standard language\nmodeling.", "published": "2020-10-10 03:14:00", "link": "http://arxiv.org/abs/2010.04887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Information Extraction from Swedish Medical Prescriptions with\n  Sig-Transformer Encoder", "abstract": "Relying on large pretrained language models such as Bidirectional Encoder\nRepresentations from Transformers (BERT) for encoding and adding a simple\nprediction layer has led to impressive performance in many clinical natural\nlanguage processing (NLP) tasks. In this work, we present a novel extension to\nthe Transformer architecture, by incorporating signature transform with the\nself-attention model. This architecture is added between embedding and\nprediction layers. Experiments on a new Swedish prescription data show the\nproposed architecture to be superior in two of the three information extraction\ntasks, comparing to baseline models. Finally, we evaluate two different\nembedding approaches between applying Multilingual BERT and translating the\nSwedish text to English then encode with a BERT model pretrained on clinical\nnotes.", "published": "2020-10-10 04:22:07", "link": "http://arxiv.org/abs/2010.04897v1", "categories": ["cs.CL", "60L10", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Latent Tree Learning with Ordered Neurons: What Parses Does It Produce?", "abstract": "Recent latent tree learning models can learn constituency parsing without any\nexposure to human-annotated tree structures. One such model is ON-LSTM (Shen et\nal., 2019), which is trained on language modelling and has\nnear-state-of-the-art performance on unsupervised parsing. In order to better\nunderstand the performance and consistency of the model as well as how the\nparses it generates are different from gold-standard PTB parses, we replicate\nthe model with different restarts and examine their parses. We find that (1)\nthe model has reasonably consistent parsing behaviors across different\nrestarts, (2) the model struggles with the internal structures of complex noun\nphrases, (3) the model has a tendency to overestimate the height of the split\npoints right before verbs. We speculate that both problems could potentially be\nsolved by adopting a different training task other than unidirectional language\nmodelling.", "published": "2020-10-10 07:12:48", "link": "http://arxiv.org/abs/2010.04926v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Hearst Is not Enough: Improving Hypernymy Detection from Corpus\n  with Distributional Models", "abstract": "We address hypernymy detection, i.e., whether an is-a relationship exists\nbetween words (x, y), with the help of large textual corpora. Most conventional\napproaches to this task have been categorized to be either pattern-based or\ndistributional. Recent studies suggest that pattern-based ones are superior, if\nlarge-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x,\ny) pairs relieved. However, they become invalid in some specific sparsity\ncases, where x or y is not involved in any pattern. For the first time, this\npaper quantifies the non-negligible existence of those specific cases. We also\ndemonstrate that distributional methods are ideal to make up for pattern-based\nones in such cases. We devise a complementary framework, under which a\npattern-based and a distributional model collaborate seamlessly in cases which\nthey each prefer. On several benchmark datasets, our framework achieves\ncompetitive improvements and the case study shows its better interpretability.", "published": "2020-10-10 08:34:19", "link": "http://arxiv.org/abs/2010.04941v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MS-Ranker: Accumulating Evidence from Potentially Correct Candidates for\n  Answer Selection", "abstract": "As conventional answer selection (AS) methods generally match the question\nwith each candidate answer independently, they suffer from the lack of matching\ninformation between the question and the candidate. To address this problem, we\npropose a novel reinforcement learning (RL) based multi-step ranking model,\nnamed MS-Ranker, which accumulates information from potentially correct\ncandidate answers as extra evidence for matching the question with a candidate.\nIn specific, we explicitly consider the potential correctness of candidates and\nupdate the evidence with a gating mechanism. Moreover, as we use a listwise\nranking reward, our model learns to pay more attention to the overall\nperformance. Experiments on two benchmarks, namely WikiQA and SemEval-2016 CQA,\nshow that our model significantly outperforms existing methods that do not rely\non external resources.", "published": "2020-10-10 10:36:58", "link": "http://arxiv.org/abs/2010.04970v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual\n  Patterns", "abstract": "This paper describes our submission of the WMT 2020 Shared Task on Sentence\nLevel Direct Assessment, Quality Estimation (QE). In this study, we empirically\nreveal the \\textit{mismatching issue} when directly adopting BERTScore to QE.\nSpecifically, there exist lots of mismatching errors between the source\nsentence and translated candidate sentence with token pairwise similarity. In\nresponse to this issue, we propose to expose explicit cross-lingual patterns,\n\\textit{e.g.} word alignments and generation score, to our proposed zero-shot\nmodels. Experiments show that our proposed QE model with explicit cross-lingual\npatterns could alleviate the mismatching issue, thereby improving the\nperformance. Encouragingly, our zero-shot QE method could achieve comparable\nperformance with supervised QE method, and even outperforms the supervised\ncounterpart on 2 out of 6 directions. We expect our work could shed light on\nthe zero-shot QE model improvement.", "published": "2020-10-10 13:11:41", "link": "http://arxiv.org/abs/2010.04989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Compressing Transformer-Based Semantic Parsing Models using\n  Compositional Code Embeddings", "abstract": "The current state-of-the-art task-oriented semantic parsing models use BERT\nor RoBERTa as pretrained encoders; these models have huge memory footprints.\nThis poses a challenge to their deployment for voice assistants such as Amazon\nAlexa and Google Assistant on edge devices with limited memory budgets. We\npropose to learn compositional code embeddings to greatly reduce the sizes of\nBERT-base and RoBERTa-base. We also apply the technique to DistilBERT,\nALBERT-base, and ALBERT-large, three already compressed BERT variants which\nattain similar state-of-the-art performances on semantic parsing with much\nsmaller model sizes. We observe 95.15% ~ 98.46% embedding compression rates and\n20.47% ~ 34.22% encoder compression rates, while preserving greater than 97.5%\nsemantic parsing performances. We provide the recipe for training and analyze\nthe trade-off between code embedding sizes and downstream performances.", "published": "2020-10-10 13:47:55", "link": "http://arxiv.org/abs/2010.05002v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Spatial Information in Radiology Reports for Ischemic Stroke\n  Phenotyping", "abstract": "Classifying fine-grained ischemic stroke phenotypes relies on identifying\nimportant clinical information. Radiology reports provide relevant information\nwith context to determine such phenotype information. We focus on stroke\nphenotypes with location-specific information: brain region affected,\nlaterality, stroke stage, and lacunarity. We use an existing fine-grained\nspatial information extraction system--Rad-SpatialNet--to identify clinically\nimportant information and apply simple domain rules on the extracted\ninformation to classify phenotypes. The performance of our proposed approach is\npromising (recall of 89.62% for classifying brain region and 74.11% for\nclassifying brain region, side, and stroke stage together). Our work\ndemonstrates that an information extraction system based on a fine-grained\nschema can be utilized to determine complex phenotypes with the inclusion of\nsimple domain rules. These phenotypes have the potential to facilitate stroke\nresearch focusing on post-stroke outcome and treatment planning based on the\nstroke location.", "published": "2020-10-10 21:35:42", "link": "http://arxiv.org/abs/2010.05096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Evidence Set Modeling for Automated Fact Extraction and\n  Verification", "abstract": "Automated fact extraction and verification is a challenging task that\ninvolves finding relevant evidence sentences from a reliable corpus to verify\nthe truthfulness of a claim. Existing models either (i) concatenate all the\nevidence sentences, leading to the inclusion of redundant and noisy\ninformation; or (ii) process each claim-evidence sentence pair separately and\naggregate all of them later, missing the early combination of related sentences\nfor more accurate claim verification. Unlike the prior works, in this paper, we\npropose Hierarchical Evidence Set Modeling (HESM), a framework to extract\nevidence sets (each of which may contain multiple evidence sentences), and\nverify a claim to be supported, refuted or not enough info, by encoding and\nattending the claim and evidence sets at different levels of hierarchy. Our\nexperimental results show that HESM outperforms 7 state-of-the-art methods for\nfact extraction and claim verification. Our source code is available at\nhttps://github.com/ShyamSubramanian/HESM.", "published": "2020-10-10 22:27:17", "link": "http://arxiv.org/abs/2010.05111v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "RatE: Relation-Adaptive Translating Embedding for Knowledge Graph\n  Completion", "abstract": "Many graph embedding approaches have been proposed for knowledge graph\ncompletion via link prediction. Among those, translating embedding approaches\nenjoy the advantages of light-weight structure, high efficiency and great\ninterpretability. Especially when extended to complex vector space, they show\nthe capability in handling various relation patterns including symmetry,\nantisymmetry, inversion and composition. However, previous translating\nembedding approaches defined in complex vector space suffer from two main\nissues: 1) representing and modeling capacities of the model are limited by the\ntranslation function with rigorous multiplication of two complex numbers; and\n2) embedding ambiguity caused by one-to-many relations is not explicitly\nalleviated. In this paper, we propose a relation-adaptive translation function\nbuilt upon a novel weighted product in complex space, where the weights are\nlearnable, relation-specific and independent to embedding size. The translation\nfunction only requires eight more scalar parameters each relation, but improves\nexpressive power and alleviates embedding ambiguity problem. Based on the\nfunction, we then present our Relation-adaptive translating Embedding (RatE)\napproach to score each graph triple. Moreover, a novel negative sampling method\nis proposed to utilize both prior knowledge and self-adversarial learning for\neffective optimization. Experiments verify RatE achieves state-of-the-art\nperformance on four link prediction benchmarks.", "published": "2020-10-10 01:30:30", "link": "http://arxiv.org/abs/2010.04863v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Self-Supervised Data-Free Distillation for Text\n  Classification", "abstract": "Large pre-trained transformer-based language models have achieved impressive\nresults on a wide range of NLP tasks. In the past few years, Knowledge\nDistillation(KD) has become a popular paradigm to compress a computationally\nexpensive model to a resource-efficient lightweight model. However, most KD\nalgorithms, especially in NLP, rely on the accessibility of the original\ntraining dataset, which may be unavailable due to privacy issues. To tackle\nthis problem, we propose a novel two-stage data-free distillation method, named\nAdversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed\nfor compressing large-scale transformer-based models (e.g., BERT). To avoid\ntext generation in discrete space, we introduce a Plug & Play Embedding\nGuessing method to craft pseudo embeddings from the teacher's hidden knowledge.\nMeanwhile, with a self-supervised module to quantify the student's ability, we\nadapt the difficulty of pseudo embeddings in an adversarial training manner. To\nthe best of our knowledge, our framework is the first data-free distillation\nframework designed for NLP tasks. We verify the effectiveness of our method on\nseveral text classification datasets.", "published": "2020-10-10 02:46:06", "link": "http://arxiv.org/abs/2010.04883v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Open-Domain Question Answering Goes Conversational via Question\n  Rewriting", "abstract": "We introduce a new dataset for Question Rewriting in Conversational Context\n(QReCC), which contains 14K conversations with 80K question-answer pairs. The\ntask in QReCC is to find answers to conversational questions within a\ncollection of 10M web pages (split into 54M passages). Answers to questions in\nthe same conversation may be distributed across several web pages. QReCC\nprovides annotations that allow us to train and evaluate individual subtasks of\nquestion rewriting, passage retrieval and reading comprehension required for\nthe end-to-end conversational question answering (QA) task. We report the\neffectiveness of a strong baseline approach that combines the state-of-the-art\nmodel for question rewriting, and competitive models for open-domain QA. Our\nresults set the first baseline for the QReCC dataset with F1 of 19.10, compared\nto the human upper bound of 75.45, indicating the difficulty of the setup and a\nlarge room for improvement.", "published": "2020-10-10 04:28:42", "link": "http://arxiv.org/abs/2010.04898v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Toward Micro-Dialect Identification in Diaglossic and Code-Switched\n  Environments", "abstract": "Although the prediction of dialects is an important language processing task,\nwith a wide range of applications, existing work is largely limited to\ncoarse-grained varieties. Inspired by geolocation research, we propose the\nnovel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new\nlanguage model with striking abilities to predict a fine-grained variety (as\nsmall as that of a city) given a single, short message. For modeling, we offer\na range of novel spatially and linguistically-motivated multi-task learning\nmodels. To showcase the utility of our models, we introduce a new, large-scale\ndataset of Arabic micro-varieties (low-resource) suited to our tasks. MARBERT\npredicts micro-dialects with 9.9% F1, ~76X better than a majority class\nbaseline. Our new language model also establishes new state-of-the-art on\nseveral external tasks.", "published": "2020-10-10 04:35:16", "link": "http://arxiv.org/abs/2010.04900v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained\n  Language Model Positional Encoding", "abstract": "In recent years, pre-trained Transformers have dominated the majority of NLP\nbenchmark tasks. Many variants of pre-trained Transformers have kept breaking\nout, and most focus on designing different pre-training objectives or variants\nof self-attention. Embedding the position information in the self-attention\nmechanism is also an indispensable factor in Transformers however is often\ndiscussed at will. Therefore, this paper carries out an empirical study on\nposition embeddings of mainstream pre-trained Transformers, which mainly\nfocuses on two questions: 1) Do position embeddings really learn the meaning of\npositions? 2) How do these different learned position embeddings affect\nTransformers for NLP tasks? This paper focuses on providing a new insight of\npre-trained position embeddings through feature-level analysis and empirical\nexperiments on most of iconic NLP tasks. It is believed that our experimental\nresults can guide the future work to choose the suitable positional encoding\nfunction for specific tasks given the application property.", "published": "2020-10-10 05:03:14", "link": "http://arxiv.org/abs/2010.04903v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Structured Self-Attention Weights Encode Semantics in Sentiment Analysis", "abstract": "Neural attention, especially the self-attention made popular by the\nTransformer, has become the workhorse of state-of-the-art natural language\nprocessing (NLP) models. Very recent work suggests that the self-attention in\nthe Transformer encodes syntactic information; Here, we show that\nself-attention scores encode semantics by considering sentiment analysis tasks.\nIn contrast to gradient-based feature attribution methods, we propose a simple\nand effective Layer-wise Attention Tracing (LAT) method to analyze structured\nattention weights. We apply our method to Transformer models trained on two\ntasks that have surface dissimilarities, but share common semantics---sentiment\nanalysis of movie reviews and time-series valence prediction in life story\nnarratives. Across both tasks, words with high aggregated attention weights\nwere rich in emotional semantics, as quantitatively validated by an emotion\nlexicon labeled by human annotators. Our results show that structured attention\nweights encode rich semantics in sentiment analysis, and match human\ninterpretations of semantics.", "published": "2020-10-10 06:49:25", "link": "http://arxiv.org/abs/2010.04922v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cue-word Driven Neural Response Generation with a Shrinking Vocabulary", "abstract": "Open-domain response generation is the task of generating sensible and\ninformative re-sponses to the source sentence. However, neural models tend to\ngenerate safe and mean-ingless responses. While cue-word introducing approaches\nencourage responses with concrete semantics and have shown tremendous\npotential, they still fail to explore di-verse responses during decoding. In\nthis paper, we propose a novel but natural approach that can produce multiple\ncue-words during decoding, and then uses the produced cue-words to drive\ndecoding and shrinks the decoding vocabulary. Thus the neural genera-tion model\ncan explore the full space of responses and discover informative ones with\nefficiency. Experimental results show that our approach significantly\noutperforms several strong baseline models with much lower decoding complexity.\nEspecially, our approach can converge to concrete semantics more efficiently\nduring decoding.", "published": "2020-10-10 07:13:32", "link": "http://arxiv.org/abs/2010.04927v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HPCC-YNU at SemEval-2020 Task 9: A Bilingual Vector Gating Mechanism for\n  Sentiment Analysis of Code-Mixed Text", "abstract": "It is fairly common to use code-mixing on a social media platform to express\nopinions and emotions in multilingual societies. The purpose of this task is to\ndetect the sentiment of code-mixed social media text. Code-mixed text poses a\ngreat challenge for the traditional NLP system, which currently uses\nmonolingual resources to deal with the problem of multilingual mixing. This\ntask has been solved in the past using lexicon lookup in respective sentiment\ndictionaries and using a long short-term memory (LSTM) neural network for\nmonolingual resources. In this paper, we (my codalab username is kongjun)\npresent a system that uses a bilingual vector gating mechanism for bilingual\nresources to complete the task. The model consists of two main parts: the\nvector gating mechanism, which combines the character and word levels, and the\nattention mechanism, which extracts the important emotional parts of the text.\nThe results show that the proposed system outperforms the baseline algorithm.\nWe achieved fifth place in Spanglish and 19th place in Hinglish.The code of\nthis paper is availabled at : https://github.com/JunKong5/Semveal2020-task9", "published": "2020-10-10 08:02:15", "link": "http://arxiv.org/abs/2010.04935v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tag Recommendation for Online Q&A Communities based on BERT Pre-Training\n  Technique", "abstract": "Online Q&A and open source communities use tags and keywords to index,\ncategorize, and search for specific content. The most obvious advantage of tag\nrecommendation is the correct classification of information. In this study, we\nused the BERT pre-training technique in tag recommendation task for online Q&A\nand open-source communities for the first time. Our evaluation on freecode\ndatasets show that the proposed method, called TagBERT, is more accurate\ncompared to deep learning and other baseline methods. Moreover, our model\nachieved a high stability by solving the problem of previous researches, where\nincreasing the number of tag recommendations significantly reduced model\nperformance.", "published": "2020-10-10 10:52:22", "link": "http://arxiv.org/abs/2010.04971v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Can RNNs trained on harder subject-verb agreement instances still\n  perform well on easier ones?", "abstract": "Previous work suggests that RNNs trained on natural language corpora can\ncapture number agreement well for simple sentences but perform less well when\nsentences contain agreement attractors: intervening nouns between the verb and\nthe main subject with grammatical number opposite to the latter. This suggests\nthese models may not learn the actual syntax of agreement, but rather infer\nshallower heuristics such as `agree with the recent noun'. In this work, we\ninvestigate RNN models with varying inductive biases trained on selectively\nchosen `hard' agreement instances, i.e., sentences with at least one agreement\nattractor. For these the verb number cannot be predicted using a simple linear\nheuristic, and hence they might help provide the model additional cues for\nhierarchical syntax. If RNNs can learn the underlying agreement rules when\ntrained on such hard instances, then they should generalize well to other\nsentences, including simpler ones. However, we observe that several RNN types,\nincluding the ONLSTM which has a soft structural inductive bias, surprisingly\nfail to perform well on sentences without attractors when trained solely on\nsentences with attractors. We analyze how these selectively trained RNNs\ncompare to the baseline (training on a natural distribution of agreement\nattractors) along the dimensions of number agreement accuracy, representational\nsimilarity, and performance across different syntactic constructions. Our\nfindings suggest that RNNs trained on our hard agreement instances still do not\ncapture the underlying syntax of agreement, but rather tend to overfit the\ntraining distribution in a way which leads them to perform poorly on `easy'\nout-of-distribution instances. Thus, while RNNs are powerful models which can\npick up non-trivial dependency patterns, inducing them to do so at the level of\nsyntax rather than surface remains a challenge.", "published": "2020-10-10 11:30:58", "link": "http://arxiv.org/abs/2010.04976v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Empirical Investigation of Beam-Aware Training in Supertagging", "abstract": "Structured prediction is often approached by training a locally normalized\nmodel with maximum likelihood and decoding approximately with beam search. This\napproach leads to mismatches as, during training, the model is not exposed to\nits mistakes and does not use beam search. Beam-aware training aims to address\nthese problems, but unfortunately, it is not yet widely used due to a lack of\nunderstanding about how it impacts performance, when it is most useful, and\nwhether it is stable. Recently, Negrinho et al. (2018) proposed a\nmeta-algorithm that captures beam-aware training algorithms and suggests new\nones, but unfortunately did not provide empirical results. In this paper, we\nbegin an empirical investigation: we train the supertagging model of Vaswani et\nal. (2016) and a simpler model with instantiations of the meta-algorithm. We\nexplore the influence of various design choices and make recommendations for\nchoosing them. We observe that beam-aware training improves performance for\nboth models, with large improvements for the simpler model which must\neffectively manage uncertainty during decoding. Our results suggest that a\nmodel must be learned with search to maximize its effectiveness.", "published": "2020-10-10 12:25:18", "link": "http://arxiv.org/abs/2010.04980v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Beyond Language: Learning Commonsense from Images for Reasoning", "abstract": "This paper proposes a novel approach to learn commonsense from images,\ninstead of limited raw texts or costly constructed knowledge bases, for the\ncommonsense reasoning problem in NLP. Our motivation comes from the fact that\nan image is worth a thousand words, where richer scene information could be\nleveraged to help distill the commonsense knowledge, which is often hidden in\nlanguages. Our approach, namely Loire, consists of two stages. In the first\nstage, a bi-modal sequence-to-sequence approach is utilized to conduct the\nscene layout generation task, based on a text representation model ViBERT. In\nthis way, the required visual scene knowledge, such as spatial relations, will\nbe encoded in ViBERT by the supervised learning process with some bi-modal data\nlike COCO. Then ViBERT is concatenated with a pre-trained language model to\nperform the downstream commonsense reasoning tasks. Experimental results on two\ncommonsense reasoning problems, i.e. commonsense question answering and pronoun\nresolution, demonstrate that Loire outperforms traditional language-based\nmethods. We also give some case studies to show what knowledge is learned from\nimages and explain how the generated scene layout helps the commonsense\nreasoning process.", "published": "2020-10-10 13:47:13", "link": "http://arxiv.org/abs/2010.05001v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Second-Order Neural Dependency Parsing with Message Passing and\n  End-to-End Training", "abstract": "In this paper, we propose second-order graph-based neural dependency parsing\nusing message passing and end-to-end neural networks. We empirically show that\nour approaches match the accuracy of very recent state-of-the-art second-order\ngraph-based neural dependency parsers and have significantly faster speed in\nboth training and testing. We also empirically show the advantage of\nsecond-order parsing over first-order parsing and observe that the usefulness\nof the head-selection structured constraint vanishes when using BERT embedding.", "published": "2020-10-10 13:49:46", "link": "http://arxiv.org/abs/2010.05003v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Importance of Adaptive Data Collection for Extremely Imbalanced\n  Pairwise Tasks", "abstract": "Many pairwise classification tasks, such as paraphrase detection and\nopen-domain question answering, naturally have extreme label imbalance (e.g.,\n$99.99\\%$ of examples are negatives). In contrast, many recent datasets\nheuristically choose examples to ensure label balance. We show that these\nheuristics lead to trained models that generalize poorly: State-of-the art\nmodels trained on QQP and WikiQA each have only $2.4\\%$ average precision when\nevaluated on realistically imbalanced test data. We instead collect training\ndata with active learning, using a BERT-based embedding model to efficiently\nretrieve uncertain points from a very large pool of unlabeled utterance pairs.\nBy creating balanced training data with more informative negative examples,\nactive learning greatly improves average precision to $32.5\\%$ on QQP and\n$20.1\\%$ on WikiQA.", "published": "2020-10-10 21:56:27", "link": "http://arxiv.org/abs/2010.05103v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine\n  Translation", "abstract": "We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural\nMachine Translation (NMT) systems to localize a semantic parser for a new\nlanguage. Our methodology is to (1) generate training data automatically in the\ntarget language by augmenting machine-translated datasets with local entities\nscraped from public websites, (2) add a few-shot boost of human-translated\nsentences and train a novel XLMR-LSTM semantic parser, and (3) test the model\non natural utterances curated using human translators.\n  We assess the effectiveness of our approach by extending the current\ncapabilities of Schema2QA, a system for English Question Answering (QA) on the\nopen web, to 10 new languages for the restaurants and hotels domains. Our\nmodels achieve an overall test accuracy ranging between 61% and 69% for the\nhotels domain and between 64% and 78% for restaurants domain, which compares\nfavorably to 69% and 80% obtained for English parser trained on gold English\ndata and a few examples from validation set. We show our approach outperforms\nthe previous state-of-the-art methodology by more than 30% for hotels and 40%\nfor restaurants with localized ontologies for the subset of languages tested.\n  Our methodology enables any software developer to add a new language\ncapability to a QA system for a new domain, leveraging machine translation, in\nless than 24 hours.", "published": "2020-10-10 22:03:58", "link": "http://arxiv.org/abs/2010.05106v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Long-Tailed Phenomena in Neural Machine Translation", "abstract": "State-of-the-art Neural Machine Translation (NMT) models struggle with\ngenerating low-frequency tokens, tackling which remains a major challenge. The\nanalysis of long-tailed phenomena in the context of structured prediction tasks\nis further hindered by the added complexities of search during inference. In\nthis work, we quantitatively characterize such long-tailed phenomena at two\nlevels of abstraction, namely, token classification and sequence generation. We\npropose a new loss function, the Anti-Focal loss, to better adapt model\ntraining to the structural dependencies of conditional text generation by\nincorporating the inductive biases of beam search in the training process. We\nshow the efficacy of the proposed technique on a number of Machine Translation\n(MT) datasets, demonstrating that it leads to significant gains over\ncross-entropy across different language pairs, especially on the generation of\nlow-frequency words. We have released the code to reproduce our results.", "published": "2020-10-10 07:00:57", "link": "http://arxiv.org/abs/2010.04924v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FIND: Human-in-the-Loop Debugging Deep Text Classifiers", "abstract": "Since obtaining a perfect training dataset (i.e., a dataset which is\nconsiderably large, unbiased, and well-representative of unseen cases) is\nhardly possible, many real-world text classifiers are trained on the available,\nyet imperfect, datasets. These classifiers are thus likely to have undesirable\nproperties. For instance, they may have biases against some sub-populations or\nmay not work effectively in the wild due to overfitting. In this paper, we\npropose FIND -- a framework which enables humans to debug deep learning text\nclassifiers by disabling irrelevant hidden features. Experiments show that by\nusing FIND, humans can improve CNN text classifiers which were trained under\ndifferent types of imperfect datasets (including datasets with biases and\ndatasets with dissimilar train-test distributions).", "published": "2020-10-10 12:52:53", "link": "http://arxiv.org/abs/2010.04987v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Concatenation of Embeddings for Structured Prediction", "abstract": "Pretrained contextualized embeddings are powerful word representations for\nstructured prediction tasks. Recent work found that better word representations\ncan be obtained by concatenating different types of embeddings. However, the\nselection of embeddings to form the best concatenated representation usually\nvaries depending on the task and the collection of candidate embeddings, and\nthe ever-increasing number of embedding types makes it a more difficult\nproblem. In this paper, we propose Automated Concatenation of Embeddings (ACE)\nto automate the process of finding better concatenations of embeddings for\nstructured prediction tasks, based on a formulation inspired by recent progress\non neural architecture search. Specifically, a controller alternately samples a\nconcatenation of embeddings, according to its current belief of the\neffectiveness of individual embedding types in consideration for a task, and\nupdates the belief based on a reward. We follow strategies in reinforcement\nlearning to optimize the parameters of the controller and compute the reward\nbased on the accuracy of a task model, which is fed with the sampled\nconcatenation as input and trained on a task dataset. Empirical results on 6\ntasks and 21 datasets show that our approach outperforms strong baselines and\nachieves state-of-the-art performance with fine-tuned embeddings in all the\nevaluations.", "published": "2020-10-10 14:03:20", "link": "http://arxiv.org/abs/2010.05006v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structural Knowledge Distillation: Tractably Distilling Information for\n  Structured Predictor", "abstract": "Knowledge distillation is a critical technique to transfer knowledge between\nmodels, typically from a large model (the teacher) to a more fine-grained one\n(the student). The objective function of knowledge distillation is typically\nthe cross-entropy between the teacher and the student's output distributions.\nHowever, for structured prediction problems, the output space is exponential in\nsize; therefore, the cross-entropy objective becomes intractable to compute and\noptimize directly. In this paper, we derive a factorized form of the knowledge\ndistillation objective for structured prediction, which is tractable for many\ntypical choices of the teacher and student models. In particular, we show the\ntractability and empirical effectiveness of structural knowledge distillation\nbetween sequence labeling and dependency parsing models under four different\nscenarios: 1) the teacher and student share the same factorization form of the\noutput structure scoring function; 2) the student factorization produces more\nfine-grained substructures than the teacher factorization; 3) the teacher\nfactorization produces more fine-grained substructures than the student\nfactorization; 4) the factorization forms from the teacher and the student are\nincompatible.", "published": "2020-10-10 14:19:25", "link": "http://arxiv.org/abs/2010.05010v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Formality Style Transfer using Language Model\n  Discriminator and Mutual Information Maximization", "abstract": "Formality style transfer is the task of converting informal sentences to\ngrammatically-correct formal sentences, which can be used to improve\nperformance of many downstream NLP tasks. In this work, we propose a\nsemi-supervised formality style transfer model that utilizes a language\nmodel-based discriminator to maximize the likelihood of the output sentence\nbeing formal, which allows us to use maximization of token-level conditional\nprobabilities for training. We further propose to maximize mutual information\nbetween source and target styles as our training objective instead of\nmaximizing the regular likelihood that often leads to repetitive and trivial\ngenerated responses. Experiments showed that our model outperformed previous\nstate-of-the-art baselines significantly in terms of both automated metrics and\nhuman judgement. We further generalized our model to unsupervised text style\ntransfer task, and achieved significant improvements on two benchmark sentiment\nstyle transfer datasets.", "published": "2020-10-10 21:05:56", "link": "http://arxiv.org/abs/2010.05090v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Acoustic Scattering Fields for Dynamic Interactive Sound\n  Propagation", "abstract": "We present a novel hybrid sound propagation algorithm for interactive\napplications. Our approach is designed for dynamic scenes and uses a neural\nnetwork-based learned scattered field representation along with ray tracing to\ngenerate specular, diffuse, diffraction, and occlusion effects efficiently. We\nuse geometric deep learning to approximate the acoustic scattering field using\nspherical harmonics. We use a large 3D dataset for training, and compare its\naccuracy with the ground truth generated using an accurate wave-based solver.\nThe additional overhead of computing the learned scattered field at runtime is\nsmall and we demonstrate its interactive performance by generating plausible\nsound effects in dynamic scenes with diffraction and occlusion effects. We\ndemonstrate the perceptual benefits of our approach based on an audio-visual\nuser study.", "published": "2020-10-10 01:43:50", "link": "http://arxiv.org/abs/2010.04865v2", "categories": ["cs.SD", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Model Compression Method with Matrix Product Operators for Speech\n  Enhancement", "abstract": "The deep neural network (DNN) based speech enhancement approaches have\nachieved promising performance. However, the number of parameters involved in\nthese methods is usually enormous for the real applications of speech\nenhancement on the device with the limited resources. This seriously restricts\nthe applications. To deal with this issue, model compression techniques are\nbeing widely studied. In this paper, we propose a model compression method\nbased on matrix product operators (MPO) to substantially reduce the number of\nparameters in DNN models for speech enhancement. In this method, the weight\nmatrices in the linear transformations of neural network model are replaced by\nthe MPO decomposition format before training. In experiment, this process is\napplied to the causal neural network models, such as the feedforward multilayer\nperceptron (MLP) and long short-term memory (LSTM) models. Both MLP and LSTM\nmodels with/without compression are then utilized to estimate the ideal ratio\nmask for monaural speech enhancement. The experimental results show that our\nproposed MPO-based method outperforms the widely-used pruning method for speech\nenhancement under various compression rates, and further improvement can be\nachieved with respect to low compression rates. Our proposal provides an\neffective model compression method for speech enhancement, especially in\ncloud-free application.", "published": "2020-10-10 08:53:25", "link": "http://arxiv.org/abs/2010.04950v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "quant-ph"], "primary_category": "cs.SD"}
