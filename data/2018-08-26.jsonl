{"title": "Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision", "abstract": "Deep learning has emerged as a versatile tool for a wide range of NLP tasks,\ndue to its superior capacity in representation learning. But its applicability\nis limited by the reliance on annotated examples, which are difficult to\nproduce at scale. Indirect supervision has emerged as a promising direction to\naddress this bottleneck, either by introducing labeling functions to\nautomatically generate noisy examples from unlabeled text, or by imposing\nconstraints over interdependent label decisions. A plethora of methods have\nbeen proposed, each with respective strengths and limitations. Probabilistic\nlogic offers a unifying language to represent indirect supervision, but\nend-to-end modeling with probabilistic logic is often infeasible due to\nintractable inference and learning. In this paper, we propose deep\nprobabilistic logic (DPL) as a general framework for indirect supervision, by\ncomposing probabilistic logic with deep learning. DPL models label decisions as\nlatent variables, represents prior knowledge on their relations using weighted\nfirst-order logical formulas, and alternates between learning a deep neural\nnetwork for the end task and refining uncertain formula weights for indirect\nsupervision, using variational EM. This framework subsumes prior indirect\nsupervision methods as special cases, and enables novel combination via\ninfusion of rich domain and linguistic knowledge. Experiments on biomedical\nmachine reading demonstrate the promise of this approach.", "published": "2018-08-26 00:02:36", "link": "http://arxiv.org/abs/1808.08485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Detection with Neural Networks: A Rigorous Empirical Evaluation", "abstract": "Detecting events and classifying them into predefined types is an important\nstep in knowledge extraction from natural language texts. While the neural\nnetwork models have generally led the state-of-the-art, the differences in\nperformance between different architectures have not been rigorously studied.\nIn this paper we present a novel GRU-based model that combines syntactic\ninformation along with temporal structure through an attention mechanism. We\nshow that it is competitive with other neural network architectures through\nempirical evaluations under different random initializations and\ntraining-validation-test splits of ACE2005 dataset.", "published": "2018-08-26 04:04:39", "link": "http://arxiv.org/abs/1808.08504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Sense Induction with Neural biLM and Symmetric Patterns", "abstract": "An established method for Word Sense Induction (WSI) uses a language model to\npredict probable substitutes for target words, and induces senses by clustering\nthese resulting substitute vectors.\n  We replace the ngram-based language model (LM) with a recurrent one. Beyond\nbeing more accurate, the use of the recurrent LM allows us to effectively query\nit in a creative way, using what we call dynamic symmetric patterns.\n  The combination of the RNN-LM and the dynamic symmetric patterns results in\nstrong substitute vectors for WSI, allowing to surpass the current\nstate-of-the-art on the SemEval 2013 WSI shared task by a large margin.", "published": "2018-08-26 08:36:15", "link": "http://arxiv.org/abs/1808.08518v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic-Unit-Based Dilated Convolution for Multi-Label Text\n  Classification", "abstract": "We propose a novel model for multi-label text classification, which is based\non sequence-to-sequence learning. The model generates higher-level semantic\nunit representations with multi-level dilated convolution as well as a\ncorresponding hybrid attention mechanism that extracts both the information at\nthe word-level and the level of the semantic unit. Our designed dilated\nconvolution effectively reduces dimension and supports an exponential expansion\nof receptive fields without loss of local information, and the\nattention-over-attention mechanism is able to capture more summary relevant\ninformation from the source context. Results of our experiments show that the\nproposed model has significant advantages over the baseline models on the\ndataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is\ncompetitive to the deterministic hierarchical models and it is more robust to\nclassifying low-frequency labels.", "published": "2018-08-26 14:36:22", "link": "http://arxiv.org/abs/1808.08561v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Learned Representations of a Deep ASR Performance Prediction\n  Model", "abstract": "This paper addresses a relatively new task: prediction of ASR performance on\nunseen broadcast programs. In a previous paper, we presented an ASR performance\nprediction system using CNNs that encode both text (ASR transcript) and speech,\nin order to predict word error rate. This work is dedicated to the analysis of\nspeech signal embeddings and text embeddings learnt by the CNN while training\nour prediction model. We try to better understand which information is captured\nby the deep model and its relation with different conditioning factors. It is\nshown that hidden layers convey a clear signal about speech style, accent and\nbroadcast type. We then try to leverage these 3 types of information at\ntraining time through multi-task learning. Our experiments show that this\nallows to train slightly more efficient ASR performance prediction systems that\n- in addition - simultaneously tag the analyzed utterances according to their\nspeech style, accent and broadcast program origin.", "published": "2018-08-26 15:10:47", "link": "http://arxiv.org/abs/1808.08573v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Title-Guided Encoding for Keyphrase Generation", "abstract": "Keyphrase generation (KG) aims to generate a set of keyphrases given a\ndocument, which is a fundamental task in natural language processing (NLP).\nMost previous methods solve this problem in an extractive manner, while\nrecently, several attempts are made under the generative setting using deep\nneural networks. However, the state-of-the-art generative methods simply treat\nthe document title and the document main body equally, ignoring the leading\nrole of the title to the overall document. To solve this problem, we introduce\na new model called Title-Guided Network (TG-Net) for automatic keyphrase\ngeneration task based on the encoder-decoder architecture with two new\nfeatures: (i) the title is additionally employed as a query-like input, and\n(ii) a title-guided encoder gathers the relevant information from the title to\neach word in the document. Experiments on a range of KG datasets demonstrate\nthat our model outperforms the state-of-the-art models with a large margin,\nespecially for documents with either very low or very high title length ratios.", "published": "2018-08-26 15:28:11", "link": "http://arxiv.org/abs/1808.08575v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Autoregressive Neural Machine Translation", "abstract": "Existing approaches to neural machine translation are typically\nautoregressive models. While these models attain state-of-the-art translation\nquality, they are suffering from low parallelizability and thus slow at\ndecoding long sequences. In this paper, we propose a novel model for fast\nsequence generation --- the semi-autoregressive Transformer (SAT). The SAT\nkeeps the autoregressive property in global but relieves in local and thus is\nable to produce multiple successive words in parallel at each time step.\nExperiments conducted on English-German and Chinese-English translation tasks\nshow that the SAT achieves a good balance between translation quality and\ndecoding speed. On WMT'14 English-German translation, the SAT achieves\n5.58$\\times$ speedup while maintains 88\\% translation quality, significantly\nbetter than the previous non-autoregressive methods. When produces two words at\neach time step, the SAT is almost lossless (only 1\\% degeneration in BLEU\nscore).", "published": "2018-08-26 16:22:30", "link": "http://arxiv.org/abs/1808.08583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised Event Extraction with Paraphrase Clusters", "abstract": "Supervised event extraction systems are limited in their accuracy due to the\nlack of available training data. We present a method for self-training event\nextraction systems by bootstrapping additional training data. This is done by\ntaking advantage of the occurrence of multiple mentions of the same event\ninstances across newswire articles from multiple sources. If our system can\nmake a highconfidence extraction of some mentions in such a cluster, it can\nthen acquire diverse training examples by adding the other mentions as well.\nOur experiments show significant performance improvements on multiple event\nextractors over ACE 2005 and TAC-KBP 2015 datasets.", "published": "2018-08-26 21:12:43", "link": "http://arxiv.org/abs/1808.08622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Domain Adjacent Instances for Semantic Parsers", "abstract": "When the semantics of a sentence are not representable in a semantic parser's\noutput schema, parsing will inevitably fail. Detection of these instances is\ncommonly treated as an out-of-domain classification problem. However, there is\nalso a more subtle scenario in which the test data is drawn from the same\ndomain. In addition to formalizing this problem of domain-adjacency, we present\na comparison of various baselines that could be used to solve it. We also\npropose a new simple sentence representation that emphasizes words which are\nunexpected. This approach improves the performance of a downstream semantic\nparser run on in-domain and domain-adjacent instances.", "published": "2018-08-26 21:33:32", "link": "http://arxiv.org/abs/1808.08626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scientific Relation Extraction with Selectively Incorporated Concept\n  Embeddings", "abstract": "This paper describes our submission for the SemEval 2018 Task 7 shared task\non semantic relation extraction and classification in scientific papers. We\nextend the end-to-end relation extraction model of (Miwa and Bansal) with\nenhancements such as a character-level encoding attention mechanism on\nselecting pretrained concept candidate embeddings. Our official submission\nranked the second in relation classification task (Subtask 1.1 and Subtask 2\nSenerio 2), and the first in the relation extraction task (Subtask 2 Scenario\n1).", "published": "2018-08-26 23:31:02", "link": "http://arxiv.org/abs/1808.08643v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Contextual Parameter Generation for Universal Neural Machine Translation", "abstract": "We propose a simple modification to existing neural machine translation (NMT)\nmodels that enables using a single universal model to translate between\nmultiple languages while allowing for language specific parameterization, and\nthat can also be used for domain adaptation. Our approach requires no changes\nto the model architecture of a standard NMT system, but instead introduces a\nnew component, the contextual parameter generator (CPG), that generates the\nparameters of the system (e.g., weights in a neural network). This parameter\ngenerator accepts source and target language embeddings as input, and generates\nthe parameters for the encoder and the decoder, respectively. The rest of the\nmodel remains unchanged and is shared across all languages. We show how this\nsimple modification enables the system to use monolingual data for training and\nalso perform zero-shot translation. We further show it is able to surpass\nstate-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and\nthat the learned language embeddings are able to uncover interesting\nrelationships between languages.", "published": "2018-08-26 01:17:50", "link": "http://arxiv.org/abs/1808.08493v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Nowcasting the Stance of Social Media Users in a Sudden Vote: The Case\n  of the Greek Referendum", "abstract": "Modelling user voting intention in social media is an important research\narea, with applications in analysing electorate behaviour, online political\ncampaigning and advertising. Previous approaches mainly focus on predicting\nnational general elections, which are regularly scheduled and where data of\npast results and opinion polls are available. However, there is no evidence of\nhow such models would perform during a sudden vote under time-constrained\ncircumstances. That poses a more challenging task compared to traditional\nelections, due to its spontaneous nature. In this paper, we focus on the 2015\nGreek bailout referendum, aiming to nowcast on a daily basis the voting\nintention of 2,197 Twitter users. We propose a semi-supervised multiple\nconvolution kernel learning approach, leveraging temporally sensitive text and\nnetwork information. Our evaluation under a real-time simulation framework\ndemonstrates the effectiveness and robustness of our approach against\ncompetitive baselines, achieving a significant 20% increase in F-score compared\nto solely text-based models.", "published": "2018-08-26 11:58:04", "link": "http://arxiv.org/abs/1808.08538v1", "categories": ["cs.CY", "cs.CL", "cs.SI"], "primary_category": "cs.CY"}
{"title": "Adversarially Regularising Neural NLI Models to Integrate Logical\n  Background Knowledge", "abstract": "Adversarial examples are inputs to machine learning models designed to cause\nthe model to make a mistake. They are useful for understanding the shortcomings\nof machine learning models, interpreting their results, and for regularisation.\nIn NLP, however, most example generation strategies produce input text by using\nknown, pre-specified semantic transformations, requiring significant manual\neffort and in-depth understanding of the problem and domain. In this paper, we\ninvestigate the problem of automatically generating adversarial examples that\nviolate a set of given First-Order Logic constraints in Natural Language\nInference (NLI). We reduce the problem of identifying such adversarial examples\nto a combinatorial optimisation problem, by maximising a quantity measuring the\ndegree of violation of such constraints and by using a language model for\ngenerating linguistically-plausible examples. Furthermore, we propose a method\nfor adversarially regularising neural NLI models for incorporating background\nknowledge. Our results show that, while the proposed method does not always\nimprove results on the SNLI and MultiNLI datasets, it significantly and\nconsistently increases the predictive accuracy on adversarially-crafted\ndatasets -- up to a 79.6% relative improvement -- while drastically reducing\nthe number of background knowledge violations. Furthermore, we show that\nadversarial examples transfer among model architectures, and that the proposed\nadversarial training procedure improves the robustness of NLI models to\nadversarial examples.", "published": "2018-08-26 18:36:20", "link": "http://arxiv.org/abs/1808.08609v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "DreamNLP: Novel NLP System for Clinical Report Metadata Extraction using\n  Count Sketch Data Streaming Algorithm: Preliminary Results", "abstract": "Extracting information from electronic health records (EHR) is a challenging\ntask since it requires prior knowledge of the reports and some natural language\nprocessing algorithm (NLP). With the growing number of EHR implementations,\nsuch knowledge is increasingly challenging to obtain in an efficient manner. We\naddress this challenge by proposing a novel methodology to analyze large sets\nof EHRs using a modified Count Sketch data streaming algorithm termed DreamNLP.\nBy using DreamNLP, we generate a dictionary of frequently occurring terms or\nheavy hitters in the EHRs using low computational memory compared to\nconventional counting approach other NLP programs use. We demonstrate the\nextraction of the most important breast diagnosis features from the EHRs in a\nset of patients that underwent breast imaging. Based on the analysis,\nextraction of these terms would be useful for defining important features for\ndownstream tasks such as machine learning for precision medicine.", "published": "2018-08-26 01:42:29", "link": "http://arxiv.org/abs/1809.02665v1", "categories": ["cs.LG", "eess.AS", "stat.ML", "E.1; E.2; F.2.2; I.2.7"], "primary_category": "cs.LG"}
