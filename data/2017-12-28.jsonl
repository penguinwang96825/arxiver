{"title": "A Syntactic Approach to Domain-Specific Automatic Question Generation", "abstract": "Factoid questions are questions that require short fact-based answers.\nAutomatic generation (AQG) of factoid questions from a given text can\ncontribute to educational activities, interactive question answering systems,\nsearch engines, and other applications. The goal of our research is to generate\nfactoid source-question-answer triplets based on a specific domain. We propose\na four-component pipeline, which obtains as input a training corpus of\ndomain-specific documents, along with a set of declarative sentences from the\nsame domain, and generates as output a set of factoid questions that refer to\nthe source sentences but are slightly different from them, so that a\nquestion-answering system or a person can be asked a question that requires a\ndeeper understanding and knowledge than a simple word-matching. Contrary to\nexisting domain-specific AQG systems that utilize the template-based approach\nto question generation, we propose to transform each source sentence into a set\nof questions by applying a series of domain-independent rules (a\nsyntactic-based approach). Our pipeline was evaluated in the domain of cyber\nsecurity using a series of experiments on each component of the pipeline\nseparately and on the end-to-end system. The proposed approach generated a\nhigher percentage of acceptable questions than a prior state-of-the-art AQG\nsystem.", "published": "2017-12-28 11:15:30", "link": "http://arxiv.org/abs/1712.09827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Compositional Neural Language Model", "abstract": "We propose a Topic Compositional Neural Language Model (TCNLM), a novel\nmethod designed to simultaneously capture both the global semantic meaning and\nthe local word ordering structure in a document. The TCNLM learns the global\nsemantic coherence of a document via a neural topic model, and the probability\nof each learned latent topic is further used to build a Mixture-of-Experts\n(MoE) language model, where each expert (corresponding to one topic) is a\nrecurrent neural network (RNN) that accounts for learning the local structure\nof a word sequence. In order to train the MoE model efficiently, a matrix\nfactorization method is applied, by extending each weight matrix of the RNN to\nbe an ensemble of topic-dependent weight matrices. The degree to which each\nmember of the ensemble is used is tied to the document-dependent probability of\nthe corresponding topics. Experimental results on several corpora show that the\nproposed approach outperforms both a pure RNN-based model and other\ntopic-guided language models. Further, our model yields sensible topics, and\nalso has the capacity to generate meaningful sentences conditioned on given\ntopics.", "published": "2017-12-28 08:05:48", "link": "http://arxiv.org/abs/1712.09783v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Challenges of Detecting Rude Conversational Behaviour", "abstract": "In this study, we aim to identify moments of rudeness between two\nindividuals. In particular, we segment all occurrences of rudeness in\nconversations into three broad, distinct categories and try to identify each.\nWe show how machine learning algorithms can be used to identify rudeness based\non acoustic and semantic signals extracted from conversations. Furthermore, we\nmake note of our shortcomings in this task and highlight what makes this\nproblem inherently difficult. Finally, we provide next steps which are needed\nto ensure further success in identifying rudeness in conversations.", "published": "2017-12-28 16:49:40", "link": "http://arxiv.org/abs/1712.09929v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Corpus specificity in LSA and Word2vec: the role of out-of-domain\n  documents", "abstract": "Latent Semantic Analysis (LSA) and Word2vec are some of the most widely used\nword embeddings. Despite the popularity of these techniques, the precise\nmechanisms by which they acquire new semantic relations between words remain\nunclear. In the present article we investigate whether LSA and Word2vec\ncapacity to identify relevant semantic dimensions increases with size of\ncorpus. One intuitive hypothesis is that the capacity to identify relevant\ndimensions should increase as the amount of data increases. However, if corpus\nsize grow in topics which are not specific to the domain of interest, signal to\nnoise ratio may weaken. Here we set to examine and distinguish these\nalternative hypothesis. To investigate the effect of corpus specificity and\nsize in word-embeddings we study two ways for progressive elimination of\ndocuments: the elimination of random documents vs. the elimination of documents\nunrelated to a specific task. We show that Word2vec can take advantage of all\nthe documents, obtaining its best performance when it is trained with the whole\ncorpus. On the contrary, the specialization (removal of out-of-domain\ndocuments) of the training corpus, accompanied by a decrease of dimensionality,\ncan increase LSA word-representation quality while speeding up the processing\ntime. Furthermore, we show that the specialization without the decrease in LSA\ndimensionality can produce a strong performance reduction in specific tasks.\nFrom a cognitive-modeling point of view, we point out that LSA's word-knowledge\nacquisitions may not be efficiently exploiting higher-order co-occurrences and\nglobal relations, whereas Word2vec does.", "published": "2017-12-28 20:56:16", "link": "http://arxiv.org/abs/1712.10054v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Continual Learning for Conversational Agents", "abstract": "While end-to-end neural conversation models have led to promising advances in\nreducing hand-crafted features and errors induced by the traditional complex\nsystem architecture, they typically require an enormous amount of data due to\nthe lack of modularity. Previous studies adopted a hybrid approach with\nknowledge-based components either to abstract out domain-specific information\nor to augment data to cover more diverse patterns. On the contrary, we propose\nto directly address the problem using recent developments in the space of\ncontinual learning for neural models. Specifically, we adopt a\ndomain-independent neural conversational model and introduce a novel neural\ncontinual learning algorithm that allows a conversational agent to accumulate\nskills across different tasks in a data-efficient way. To the best of our\nknowledge, this is the first work that applies continual learning to\nconversation systems. We verified the efficacy of our method through a\nconversational skill transfer from either synthetic dialogs or human-human\ndialogs to human-computer conversations in a customer support domain.", "published": "2017-12-28 17:21:42", "link": "http://arxiv.org/abs/1712.09943v3", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
