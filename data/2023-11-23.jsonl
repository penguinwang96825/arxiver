{"title": "Transformer-based Named Entity Recognition in Construction Supply Chain\n  Risk Management in Australia", "abstract": "The construction industry in Australia is characterized by its intricate\nsupply chains and vulnerability to myriad risks. As such, effective supply\nchain risk management (SCRM) becomes imperative. This paper employs different\ntransformer models, and train for Named Entity Recognition (NER) in the context\nof Australian construction SCRM. Utilizing NER, transformer models identify and\nclassify specific risk-associated entities in news articles, offering a\ndetailed insight into supply chain vulnerabilities. By analysing news articles\nthrough different transformer models, we can extract relevant entities and\ninsights related to specific risk taxonomies local (milieu) to the Australian\nconstruction landscape. This research emphasises the potential of NLP-driven\nsolutions, like transformer models, in revolutionising SCRM for construction in\ngeo-media specific contexts.", "published": "2023-11-23 01:06:08", "link": "http://arxiv.org/abs/2311.13755v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DaG LLM ver 1.0: Pioneering Instruction-Tuned Language Modeling for\n  Korean NLP", "abstract": "This paper presents the DaG LLM (David and Goliath Large Language Model), a\nlanguage model specialized for Korean and fine-tuned through Instruction Tuning\nacross 41 tasks within 13 distinct categories.", "published": "2023-11-23 03:03:54", "link": "http://arxiv.org/abs/2311.13784v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Error Correction via Mixed-Grained Weighted Training", "abstract": "The task of Grammatical Error Correction (GEC) aims to automatically correct\ngrammatical errors in natural texts. Almost all previous works treat annotated\ntraining data equally, but inherent discrepancies in data are neglected. In\nthis paper, the inherent discrepancies are manifested in two aspects, namely,\naccuracy of data annotation and diversity of potential annotations. To this\nend, we propose MainGEC, which designs token-level and sentence-level training\nweights based on inherent discrepancies in accuracy and potential diversity of\ndata annotation, respectively, and then conducts mixed-grained weighted\ntraining to improve the training effect for GEC. Empirical evaluation shows\nthat whether in the Seq2Seq or Seq2Edit manner, MainGEC achieves consistent and\nsignificant performance improvements on two benchmark datasets, demonstrating\nthe effectiveness and superiority of the mixed-grained weighted training.\nFurther ablation experiments verify the effectiveness of designed weights of\nboth granularities in MainGEC.", "published": "2023-11-23 08:34:37", "link": "http://arxiv.org/abs/2311.13848v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialogue Quality and Emotion Annotations for Customer Support\n  Conversations", "abstract": "Task-oriented conversational datasets often lack topic variability and\nlinguistic diversity. However, with the advent of Large Language Models (LLMs)\npretrained on extensive, multilingual and diverse text data, these limitations\nseem overcome. Nevertheless, their generalisability to different languages and\ndomains in dialogue applications remains uncertain without benchmarking\ndatasets. This paper presents a holistic annotation approach for emotion and\nconversational quality in the context of bilingual customer support\nconversations. By performing annotations that take into consideration the\ncomplete instances that compose a conversation, one can form a broader\nperspective of the dialogue as a whole. Furthermore, it provides a unique and\nvaluable resource for the development of text classification models. To this\nend, we present benchmarks for Emotion Recognition and Dialogue Quality\nEstimation and show that further research is needed to leverage these models in\na production setting.", "published": "2023-11-23 10:56:14", "link": "http://arxiv.org/abs/2311.13910v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Methods for Cross-lingual Text Style Transfer: The Case of\n  Text Detoxification", "abstract": "Text detoxification is the task of transferring the style of text from toxic\nto neutral. While here are approaches yielding promising results in monolingual\nsetup, e.g., (Dale et al., 2021; Hallinan et al., 2022), cross-lingual transfer\nfor this task remains a challenging open problem (Moskovskiy et al., 2022). In\nthis work, we present a large-scale study of strategies for cross-lingual text\ndetoxification -- given a parallel detoxification corpus for one language; the\ngoal is to transfer detoxification ability to another language for which we do\nnot have such a corpus. Moreover, we are the first to explore a new task where\ntext translation and detoxification are performed simultaneously, providing\nseveral strong baselines for this task. Finally, we introduce new automatic\ndetoxification evaluation metrics with higher correlations with human judgments\nthan previous benchmarks. We assess the most promising approaches also with\nmanual markup, determining the answer for the best strategy to transfer the\nknowledge of text detoxification between languages.", "published": "2023-11-23 11:40:28", "link": "http://arxiv.org/abs/2311.13937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria", "abstract": "Multimodal large language models (MLLMs) have broadened the scope of AI\napplications. Existing automatic evaluation methodologies for MLLMs are mainly\nlimited in evaluating queries without considering user experiences,\ninadequately addressing the nuances of creative and associative multimodal\ntasks. However, the open-ended and subjective nature of such tasks poses a\nsignificant challenge to the evaluation methodology, where it is difficult to\ndefine the ground-truth answers for them. To this end, in our paper, we propose\na new evaluation paradigm for MLLMs, which is evaluating MLLMs with per-sample\ncriteria using potent MLLM as the judge. To validate the feasibility and\neffectiveness of this paradigm, we design a benchmark, dubbed MLLM-Bench, by\ncurating the evaluation samples across six comprehensive cognitive levels. We\nbenchmark 21 popular MLLMs in a pairwise-comparison fashion, showing diverse\nperformance across models. Moreover, the validity of our benchmark manifests\nitself in reaching 88.02% agreement with human evaluation. We contend that the\nproposed paradigm explores the potential of MLLMs as effective evaluation tools\nwith the help of per-sample criteria. See online leaderboard at\n\\url{https://mllm-bench.llmzoo.com}.", "published": "2023-11-23 12:04:25", "link": "http://arxiv.org/abs/2311.13951v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study\n  Based on Lexical Diversity and Divergence", "abstract": "As a recent development, task-oriented dialogues (TODs) have been enriched\nwith chitchat in an effort to make dialogues more diverse and engaging. This\nenhancement is particularly valuable as TODs are often confined to narrow\ndomains, making the mitigation of repetitive and predictable responses a\nsignificant challenge. This paper presents a comparative analysis of three\nchitchat enhancements, aiming to identify the most effective approach in terms\nof diversity. Additionally, we quantify the divergence between the added\nchitchat, the original task-oriented language, and chitchat typically found in\nchitchat datasets, highlighting the top 20 divergent keywords for each\ncomparison. Our findings drive a discussion on future enhancements for\naugmenting TODs, emphasizing the importance of grounding dialogues beyond the\ntask to achieve more diverse and natural exchanges.", "published": "2023-11-23 15:50:42", "link": "http://arxiv.org/abs/2311.14067v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Snippets of Open-Domain Dialogue in Task-Oriented Dialogue\n  Datasets", "abstract": "Most existing dialogue corpora and models have been designed to fit into 2\npredominant categories : task-oriented dialogues portray functional goals, such\nas making a restaurant reservation or booking a plane ticket, while\nchit-chat/open-domain dialogues focus on holding a socially engaging talk with\na user. However, humans tend to seamlessly switch between modes and even use\nchitchat to enhance task-oriented conversations. To bridge this gap, new\ndatasets have recently been created, blending both communication modes into\nconversation examples. The approaches used tend to rely on adding chit-chat\nsnippets to pre-existing, human-generated task-oriented datasets. Given the\ntendencies observed in humans, we wonder however if the latter do not\n\\textit{already} hold chit-chat sequences. By using topic modeling and\nsearching for topics which are most similar to a set of keywords related to\nsocial talk, we explore the training sets of Schema-Guided Dialogues and\nMultiWOZ. Our study shows that sequences related to social talk are indeed\nnaturally present, motivating further research on ways chitchat is combined\ninto task-oriented dialogues.", "published": "2023-11-23 16:08:39", "link": "http://arxiv.org/abs/2311.14076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering in Natural Language: the Special Case of Temporal\n  Expressions", "abstract": "Although general question answering has been well explored in recent years,\ntemporal question answering is a task which has not received as much focus. Our\nwork aims to leverage a popular approach used for general question answering,\nanswer extraction, in order to find answers to temporal questions within a\nparagraph. To train our model, we propose a new dataset, inspired by SQuAD,\nspecifically tailored to provide rich temporal information. We chose to adapt\nthe corpus WikiWars, which contains several documents on history's greatest\nconflicts. Our evaluation shows that a deep learning model trained to perform\npattern matching, often used in general question answering, can be adapted to\ntemporal question answering, if we accept to ask questions whose answers must\nbe directly present within a text.", "published": "2023-11-23 16:26:24", "link": "http://arxiv.org/abs/2311.14087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimizing Factual Inconsistency and Hallucination in Large Language\n  Models", "abstract": "Large Language Models (LLMs) are widely used in critical fields such as\nhealthcare, education, and finance due to their remarkable proficiency in\nvarious language-related tasks. However, LLMs are prone to generating factually\nincorrect responses or \"hallucinations,\" which can lead to a loss of\ncredibility and trust among users. To address this issue, we propose a\nmulti-stage framework that generates the rationale first, verifies and refines\nincorrect ones, and uses them as supporting references to generate the answer.\nThe generated rationale enhances the transparency of the answer and our\nframework provides insights into how the model arrived at this answer, by using\nthis rationale and the references to the context. In this paper, we demonstrate\nits effectiveness in improving the quality of responses to drug-related\ninquiries in the life sciences industry. Our framework improves traditional\nRetrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be\n14-25% more faithful and 16-22% more accurate on two datasets. Furthermore,\nfine-tuning samples based on our framework improves the accuracy of smaller\nopen-access LLMs by 33-42% and competes with RAG on commercial models.", "published": "2023-11-23 09:58:39", "link": "http://arxiv.org/abs/2311.13878v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "General Phrase Debiaser: Debiasing Masked Language Models at a\n  Multi-Token Level", "abstract": "The social biases and unwelcome stereotypes revealed by pretrained language\nmodels are becoming obstacles to their application. Compared to numerous\ndebiasing methods targeting word level, there has been relatively less\nattention on biases present at phrase level, limiting the performance of\ndebiasing in discipline domains. In this paper, we propose an automatic\nmulti-token debiasing pipeline called \\textbf{General Phrase Debiaser}, which\nis capable of mitigating phrase-level biases in masked language models.\nSpecifically, our method consists of a \\textit{phrase filter stage} that\ngenerates stereotypical phrases from Wikipedia pages as well as a \\textit{model\ndebias stage} that can debias models at the multi-token level to tackle bias\nchallenges on phrases. The latter searches for prompts that trigger model's\nbias, and then uses them for debiasing. State-of-the-art results on standard\ndatasets and metrics show that our approach can significantly reduce gender\nbiases on both career and multiple disciplines, across models with varying\nparameter sizes.", "published": "2023-11-23 10:23:51", "link": "http://arxiv.org/abs/2311.13892v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Some Like It Small: Czech Semantic Embedding Models for Industry\n  Applications", "abstract": "This article focuses on the development and evaluation of Small-sized Czech\nsentence embedding models. Small models are important components for real-time\nindustry applications in resource-constrained environments. Given the limited\navailability of labeled Czech data, alternative approaches, including\npre-training, knowledge distillation, and unsupervised contrastive fine-tuning,\nare investigated. Comprehensive intrinsic and extrinsic analyses are conducted,\nshowcasing the competitive performance of our models compared to significantly\nlarger counterparts, with approximately 8 times smaller size and 5 times faster\nspeed than conventional Base-sized models. To promote cooperation and\nreproducibility, both the models and the evaluation pipeline are made publicly\naccessible. Ultimately, this article presents practical applications of the\ndeveloped sentence embedding models in Seznam.cz, the Czech search engine.\nThese models have effectively replaced previous counterparts, enhancing the\noverall search experience for instance, in organic search, featured snippets,\nand image search. This transition has yielded improved performance.", "published": "2023-11-23 11:14:13", "link": "http://arxiv.org/abs/2311.13921v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Efficient Trigger Word Insertion", "abstract": "With the boom in the natural language processing (NLP) field these years,\nbackdoor attacks pose immense threats against deep neural network models.\nHowever, previous works hardly consider the effect of the poisoning rate. In\nthis paper, our main objective is to reduce the number of poisoned samples\nwhile still achieving a satisfactory Attack Success Rate (ASR) in text backdoor\nattacks. To accomplish this, we propose an efficient trigger word insertion\nstrategy in terms of trigger word optimization and poisoned sample selection.\nExtensive experiments on different datasets and models demonstrate that our\nproposed method can significantly improve attack effectiveness in text\nclassification tasks. Remarkably, our approach achieves an ASR of over 90% with\nonly 10 poisoned samples in the dirty-label setting and requires merely 1.5% of\nthe training data in the clean-label setting.", "published": "2023-11-23 12:15:56", "link": "http://arxiv.org/abs/2311.13957v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Probabilistic Tree-of-thought Reasoning for Answering\n  Knowledge-intensive Complex Questions", "abstract": "Large language models (LLMs) are capable of answering knowledge-intensive\ncomplex questions with chain-of-thought (CoT) reasoning. However, they tend to\ngenerate factually incorrect reasoning steps when the required knowledge is not\navailable or up-to-date in models' parameters. Recent works turn to retrieving\nexternal knowledge to augment CoT reasoning. Despite being promising, these\nchain-based methods suffer from: 1) Negative retrieval. Unnecessary or\nincorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the\nability to look backward or forward, a local error in one step will propagate\nalong the chain.\n  In this paper, we propose a novel approach: Probabilistic Tree-of-thought\nReasoning (ProbTree). First, LLMs translate a complex question into a query\ntree, in which each non-root node denotes a sub-question of its parent node.\nThen, probabilistic reasoning is conducted over the tree, by solving questions\nfrom leaf to root considering the confidence of both question decomposing and\nanswering. During reasoning, for leaf nodes, LLMs choose a more confident\nanswer from Closed-book QA that employs parametric knowledge and Open-book QA\nthat employs retrieved external knowledge, thus eliminating the negative\nretrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs\nhave broader sights and are able to globally reason with the information from\nchild nodes, thus recovering from local errors. The experiments on three\nComplex QA datasets under the open-domain setting show that our approach\noutperforms SOTA methods significantly, demonstrating the effect of\nprobabilistic tree-of-thought reasoning.", "published": "2023-11-23 12:52:37", "link": "http://arxiv.org/abs/2311.13982v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cultural Bias and Cultural Alignment of Large Language Models", "abstract": "Culture fundamentally shapes people's reasoning, behavior, and communication.\nAs people increasingly use generative artificial intelligence (AI) to expedite\nand automate personal and professional tasks, cultural values embedded in AI\nmodels may bias people's authentic expression and contribute to the dominance\nof certain cultures. We conduct a disaggregated evaluation of cultural bias for\nfive widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3)\nby comparing the models' responses to nationally representative survey data.\nAll models exhibit cultural values resembling English-speaking and Protestant\nEuropean countries. We test cultural prompting as a control strategy to\nincrease cultural alignment for each country/territory. For recent models\n(GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models'\noutput for 71-81% of countries and territories. We suggest using cultural\nprompting and ongoing evaluation to reduce cultural bias in the output of\ngenerative AI.", "published": "2023-11-23 16:45:56", "link": "http://arxiv.org/abs/2311.14096v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Systematic Review of Deep Learning-based Research on Radiology Report\n  Generation", "abstract": "Radiology report generation (RRG) aims to automatically generate free-text\ndescriptions from clinical radiographs, e.g., chest X-Ray images. RRG plays an\nessential role in promoting clinical automation and presents significant help\nto provide practical assistance for inexperienced doctors and alleviate\nradiologists' workloads. Therefore, consider these meaningful potentials,\nresearch on RRG is experiencing explosive growth in the past half-decade,\nespecially with the rapid development of deep learning approaches. Existing\nstudies perform RRG from the perspective of enhancing different modalities,\nprovide insights on optimizing the report generation process with elaborated\nfeatures from both visual and textual information, and further facilitate RRG\nwith the cross-modal interactions among them. In this paper, we present a\ncomprehensive review of deep learning-based RRG from various perspectives.\nSpecifically, we firstly cover pivotal RRG approaches based on the\ntask-specific features of radiographs, reports, and the cross-modal relations\nbetween them, and then illustrate the benchmark datasets conventionally used\nfor this task with evaluation metrics, subsequently analyze the performance of\ndifferent approaches and finally offer our summary on the challenges and the\ntrends in future directions. Overall, the goal of this paper is to serve as a\ntool for understanding existing literature and inspiring potential valuable\nresearch in the field of RRG.", "published": "2023-11-23 20:52:44", "link": "http://arxiv.org/abs/2311.14199v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AdaTyper: Adaptive Semantic Column Type Detection", "abstract": "Understanding the semantics of relational tables is instrumental for\nautomation in data exploration and preparation systems. A key source for\nunderstanding a table is the semantics of its columns. With the rise of deep\nlearning, learned table representations are now available, which can be applied\nfor semantic type detection and achieve good performance on benchmarks.\nNevertheless, we observe a gap between this performance and its applicability\nin practice. In this paper, we propose AdaTyper to address one of the most\ncritical deployment challenges: adaptation. AdaTyper uses weak-supervision to\nadapt a hybrid type predictor towards new semantic types and shifted data\ndistributions at inference time, using minimal human feedback. The hybrid type\npredictor of AdaTyper combines rule-based methods and a light machine learning\nmodel for semantic column type detection. We evaluate the adaptation\nperformance of AdaTyper on real-world database tables hand-annotated with\nsemantic column types through crowdsourcing and find that the f1-score improves\nfor new and existing types. AdaTyper approaches an average precision of 0.6\nafter only seeing 5 examples, significantly outperforming existing adaptation\nmethods based on human-provided regular expressions or dictionaries.", "published": "2023-11-23 04:42:27", "link": "http://arxiv.org/abs/2311.13806v1", "categories": ["cs.DB", "cs.CL", "cs.LG"], "primary_category": "cs.DB"}
{"title": "Lego: Learning to Disentangle and Invert Personalized Concepts Beyond\n  Object Appearance in Text-to-Image Diffusion Models", "abstract": "Text-to-Image (T2I) models excel at synthesizing concepts such as nouns,\nappearances, and styles. To enable customized content creation based on a few\nexample images of a concept, methods such as Textual Inversion and DreamBooth\ninvert the desired concept and enable synthesizing it in new scenes. However,\ninverting personalized concepts that go beyond object appearance and style\n(adjectives and verbs) through natural language remains a challenge. Two key\ncharacteristics of these concepts contribute to the limitations of current\ninversion methods. 1) Adjectives and verbs are entangled with nouns (subject)\nand can hinder appearance-based inversion methods, where the subject appearance\nleaks into the concept embedding, and 2) describing such concepts often extends\nbeyond single word embeddings.\n  In this study, we introduce Lego, a textual inversion method designed to\ninvert subject-entangled concepts from a few example images. Lego disentangles\nconcepts from their associated subjects using a simple yet effective Subject\nSeparation step and employs a Context Loss that guides the inversion of\nsingle/multi-embedding concepts. In a thorough user study, Lego-generated\nconcepts were preferred over 70% of the time when compared to the baseline in\nterms of authentically generating concepts according to a reference.\nAdditionally, visual question answering using an LLM suggested Lego-generated\nconcepts are better aligned with the text description of the concept.", "published": "2023-11-23 07:33:38", "link": "http://arxiv.org/abs/2311.13833v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Challenges of Large Language Models for Mental Health Counseling", "abstract": "The global mental health crisis is looming with a rapid increase in mental\ndisorders, limited resources, and the social stigma of seeking treatment. As\nthe field of artificial intelligence (AI) has witnessed significant\nadvancements in recent years, large language models (LLMs) capable of\nunderstanding and generating human-like text may be used in supporting or\nproviding psychological counseling. However, the application of LLMs in the\nmental health domain raises concerns regarding the accuracy, effectiveness, and\nreliability of the information provided. This paper investigates the major\nchallenges associated with the development of LLMs for psychological\ncounseling, including model hallucination, interpretability, bias, privacy, and\nclinical effectiveness. We explore potential solutions to these challenges that\nare practical and applicable to the current paradigm of AI. From our experience\nin developing and deploying LLMs for mental health, AI holds a great promise\nfor improving mental health care, if we can carefully navigate and overcome\npitfalls of LLMs.", "published": "2023-11-23 08:56:41", "link": "http://arxiv.org/abs/2311.13857v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Jam-ALT: A Formatting-Aware Lyrics Transcription Benchmark", "abstract": "Current automatic lyrics transcription (ALT) benchmarks focus exclusively on\nword content and ignore the finer nuances of written lyrics including\nformatting and punctuation, which leads to a potential misalignment with the\ncreative products of musicians and songwriters as well as listeners'\nexperiences. For example, line breaks are important in conveying information\nabout rhythm, emotional emphasis, rhyme, and high-level structure. To address\nthis issue, we introduce Jam-ALT, a new lyrics transcription benchmark based on\nthe JamendoLyrics dataset. Our contribution is twofold. Firstly, a complete\nrevision of the transcripts, geared specifically towards ALT evaluation by\nfollowing a newly created annotation guide that unifies the music industry's\nguidelines, covering aspects such as punctuation, line breaks, spelling,\nbackground vocals, and non-word sounds. Secondly, a suite of evaluation metrics\ndesigned, unlike the traditional word error rate, to capture such phenomena. We\nhope that the proposed benchmark contributes to the ALT task, enabling more\nprecise and reliable assessments of transcription systems and enhancing the\nuser experience in lyrics applications such as subtitle renderings for live\ncaptioning or karaoke.", "published": "2023-11-23 13:13:48", "link": "http://arxiv.org/abs/2311.13987v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Do VSR Models Generalize Beyond LRS3?", "abstract": "The Lip Reading Sentences-3 (LRS3) benchmark has primarily been the focus of\nintense research in visual speech recognition (VSR) during the last few years.\nAs a result, there is an increased risk of overfitting to its excessively used\ntest set, which is only one hour duration. To alleviate this issue, we build a\nnew VSR test set named WildVSR, by closely following the LRS3 dataset creation\nprocesses. We then evaluate and analyse the extent to which the current VSR\nmodels generalize to the new test data. We evaluate a broad range of publicly\navailable VSR models and find significant drops in performance on our test set,\ncompared to their corresponding LRS3 results. Our results suggest that the\nincrease in word error rates is caused by the models inability to generalize to\nslightly harder and in the wild lip sequences than those found in the LRS3 test\nset. Our new test benchmark is made public in order to enable future research\ntowards more robust VSR models.", "published": "2023-11-23 15:42:00", "link": "http://arxiv.org/abs/2311.14063v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A density estimation perspective on learning from pairwise human\n  preferences", "abstract": "Learning from human feedback (LHF) -- and in particular learning from\npairwise preferences -- has recently become a crucial ingredient in training\nlarge language models (LLMs), and has been the subject of much research. Most\nrecent works frame it as a reinforcement learning problem, where a reward\nfunction is learned from pairwise preference data and the LLM is treated as a\npolicy which is adapted to maximize the rewards, often under additional\nregularization constraints. We propose an alternative interpretation which\ncenters on the generative process for pairwise preferences and treats LHF as a\ndensity estimation problem. We provide theoretical and empirical results\nshowing that for a family of generative processes defined via preference\nbehavior distribution equations, training a reward function on pairwise\npreferences effectively models an annotator's implicit preference distribution.\nFinally, we discuss and present findings on \"annotator misspecification\" --\nfailure cases where wrong modeling assumptions are made about annotator\nbehavior, resulting in poorly-adapted models -- suggesting that approaches that\nlearn from pairwise human preferences could have trouble learning from a\npopulation of annotators with diverse viewpoints.", "published": "2023-11-23 17:20:36", "link": "http://arxiv.org/abs/2311.14115v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Auditing Large Language Models: Improving Text-based Stereotype\n  Detection", "abstract": "Large Language Models (LLM) have made significant advances in the recent past\nbecoming more mainstream in Artificial Intelligence (AI) enabled human-facing\napplications. However, LLMs often generate stereotypical output inherited from\nhistorical data, amplifying societal biases and raising ethical concerns. This\nwork introduces i) the Multi-Grain Stereotype Dataset, which includes 52,751\ninstances of gender, race, profession and religion stereotypic text and ii) a\nnovel stereotype classifier for English text. We design several experiments to\nrigorously test the proposed model trained on the novel dataset. Our\nexperiments show that training the model in a multi-class setting can\noutperform the one-vs-all binary counterpart. Consistent feature importance\nsignals from different eXplainable AI tools demonstrate that the new model\nexploits relevant text features. We utilise the newly created model to assess\nthe stereotypic behaviour of the popular GPT family of models and observe the\nreduction of bias over time. In summary, our work establishes a robust and\npractical framework for auditing and evaluating the stereotypic bias in LLM.", "published": "2023-11-23 17:47:14", "link": "http://arxiv.org/abs/2311.14126v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating GPT-4's Vision Capabilities on Brazilian University Admission\n  Exams", "abstract": "Recent advancements in language models have showcased human-comparable\nperformance in academic entrance exams. However, existing studies often\noverlook questions that require the integration of visual comprehension, thus\ncompromising the full spectrum and complexity inherent in real-world scenarios.\nTo address this gap, we present a comprehensive framework to evaluate language\nmodels on entrance exams, which incorporates both textual and visual elements.\nWe evaluate the two most recent editions of Exame Nacional do Ensino M\\'edio\n(ENEM), the main standardized entrance examination adopted by Brazilian\nuniversities. Our study not only reaffirms the capabilities of GPT-4 as the\nstate of the art for handling complex multidisciplinary questions, but also\npioneers in offering a realistic assessment of multimodal language models on\nPortuguese examinations. One of the highlights is that text captions\ntranscribing visual content outperform the direct use of images, suggesting\nthat the vision model has room for improvement. Yet, despite improvements\nafforded by images or captions, mathematical questions remain a challenge for\nthese state-of-the-art models. The code and data used on experiments are\navailable at https://github.com/piresramon/gpt-4-enem.", "published": "2023-11-23 19:20:59", "link": "http://arxiv.org/abs/2311.14169v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Annotation Sensitivity: Training Data Collection Methods Affect Model\n  Performance", "abstract": "When training data are collected from human annotators, the design of the\nannotation instrument, the instructions given to annotators, the\ncharacteristics of the annotators, and their interactions can impact training\ndata. This study demonstrates that design choices made when creating an\nannotation instrument also impact the models trained on the resulting\nannotations. We introduce the term annotation sensitivity to refer to the\nimpact of annotation data collection methods on the annotations themselves and\non downstream model performance and predictions. We collect annotations of hate\nspeech and offensive language in five experimental conditions of an annotation\ninstrument, randomly assigning annotators to conditions. We then fine-tune BERT\nmodels on each of the five resulting datasets and evaluate model performance on\na holdout portion of each condition. We find considerable differences between\nthe conditions for 1) the share of hate speech/offensive language annotations,\n2) model performance, 3) model predictions, and 4) model learning curves. Our\nresults emphasize the crucial role played by the annotation instrument which\nhas received little attention in the machine learning literature. We call for\nadditional research into how and why the instrument impacts the annotations to\ninform the development of best practices in instrument design.", "published": "2023-11-23 21:54:22", "link": "http://arxiv.org/abs/2311.14212v3", "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Federated Learning for Short Text Clustering", "abstract": "Short text clustering has been popularly studied for its significance in\nmining valuable insights from many short texts. In this paper, we focus on the\nfederated short text clustering (FSTC) problem, i.e., clustering short texts\nthat are distributed in different clients, which is a realistic problem under\nprivacy requirements. Compared with the centralized short text clustering\nproblem that short texts are stored on a central server, the FSTC problem has\nnot been explored yet. To fill this gap, we propose a Federated Robust Short\nText Clustering (FSTC) framework. FSTC includes two main modules, i.e., robust\nshort text clustering module and federated cluster center aggregation module.\nThe robust short text clustering module aims to train an effective short text\nclustering model with local data in each client. We innovatively combine\noptimal transport to generate pseudo-labels with Gaussian-uniform mixture model\nto ensure the reliability of the pseudo-supervised data. The federated cluster\ncenter aggregation module aims to exchange knowledge across clients without\nsharing local raw data in an efficient way. The server aggregates the local\ncluster centers from different clients and then sends the global centers back\nto all clients in each communication round. Our empirical studies on three\nshort text clustering datasets demonstrate that FSTC significantly outperforms\nthe federated short text clustering baselines.", "published": "2023-11-23 12:19:41", "link": "http://arxiv.org/abs/2312.07556v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Interactive Dual-Conformer with Scene-Inspired Mask for Soft Sound Event\n  Detection", "abstract": "Traditional binary hard labels for sound event detection (SED) lack details\nabout the complexity and variability of sound event distributions. Recently, a\nnovel annotation workflow is proposed to generate fine-grained non-binary soft\nlabels, resulting in a new real-life dataset named MAESTRO Real for SED. In\nthis paper, we first propose an interactive dual-conformer (IDC) module, in\nwhich a cross-interaction mechanism is applied to effectively exploit the\ninformation from soft labels. In addition, a novel scene-inspired mask (SIM)\nbased on soft labels is incorporated for more precise SED predictions. The SIM\nis initially generated through a statistical approach, referred as SIM-V1.\nHowever, the fixed artificial mask may mismatch the SED model, resulting in\nlimited effectiveness. Therefore, we further propose SIM-V2, which employs a\nword embedding model for adaptive SIM estimation. Experimental results show\nthat the proposed IDC module can effectively utilize the information from soft\nlabels, and the integration of SIM-V1 can further improve the accuracy. In\naddition, the impact of different word embedding dimensions on SIM-V2 is\nexplored, and the results show that the appropriate dimension can enable SIM-V2\nachieve superior performance than SIM-V1. In DCASE 2023 Challenge Task4B, the\nproposed system achieved the top ranking performance on the evaluation dataset\nof MAESTRO Real.", "published": "2023-11-23 15:51:53", "link": "http://arxiv.org/abs/2311.14068v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Learning to Solve Inverse Problems for Perceptual Sound Matching", "abstract": "Perceptual sound matching (PSM) aims to find the input parameters to a\nsynthesizer so as to best imitate an audio target. Deep learning for PSM\noptimizes a neural network to analyze and reconstruct prerecorded samples. In\nthis context, our article addresses the problem of designing a suitable loss\nfunction when the training set is generated by a differentiable synthesizer.\nOur main contribution is perceptual-neural-physical loss (PNP), which aims at\naddressing a tradeoff between perceptual relevance and computational\nefficiency. The key idea behind PNP is to linearize the effect of synthesis\nparameters upon auditory features in the vicinity of each training sample. The\nlinearization procedure is massively paralellizable, can be precomputed, and\noffers a 100-fold speedup during gradient descent compared to differentiable\ndigital signal processing (DDSP). We demonstrate PNP on two datasets of\nnonstationary sounds: an AM/FM arpeggiator and a physical model of rectangular\nmembranes. We show that PNP is able to accelerate DDSP with joint\ntime-frequency scattering transform (JTFS) as auditory feature, while\npreserving its perceptual fidelity. Additionally, we evaluate the impact of\nother design choices in PSM: parameter rescaling, pretraining, auditory\nrepresentation, and gradient clipping. We report state-of-the-art results on\nboth datasets and find that PNP-accelerated JTFS has greater influence on PSM\nperformance than any other design choice.", "published": "2023-11-23 22:00:58", "link": "http://arxiv.org/abs/2311.14213v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustic Cybersecurity: Exploiting Voice-Activated Systems", "abstract": "In this study, we investigate the emerging threat of inaudible acoustic\nattacks targeting digital voice assistants, a critical concern given their\nprojected prevalence to exceed the global population by 2024. Our research\nextends the feasibility of these attacks across various platforms like Amazon's\nAlexa, Android, iOS, and Cortana, revealing significant vulnerabilities in\nsmart devices. The twelve attack vectors identified include successful\nmanipulation of smart home devices and automotive systems, potential breaches\nin military communication, and challenges in critical infrastructure security.\nWe quantitatively show that attack success rates hover around 60%, with the\nability to activate devices remotely from over 100 feet away. Additionally,\nthese attacks threaten critical infrastructure, emphasizing the need for\nmultifaceted defensive strategies combining acoustic shielding, advanced signal\nprocessing, machine learning, and robust user authentication to mitigate these\nrisks.", "published": "2023-11-23 02:26:11", "link": "http://arxiv.org/abs/2312.00039v1", "categories": ["cs.CR", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
