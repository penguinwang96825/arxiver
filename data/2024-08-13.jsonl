{"title": "Introducing the NewsPaLM MBR and QE Dataset: LLM-Generated High-Quality\n  Parallel Data Outperforms Traditional Web-Crawled Data", "abstract": "Recent research in neural machine translation (NMT) has shown that training\non high-quality machine-generated data can outperform training on\nhuman-generated data. This work accompanies the first-ever release of a\nLLM-generated, MBR-decoded and QE-reranked dataset with both sentence-level and\nmulti-sentence examples. We perform extensive experiments to demonstrate the\nquality of our dataset in terms of its downstream impact on NMT model\nperformance. We find that training from scratch on our (machine-generated)\ndataset outperforms training on the (web-crawled) WMT'23 training dataset\n(which is 300 times larger), and also outperforms training on the top-quality\nsubset of the WMT'23 training dataset. We also find that performing\nself-distillation by finetuning the LLM which generated this dataset\noutperforms the LLM's strong few-shot baseline. These findings corroborate the\nquality of our dataset, and demonstrate the value of high-quality\nmachine-generated data in improving performance of NMT models.", "published": "2024-08-13 00:06:56", "link": "http://arxiv.org/abs/2408.06537v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark\n  Large Language Model", "abstract": "Large language models (LLMs) have shown remarkable achievements across\nvarious language tasks.To enhance the performance of LLMs in scientific\nliterature services, we developed the scientific literature LLM (SciLit-LLM)\nthrough pre-training and supervised fine-tuning on scientific literature,\nbuilding upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge\nservice system Spark Research Assistant (SparkRA) based on our SciLit-LLM.\nSparkRA is accessible online and provides three primary functions: literature\ninvestigation, paper reading, and academic writing. As of July 30, 2024,\nSparkRA has garnered over 50,000 registered users, with a total usage count\nexceeding 1.3 million.", "published": "2024-08-13 02:18:47", "link": "http://arxiv.org/abs/2408.06574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CTISum: A New Benchmark Dataset For Cyber Threat Intelligence\n  Summarization", "abstract": "Cyber Threat Intelligence (CTI) summarization task requires the system to\ngenerate concise and accurate highlights from raw intelligence data, which\nplays an important role in providing decision-makers with crucial information\nto quickly detect and respond to cyber threats in the cybersecurity domain.\nHowever, efficient techniques for summarizing CTI reports, including facts,\nanalytical insights, attack processes, etc., have largely been unexplored,\nprimarily due to the lack of available dataset. To this end, we present CTISum,\na new benchmark for CTI summarization task. Considering the importance of\nattack process, a novel fine-grained subtask of attack process summarization is\nproposed to enable defenders to assess risk, identify security gaps,\nvulnerabilities, and so on. Specifically, we first design a multi-stage\nannotation pipeline to gather and annotate the CTI data, and then benchmark the\nCTISum with a collection of extractive and abstractive summarization methods.\nExperimental results show that current state-of-the-art models exhibit\nlimitations when applied to CTISum, underscoring the fact that automatically\nproducing concise summaries of CTI reports remains an open research challenge.", "published": "2024-08-13 02:25:16", "link": "http://arxiv.org/abs/2408.06576v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OpenEP: Open-Ended Future Event Prediction", "abstract": "Future event prediction (FEP) is a long-standing and crucial task in the\nworld, as understanding the evolution of events enables early risk\nidentification, informed decision-making, and strategic planning. Existing work\ntypically treats event prediction as classification tasks and confines the\noutcomes of future events to a fixed scope, such as yes/no questions, candidate\nset, and taxonomy, which is difficult to include all possible outcomes of\nfuture events. In this paper, we introduce OpenEP (an Open-Ended Future Event\nPrediction task), which generates flexible and diverse predictions aligned with\nreal-world scenarios. This is mainly reflected in two aspects: firstly, the\npredictive questions are diverse, covering different stages of event\ndevelopment and perspectives; secondly, the outcomes are flexible, without\nconstraints on scope or format. To facilitate the study of this task, we\nconstruct OpenEPBench, an open-ended future event prediction dataset. For\nquestion construction, we pose questions from seven perspectives, including\nlocation, time, event development, event outcome, event impact, event response,\nand other, to facilitate an in-depth analysis and understanding of the\ncomprehensive evolution of events. For outcome construction, we collect\nfree-form text containing the outcomes as ground truth to provide semantically\ncomplete and detail-enriched outcomes. Furthermore, we propose StkFEP, a\nstakeholder-enhanced future event prediction framework, that incorporates event\ncharacteristics for open-ended settings. Our method extracts stakeholders\ninvolved in events to extend questions to gather diverse information. We also\ncollect historically events that are relevant and similar to the question to\nreveal potential evolutionary patterns. Experiment results indicate that\naccurately predicting future events in open-ended settings is challenging for\nexisting LLMs.", "published": "2024-08-13 02:35:54", "link": "http://arxiv.org/abs/2408.06578v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Structure-aware Generative Model for Biomedical Event Extraction", "abstract": "Biomedical Event Extraction (BEE) is a challenging task that involves\nmodeling complex relationships between fine-grained entities in biomedical\ntext. BEE has traditionally been formulated as a classification problem. With\nrecent advancements in large language models (LLMs), generation-based models\nthat cast event extraction as a sequence generation problem have attracted\nattention in the NLP research community. However, current generative models\noften overlook cross-instance information in complex event structures, such as\nnested and overlapping events, which constitute over 20% of events in benchmark\ndatasets. In this paper, we propose GenBEE, an event structure-aware generative\nmodel that captures complex event structures in biomedical text for biomedical\nevent extraction. GenBEE constructs event prompts that distill knowledge from\nLLMs to incorporate both label semantics and argument dependency relationships.\nIn addition, GenBEE generates prefixes with event structural prompts to\nincorporate structural features to improve the model's overall performance. We\nhave evaluated the proposed GenBEE model on three widely used BEE benchmark\ndatasets, namely MLEE, GE11, and PHEE. Experimental results show that GenBEE\nhas achieved state-of-the-art performance on the MLEE and GE11 datasets, and\nachieved competitive results when compared to the state-of-the-art\nclassification-based models on the PHEE dataset.", "published": "2024-08-13 02:43:19", "link": "http://arxiv.org/abs/2408.06583v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IFShip: Interpretable Fine-grained Ship Classification with Domain\n  Knowledge-Enhanced Vision-Language Models", "abstract": "End-to-end interpretation currently dominates the remote sensing fine-grained\nship classification (RS-FGSC) task. However, the inference process remains\nuninterpretable, leading to criticisms of these models as \"black box\" systems.\nTo address this issue, we propose a domain knowledge-enhanced Chain-of-Thought\n(CoT) prompt generation mechanism, which is used to semi-automatically\nconstruct a task-specific instruction-following dataset, TITANIC-FGS. By\ntraining on TITANIC-FGS, we adapt general-domain vision-language models (VLMs)\nto the FGSC task, resulting in a model named IFShip. Building upon IFShip, we\ndevelop an FGSC visual chatbot that redefines the FGSC problem as a\nstep-by-step reasoning task and conveys the reasoning process in natural\nlanguage. Experimental results show that IFShip outperforms state-of-the-art\nFGSC algorithms in both interpretability and classification accuracy.\nFurthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates\nsuperior performance on the FGSC task. It provides an accurate chain of\nreasoning when fine-grained ship types are recognizable to the human eye and\noffers interpretable explanations when they are not.", "published": "2024-08-13 04:36:18", "link": "http://arxiv.org/abs/2408.06631v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pragmatic inference of scalar implicature by LLMs", "abstract": "This study investigates how Large Language Models (LLMs), particularly BERT\n(Devlin et al., 2019) and GPT-2 (Radford et al., 2019), engage in pragmatic\ninference of scalar implicature, such as some. Two sets of experiments were\nconducted using cosine similarity and next sentence/token prediction as\nexperimental methods. The results in experiment 1 showed that, both models\ninterpret some as pragmatic implicature not all in the absence of context,\naligning with human language processing. In experiment 2, in which Question\nUnder Discussion (QUD) was presented as a contextual cue, BERT showed\nconsistent performance regardless of types of QUDs, while GPT-2 encountered\nprocessing difficulties since a certain type of QUD required pragmatic\ninference for implicature. The findings revealed that, in terms of theoretical\napproaches, BERT inherently incorporates pragmatic implicature not all within\nthe term some, adhering to Default model (Levinson, 2000). In contrast, GPT-2\nseems to encounter processing difficulties in inferring pragmatic implicature\nwithin context, consistent with Context-driven model (Sperber and Wilson,\n2002).", "published": "2024-08-13 06:52:29", "link": "http://arxiv.org/abs/2408.06673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latin Treebanks in Review: An Evaluation of Morphological Tagging Across\n  Time", "abstract": "Existing Latin treebanks draw from Latin's long written tradition, spanning\n17 centuries and a variety of cultures. Recent efforts have begun to harmonize\nthese treebanks' annotations to better train and evaluate morphological\ntaggers. However, the heterogeneity of these treebanks must be carefully\nconsidered to build effective and reliable data. In this work, we review\nexisting Latin treebanks to identify the texts they draw from, identify their\noverlap, and document their coverage across time and genre. We additionally\ndesign automated conversions of their morphological feature annotations into\nthe conventions of standard Latin grammar. From this, we build new time-period\ndata splits that draw from the existing treebanks which we use to perform a\nbroad cross-time analysis for POS and morphological feature tagging. We find\nthat BERT-based taggers outperform existing taggers while also being more\nrobust to cross-domain shifts.", "published": "2024-08-13 06:55:54", "link": "http://arxiv.org/abs/2408.06675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Models for Check-Worthy Social Media Posts Detection", "abstract": "This work presents an extensive study of transformer-based NLP models for\ndetection of social media posts that contain verifiable factual claims and\nharmful claims. The study covers various activities, including dataset\ncollection, dataset pre-processing, architecture selection, setup of settings,\nmodel training (fine-tuning), model testing, and implementation. The study\nincludes a comprehensive analysis of different models, with a special focus on\nmultilingual models where the same model is capable of processing social media\nposts in both English and in low-resource languages such as Arabic, Bulgarian,\nDutch, Polish, Czech, Slovak. The results obtained from the study were\nvalidated against state-of-the-art models, and the comparison demonstrated the\nrobustness of the proposed models. The novelty of this work lies in the\ndevelopment of multi-label multilingual classification models that can\nsimultaneously detect harmful posts and posts that contain verifiable factual\nclaims in an efficient way.", "published": "2024-08-13 08:55:28", "link": "http://arxiv.org/abs/2408.06737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors", "abstract": "We propose Fast-and-Frugal Text-Graph (FnF-TG) Transformers, a\nTransformer-based framework that unifies textual and structural information for\ninductive link prediction in text-attributed knowledge graphs. We demonstrate\nthat, by effectively encoding ego-graphs (1-hop neighbourhoods), we can reduce\nthe reliance on resource-intensive textual encoders. This makes the model both\nfast at training and inference time, as well as frugal in terms of cost. We\nperform a comprehensive evaluation on three popular datasets and show that\nFnF-TG can achieve superior performance compared to previous state-of-the-art\nmethods. We also extend inductive learning to a fully inductive setting, where\nrelations don't rely on transductive (fixed) representations, as in previous\nwork, but are a function of their textual description. Additionally, we\nintroduce new variants of existing datasets, specifically designed to test the\nperformance of models on unseen relations at inference time, thus offering a\nnew test-bench for fully inductive link prediction.", "published": "2024-08-13 10:04:29", "link": "http://arxiv.org/abs/2408.06778v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlock the Power of Frozen LLMs in Knowledge Graph Completion", "abstract": "Traditional knowledge graph completion (KGC) methods rely solely on\nstructural information, struggling with the inherent sparsity of knowledge\ngraphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large\ncorpora with powerful context modeling, making them promising for mitigating\nthe limitations of previous methods. Directly fine-tuning LLMs offers great\ncapability but comes at the cost of huge time and memory consumption, while\nutilizing frozen LLMs yields suboptimal results.In this work, we aim to\nleverage LLMs for KGC effectively and efficiently. We capture the context-aware\nhidden states of knowledge triples by employing prompts to stimulate the\nintermediate layers of LLMs. We then train a data-efficient classifier on these\nhidden states to harness the inherent capabilities of frozen LLMs in KGC.\nAdditionally, to reduce ambiguity and enrich knowledge representation, we\ngenerate detailed entity descriptions through subgraph sampling on KGs.\nExtensive experiments on standard benchmarks demonstrate the efficiency and\neffectiveness of our approach. We outperform traditional KGC methods across\nmost datasets and, notably, achieve classification performance comparable to\nfine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and\naccelerating training and inference by $13.48\\times$.", "published": "2024-08-13 10:15:55", "link": "http://arxiv.org/abs/2408.06787v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Layerwise Recurrent Router for Mixture-of-Experts", "abstract": "The scaling of large language models (LLMs) has revolutionized their\ncapabilities in various tasks, yet this growth must be matched with efficient\ncomputational strategies. The Mixture-of-Experts (MoE) architecture stands out\nfor its ability to scale model size without significantly increasing training\ncosts. Despite their advantages, current MoE models often display parameter\ninefficiency. For instance, a pre-trained MoE-based LLM with 52 billion\nparameters might perform comparably to a standard model with 6.7 billion\nparameters. Being a crucial part of MoE, current routers in different layers\nindependently assign tokens without leveraging historical routing information,\npotentially leading to suboptimal token-expert combinations and the parameter\ninefficiency problem. To alleviate this issue, we introduce the Layerwise\nRecurrent Router for Mixture-of-Experts (RMoE). RMoE leverages a Gated\nRecurrent Unit (GRU) to establish dependencies between routing decisions across\nconsecutive layers. Such layerwise recurrence can be efficiently parallelly\ncomputed for input tokens and introduces negotiable costs. Our extensive\nempirical evaluations demonstrate that RMoE-based language models consistently\noutperform a spectrum of baseline models. Furthermore, RMoE integrates a novel\ncomputation stage orthogonal to existing methods, allowing seamless\ncompatibility with other MoE architectures. Our analyses attribute RMoE's gains\nto its effective cross-layer information sharing, which also improves expert\nselection and diversity. Our code is at https://github.com/qiuzh20/RMoE .", "published": "2024-08-13 10:25:13", "link": "http://arxiv.org/abs/2408.06793v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large\n  Language Models", "abstract": "Fine-tuning large language models (LLMs) with high parameter efficiency for\ndownstream tasks has become a new paradigm. Low-Rank Adaptation (LoRA)\nsignificantly reduces the number of trainable parameters for fine-tuning.\nAlthough it has demonstrated commendable performance, updating parameters\nwithin a single scale may not be the optimal choice for complex downstream\ntasks.In this paper, we extend the LoRA to multiple scales, dubbed as LoRA$^2$.\nWe first combine orthogonal projection theory to train a set of LoRAs in two\nmutually orthogonal planes. Then, we improve the importance score algorithm,\nwhich reduce parameter sensitivity score calculations by approximately 98.5\\%.\nBy pruning singular values with lower importance scores, thereby enhancing\nadaptability to various downstream tasks. Extensive experiments are conducted\non two widely used pre-trained models to validate the effectiveness of\nLoRA$^2$. Results show that it significantly reduces the number of trainable\nparameters to just 0.72\\% compared to full fine-tuning, while still delivering\nhighly impressive performance. Even when the parameters are further reduced to\n0.17M, it still achieves comparable results to the baseline with 8 times more\nparameters. Our code is available here:\nhttps://anonymous.4open.science/r/LoRA-2-5B4C", "published": "2024-08-13 12:31:30", "link": "http://arxiv.org/abs/2408.06854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Language Models for Emotion and Behavior Analysis in\n  Education", "abstract": "The analysis of students' emotions and behaviors is crucial for enhancing\nlearning outcomes and personalizing educational experiences. Traditional\nmethods often rely on intrusive visual and physiological data collection,\nposing privacy concerns and scalability issues. This paper proposes a novel\nmethod leveraging large language models (LLMs) and prompt engineering to\nanalyze textual data from students. Our approach utilizes tailored prompts to\nguide LLMs in detecting emotional and engagement states, providing a\nnon-intrusive and scalable solution. We conducted experiments using Qwen,\nChatGPT, Claude2, and GPT-4, comparing our method against baseline models and\nchain-of-thought (CoT) prompting. Results demonstrate that our method\nsignificantly outperforms the baselines in both accuracy and contextual\nunderstanding. This study highlights the potential of LLMs combined with prompt\nengineering to offer practical and effective tools for educational emotion and\nbehavior analysis.", "published": "2024-08-13 13:11:53", "link": "http://arxiv.org/abs/2408.06874v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge\n  Perspectives", "abstract": "The Chain-of-Thought (CoT) paradigm has become a pivotal method for solving\ncomplex problems. However, its application to intricate, domain-specific tasks\nremains challenging, as large language models (LLMs) often struggle to\naccurately decompose these tasks and, even when decomposition is correct, fail\nto execute the subtasks effectively. This paper introduces the Re-TASK\nframework, a novel theoretical model that revisits LLM tasks from the\nperspectives of capability, skill, and knowledge, drawing on the principles of\nBloom's Taxonomy and Knowledge Space Theory. While CoT offers a workflow\nperspective on tasks, the Re-TASK framework introduces a Chain-of-Learning\nview, illustrating how tasks and their corresponding subtasks depend on various\ncapability items. Each capability item is further dissected into its\nconstituent aspects of knowledge and skills. Our framework reveals that many\nCoT failures in domain-specific tasks stem from insufficient knowledge or\ninadequate skill adaptation. In response, we combine CoT with the Re-TASK\nframework and implement a carefully designed Re-TASK prompting strategy to\nimprove task performance. Specifically, we identify core capability items\nlinked to tasks and subtasks, then strengthen these capabilities through\ntargeted knowledge injection and skill adaptation. We validate the Re-TASK\nframework on three datasets across the law, finance, and mathematics domains,\nachieving significant improvements over the baseline models. Notably, our\napproach yields a remarkable 44.42% improvement with the Yi-1.5-9B model and a\n33.08% improvement with the Llama3-Chinese-8b on the legal dataset. These\nexperimental results confirm the effectiveness of the Re-TASK framework,\ndemonstrating substantial enhancements in both the performance and\napplicability of LLMs.", "published": "2024-08-13 13:58:23", "link": "http://arxiv.org/abs/2408.06904v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Cultural Adaptability of a Large Language Model via\n  Simulation of Synthetic Personas", "abstract": "The success of Large Language Models (LLMs) in multicultural environments\nhinges on their ability to understand users' diverse cultural backgrounds. We\nmeasure this capability by having an LLM simulate human profiles representing\nvarious nationalities within the scope of a questionnaire-style psychological\nexperiment. Specifically, we employ GPT-3.5 to reproduce reactions to\npersuasive news articles of 7,286 participants from 15 countries; comparing the\nresults with a dataset of real participants sharing the same demographic\ntraits. Our analysis shows that specifying a person's country of residence\nimproves GPT-3.5's alignment with their responses. In contrast, using native\nlanguage prompting introduces shifts that significantly reduce overall\nalignment, with some languages particularly impairing performance. These\nfindings suggest that while direct nationality information enhances the model's\ncultural adaptability, native language cues do not reliably improve simulation\nfidelity and can detract from the model's effectiveness.", "published": "2024-08-13 14:32:43", "link": "http://arxiv.org/abs/2408.06929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELLA: Empowering LLMs for Interpretable, Accurate and Informative Legal\n  Advice", "abstract": "Despite remarkable performance in legal consultation exhibited by legal Large\nLanguage Models(LLMs) combined with legal article retrieval components, there\nare still cases when the advice given is incorrect or baseless. To alleviate\nthese problems, we propose {\\bf ELLA}, a tool for {\\bf E}mpowering {\\bf L}LMs\nfor interpretable, accurate, and informative {\\bf L}egal {\\bf A}dvice. ELLA\nvisually presents the correlation between legal articles and LLM's response by\ncalculating their similarities, providing users with an intuitive legal basis\nfor the responses. Besides, based on the users' queries, ELLA retrieves\nrelevant legal articles and displays them to users. Users can interactively\nselect legal articles for LLM to generate more accurate responses. ELLA also\nretrieves relevant legal cases for user reference. Our user study shows that\npresenting the legal basis for the response helps users understand better. The\naccuracy of LLM's responses also improves when users intervene in selecting\nlegal articles for LLM. Providing relevant legal cases also aids individuals in\nobtaining comprehensive information.", "published": "2024-08-13 18:12:00", "link": "http://arxiv.org/abs/2408.07137v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models as Models of Language", "abstract": "This chapter critically examines the potential contributions of modern\nlanguage models to theoretical linguistics. Despite their focus on engineering\ngoals, these models' ability to acquire sophisticated linguistic knowledge from\nmere exposure to data warrants a careful reassessment of their relevance to\nlinguistic theory. I review a growing body of empirical evidence suggesting\nthat language models can learn hierarchical syntactic structure and exhibit\nsensitivity to various linguistic phenomena, even when trained on\ndevelopmentally plausible amounts of data. While the competence/performance\ndistinction has been invoked to dismiss the relevance of such models to\nlinguistic theory, I argue that this assessment may be premature. By carefully\ncontrolling learning conditions and making use of causal intervention methods,\nexperiments with language models can potentially constrain hypotheses about\nlanguage acquisition and competence. I conclude that closer collaboration\nbetween theoretical linguists and computational researchers could yield\nvaluable insights, particularly in advancing debates about linguistic nativism.", "published": "2024-08-13 18:26:04", "link": "http://arxiv.org/abs/2408.07144v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unlocking Efficiency: Adaptive Masking for Gene Transformer Models", "abstract": "Gene transformer models such as Nucleotide Transformer, DNABert, and LOGO are\ntrained to learn optimal gene sequence representations by using the Masked\nLanguage Modeling (MLM) training objective over the complete Human Reference\nGenome. However, the typical tokenization methods employ a basic sliding window\nof tokens, such as k-mers, that fail to utilize gene-centric semantics. This\ncould result in the (trivial) masking of easily predictable sequences, leading\nto inefficient MLM training. Time-variant training strategies are known to\nimprove pretraining efficiency in both language and vision tasks. In this work,\nwe focus on using curriculum masking where we systematically increase the\ndifficulty of masked token prediction task by using a Pointwise Mutual\nInformation-based difficulty criterion, as gene sequences lack well-defined\nsemantic units similar to words or sentences of NLP domain. Our proposed\nCurriculum Masking-based Gene Masking Strategy (CM-GEMS) demonstrates superior\nrepresentation learning capabilities compared to baseline masking approaches\nwhen evaluated on downstream gene sequence classification tasks. We perform\nextensive evaluation in both few-shot (five datasets) and full dataset settings\n(Genomic Understanding Evaluation benchmark consisting of 27 tasks). Our\nfindings reveal that CM-GEMS outperforms state-of-the-art models (DNABert-2,\nNucleotide transformer, DNABert) trained at 120K steps, achieving similar\nresults in just 10K and 1K steps. We also demonstrate that Curriculum-Learned\nLOGO (a 2-layer DNABert-like model) can achieve nearly 90% of the\nstate-of-the-art model performance of 120K steps. We will make the models and\ncodes publicly available at https://github.com/roysoumya/curriculum-GeneMask.", "published": "2024-08-13 19:45:02", "link": "http://arxiv.org/abs/2408.07180v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT's Conceptual Cartography: Mapping the Landscapes of Meaning", "abstract": "Conceptual Engineers want to make words better. However, they often\nunderestimate how varied our usage of words is. In this paper, we take the\nfirst steps in exploring the contextual nuances of words by creating conceptual\nlandscapes -- 2D surfaces representing the pragmatic usage of words -- that\nconceptual engineers can use to inform their projects. We use the spoken\ncomponent of the British National Corpus and BERT to create contextualised word\nembeddings, and use Gaussian Mixture Models, a selection of metrics, and\nqualitative analysis to visualise and numerically represent lexical landscapes.\nSuch an approach has not yet been used in the conceptual engineering literature\nand provides a detailed examination of how different words manifest in various\ncontexts that is potentially useful to conceptual engineering projects. Our\nfindings highlight the inherent complexity of conceptual engineering, revealing\nthat each word exhibits a unique and intricate landscape. Conceptual Engineers\ncannot, therefore, use a one-size-fits-all approach when improving words -- a\ntask that may be practically intractable at scale.", "published": "2024-08-13 20:08:26", "link": "http://arxiv.org/abs/2408.07190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PEARL: Parallel Speculative Decoding with Adaptive Draft Length", "abstract": "Speculative decoding (SD), where an extra draft model is employed to provide\nmultiple draft tokens first, and then the original target model verifies these\ntokens in parallel, has shown great power for LLM inference acceleration.\nHowever, existing SD methods suffer from the mutual waiting problem, i.e., the\ntarget model gets stuck when the draft model is guessing tokens, and vice\nversa. This problem is directly incurred by the asynchronous execution of the\ndraft model and the target model and is exacerbated due to the fixed draft\nlength in speculative decoding. To address these challenges, we propose a\nconceptually simple, flexible, and general framework to boost speculative\ndecoding, namely Parallel spEculative decoding with Adaptive dRaft Length\n(PEARL). Specifically, PEARL proposes pre-verify to verify the first draft\ntoken in advance during the drafting phase, and post-verify to generate more\ndraft tokens during the verification phase. PEARL parallels the drafting phase\nand the verification phase via applying the two strategies, and achieves\nadaptive draft length for different scenarios, which effectively alleviates the\nmutual waiting problem. Experiments on various text generation benchmarks\ndemonstrate the effectiveness of our PEARL, leading to a superior speed up\nperformance up to 4.43$\\times$ and 1.50$\\times$, compared to auto-regressive\ndecoding and vanilla speculative decoding, respectively. Our code is available\nat https://github.com/smart-lty/ParallelSpeculativeDecoding.", "published": "2024-08-13 08:32:06", "link": "http://arxiv.org/abs/2408.11850v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out\n  Strategies", "abstract": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.", "published": "2024-08-13 02:07:00", "link": "http://arxiv.org/abs/2408.06567v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Social Debiasing for Fair Multi-modal LLMs", "abstract": "Multi-modal Large Language Models (MLLMs) have advanced significantly,\noffering powerful vision-language understanding capabilities. However, these\nmodels often inherit severe social biases from their training datasets, leading\nto unfair predictions based on attributes like race and gender. This paper\naddresses the issue of social biases in MLLMs by i) Introducing a comprehensive\nCounterfactual dataset with Multiple Social Concepts (CMSC), which provides a\nmore diverse and extensive training set compared to existing datasets. ii)\nProposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by\nrevisiting the MLLM training process, rescaling the autoregressive loss\nfunction, and improving data sampling methods to counteract biases. Through\nextensive experiments on various MLLMs, our CMSC dataset and ASD method\ndemonstrate a significant reduction in social biases while maintaining the\nmodels' original performance.", "published": "2024-08-13 02:08:32", "link": "http://arxiv.org/abs/2408.06569v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Perspective on Large Language Models, Intelligent Machines, and\n  Knowledge Acquisition", "abstract": "Large Language Models (LLMs) are known for their remarkable ability to\ngenerate synthesized 'knowledge', such as text documents, music, images, etc.\nHowever, there is a huge gap between LLM's and human capabilities for\nunderstanding abstract concepts and reasoning. We discuss these issues in a\nlarger philosophical context of human knowledge acquisition and the Turing\ntest. In addition, we illustrate the limitations of LLMs by analyzing GPT-4\nresponses to questions ranging from science and math to common sense reasoning.\nThese examples show that GPT-4 can often imitate human reasoning, even though\nit lacks understanding. However, LLM responses are synthesized from a large LLM\nmodel trained on all available data. In contrast, human understanding is based\non a small number of abstract concepts. Based on this distinction, we discuss\nthe impact of LLMs on acquisition of human knowledge and education.", "published": "2024-08-13 03:25:49", "link": "http://arxiv.org/abs/2408.06598v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs", "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We find that\ncombining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables\nrobust and efficient unlearning for LLMs. First, we introduce Inverted Hinge\nLoss, which suppresses unwanted tokens while maintaining fluency by boosting\nthe probability of the next most likely token. Second, we develop a\ndata-adaptive initialization for LoRA adapters via low-rank approximation\nweighted with relative Fisher information, thereby focusing updates on\nparameters critical for removing targeted knowledge. Experiments on the\nTraining Data Extraction Challenge dataset using GPT-Neo models as well as on\nthe TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our\napproach effectively removes sensitive information while maintaining reasoning\nand generative capabilities with minimal impact. Our implementation can be\nfound in https://github.com/csm9493/efficient-llm-unlearning.", "published": "2024-08-13 04:18:32", "link": "http://arxiv.org/abs/2408.06621v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Amuro and Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models", "abstract": "The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.", "published": "2024-08-13 06:28:43", "link": "http://arxiv.org/abs/2408.06663v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sumotosima: A Framework and Dataset for Classifying and Summarizing\n  Otoscopic Images", "abstract": "Otoscopy is a diagnostic procedure to examine the ear canal and eardrum using\nan otoscope. It identifies conditions like infections, foreign bodies, ear drum\nperforations and ear abnormalities. We propose a novel resource efficient deep\nlearning and transformer based framework, Sumotosima (Summarizer for otoscopic\nimages), an end-to-end pipeline for classification followed by summarization.\nOur framework works on combination of triplet and cross-entropy losses.\nAdditionally, we use Knowledge Enhanced Multimodal BART whose input is fused\ntextual and image embedding. The objective is to provide summaries that are\nwell-suited for patients, ensuring clarity and efficiency in understanding\notoscopic images. Given the lack of existing datasets, we have curated our own\nOCASD (Otoscopic Classification And Summary Dataset), which includes 500 images\nwith 5 unique categories annotated with their class and summaries by\nOtolaryngologists. Sumotosima achieved a result of 98.03%, which is 7.00%,\n3.10%, 3.01% higher than K-Nearest Neighbors, Random Forest and Support Vector\nMachines, respectively, in classification tasks. For summarization, Sumotosima\noutperformed GPT-4o and LLaVA by 88.53% and 107.57% in ROUGE scores,\nrespectively. We have made our code and dataset publicly available at\nhttps://github.com/anas2908/Sumotosima", "published": "2024-08-13 09:26:41", "link": "http://arxiv.org/abs/2408.06755v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data\n  Uncertainty", "abstract": "Despite the massive advancements in large language models (LLMs), they still\nsuffer from producing plausible but incorrect responses. To improve the\nreliability of LLMs, recent research has focused on uncertainty quantification\nto predict whether a response is correct or not. However, most uncertainty\nquantification methods have been evaluated on single-labeled questions, which\nremoves data uncertainty: the irreducible randomness often present in user\nqueries, which can arise from factors like multiple possible answers. This\nlimitation may cause uncertainty quantification results to be unreliable in\npractical settings. In this paper, we investigate previous uncertainty\nquantification methods under the presence of data uncertainty. Our\ncontributions are two-fold: 1) proposing a new Multi-Answer Question Answering\ndataset, MAQA, consisting of world knowledge, mathematical reasoning, and\ncommonsense reasoning tasks to evaluate uncertainty quantification regarding\ndata uncertainty, and 2) assessing 5 uncertainty quantification methods of\ndiverse white- and black-box LLMs. Our findings show that previous methods\nrelatively struggle compared to single-answer settings, though this varies\ndepending on the task. Moreover, we observe that entropy- and consistency-based\nmethods effectively estimate model uncertainty, even in the presence of data\nuncertainty. We believe these observations will guide future work on\nuncertainty quantification in more realistic settings.", "published": "2024-08-13 11:17:31", "link": "http://arxiv.org/abs/2408.06816v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Causal Agent based on Large Language Model", "abstract": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.", "published": "2024-08-13 12:22:26", "link": "http://arxiv.org/abs/2408.06849v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Diagnosis extraction from unstructured Dutch echocardiogram reports\n  using span- and document-level characteristic classification", "abstract": "Clinical machine learning research and AI driven clinical decision support\nmodels rely on clinically accurate labels. Manually extracting these labels\nwith the help of clinical specialists is often time-consuming and expensive.\nThis study tests the feasibility of automatic span- and document-level\ndiagnosis extraction from unstructured Dutch echocardiogram reports. We\nincluded 115,692 unstructured echocardiogram reports from the UMCU a large\nuniversity hospital in the Netherlands. A randomly selected subset was manually\nannotated for the occurrence and severity of eleven commonly described cardiac\ncharacteristics. We developed and tested several automatic labelling techniques\nat both span and document levels, using weighted and macro F1-score, precision,\nand recall for performance evaluation. We compared the performance of span\nlabelling against document labelling methods, which included both direct\ndocument classifiers and indirect document classifiers that rely on span\nclassification results. The SpanCategorizer and MedRoBERTa$.$nl models\noutperformed all other span and document classifiers, respectively. The\nweighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in\nSpanCategorizer and 0.96 to 0.98 in MedRoBERTa$.$nl. Direct document\nclassification was superior to indirect document classification using span\nclassifiers. SetFit achieved competitive document classification performance\nusing only 10% of the training data. Utilizing a reduced label set yielded\nnear-perfect document classification results. We recommend using our published\nSpanCategorizer and MedRoBERTa$.$nl models for span- and document-level\ndiagnosis extraction from Dutch echocardiography reports. For settings with\nlimited training data, SetFit may be a promising alternative for document\nclassification.", "published": "2024-08-13 14:33:32", "link": "http://arxiv.org/abs/2408.06930v2", "categories": ["cs.CL", "cs.AI", "68T50, 68P20", "I.2.7; J.3; H.3.3"], "primary_category": "cs.CL"}
{"title": "The advantages of context specific language models: the case of the\n  Erasmian Language Model", "abstract": "The current trend to improve language model performance seems to be based on\nscaling up with the number of parameters (e.g. the state of the art GPT4 model\nhas approximately 1.7 trillion parameters) or the amount of training data fed\ninto the model. However this comes at significant costs in terms of\ncomputational resources and energy costs that compromise the sustainability of\nAI solutions, as well as risk relating to privacy and misuse. In this paper we\npresent the Erasmian Language Model (ELM) a small context specific, 900 million\nparameter model, pre-trained and fine-tuned by and for Erasmus University\nRotterdam. We show how the model performs adequately in a classroom context for\nessay writing, and how it achieves superior performance in subjects that are\npart of its context. This has implications for a wide range of institutions and\norganizations, showing that context specific language models may be a viable\nalternative for resource constrained, privacy sensitive use cases.", "published": "2024-08-13 14:34:59", "link": "http://arxiv.org/abs/2408.06931v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative AI for automatic topic labelling", "abstract": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics.", "published": "2024-08-13 16:07:16", "link": "http://arxiv.org/abs/2408.07003v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs", "abstract": "Current long context large language models (LLMs) can process inputs up to\n100,000 tokens, yet struggle to generate outputs exceeding even a modest length\nof 2,000 words. Through controlled experiments, we find that the model's\neffective generation length is inherently bounded by the sample it has seen\nduring supervised fine-tuning (SFT). In other words, their output limitation is\ndue to the scarcity of long-output examples in existing SFT datasets. To\naddress this, we introduce AgentWrite, an agent-based pipeline that decomposes\nultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to\ngenerate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we\nconstruct LongWriter-6k, a dataset containing 6,000 SFT data with output\nlengths ranging from 2k to 32k words. By incorporating this dataset into model\ntraining, we successfully scale the output length of existing models to over\n10,000 words while maintaining output quality. We also develop LongBench-Write,\na comprehensive benchmark for evaluating ultra-long generation capabilities.\nOur 9B parameter model, further improved through DPO, achieves state-of-the-art\nperformance on this benchmark, surpassing even much larger proprietary models.\nIn general, our work demonstrates that existing long context LLM already\npossesses the potential for a larger output window--all you need is data with\nextended output during model alignment to unlock this capability. Our code &\nmodels are at: https://github.com/THUDM/LongWriter.", "published": "2024-08-13 17:46:12", "link": "http://arxiv.org/abs/2408.07055v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fingerspelling within Sign Language Translation", "abstract": "Fingerspelling poses challenges for sign language processing due to its\nhigh-frequency motion and use for open-vocabulary terms. While prior work has\nstudied fingerspelling recognition, there has been little attention to\nevaluating how well sign language translation models understand fingerspelling\nin the context of entire sentences -- and improving this capability. We\nmanually annotate instances of fingerspelling within FLEURS-ASL and use them to\nevaluate the effect of two simple measures to improve fingerspelling\nrecognition within American Sign Language to English translation: 1) use a\nmodel family (ByT5) with character- rather than subword-level tokenization, and\n2) mix fingerspelling recognition data into the translation training mixture.\nWe find that 1) substantially improves understanding of fingerspelling (and\ntherefore translation quality overall), but the effect of 2) is mixed.", "published": "2024-08-13 17:57:14", "link": "http://arxiv.org/abs/2408.07065v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Self-folding Self-replication", "abstract": "Inspired by protein folding, we explored the construction of\nthree-dimensional structures and machines from one-dimensional chains of simple\nbuilding blocks. This approach not only allows us to recreate the\nself-replication mechanism introduced earlier, but also significantly\nsimplifies the process. We introduced a new set of folding blocks that\nfacilitate the formation of secondary structures such as {\\alpha}-helices and\n\\b{eta}-sheets, as well as more advanced tertiary and quaternary structures,\nincluding self-replicating machines. The introduction of rotational degrees of\nfreedom leads to a reduced variety of blocks and, most importantly, reduces the\noverall size of the machines by a factor of five. In addition, we present a\nuniversal copier-constructor, a highly efficient self-replicating mechanism\ncomposed of approximately 40 blocks, including the restictions posed on it. The\npaper also addresses evolutionary considerations, outlining several steps on\nthe evolutionary ladder towards more sophisticated self-replicating systems.\nFinally, this study offers a clear rationale for nature's preference for\none-dimensional chains in constructing three-dimensional structures.", "published": "2024-08-13 18:50:07", "link": "http://arxiv.org/abs/2408.07154v1", "categories": ["cs.CL", "physics.bio-ph"], "primary_category": "cs.CL"}
{"title": "MGH Radiology Llama: A Llama 3 70B Model for Radiology", "abstract": "In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs.", "published": "2024-08-13 01:30:03", "link": "http://arxiv.org/abs/2408.11848v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CROME: Cross-Modal Adapters for Efficient Multimodal LLM", "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable\nimage-language capabilities, but their widespread use faces challenges in\ncost-effective training and adaptation. Existing approaches often necessitate\nexpensive language model retraining and limited adaptability. Additionally, the\ncurrent focus on zero-shot performance improvements offers insufficient\nguidance for task-specific tuning. We propose CROME, an efficient\nvision-language instruction tuning framework. It features a novel gated\ncross-modal adapter that effectively combines visual and textual\nrepresentations prior to input into a frozen LLM. This lightweight adapter,\ntrained with minimal parameters, enables efficient cross-modal understanding.\nNotably, CROME demonstrates superior zero-shot performance on standard visual\nquestion answering and instruction-following benchmarks. Moreover, it yields\nfine-tuning with exceptional parameter efficiency, competing with task-specific\nspecialist state-of-the-art methods. CROME demonstrates the potential of pre-LM\nalignment for building scalable, adaptable, and parameter-efficient multimodal\nmodels.", "published": "2024-08-13 03:45:11", "link": "http://arxiv.org/abs/2408.06610v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Generalized knowledge-enhanced framework for biomedical entity and\n  relation extraction", "abstract": "In recent years, there has been an increasing number of frameworks developed\nfor biomedical entity and relation extraction. This research effort aims to\naddress the accelerating growth in biomedical publications and the intricate\nnature of biomedical texts, which are written for mainly domain experts. To\nhandle these challenges, we develop a novel framework that utilizes external\nknowledge to construct a task-independent and reusable background knowledge\ngraph for biomedical entity and relation extraction. The design of our model is\ninspired by how humans learn domain-specific topics. In particular, humans\noften first acquire the most basic and common knowledge regarding a field to\nbuild the foundational knowledge and then use that as a basis for extending to\nvarious specialized topics. Our framework employs such common-knowledge-sharing\nmechanism to build a general neural-network knowledge graph that is learning\ntransferable to different domain-specific biomedical texts effectively.\nExperimental evaluations demonstrate that our model, equipped with this\ngeneralized and cross-transferable knowledge base, achieves competitive\nperformance benchmarks, including BioRelEx for binding interaction detection\nand ADE for Adverse Drug Effect identification.", "published": "2024-08-13 04:06:45", "link": "http://arxiv.org/abs/2408.06618v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "WorldScribe: Towards Context-Aware Live Visual Descriptions", "abstract": "Automated live visual descriptions can aid blind people in understanding\ntheir surroundings with autonomy and independence. However, providing\ndescriptions that are rich, contextual, and just-in-time has been a\nlong-standing challenge in accessibility. In this work, we develop WorldScribe,\na system that generates automated live real-world visual descriptions that are\ncustomizable and adaptive to users' contexts: (i) WorldScribe's descriptions\nare tailored to users' intents and prioritized based on semantic relevance.\n(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively\nsuccinct descriptions for dynamic scenes, while presenting longer and detailed\nones for stable settings. (iii) WorldScribe is adaptive to sound contexts,\ne.g., increasing volume in noisy environments, or pausing when conversations\nstart. Powered by a suite of vision, language, and sound recognition models,\nWorldScribe introduces a description generation pipeline that balances the\ntradeoffs between their richness and latency to support real-time use. The\ndesign of WorldScribe is informed by prior work on providing visual\ndescriptions and a formative study with blind participants. Our user study and\nsubsequent pipeline evaluation show that WorldScribe can provide real-time and\nfairly accurate visual descriptions to facilitate environment understanding\nthat is adaptive and customized to users' contexts. Finally, we discuss the\nimplications and further steps toward making live visual descriptions more\ncontext-aware and humanized.", "published": "2024-08-13 04:32:45", "link": "http://arxiv.org/abs/2408.06627v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "EditScribe: Non-Visual Image Editing with Natural Language Verification\n  Loops", "abstract": "Image editing is an iterative process that requires precise visual evaluation\nand manipulation for the output to match the editing intent. However, current\nimage editing tools do not provide accessible interaction nor sufficient\nfeedback for blind and low vision individuals to achieve this level of control.\nTo address this, we developed EditScribe, a prototype system that makes image\nediting accessible using natural language verification loops powered by large\nmultimodal models. Using EditScribe, the user first comprehends the image\ncontent through initial general and object descriptions, then specifies edit\nactions using open-ended natural language prompts. EditScribe performs the\nimage edit, and provides four types of verification feedback for the user to\nverify the performed edit, including a summary of visual changes, AI judgement,\nand updated general and object descriptions. The user can ask follow-up\nquestions to clarify and probe into the edits or verification feedback, before\nperforming another edit. In a study with ten blind or low-vision users, we\nfound that EditScribe supported participants to perform and verify image edit\nactions non-visually. We observed different prompting strategies from\nparticipants, and their perceptions on the various types of verification\nfeedback. Finally, we discuss the implications of leveraging natural language\nverification loops to make visual authoring non-visually accessible.", "published": "2024-08-13 04:40:56", "link": "http://arxiv.org/abs/2408.06632v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Enhancing Visual Dialog State Tracking through Iterative Object-Entity\n  Alignment in Multi-Round Conversations", "abstract": "Visual Dialog (VD) is a task where an agent answers a series of image-related\nquestions based on a multi-round dialog history. However, previous VD methods\noften treat the entire dialog history as a simple text input, disregarding the\ninherent conversational information flows at the round level. In this paper, we\nintroduce Multi-round Dialogue State Tracking model (MDST), a framework that\naddresses this limitation by leveraging the dialogue state learned from dialog\nhistory to answer questions. MDST captures each round of dialog history,\nconstructing internal dialogue state representations defined as 2-tuples of\nvision-language representations. These representations effectively ground the\ncurrent question, enabling the generation of accurate answers. Experimental\nresults on the VisDial v1.0 dataset demonstrate that MDST achieves a new\nstate-of-the-art performance in generative setting. Furthermore, through a\nseries of human studies, we validate the effectiveness of MDST in generating\nlong, consistent, and human-like answers while consistently answering a series\nof questions correctly.", "published": "2024-08-13 08:36:15", "link": "http://arxiv.org/abs/2408.06725v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Large language models can consistently generate high-quality content for\n  election disinformation operations", "abstract": "Advances in large language models have raised concerns about their potential\nuse in generating compelling election disinformation at scale. This study\npresents a two-part investigation into the capabilities of LLMs to automate\nstages of an election disinformation operation. First, we introduce DisElect, a\nnovel evaluation dataset designed to measure LLM compliance with instructions\nto generate content for an election disinformation operation in localised UK\ncontext, containing 2,200 malicious prompts and 50 benign prompts. Using\nDisElect, we test 13 LLMs and find that most models broadly comply with these\nrequests; we also find that the few models which refuse malicious prompts also\nrefuse benign election-related prompts, and are more likely to refuse to\ngenerate content from a right-wing perspective. Secondly, we conduct a series\nof experiments (N=2,340) to assess the \"humanness\" of LLMs: the extent to which\ndisinformation operation content generated by an LLM is able to pass as\nhuman-written. Our experiments suggest that almost all LLMs tested released\nsince 2022 produce election disinformation operation content indiscernible by\nhuman evaluators over 50% of the time. Notably, we observe that multiple models\nachieve above-human levels of humanness. Taken together, these findings suggest\nthat current LLMs can be used to generate high-quality content for election\ndisinformation operations, even in hyperlocalised scenarios, at far lower costs\nthan traditional methods, and offer researchers and policymakers an empirical\nbenchmark for the measurement and evaluation of these capabilities in current\nand future models.", "published": "2024-08-13 08:45:34", "link": "http://arxiv.org/abs/2408.06731v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Exploring the anatomy of articulation rate in spontaneous English\n  speech: relationships between utterance length effects and social factors", "abstract": "Speech rate has been shown to vary across social categories such as gender,\nage, and dialect, while also being conditioned by properties of speech\nplanning. The effect of utterance length, where speech rate is faster and less\nvariable for longer utterances, has also been shown to reduce the role of\nsocial factors once it has been accounted for, leaving unclear the relationship\nbetween social factors and speech production in conditioning speech rate.\nThrough modelling of speech rate across 13 English speech corpora, it is found\nthat utterance length has the largest effect on speech rate, though this effect\nitself varies little across corpora and speakers. While age and gender also\nmodulate speech rate, their effects are much smaller in magnitude. These\nfindings suggest utterance length effects may be conditioned by articulatory\nand perceptual constraints, and that social influences on speech rate should be\ninterpreted in the broader context of how speech rate variation is structured.", "published": "2024-08-13 08:47:29", "link": "http://arxiv.org/abs/2408.06732v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Survey on Model MoErging: Recycling and Routing Among Specialized\n  Experts for Collaborative Learning", "abstract": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field.", "published": "2024-08-13 17:49:00", "link": "http://arxiv.org/abs/2408.07057v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Diversity Empowers Intelligence: Integrating Expertise of Software\n  Engineering Agents", "abstract": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.", "published": "2024-08-13 17:50:28", "link": "http://arxiv.org/abs/2408.07060v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Neural embedding of beliefs reveals the role of relative dissonance in\n  human decision-making", "abstract": "Beliefs form the foundation of human cognition and decision-making, guiding\nour actions and social connections. A model encapsulating beliefs and their\ninterrelationships is crucial for understanding their influence on our actions.\nHowever, research on belief interplay has often been limited to beliefs related\nto specific issues and relied heavily on surveys. We propose a method to study\nthe nuanced interplay between thousands of beliefs by leveraging an online user\ndebate data and mapping beliefs onto a neural embedding space constructed using\na fine-tuned large language model (LLM). This belief space captures the\ninterconnectedness and polarization of diverse beliefs across social issues.\nOur findings show that positions within this belief space predict new beliefs\nof individuals and estimate cognitive dissonance based on the distance between\nexisting and new beliefs. This study demonstrates how LLMs, combined with\ncollective online records of human beliefs, can offer insights into the\nfundamental principles that govern human decision-making.", "published": "2024-08-13 23:58:45", "link": "http://arxiv.org/abs/2408.07237v2", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge\n  Distillation Approach", "abstract": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior\nperformance in complex human-like interactions. But they are costly, or too\nlarge for edge devices such as smartphones and harder to self-host, leading to\nsecurity and privacy concerns. This paper introduces a novel interpretable\nknowledge distillation approach to enhance the performance of smaller, more\neconomical LLMs that firms can self-host. We study this problem in the context\nof building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge\ndistillation, where the \"student\" model learns directly from the \"teacher\"\nmodel's responses via fine-tuning, our interpretable \"strategy\" teaching\napproach involves the teacher providing strategies to improve the student's\nperformance in various scenarios. This method alternates between a \"scenario\ngeneration\" step and a \"strategies for improvement\" step, creating a customized\nlibrary of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence\nit can be used without manipulating model parameters. In our customer service\napplication, the method improves performance, and the learned strategies are\ntransferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit.", "published": "2024-08-13 23:59:36", "link": "http://arxiv.org/abs/2408.07238v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "What should I wear to a party in a Greek taverna? Evaluation for\n  Conversational Agents in the Fashion Domain", "abstract": "Large language models (LLMs) are poised to revolutionize the domain of online\nfashion retail, enhancing customer experience and discovery of fashion online.\nLLM-powered conversational agents introduce a new way of discovery by directly\ninteracting with customers, enabling them to express in their own ways, refine\ntheir needs, obtain fashion and shopping advice that is relevant to their taste\nand intent. For many tasks in e-commerce, such as finding a specific product,\nconversational agents need to convert their interactions with a customer to a\nspecific call to different backend systems, e.g., a search system to showcase a\nrelevant set of products. Therefore, evaluating the capabilities of LLMs to\nperform those tasks related to calling other services is vital. However, those\nevaluations are generally complex, due to the lack of relevant and high quality\ndatasets, and do not align seamlessly with business needs, amongst others. To\nthis end, we created a multilingual evaluation dataset of 4k conversations\nbetween customers and a fashion assistant in a large e-commerce fashion\nplatform to measure the capabilities of LLMs to serve as an assistant between\ncustomers and a backend engine. We evaluate a range of models, showcasing how\nour dataset scales to business needs and facilitates iterative development of\ntools.", "published": "2024-08-13 11:11:27", "link": "http://arxiv.org/abs/2408.08907v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
{"title": "Style-Talker: Finetuning Audio Language Model and Style-Based\n  Text-to-Speech Model for Fast Spoken Dialogue Generation", "abstract": "The rapid advancement of large language models (LLMs) has significantly\npropelled the development of text-based chatbots, demonstrating their\ncapability to engage in coherent and contextually relevant dialogues. However,\nextending these advancements to enable end-to-end speech-to-speech conversation\nbots remains a formidable challenge, primarily due to the extensive dataset and\ncomputational resources required. The conventional approach of cascading\nautomatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a\npipeline, while effective, suffers from unnatural prosody because it lacks\ndirect interactions between the input audio and its transcribed text and the\noutput audio. These systems are also limited by their inherent latency from the\nASR process for real-time applications. This paper introduces Style-Talker, an\ninnovative framework that fine-tunes an audio LLM alongside a style-based TTS\nmodel for fast spoken dialog generation. Style-Talker takes user input audio\nand uses transcribed chat history and speech styles to generate both the\nspeaking style and text for the response. Subsequently, the TTS model\nsynthesizes the speech, which is then played back to the user. While the\nresponse speech is being played, the input speech undergoes ASR processing to\nextract the transcription and speaking style, serving as the context for the\nensuing dialogue turn. This novel pipeline accelerates the traditional cascade\nASR-LLM-TTS systems while integrating rich paralinguistic information from\ninput speech. Our experimental results show that Style-Talker significantly\noutperforms the conventional cascade and speech-to-speech baselines in terms of\nboth dialogue naturalness and coherence while being more than 50% faster.", "published": "2024-08-13 04:35:11", "link": "http://arxiv.org/abs/2408.11849v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM\n  Approach", "abstract": "Accurate stock market predictions following earnings reports are crucial for\ninvestors. Traditional methods, particularly classical machine learning models,\nstruggle with these predictions because they cannot effectively process and\ninterpret extensive textual data contained in earnings reports and often\noverlook nuances that influence market movements. This paper introduces an\nadvanced approach by employing Large Language Models (LLMs) instruction\nfine-tuned with a novel combination of instruction-based techniques and\nquantized low-rank adaptation (QLoRA) compression. Our methodology integrates\n'base factors', such as financial metric growth and earnings transcripts, with\n'external factors', including recent market indices performances and analyst\ngrades, to create a rich, supervised dataset. This comprehensive dataset\nenables our models to achieve superior predictive performance in terms of\naccuracy, weighted F1, and Matthews correlation coefficient (MCC), especially\nevident in the comparison with benchmarks such as GPT-4. We specifically\nhighlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases\nsignificant improvements over baseline models. The paper also discusses the\npotential of expanding the output capabilities to include a 'Hold' option and\nextending the prediction horizon, aiming to accommodate various investment\nstyles and time frames. This study not only demonstrates the power of\nintegrating cutting-edge AI with fine-tuned financial data but also paves the\nway for future research in enhancing AI-driven financial analysis tools.", "published": "2024-08-13 04:53:31", "link": "http://arxiv.org/abs/2408.06634v2", "categories": ["q-fin.CP", "cs.AI", "cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "q-fin.CP"}
{"title": "TableGuard -- Securing Structured & Unstructured Data", "abstract": "With the increasing demand for data sharing across platforms and\norganizations, ensuring the privacy and security of sensitive information has\nbecome a critical challenge. This paper introduces \"TableGuard\". An innovative\napproach to data obfuscation tailored for relational databases. Building on the\nprinciples and techniques developed in prior work on context-sensitive\nobfuscation, TableGuard applies these methods to ensure that API calls return\nonly obfuscated data, thereby safeguarding privacy when sharing data with third\nparties. TableGuard leverages advanced context-sensitive obfuscation techniques\nto replace sensitive data elements with contextually appropriate alternatives.\nBy maintaining the relational integrity and coherence of the data, our approach\nmitigates the risks of cognitive dissonance and data leakage. We demonstrate\nthe implementation of TableGuard using a BERT based transformer model, which\nidentifies and obfuscates sensitive entities within relational tables. Our\nevaluation shows that TableGuard effectively balances privacy protection with\ndata utility, minimizing information loss while ensuring that the obfuscated\ndata remains functionally useful for downstream applications. The results\nhighlight the importance of domain-specific obfuscation strategies and the role\nof context length in preserving data integrity. The implications of this\nresearch are significant for organizations that need to share data securely\nwith external parties. TableGuard offers a robust framework for implementing\nprivacy-preserving data sharing mechanisms, thereby contributing to the broader\nfield of data privacy and security.", "published": "2024-08-13 17:20:52", "link": "http://arxiv.org/abs/2408.07045v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "The News Comment Gap and Algorithmic Agenda Setting in Online Forums", "abstract": "The disparity between news stories valued by journalists and those preferred\nby readers, known as the \"News Gap\", is well-documented. However, the\ndifference in expectations regarding news related user-generated content is\nless studied. Comment sections, hosted by news websites, are popular venues for\nreader engagement, yet still subject to editorial decisions. It is thus\nimportant to understand journalist vs reader comment preferences and how these\nare served by various comment ranking algorithms that represent discussions\ndifferently. We analyse 1.2 million comments from Austrian newspaper Der\nStandard to understand the \"News Comment Gap\" and the effects of different\nranking algorithms. We find that journalists prefer positive, timely, complex,\ndirect responses, while readers favour comments similar to article content from\nelite authors. We introduce the versatile Feature-Oriented Ranking Utility\nMetric (FORUM) to assess the impact of different ranking algorithms and find\ndramatic differences in how they prioritise the display of comments by\nsentiment, topical relevance, lexical diversity, and readability. Journalists\ncan exert substantial influence over the discourse through both curatorial and\nalgorithmic means. Understanding these choices' implications is vital in\nfostering engaging and civil discussions while aligning with journalistic\nobjectives, especially given the increasing legal scrutiny and societal\nimportance of online discourse.", "published": "2024-08-13 17:43:32", "link": "http://arxiv.org/abs/2408.07052v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CY"}
{"title": "SaSLaW: Dialogue Speech Corpus with Audio-visual Egocentric Information\n  Toward Environment-adaptive Dialogue Speech Synthesis", "abstract": "This paper presents SaSLaW, a spontaneous dialogue speech corpus containing\nsynchronous recordings of what speakers speak, listen to, and watch. Humans\nconsider the diverse environmental factors and then control the features of\ntheir utterances in face-to-face voice communications. Spoken dialogue systems\ncapable of this adaptation to these audio environments enable natural and\nseamless communications. SaSLaW was developed to model human-speech adjustment\nfor audio environments via first-person audio-visual perceptions in spontaneous\ndialogues. We propose the construction methodology of SaSLaW and display the\nanalysis result of the corpus. We additionally conducted an experiment to\ndevelop text-to-speech models using SaSLaW and evaluate their performance of\nadaptations to audio environments. The results indicate that models\nincorporating hearing-audio data output more plausible speech tailored to\ndiverse audio environments than the vanilla text-to-speech model.", "published": "2024-08-13 12:38:56", "link": "http://arxiv.org/abs/2408.06858v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "PRESENT: Zero-Shot Text-to-Prosody Control", "abstract": "Current strategies for achieving fine-grained prosody control in speech\nsynthesis entail extracting additional style embeddings or adopting more\ncomplex architectures. To enable zero-shot application of pretrained\ntext-to-speech (TTS) models, we present PRESENT (PRosody Editing without Style\nEmbeddings or New Training), which exploits explicit prosody prediction in\nFastSpeech2-based models by modifying the inference process directly. We apply\nour text-to-prosody framework to zero-shot language transfer using a JETS model\nexclusively trained on English LJSpeech data. We obtain character error rates\n(CER) of 12.8%, 18.7% and 5.9% for German, Hungarian and Spanish respectively,\nbeating the previous state-of-the-art CER by over 2x for all three languages.\nFurthermore, we allow subphoneme-level control, a first in this field. To\nevaluate its effectiveness, we show that PRESENT can improve the prosody of\nquestions, and use it to generate Mandarin, a tonal language where vowel pitch\nvaries at subphoneme level. We attain 25.3% hanzi CER and 13.0% pinyin CER with\nthe JETS model. All our code and audio samples are available online.", "published": "2024-08-13 11:39:07", "link": "http://arxiv.org/abs/2408.06827v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "BSS-CFFMA: Cross-Domain Feature Fusion and Multi-Attention Speech\n  Enhancement Network based on Self-Supervised Embedding", "abstract": "Speech self-supervised learning (SSL) represents has achieved\nstate-of-the-art (SOTA) performance in multiple downstream tasks. However, its\napplication in speech enhancement (SE) tasks remains immature, offering\nopportunities for improvement. In this study, we introduce a novel cross-domain\nfeature fusion and multi-attention speech enhancement network, termed\nBSS-CFFMA, which leverages self-supervised embeddings. BSS-CFFMA comprises a\nmulti-scale cross-domain feature fusion (MSCFF) block and a residual hybrid\nmulti-attention (RHMA) block. The MSCFF block effectively integrates\ncross-domain features, facilitating the extraction of rich acoustic\ninformation. The RHMA block, serving as the primary enhancement module,\nutilizes three distinct attention modules to capture diverse attention\nrepresentations and estimate high-quality speech signals.\n  We evaluate the performance of the BSS-CFFMA model through comparative and\nablation studies on the VoiceBank-DEMAND dataset, achieving SOTA results.\nFurthermore, we select three types of data from the WHAMR! dataset, a\ncollection specifically designed for speech enhancement tasks, to assess the\ncapabilities of BSS-CFFMA in tasks such as denoising only, dereverberation\nonly, and simultaneous denoising and dereverberation. This study marks the\nfirst attempt to explore the effectiveness of self-supervised embedding-based\nspeech enhancement methods in complex tasks encompassing dereverberation and\nsimultaneous denoising and dereverberation. The demo implementation of\nBSS-CFFMA is available online\\footnote[2]{https://github.com/AlimMat/BSS-CFFMA.\n\\label{s1}}.", "published": "2024-08-13 12:27:24", "link": "http://arxiv.org/abs/2408.06851v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "VNet: A GAN-based Multi-Tier Discriminator Network for Speech Synthesis\n  Vocoders", "abstract": "Since the introduction of Generative Adversarial Networks (GANs) in speech\nsynthesis, remarkable achievements have been attained. In a thorough\nexploration of vocoders, it has been discovered that audio waveforms can be\ngenerated at speeds exceeding real-time while maintaining high fidelity,\nachieved through the utilization of GAN-based models. Typically, the inputs to\nthe vocoder consist of band-limited spectral information, which inevitably\nsacrifices high-frequency details. To address this, we adopt the full-band Mel\nspectrogram information as input, aiming to provide the vocoder with the most\ncomprehensive information possible. However, previous studies have revealed\nthat the use of full-band spectral information as input can result in the issue\nof over-smoothing, compromising the naturalness of the synthesized speech. To\ntackle this challenge, we propose VNet, a GAN-based neural vocoder network that\nincorporates full-band spectral information and introduces a Multi-Tier\nDiscriminator (MTD) comprising multiple sub-discriminators to generate\nhigh-resolution signals. Additionally, we introduce an asymptotically\nconstrained method that modifies the adversarial loss of the generator and\ndiscriminator, enhancing the stability of the training process. Through\nrigorous experiments, we demonstrate that the VNet model is capable of\ngenerating high-fidelity speech and significantly improving the performance of\nthe vocoder.", "published": "2024-08-13 14:00:02", "link": "http://arxiv.org/abs/2408.06906v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Heterogeneous Space Fusion and Dual-Dimension Attention: A New Paradigm\n  for Speech Enhancement", "abstract": "Self-supervised learning has demonstrated impressive performance in speech\ntasks, yet there remains ample opportunity for advancement in the realm of\nspeech enhancement research. In addressing speech tasks, confining the\nattention mechanism solely to the temporal dimension poses limitations in\neffectively focusing on critical speech features. Considering the\naforementioned issues, our study introduces a novel speech enhancement\nframework, HFSDA, which skillfully integrates heterogeneous spatial features\nand incorporates a dual-dimension attention mechanism to significantly enhance\nspeech clarity and quality in noisy environments. By leveraging self-supervised\nlearning embeddings in tandem with Short-Time Fourier Transform (STFT)\nspectrogram features, our model excels at capturing both high-level semantic\ninformation and detailed spectral data, enabling a more thorough analysis and\nrefinement of speech signals. Furthermore, we employ the innovative\nOmni-dimensional Dynamic Convolution (ODConv) technology within the spectrogram\ninput branch, enabling enhanced extraction and integration of crucial\ninformation across multiple dimensions. Additionally, we refine the Conformer\nmodel by enhancing its feature extraction capabilities not only in the temporal\ndimension but also across the spectral domain. Extensive experiments on the\nVCTK-DEMAND dataset show that HFSDA is comparable to existing state-of-the-art\nmodels, confirming the validity of our approach.", "published": "2024-08-13 14:04:24", "link": "http://arxiv.org/abs/2408.06911v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Direction of Arrival Correction through Speech Quality Feedback", "abstract": "Real-time speech enhancement has began to rise in performance, and the Demucs\nDenoiser model has recently demonstrated strong performance in\nmultiple-speech-source scenarios when accompanied by a location-based speech\ntarget selection strategy. However, it has shown to be sensitive to errors in\nthe direction-of-arrival (DOA) estimation. In this work, a DOA correction\nscheme is proposed that uses the real-time estimated speech quality of its\nenhanced output as the observed variable in an Adam-based optimization feedback\nloop to find the correct DOA. In spite of the high variability of the speech\nquality estimation, the proposed system is able to correct in real-time an\nerror of up to 15$^o$ using only the speech quality as its guide. Several\ninsights are provided for future versions of the proposed system to speed up\nconvergence and further reduce the speech quality estimation variability.", "published": "2024-08-13 23:43:20", "link": "http://arxiv.org/abs/2408.07234v1", "categories": ["eess.AS", "cs.AI"], "primary_category": "eess.AS"}
{"title": "Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies", "abstract": "Existing methods on audio-visual deepfake detection mainly focus on\nhigh-level features for modeling inconsistencies between audio and visual data.\nAs a result, these approaches usually overlook finer audio-visual artifacts,\nwhich are inherent to deepfakes. Herein, we propose the introduction of\nfine-grained mechanisms for detecting subtle artifacts in both spatial and\ntemporal domains. First, we introduce a local audio-visual model capable of\ncapturing small spatial regions that are prone to inconsistencies with audio.\nFor that purpose, a fine-grained mechanism based on a spatially-local distance\ncoupled with an attention module is adopted. Second, we introduce a\ntemporally-local pseudo-fake augmentation to include samples incorporating\nsubtle temporal inconsistencies in our training set. Experiments on the DFDC\nand the FakeAVCeleb datasets demonstrate the superiority of the proposed method\nin terms of generalization as compared to the state-of-the-art under both\nin-dataset and cross-dataset settings.", "published": "2024-08-13 09:19:59", "link": "http://arxiv.org/abs/2408.06753v3", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Deep Learning for Speaker Identification: Architectural Insights from\n  AB-1 Corpus Analysis and Performance Evaluation", "abstract": "In the fields of security systems, forensic investigations, and personalized\nservices, the importance of speech as a fundamental human input outweighs\ntext-based interactions. This research delves deeply into the complex field of\nSpeaker Identification (SID), examining its essential components and\nemphasising Mel Spectrogram and Mel Frequency Cepstral Coefficients (MFCC) for\nfeature extraction. Moreover, this study evaluates six slightly distinct model\narchitectures using extensive analysis to evaluate their performance, with\nhyperparameter tuning applied to the best-performing model. This work performs\na linguistic analysis to verify accent and gender accuracy, in addition to bias\nevaluation within the AB-1 Corpus dataset.", "published": "2024-08-13 10:46:50", "link": "http://arxiv.org/abs/2408.06804v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Temporal Variability and Multi-Viewed Self-Supervised Representations to\n  Tackle the ASVspoof5 Deepfake Challenge", "abstract": "ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest\nglobal audio security challenges. It aims to advance the development of\ncountermeasure (CM) to discriminate bonafide and spoofed speech utterances. In\nthis paper, we focus on addressing the problem of open-domain audio deepfake\ndetection, which corresponds directly to the ASVspoof5 Track1 open condition.\nAt first, we comprehensively investigate various CM on ASVspoof5, including\ndata expansion, data augmentation, and self-supervised learning (SSL) features.\nDue to the high-frequency gaps characteristic of the ASVspoof5 dataset, we\nintroduce Frequency Mask, a data augmentation method that masks specific\nfrequency bands to improve CM robustness. Combining various scale of temporal\ninformation with multiple SSL features, our experiments achieved a minDCF of\n0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.", "published": "2024-08-13 14:15:15", "link": "http://arxiv.org/abs/2408.06922v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional\n  Codecs", "abstract": "This paper explores the integration of model-based and data-driven approaches\nwithin the realm of neural speech and audio coding systems. It highlights the\nchallenges posed by the subjective evaluation processes of speech and audio\ncodecs and discusses the limitations of purely data-driven approaches, which\noften require inefficiently large architectures to match the performance of\nmodel-based methods. The study presents hybrid systems as a viable solution,\noffering significant improvements to the performance of conventional codecs\nthrough meticulously chosen design enhancements. Specifically, it introduces a\nneural network-based signal enhancer designed to post-process existing codecs'\noutput, along with the autoencoder-based end-to-end models and LPCNet--hybrid\nsystems that combine linear predictive coding (LPC) with neural networks.\nFurthermore, the paper delves into predictive models operating within custom\nfeature spaces (TF-Codec) or predefined transform domains (MDCTNet) and\nexamines the use of psychoacoustically calibrated loss functions to train\nend-to-end neural audio codecs. Through these investigations, the paper\ndemonstrates the potential of hybrid systems to advance the field of speech and\naudio coding by bridging the gap between traditional model-based approaches and\nmodern data-driven techniques.", "published": "2024-08-13 15:13:21", "link": "http://arxiv.org/abs/2408.06954v2", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Content and Style Aware Audio-Driven Facial Animation", "abstract": "Audio-driven 3D facial animation has several virtual humans applications for\ncontent creation and editing. While several existing methods provide solutions\nfor speech-driven animation, precise control over content (what) and style\n(how) of the final performance is still challenging. We propose a novel\napproach that takes as input an audio, and the corresponding text to extract\ntemporally-aligned content and disentangled style representations, in order to\nprovide controls over 3D facial animation. Our method is trained in two stages,\nthat evolves from audio prominent styles (how it sounds) to visual prominent\nstyles (how it looks). We leverage a high-resource audio dataset in stage I to\nlearn styles that control speech generation in a self-supervised learning\nframework, and then fine-tune this model with low-resource audio/3D mesh pairs\nin stage II to control 3D vertex generation. We employ a non-autoregressive\nseq2seq formulation to model sentence-level dependencies, and better mouth\narticulations. Our method provides flexibility that the style of a reference\naudio and the content of a source audio can be combined to enable audio style\ntransfer. Similarly, the content can be modified, e.g. muting or swapping\nwords, that enables style-preserving content editing.", "published": "2024-08-13 16:12:25", "link": "http://arxiv.org/abs/2408.07005v2", "categories": ["cs.SD", "cs.GR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot\n  Soundscape Mapping", "abstract": "A soundscape is defined by the acoustic environment a person perceives at a\nlocation. In this work, we propose a framework for mapping soundscapes across\nthe Earth. Since soundscapes involve sound distributions that span varying\nspatial scales, we represent locations with multi-scale satellite imagery and\nlearn a joint representation among this imagery, audio, and text. To capture\nthe inherent uncertainty in the soundscape of a location, we design the\nrepresentation space to be probabilistic. We also fuse ubiquitous metadata\n(including geolocation, time, and data source) to enable learning of spatially\nand temporally dynamic representations of soundscapes. We demonstrate the\nutility of our framework by creating large-scale soundscape maps integrating\nboth audio and text with temporal control. To facilitate future research on\nthis task, we also introduce a large-scale dataset, GeoSound, containing over\n$300k$ geotagged audio samples paired with both low- and high-resolution\nsatellite imagery. We demonstrate that our method outperforms the existing\nstate-of-the-art on both GeoSound and the existing SoundingEarth dataset. Our\ndataset and code is available at https://github.com/mvrl/PSM.", "published": "2024-08-13 17:37:40", "link": "http://arxiv.org/abs/2408.07050v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Theory-Based Explainable Deep Learning Architecture for Music Emotion", "abstract": "This paper paper develops a theory-based, explainable deep learning\nconvolutional neural network (CNN) classifier to predict the time-varying\nemotional response to music. We design novel CNN filters that leverage the\nfrequency harmonics structure from acoustic physics known to impact the\nperception of musical features. Our theory-based model is more parsimonious,\nbut provides comparable predictive performance to atheoretical deep learning\nmodels, while performing better than models using handcrafted features. Our\nmodel can be complemented with handcrafted features, but the performance\nimprovement is marginal. Importantly, the harmonics-based structure placed on\nthe CNN filters provides better explainability for how the model predicts\nemotional response (valence and arousal), because emotion is closely related to\nconsonance--a perceptual feature defined by the alignment of harmonics.\nFinally, we illustrate the utility of our model with an application involving\ndigital advertising. Motivated by YouTube mid-roll ads, we conduct a lab\nexperiment in which we exogenously insert ads at different times within videos.\nWe find that ads placed in emotionally similar contexts increase ad engagement\n(lower skip rates, higher brand recall rates). Ad insertion based on emotional\nsimilarity metrics predicted by our theory-based, explainable model produces\ncomparable or better engagement relative to atheoretical models.", "published": "2024-08-13 16:01:27", "link": "http://arxiv.org/abs/2408.07113v1", "categories": ["cs.SD", "cs.AI", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Play Me Something Icy: Practical Challenges, Explainability and the\n  Semantic Gap in Generative AI Music", "abstract": "This pictorial aims to critically consider the nature of text-to-audio and\ntext-to-music generative tools in the context of explainable AI. As a group of\nexperimental musicians and researchers, we are enthusiastic about the creative\npotential of these tools and have sought to understand and evaluate them from\nperspectives of prompt creation, control, usability, understandability,\nexplainability of the AI process, and overall aesthetic effectiveness of the\nresults. One of the challenges we have identified that is not explicitly\naddressed by these tools is the inherent semantic gap in using text-based tools\nto describe something as abstract as music. Other gaps include explainability\nvs. useability, and user control and input vs. the human creative process. The\naim of this pictorial is to raise questions for discussion and make a few\ngeneral suggestions on the kinds of improvements we would like to see in\ngenerative AI music tools.", "published": "2024-08-13 22:42:05", "link": "http://arxiv.org/abs/2408.07224v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
