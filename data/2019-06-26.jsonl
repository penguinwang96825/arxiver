{"title": "Leveraging Text Repetitions and Denoising Autoencoders in OCR\n  Post-correction", "abstract": "A common approach for improving OCR quality is a post-processing step based\non models correcting misdetected characters and tokens. These models are\ntypically trained on aligned pairs of OCR read text and their manually\ncorrected counterparts. In this paper we show that the requirement of manually\ncorrected training data can be alleviated by estimating the OCR errors from\nrepeating text spans found in large OCR read text corpora and generating\nsynthetic training examples following this error distribution. We use the\ngenerated data for training a character-level neural seq2seq model and evaluate\nthe performance of the suggested model on a manually corrected corpus of\nFinnish newspapers mostly from the 19th century. The results show that a clear\nimprovement over the underlying OCR system as well as previously suggested\nmodels utilizing uniformly generated noise can be achieved.", "published": "2019-06-26 08:28:51", "link": "http://arxiv.org/abs/1906.10907v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sharing Attention Weights for Fast Transformer", "abstract": "Recently, the Transformer machine translation system has shown strong results\nby stacking attention layers on both the source and target-language sides. But\nthe inference of this model is slow due to the heavy use of dot-product\nattention in auto-regressive decoding. In this paper we speed up Transformer\nvia a fast and lightweight attention model. More specifically, we share\nattention weights in adjacent layers and enable the efficient re-use of hidden\nstates in a vertical manner. Moreover, the sharing policy can be jointly\nlearned with the MT model. We test our approach on ten WMT and NIST OpenMT\ntasks. Experimental results show that it yields an average of 1.3X speed-up\n(with almost no decrease in BLEU) on top of a state-of-the-art implementation\nthat has already adopted a cache for fast inference. Also, our approach obtains\na 1.8X speed-up when it works with the \\textsc{Aan} model. This is even 16\ntimes faster than the baseline with no use of the attention cache.", "published": "2019-06-26 12:27:05", "link": "http://arxiv.org/abs/1906.11024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing PIO Element Detection in Medical Text Using Contextualized\n  Embedding", "abstract": "In this paper, we investigate a new approach to Population, Intervention and\nOutcome (PIO) element detection, a common task in Evidence Based Medicine\n(EBM). The purpose of this study is two-fold: to build a training dataset for\nPIO element detection with minimum redundancy and ambiguity and to investigate\npossible options in utilizing state of the art embedding methods for the task\nof PIO element detection. For the former purpose, we build a new and improved\ndataset by investigating the shortcomings of previously released datasets. For\nthe latter purpose, we leverage the state of the art text embedding,\nBidirectional Encoder Representations from Transformers (BERT), and build a\nmulti-label classifier. We show that choosing a domain specific pre-trained\nembedding further optimizes the performance of the classifier. Furthermore, we\nshow that the model could be enhanced by using ensemble methods and boosting\ntechniques provided that features are adequately chosen.", "published": "2019-06-26 13:27:39", "link": "http://arxiv.org/abs/1906.11085v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eliciting Knowledge from Experts:Automatic Transcript Parsing for\n  Cognitive Task Analysis", "abstract": "Cognitive task analysis (CTA) is a type of analysis in applied psychology\naimed at eliciting and representing the knowledge and thought processes of\ndomain experts. In CTA, often heavy human labor is involved to parse the\ninterview transcript into structured knowledge (e.g., flowchart for different\nactions). To reduce human efforts and scale the process, automated CTA\ntranscript parsing is desirable. However, this task has unique challenges as\n(1) it requires the understanding of long-range context information in\nconversational text; and (2) the amount of labeled data is limited and\nindirect---i.e., context-aware, noisy, and low-resource. In this paper, we\npropose a weakly-supervised information extraction framework for automated CTA\ntranscript parsing. We partition the parsing process into a sequence labeling\ntask and a text span-pair relation extraction task, with distant supervision\nfrom human-curated protocol files. To model long-range context information for\nextracting sentence relations, neighbor sentences are involved as a part of\ninput. Different types of models for capturing context dependency are then\napplied. We manually annotate real-world CTA transcripts to facilitate the\nevaluation of the parsing tasks", "published": "2019-06-26 23:34:22", "link": "http://arxiv.org/abs/1906.11384v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Information-Lossless Biological Knowledge Graphs from\n  Conditional Statements", "abstract": "Conditions are essential in the statements of biological literature. Without\nthe conditions (e.g., environment, equipment) that were precisely specified,\nthe facts (e.g., observations) in the statements may no longer be valid. One\nbiological statement has one or multiple fact(s) and/or condition(s). Their\nsubject and object can be either a concept or a concept's attribute. Existing\ninformation extraction methods do not consider the role of condition in the\nbiological statement nor the role of attribute in the subject/object. In this\nwork, we design a new tag schema and propose a deep sequence tagging framework\nto structure conditional statement into fact and condition tuples from\nbiological text. Experiments demonstrate that our method yields a\ninformation-lossless structure of the literature.", "published": "2019-06-26 14:50:39", "link": "http://arxiv.org/abs/1907.00720v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reconfigurable Interaction for MAS Modelling", "abstract": "We propose a formalism to model and reason about multi-agent systems. We\nallow agents to interact and communicate in different modes so that they can\npursue joint tasks; agents may dynamically synchronize, exchange data, adapt\ntheir behaviour, and reconfigure their communication interfaces. The formalism\ndefines a local behaviour based on shared variables and a global one based on\nmessage passing. We extend LTL to be able to reason explicitly about the\nintentions of the different agents and their interaction protocols. We also\nstudy the complexity of satisfiability and model-checking of this extension.", "published": "2019-06-26 00:31:46", "link": "http://arxiv.org/abs/1906.10793v2", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "On the Coherence of Fake News Articles", "abstract": "The generation and spread of fake news within new and online media sources is\nemerging as a phenomenon of high societal significance. Combating them using\ndata-driven analytics has been attracting much recent scholarly interest. In\nthis study, we analyze the textual coherence of fake news articles vis-a-vis\nlegitimate ones. We develop three computational formulations of textual\ncoherence drawing upon the state-of-the-art methods in natural language\nprocessing and data science. Two real-world datasets from widely different\ndomains which have fake/legitimate article labellings are then analyzed with\nrespect to textual coherence. We observe apparent differences in textual\ncoherence across fake and legitimate news articles, with fake news articles\nconsistently scoring lower on coherence as compared to legitimate news ones.\nWhile the relative coherence shortfall of fake news articles as compared to\nlegitimate ones form the main observation from our study, we analyze several\naspects of the differences and outline potential avenues of further inquiry.", "published": "2019-06-26 14:33:18", "link": "http://arxiv.org/abs/1906.11126v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Canonicalizing Knowledge Base Literals", "abstract": "Ontology-based knowledge bases (KBs) like DBpedia are very valuable\nresources, but their usefulness and usability is limited by various quality\nissues. One such issue is the use of string literals instead of semantically\ntyped entities. In this paper we study the automated canonicalization of such\nliterals, i.e., replacing the literal with an existing entity from the KB or\nwith a new entity that is typed using classes from the KB. We propose a\nframework that combines both reasoning and machine learning in order to predict\nthe relevant entities and types, and we evaluate this framework against\nstate-of-the-art baselines for both semantic typing and entity matching.", "published": "2019-06-26 15:53:59", "link": "http://arxiv.org/abs/1906.11180v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Generative Model for Punctuation in Dependency Trees", "abstract": "Treebanks traditionally treat punctuation marks as ordinary words, but\nlinguists have suggested that a tree's \"true\" punctuation marks are not\nobserved (Nunberg, 1990). These latent \"underlying\" marks serve to delimit or\nseparate constituents in the syntax tree. When the tree's yield is rendered as\na written sentence, a string rewriting mechanism transduces the underlying\nmarks into \"surface\" marks, which are part of the observed (surface) string but\nshould not be regarded as part of the tree. We formalize this idea in a\ngenerative model of punctuation that admits efficient dynamic programming. We\ntrain it without observing the underlying marks, by locally maximizing the\nincomplete data likelihood (similarly to EM). When we use the trained model to\nreconstruct the tree's underlying punctuation, the results appear plausible\nacross 5 languages, and in particular, are consistent with Nunberg's analysis\nof English. We show that our generative model can be used to beat baselines on\npunctuation restoration. Also, our reconstruction of a sentence's underlying\npunctuation lets us appropriately render the surface punctuation (via our\ntrained underlying-to-surface mechanism) when we syntactically transform the\nsentence.", "published": "2019-06-26 19:03:17", "link": "http://arxiv.org/abs/1906.11298v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the Role of Prior Beliefs for Argument Persuasion", "abstract": "Public debate forums provide a common platform for exchanging opinions on a\ntopic of interest. While recent studies in natural language processing (NLP)\nhave provided empirical evidence that the language of the debaters and their\npatterns of interaction play a key role in changing the mind of a reader,\nresearch in psychology has shown that prior beliefs can affect our\ninterpretation of an argument and could therefore constitute a competing\nalternative explanation for resistance to changing one's stance. To study the\nactual effect of language use vs. prior beliefs on persuasion, we provide a new\ndataset and propose a controlled setting that takes into consideration two\nreader level factors: political and religious ideology. We find that prior\nbeliefs affected by these reader level factors play a more important role than\nlanguage use effects and argue that it is important to account for them in NLP\nstudies of persuasion.", "published": "2019-06-26 19:15:41", "link": "http://arxiv.org/abs/1906.11301v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Corpus for Modeling User and Language Effects in Argumentation on\n  Online Debating", "abstract": "Existing argumentation datasets have succeeded in allowing researchers to\ndevelop computational methods for analyzing the content, structure and\nlinguistic features of argumentative text. They have been much less successful\nin fostering studies of the effect of \"user\" traits -- characteristics and\nbeliefs of the participants -- on the debate/argument outcome as this type of\nuser information is generally not available. This paper presents a dataset of\n78, 376 debates generated over a 10-year period along with surprisingly\ncomprehensive participant profiles. We also complete an example study using the\ndataset to analyze the effect of selected user traits on the debate outcome in\ncomparison to the linguistic features typically employed in studies of this\nkind.", "published": "2019-06-26 19:38:02", "link": "http://arxiv.org/abs/1906.11310v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Determining Relative Argument Specificity and Stance for Complex\n  Argumentative Structures", "abstract": "Systems for automatic argument generation and debate require the ability to\n(1) determine the stance of any claims employed in the argument and (2) assess\nthe specificity of each claim relative to the argument context. Existing work\non understanding claim specificity and stance, however, has been limited to the\nstudy of argumentative structures that are relatively shallow, most often\nconsisting of a single claim that directly supports or opposes the argument\nthesis. In this paper, we tackle these tasks in the context of complex\narguments on a diverse set of topics. In particular, our dataset consists of\nmanually curated argument trees for 741 controversial topics covering 95,312\nunique claims; lines of argument are generally of depth 2 to 6. We find that as\nthe distance between a pair of claims increases along the argument path,\ndetermining the relative specificity of a pair of claims becomes easier and\ndetermining their relative stance becomes harder.", "published": "2019-06-26 19:45:42", "link": "http://arxiv.org/abs/1906.11313v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generalization to Novel Objects using Prior Relational Knowledge", "abstract": "To solve tasks in new environments involving objects unseen during training,\nagents must reason over prior information about those objects and their\nrelations. We introduce the Prior Knowledge Graph network, an architecture for\ncombining prior information, structured as a knowledge graph, with a symbolic\nparsing of the visual scene, and demonstrate that this approach is able to\napply learned relations to novel objects whereas the baseline algorithms fail.\nAblation experiments show that the agents ground the knowledge graph relations\nto semantically-relevant behaviors. In both a Sokoban game and the more complex\nPacman environment, our network is also more sample efficient than the\nbaselines, reaching the same performance in 5-10x fewer episodes. Once the\nagents are trained with our approach, we can manipulate agent behavior by\nmodifying the knowledge graph in semantically meaningful ways. These results\nsuggest that our network provides a framework for agents to reason over\nstructured knowledge graphs while still leveraging gradient based learning\napproaches.", "published": "2019-06-26 19:48:25", "link": "http://arxiv.org/abs/1906.11315v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Analyzing Verbal and Nonverbal Features for Predicting Group Performance", "abstract": "This work analyzes the efficacy of verbal and nonverbal features of group\nconversation for the task of automatic prediction of group task performance. We\ndescribe a new publicly available survival task dataset that was collected and\nannotated to facilitate this prediction task. In these experiments, the new\ndataset is merged with an existing survival task dataset, allowing us to\ncompare feature sets on a much larger amount of data than has been used in\nrecent related work. This work is also distinct from related research on social\nsignal processing (SSP) in that we compare verbal and nonverbal features,\nwhereas SSP is almost exclusively concerned with nonverbal aspects of social\ninteraction. A key finding is that nonverbal features from the speech signal\nare extremely effective for this task, even on their own. However, the most\neffective individual features are verbal features, and we highlight the most\nimportant ones.", "published": "2019-06-26 17:07:03", "link": "http://arxiv.org/abs/1907.01369v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Hierarchical Optimal Transport for Document Representation", "abstract": "The ability to measure similarity between documents enables intelligent\nsummarization and analysis of large corpora. Past distances between documents\nsuffer from either an inability to incorporate semantic similarities between\nwords or from scalability issues. As an alternative, we introduce hierarchical\noptimal transport as a meta-distance between documents, where documents are\nmodeled as distributions over topics, which themselves are modeled as\ndistributions over words. We then solve an optimal transport problem on the\nsmaller topic space to compute a similarity score. We give conditions on the\ntopics under which this construction defines a distance, and we relate it to\nthe word mover's distance. We evaluate our technique for k-NN classification\nand show better interpretability and scalability with comparable performance to\ncurrent methods at a fraction of the cost.", "published": "2019-06-26 03:26:23", "link": "http://arxiv.org/abs/1906.10827v2", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Essence Knowledge Distillation for Speech Recognition", "abstract": "It is well known that a speech recognition system that combines multiple\nacoustic models trained on the same data significantly outperforms a\nsingle-model system. Unfortunately, real time speech recognition using a whole\nensemble of models is too computationally expensive. In this paper, we propose\nto distill the knowledge of essence in an ensemble of models (i.e. the teacher\nmodel) to a single model (i.e. the student model) that needs much less\ncomputation to deploy. Previously, all the soften outputs of the teacher model\nare used to optimize the student model. We argue that not all the outputs of\nthe ensemble are necessary to be distilled. Some of the outputs may even\ncontain noisy information that is useless or even harmful to the training of\nthe student model. In addition, we propose to train the student model with a\nmultitask learning approach by utilizing both the soften outputs of the teacher\nmodel and the correct hard labels. The proposed method achieves some surprising\nresults on the Switchboard data set. When the student model is trained together\nwith the correct labels and the essence knowledge from the teacher model, it\nnot only significantly outperforms another single model with the same\narchitecture that is trained only with the correct labels, but also\nconsistently outperforms the teacher model that is used to generate the soft\nlabels.", "published": "2019-06-26 03:58:29", "link": "http://arxiv.org/abs/1906.10834v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Auxiliary Interference Speaker Loss for Target-Speaker Speech\n  Recognition", "abstract": "In this paper, we propose a novel auxiliary loss function for target-speaker\nautomatic speech recognition (ASR). Our method automatically extracts and\ntranscribes target speaker's utterances from a monaural mixture of multiple\nspeakers speech given a short sample of the target speaker. The proposed\nauxiliary loss function attempts to additionally maximize interference speaker\nASR accuracy during training. This will regularize the network to achieve a\nbetter representation for speaker separation, thus achieving better accuracy on\nthe target-speaker ASR. We evaluated our proposed method using\ntwo-speaker-mixed speech in various signal-to-interference-ratio conditions. We\nfirst built a strong target-speaker ASR baseline based on the state-of-the-art\nlattice-free maximum mutual information. This baseline achieved a word error\nrate (WER) of 18.06% on the test set while a normal ASR trained with clean data\nproduced a completely corrupted result (WER of 84.71%). Then, our proposed loss\nfurther reduced the WER by 6.6% relative to this strong baseline, achieving a\nWER of 16.87%. In addition to the accuracy improvement, we also showed that the\nauxiliary output branch for the proposed loss can even be used for a secondary\nASR for interference speakers' speech.", "published": "2019-06-26 07:09:57", "link": "http://arxiv.org/abs/1906.10876v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Creating A Neural Pedagogical Agent by Jointly Learning to Review and\n  Assess", "abstract": "Machine learning plays an increasing role in intelligent tutoring systems as\nboth the amount of data available and specialization among students grow.\nNowadays, these systems are frequently deployed on mobile applications. Users\non such mobile education platforms are dynamic, frequently being added,\naccessing the application with varying levels of focus, and changing while\nusing the service. The education material itself, on the other hand, is often\nstatic and is an exhaustible resource whose use in tasks such as problem\nrecommendation must be optimized. The ability to update user models with\nrespect to educational material in real-time is thus essential; however,\nexisting approaches require time-consuming re-training of user features\nwhenever new data is added. In this paper, we introduce a neural pedagogical\nagent for real-time user modeling in the task of predicting user response\ncorrectness, a central task for mobile education applications. Our model,\ninspired by work in natural language processing on sequence modeling and\nmachine translation, updates user features in real-time via bidirectional\nrecurrent neural networks with an attention mechanism over embedded\nquestion-response pairs. We experiment on the mobile education application\nSantaTOEIC, which has 559k users, 66M response data points as well as a set of\n10k study problems each expert-annotated with topic tags and gathered since\n2016. Our model outperforms existing approaches over several metrics in\npredicting user response correctness, notably out-performing other methods on\nnew users without large question-response histories. Additionally, our\nattention mechanism and annotated tag set allow us to create an interpretable\neducation platform, with a smart review system that addresses the\naforementioned issue of varied user attention and problem exhaustion.", "published": "2019-06-26 08:37:44", "link": "http://arxiv.org/abs/1906.10910v2", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Interpretable Question Answering on Knowledge Bases and Text", "abstract": "Interpretability of machine learning (ML) models becomes more relevant with\ntheir increasing adoption. In this work, we address the interpretability of ML\nbased question answering (QA) models on a combination of knowledge bases (KB)\nand text documents. We adapt post hoc explanation methods such as LIME and\ninput perturbation (IP) and compare them with the self-explanatory attention\nmechanism of the model. For this purpose, we propose an automatic evaluation\nparadigm for explanation methods in the context of QA. We also conduct a study\nwith human annotators to evaluate whether explanations help them identify\nbetter QA models. Our results suggest that IP provides better explanations than\nLIME or attention, according to both automatic and human evaluation. We obtain\nthe same ranking of methods in both experiments, which supports the validity of\nour automatic evaluation paradigm.", "published": "2019-06-26 09:10:33", "link": "http://arxiv.org/abs/1906.10924v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Deep Decoder Structure Based on WordEmbedding Regression for An\n  Encoder-Decoder Based Model for Image Captioning", "abstract": "Generating textual descriptions for images has been an attractive problem for\nthe computer vision and natural language processing researchers in recent\nyears. Dozens of models based on deep learning have been proposed to solve this\nproblem. The existing approaches are based on neural encoder-decoder structures\nequipped with the attention mechanism. These methods strive to train decoders\nto minimize the log likelihood of the next word in a sentence given the\nprevious ones, which results in the sparsity of the output space. In this work,\nwe propose a new approach to train decoders to regress the word embedding of\nthe next word with respect to the previous ones instead of minimizing the log\nlikelihood. The proposed method is able to learn and extract long-term\ninformation and can generate longer fine-grained captions without introducing\nany external memory cell. Furthermore, decoders trained by the proposed\ntechnique can take the importance of the generated words into consideration\nwhile generating captions. In addition, a novel semantic attention mechanism is\nproposed that guides attention points through the image, taking the meaning of\nthe previously generated word into account. We evaluate the proposed approach\nwith the MS-COCO dataset. The proposed model outperformed the state of the art\nmodels especially in generating longer captions. It achieved a CIDEr score\nequal to 125.0 and a BLEU-4 score equal to 50.5, while the best scores of the\nstate of the art models are 117.1 and 48.0, respectively.", "published": "2019-06-26 13:51:59", "link": "http://arxiv.org/abs/1906.12188v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Program Synthesis and Semantic Parsing with Learned Code Idioms", "abstract": "Program synthesis of general-purpose source code from natural language\nspecifications is challenging due to the need to reason about high-level\npatterns in the target program and low-level implementation details at the same\ntime. In this work, we present PATOIS, a system that allows a neural program\nsynthesizer to explicitly interleave high-level and low-level reasoning at\nevery generation step. It accomplishes this by automatically mining common code\nidioms from a given corpus, incorporating them into the underlying language for\nneural synthesis, and training a tree-based neural synthesizer to use these\nidioms during code generation. We evaluate PATOIS on two complex semantic\nparsing datasets and show that using learned code idioms improves the\nsynthesizer's accuracy.", "published": "2019-06-26 02:28:10", "link": "http://arxiv.org/abs/1906.10816v4", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.PL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "End-to-End Emotional Speech Synthesis Using Style Tokens and\n  Semi-Supervised Training", "abstract": "This paper proposes an end-to-end emotional speech synthesis (ESS) method\nwhich adopts global style tokens (GSTs) for semi-supervised training. This\nmodel is built based on the GST-Tacotron framework. The style tokens are\ndefined to present emotion categories. A cross entropy loss function between\ntoken weights and emotion labels is designed to obtain the interpretability of\nstyle tokens utilizing the small portion of training data with emotion labels.\nEmotion recognition experiments confirm that this method can achieve one-to-one\ncorrespondence between style tokens and emotion categories effectively.\nObjective and subjective evaluation results show that our model outperforms the\nconventional Tacotron model for ESS when only 5\\% of training data has emotion\nlabels. Its subjective performance is close to the Tacotron model trained using\nall emotion labels.", "published": "2019-06-26 06:12:59", "link": "http://arxiv.org/abs/1906.10859v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning a Joint Embedding Space of Monophonic and Mixed Music Signals\n  for Singing Voice", "abstract": "Previous approaches in singer identification have used one of monophonic\nvocal tracks or mixed tracks containing multiple instruments, leaving a\nsemantic gap between these two domains of audio. In this paper, we present a\nsystem to learn a joint embedding space of monophonic and mixed tracks for\nsinging voice. We use a metric learning method, which ensures that tracks from\nboth domains of the same singer are mapped closer to each other than those of\ndifferent singers. We train the system on a large synthetic dataset generated\nby music mashup to reflect real-world music recordings. Our approach opens up\nnew possibilities for cross-domain tasks, e.g., given a monophonic track of a\nsinger as a query, retrieving mixed tracks sung by the same singer from the\ndatabase. Also, it requires no additional vocal enhancement steps such as\nsource separation. We show the effectiveness of our system for singer\nidentification and query-by-singer in both the same-domain and cross-domain\ntasks.", "published": "2019-06-26 14:55:16", "link": "http://arxiv.org/abs/1906.11139v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On the performance of residual block design alternatives in\n  convolutional neural networks for end-to-end audio classification", "abstract": "Residual learning is a recently proposed learning framework to facilitate the\ntraining of very deep neural networks. Residual blocks or units are made of a\nset of stacked layers, where the inputs are added back to their outputs with\nthe aim of creating identity mappings. In practice, such identity mappings are\naccomplished by means of the so-called skip or residual connections. However,\nmultiple implementation alternatives arise with respect to where such skip\nconnections are applied within the set of stacked layers that make up a\nresidual block. While ResNet architectures for image classification using\nconvolutional neural networks (CNNs) have been widely discussed in the\nliterature, few works have adopted ResNet architectures so far for 1D audio\nclassification tasks. Thus, the suitability of different residual block designs\nfor raw audio classification is partly unknown. The purpose of this paper is to\nanalyze and discuss the performance of several residual block implementations\nwithin a state-of-the-art CNN-based architecture for end-to-end audio\nclassification using raw audio waveforms. For comparison purposes, we analyze\nas well the performance of the residual blocks under a similar 2D architecture\nusing a conventional time-frequency audio represen-tation as input. The results\nshow that the achieved accuracy is considerably dependent, not only on the\nspecific residual block implementation, but also on the selected input\nnormalization.", "published": "2019-06-26 07:47:54", "link": "http://arxiv.org/abs/1906.10891v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "RUSLAN: Russian Spoken Language Corpus for Speech Synthesis", "abstract": "We present RUSLAN -- a new open Russian spoken language corpus for the\ntext-to-speech task. RUSLAN contains 22200 audio samples with text annotations\n-- more than 31 hours of high-quality speech of one person -- being the largest\nannotated Russian corpus in terms of speech duration for a single speaker. We\ntrained an end-to-end neural network for the text-to-speech task on our corpus\nand evaluated the quality of the synthesized speech using Mean Opinion Score\ntest. Synthesized speech achieves 4.05 score for naturalness and 3.78 score for\nintelligibility on a 5-point MOS scale.", "published": "2019-06-26 11:06:05", "link": "http://arxiv.org/abs/1906.11645v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML", "I.2.7"], "primary_category": "eess.AS"}
{"title": "Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music\n  Retrieval", "abstract": "Connecting large libraries of digitized audio recordings to their\ncorresponding sheet music images has long been a motivation for researchers to\ndevelop new cross-modal retrieval systems. In recent years, retrieval systems\nbased on embedding space learning with deep neural networks got a step closer\nto fulfilling this vision. However, global and local tempo deviations in the\nmusic recordings still require careful tuning of the amount of temporal context\ngiven to the system. In this paper, we address this problem by introducing an\nadditional soft-attention mechanism on the audio input. Quantitative and\nqualitative results on synthesized piano data indicate that this attention\nincreases the robustness of the retrieval system by focusing on different parts\nof the input representation based on the tempo of the audio. Encouraged by\nthese results, we argue for the potential of attention models as a very general\ntool for many MIR tasks.", "published": "2019-06-26 11:52:49", "link": "http://arxiv.org/abs/1906.10996v1", "categories": ["cs.IR", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
