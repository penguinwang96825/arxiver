{"title": "The UN Parallel Corpus Annotated for Translation Direction", "abstract": "This work distinguishes between translated and original text in the UN\nprotocol corpus. By modeling the problem as classification problem, we can\nachieve up to 95% classification accuracy. We begin by deriving a parallel\ncorpus for different language-pairs annotated for translation direction, and\nthen classify the data by using various feature extraction methods. We compare\nthe different methods as well as the ability to distinguish between translated\nand original texts in the different languages. The annotated corpus is publicly\navailable.", "published": "2018-05-20 03:42:50", "link": "http://arxiv.org/abs/1805.07697v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating High-Quality Surface Realizations Using Data Augmentation and\n  Factored Sequence Models", "abstract": "This work presents a new state of the art in reconstruction of surface\nrealizations from obfuscated text. We identify the lack of sufficient training\ndata as the major obstacle to training high-performing models, and solve this\nissue by generating large amounts of synthetic training data. We also propose\npreprocessing techniques which make the structure contained in the input\nfeatures more accessible to sequence models. Our models were ranked first on\nall evaluation metrics in the English portion of the 2018 Surface Realization\nshared task.", "published": "2018-05-20 08:40:06", "link": "http://arxiv.org/abs/1805.07731v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Text Classification Using Sequence-to-convolution Neural\n  Networks", "abstract": "We propose a new deep neural network model and its training scheme for text\nclassification. Our model Sequence-to-convolution Neural Networks(Seq2CNN)\nconsists of two blocks: Sequential Block that summarizes input texts and\nConvolution Block that receives summary of input and classifies it to a label.\nSeq2CNN is trained end-to-end to classify various-length texts without\npreprocessing inputs into fixed length. We also present Gradual Weight\nShift(GWS) method that stabilizes training. GWS is applied to our model's loss\nfunction. We compared our model with word-based TextCNN trained with different\ndata preprocessing methods. We obtained significant improvement in\nclassification accuracy over word-based TextCNN without any ensemble or data\naugmentation.", "published": "2018-05-20 09:34:20", "link": "http://arxiv.org/abs/1805.07745v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hierarchical Structured Self-Attentive Model for Extractive Document\n  Summarization (HSSAS)", "abstract": "The recent advance in neural network architecture and training algorithms\nhave shown the effectiveness of representation learning. The neural\nnetwork-based models generate better representation than the traditional ones.\nThey have the ability to automatically learn the distributed representation for\nsentences and documents. To this end, we proposed a novel model that addresses\nseveral issues that are not adequately modeled by the previously proposed\nmodels, such as the memory problem and incorporating the knowledge of document\nstructure. Our model uses a hierarchical structured self-attention mechanism to\ncreate the sentence and document embeddings. This architecture mirrors the\nhierarchical structure of the document and in turn enables us to obtain better\nfeature representation. The attention mechanism provides extra source of\ninformation to guide the summary extraction. The new model treated the\nsummarization task as a classification problem in which the model computes the\nrespective probabilities of sentence-summary membership. The model predictions\nare broken up by several features such as information content, salience,\nnovelty and positional representation. The proposed model was evaluated on two\nwell-known datasets, the CNN / Daily Mail, and DUC 2002. The experimental\nresults show that our model outperforms the current extractive state-of-the-art\nby a considerable margin.", "published": "2018-05-20 17:16:49", "link": "http://arxiv.org/abs/1805.07799v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge-enriched Two-layered Attention Network for Sentiment Analysis", "abstract": "We propose a novel two-layered attention network based on Bidirectional Long\nShort-Term Memory for sentiment analysis. The novel two-layered attention\nnetwork takes advantage of the external knowledge bases to improve the\nsentiment prediction. It uses the Knowledge Graph Embedding generated using the\nWordNet. We build our model by combining the two-layered attention network with\nthe supervised model based on Support Vector Regression using a Multilayer\nPerceptron network for sentiment analysis. We evaluate our model on the\nbenchmark dataset of SemEval 2017 Task 5. Experimental results show that the\nproposed model surpasses the top system of SemEval 2017 Task 5. The model\nperforms significantly better by improving the state-of-the-art system at\nSemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.", "published": "2018-05-20 20:00:27", "link": "http://arxiv.org/abs/1805.07819v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Validating WordNet Meronymy Relations using Adimen-SUMO", "abstract": "In this paper, we report on the practical application of a novel approach for\nvalidating the knowledge of WordNet using Adimen-SUMO. In particular, this\npaper focuses on cross-checking the WordNet meronymy relations against the\nknowledge encoded in Adimen-SUMO. Our validation approach tests a large set of\ncompetency questions (CQs), which are derived (semi)-automatically from the\nknowledge encoded in WordNet, SUMO and their mapping, by applying efficient\nfirst-order logic automated theorem provers. Unfortunately, despite of being\ncreated manually, these knowledge resources are not free of errors and\ndiscrepancies. In consequence, some of the resulting CQs are not plausible\naccording to the knowledge included in Adimen-SUMO. Thus, first we focus on\n(semi)-automatically improving the alignment between these knowledge resources,\nand second, we perform a minimal set of corrections in the ontology. Our aim is\nto minimize the manual effort required for an extensive validation process. We\nreport on the strategies followed, the changes made, the effort needed and its\nimpact when validating the WordNet meronymy relations using improved versions\nof the mapping and the ontology. Based on the new results, we discuss the\nimplications of the appropriate corrections and the need of future\nenhancements.", "published": "2018-05-20 20:50:17", "link": "http://arxiv.org/abs/1805.07824v1", "categories": ["cs.CL", "68T30", "I.2.4"], "primary_category": "cs.CL"}
{"title": "Fighting Offensive Language on Social Media with Unsupervised Text Style\n  Transfer", "abstract": "We introduce a new approach to tackle the problem of offensive language in\nonline social media. Our approach uses unsupervised text style transfer to\ntranslate offensive sentences into non-offensive ones. We propose a new method\nfor training encoder-decoders using non-parallel data that combines a\ncollaborative classifier, attention and the cycle consistency loss.\nExperimental results on data from Twitter and Reddit show that our method\noutperforms a state-of-the-art text style transfer system in two out of three\nquantitative metrics and produces reliable non-offensive transferred sentences.", "published": "2018-05-20 00:57:43", "link": "http://arxiv.org/abs/1805.07685v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Balancing Shared Autonomy with Human-Robot Communication", "abstract": "Robotic agents that share autonomy with a human should leverage human domain\nknowledge and account for their preferences when completing a task. This extra\nknowledge can dramatically improve plan efficiency and user-satisfaction, but\nthese gains are lost if communicating with a robot is taxing and unnatural. In\nthis paper, we show how viewing humanrobot language through the lens of shared\nautonomy explains the efficiency versus cognitive load trade-offs humans make\nwhen deciding how cooperative and explicit to make their instructions.", "published": "2018-05-20 07:19:10", "link": "http://arxiv.org/abs/1805.07719v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Learning compositionally through attentive guidance", "abstract": "While neural network models have been successfully applied to domains that\nrequire substantial generalisation skills, recent studies have implied that\nthey struggle when solving the task they are trained on requires inferring its\nunderlying compositional structure. In this paper, we introduce Attentive\nGuidance, a mechanism to direct a sequence to sequence model equipped with\nattention to find more compositional solutions. We test it on two tasks,\ndevised precisely to assess the compositional capabilities of neural models,\nand we show that vanilla sequence to sequence models with attention overfit the\ntraining distribution, while the guided versions come up with compositional\nsolutions that fit the training and testing distributions almost equally well.\nMoreover, the learned solutions generalise even in cases where the training and\ntesting distributions strongly diverge. In this way, we demonstrate that\nsequence to sequence models are capable of finding compositional solutions\nwithout requiring extra components. These results helps to disentangle the\ncauses for the lack of systematic compositionality in neural networks, which\ncan in turn fuel future work.", "published": "2018-05-20 10:33:00", "link": "http://arxiv.org/abs/1805.09657v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Targeted Adversarial Examples for Black Box Audio Systems", "abstract": "The application of deep recurrent networks to audio transcription has led to\nimpressive gains in automatic speech recognition (ASR) systems. Many have\ndemonstrated that small adversarial perturbations can fool deep neural networks\ninto incorrectly predicting a specified target with high confidence. Current\nwork on fooling ASR systems have focused on white-box attacks, in which the\nmodel architecture and parameters are known. In this paper, we adopt a\nblack-box approach to adversarial generation, combining the approaches of both\ngenetic algorithms and gradient estimation to solve the task. We achieve a\n89.25% targeted attack similarity after 3000 generations while maintaining\n94.6% audio file similarity.", "published": "2018-05-20 20:04:09", "link": "http://arxiv.org/abs/1805.07820v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
