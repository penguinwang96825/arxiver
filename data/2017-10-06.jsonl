{"title": "A Semantic Relevance Based Neural Network for Text Summarization and\n  Text Simplification", "abstract": "Text summarization and text simplification are two major ways to simplify the\ntext for poor readers, including children, non-native speakers, and the\nfunctionally illiterate. Text summarization is to produce a brief summary of\nthe main ideas of the text, while text simplification aims to reduce the\nlinguistic complexity of the text and retain the original meaning. Recently,\nmost approaches for text summarization and text simplification are based on the\nsequence-to-sequence model, which achieves much success in many text generation\ntasks. However, although the generated simplified texts are similar to source\ntexts literally, they have low semantic relevance. In this work, our goal is to\nimprove semantic relevance between source texts and simplified texts for text\nsummarization and text simplification. We introduce a Semantic Relevance Based\nneural model to encourage high semantic similarity between texts and summaries.\nIn our model, the source text is represented by a gated attention encoder,\nwhile the summary representation is produced by a decoder. Besides, the\nsimilarity score between the representations is maximized during training. Our\nexperiments show that the proposed model outperforms the state-of-the-art\nsystems on two benchmark corpus.", "published": "2017-10-06 09:06:33", "link": "http://arxiv.org/abs/1710.02318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Czech Text Document Corpus v 2.0", "abstract": "This paper introduces \"Czech Text Document Corpus v 2.0\", a collection of\ntext documents for automatic document classification in Czech language. It is\ncomposed of the text documents provided by the Czech News Agency and is freely\navailable for research purposes at http://ctdc.kiv.zcu.cz/. This corpus was\ncreated in order to facilitate a straightforward comparison of the document\nclassification approaches on Czech data. It is particularly dedicated to\nevaluation of multi-label document classification approaches, because one\ndocument is usually labelled with more than one label. Besides the information\nabout the document classes, the corpus is also annotated at the morphological\nlayer. This paper further shows the results of selected state-of-the-art\nmethods on this corpus to offer the possibility of an easy comparison with\nthese approaches.", "published": "2017-10-06 12:22:44", "link": "http://arxiv.org/abs/1710.02365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Word Embeddings for Hyponymy with Entailment-Based\n  Distributional Semantics", "abstract": "Lexical entailment, such as hyponymy, is a fundamental issue in the semantics\nof natural language. This paper proposes distributional semantic models which\nefficiently learn word embeddings for entailment, using a recently-proposed\nframework for modelling entailment in a vector-space. These models postulate a\nlatent vector for a pseudo-phrase containing two neighbouring word vectors. We\ninvestigate both modelling words as the evidence they contribute about this\nphrase vector, or as the posterior distribution of a one-word phrase vector,\nand find that the posterior vectors perform better. The resulting word\nembeddings outperform the best previous results on predicting hyponymy between\nwords, in unsupervised and semi-supervised experiments.", "published": "2017-10-06 14:54:04", "link": "http://arxiv.org/abs/1710.02437v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Challenges of Sentiment Analysis for Dynamic Events", "abstract": "With the proliferation of social media over the last decade, determining\npeople's attitude with respect to a specific topic, document, interaction or\nevents has fueled research interest in natural language processing and\nintroduced a new channel called sentiment and emotion analysis. For instance,\nbusinesses routinely look to develop systems to automatically understand their\ncustomer conversations by identifying the relevant content to enhance marketing\ntheir products and managing their reputations. Previous efforts to assess\npeople's sentiment on Twitter have suggested that Twitter may be a valuable\nresource for studying political sentiment and that it reflects the offline\npolitical landscape. According to a Pew Research Center report, in January 2016\n44 percent of US adults stated having learned about the presidential election\nthrough social media. Furthermore, 24 percent reported use of social media\nposts of the two candidates as a source of news and information, which is more\nthan the 15 percent who have used both candidates' websites or emails combined.\nThe first presidential debate between Trump and Hillary was the most tweeted\ndebate ever with 17.1 million tweets.", "published": "2017-10-06 17:46:53", "link": "http://arxiv.org/abs/1710.02514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-resource bilingual lexicon extraction using graph based word\n  embeddings", "abstract": "In this work we focus on the task of automatically extracting bilingual\nlexicon for the language pair Spanish-Nahuatl. This is a low-resource setting\nwhere only a small amount of parallel corpus is available. Most of the\ndownstream methods do not work well under low-resources conditions. This is\nspecially true for the approaches that use vectorial representations like\nWord2Vec. Our proposal is to construct bilingual word vectors from a graph.\nThis graph is generated using translation pairs obtained from an unsupervised\nword alignment method.\n  We show that, in a low-resource setting, these type of vectors are successful\nin representing words in a bilingual semantic space. Moreover, when a linear\ntransformation is applied to translate words from one language to another, our\ngraph based representations considerably outperform the popular setting that\nuses Word2Vec.", "published": "2017-10-06 19:57:59", "link": "http://arxiv.org/abs/1710.02569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Low-Rank RNN Adaptation for Context-Aware Language Modeling", "abstract": "A context-aware language model uses location, user and/or domain metadata\n(context) to adapt its predictions. In neural language models, context\ninformation is typically represented as an embedding and it is given to the RNN\nas an additional input, which has been shown to be useful in many applications.\nWe introduce a more powerful mechanism for using context to adapt an RNN by\nletting the context vector control a low-rank transformation of the recurrent\nlayer weight matrix. Experiments show that allowing a greater fraction of the\nmodel parameters to be adjusted has benefits in terms of perplexity and\nclassification for several different types of context.", "published": "2017-10-06 22:40:36", "link": "http://arxiv.org/abs/1710.02603v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The DIRHA-English corpus and related tasks for distant-speech\n  recognition in domestic environments", "abstract": "This paper introduces the contents and the possible usage of the\nDIRHA-ENGLISH multi-microphone corpus, recently realized under the EC DIRHA\nproject. The reference scenario is a domestic environment equipped with a large\nnumber of microphones and microphone arrays distributed in space.\n  The corpus is composed of both real and simulated material, and it includes\n12 US and 12 UK English native speakers. Each speaker uttered different sets of\nphonetically-rich sentences, newspaper articles, conversational speech,\nkeywords, and commands. From this material, a large set of 1-minute sequences\nwas generated, which also includes typical domestic background noise as well as\ninter/intra-room reverberation effects. Dev and test sets were derived, which\nrepresent a very precious material for different studies on multi-microphone\nspeech processing and distant-speech recognition. Various tasks and\ncorresponding Kaldi recipes have already been developed.\n  The paper reports a first set of baseline results obtained using different\ntechniques, including Deep Neural Networks (DNN), aligned with the\nstate-of-the-art at international level.", "published": "2017-10-06 19:20:38", "link": "http://arxiv.org/abs/1710.02560v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-end DNN Based Speaker Recognition Inspired by i-vector and PLDA", "abstract": "Recently several end-to-end speaker verification systems based on deep neural\nnetworks (DNNs) have been proposed. These systems have been proven to be\ncompetitive for text-dependent tasks as well as for text-independent tasks with\nshort utterances. However, for text-independent tasks with longer utterances,\nend-to-end systems are still outperformed by standard i-vector + PLDA systems.\nIn this work, we develop an end-to-end speaker verification system that is\ninitialized to mimic an i-vector + PLDA baseline. The system is then further\ntrained in an end-to-end manner but regularized so that it does not deviate too\nfar from the initial system. In this way we mitigate overfitting which normally\nlimits the performance of end-to-end systems. The proposed system outperforms\nthe i-vector + PLDA baseline on both long and short duration utterances.", "published": "2017-10-06 12:42:01", "link": "http://arxiv.org/abs/1710.02369v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generating Nontrivial Melodies for Music as a Service", "abstract": "We present a hybrid neural network and rule-based system that generates pop\nmusic. Music produced by pure rule-based systems often sounds mechanical. Music\nproduced by machine learning sounds better, but still lacks hierarchical\ntemporal structure. We restore temporal hierarchy by augmenting machine\nlearning with a temporal production grammar, which generates the music's\noverall structure and chord progressions. A compatible melody is then generated\nby a conditional variational recurrent autoencoder. The autoencoder is trained\nwith eight-measure segments from a corpus of 10,000 MIDI files, each of which\nhas had its melody track and chord progressions identified heuristically. The\nautoencoder maps melody into a multi-dimensional feature space, conditioned by\nthe underlying chord progression. A melody is then generated by feeding a\nrandom sample from that space to the autoencoder's decoder, along with the\nchord progression generated by the grammar. The autoencoder can make musically\nplausible variations on an existing melody, suitable for recurring motifs. It\ncan also reharmonize a melody to a new chord progression, keeping the rhythm\nand contour. The generated music compares favorably with that generated by\nother academic and commercial software designed for the music-as-a-service\nindustry.", "published": "2017-10-06 05:53:20", "link": "http://arxiv.org/abs/1710.02280v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
