{"title": "An Empirical Evaluation of doc2vec with Practical Insights into Document\n  Embedding Generation", "abstract": "Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec\n(Mikolov et al., 2013a) to learn document-level embeddings. Despite promising\nresults in the original paper, others have struggled to reproduce those\nresults. This paper presents a rigorous empirical evaluation of doc2vec over\ntwo tasks. We compare doc2vec to two baselines and two state-of-the-art\ndocument embedding methodologies. We found that doc2vec performs robustly when\nusing models trained on large external corpora, and can be further improved by\nusing pre-trained word embeddings. We also provide recommendations on\nhyper-parameter settings for general purpose applications, and release source\ncode to induce document embeddings using our trained doc2vec models.", "published": "2016-07-19 01:55:55", "link": "http://arxiv.org/abs/1607.05368v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discriminating between similar languages in Twitter using label\n  propagation", "abstract": "Identifying the language of social media messages is an important first step\nin linguistic processing. Existing models for Twitter focus on content\nanalysis, which is successful for dissimilar language pairs. We propose a label\npropagation approach that takes the social graph of tweet authors into account\nas well as content to better tease apart similar languages. This results in\nstate-of-the-art shared task performance of $76.63\\%$, $1.4\\%$ higher than the\ntop system.", "published": "2016-07-19 05:38:58", "link": "http://arxiv.org/abs/1607.05408v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Novel Information Theoretic Framework for Finding Semantic Similarity\n  in WordNet", "abstract": "Information content (IC) based measures for finding semantic similarity is\ngaining preferences day by day. Semantics of concepts can be highly\ncharacterized by information theory. The conventional way for calculating IC is\nbased on the probability of appearance of concepts in corpora. Due to data\nsparseness and corpora dependency issues of those conventional approaches, a\nnew corpora independent intrinsic IC calculation measure has evolved. In this\npaper, we mainly focus on such intrinsic IC model and several topological\naspects of the underlying ontology. Accuracy of intrinsic IC calculation and\nsemantic similarity measure rely on these aspects deeply. Based on these\nanalysis we propose an information theoretic framework which comprises an\nintrinsic IC calculator and a semantic similarity model. Our approach is\ncompared with state of the art semantic similarity measures based on corpora\ndependent IC calculation as well as intrinsic IC based methods using several\nbenchmark data set. We also compare our model with the related Edge based,\nFeature based and Distributional approaches. Experimental results show that our\nintrinsic IC model gives high correlation value when applied to different\nsemantic similarity models. Our proposed semantic similarity model also\nachieves significant results when embedded with some state of the art IC models\nincluding ours.", "published": "2016-07-19 06:32:26", "link": "http://arxiv.org/abs/1607.05422v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Trainable Frontend For Robust and Far-Field Keyword Spotting", "abstract": "Robust and far-field speech recognition is critical to enable true hands-free\ncommunication. In far-field conditions, signals are attenuated due to distance.\nTo improve robustness to loudness variation, we introduce a novel frontend\ncalled per-channel energy normalization (PCEN). The key ingredient of PCEN is\nthe use of an automatic gain control based dynamic compression to replace the\nwidely used static (such as log or root) compression. We evaluate PCEN on the\nkeyword spotting task. On our large rerecorded noisy and far-field eval sets,\nwe show that PCEN significantly improves recognition performance. Furthermore,\nwe model PCEN as neural network layers and optimize high-dimensional PCEN\nparameters jointly with the keyword spotting acoustic model. The trained PCEN\nfrontend demonstrates significant further improvements without increasing model\ncomplexity or inference-time cost.", "published": "2016-07-19 17:17:58", "link": "http://arxiv.org/abs/1607.05666v1", "categories": ["cs.CL", "cs.NE"], "primary_category": "cs.CL"}
