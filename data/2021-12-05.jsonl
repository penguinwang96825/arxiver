{"title": "Consistent Training and Decoding For End-to-end Speech Recognition Using\n  Lattice-free MMI", "abstract": "Recently, End-to-End (E2E) frameworks have achieved remarkable results on\nvarious Automatic Speech Recognition (ASR) tasks. However, Lattice-Free Maximum\nMutual Information (LF-MMI), as one of the discriminative training criteria\nthat show superior performance in hybrid ASR systems, is rarely adopted in E2E\nASR frameworks. In this work, we propose a novel approach to integrate LF-MMI\ncriterion into E2E ASR frameworks in both training and decoding stages. The\nproposed approach shows its effectiveness on two of the most widely used E2E\nframeworks including Attention-Based Encoder-Decoders (AEDs) and Neural\nTransducers (NTs). Experiments suggest that the introduction of the LF-MMI\ncriterion consistently leads to significant performance improvements on various\ndatasets and different E2E ASR frameworks. The best of our models achieves\ncompetitive CER of 4.1\\% / 4.4\\% on Aishell-1 dev/test set; we also achieve\nsignificant error reduction on Aishell-2 and Librispeech datasets over strong\nbaselines.", "published": "2021-12-05 07:30:17", "link": "http://arxiv.org/abs/2112.02498v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Causal Distillation for Language Models", "abstract": "Distillation efforts have led to language models that are more compact and\nefficient without serious drops in performance. The standard approach to\ndistillation trains a student model against two objectives: a task-specific\nobjective (e.g., language modeling) and an imitation objective that encourages\nthe hidden states of the student model to be similar to those of the larger\nteacher model. In this paper, we show that it is beneficial to augment\ndistillation with a third objective that encourages the student to imitate the\ncausal computation process of the teacher through interchange intervention\ntraining(IIT). IIT pushes the student model to become a causal abstraction of\nthe teacher model - a simpler model with the same causal structure. IIT is\nfully differentiable, easily implemented, and combines flexibly with other\nobjectives. Compared with standard distillation of BERT, distillation via IIT\nresults in lower perplexity on Wikipedia (masked language modeling) and marked\nimprovements on the GLUE benchmark (natural language understanding), SQuAD\n(question answering), and CoNLL-2003 (named entity recognition).", "published": "2021-12-05 08:13:09", "link": "http://arxiv.org/abs/2112.02505v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Linear Arrangement Library. A new tool for research on syntactic\n  dependency structures", "abstract": "The new and growing field of Quantitative Dependency Syntax has emerged at\nthe crossroads between Dependency Syntax and Quantitative Linguistics. One of\nthe main concerns in this field is the statistical patterns of syntactic\ndependency structures. These structures, grouped in treebanks, are the source\nfor statistical analyses in these and related areas; dozens of scores devised\nover the years are the tools of a new industry to search for patterns and\nperform other sorts of analyses. The plethora of such metrics and their\nincreasing complexity require sharing the source code of the programs used to\nperform such analyses. However, such code is not often shared with the\nscientific community or is tested following unknown standards. Here we present\na new open-source tool, the Linear Arrangement Library (LAL), which caters to\nthe needs of, especially, inexperienced programmers. This tool enables the\ncalculation of these metrics on single syntactic dependency structures,\ntreebanks, and collection of treebanks, grounded on ease of use and yet with\ngreat flexibility. LAL has been designed to be efficient, easy to use (while\nsatisfying the needs of all levels of programming expertise), reliable (thanks\nto thorough testing), and to unite research from different traditions,\ngeographic areas, and research fields.", "published": "2021-12-05 08:48:52", "link": "http://arxiv.org/abs/2112.02512v1", "categories": ["cs.CL", "cs.DM"], "primary_category": "cs.CL"}
{"title": "Interpretable Privacy Preservation of Text Representations Using Vector\n  Steganography", "abstract": "Contextual word representations generated by language models (LMs) learn\nspurious associations present in the training corpora. Recent findings reveal\nthat adversaries can exploit these associations to reverse-engineer the private\nattributes of entities mentioned within the corpora. These findings have led to\nefforts towards minimizing the privacy risks of language models. However,\nexisting approaches lack interpretability, compromise on data utility and fail\nto provide privacy guarantees. Thus, the goal of my doctoral research is to\ndevelop interpretable approaches towards privacy preservation of text\nrepresentations that retain data utility while guaranteeing privacy. To this\nend, I aim to study and develop methods to incorporate steganographic\nmodifications within the vector geometry to obfuscate underlying spurious\nassociations and preserve the distributional semantic properties learnt during\ntraining.", "published": "2021-12-05 12:42:40", "link": "http://arxiv.org/abs/2112.02557v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-View Active Learning for Short Text Classification in\n  User-Generated Data", "abstract": "Mining user-generated data often suffers from the lack of enough labeled\ndata, short document lengths, and the informal user language. In this paper, we\npropose a novel active learning model to overcome these obstacles in the tasks\ntailored for query phrases--e.g., detecting positive reports of natural\ndisasters. Our model has three novelties: 1) It is the first approach to employ\nmulti-view active learning in this domain. 2) It uses the Parzen-Rosenblatt\nwindow method to integrate the representativeness measure into multi-view\nactive learning. 3) It employs a query-by-committee strategy, based on the\nagreement between predictors, to address the usually noisy language of the\ndocuments in this domain. We evaluate our model in four publicly available\nTwitter datasets with distinctly different applications. We also compare our\nmodel with a wide range of baselines including those with multiple classifiers.\nThe experiments testify that our model is highly consistent and outperforms\nexisting models.", "published": "2021-12-05 16:17:21", "link": "http://arxiv.org/abs/2112.02611v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Protecting Intellectual Property of Language Generation APIs with\n  Lexical Watermark", "abstract": "Nowadays, due to the breakthrough in natural language generation (NLG),\nincluding machine translation, document summarization, image captioning, etc\nNLG models have been encapsulated in cloud APIs to serve over half a billion\npeople worldwide and process over one hundred billion word generations per day.\nThus, NLG APIs have already become essential profitable services in many\ncommercial companies. Due to the substantial financial and intellectual\ninvestments, service providers adopt a pay-as-you-use policy to promote\nsustainable market growth. However, recent works have shown that cloud\nplatforms suffer from financial losses imposed by model extraction attacks,\nwhich aim to imitate the functionality and utility of the victim services, thus\nviolating the intellectual property (IP) of cloud APIs. This work targets at\nprotecting IP of NLG APIs by identifying the attackers who have utilized\nwatermarked responses from the victim NLG APIs. However, most existing\nwatermarking techniques are not directly amenable for IP protection of NLG\nAPIs. To bridge this gap, we first present a novel watermarking method for text\ngeneration APIs by conducting lexical modification to the original outputs.\nCompared with the competitive baselines, our watermark approach achieves better\nidentifiable performance in terms of p-value, with fewer semantic losses. In\naddition, our watermarks are more understandable and intuitive to humans than\nthe baselines. Finally, the empirical studies show our approach is also\napplicable to queries from different domains, and is effective on the attacker\ntrained on a mixture of the corpus which includes less than 10\\% watermarked\nsamples.", "published": "2021-12-05 22:54:54", "link": "http://arxiv.org/abs/2112.02701v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Differentiating Approach and Avoidance from Traditional Notions of\n  Sentiment in Economic Contexts", "abstract": "There is growing interest in the role of sentiment in economic\ndecision-making. However, most research on the subject has focused on positive\nand negative valence. Conviction Narrative Theory (CNT) places Approach and\nAvoidance sentiment (that which drives action) at the heart of real-world\ndecision-making, and argues that it better captures emotion in financial\nmarkets. This research, bringing together psychology and machine learning,\nintroduces new techniques to differentiate Approach and Avoidance from positive\nand negative sentiment on a fundamental level of meaning. It does this by\ncomparing word-lists, previously constructed to capture these concepts in text\ndata, across a large range of semantic features. The results demonstrate that\nAvoidance in particular is well defined as a separate type of emotion, which is\nevaluative/cognitive and action-orientated in nature. Refining the Avoidance\nword-list according to these features improves macroeconomic models, suggesting\nthat they capture the essence of Avoidance and that it plays a crucial role in\ndriving real-world economic decision-making.", "published": "2021-12-05 16:05:16", "link": "http://arxiv.org/abs/2112.02607v1", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "primary_category": "cs.CL"}
{"title": "VarCLR: Variable Semantic Representation Pre-training via Contrastive\n  Learning", "abstract": "Variable names are critical for conveying intended program behavior. Machine\nlearning-based program analysis methods use variable name representations for a\nwide range of tasks, such as suggesting new variable names and bug detection.\nIdeally, such methods could capture semantic relationships between names beyond\nsyntactic similarity, e.g., the fact that the names average and mean are\nsimilar. Unfortunately, previous work has found that even the best of previous\nrepresentation approaches primarily capture relatedness (whether two variables\nare linked at all), rather than similarity (whether they actually have the same\nmeaning).\n  We propose VarCLR, a new approach for learning semantic representations of\nvariable names that effectively captures variable similarity in this stricter\nsense. We observe that this problem is an excellent fit for contrastive\nlearning, which aims to minimize the distance between explicitly similar\ninputs, while maximizing the distance between dissimilar inputs. This requires\nlabeled training data, and thus we construct a novel, weakly-supervised\nvariable renaming dataset mined from GitHub edits. We show that VarCLR enables\nthe effective application of sophisticated, general-purpose language models\nlike BERT, to variable name representation and thus also to related downstream\ntasks like variable name similarity search or spelling correction. VarCLR\nproduces models that significantly outperform the state-of-the-art on IdBench,\nan existing benchmark that explicitly captures variable similarity (as distinct\nfrom relatedness). Finally, we contribute a release of all data, code, and\npre-trained models, aiming to provide a drop-in replacement for variable\nrepresentations used in either existing or future program analyses that rely on\nvariable names.", "published": "2021-12-05 18:40:32", "link": "http://arxiv.org/abs/2112.02650v1", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "primary_category": "cs.SE"}
{"title": "BERTMap: A BERT-based Ontology Alignment System", "abstract": "Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in\nknowledge integration. Owing to the success of machine learning in many\ndomains, it has been applied in OM. However, the existing methods, which often\nadopt ad-hoc feature engineering or non-contextual word embeddings, have not\nyet outperformed rule-based systems especially in an unsupervised setting. In\nthis paper, we propose a novel OM system named BERTMap which can support both\nunsupervised and semi-supervised settings. It first predicts mappings using a\nclassifier based on fine-tuning the contextual embedding model BERT on text\nsemantics corpora extracted from ontologies, and then refines the mappings\nthrough extension and repair by utilizing the ontology structure and logic. Our\nevaluation with three alignment tasks on biomedical ontologies demonstrates\nthat BERTMap can often perform better than the leading OM systems LogMap and\nAML.", "published": "2021-12-05 21:08:44", "link": "http://arxiv.org/abs/2112.02682v4", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Achieving Forgetting Prevention and Knowledge Transfer in Continual\n  Learning", "abstract": "Continual learning (CL) learns a sequence of tasks incrementally with the\ngoal of achieving two main objectives: overcoming catastrophic forgetting (CF)\nand encouraging knowledge transfer (KT) across tasks. However, most existing\ntechniques focus only on overcoming CF and have no mechanism to encourage KT,\nand thus do not do well in KT. Although several papers have tried to deal with\nboth CF and KT, our experiments show that they suffer from serious CF when the\ntasks do not have much shared knowledge. Another observation is that most\ncurrent CL methods do not use pre-trained models, but it has been shown that\nsuch models can significantly improve the end task performance. For example, in\nnatural language processing, fine-tuning a BERT-like pre-trained language model\nis one of the most effective approaches. However, for CL, this approach suffers\nfrom serious CF. An interesting question is how to make the best use of\npre-trained models for CL. This paper proposes a novel model called CTR to\nsolve these problems. Our experimental results demonstrate the effectiveness of\nCTR", "published": "2021-12-05 23:13:13", "link": "http://arxiv.org/abs/2112.02706v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "CLASSIC: Continual and Contrastive Learning of Aspect Sentiment\n  Classification Tasks", "abstract": "This paper studies continual learning (CL) of a sequence of aspect sentiment\nclassification(ASC) tasks in a particular CL setting called domain incremental\nlearning (DIL). Each task is from a different domain or product. The DIL\nsetting is particularly suited to ASC because in testing the system needs not\nknow the task/domain to which the test data belongs. To our knowledge, this\nsetting has not been studied before for ASC. This paper proposes a novel model\ncalled CLASSIC. The key novelty is a contrastive continual learning method that\nenables both knowledge transfer across tasks and knowledge distillation from\nold tasks to the new task, which eliminates the need for task ids in testing.\nExperimental results show the high effectiveness of CLASSIC.", "published": "2021-12-05 23:55:53", "link": "http://arxiv.org/abs/2112.02714v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Visual Persuasion in COVID-19 Social Media Content: A Multi-Modal\n  Characterization", "abstract": "Social media content routinely incorporates multi-modal design to covey\ninformation and shape meanings, and sway interpretations toward desirable\nimplications, but the choices and outcomes of using both texts and visual\nimages have not been sufficiently studied. This work proposes a computational\napproach to analyze the outcome of persuasive information in multi-modal\ncontent, focusing on two aspects, popularity and reliability, in\nCOVID-19-related news articles shared on Twitter. The two aspects are\nintertwined in the spread of misinformation: for example, an unreliable article\nthat aims to misinform has to attain some popularity. This work has several\ncontributions. First, we propose a multi-modal (image and text) approach to\neffectively identify popularity and reliability of information sources\nsimultaneously. Second, we identify textual and visual elements that are\npredictive to information popularity and reliability. Third, by modeling\ncross-modal relations and similarity, we are able to uncover how unreliable\narticles construct multi-modal meaning in a distorted, biased fashion. Our work\ndemonstrates how to use multi-modal analysis for understanding influential\ncontent and has implications to social media literacy and engagement.", "published": "2021-12-05 02:15:01", "link": "http://arxiv.org/abs/2112.13910v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Toward Real-World Voice Disorder Classification", "abstract": "Objective: Voice disorders significantly compromise individuals' ability to\nspeak in their daily lives. Without early diagnosis and treatment, these\ndisorders may deteriorate drastically. Thus, automatic classification systems\nat home are desirable for people who are inaccessible to clinical disease\nassessments. However, the performance of such systems may be weakened due to\nthe constrained resources and domain mismatch between the clinical data and\nnoisy real-world data. Methods: This study develops a compact and domain-robust\nvoice disorder classification system to identify the utterances of health,\nneoplasm, and benign structural diseases. Our proposed system utilizes a\nfeature extractor model composed of factorized convolutional neural networks\nand subsequently deploys domain adversarial training to reconcile the domain\nmismatch by extracting domain invariant features. Results: The results show\nthat the unweighted average recall in the noisy real-world domain improved by\n13% and remained at 80% in the clinic domain with only slight degradation. The\ndomain mismatch was effectively eliminated. Moreover, the proposed system\nreduced the usage of both memory and computation by over 73.9%. Conclusion: By\ndeploying factorized convolutional neural networks and domain adversarial\ntraining, domain-invariant features can be derived for voice disorder\nclassification with limited resources. The promising results confirm that the\nproposed system can significantly reduce resource consumption and improve\nclassification accuracy by considering the domain mismatch. Significance: To\nthe best of our knowledge, this is the first study that jointly considers\nreal-world model compression and noise-robustness issues in voice disorder\nclassification. The proposed system is intended for application to embedded\nsystems with limited resources.", "published": "2021-12-05 11:00:21", "link": "http://arxiv.org/abs/2112.02538v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
