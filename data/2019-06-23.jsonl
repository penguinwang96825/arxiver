{"title": "Smaller Text Classifiers with Discriminative Cluster Embeddings", "abstract": "Word embedding parameters often dominate overall model sizes in neural\nmethods for natural language processing. We reduce deployed model sizes of text\nclassifiers by learning a hard word clustering in an end-to-end manner. We use\nthe Gumbel-Softmax distribution to maximize over the latent clustering while\nminimizing the task loss. We propose variations that selectively assign\nadditional parameters to words, which further improves accuracy while still\nremaining parameter-efficient.", "published": "2019-06-23 02:00:40", "link": "http://arxiv.org/abs/1906.09532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Sequential Labelers for Semi-Supervised Learning", "abstract": "We introduce a family of multitask variational methods for semi-supervised\nsequence labeling. Our model family consists of a latent-variable generative\nmodel and a discriminative labeler. The generative models use latent variables\nto define the conditional probability of a word given its context, drawing\ninspiration from word prediction objectives commonly used in learning word\nembeddings. The labeler helps inject discriminative information into the latent\nspace. We explore several latent variable configurations, including ones with\nhierarchical structure, which enables the model to account for both\nlabel-specific and word-specific information. Our models consistently\noutperform standard sequential baselines on 8 sequence labeling datasets, and\nimprove further with unlabeled data.", "published": "2019-06-23 02:17:46", "link": "http://arxiv.org/abs/1906.09535v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAL: Dual Adversarial Learning for Dialogue Generation", "abstract": "In open-domain dialogue systems, generative approaches have attracted much\nattention for response generation. However, existing methods are heavily\nplagued by generating safe responses and unnatural responses. To alleviate\nthese two problems, we propose a novel framework named Dual Adversarial\nLearning (DAL) for high-quality response generation. DAL is the first work to\ninnovatively utilizes the duality between query generation and response\ngeneration to avoid safe responses and increase the diversity of the generated\nresponses. Additionally, DAL uses adversarial learning to mimic human judges\nand guides the system to generate natural responses. Experimental results\ndemonstrate that DAL effectively improves both diversity and overall quality of\nthe generated responses. DAL outperforms the state-of-the-art methods regarding\nautomatic metrics and human evaluations.", "published": "2019-06-23 05:28:03", "link": "http://arxiv.org/abs/1906.09556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Data Transformation and Combination for Text\n  Classification", "abstract": "Text classification is a fundamental task for text data mining. In order to\ntrain a generalizable model, a large volume of text must be collected. To\naddress data insufficiency, cross-lingual data may occasionally be necessary.\nCross-lingual data sources may however suffer from data incompatibility, as\ntext written in different languages can hold distinct word sequences and\nsemantic patterns. Machine translation and word embedding alignment provide an\neffective way to transform and combine data for cross-lingual data training. To\nthe best of our knowledge, there has been little work done on evaluating how\nthe methodology used to conduct semantic space transformation and data\ncombination affects the performance of classification models trained from\ncross-lingual resources. In this paper, we systematically evaluated the\nperformance of two commonly used CNN (Convolutional Neural Network) and RNN\n(Recurrent Neural Network) text classifiers with differing data transformation\nand combination strategies. Monolingual models were trained from English and\nFrench alongside their translated and aligned embeddings. Our results suggested\nthat semantic space transformation may conditionally promote the performance of\nmonolingual models. Bilingual models were trained from a combination of both\nEnglish and French. Our results indicate that a cross-lingual classification\nmodel can significantly benefit from cross-lingual data by learning from\ntranslated or aligned embedding spaces.", "published": "2019-06-23 02:56:02", "link": "http://arxiv.org/abs/1906.09543v1", "categories": ["cs.IR", "cs.CL", "68U15"], "primary_category": "cs.IR"}
{"title": "Investigating Biases in Textual Entailment Datasets", "abstract": "The ability to understand logical relationships between sentences is an\nimportant task in language understanding. To aid in progress for this task,\nresearchers have collected datasets for machine learning and evaluation of\ncurrent systems. However, like in the crowdsourced Visual Question Answering\n(VQA) task, some biases in the data inevitably occur. In our experiments, we\nfind that performing classification on just the hypotheses on the SNLI dataset\nyields an accuracy of 64%. We analyze the bias extent in the SNLI and the\nMultiNLI dataset, discuss its implication, and propose a simple method to\nreduce the biases in the datasets.", "published": "2019-06-23 19:38:53", "link": "http://arxiv.org/abs/1906.09635v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Systematic improvement of user engagement with academic titles using\n  computational linguistics", "abstract": "This paper describes a novel approach to systematically improve information\ninteractions based solely on its wording. Following an interdisciplinary\nliterature review, we recognized three key attributes of words that drive user\nengagement: (1) Novelty (2) Familiarity (3) Emotionality. Based on these\nattributes, we developed a model to systematically improve a given content\nusing computational linguistics, natural language processing (NLP) and text\nanalysis (word frequency, sentiment analysis and lexical substitution). We\nconducted a pilot study (n=216) in which the model was used to formalize\nevaluation and optimization of academic titles. A between-group design (A/B\ntesting) was used to compare responses to the original and modified (treatment)\ntitles. Data was collected for selection and evaluation (User Engagement\nScale). The pilot results suggest that user engagement with digital information\nis fostered by, and perhaps dependent upon, the wording being used. They also\nprovide empirical support that engaging content can be systematically evaluated\nand produced. The preliminary results show that the modified (treatment) titles\nhad significantly higher scores for information use and user engagement\n(selection and evaluation). We propose that computational linguistics is a\nuseful approach for optimizing information interactions. The empirically based\ninsights can inform the development of digital content strategies, thereby\nimproving the success of information interactions.elop more sophisticated\ninteraction measures.", "published": "2019-06-23 09:23:08", "link": "http://arxiv.org/abs/1906.09569v1", "categories": ["cs.CL", "cs.DL", "cs.HC", "94A05", "H.5.2"], "primary_category": "cs.CL"}
{"title": "Sequence Generation: From Both Sides to the Middle", "abstract": "The encoder-decoder framework has achieved promising process for many\nsequence generation tasks, such as neural machine translation and text\nsummarization. Such a framework usually generates a sequence token by token\nfrom left to right, hence (1) this autoregressive decoding procedure is\ntime-consuming when the output sentence becomes longer, and (2) it lacks the\nguidance of future context which is crucial to avoid under translation. To\nalleviate these issues, we propose a synchronous bidirectional sequence\ngeneration (SBSG) model which predicts its outputs from both sides to the\nmiddle simultaneously. In the SBSG model, we enable the left-to-right (L2R) and\nright-to-left (R2L) generation to help and interact with each other by\nleveraging interactive bidirectional attention network. Experiments on neural\nmachine translation (En-De, Ch-En, and En-Ro) and text summarization tasks show\nthat the proposed model significantly speeds up decoding while improving the\ngeneration quality compared to the autoregressive Transformer.", "published": "2019-06-23 16:13:45", "link": "http://arxiv.org/abs/1906.09601v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Neural Vocoder with Hierarchical Generation of Amplitude and Phase\n  Spectra for Statistical Parametric Speech Synthesis", "abstract": "This paper presents a neural vocoder named HiNet which reconstructs speech\nwaveforms from acoustic features by predicting amplitude and phase spectra\nhierarchically. Different from existing neural vocoders such as WaveNet,\nSampleRNN and WaveRNN which directly generate waveform samples using single\nneural networks, the HiNet vocoder is composed of an amplitude spectrum\npredictor (ASP) and a phase spectrum predictor (PSP). The ASP is a simple DNN\nmodel which predicts log amplitude spectra (LAS) from acoustic features. The\npredicted LAS are sent into the PSP for phase recovery. Considering the issue\nof phase warping and the difficulty of phase modeling, the PSP is constructed\nby concatenating a neural source-filter (NSF) waveform generator with a phase\nextractor. We also introduce generative adversarial networks (GANs) into both\nASP and PSP. Finally, the outputs of ASP and PSP are combined to reconstruct\nspeech waveforms by short-time Fourier synthesis. Since there are no\nautoregressive structures in both predictors, the HiNet vocoder can generate\nspeech waveforms with high efficiency. Objective and subjective experimental\nresults show that our proposed HiNet vocoder achieves better naturalness of\nreconstructed speech than the conventional STRAIGHT vocoder, a 16-bit WaveNet\nvocoder using open source implementation and an NSF vocoder with similar\ncomplexity to the PSP and obtains similar performance with a 16-bit WaveRNN\nvocoder. We also find that the performance of HiNet is insensitive to the\ncomplexity of the neural waveform generator in PSP to some extend. After\nsimplifying its model structure, the time consumed for generating 1s waveforms\nof 16kHz speech using a GPU can be further reduced from 0.34s to 0.19s without\nsignificant quality degradation.", "published": "2019-06-23 10:01:33", "link": "http://arxiv.org/abs/1906.09573v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
