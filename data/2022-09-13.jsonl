{"title": "Rethink about the Word-level Quality Estimation for Machine Translation\n  from Human Judgement", "abstract": "Word-level Quality Estimation (QE) of Machine Translation (MT) aims to find\nout potential translation errors in the translated sentence without reference.\nTypically, conventional works on word-level QE are designed to predict the\ntranslation quality in terms of the post-editing effort, where the word labels\n(\"OK\" and \"BAD\") are automatically generated by comparing words between MT\nsentences and the post-edited sentences through a Translation Error Rate (TER)\ntoolkit. While the post-editing effort can be used to measure the translation\nquality to some extent, we find it usually conflicts with the human judgement\non whether the word is well or poorly translated. To overcome the limitation,\nwe first create a golden benchmark dataset, namely \\emph{HJQE} (Human Judgement\non Quality Estimation), where the expert translators directly annotate the\npoorly translated words on their judgements. Additionally, to further make use\nof the parallel corpus, we propose the self-supervised pre-training with two\ntag correcting strategies, namely tag refinement strategy and tree-based\nannotation strategy, to make the TER-based artificial QE corpus closer to\n\\emph{HJQE}. We conduct substantial experiments based on the publicly available\nWMT En-De and En-Zh corpora. The results not only show our proposed dataset is\nmore consistent with human judgment but also confirm the effectiveness of the\nproposed tag correcting strategies.\\footnote{The data can be found at\n\\url{https://github.com/ZhenYangIACAS/HJQE}.}", "published": "2022-09-13 02:37:12", "link": "http://arxiv.org/abs/2209.05695v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Non-Parametric Temporal Adaptation for Social Media Topic Classification", "abstract": "User-generated social media data is constantly changing as new trends\ninfluence online discussion and personal information is deleted due to privacy\nconcerns. However, most current NLP models are static and rely on fixed\ntraining data, which means they are unable to adapt to temporal change -- both\ntest distribution shift and deleted training data -- without frequent, costly\nre-training. In this paper, we study temporal adaptation through the task of\nlongitudinal hashtag prediction and propose a non-parametric dense retrieval\ntechnique, which does not require re-training, as a simple but effective\nsolution. In experiments on a newly collected, publicly available, year-long\nTwitter dataset exhibiting temporal distribution shift, our method improves by\n64.12% over the best parametric baseline without any of its costly\ngradient-based updating. Our dense retrieval approach is also particularly\nwell-suited to dynamically deleted user data in line with data privacy laws,\nwith negligible computational cost and performance loss.", "published": "2022-09-13 03:31:38", "link": "http://arxiv.org/abs/2209.05706v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A virtual reality-based method for examining audiovisual prosody\n  perception", "abstract": "Prosody plays a vital role in verbal communication. Acoustic cues of prosody\nhave been examined extensively. However, prosodic characteristics are not only\nperceived auditorily, but also visually based on head and facial movements. The\npurpose of this report is to present a method for examining audiovisual prosody\nusing virtual reality. We show that animations based on a virtual human provide\nmotion cues similar to those obtained from video recordings of a real talker.\nThe use of virtual reality opens up new avenues for examining multimodal\neffects of verbal communication. We discuss the method in the framework of\nexamining prosody perception in cochlear implant listeners.", "published": "2022-09-13 06:10:08", "link": "http://arxiv.org/abs/2209.05745v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-stage Distillation Framework for Cross-Lingual Semantic Similarity\n  Matching", "abstract": "Previous studies have proved that cross-lingual knowledge distillation can\nsignificantly improve the performance of pre-trained models for cross-lingual\nsimilarity matching tasks. However, the student model needs to be large in this\noperation. Otherwise, its performance will drop sharply, thus making it\nimpractical to be deployed to memory-limited devices. To address this issue, we\ndelve into cross-lingual knowledge distillation and propose a multi-stage\ndistillation framework for constructing a small-size but high-performance\ncross-lingual model. In our framework, contrastive learning, bottleneck, and\nparameter recurrent strategies are combined to prevent performance from being\ncompromised during the compression process. The experimental results\ndemonstrate that our method can compress the size of XLM-R and MiniLM by more\nthan 50\\%, while the performance is only reduced by about 1%.", "published": "2022-09-13 10:33:04", "link": "http://arxiv.org/abs/2209.05869v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-aware Positional Encoding and Linguistic-guided Encoding for\n  Abstractive Multi-document Summarization", "abstract": "One key challenge in multi-document summarization is to capture the relations\namong input documents that distinguish between single document summarization\n(SDS) and multi-document summarization (MDS). Few existing MDS works address\nthis issue. One effective way is to encode document positional information to\nassist models in capturing cross-document relations. However, existing MDS\nmodels, such as Transformer-based models, only consider token-level positional\ninformation. Moreover, these models fail to capture sentences' linguistic\nstructure, which inevitably causes confusions in the generated summaries.\nTherefore, in this paper, we propose document-aware positional encoding and\nlinguistic-guided encoding that can be fused with Transformer architecture for\nMDS. For document-aware positional encoding, we introduce a general protocol to\nguide the selection of document encoding functions. For linguistic-guided\nencoding, we propose to embed syntactic dependency relations into the\ndependency relation mask with a simple but effective non-linear encoding\nlearner for feature learning. Extensive experiments show the proposed model can\ngenerate summaries with high quality.", "published": "2022-09-13 12:22:38", "link": "http://arxiv.org/abs/2209.05929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Design of Negative Sampling Strategies for Distantly Supervised Skill\n  Extraction", "abstract": "Skills play a central role in the job market and many human resources (HR)\nprocesses. In the wake of other digital experiences, today's online job market\nhas candidates expecting to see the right opportunities based on their skill\nset. Similarly, enterprises increasingly need to use data to guarantee that the\nskills within their workforce remain future-proof. However, structured\ninformation about skills is often missing, and processes building on self- or\nmanager-assessment have shown to struggle with issues around adoption,\ncompleteness, and freshness of the resulting data. Extracting skills is a\nhighly challenging task, given the many thousands of possible skill labels\nmentioned either explicitly or merely described implicitly and the lack of\nfinely annotated training corpora. Previous work on skill extraction overly\nsimplifies the task to an explicit entity detection task or builds on manually\nannotated training data that would be infeasible if applied to a complete\nvocabulary of skills. We propose an end-to-end system for skill extraction,\nbased on distant supervision through literal matching. We propose and evaluate\nseveral negative sampling strategies, tuned on a small validation dataset, to\nimprove the generalization of skill extraction towards implicitly mentioned\nskills, despite the lack of such implicit skills in the distantly supervised\ndata. We observe that using the ESCO taxonomy to select negative examples from\nrelated skills yields the biggest improvements, and combining three different\nstrategies in one model further increases the performance, up to 8 percentage\npoints in RP@5. We introduce a manually annotated evaluation benchmark for\nskill extraction based on the ESCO taxonomy, on which we validate our models.\nWe release the benchmark dataset for research purposes to stimulate further\nresearch on the task.", "published": "2022-09-13 13:37:06", "link": "http://arxiv.org/abs/2209.05987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalized Intent Discovery: Learning from Open World Dialogue System", "abstract": "Traditional intent classification models are based on a pre-defined intent\nset and only recognize limited in-domain (IND) intent classes. But users may\ninput out-of-domain (OOD) queries in a practical dialogue system. Such OOD\nqueries can provide directions for future improvement. In this paper, we define\na new task, Generalized Intent Discovery (GID), which aims to extend an IND\nintent classifier to an open-world intent set including IND and OOD intents. We\nhope to simultaneously classify a set of labeled IND intent classes while\ndiscovering and recognizing new unlabeled OOD types incrementally. We construct\nthree public datasets for different application scenarios and propose two kinds\nof frameworks, pipeline-based and end-to-end for future work. Further, we\nconduct exhaustive experiments and qualitative analysis to comprehend key\nchallenges and provide new guidance for future GID research.", "published": "2022-09-13 14:31:53", "link": "http://arxiv.org/abs/2209.06030v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Role of Explanatory Value in Natural Language Processing", "abstract": "A key aim of science is explanation, yet the idea of explaining language\nphenomena has taken a backseat in mainstream Natural Language Processing (NLP)\nand many other areas of Artificial Intelligence. I argue that explanation of\nlinguistic behaviour should be a main goal of NLP, and that this is not the\nsame as making NLP models explainable. To illustrate these ideas, some recent\nmodels of human language production are compared with each other. I conclude by\nasking what it would mean for NLP research and institutional policies if our\ncommunity took explanatory value seriously, while heeding some possible\npitfalls.", "published": "2022-09-13 17:19:04", "link": "http://arxiv.org/abs/2209.06169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Sarcasm Analysis on Social Media: A Systematic Review", "abstract": "Sarcasm can be defined as saying or writing the opposite of what one truly\nwants to express, usually to insult, irritate, or amuse someone. Because of the\nobscure nature of sarcasm in textual data, detecting it is difficult and of\ngreat interest to the sentiment analysis research community. Though the\nresearch in sarcasm detection spans more than a decade, some significant\nadvancements have been made recently, including employing unsupervised\npre-trained transformers in multimodal environments and integrating context to\nidentify sarcasm. In this study, we aim to provide a brief overview of recent\nadvancements and trends in computational sarcasm research for the English\nlanguage. We describe relevant datasets, methodologies, trends, issues,\nchallenges, and tasks relating to sarcasm that are beyond detection. Our study\nprovides well-summarized tables of sarcasm datasets, sarcastic features and\ntheir extraction methods, and performance analysis of various approaches which\ncan help researchers in related domains understand current state-of-the-art\npractices in sarcasm detection.", "published": "2022-09-13 17:20:19", "link": "http://arxiv.org/abs/2209.06170v2", "categories": ["cs.CL", "A.1"], "primary_category": "cs.CL"}
{"title": "Exploring Code Style Transfer with Neural Networks", "abstract": "Style is a significant component of natural language text, reflecting a\nchange in the tone of text while keeping the underlying information the same.\nEven though programming languages have strict syntax rules, they also have\nstyle. Code can be written with the same functionality but using different\nlanguage features. However, programming style is difficult to quantify, and\nthus as part of this work, we define style attributes, specifically for Python.\nTo build a definition of style, we utilized hierarchical clustering to capture\na style definition without needing to specify transformations. In addition to\ndefining style, we explore the capability of a pre-trained code language model\nto capture information about code style. To do this, we fine-tuned pre-trained\ncode-language models and evaluated their performance in code style transfer\ntasks.", "published": "2022-09-13 19:34:42", "link": "http://arxiv.org/abs/2209.06273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-adaptive Transfer Learning for Translation: A Case Study in Haitian\n  and Jamaican", "abstract": "Multilingual transfer techniques often improve low-resource machine\ntranslation (MT). Many of these techniques are applied without considering data\ncharacteristics. We show in the context of Haitian-to-English translation that\ntransfer effectiveness is correlated with amount of training data and\nrelationships between knowledge-sharing languages. Our experiments suggest that\nfor some languages beyond a threshold of authentic data, back-translation\naugmentation methods are counterproductive, while cross-lingual transfer from a\nsufficiently related language is preferred. We complement this finding by\ncontributing a rule-based French-Haitian orthographic and syntactic engine and\na novel method for phonological embedding. When used with multilingual\ntechniques, orthographic transformation makes statistically significant\nimprovements over conventional methods. And in very low-resource Jamaican MT,\ncode-switching with a transfer language for orthographic resemblance yields a\n6.63 BLEU point advantage.", "published": "2022-09-13 20:58:46", "link": "http://arxiv.org/abs/2209.06295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CNN-Trans-Enc: A CNN-Enhanced Transformer-Encoder On Top Of Static BERT\n  representations for Document Classification", "abstract": "BERT achieves remarkable results in text classification tasks, it is yet not\nfully exploited, since only the last layer is used as a representation output\nfor downstream classifiers. The most recent studies on the nature of linguistic\nfeatures learned by BERT, suggest that different layers focus on different\nkinds of linguistic features. We propose a CNN-Enhanced Transformer-Encoder\nmodel which is trained on top of fixed BERT $[CLS]$ representations from all\nlayers, employing Convolutional Neural Networks to generate QKV feature maps\ninside the Transformer-Encoder, instead of linear projections of the input into\nthe embedding space. CNN-Trans-Enc is relatively small as a downstream\nclassifier and doesn't require any fine-tuning of BERT, as it ensures an\noptimal use of the $[CLS]$ representations from all layers, leveraging\ndifferent linguistic features with more meaningful, and generalizable QKV\nrepresentations of the input. Using BERT with CNN-Trans-Enc keeps $98.9\\%$ and\n$94.8\\%$ of current state-of-the-art performance on the IMDB and SST-5 datasets\nrespectably, while obtaining new state-of-the-art on YELP-5 with $82.23$\n($8.9\\%$ improvement), and on Amazon-Polarity with $0.98\\%$ ($0.2\\%$\nimprovement) (K-fold Cross Validation on a 1M sample subset from both\ndatasets). On the AG news dataset CNN-Trans-Enc achieves $99.94\\%$ of the\ncurrent state-of-the-art, and achieves a new top performance with an average\naccuracy of $99.51\\%$ on DBPedia-14.\n  Index terms: Text Classification, Natural Language Processing, Convolutional\nNeural Networks, Transformers, BERT", "published": "2022-09-13 23:23:08", "link": "http://arxiv.org/abs/2209.06344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robin: A Novel Online Suicidal Text Corpus of Substantial Breadth and\n  Scale", "abstract": "Suicide is a major public health crisis. With more than 20,000,000 suicide\nattempts each year, the early detection of suicidal intent has the potential to\nsave hundreds of thousands of lives. Traditional mental health screening\nmethods are time-consuming, costly, and often inaccessible to disadvantaged\npopulations; online detection of suicidal intent using machine learning offers\na viable alternative. Here we present Robin, the largest non-keyword generated\nsuicidal corpus to date, consisting of over 1.1 million online forum postings.\nIn addition to its unprecedented size, Robin is specially constructed to\ninclude various categories of suicidal text, such as suicide bereavement and\nflippant references, better enabling models trained on Robin to learn the\nsubtle nuances of text expressing suicidal ideation. Experimental results\nachieve state-of-the-art performance for the classification of suicidal text,\nboth with traditional methods like logistic regression (F1=0.85), as well as\nwith large-scale pre-trained language models like BERT (F1=0.92). Finally, we\nrelease the Robin dataset publicly as a machine learning resource with the\npotential to drive the next generation of suicidal sentiment research.", "published": "2022-09-13 03:32:47", "link": "http://arxiv.org/abs/2209.05707v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning ASR pathways: A sparse multilingual ASR model", "abstract": "Neural network pruning compresses automatic speech recognition (ASR) models\neffectively. However, in multilingual ASR, language-agnostic pruning may lead\nto severe performance drops on some languages because language-agnostic pruning\nmasks may not fit all languages and discard important language-specific\nparameters. In this work, we present ASR pathways, a sparse multilingual ASR\nmodel that activates language-specific sub-networks (\"pathways\"), such that the\nparameters for each language are learned explicitly. With the overlapping\nsub-networks, the shared parameters can also enable knowledge transfer for\nlower-resource languages via joint multilingual training. We propose a novel\nalgorithm to learn ASR pathways, and evaluate the proposed method on 4\nlanguages with a streaming RNN-T model. Our proposed ASR pathways outperform\nboth dense models and a language-agnostically pruned model, and provide better\nperformance on low-resource languages compared to the monolingual sparse\nmodels.", "published": "2022-09-13 05:14:08", "link": "http://arxiv.org/abs/2209.05735v4", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical\n  Corpus", "abstract": "BERT is a widely used pre-trained model in natural language processing.\nHowever, since BERT is quadratic to the text length, the BERT model is\ndifficult to be used directly on the long-text corpus. In some fields, the\ncollected text data may be quite long, such as in the health care field.\nTherefore, to apply the pre-trained language knowledge of BERT to long text, in\nthis paper, imitating the skimming-intensive reading method used by humans when\nreading a long paragraph, the Skimming-Intensive Model (SkIn) is proposed. It\ncan dynamically select the critical information in the text so that the\nsentence input into the BERT-Base model is significantly shortened, which can\neffectively save the cost of the classification algorithm. Experiments show\nthat the SkIn method has achieved superior accuracy than the baselines on\nlong-text classification datasets in the medical field, while its time and\nspace requirements increase linearly with the text length, alleviating the time\nand space overflow problem of basic BERT on long-text data.", "published": "2022-09-13 05:49:10", "link": "http://arxiv.org/abs/2209.05741v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Recipe Flow: A Dataset for Learning Visual State Changes of\n  Objects with Recipe Flows", "abstract": "We present a new multimodal dataset called Visual Recipe Flow, which enables\nus to learn each cooking action result in a recipe text. The dataset consists\nof object state changes and the workflow of the recipe text. The state change\nis represented as an image pair, while the workflow is represented as a recipe\nflow graph (r-FG). The image pairs are grounded in the r-FG, which provides the\ncross-modal relation. With our dataset, one can try a range of applications,\nfrom multimodal commonsense reasoning and procedural text generation.", "published": "2022-09-13 09:38:32", "link": "http://arxiv.org/abs/2209.05840v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unified Generative & Dense Retrieval for Query Rewriting in Sponsored\n  Search", "abstract": "Sponsored search is a key revenue source for search engines, where\nadvertisers bid on keywords to target users or search queries of interest.\nHowever, finding relevant keywords for a given query is challenging due to the\nlarge and dynamic keyword space, ambiguous user/advertiser intents, and diverse\npossible topics and languages. In this work, we present a comprehensive\ncomparison between two paradigms for online query rewriting: Generative (NLG)\nand Dense Retrieval (DR) methods. We observe that both methods offer\ncomplementary benefits that are additive. As a result, we show that around 40%\nof the high-quality keywords retrieved by the two approaches are unique and not\nretrieved by the other. To leverage the strengths of both methods, we propose\nCLOVER-Unity, a novel approach that unifies generative and dense retrieval\nmethods in one single model. Through offline experiments, we show that the NLG\nand DR components of CLOVER-Unity consistently outperform individually trained\nNLG and DR models on public and internal benchmarks. Furthermore, we show that\nCLOVER-Unity achieves 9.8% higher good keyword density than the ensemble of two\nseparate DR and NLG models while reducing computational costs by almost half.\nWe conduct extensive online A/B experiments on Microsoft Bing in 140+ countries\nand achieve improved user engagement, with an average increase in total clicks\nby 0.89% and increased revenue by 1.27%. We also share our practical lessons\nand optimization tricks for deploying such unified models in production.", "published": "2022-09-13 10:19:23", "link": "http://arxiv.org/abs/2209.05861v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Don't Judge a Language Model by Its Last Layer: Contrastive Learning\n  with Layer-Wise Attention Pooling", "abstract": "Recent pre-trained language models (PLMs) achieved great success on many\nnatural language processing tasks through learning linguistic features and\ncontextualized sentence representation. Since attributes captured in stacked\nlayers of PLMs are not clearly identified, straightforward approaches such as\nembedding the last layer are commonly preferred to derive sentence\nrepresentations from PLMs. This paper introduces the attention-based pooling\nstrategy, which enables the model to preserve layer-wise signals captured in\neach layer and learn digested linguistic features for downstream tasks. The\ncontrastive learning objective can adapt the layer-wise attention pooling to\nboth unsupervised and supervised manners. It results in regularizing the\nanisotropic space of pre-trained embeddings and being more uniform. We evaluate\nour model on standard semantic textual similarity (STS) and semantic search\ntasks. As a result, our method improved the performance of the base contrastive\nlearned BERT_base and variants.", "published": "2022-09-13 13:09:49", "link": "http://arxiv.org/abs/2209.05972v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Streaming End-to-End Multilingual Speech Recognition with Joint Language\n  Identification", "abstract": "Language identification is critical for many downstream tasks in automatic\nspeech recognition (ASR), and is beneficial to integrate into multilingual\nend-to-end ASR as an additional task. In this paper, we propose to modify the\nstructure of the cascaded-encoder-based recurrent neural network transducer\n(RNN-T) model by integrating a per-frame language identifier (LID) predictor.\nRNN-T with cascaded encoders can achieve streaming ASR with low latency using\nfirst-pass decoding with no right-context, and achieve lower word error rates\n(WERs) using second-pass decoding with longer right-context. By leveraging such\ndifferences in the right-contexts and a streaming implementation of statistics\npooling, the proposed method can achieve accurate streaming LID prediction with\nlittle extra test-time cost. Experimental results on a voice search dataset\nwith 9 language locales shows that the proposed method achieves an average of\n96.2% LID prediction accuracy and the same second-pass WER as that obtained by\nincluding oracle LID in the input.", "published": "2022-09-13 15:10:41", "link": "http://arxiv.org/abs/2209.06058v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared\n  Task", "abstract": "We present the joint contribution of IST and Unbabel to the WMT 2022 Shared\nTask on Quality Estimation (QE). Our team participated on all three subtasks:\n(i) Sentence and Word-level Quality Prediction; (ii) Explainable QE; and (iii)\nCritical Error Detection. For all tasks we build on top of the COMET framework,\nconnecting it with the predictor-estimator architecture of OpenKiwi, and\nequipping it with a word-level sequence tagger and an explanation extractor.\nOur results suggest that incorporating references during pretraining improves\nperformance across several language pairs on downstream tasks, and that jointly\ntraining with sentence and word-level objectives yields a further boost.\nFurthermore, combining attention and gradient information proved to be the top\nstrategy for extracting good explanations of sentence-level QE models. Overall,\nour submissions achieved the best results for all three tasks for almost all\nlanguage pairs by a considerable margin.", "published": "2022-09-13 18:05:12", "link": "http://arxiv.org/abs/2209.06243v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks\n  from The New Yorker Caption Contest", "abstract": "Large neural networks can now generate jokes, but do they really \"understand\"\nhumor? We challenge AI models with three tasks derived from the New Yorker\nCartoon Caption Contest: matching a joke to a cartoon, identifying a winning\ncaption, and explaining why a winning caption is funny. These tasks encapsulate\nprogressively more sophisticated aspects of \"understanding\" a cartoon; key\nelements are the complex, often surprising relationships between images and\ncaptions and the frequent inclusion of indirect and playful allusions to human\nexperience and culture. We investigate both multimodal and language-only\nmodels: the former are challenged with the cartoon images directly, while the\nlatter are given multifaceted descriptions of the visual scene to simulate\nhuman-level visual understanding. We find that both types of models struggle at\nall three tasks. For example, our best multimodal models fall 30 accuracy\npoints behind human performance on the matching task, and, even when provided\nground-truth visual scene descriptors, human-authored explanations are\npreferred head-to-head over the best machine-authored ones (few-shot GPT-4) in\nmore than 2/3 of cases. We release models, code, leaderboard, and corpus, which\nincludes newly-gathered annotations describing the image's locations/entities,\nwhat's unusual in the scene, and an explanation of the joke.", "published": "2022-09-13 20:54:00", "link": "http://arxiv.org/abs/2209.06293v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "CovidMis20: COVID-19 Misinformation Detection System on Twitter Tweets\n  using Deep Learning Models", "abstract": "Online news and information sources are convenient and accessible ways to\nlearn about current issues. For instance, more than 300 million people engage\nwith posts on Twitter globally, which provides the possibility to disseminate\nmisleading information. There are numerous cases where violent crimes have been\ncommitted due to fake news. This research presents the CovidMis20 dataset\n(COVID-19 Misinformation 2020 dataset), which consists of 1,375,592 tweets\ncollected from February to July 2020. CovidMis20 can be automatically updated\nto fetch the latest news and is publicly available at:\nhttps://github.com/everythingguy/CovidMis20. This research was conducted using\nBi-LSTM deep learning and an ensemble CNN+Bi-GRU for fake news detection. The\nresults showed that, with testing accuracy of 92.23% and 90.56%, respectively,\nthe ensemble CNN+Bi-GRU model consistently provided higher accuracy than the\nBi-LSTM model.", "published": "2022-09-13 00:43:44", "link": "http://arxiv.org/abs/2209.05667v1", "categories": ["cs.LG", "cs.CL", "cs.HC", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Pre-trained Language Models for the Legal Domain: A Case Study on Indian\n  Law", "abstract": "NLP in the legal domain has seen increasing success with the emergence of\nTransformer-based Pre-trained Language Models (PLMs) pre-trained on legal text.\nPLMs trained over European and US legal text are available publicly; however,\nlegal text from other domains (countries), such as India, have a lot of\ndistinguishing characteristics. With the rapidly increasing volume of Legal NLP\napplications in various countries, it has become necessary to pre-train such\nLMs over legal text of other countries as well. In this work, we attempt to\ninvestigate pre-training in the Indian legal domain. We re-train (continue\npre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian\nlegal data, as well as train a model from scratch with a vocabulary based on\nIndian legal text. We apply these PLMs over three benchmark legal NLP tasks --\nLegal Statute Identification from facts, Semantic Segmentation of Court\nJudgment Documents, and Court Appeal Judgment Prediction -- over both Indian\nand non-Indian (EU, UK) datasets. We observe that our approach not only\nenhances performance on the new domain (Indian texts) but also over the\noriginal domain (European and UK texts). We also conduct explainability\nexperiments for a qualitative comparison of all these different PLMs.", "published": "2022-09-13 15:01:11", "link": "http://arxiv.org/abs/2209.06049v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Acoustic-Linguistic Features for Modeling Neurological Task Score in\n  Alzheimer's", "abstract": "The average life expectancy is increasing globally due to advancements in\nmedical technology, preventive health care, and a growing emphasis on\ngerontological health. Therefore, developing technologies that detect and track\naging-associated disease in cognitive function among older adult populations is\nimperative. In particular, research related to automatic detection and\nevaluation of Alzheimer's disease (AD) is critical given the disease's\nprevalence and the cost of current methods. As AD impacts the acoustics of\nspeech and vocabulary, natural language processing and machine learning provide\npromising techniques for reliably detecting AD. We compare and contrast the\nperformance of ten linear regression models for predicting Mini-Mental Status\nExam scores on the ADReSS challenge dataset. We extracted 13000+ handcrafted\nand learned features that capture linguistic and acoustic phenomena. Using a\nsubset of 54 top features selected by two methods: (1) recursive elimination\nand (2) correlation scores, we outperform a state-of-the-art baseline for the\nsame task. Upon scoring and evaluating the statistical significance of each of\nthe selected subset of features for each model, we find that, for the given\ntask, handcrafted linguistic features are more significant than acoustic and\nlearned features.", "published": "2022-09-13 15:35:31", "link": "http://arxiv.org/abs/2209.06085v1", "categories": ["cs.CE", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CE"}
{"title": "Analysis of Self-Attention Head Diversity for Conformer-based Automatic\n  Speech Recognition", "abstract": "Attention layers are an integral part of modern end-to-end automatic speech\nrecognition systems, for instance as part of the Transformer or Conformer\narchitecture. Attention is typically multi-headed, where each head has an\nindependent set of learned parameters and operates on the same input feature\nsequence. The output of multi-headed attention is a fusion of the outputs from\nthe individual heads. We empirically analyze the diversity between\nrepresentations produced by the different attention heads and demonstrate that\nthe heads become highly correlated during the course of training. We\ninvestigate a few approaches to increasing attention head diversity, including\nusing different attention mechanisms for each head and auxiliary training loss\nfunctions to promote head diversity. We show that introducing\ndiversity-promoting auxiliary loss functions during training is a more\neffective approach, and obtain WER improvements of up to 6% relative on the\nLibrispeech corpus. Finally, we draw a connection between the diversity of\nattention heads and the similarity of the gradients of head parameters.", "published": "2022-09-13 15:50:03", "link": "http://arxiv.org/abs/2209.06096v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story\n  Continuation", "abstract": "Recent advances in text-to-image synthesis have led to large pretrained\ntransformers with excellent capabilities to generate visualizations from a\ngiven text. However, these models are ill-suited for specialized tasks like\nstory visualization, which requires an agent to produce a sequence of images\ngiven a corresponding sequence of captions, forming a narrative. Moreover, we\nfind that the story visualization task fails to accommodate generalization to\nunseen plots and characters in new narratives. Hence, we first propose the task\nof story continuation, where the generated visual story is conditioned on a\nsource image, allowing for better generalization to narratives with new\ncharacters. Then, we enhance or 'retro-fit' the pretrained text-to-image\nsynthesis models with task-specific modules for (a) sequential image generation\nand (b) copying relevant elements from an initial frame. Then, we explore\nfull-model finetuning, as well as prompt-based tuning for parameter-efficient\nadaptation, of the pre-trained model. We evaluate our approach StoryDALL-E on\ntwo existing datasets, PororoSV and FlintstonesSV, and introduce a new dataset\nDiDeMoSV collected from a video-captioning dataset. We also develop a model\nStoryGANc based on Generative Adversarial Networks (GAN) for story\ncontinuation, and compare it with the StoryDALL-E model to demonstrate the\nadvantages of our approach. We show that our retro-fitting approach outperforms\nGAN-based models for story continuation and facilitates copying of visual\nelements from the source image, thereby improving continuity in the generated\nvisual story. Finally, our analysis suggests that pretrained transformers\nstruggle to comprehend narratives containing several characters. Overall, our\nwork demonstrates that pretrained text-to-image synthesis models can be adapted\nfor complex and low-resource tasks like story continuation.", "published": "2022-09-13 17:47:39", "link": "http://arxiv.org/abs/2209.06192v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue Twisters\n  Automatically", "abstract": "Tongue twisters are meaningful sentences that are difficult to pronounce. The\nprocess of automatically generating tongue twisters is challenging since the\ngenerated utterance must satisfy two conditions at once: phonetic difficulty\nand semantic meaning. Furthermore, phonetic difficulty is itself hard to\ncharacterize and is expressed in natural tongue twisters through a\nheterogeneous mix of phenomena such as alliteration and homophony. In this\npaper, we propose PANCETTA: Phoneme Aware Neural Completion to Elicit Tongue\nTwisters Automatically. We leverage phoneme representations to capture the\nnotion of phonetic difficulty, and we train language models to generate\noriginal tongue twisters on two proposed task settings. To do this, we curate a\ndataset called PANCETTA, consisting of existing English tongue twisters.\nThrough automatic and human evaluation, as well as qualitative analysis, we\nshow that PANCETTA generates novel, phonetically difficult, fluent, and\nsemantically meaningful tongue twisters.", "published": "2022-09-13 19:46:15", "link": "http://arxiv.org/abs/2209.06275v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Alexa, Let's Work Together: Introducing the First Alexa Prize TaskBot\n  Challenge on Conversational Task Assistance", "abstract": "Since its inception in 2016, the Alexa Prize program has enabled hundreds of\nuniversity students to explore and compete to develop conversational agents\nthrough the SocialBot Grand Challenge. The goal of the challenge is to build\nagents capable of conversing coherently and engagingly with humans on popular\ntopics for 20 minutes, while achieving an average rating of at least 4.0/5.0.\nHowever, as conversational agents attempt to assist users with increasingly\ncomplex tasks, new conversational AI techniques and evaluation platforms are\nneeded. The Alexa Prize TaskBot challenge, established in 2021, builds on the\nsuccess of the SocialBot challenge by introducing the requirements of\ninteractively assisting humans with real-world Cooking and Do-It-Yourself\ntasks, while making use of both voice and visual modalities. This challenge\nrequires the TaskBots to identify and understand the user's need, identify and\nintegrate task and domain knowledge into the interaction, and develop new ways\nof engaging the user without distracting them from the task at hand, among\nother challenges. This paper provides an overview of the TaskBot challenge,\ndescribes the infrastructure support provided to the teams with the CoBot\nToolkit, and summarizes the approaches the participating teams took to overcome\nthe research challenges. Finally, it analyzes the performance of the competing\nTaskBots during the first year of the competition.", "published": "2022-09-13 22:01:42", "link": "http://arxiv.org/abs/2209.06321v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "I.2.7; J.0; H.5.1; H.5.2"], "primary_category": "cs.CL"}
{"title": "Improving Language Model Prompting in Support of Semi-autonomous Task\n  Learning", "abstract": "Language models (LLMs) offer potential as a source of knowledge for agents\nthat need to acquire new task competencies within a performance environment. We\ndescribe efforts toward a novel agent capability that can construct cues (or\n\"prompts\") that result in useful LLM responses for an agent learning a new\ntask. Importantly, responses must not only be \"reasonable\" (a measure used\ncommonly in research on knowledge extraction from LLMs) but also specific to\nthe agent's task context and in a form that the agent can interpret given its\nnative language capacities. We summarize a series of empirical investigations\nof prompting strategies and evaluate responses against the goals of targeted\nand actionable responses for task learning. Our results demonstrate that\nactionable task knowledge can be obtained from LLMs in support of online agent\ntask learning.", "published": "2022-09-13 15:36:01", "link": "http://arxiv.org/abs/2209.07636v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7; I.2.11"], "primary_category": "cs.LG"}
{"title": "OCR for TIFF Compressed Document Images Directly in Compressed Domain\n  Using Text segmentation and Hidden Markov Model", "abstract": "In today's technological era, document images play an important and integral\npart in our day to day life, and specifically with the surge of Covid-19,\ndigitally scanned documents have become key source of communication, thus\navoiding any sort of infection through physical contact. Storage and\ntransmission of scanned document images is a very memory intensive task, hence\ncompression techniques are being used to reduce the image size before archival\nand transmission. To extract information or to operate on the compressed\nimages, we have two ways of doing it. The first way is to decompress the image\nand operate on it and subsequently compress it again for the efficiency of\nstorage and transmission. The other way is to use the characteristics of the\nunderlying compression algorithm to directly process the images in their\ncompressed form without involving decompression and re-compression. In this\npaper, we propose a novel idea of developing an OCR for CCITT (The\nInternational Telegraph and Telephone Consultative Committee) compressed\nmachine printed TIFF document images directly in the compressed domain. After\nsegmenting text regions into lines and words, HMM is applied for recognition\nusing three coding modes of CCITT- horizontal, vertical and the pass mode.\nExperimental results show that OCR on pass modes give a promising results.", "published": "2022-09-13 06:34:26", "link": "http://arxiv.org/abs/2209.09118v1", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing\n  N-gram Language Models", "abstract": "Although over 300M around the world speak Bangla, scant work has been done in\nimproving Bangla voice-to-text transcription due to Bangla being a low-resource\nlanguage. However, with the introduction of the Bengali Common Voice 9.0 speech\ndataset, Automatic Speech Recognition (ASR) models can now be significantly\nimproved. With 399hrs of speech recordings, Bengali Common Voice is the largest\nand most diversified open-source Bengali speech corpus in the world. In this\npaper, we outperform the SOTA pretrained Bengali ASR models by finetuning a\npretrained wav2vec2 model on the common voice dataset. We also demonstrate how\nto significantly improve the performance of an ASR model by adding an n-gram\nlanguage model as a post-processor. Finally, we do some experiments and\nhyperparameter tuning to generate a robust Bangla ASR model that is better than\nthe existing ASR models.", "published": "2022-09-13 17:59:21", "link": "http://arxiv.org/abs/2209.12650v1", "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AI-powered Language Assessment Tools for Dementia", "abstract": "The main objective of this paper is to propose an approach for developing an\nArtificial Intelligence (AI)-powered Language Assessment (LA) tool. Such tools\ncan be used to assess language impairments associated with dementia in older\nadults. The Machine Learning (ML) classifiers are the main parts of our\nproposed approach, therefore to develop an accurate tool with high sensitivity\nand specificity, we consider different binary classifiers and evaluate their\nperformances. We also assess the reliability and validity of our approach by\ncomparing the impact of different types of language tasks, features, and\nrecording media on the performance of ML classifiers.", "published": "2022-09-13 16:34:46", "link": "http://arxiv.org/abs/2209.12652v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Subsampling for Knowledge Graph Embedding Explained", "abstract": "In this article, we explain the recent advance of subsampling methods in\nknowledge graph embedding (KGE) starting from the original one used in\nword2vec.", "published": "2022-09-13 09:17:46", "link": "http://arxiv.org/abs/2209.12801v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Binaural Signal Representations for Joint Sound Event Detection and\n  Acoustic Scene Classification", "abstract": "Sound event detection (SED) and Acoustic scene classification (ASC) are two\nwidely researched audio tasks that constitute an important part of research on\nacoustic scene analysis. Considering shared information between sound events\nand acoustic scenes, performing both tasks jointly is a natural part of a\ncomplex machine listening system. In this paper, we investigate the usefulness\nof several spatial audio features in training a joint deep neural network (DNN)\nmodel performing SED and ASC. Experiments are performed for two different\ndatasets containing binaural recordings and synchronous sound event and\nacoustic scene labels to analyse the differences between performing SED and ASC\nseparately or jointly. The presented results show that the use of specific\nbinaural features, mainly the Generalized Cross Correlation with Phase\nTransform (GCC-phat) and sines and cosines of phase differences, result in a\nbetter performing model in both separate and joint tasks as compared with\nbaseline methods based on logmel energies only.", "published": "2022-09-13 11:29:00", "link": "http://arxiv.org/abs/2209.05900v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Distributed Acoustic Sensor System for Intelligent Transportation\n  using Deep Learning", "abstract": "Intelligent transport systems (ITS) are pivotal in the development of\nsustainable and green urban living. ITS is data-driven and enabled by the\nprofusion of sensors ranging from pneumatic tubes to smart cameras. This work\nexplores a novel data source based on optical fibre-based distributed acoustic\nsensors (DAS) for traffic analysis. Detecting the type of vehicle and\nestimating the occupancy of vehicles are prime concerns in ITS. The first is\nmotivated by the need for tracking, controlling, and forecasting traffic flow.\nThe second targets the regulation of high occupancy vehicle lanes in an attempt\nto reduce emissions and congestion. These tasks are often conducted by\nindividuals inspecting vehicles or through the use of emerging computer vision\ntechnologies. The former is not scale-able nor efficient whereas the latter is\nintrusive to passengers' privacy. To this end, we propose a deep learning\ntechnique to analyse DAS signals to address this challenge through continuous\nsensing and without exposing personal information. We propose a deep learning\nmethod for processing DAS signals and achieve 92% vehicle classification\naccuracy and 92-97% in occupancy detection based on DAS data collected under\ncontrolled conditions.", "published": "2022-09-13 13:23:30", "link": "http://arxiv.org/abs/2209.05978v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "SongDriver: Real-time Music Accompaniment Generation without Logical\n  Latency nor Exposure Bias", "abstract": "Real-time music accompaniment generation has a wide range of applications in\nthe music industry, such as music education and live performances. However,\nautomatic real-time music accompaniment generation is still understudied and\noften faces a trade-off between logical latency and exposure bias. In this\npaper, we propose SongDriver, a real-time music accompaniment generation system\nwithout logical latency nor exposure bias. Specifically, SongDriver divides one\naccompaniment generation task into two phases: 1) The arrangement phase, where\na Transformer model first arranges chords for input melodies in real-time, and\ncaches the chords for the next phase instead of playing them out. 2) The\nprediction phase, where a CRF model generates playable multi-track\naccompaniments for the coming melodies based on previously cached chords. With\nthis two-phase strategy, SongDriver directly generates the accompaniment for\nthe upcoming melody, achieving zero logical latency. Furthermore, when\npredicting chords for a timestep, SongDriver refers to the cached chords from\nthe first phase rather than its previous predictions, which avoids the exposure\nbias problem. Since the input length is often constrained under real-time\nconditions, another potential problem is the loss of long-term sequential\ninformation. To make up for this disadvantage, we extract four musical features\nfrom a long-term music piece before the current time step as global\ninformation. In the experiment, we train SongDriver on some open-source\ndatasets and an original \\`aiSong Dataset built from Chinese-style modern pop\nmusic scores. The results show that SongDriver outperforms existing SOTA\n(state-of-the-art) models on both objective and subjective metrics, meanwhile\nsignificantly reducing the physical latency.", "published": "2022-09-13 15:05:27", "link": "http://arxiv.org/abs/2209.06054v2", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Automated detection of pronunciation errors in non-native English speech\n  employing deep learning", "abstract": "Despite significant advances in recent years, the existing Computer-Assisted\nPronunciation Training (CAPT) methods detect pronunciation errors with a\nrelatively low accuracy (precision of 60% at 40%-80% recall). This Ph.D. work\nproposes novel deep learning methods for detecting pronunciation errors in\nnon-native (L2) English speech, outperforming the state-of-the-art method in\nAUC metric (Area under the Curve) by 41%, i.e., from 0.528 to 0.749. One of the\nproblems with existing CAPT methods is the low availability of annotated\nmispronounced speech needed for reliable training of pronunciation error\ndetection models. Therefore, the detection of pronunciation errors is\nreformulated to the task of generating synthetic mispronounced speech.\nIntuitively, if we could mimic mispronounced speech and produce any amount of\ntraining data, detecting pronunciation errors would be more effective.\nFurthermore, to eliminate the need to align canonical and recognized phonemes,\na novel end-to-end multi-task technique to directly detect pronunciation errors\nwas proposed. The pronunciation error detection models have been used at Amazon\nto automatically detect pronunciation errors in synthetic speech to accelerate\nthe research into new speech synthesis methods. It was demonstrated that the\nproposed deep learning methods are applicable in the tasks of detecting and\nreconstructing dysarthric speech.", "published": "2022-09-13 19:09:49", "link": "http://arxiv.org/abs/2209.06265v1", "categories": ["eess.AS", "cs.SD", "q-bio.OT"], "primary_category": "eess.AS"}
{"title": "Deep Speech Synthesis from Articulatory Representations", "abstract": "In the articulatory synthesis task, speech is synthesized from input features\ncontaining information about the physical behavior of the human vocal tract.\nThis task provides a promising direction for speech synthesis research, as the\narticulatory space is compact, smooth, and interpretable. Current works have\nhighlighted the potential for deep learning models to perform articulatory\nsynthesis. However, it remains unclear whether these models can achieve the\nefficiency and fidelity of the human speech production system. To help bridge\nthis gap, we propose a time-domain articulatory synthesis methodology and\ndemonstrate its efficacy with both electromagnetic articulography (EMA) and\nsynthetic articulatory feature inputs. Our model is computationally efficient\nand achieves a transcription word error rate (WER) of 18.5% for the\nEMA-to-speech task, yielding an improvement of 11.6% compared to prior work.\nThrough interpolation experiments, we also highlight the generalizability and\ninterpretability of our approach.", "published": "2022-09-13 22:51:47", "link": "http://arxiv.org/abs/2209.06337v1", "categories": ["eess.AS", "cs.SD", "q-bio.QM"], "primary_category": "eess.AS"}
