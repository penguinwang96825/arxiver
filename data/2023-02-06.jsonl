{"title": "Guide the Learner: Controlling Product of Experts Debiasing Method Based\n  on Token Attribution Similarities", "abstract": "Several proposals have been put forward in recent years for improving\nout-of-distribution (OOD) performance through mitigating dataset biases. A\npopular workaround is to train a robust model by re-weighting training examples\nbased on a secondary biased model. Here, the underlying assumption is that the\nbiased model resorts to shortcut features. Hence, those training examples that\nare correctly predicted by the biased model are flagged as being biased and are\ndown-weighted during the training of the main model. However, assessing the\nimportance of an instance merely based on the predictions of the biased model\nmay be too naive. It is possible that the prediction of the main model can be\nderived from another decision-making process that is distinct from the behavior\nof the biased model. To circumvent this, we introduce a fine-tuning strategy\nthat incorporates the similarity between the main and biased model attribution\nscores in a Product of Experts (PoE) loss function to further improve OOD\nperformance. With experiments conducted on natural language inference and fact\nverification benchmarks, we show that our method improves OOD results while\nmaintaining in-distribution (ID) performance.", "published": "2023-02-06 15:21:41", "link": "http://arxiv.org/abs/2302.02852v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoFT: Enhancing Faithfulness and Diversity for Table-to-Text Generation\n  via Logic Form Control", "abstract": "Logical Table-to-Text (LT2T) generation is tasked with generating logically\nfaithful sentences from tables. There currently exists two challenges in the\nfield: 1) Faithfulness: how to generate sentences that are factually correct\ngiven the table content; 2) Diversity: how to generate multiple sentences that\noffer different perspectives on the table. This work proposes LoFT, which\nutilizes logic forms as fact verifiers and content planners to control LT2T\ngeneration. Experimental results on the LogicNLG dataset demonstrate that LoFT\nis the first model that addresses unfaithfulness and lack of diversity issues\nsimultaneously. Our code is publicly available at\nhttps://github.com/Yale-LILY/LoFT.", "published": "2023-02-06 17:49:27", "link": "http://arxiv.org/abs/2302.02962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Erasure of Unaligned Attributes from Neural Representations", "abstract": "We present the Assignment-Maximization Spectral Attribute removaL (AMSAL)\nalgorithm, which erases information from neural representations when the\ninformation to be erased is implicit rather than directly being aligned to each\ninput example. Our algorithm works by alternating between two steps. In one, it\nfinds an assignment of the input representations to the information to be\nerased, and in the other, it creates projections of both the input\nrepresentations and the information to be erased into a joint latent space. We\ntest our algorithm on an extensive array of datasets, including a Twitter\ndataset with multiple guarded attributes, the BiasBios dataset and the\nBiasBench benchmark. The last benchmark includes four datasets with various\ntypes of protected attributes. Our results demonstrate that bias can often be\nremoved in our setup. We also discuss the limitations of our approach when\nthere is a strong entanglement between the main task and the information to be\nerased.", "published": "2023-02-06 18:32:17", "link": "http://arxiv.org/abs/2302.02997v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Gloss Augmentation for Improving Arabic Target Sense\n  Verification", "abstract": "Arabic language lacks semantic datasets and sense inventories. The most\ncommon semantically-labeled dataset for Arabic is the ArabGlossBERT, a\nrelatively small dataset that consists of 167K context-gloss pairs (about 60K\npositive and 107K negative pairs), collected from Arabic dictionaries. This\npaper presents an enrichment to the ArabGlossBERT dataset, by augmenting it\nusing (Arabic-English-Arabic) machine back-translation. Augmentation increased\nthe dataset size to 352K pairs (149K positive and 203K negative pairs). We\nmeasure the impact of augmentation using different data configurations to\nfine-tune BERT on target sense verification (TSV) task. Overall, the accuracy\nranges between 78% to 84% for different data configurations. Although our\napproach performed at par with the baseline, we did observe some improvements\nfor some POS tags in some experiments. Furthermore, our fine-tuned models are\ntrained on a larger dataset covering larger vocabulary and contexts. We provide\nan in-depth analysis of the accuracy for each part-of-speech (POS).", "published": "2023-02-06 21:24:02", "link": "http://arxiv.org/abs/2302.03126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's about Time: Rethinking Evaluation on Rumor Detection Benchmarks\n  using Chronological Splits", "abstract": "New events emerge over time influencing the topics of rumors in social media.\nCurrent rumor detection benchmarks use random splits as training, development\nand test sets which typically results in topical overlaps. Consequently, models\ntrained on random splits may not perform well on rumor classification on\npreviously unseen topics due to the temporal concept drift. In this paper, we\nprovide a re-evaluation of classification models on four popular rumor\ndetection benchmarks considering chronological instead of random splits. Our\nexperimental results show that the use of random splits can significantly\noverestimate predictive performance across all datasets and models. Therefore,\nwe suggest that rumor detection models should always be evaluated using\nchronological splits for minimizing topical overlaps.", "published": "2023-02-06 22:53:13", "link": "http://arxiv.org/abs/2302.03147v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolution of grammatical forms: some quantitative approaches", "abstract": "Grammatical forms are said to evolve via two main mechanisms. These are,\nrespectively, the `descent' mechanism, where current forms can be seen to have\ndescended (albeit with occasional modifications) from their roots in ancient\nlanguages, and the `contact' mechanism, where evolution in a given language\noccurs via borrowing from other languages with which it is in contact. We use\nideas and concepts from statistical physics to formulate a series of static and\ndynamical models which illustrate these issues in general terms. The static\nmodels emphasise the relative numbers of rules and exceptions, while the\ndynamical models focus on the emergence of exceptional forms. These unlikely\nsurvivors among various competing grammatical forms are winners against the\nodds. Our analysis suggests that they emerge when the influence of neighbouring\nlanguages exceeds the generic tendency towards regularisation within individual\nlanguages.", "published": "2023-02-06 09:50:48", "link": "http://arxiv.org/abs/2302.02655v1", "categories": ["cond-mat.stat-mech", "cs.CL"], "primary_category": "cond-mat.stat-mech"}
{"title": "Chain of Hindsight Aligns Language Models with Feedback", "abstract": "Learning from human preferences is important for language models to match\nhuman needs and to align with human and social values. Prior works have\nachieved remarkable successes by learning from human feedback to understand and\nfollow instructions. Nonetheless, these methods are either founded on\nhand-picked model generations that are favored by human annotators, rendering\nthem inefficient in terms of data utilization and challenging to apply in\ngeneral, or they depend on reinforcement learning, which often suffers from\nimperfect reward functions and relies on extremely challenging optimizations.\nIn this work, we propose a novel technique, Chain of Hindsight, that is easy to\noptimize and can learn from any form of feedback, regardless of its polarity.\nOur idea is inspired by how humans learn from extensive feedback presented in\nthe form of languages. We convert all types of feedback into sequences of\nsentences, which are then used to fine-tune the model, allowing us to take\nadvantage of the language comprehension capabilities of language models. We\ncondition the model on a sequence of model generations paired with feedback. By\ndoing so, the model is trained to generate outputs based on feedback, while\nlearning to identify and correct negative attributes or errors. Applying our\nmethod to large language models, we observed that Chain of Hindsight\nsignificantly surpasses previous methods in aligning language models with human\npreferences. We report significant improvements on summarization and dialogue\nbenchmarks, with our approach markedly preferred in human evaluations.", "published": "2023-02-06 10:28:16", "link": "http://arxiv.org/abs/2302.02676v8", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Coherence and Diversity through Noise: Self-Supervised Paraphrase\n  Generation via Structure-Aware Denoising", "abstract": "In this paper, we propose SCANING, an unsupervised framework for paraphrasing\nvia controlled noise injection. We focus on the novel task of paraphrasing\nalgebraic word problems having practical applications in online pedagogy as a\nmeans to reduce plagiarism as well as ensure understanding on the part of the\nstudent instead of rote memorization. This task is more complex than\nparaphrasing general-domain corpora due to the difficulty in preserving\ncritical information for solution consistency of the paraphrased word problem,\nmanaging the increased length of the text and ensuring diversity in the\ngenerated paraphrase. Existing approaches fail to demonstrate adequate\nperformance on at least one, if not all, of these facets, necessitating the\nneed for a more comprehensive solution. To this end, we model the noising\nsearch space as a composition of contextual and syntactic aspects and sample\nnoising functions consisting of either one or both aspects. This allows for\nlearning a denoising function that operates over both aspects and produces\nsemantically equivalent and syntactically diverse outputs through grounded\nnoise injection. The denoising function serves as a foundation for learning a\nparaphrasing function which operates solely in the input-paraphrase space\nwithout carrying any direct dependency on noise. We demonstrate SCANING\nconsiderably improves performance in terms of both semantic preservation and\nproducing diverse paraphrases through extensive automated and manual evaluation\nacross 4 datasets.", "published": "2023-02-06 13:50:57", "link": "http://arxiv.org/abs/2302.02780v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Findings of the TSAR-2022 Shared Task on Multilingual Lexical\n  Simplification", "abstract": "We report findings of the TSAR-2022 shared task on multilingual lexical\nsimplification, organized as part of the Workshop on Text Simplification,\nAccessibility, and Readability TSAR-2022 held in conjunction with EMNLP 2022.\nThe task called the Natural Language Processing research community to\ncontribute with methods to advance the state of the art in multilingual lexical\nsimplification for English, Portuguese, and Spanish. A total of 14 teams\nsubmitted the results of their lexical simplification systems for the provided\ntest data. Results of the shared task indicate new benchmarks in Lexical\nSimplification with English lexical simplification quantitative results\nnoticeably higher than those obtained for Spanish and (Brazilian) Portuguese.", "published": "2023-02-06 15:53:51", "link": "http://arxiv.org/abs/2302.02888v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Controllable Lexical Simplification for English", "abstract": "Fine-tuning Transformer-based approaches have recently shown exciting results\non sentence simplification task. However, so far, no research has applied\nsimilar approaches to the Lexical Simplification (LS) task. In this paper, we\npresent ConLS, a Controllable Lexical Simplification system fine-tuned with T5\n(a Transformer-based model pre-trained with a BERT-style approach and several\nother tasks). The evaluation results on three datasets (LexMTurk, BenchLS, and\nNNSeval) have shown that our model performs comparable to LSBert (the current\nstate-of-the-art) and even outperforms it in some cases. We also conducted a\ndetailed comparison on the effectiveness of control tokens to give a clear view\nof how each token contributes to the model.", "published": "2023-02-06 16:09:27", "link": "http://arxiv.org/abs/2302.02900v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient and Flexible Topic Modeling using Pretrained Embeddings and\n  Bag of Sentences", "abstract": "Pre-trained language models have led to a new state-of-the-art in many NLP\ntasks. However, for topic modeling, statistical generative models such as LDA\nare still prevalent, which do not easily allow incorporating contextual word\nvectors. They might yield topics that do not align well with human judgment. In\nthis work, we propose a novel topic modeling and inference algorithm. We\nsuggest a bag of sentences (BoS) approach using sentences as the unit of\nanalysis. We leverage pre-trained sentence embeddings by combining generative\nprocess models and clustering. We derive a fast inference algorithm based on\nexpectation maximization, hard assignments, and an annealing process. The\nevaluation shows that our method yields state-of-the art results with\nrelatively little computational demands. Our method is also more flexible\ncompared to prior works leveraging word embeddings, since it provides the\npossibility to customize topic-document distributions using priors. Code and\ndata is at \\url{https://github.com/JohnTailor/BertSenClu}.", "published": "2023-02-06 20:13:11", "link": "http://arxiv.org/abs/2302.03106v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Techniques to Improve Neural Math Word Problem Solvers", "abstract": "Developing automatic Math Word Problem (MWP) solvers is a challenging task\nthat demands the ability of understanding and mathematical reasoning over the\nnatural language. Recent neural-based approaches mainly encode the problem text\nusing a language model and decode a mathematical expression over quantities and\noperators iteratively. Note the problem text of a MWP consists of a context\npart and a question part, a recent work finds these neural solvers may only\nperform shallow pattern matching between the context text and the golden\nexpression, where question text is not well used. Meanwhile, existing decoding\nprocesses fail to enforce the mathematical laws into the design, where the\nrepresentations for mathematical equivalent expressions are different. To\naddress these two issues, we propose a new encoder-decoder architecture that\nfully leverages the question text and preserves step-wise commutative law.\nBesides generating quantity embeddings, our encoder further encodes the\nquestion text and uses it to guide the decoding process. At each step, our\ndecoder uses Deep Sets to compute expression representations so that these\nembeddings are invariant under any permutation of quantities. Experiments on\nfour established benchmarks demonstrate that our framework outperforms\nstate-of-the-art neural MWP solvers, showing the effectiveness of our\ntechniques. We also conduct a detailed analysis of the results to show the\nlimitations of our approach and further discuss the potential future work. Code\nis available at https://github.com/sophistz/Question-Aware-Deductive-MWP.", "published": "2023-02-06 22:41:51", "link": "http://arxiv.org/abs/2302.03145v1", "categories": ["cs.CL", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Data Selection for Language Models via Importance Resampling", "abstract": "Selecting a suitable pretraining dataset is crucial for both general-domain\n(e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We\nformalize this problem as selecting a subset of a large raw unlabeled dataset\nto match a desired target distribution given unlabeled target samples. Due to\nthe scale and dimensionality of the raw text data, existing methods use simple\nheuristics or require human experts to manually curate data. Instead, we extend\nthe classic importance resampling approach used in low-dimensions for LM data\nselection. We propose Data Selection with Importance Resampling (DSIR), an\nefficient and scalable framework that estimates importance weights in a reduced\nfeature space for tractability and selects data with importance resampling\naccording to these weights. We instantiate the DSIR framework with hashed\nn-gram features for efficiency, enabling the selection of 100M documents from\nthe full Pile dataset in 4.5 hours. To measure whether hashed n-gram features\npreserve the aspects of the data that are relevant to the target, we define KL\nreduction, a data metric that measures the proximity between the selected\npretraining data and the target on some feature space. Across 8 data selection\nmethods (including expert selection), KL reduction on hashed n-gram features\nhighly correlates with average downstream accuracy (r=0.82). When selecting\ndata for continued pretraining on a specific domain, DSIR performs comparably\nto expert curation across 8 target distributions. When pretraining\ngeneral-domain models (target is Wikipedia and books), DSIR improves over\nrandom selection and heuristic filtering baselines by 2-2.5% on the GLUE\nbenchmark. Code is available at https://github.com/p-lambda/dsir.", "published": "2023-02-06 23:57:56", "link": "http://arxiv.org/abs/2302.03169v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model\n  for Noisy Labels and Long-tailed Learning", "abstract": "Practical natural language processing (NLP) tasks are commonly long-tailed\nwith noisy labels. Those problems challenge the generalization and robustness\nof complex models such as Deep Neural Networks (DNNs). Some commonly used\nresampling techniques, such as oversampling or undersampling, could easily lead\nto overfitting. It is growing popular to learn the data weights leveraging a\nsmall amount of metadata. Besides, recent studies have shown the advantages of\nself-supervised pre-training, particularly to the under-represented data. In\nthis work, we propose a general framework to handle the problem of both\nlong-tail and noisy labels. The model is adapted to the domain of problems in a\ncontrastive learning manner. The re-weighting module is a feed-forward network\nthat learns explicit weighting functions and adapts weights according to\nmetadata. The framework further adapts weights of terms in the loss function\nthrough a combination of the polynomial expansion of cross-entropy loss and\nfocal loss. Our extensive experiments show that the proposed framework\nconsistently outperforms baseline methods. Lastly, our sensitive analysis\nemphasizes the capability of the proposed framework to handle the long-tailed\nproblem and mitigate the negative impact of noisy labels.", "published": "2023-02-06 18:40:04", "link": "http://arxiv.org/abs/2302.03488v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Less is More: Understanding Word-level Textual Adversarial Attack via\n  n-gram Frequency Descend", "abstract": "Word-level textual adversarial attacks have demonstrated notable efficacy in\nmisleading Natural Language Processing (NLP) models. Despite their success, the\nunderlying reasons for their effectiveness and the fundamental characteristics\nof adversarial examples (AEs) remain obscure. This work aims to interpret\nword-level attacks by examining their $n$-gram frequency patterns. Our\ncomprehensive experiments reveal that in approximately 90\\% of cases,\nword-level attacks lead to the generation of examples where the frequency of\n$n$-grams decreases, a tendency we term as the $n$-gram Frequency Descend\n($n$-FD). This finding suggests a straightforward strategy to enhance model\nrobustness: training models using examples with $n$-FD. To examine the\nfeasibility of this strategy, we employed the $n$-gram frequency information,\nas an alternative to conventional loss gradients, to generate perturbed\nexamples in adversarial training. The experiment results indicate that the\nfrequency-based approach performs comparably with the gradient-based approach\nin improving model robustness. Our research offers a novel and more intuitive\nperspective for understanding word-level textual adversarial attacks and\nproposes a new direction to improve model robustness.", "published": "2023-02-06 05:11:27", "link": "http://arxiv.org/abs/2302.02568v4", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Migration Reframed? A multilingual analysis on the stance shift in\n  Europe during the Ukrainian crisis", "abstract": "The war in Ukraine seems to have positively changed the attitude toward the\ncritical societal topic of migration in Europe -- at least towards refugees\nfrom Ukraine. We investigate whether this impression is substantiated by how\nthe topic is reflected in online news and social media, thus linking the\nrepresentation of the issue on the Web to its perception in society. For this\npurpose, we combine and adapt leading-edge automatic text processing for a\nnovel multilingual stance detection approach. Starting from 5.5M Twitter posts\npublished by 565 European news outlets in one year, beginning September 2021,\nplus replies, we perform a multilingual analysis of migration-related media\ncoverage and associated social media interaction for Europe and selected\nEuropean countries.\n  The results of our analysis show that there is actually a reframing of the\ndiscussion illustrated by the terminology change, e.g., from \"migrant\" to\n\"refugee\", often even accentuated with phrases such as \"real refugees\".\nHowever, concerning a stance shift in public perception, the picture is more\ndiverse than expected. All analyzed cases show a noticeable temporal stance\nshift around the start of the war in Ukraine. Still, there are apparent\nnational differences in the size and stability of this shift.", "published": "2023-02-06 14:36:33", "link": "http://arxiv.org/abs/2302.02813v2", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Protecting Language Generation Models via Invisible Watermarking", "abstract": "Language generation models have been an increasingly powerful enabler for\nmany applications. Many such models offer free or affordable API access, which\nmakes them potentially vulnerable to model extraction attacks through\ndistillation. To protect intellectual property (IP) and ensure fair use of\nthese models, various techniques such as lexical watermarking and synonym\nreplacement have been proposed. However, these methods can be nullified by\nobvious countermeasures such as \"synonym randomization\". To address this issue,\nwe propose GINSEW, a novel method to protect text generation models from being\nstolen through distillation. The key idea of our method is to inject secret\nsignals into the probability vector of the decoding steps for each target\ntoken. We can then detect the secret message by probing a suspect model to tell\nif it is distilled from the protected one. Experimental results show that\nGINSEW can effectively identify instances of IP infringement with minimal\nimpact on the generation quality of protected APIs. Our method demonstrates an\nabsolute improvement of 19 to 29 points on mean average precision (mAP) in\ndetecting suspects compared to previous methods against watermark removal\nattacks.", "published": "2023-02-06 23:42:03", "link": "http://arxiv.org/abs/2302.03162v3", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "A Categorical Archive of ChatGPT Failures", "abstract": "Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.", "published": "2023-02-06 04:21:59", "link": "http://arxiv.org/abs/2302.03494v8", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improved Vehicle Sub-type Classification for Acoustic Traffic Monitoring", "abstract": "The detection and classification of vehicles on the road is a crucial task\nfor traffic monitoring. Usually, Computer Vision (CV) algorithms dominate the\ntask of vehicle classification on the road, but CV methodologies might suffer\nin poor lighting conditions and require greater amounts of computational power.\nAdditionally, there is a privacy concern with installing cameras in sensitive\nand secure areas. In contrast, acoustic traffic monitoring is cost-effective,\nand can provide greater accuracy, particularly in low lighting conditions and\nin places where cameras cannot be installed. In this paper, we consider the\ntask of acoustic vehicle sub-type classification, where we classify acoustic\nsignals into 4 classes: car, truck, bike, and no vehicle. We experimented with\nMel spectrograms, MFCC and GFCC as features and performed data pre-processing\nto train a simple, well optimized CNN that performs well at the task. When used\nwith MFCC as features and careful data pre-processing, our proposed methodology\nimproves upon the established state-of-the-art baseline on the IDMT Traffic\ndataset with an accuracy of 98.95%.", "published": "2023-02-06 17:26:51", "link": "http://arxiv.org/abs/2302.02945v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Residual Information in Deep Speaker Embedding Architectures", "abstract": "Speaker embeddings represent a means to extract representative vectorial\nrepresentations from a speech signal such that the representation pertains to\nthe speaker identity alone. The embeddings are commonly used to classify and\ndiscriminate between different speakers. However, there is no objective measure\nto evaluate the ability of a speaker embedding to disentangle the speaker\nidentity from the other speech characteristics. This means that the embeddings\nare far from ideal, highly dependent on the training corpus and still include a\ndegree of residual information pertaining to factors such as linguistic\ncontent, recording conditions or speaking style of the utterance. This paper\nintroduces an analysis over six sets of speaker embeddings extracted with some\nof the most recent and high-performing DNN architectures, and in particular,\nthe degree to which they are able to truly disentangle the speaker identity\nfrom the speech signal. To correctly evaluate the architectures, a large\nmulti-speaker parallel speech dataset is used. The dataset includes 46 speakers\nuttering the same set of prompts, recorded in either a professional studio or\ntheir home environments. The analysis looks into the intra- and inter-speaker\nsimilarity measures computed over the different embedding sets, as well as if\nsimple classification and regression methods are able to extract several\nresidual information factors from the speaker embeddings. The results show that\nthe discriminative power of the analyzed embeddings is very high, yet across\nall the analyzed architectures, residual information is still present in the\nrepresentations in the form of a high correlation to the recording conditions,\nlinguistic contents and utterance duration.", "published": "2023-02-06 12:37:57", "link": "http://arxiv.org/abs/2302.02742v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Audio Representation Learning by Distilling Video as Privileged\n  Information", "abstract": "Deep audio representation learning using multi-modal audio-visual data often\nleads to a better performance compared to uni-modal approaches. However, in\nreal-world scenarios both modalities are not always available at the time of\ninference, leading to performance degradation by models trained for multi-modal\ninference. In this work, we propose a novel approach for deep audio\nrepresentation learning using audio-visual data when the video modality is\nabsent at inference. For this purpose, we adopt teacher-student knowledge\ndistillation under the framework of learning using privileged information\n(LUPI). While the previous methods proposed for LUPI use soft-labels generated\nby the teacher, in our proposed method we use embeddings learned by the teacher\nto train the student network. We integrate our method in two different\nsettings: sequential data where the features are divided into multiple segments\nthroughout time, and non-sequential data where the entire features are treated\nas one whole segment. In the non-sequential setting both the teacher and\nstudent networks are comprised of an encoder component and a task header. We\nuse the embeddings produced by the encoder component of the teacher to train\nthe encoder of the student, while the task header of the student is trained\nusing ground-truth labels. In the sequential setting, the networks have an\nadditional aggregation component that is placed between the encoder and task\nheader. We use two sets of embeddings produced by the encoder and aggregation\ncomponent of the teacher to train the student. Similar to the non-sequential\nsetting, the task header of the student network is trained using ground-truth\nlabels. We test our framework on two different audio-visual tasks, namely\nspeaker recognition and speech emotion recognition and show considerable\nimprovements over sole audio-based recognition as well as prior works that use\nLUPI.", "published": "2023-02-06 15:09:34", "link": "http://arxiv.org/abs/2302.02845v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Autodecompose: A generative self-supervised model for semantic\n  decomposition", "abstract": "We introduce Autodecompose, a novel self-supervised generative model that\ndecomposes data into two semantically independent properties: the desired\nproperty, which captures a specific aspect of the data (e.g. the voice in an\naudio signal), and the context property, which aggregates all other information\n(e.g. the content of the audio signal), without any labels given. Autodecompose\nuses two complementary augmentations, one that manipulates the context while\npreserving the desired property and the other that manipulates the desired\nproperty while preserving the context. The augmented variants of the data are\nencoded by two encoders and reconstructed by a decoder. We prove that one of\nthe encoders embeds the desired property while the other embeds the context\nproperty. We apply Autodecompose to audio signals to encode sound source (human\nvoice) and content. We pre-trained the model on YouTube and LibriSpeech\ndatasets and fine-tuned in a self-supervised manner without exposing the\nlabels. Our results showed that, using the sound source encoder of pre-trained\nAutodecompose, a linear classifier achieves F1 score of 97.6\\% in recognizing\nthe voice of 30 speakers using only 10 seconds of labeled samples, compared to\n95.7\\% for supervised models. Additionally, our experiments showed that\nAutodecompose is robust against overfitting even when a large model is\npre-trained on a small dataset. A large Autodecompose model was pre-trained\nfrom scratch on 60 seconds of audio from 3 speakers achieved over 98.5\\% F1\nscore in recognizing those three speakers in other unseen utterances. We\nfinally show that the context encoder embeds information about the content of\nthe speech and ignores the sound source information.\n  Our sample code for training the model, as well as examples for using the\npre-trained models are available here:\n\\url{https://github.com/rezabonyadi/autodecompose}", "published": "2023-02-06 21:18:09", "link": "http://arxiv.org/abs/2302.03124v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
