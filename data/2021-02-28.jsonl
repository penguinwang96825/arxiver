{"title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based\n  Bias in NLP", "abstract": "When trained on large, unfiltered crawls from the internet, language models\npick up and reproduce all kinds of undesirable biases that can be found in the\ndata: they often generate racist, sexist, violent or otherwise toxic language.\nAs large models require millions of training examples to achieve good\nperformance, it is difficult to completely prevent them from being exposed to\nsuch content. In this paper, we first demonstrate a surprising finding:\npretrained language models recognize, to a considerable degree, their\nundesirable biases and the toxicity of the content they produce. We refer to\nthis capability as self-diagnosis. Based on this finding, we then propose a\ndecoding algorithm that, given only a textual description of the undesired\nbehavior, reduces the probability of a language model producing problematic\ntext. We refer to this approach as self-debiasing. Self-debiasing does not rely\non manually curated word lists, nor does it require any training data or\nchanges to the model's parameters. While we by no means eliminate the issue of\nlanguage models generating biased text, we believe our approach to be an\nimportant step in this direction.", "published": "2021-02-28 11:07:37", "link": "http://arxiv.org/abs/2103.00453v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP-CUET@DravidianLangTech-EACL2021: Offensive Language Detection from\n  Multilingual Code-Mixed Text using Transformers", "abstract": "The increasing accessibility of the internet facilitated social media usage\nand encouraged individuals to express their opinions liberally. Nevertheless,\nit also creates a place for content polluters to disseminate offensive posts or\ncontents. Most of such offensive posts are written in a cross-lingual manner\nand can easily evade the online surveillance systems. This paper presents an\nautomated system that can identify offensive text from multilingual code-mixed\ndata. In the task, datasets provided in three languages including Tamil,\nMalayalam and Kannada code-mixed with English where participants are asked to\nimplement separate models for each language. To accomplish the tasks, we\nemployed two machine learning techniques (LR, SVM), three deep learning (LSTM,\nLSTM+Attention) techniques and three transformers (m-BERT, Indic-BERT, XLM-R)\nbased methods. Results show that XLM-R outperforms other techniques in Tamil\nand Malayalam languages while m-BERT achieves the highest score in the Kannada\nlanguage. The proposed models gained weighted $f_1$ score of $0.76$ (for\nTamil), $0.93$ (for Malayalam), and $0.71$ (for Kannada) with a rank of\n$3^{rd}$, $5^{th}$ and $4^{th}$ respectively.", "published": "2021-02-28 11:10:32", "link": "http://arxiv.org/abs/2103.00455v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP-CUET@LT-EDI-EACL2021: Multilingual Code-Mixed Hope Speech Detection\n  using Cross-lingual Representation Learner", "abstract": "In recent years, several systems have been developed to regulate the spread\nof negativity and eliminate aggressive, offensive or abusive contents from the\nonline platforms. Nevertheless, a limited number of researches carried out to\nidentify positive, encouraging and supportive contents. In this work, our goal\nis to identify whether a social media post/comment contains hope speech or not.\nWe propose three distinct models to identify hope speech in English, Tamil and\nMalayalam language to serve this purpose. To attain this goal, we employed\nvarious machine learning (support vector machine, logistic regression,\nensemble), deep learning (convolutional neural network + long short term\nmemory) and transformer (m-BERT, Indic-BERT, XLNet, XLM-Roberta) based methods.\nResults indicate that XLM-Roberta outdoes all other techniques by gaining a\nweighted $f_1$-score of $0.93$, $0.60$ and $0.85$ respectively for English,\nTamil and Malayalam language. Our team has achieved $1^{st}$, $2^{nd}$ and\n$1^{st}$ rank in these three tasks respectively.", "published": "2021-02-28 11:30:52", "link": "http://arxiv.org/abs/2103.00464v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RuSentEval: Linguistic Source, Encoder Force!", "abstract": "The success of pre-trained transformer language models has brought a great\ndeal of interest on how these models work, and what they learn about language.\nHowever, prior research in the field is mainly devoted to English, and little\nis known regarding other languages. To this end, we introduce RuSentEval, an\nenhanced set of 14 probing tasks for Russian, including ones that have not been\nexplored yet. We apply a combination of complementary probing methods to\nexplore the distribution of various linguistic properties in five multilingual\ntransformers for two typologically contrasting languages -- Russian and\nEnglish. Our results provide intriguing findings that contradict the common\nunderstanding of how linguistic knowledge is represented, and demonstrate that\nsome properties are learned in a similar manner despite the language\ndifferences.", "published": "2021-02-28 17:43:42", "link": "http://arxiv.org/abs/2103.00573v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Moroccan Dialect -Darija- Open Dataset", "abstract": "Darija Open Dataset (DODa) is an open-source project for the Moroccan\ndialect. With more than 10,000 entries DODa is arguably the largest open-source\ncollaborative project for Darija-English translation built for Natural Language\nProcessing purposes. In fact, besides semantic categorization, DODa also adopts\na syntactic one, presents words under different spellings, offers verb-to-noun\nand masculine-to-feminine correspondences, contains the conjugation of hundreds\nof verbs in different tenses, and many other subsets to help researchers better\nunderstand and study Moroccan dialect. This data paper presents a description\nof DODa, its features, how it was collected, as well as a first application in\nImage Classification using ImageNet labels translated to Darija. This\ncollaborative project is hosted on GitHub platform under MIT's Open-Source\nlicense and aims to be a standard resource for researchers, students, and\nanyone who is interested in Moroccan Dialect", "published": "2021-02-28 13:37:59", "link": "http://arxiv.org/abs/2103.09687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NLP-CUET@DravidianLangTech-EACL2021: Investigating Visual and Textual\n  Features to Identify Trolls from Multimodal Social Media Memes", "abstract": "In the past few years, the meme has become a new way of communication on the\nInternet. As memes are the images with embedded text, it can quickly spread\nhate, offence and violence. Classifying memes are very challenging because of\ntheir multimodal nature and region-specific interpretation. A shared task is\norganized to develop models that can identify trolls from multimodal social\nmedia memes. This work presents a computational model that we have developed as\npart of our participation in the task. Training data comes in two forms: an\nimage with embedded Tamil code-mixed text and an associated caption given in\nEnglish. We investigated the visual and textual features using CNN, VGG16,\nInception, Multilingual-BERT, XLM-Roberta, XLNet models. Multimodal features\nare extracted by combining image (CNN, ResNet50, Inception) and text (Long\nshort term memory network) features via early fusion approach. Results indicate\nthat the textual approach with XLNet achieved the highest weighted $f_1$-score\nof $0.58$, which enabled our model to secure $3^{rd}$ rank in this task.", "published": "2021-02-28 11:36:50", "link": "http://arxiv.org/abs/2103.00466v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CREATe: Clinical Report Extraction and Annotation Technology", "abstract": "Clinical case reports are written descriptions of the unique aspects of a\nparticular clinical case, playing an essential role in sharing clinical\nexperiences about atypical disease phenotypes and new therapies. However, to\nour knowledge, there has been no attempt to develop an end-to-end system to\nannotate, index, or otherwise curate these reports. In this paper, we propose a\nnovel computational resource platform, CREATe, for extracting, indexing, and\nquerying the contents of clinical case reports. CREATe fosters an environment\nof sustainable resource support and discovery, enabling researchers to overcome\nthe challenges of information science. An online video of the demonstration can\nbe viewed at https://youtu.be/Q8owBQYTjDc.", "published": "2021-02-28 16:50:14", "link": "http://arxiv.org/abs/2103.00562v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LRG at TREC 2020: Document Ranking with XLNet-Based Models", "abstract": "Establishing a good information retrieval system in popular mediums of\nentertainment is a quickly growing area of investigation for companies and\nresearchers alike. We delve into the domain of information retrieval for\npodcasts. In Spotify's Podcast Challenge, we are given a user's query with a\ndescription to find the most relevant short segment from the given dataset\nhaving all the podcasts. Previous techniques that include solely classical\nInformation Retrieval (IR) techniques, perform poorly when descriptive queries\nare presented. On the other hand, models which exclusively rely on large neural\nnetworks tend to perform better. The downside to this technique is that a\nconsiderable amount of time and computing power are required to infer the\nresult. We experiment with two hybrid models which first filter out the best\npodcasts based on user's query with a classical IR technique, and then perform\nre-ranking on the shortlisted documents based on the detailed description using\na transformer-based model.", "published": "2021-02-28 03:04:29", "link": "http://arxiv.org/abs/2103.00380v2", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Alignment Knowledge Distillation for Online Streaming Attention-based\n  Speech Recognition", "abstract": "This article describes an efficient training method for online streaming\nattention-based encoder-decoder (AED) automatic speech recognition (ASR)\nsystems. AED models have achieved competitive performance in offline scenarios\nby jointly optimizing all components. They have recently been extended to an\nonline streaming framework via models such as monotonic chunkwise attention\n(MoChA). However, the elaborate attention calculation process is not robust for\nlong-form speech utterances. Moreover, the sequence-level training objective\nand time-restricted streaming encoder cause a nonnegligible delay in token\nemission during inference. To address these problems, we propose CTC\nsynchronous training (CTC-ST), in which CTC alignments are leveraged as a\nreference for token boundaries to enable a MoChA model to learn optimal\nmonotonic input-output alignments. We formulate a purely end-to-end training\nobjective to synchronize the boundaries of MoChA to those of CTC. The CTC model\nshares an encoder with the MoChA model to enhance the encoder representation.\nMoreover, the proposed method provides alignment information learned in the CTC\nbranch to the attention-based decoder. Therefore, CTC-ST can be regarded as\nself-distillation of alignment knowledge from CTC to MoChA. Experimental\nevaluations on a variety of benchmark datasets show that the proposed method\nsignificantly reduces recognition errors and emission latency simultaneously.\nThe robustness to long-form and noisy speech is also demonstrated. We compare\nCTC-ST with several methods that distill alignment knowledge from a hybrid ASR\nsystem and show that the CTC-ST can achieve a comparable tradeoff of accuracy\nand latency without relying on external alignment information. The best MoChA\nsystem shows recognition accuracy comparable to that of RNN-transducer (RNN-T)\nwhile achieving lower emission latency.", "published": "2021-02-28 08:17:38", "link": "http://arxiv.org/abs/2103.00422v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Topic Modelling Meets Deep Neural Networks: A Survey", "abstract": "Topic modelling has been a successful technique for text analysis for almost\ntwenty years. When topic modelling met deep neural networks, there emerged a\nnew and increasingly popular research area, neural topic models, with over a\nhundred models developed and a wide range of applications in neural language\nunderstanding such as text generation, summarisation and language models. There\nis a need to summarise research developments and discuss open problems and\nfuture directions. In this paper, we provide a focused yet comprehensive\noverview of neural topic models for interested researchers in the AI community,\nso as to facilitate them to navigate and innovate in this fast-growing research\narea. To the best of our knowledge, ours is the first review focusing on this\nspecific topic.", "published": "2021-02-28 12:59:28", "link": "http://arxiv.org/abs/2103.00498v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Citizen Participation and Machine Learning for a Better Democracy", "abstract": "The development of democratic systems is a crucial task as confirmed by its\nselection as one of the Millennium Sustainable Development Goals by the United\nNations. In this article, we report on the progress of a project that aims to\naddress barriers, one of which is information overload, to achieving effective\ndirect citizen participation in democratic decision-making processes. The main\nobjectives are to explore if the application of Natural Language Processing\n(NLP) and machine learning can improve citizens' experience of digital citizen\nparticipation platforms. Taking as a case study the \"Decide Madrid\" Consul\nplatform, which enables citizens to post proposals for policies they would like\nto see adopted by the city council, we used NLP and machine learning to provide\nnew ways to (a) suggest to citizens proposals they might wish to support; (b)\ngroup citizens by interests so that they can more easily interact with each\nother; (c) summarise comments posted in response to proposals; (d) assist\ncitizens in aggregating and developing proposals. Evaluation of the results\nconfirms that NLP and machine learning have a role to play in addressing some\nof the barriers users of platforms such as Consul currently experience.", "published": "2021-02-28 13:30:07", "link": "http://arxiv.org/abs/2103.00508v1", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Conversational Humor Analysis and Design", "abstract": "Well-defined jokes can be divided neatly into a setup and a punchline. While\nmost works on humor today talk about a joke as a whole, the idea of generating\npunchlines to a setup has applications in conversational humor, where funny\nremarks usually occur with a non-funny context. Thus, this paper is based\naround two core concepts: Classification and the Generation of a punchline from\na particular setup based on the Incongruity Theory. We first implement a\nfeature-based machine learning model to classify humor. For humor generation,\nwe use a neural model, and then merge the classical rule-based approaches with\nthe neural approach to create a hybrid model. The idea behind being: combining\ninsights gained from other tasks with the setup-punchline model and thus\napplying it to existing text generation approaches. We then use and compare our\nmodel with human written jokes with the help of human evaluators in a\ndouble-blind study.", "published": "2021-02-28 15:22:57", "link": "http://arxiv.org/abs/2103.00536v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Brain Signals to Rescue Aphasia, Apraxia and Dysarthria Speech\n  Recognition", "abstract": "In this paper, we propose a deep learning-based algorithm to improve the\nperformance of automatic speech recognition (ASR) systems for aphasia, apraxia,\nand dysarthria speech by utilizing electroencephalography (EEG) features\nrecorded synchronously with aphasia, apraxia, and dysarthria speech. We\ndemonstrate a significant decoding performance improvement by more than 50\\%\nduring test time for isolated speech recognition task and we also provide\npreliminary results indicating performance improvement for the more challenging\ncontinuous speech recognition task by utilizing EEG features. The results\npresented in this paper show the first step towards demonstrating the\npossibility of utilizing non-invasive neural signals to design a real-time\nrobust speech prosthetic for stroke survivors recovering from aphasia, apraxia,\nand dysarthria. Our aphasia, apraxia, and dysarthria speech-EEG data set will\nbe released to the public to help further advance this interesting and crucial\nresearch.", "published": "2021-02-28 03:27:02", "link": "http://arxiv.org/abs/2103.00383v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.QM"], "primary_category": "cs.SD"}
{"title": "Exploiting Attention-based Sequence-to-Sequence Architectures for Sound\n  Event Localization", "abstract": "Sound event localization frameworks based on deep neural networks have shown\nincreased robustness with respect to reverberation and noise in comparison to\nclassical parametric approaches. In particular, recurrent architectures that\nincorporate temporal context into the estimation process seem to be well-suited\nfor this task. This paper proposes a novel approach to sound event localization\nby utilizing an attention-based sequence-to-sequence model. These types of\nmodels have been successfully applied to problems in natural language\nprocessing and automatic speech recognition. In this work, a multi-channel\naudio signal is encoded to a latent representation, which is subsequently\ndecoded to a sequence of estimated directions-of-arrival. Herein, attentions\nallow for capturing temporal dependencies in the audio signal by focusing on\nspecific frames that are relevant for estimating the activity and\ndirection-of-arrival of sound events at the current time-step. The framework is\nevaluated on three publicly available datasets for sound event localization. It\nyields superior localization performance compared to state-of-the-art methods\nin both anechoic and reverberant conditions.", "published": "2021-02-28 07:52:20", "link": "http://arxiv.org/abs/2103.00417v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
