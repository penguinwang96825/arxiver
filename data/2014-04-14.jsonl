{"title": "Meta-evaluation of comparability metrics using parallel corpora", "abstract": "Metrics for measuring the comparability of corpora or texts need to be\ndeveloped and evaluated systematically. Applications based on a corpus, such as\ntraining Statistical MT systems in specialised narrow domains, require finding\na reasonable balance between the size of the corpus and its consistency, with\ncontrolled and benchmarked levels of comparability for any newly added\nsections. In this article we propose a method that can meta-evaluate\ncomparability metrics by calculating monolingual comparability scores\nseparately on the 'source' and 'target' sides of parallel corpora. The range of\nscores on the source side is then correlated (using Pearson's r coefficient)\nwith the range of 'target' scores; the higher the correlation - the more\nreliable is the metric. The intuition is that a good metric should yield the\nsame distance between different domains in different languages. Our method\ngives consistent results for the same metrics on different data sets, which\nindicates that it is reliable and can be used for metric comparison or for\noptimising settings of parametrised metrics.", "published": "2014-04-14 21:33:42", "link": "http://arxiv.org/abs/1404.3759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
