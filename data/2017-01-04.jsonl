{"title": "Joint Semantic Synthesis and Morphological Analysis of the Derived Word", "abstract": "Much like sentences are composed of words, words themselves are composed of\nsmaller units. For example, the English word questionably can be analyzed as\nquestion+able+ly. However, this structural decomposition of the word does not\ndirectly give us a semantic representation of the word's meaning. Since\nmorphology obeys the principle of compositionality, the semantics of the word\ncan be systematically derived from the meaning of its parts. In this work, we\npropose a novel probabilistic model of word formation that captures both the\nanalysis of a word w into its constituents segments and the synthesis of the\nmeaning of w from the meanings of those segments. Our model jointly learns to\nsegment words into morphemes and compose distributional semantic vectors of\nthose morphemes. We experiment with the model on English CELEX data and German\nDerivBase (Zeller et al., 2013) data. We show that jointly modeling semantics\nincreases both segmentation accuracy and morpheme F1 by between 3% and 5%.\nAdditionally, we investigate different models of vector composition, showing\nthat recurrent neural networks yield an improvement over simple additive\nmodels. Finally, we study the degree to which the representations correspond to\na linguist's notion of morphological productivity.", "published": "2017-01-04 10:13:02", "link": "http://arxiv.org/abs/1701.00946v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Textual Entailment with Structured Attentions and Composition", "abstract": "Deep learning techniques are increasingly popular in the textual entailment\ntask, overcoming the fragility of traditional discrete models with hard\nalignments and logics. In particular, the recently proposed attention models\n(Rockt\\\"aschel et al., 2015; Wang and Jiang, 2015) achieves state-of-the-art\naccuracy by computing soft word alignments between the premise and hypothesis\nsentences. However, there remains a major limitation: this line of work\ncompletely ignores syntax and recursion, which is helpful in many traditional\nefforts. We show that it is beneficial to extend the attention model to tree\nnodes between premise and hypothesis. More importantly, this subtree-level\nattention reveals information about entailment relation. We study the recursive\ncomposition of this subtree-level entailment relation, which can be viewed as a\nsoft version of the Natural Logic framework (MacCartney and Manning, 2009).\nExperiments show that our structured attention and entailment composition model\ncan correctly identify and infer entailment relations from the bottom up, and\nbring significant improvements in accuracy.", "published": "2017-01-04 19:14:37", "link": "http://arxiv.org/abs/1701.01126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "World Literature According to Wikipedia: Introduction to a DBpedia-Based\n  Framework", "abstract": "Among the manifold takes on world literature, it is our goal to contribute to\nthe discussion from a digital point of view by analyzing the representation of\nworld literature in Wikipedia with its millions of articles in hundreds of\nlanguages. As a preliminary, we introduce and compare three different\napproaches to identify writers on Wikipedia using data from DBpedia, a\ncommunity project with the goal of extracting and providing structured\ninformation from Wikipedia. Equipped with our basic set of writers, we analyze\nhow they are represented throughout the 15 biggest Wikipedia language versions.\nWe combine intrinsic measures (mostly examining the connectedness of articles)\nwith extrinsic ones (analyzing how often articles are frequented by readers)\nand develop methods to evaluate our results. The better part of our findings\nseems to convey a rather conservative, old-fashioned version of world\nliterature, but a version derived from reproducible facts revealing an implicit\nliterary canon based on the editing and reading behavior of millions of people.\nWhile still having to solve some known issues, the introduced methods will help\nus build an observatory of world literature to further investigate its\nrepresentativeness and biases.", "published": "2017-01-04 13:06:18", "link": "http://arxiv.org/abs/1701.00991v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Neural Probabilistic Model for Non-projective MST Parsing", "abstract": "In this paper, we propose a probabilistic parsing model, which defines a\nproper conditional probability distribution over non-projective dependency\ntrees for a given sentence, using neural representations as inputs. The neural\nnetwork architecture is based on bi-directional LSTM-CNNs which benefits from\nboth word- and character-level representations automatically, by using\ncombination of bidirectional LSTM and CNN. On top of the neural network, we\nintroduce a probabilistic structured layer, defining a conditional log-linear\nmodel over non-projective trees. We evaluate our model on 17 different\ndatasets, across 14 different languages. By exploiting Kirchhoff's Matrix-Tree\nTheorem (Tutte, 1984), the partition functions and marginals can be computed\nefficiently, leading to a straight-forward end-to-end model training procedure\nvia back-propagation. Our parser achieves state-of-the-art parsing performance\non nine datasets.", "published": "2017-01-04 00:10:17", "link": "http://arxiv.org/abs/1701.00874v4", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
