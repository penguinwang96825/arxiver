{"title": "Translating Pro-Drop Languages with Reconstruction Models", "abstract": "Pronouns are frequently omitted in pro-drop languages, such as Chinese,\ngenerally leading to significant challenges with respect to the production of\ncomplete translations. To date, very little attention has been paid to the\ndropped pronoun (DP) problem within neural machine translation (NMT). In this\nwork, we propose a novel reconstruction-based approach to alleviating DP\ntranslation problems for NMT models. Firstly, DPs within all source sentences\nare automatically annotated with parallel information extracted from the\nbilingual training corpus. Next, the annotated source sentence is reconstructed\nfrom hidden representations in the NMT model. With auxiliary training\nobjectives, in terms of reconstruction scores, the parameters associated with\nthe NMT model are guided to produce enhanced hidden representations that are\nencouraged as much as possible to embed annotated DP information. Experimental\nresults on both Chinese-English and Japanese-English dialogue translation tasks\nshow that the proposed approach significantly and consistently improves\ntranslation performance over a strong NMT baseline, which is directly built on\nthe training data annotated with DPs.", "published": "2018-01-10 07:53:22", "link": "http://arxiv.org/abs/1801.03257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MilkQA: a Dataset of Consumer Questions for the Task of Answer Selection", "abstract": "We introduce MilkQA, a question answering dataset from the dairy domain\ndedicated to the study of consumer questions. The dataset contains 2,657 pairs\nof questions and answers, written in the Portuguese language and originally\ncollected by the Brazilian Agricultural Research Corporation (Embrapa). All\nquestions were motivated by real situations and written by thousands of authors\nwith very different backgrounds and levels of literacy, while answers were\nelaborated by specialists from Embrapa's customer service. Our dataset was\nfiltered and anonymized by three human annotators. Consumer questions are a\nchallenging kind of question that is usually employed as a form of seeking\ninformation. Although several question answering datasets are available, most\nof such resources are not suitable for research on answer selection models for\nconsumer questions. We aim to fill this gap by making MilkQA publicly\navailable. We study the behavior of four answer selection models on MilkQA: two\nbaseline models and two convolutional neural network archictetures. Our results\nshow that MilkQA poses real challenges to computational models, particularly\ndue to linguistic characteristics of its questions and to their unusually\nlonger lengths. Only one of the experimented models gives reasonable results,\nat the cost of high computational requirements.", "published": "2018-01-10 17:16:36", "link": "http://arxiv.org/abs/1801.03460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Part-of-Speech Induction", "abstract": "Part-of-Speech (POS) tagging is an old and fundamental task in natural\nlanguage processing. While supervised POS taggers have shown promising\naccuracy, it is not always feasible to use supervised methods due to lack of\nlabeled data. In this project, we attempt to unsurprisingly induce POS tags by\niteratively looking for a recurring pattern of words through a hierarchical\nagglomerative clustering process. Our approach shows promising results when\ncompared to the tagging results of the state-of-the-art unsupervised POS\ntaggers.", "published": "2018-01-10 21:47:26", "link": "http://arxiv.org/abs/1801.03564v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fooling End-to-end Speaker Verification by Adversarial Examples", "abstract": "Automatic speaker verification systems are increasingly used as the primary\nmeans to authenticate costumers. Recently, it has been proposed to train\nspeaker verification systems using end-to-end deep neural models. In this\npaper, we show that such systems are vulnerable to adversarial example attack.\nAdversarial examples are generated by adding a peculiar noise to original\nspeaker examples, in such a way that they are almost indistinguishable from the\noriginal examples by a human listener. Yet, the generated waveforms, which\nsound as speaker A can be used to fool such a system by claiming as if the\nwaveforms were uttered by speaker B. We present white-box attacks on an\nend-to-end deep network that was either trained on YOHO or NTIMIT. We also\npresent two black-box attacks: where the adversarial examples were generated\nwith a system that was trained on YOHO, but the attack is on a system that was\ntrained on NTIMIT; and when the adversarial examples were generated with a\nsystem that was trained on Mel-spectrum feature set, but the attack is on a\nsystem that was trained on MFCC. Results suggest that the accuracy of the\nattacked system was decreased and the false-positive rate was dramatically\nincreased.", "published": "2018-01-10 12:24:34", "link": "http://arxiv.org/abs/1801.03339v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
