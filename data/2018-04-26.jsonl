{"title": "Integrating Local Context and Global Cohesiveness for Open Information\n  Extraction", "abstract": "Extracting entities and their relations from text is an important task for\nunderstanding massive text corpora. Open information extraction (IE) systems\nmine relation tuples (i.e., entity arguments and a predicate string to describe\ntheir relation) from sentences. These relation tuples are not confined to a\npredefined schema for the relations of interests. However, current Open IE\nsystems focus on modeling local context information in a sentence to extract\nrelation tuples, while ignoring the fact that global statistics in a large\ncorpus can be collectively leveraged to identify high-quality sentence-level\nextractions. In this paper, we propose a novel Open IE system, called ReMine,\nwhich integrates local context signals and global structural signals in a\nunified, distant-supervision framework. Leveraging facts from external\nknowledge bases as supervision, the new system can be applied to many different\ndomains to facilitate sentence-level tuple extractions using corpus-level\nstatistics. Our system operates by solving a joint optimization problem to\nunify (1) segmenting entity/relation phrases in individual sentences based on\nlocal context; and (2) measuring the quality of tuples extracted from\nindividual sentences with a translating-based objective. Learning the two\nsubtasks jointly helps correct errors produced in each subtask so that they can\nmutually enhance each other. Experiments on two real-world corpora from\ndifferent domains demonstrate the effectiveness, generality, and robustness of\nReMine when compared to state-of-the-art open IE systems.", "published": "2018-04-26 08:10:58", "link": "http://arxiv.org/abs/1804.09931v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lessons from the Bible on Modern Topics: Low-Resource Multilingual Topic\n  Model Evaluation", "abstract": "Multilingual topic models enable document analysis across languages through\ncoherent multilingual summaries of the data. However, there is no standard and\neffective metric to evaluate the quality of multilingual topics. We introduce a\nnew intrinsic evaluation of multilingual topic models that correlates well with\nhuman judgments of multilingual topic coherence as well as performance in\ndownstream applications. Importantly, we also study evaluation for low-resource\nlanguages. Because standard metrics fail to accurately measure topic quality\nwhen robust external resources are unavailable, we propose an adaptation model\nthat improves the accuracy and reliability of these metrics in low-resource\nsettings.", "published": "2018-04-26 17:35:15", "link": "http://arxiv.org/abs/1804.10184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine\n  Translation", "abstract": "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq)\nmodeling for Machine Translation (MT). The classic RNN-based approaches to MT\nwere first out-performed by the convolutional seq2seq model, which was then\nout-performed by the more recent Transformer model. Each of these new\napproaches consists of a fundamental architecture accompanied by a set of\nmodeling and training techniques that are in principle applicable to other\nseq2seq architectures. In this paper, we tease apart the new architectures and\ntheir accompanying techniques in two ways. First, we identify several key\nmodeling and training techniques, and apply them to the RNN architecture,\nyielding a new RNMT+ model that outperforms all of the three fundamental\narchitectures on the benchmark WMT'14 English to French and English to German\ntasks. Second, we analyze the properties of each fundamental seq2seq\narchitecture and devise new hybrid architectures intended to combine their\nstrengths. Our hybrid models obtain further improvements, outperforming the\nRNMT+ model on both benchmark datasets.", "published": "2018-04-26 01:24:39", "link": "http://arxiv.org/abs/1804.09849v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pay Attention to Virality: understanding popularity of social media\n  videos with the attention mechanism", "abstract": "Predicting popularity of social media videos before they are published is a\nchallenging task, mainly due to the complexity of content distribution network\nas well as the number of factors that play part in this process. As solving\nthis task provides tremendous help for media content creators, many successful\nmethods were proposed to solve this problem with machine learning. In this\nwork, we change the viewpoint and postulate that it is not only the predicted\npopularity that matters, but also, maybe even more importantly, understanding\nof how individual parts influence the final popularity score. To that end, we\npropose to combine the Grad-CAM visualization method with a soft attention\nmechanism. Our preliminary results show that this approach allows for more\nintuitive interpretation of the content impact on video popularity, while\nachieving competitive results in terms of prediction accuracy.", "published": "2018-04-26 09:06:06", "link": "http://arxiv.org/abs/1804.09949v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Interactive Language Acquisition with One-shot Visual Concept Learning\n  through a Conversational Game", "abstract": "Building intelligent agents that can communicate with and learn from humans\nin natural language is of great value. Supervised language learning is limited\nby the ability of capturing mainly the statistics of training data, and is\nhardly adaptive to new scenarios or flexible for acquiring new knowledge\nwithout inefficient retraining or catastrophic forgetting. We highlight the\nperspective that conversational interaction serves as a natural interface both\nfor language learning and for novel knowledge acquisition and propose a joint\nimitation and reinforcement approach for grounded language learning through an\ninteractive conversational game. The agent trained with this approach is able\nto actively acquire information by asking questions about novel objects and use\nthe just-learned knowledge in subsequent conversations in a one-shot fashion.\nResults compared with other methods verified the effectiveness of the proposed\napproach.", "published": "2018-04-26 07:14:59", "link": "http://arxiv.org/abs/1805.00462v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Hierarchical Density Order Embeddings", "abstract": "By representing words with probability densities rather than point vectors,\nprobabilistic word embeddings can capture rich and interpretable semantic\ninformation and uncertainty. The uncertainty information can be particularly\nmeaningful in capturing entailment relationships -- whereby general words such\nas \"entity\" correspond to broad distributions that encompass more specific\nwords such as \"animal\" or \"instrument\". We introduce density order embeddings,\nwhich learn hierarchical representations through encapsulation of probability\ndensities. In particular, we propose simple yet effective loss functions and\ndistance metrics, as well as graph-based schemes to select negative samples to\nbetter learn hierarchical density representations. Our approach provides\nstate-of-the-art performance on the WordNet hypernym relationship prediction\ntask and the challenging HyperLex lexical entailment dataset -- while retaining\na rich and interpretable density representation.", "published": "2018-04-26 00:43:49", "link": "http://arxiv.org/abs/1804.09843v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "On deep speaker embeddings for text-independent speaker recognition", "abstract": "We investigate deep neural network performance in the textindependent speaker\nrecognition task. We demonstrate that using angular softmax activation at the\nlast classification layer of a classification neural network instead of a\nsimple softmax activation allows to train a more generalized discriminative\nspeaker embedding extractor. Cosine similarity is an effective metric for\nspeaker verification in this embedding space. We also address the problem of\nchoosing an architecture for the extractor. We found that deep networks with\nresidual frame level connections outperform wide but relatively shallow\narchitectures. This paper also proposes several improvements for previous\nDNN-based extractor systems to increase the speaker recognition accuracy. We\nshow that the discriminatively trained similarity metric learning approach\noutperforms the standard LDA-PLDA method as an embedding backend. The results\nobtained on Speakers in the Wild and NIST SRE 2016 evaluation sets demonstrate\nrobustness of the proposed systems when dealing with close to real-life\nconditions.", "published": "2018-04-26 14:22:01", "link": "http://arxiv.org/abs/1804.10080v1", "categories": ["cs.SD", "cs.CL", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Sounding Board: A User-Centric and Content-Driven Social Chatbot", "abstract": "We present Sounding Board, a social chatbot that won the 2017 Amazon Alexa\nPrize. The system architecture consists of several components including spoken\nlanguage processing, dialogue management, language generation, and content\nmanagement, with emphasis on user-centric and content-driven design. We also\nshare insights gained from large-scale online logs based on 160,000\nconversations with real-world users.", "published": "2018-04-26 08:11:16", "link": "http://arxiv.org/abs/1804.10202v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "End-to-End Speech Separation with Unfolded Iterative Phase\n  Reconstruction", "abstract": "This paper proposes an end-to-end approach for single-channel\nspeaker-independent multi-speaker speech separation, where time-frequency (T-F)\nmasking, the short-time Fourier transform (STFT), and its inverse are\nrepresented as layers within a deep network. Previous approaches, rather than\ncomputing a loss on the reconstructed signal, used a surrogate loss based on\nthe target STFT magnitudes. This ignores reconstruction error introduced by\nphase inconsistency. In our approach, the loss function is directly defined on\nthe reconstructed signals, which are optimized for best separation. In\naddition, we train through unfolded iterations of a phase reconstruction\nalgorithm, represented as a series of STFT and inverse STFT layers. While mask\nvalues are typically limited to lie between zero and one for approaches using\nthe mixture phase for reconstruction, this limitation is less relevant if the\nestimated magnitudes are to be used together with phase reconstruction. We thus\npropose several novel activation functions for the output layer of the T-F\nmasking, to allow mask values beyond one. On the publicly-available wsj0-2mix\ndataset, our approach achieves state-of-the-art 12.6 dB scale-invariant\nsignal-to-distortion ratio (SI-SDR) and 13.1 dB SDR, revealing new\npossibilities for deep learning based phase reconstruction and representing a\nfundamental progress towards solving the notoriously-hard cocktail party\nproblem.", "published": "2018-04-26 13:14:22", "link": "http://arxiv.org/abs/1804.10204v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Modeling Psychotherapy Dialogues with Kernelized Hashcode\n  Representations: A Nonparametric Information-Theoretic Approach", "abstract": "We propose a novel dialogue modeling framework, the first-ever nonparametric\nkernel functions based approach for dialogue modeling, which learns kernelized\nhashcodes as compressed text representations; unlike traditional deep learning\nmodels, it handles well relatively small datasets, while also scaling to large\nones. We also derive a novel lower bound on mutual information, used as a\nmodel-selection criterion favoring representations with better alignment\nbetween the utterances of participants in a collaborative dialogue setting, as\nwell as higher predictability of the generated responses. As demonstrated on\nthree real-life datasets, including prominently psychotherapy sessions, the\nproposed approach significantly outperforms several state-of-art neural network\nbased dialogue systems, both in terms of computational efficiency, reducing\ntraining time from days or weeks to hours, and the response quality, achieving\nan order of magnitude improvement over competitors in frequency of being chosen\nas the best model by human evaluators.", "published": "2018-04-26 17:39:28", "link": "http://arxiv.org/abs/1804.10188v7", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Detection of Glottal Closure Instants from Raw Speech using\n  Convolutional Neural Networks", "abstract": "Glottal Closure Instants (GCIs) correspond to the temporal locations of\nsignificant excitation to the vocal tract occurring during the production of\nvoiced speech. GCI detection from speech signals is a well-studied problem\ngiven its importance in speech processing. Most of the existing approaches for\nGCI detection adopt a two-stage approach (i) Transformation of speech signal\ninto a representative signal where GCIs are localized better, (ii) extraction\nof GCIs using the representative signal obtained in first stage. The former\nstage is accomplished using signal processing techniques based on the\nprinciples of speech production and the latter with heuristic-algorithms such\nas dynamic-programming and peak-picking. These methods are thus task-specific\nand rely on the methods used for representative signal extraction. However, in\nthis paper, we formulate the GCI detection problem from a representation\nlearning perspective where appropriate representation is implicitly learned\nfrom the raw-speech data samples. Specifically, GCI detection is cast as a\nsupervised multi-task learning problem solved using a deep convolutional neural\nnetwork jointly optimizing a classification and regression cost. The learning\ncapability is demonstrated with several experiments on standard datasets. The\nresults compare well with the state-of-the-art algorithms while performing\nbetter in the case of presence of real-world non-stationary noise.", "published": "2018-04-26 16:21:34", "link": "http://arxiv.org/abs/1804.10147v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive pooling operators for weakly labeled sound event detection", "abstract": "Sound event detection (SED) methods are tasked with labeling segments of\naudio recordings by the presence of active sound sources. SED is typically\nposed as a supervised machine learning problem, requiring strong annotations\nfor the presence or absence of each sound source at every time instant within\nthe recording. However, strong annotations of this type are both labor- and\ncost-intensive for human annotators to produce, which limits the practical\nscalability of SED methods.\n  In this work, we treat SED as a multiple instance learning (MIL) problem,\nwhere training labels are static over a short excerpt, indicating the presence\nor absence of sound sources but not their temporal locality. The models,\nhowever, must still produce temporally dynamic predictions, which must be\naggregated (pooled) when comparing against static labels during training. To\nfacilitate this aggregation, we develop a family of adaptive pooling\noperators---referred to as auto-pool---which smoothly interpolate between\ncommon pooling operators, such as min-, max-, or average-pooling, and\nautomatically adapt to the characteristics of the sound sources in question. We\nevaluate the proposed pooling operators on three datasets, and demonstrate that\nin each case, the proposed methods outperform non-adaptive pooling operators\nfor static prediction, and nearly match the performance of models trained with\nstrong, dynamic annotations. The proposed method is evaluated in conjunction\nwith convolutional neural networks, but can be readily applied to any\ndifferentiable model for time-series label prediction.", "published": "2018-04-26 14:01:12", "link": "http://arxiv.org/abs/1804.10070v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
