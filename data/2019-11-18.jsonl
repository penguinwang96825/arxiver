{"title": "Deep and Dense Sarcasm Detection", "abstract": "Recent work in automated sarcasm detection has placed a heavy focus on\ncontext and meta-data. Whilst certain utterances indeed require background\nknowledge and commonsense reasoning, previous works have only explored shallow\nmodels for capturing the lexical, syntactic and semantic cues present within a\ntext. In this paper, we propose a deep 56 layer network, implemented with dense\nconnectivity to model the isolated utterance and extract richer features\ntherein. We compare our approach against recent state-of-the-art architectures\nwhich make considerable use of extrinsic information, and demonstrate\ncompetitive results whilst using only the local features of the text. Further,\nwe provide an analysis of the dependency of prior convolution outputs in\ngenerating the final feature maps. Finally a case study is presented,\nsupporting that our approach accurately classifies additional uses of clear\nsarcasm, which a standard CNN misclassifies.", "published": "2019-11-18 07:49:13", "link": "http://arxiv.org/abs/1911.07474v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Short Text Language Identification for Under Resourced Languages", "abstract": "The paper presents a hierarchical naive Bayesian and lexicon based classifier\nfor short text language identification (LID) useful for under resourced\nlanguages. The algorithm is evaluated on short pieces of text for the 11\nofficial South African languages some of which are similar languages. The\nalgorithm is compared to recent approaches using test sets from previous works\non South African languages as well as the Discriminating between Similar\nLanguages (DSL) shared tasks' datasets. Remaining research opportunities and\npressing concerns in evaluating and comparing LID approaches are also\ndiscussed.", "published": "2019-11-18 11:34:38", "link": "http://arxiv.org/abs/1911.07555v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Graph Transformer for Graph-to-Sequence Learning", "abstract": "The dominant graph-to-sequence transduction models employ graph neural\nnetworks for graph representation learning, where the structural information is\nreflected by the receptive field of neurons. Unlike graph neural networks that\nrestrict the information exchange between immediate neighborhood, we propose a\nnew model, known as Graph Transformer, that uses explicit relation encoding and\nallows direct communication between two distant nodes. It provides a more\nefficient way for global graph structure modeling. Experiments on the\napplications of text generation from Abstract Meaning Representation (AMR) and\nsyntax-based neural machine translation show the superiority of our proposed\nmodel. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU\non LDC2017T10 for AMR-to-text generation, outperforming the state-of-the-art\nresults by up to 2.2 points. On the syntax-based translation tasks, our model\nestablishes new single-model state-of-the-art BLEU scores, 21.3 for\nEnglish-to-German and 14.1 for English-to-Czech, improving over the existing\nbest results, including ensembles, by over 1 BLEU.", "published": "2019-11-18 07:45:19", "link": "http://arxiv.org/abs/1911.07470v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Annotated Corpus of Reference Resolution for Interpreting Common\n  Grounding", "abstract": "Common grounding is the process of creating, repairing and updating mutual\nunderstandings, which is a fundamental aspect of natural language conversation.\nHowever, interpreting the process of common grounding is a challenging task,\nespecially under continuous and partially-observable context where complex\nambiguity, uncertainty, partial understandings and misunderstandings are\nintroduced. Interpretation becomes even more challenging when we deal with\ndialogue systems which still have limited capability of natural language\nunderstanding and generation. To address this problem, we consider reference\nresolution as the central subtask of common grounding and propose a new\nresource to study its intermediate process. Based on a simple and general\nannotation schema, we collected a total of 40,172 referring expressions in\n5,191 dialogues curated from an existing corpus, along with multiple judgements\nof referent interpretations. We show that our annotation is highly reliable,\ncaptures the complexity of common grounding through a natural degree of\nreasonable disagreements, and allows for more detailed and quantitative\nanalyses of common grounding strategies. Finally, we demonstrate the advantages\nof our annotation for interpreting, analyzing and improving common grounding in\nbaseline dialogue systems.", "published": "2019-11-18 12:41:25", "link": "http://arxiv.org/abs/1911.07588v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Universal and non-universal text statistics: Clustering coefficient for\n  language identification", "abstract": "In this work we analyze statistical properties of 91 relatively small texts\nin 7 different languages (Spanish, English, French, German, Turkish, Russian,\nIcelandic) as well as texts with randomly inserted spaces. Despite the size\n(around 11260 different words), the well known universal statistical laws --\nnamely Zipf and Herdan-Heap's laws -- are confirmed, and are in close agreement\nwith results obtained elsewhere. We also construct a word co-occurrence network\nof each text. While the degree distribution is again universal, we note that\nthe distribution of Clustering Coefficients, which depend strongly on the local\nstructure of networks, can be used to differentiate between languages, as well\nas to distinguish natural languages from random texts.", "published": "2019-11-18 21:39:19", "link": "http://arxiv.org/abs/1911.08915v2", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Multi-task Sentence Encoding Model for Semantic Retrieval in Question\n  Answering Systems", "abstract": "Question Answering (QA) systems are used to provide proper responses to\nusers' questions automatically. Sentence matching is an essential task in the\nQA systems and is usually reformulated as a Paraphrase Identification (PI)\nproblem. Given a question, the aim of the task is to find the most similar\nquestion from a QA knowledge base. In this paper, we propose a Multi-task\nSentence Encoding Model (MSEM) for the PI problem, wherein a connected graph is\nemployed to depict the relation between sentences, and a multi-task learning\nmodel is applied to address both the sentence matching and sentence intent\nclassification problem. In addition, we implement a general semantic retrieval\nframework that combines our proposed model and the Approximate Nearest Neighbor\n(ANN) technology, which enables us to find the most similar question from all\navailable candidates very quickly during online serving. The experiments show\nthe superiority of our proposed method as compared with the existing sentence\nmatching models.", "published": "2019-11-18 03:11:36", "link": "http://arxiv.org/abs/1911.07405v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Drug Repurposing for Cancer: An NLP Approach to Identify Low-Cost\n  Therapies", "abstract": "More than 200 generic drugs approved by the U.S. Food and Drug Administration\nfor non-cancer indications have shown promise for treating cancer. Due to their\nlong history of safe patient use, low cost, and widespread availability,\nrepurposing of generic drugs represents a major opportunity to rapidly improve\noutcomes for cancer patients and reduce healthcare costs worldwide. Evidence on\nthe efficacy of non-cancer generic drugs being tested for cancer exists in\nscientific publications, but trying to manually identify and extract such\nevidence is intractable. In this paper, we introduce a system to automate this\nevidence extraction from PubMed abstracts. Our primary contribution is to\ndefine the natural language processing pipeline required to obtain such\nevidence, comprising the following modules: querying, filtering, cancer type\nentity extraction, therapeutic association classification, and study type\nclassification. Using the subject matter expertise on our team, we create our\nown datasets for these specialized domain-specific tasks. We obtain promising\nperformance in each of the modules by utilizing modern language modeling\ntechniques and plan to treat them as baseline approaches for future improvement\nof individual components.", "published": "2019-11-18 18:32:25", "link": "http://arxiv.org/abs/1911.07819v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Improving Document Classification with Multi-Sense Embeddings", "abstract": "Efficient representation of text documents is an important building block in\nmany NLP tasks. Research on long text categorization has shown that simple\nweighted averaging of word vectors for sentence representation often\noutperforms more sophisticated neural models. Recently proposed Sparse\nComposite Document Vector (SCDV) (Mekala et. al, 2017) extends this approach\nfrom sentences to documents using soft clustering over word vectors. However,\nSCDV disregards the multi-sense nature of words, and it also suffers from the\ncurse of higher dimensionality. In this work, we address these shortcomings and\npropose SCDV-MS. SCDV-MS utilizes multi-sense word embeddings and learns a\nlower dimensional manifold. Through extensive experiments on multiple\nreal-world datasets, we show that SCDV-MS embeddings outperform previous\nstate-of-the-art embeddings on multi-class and multi-label text categorization\ntasks. Furthermore, SCDV-MS embeddings are more efficient than SCDV in terms of\ntime and space complexity on textual classification tasks.", "published": "2019-11-18 20:30:06", "link": "http://arxiv.org/abs/1911.07918v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Vietnamese Name Entity Recognition: A Deep Learning Method\n  Approach", "abstract": "Named entity recognition (NER) plays an important role in text-based\ninformation retrieval. In this paper, we combine Bidirectional Long Short-Term\nMemory (Bi-LSTM) \\cite{hochreiter1997,schuster1997} with Conditional Random\nField (CRF) \\cite{lafferty2001} to create a novel deep learning model for the\nNER problem. Each word as input of the deep learning model is represented by a\nWord2vec-trained vector. A word embedding set trained from about one million\narticles in 2018 collected through a Vietnamese news portal (baomoi.com). In\naddition, we concatenate a Word2Vec\\cite{mikolov2013}-trained vector with\nsemantic feature vector (Part-Of-Speech (POS) tagging, chunk-tag) and hidden\nsyntactic feature vector (extracted by Bi-LSTM nerwork) to achieve the (so far\nbest) result in Vietnamese NER system. The result was conducted on the data set\nVLSP2016 (Vietnamese Language and Speech Processing 2016 \\cite{vlsp2016})\ncompetition.", "published": "2019-11-18 13:28:37", "link": "http://arxiv.org/abs/1912.01109v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Static Detection of Obfuscation Transforms Using\n  Ensemble-Learning and Semantic Reasoning", "abstract": "The ability to efficiently detect the software protections used is at a prime\nto facilitate the selection and application of adequate deob-fuscation\ntechniques. We present a novel approach that combines semantic reasoning\ntechniques with ensemble learning classification for the purpose of providing a\nstatic detection framework for obfuscation transformations. By contrast to\nexisting work, we provide a methodology that can detect multiple layers of\nobfuscation, without depending on knowledge of the underlying functionality of\nthe training-set used. We also extend our work to detect constructions of\nobfuscation transformations, thus providing a fine-grained methodology. To that\nend, we provide several studies for the best practices of the use of machine\nlearning techniques for a scalable and efficient model. According to our\nexperimental results and evaluations on obfuscators such as Tigress and OLLVM,\nour models have up to 91% accuracy on state-of-the-art obfuscation\ntransformations. Our overall accuracies for their constructions are up to 100%.", "published": "2019-11-18 10:16:38", "link": "http://arxiv.org/abs/1911.07523v1", "categories": ["cs.CL", "cs.CR", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "A Spatial Sampling Approach to Wave Field Synthesis: PBAP and Huygens\n  Arrays", "abstract": "A simple approach to microphone- and speaker-arrays is described in which the\nmicrophone array is regarded as a sampling grid for the acoustic field, and the\ncorresponding speaker-array is treated as a \"spatial digital to analog\nconverter\" that reconstructs the acoustic field from its spatial samples.\nAdvantages of this approach include ease of understanding and teaching, ease of\ndeployment, effective practical guidelines for deployment, and significant\ncomputational savings in special cases. In particular, in the far-field case\n(acoustic sources many wavelengths away from a linear array of speakers) it is\npossible to quantize source angles slightly so that no processing per speaker\nis required beyond pure integer delay. Smoothly moving sources are obtained\nusing well known delay-line interpolation techniques such as linear\n(cross-fading) and Lagrange (polynomial) interpolation between/among speakers.\nWe call the far-field line-array case Planewave-Based Angle Panning (PBAP), in\nreference to the well-known Vector-Based Amplitude Panning (VBAP) family of\ntechniques, some of which are derived here as special cases: When speakers\nundersample the acoustic field, the result may be considered a form of VBAP,\nand VBAP is also obtained as a limiting case of polygonal PBAP arrays truncated\nto the polygon perimeter. Spatial samples need not be on a linear array,\nleading to a simple spatial audio system we call Huygens Arrays (HA). HAs are\nquite general for sources located behind the speaker array, which no longer\nneeds to be linear, and the sources are no longer restricted to the far field.\nMultiband and hybrid arrays employing VBAP (or stereo) and subwoofer(s) are\ndiscussed, using sampling theory to inform the choices of crossover\nfrequencies.", "published": "2019-11-18 12:08:18", "link": "http://arxiv.org/abs/1911.07575v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Linguistically Aided Speaker Diarization Using Speaker Role Information", "abstract": "Speaker diarization relies on the assumption that speech segments\ncorresponding to a particular speaker are concentrated in a specific region of\nthe speaker space; a region which represents that speaker's identity. These\nidentities are not known a priori, so a clustering algorithm is typically\nemployed, which is traditionally based solely on audio. Under noisy conditions,\nhowever, such an approach poses the risk of generating unreliable speaker\nclusters. In this work we aim to utilize linguistic information as a\nsupplemental modality to identify the various speakers in a more robust way. We\nare focused on conversational scenarios where the speakers assume distinct\nroles and are expected to follow different linguistic patterns. This distinct\nlinguistic variability can be exploited to help us construct the speaker\nidentities. That way, we are able to boost the diarization performance by\nconverting the clustering task to a classification one. The proposed method is\napplied in real-world dyadic psychotherapy interactions between a provider and\na patient and demonstrated to show improved results.", "published": "2019-11-18 22:53:56", "link": "http://arxiv.org/abs/1911.07994v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Universal Sound Separation Using Sound Classification", "abstract": "Deep learning approaches have recently achieved impressive performance on\nboth audio source separation and sound classification. Most audio source\nseparation approaches focus only on separating sources belonging to a\nrestricted domain of source classes, such as speech and music. However, recent\nwork has demonstrated the possibility of \"universal sound separation\", which\naims to separate acoustic sources from an open domain, regardless of their\nclass. In this paper, we utilize the semantic information learned by sound\nclassifier networks trained on a vast amount of diverse sounds to improve\nuniversal sound separation. In particular, we show that semantic embeddings\nextracted from a sound classifier can be used to condition a separation\nnetwork, providing it with useful additional information. This approach is\nespecially useful in an iterative setup, where source estimates from an initial\nseparation stage and their corresponding classifier-derived embeddings are fed\nto a second separation network. By performing a thorough hyperparameter search\nconsisting of over a thousand experiments, we find that classifier embeddings\nfrom clean sources provide nearly one dB of SNR gain, and our best iterative\nmodels achieve a significant fraction of this oracle performance, establishing\na new state-of-the-art for universal sound separation.", "published": "2019-11-18 20:56:26", "link": "http://arxiv.org/abs/1911.07951v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and\n  Enhancement", "abstract": "This work introduces sequential neural beamforming, which alternates between\nneural network based spectral separation and beamforming based spatial\nseparation. Our neural networks for separation use an advanced convolutional\narchitecture trained with a novel stabilized signal-to-noise ratio loss\nfunction. For beamforming, we explore multiple ways of computing time-varying\ncovariance matrices, including factorizing the spatial covariance into a\ntime-varying amplitude component and a time-invariant spatial component, as\nwell as using block-based techniques. In addition, we introduce a multi-frame\nbeamforming method which improves the results significantly by adding\ncontextual frames to the beamforming formulations. We extensively evaluate and\nanalyze the effects of window size, block size, and multi-frame context size\nfor these methods. Our best method utilizes a sequence of three neural\nseparation and multi-frame time-invariant spatial beamforming stages, and\ndemonstrates an average improvement of 2.75 dB in scale-invariant\nsignal-to-noise ratio and 14.2% absolute reduction in a comparative speech\nrecognition metric across four challenging reverberant speech enhancement and\nseparation tasks. We also use our three-speaker separation model to separate\nreal recordings in the LibriCSS evaluation set into non-overlapping tracks, and\nachieve a better word error rate as compared to a baseline mask based\nbeamformer.", "published": "2019-11-18 20:59:03", "link": "http://arxiv.org/abs/1911.07953v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
