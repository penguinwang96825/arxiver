{"title": "Using Clinical Narratives and Structured Data to Identify Distant\n  Recurrences in Breast Cancer", "abstract": "Accurately identifying distant recurrences in breast cancer from the\nElectronic Health Records (EHR) is important for both clinical care and\nsecondary analysis. Although multiple applications have been developed for\ncomputational phenotyping in breast cancer, distant recurrence identification\nstill relies heavily on manual chart review. In this study, we aim to develop a\nmodel that identifies distant recurrences in breast cancer using clinical\nnarratives and structured data from EHR. We apply MetaMap to extract features\nfrom clinical narratives and also retrieve structured clinical data from EHR.\nUsing these features, we train a support vector machine model to identify\ndistant recurrences in breast cancer patients. We train the model using 1,396\ndouble-annotated subjects and validate the model using 599 double-annotated\nsubjects. In addition, we validate the model on a set of 4,904 single-annotated\nsubjects as a generalization test. We obtained a high area under curve (AUC)\nscore of 0.92 (SD=0.01) in the cross-validation using the training dataset,\nthen obtained AUC scores of 0.95 and 0.93 in the held-out test and\ngeneralization test using 599 and 4,904 samples respectively. Our model can\naccurately and efficiently identify distant recurrences in breast cancer by\ncombining features extracted from unstructured clinical narratives and\nstructured clinical data.", "published": "2018-06-13 01:58:22", "link": "http://arxiv.org/abs/1806.04818v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural Language Processing for EHR-Based Computational Phenotyping", "abstract": "This article reviews recent advances in applying natural language processing\n(NLP) to Electronic Health Records (EHRs) for computational phenotyping.\nNLP-based computational phenotyping has numerous applications including\ndiagnosis categorization, novel phenotype discovery, clinical trial screening,\npharmacogenomics, drug-drug interaction (DDI) and adverse drug event (ADE)\ndetection, as well as genome-wide and phenome-wide association studies.\nSignificant progress has been made in algorithm development and resource\nconstruction for computational phenotyping. Among the surveyed methods,\nwell-designed keyword search and rule-based systems often achieve good\nperformance. However, the construction of keyword and rule lists requires\nsignificant manual effort, which is difficult to scale. Supervised machine\nlearning models have been favored because they are capable of acquiring both\nclassification patterns and structures from data. Recently, deep learning and\nunsupervised learning have received growing attention, with the former favored\nfor its performance and the latter for its ability to find novel phenotypes.\nIntegrating heterogeneous data sources have become increasingly important and\nhave shown promise in improving model performance. Often better performance is\nachieved by combining multiple modalities of information. Despite these many\nadvances, challenges and opportunities remain for NLP-based computational\nphenotyping, including better model interpretability and generalizability, and\nproper characterization of feature relations in clinical narratives", "published": "2018-06-13 02:14:19", "link": "http://arxiv.org/abs/1806.04820v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SGM: Sequence Generation Model for Multi-label Classification", "abstract": "Multi-label classification is an important yet challenging task in natural\nlanguage processing. It is more complex than single-label classification in\nthat the labels tend to be correlated. Existing methods tend to ignore the\ncorrelations between labels. Besides, different parts of the text can\ncontribute differently for predicting different labels, which is not considered\nby existing models. In this paper, we propose to view the multi-label\nclassification task as a sequence generation problem, and apply a sequence\ngeneration model with a novel decoder structure to solve it. Extensive\nexperimental results show that our proposed methods outperform previous work by\na substantial margin. Further analysis of experimental results demonstrates\nthat the proposed methods not only capture the correlations between labels, but\nalso select the most informative words automatically when predicting different\nlabels.", "published": "2018-06-13 02:16:01", "link": "http://arxiv.org/abs/1806.04822v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Double Path Networks for Sequence to Sequence Learning", "abstract": "Encoder-decoder based Sequence to Sequence learning (S2S) has made remarkable\nprogress in recent years. Different network architectures have been used in the\nencoder/decoder. Among them, Convolutional Neural Networks (CNN) and Self\nAttention Networks (SAN) are the prominent ones. The two architectures achieve\nsimilar performances but use very different ways to encode and decode context:\nCNN use convolutional layers to focus on the local connectivity of the\nsequence, while SAN uses self-attention layers to focus on global semantics. In\nthis work we propose Double Path Networks for Sequence to Sequence learning\n(DPN-S2S), which leverage the advantages of both models by using double path\ninformation fusion. During the encoding step, we develop a double path\narchitecture to maintain the information coming from different paths with\nconvolutional layers and self-attention layers separately. To effectively use\nthe encoded context, we develop a cross attention module with gating and use it\nto automatically pick up the information needed during the decoding step. By\ndeeply integrating the two paths with cross attention, both types of\ninformation are combined and well exploited. Experiments show that our proposed\nmethod can significantly improve the performance of sequence to sequence\nlearning over state-of-the-art systems.", "published": "2018-06-13 05:51:10", "link": "http://arxiv.org/abs/1806.04856v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Accurate Evaluation of GANs for Language Generation", "abstract": "Generative Adversarial Networks (GANs) are a promising approach to language\ngeneration. The latest works introducing novel GAN models for language\ngeneration use n-gram based metrics for evaluation and only report single\nscores of the best run. In this paper, we argue that this often misrepresents\nthe true picture and does not tell the full story, as GAN models can be\nextremely sensitive to the random initialization and small deviations from the\nbest hyperparameter choice. In particular, we demonstrate that the previously\nused BLEU score is not sensitive to semantic deterioration of generated texts\nand propose alternative metrics that better capture the quality and diversity\nof the generated samples. We also conduct a set of experiments comparing a\nnumber of GAN models for text with a conventional Language Model (LM) and find\nthat neither of the considered models performs convincingly better than the LM.", "published": "2018-06-13 10:35:45", "link": "http://arxiv.org/abs/1806.04936v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-Based Decoding for Event Sequencing and Coreference Resolution", "abstract": "Events in text documents are interrelated in complex ways. In this paper, we\nstudy two types of relation: Event Coreference and Event Sequencing. We show\nthat the popular tree-like decoding structure for automated Event Coreference\nis not suitable for Event Sequencing. To this end, we propose a graph-based\ndecoding algorithm that is applicable to both tasks. The new decoding algorithm\nsupports flexible feature sets for both tasks. Empirically, our event\ncoreference system has achieved state-of-the-art performance on the TAC-KBP\n2015 event coreference task and our event sequencing system beats a strong\ntemporal-based, oracle-informed baseline. We discuss the challenges of studying\nthese event relations.", "published": "2018-06-13 15:05:39", "link": "http://arxiv.org/abs/1806.05099v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Evaluation of Neural Machine Translation Models on Historical\n  Spelling Normalization", "abstract": "In this paper, we apply different NMT models to the problem of historical\nspelling normalization for five languages: English, German, Hungarian,\nIcelandic, and Swedish. The NMT models are at different levels, have different\nattention mechanisms, and different neural network architectures. Our results\nshow that NMT models are much better than SMT models in terms of character\nerror rate. The vanilla RNNs are competitive to GRUs/LSTMs in historical\nspelling normalization. Transformer models perform better only when provided\nwith more training data. We also find that subword-level models with a small\nsubword vocabulary are better than character-level models for low-resource\nlanguages. In addition, we propose a hybrid method which further improves the\nperformance of historical spelling normalization.", "published": "2018-06-13 18:29:09", "link": "http://arxiv.org/abs/1806.05210v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bringing replication and reproduction together with generalisability in\n  NLP: Three reproduction studies for Target Dependent Sentiment Analysis", "abstract": "Lack of repeatability and generalisability are two significant threats to\ncontinuing scientific development in Natural Language Processing. Language\nmodels and learning methods are so complex that scientific conference papers no\nlonger contain enough space for the technical depth required for replication or\nreproduction. Taking Target Dependent Sentiment Analysis as a case study, we\nshow how recent work in the field has not consistently released code, or\ndescribed settings for learning methods in enough detail, and lacks\ncomparability and generalisability in train, test or validation data. To\ninvestigate generalisability and to enable state of the art comparative\nevaluations, we carry out the first reproduction studies of three groups of\ncomplementary methods and perform the first large-scale mass evaluation on six\ndifferent English datasets. Reflecting on our experiences, we recommend that\nfuture replication or reproduction experiments should always consider a variety\nof datasets alongside documenting and releasing their methods and published\ncode in order to minimise the barriers to both repeatability and\ngeneralisability. We have released our code with a model zoo on GitHub with\nJupyter Notebooks to aid understanding and full documentation, and we recommend\nthat others do the same with their papers at submission time through an\nanonymised GitHub account.", "published": "2018-06-13 18:51:05", "link": "http://arxiv.org/abs/1806.05219v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Bags of Words: Inferring Systemic Nets", "abstract": "Textual analytics based on representations of documents as bags of words have\nbeen reasonably successful. However, analysis that requires deeper insight into\nlanguage, into author properties, or into the contexts in which documents were\ncreated requires a richer representation. Systemic nets are one such\nrepresentation. They have not been extensively used because they required human\neffort to construct. We show that systemic nets can be algorithmically inferred\nfrom corpora, that the resulting nets are plausible, and that they can provide\npractical benefits for knowledge discovery problems. This opens up a new class\nof practical analysis techniques for textual analytics.", "published": "2018-06-13 19:14:14", "link": "http://arxiv.org/abs/1806.05231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMHD: A Large-Scale Resource for Exploring Online Language Usage for\n  Multiple Mental Health Conditions", "abstract": "Mental health is a significant and growing public health concern. As language\nusage can be leveraged to obtain crucial insights into mental health\nconditions, there is a need for large-scale, labeled, mental health-related\ndatasets of users who have been diagnosed with one or more of such conditions.\nIn this paper, we investigate the creation of high-precision patterns to\nidentify self-reported diagnoses of nine different mental health conditions,\nand obtain high-quality labeled data without the need for manual labelling. We\nintroduce the SMHD (Self-reported Mental Health Diagnoses) dataset and make it\navailable. SMHD is a novel large dataset of social media posts from users with\none or multiple mental health conditions along with matched control users. We\nexamine distinctions in users' language, as measured by linguistic and\npsychological variables. We further explore text classification methods to\nidentify individuals with mental conditions through their language.", "published": "2018-06-13 20:29:25", "link": "http://arxiv.org/abs/1806.05258v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Predictable is Your State? Leveraging Lexical and Contextual\n  Information for Predicting Legislative Floor Action at the State Level", "abstract": "Modeling U.S. Congressional legislation and roll-call votes has received\nsignificant attention in previous literature. However, while legislators across\n50 state governments and D.C. propose over 100,000 bills each year, and on\naverage enact over 30% of them, state level analysis has received relatively\nless attention due in part to the difficulty in obtaining the necessary data.\nSince each state legislature is guided by their own procedures, politics and\nissues, however, it is difficult to qualitatively asses the factors that affect\nthe likelihood of a legislative initiative succeeding. Herein, we present\nseveral methods for modeling the likelihood of a bill receiving floor action\nacross all 50 states and D.C. We utilize the lexical content of over 1 million\nbills, along with contextual legislature and legislator derived features to\nbuild our predictive models, allowing a comparison of the factors that are\nimportant to the lawmaking process. Furthermore, we show that these signals\nhold complementary predictive power, together achieving an average improvement\nin accuracy of 18% over state specific baselines.", "published": "2018-06-13 22:05:10", "link": "http://arxiv.org/abs/1806.05284v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Visual Knowledge Memory Networks for Visual Question Answering", "abstract": "Visual question answering (VQA) requires joint comprehension of images and\nnatural language questions, where many questions can't be directly or clearly\nanswered from visual content but require reasoning from structured human\nknowledge with confirmation from visual content. This paper proposes visual\nknowledge memory network (VKMN) to address this issue, which seamlessly\nincorporates structured human knowledge and deep visual features into memory\nnetworks in an end-to-end learning framework. Comparing to existing methods for\nleveraging external knowledge for supporting VQA, this paper stresses more on\ntwo missing mechanisms. First is the mechanism for integrating visual contents\nwith knowledge facts. VKMN handles this issue by embedding knowledge triples\n(subject, relation, target) and deep visual features jointly into the visual\nknowledge features. Second is the mechanism for handling multiple knowledge\nfacts expanding from question and answer pairs. VKMN stores joint embedding\nusing key-value pair structure in the memory networks so that it is easy to\nhandle multiple facts. Experiments show that the proposed method achieves\npromising results on both VQA v1.0 and v2.0 benchmarks, while outperforms\nstate-of-the-art methods on the knowledge-reasoning related questions.", "published": "2018-06-13 06:37:42", "link": "http://arxiv.org/abs/1806.04860v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "OpenEDGAR: Open Source Software for SEC EDGAR Analysis", "abstract": "OpenEDGAR is an open source Python framework designed to rapidly construct\nresearch databases based on the Electronic Data Gathering, Analysis, and\nRetrieval (EDGAR) system operated by the US Securities and Exchange Commission\n(SEC). OpenEDGAR is built on the Django application framework, supports\ndistributed compute across one or more servers, and includes functionality to\n(i) retrieve and parse index and filing data from EDGAR, (ii) build tables for\nkey metadata like form type and filer, (iii) retrieve, parse, and update CIK to\nticker and industry mappings, (iv) extract content and metadata from filing\ndocuments, and (v) search filing document contents. OpenEDGAR is designed for\nuse in both academic research and industrial applications, and is distributed\nunder MIT License at https://github.com/LexPredict/openedgar.", "published": "2018-06-13 12:16:37", "link": "http://arxiv.org/abs/1806.04973v1", "categories": ["cs.CL", "cs.DB", "I.2.7; F.2.2; H.3.1; H.3.3; I.7"], "primary_category": "cs.CL"}
{"title": "Visually grounded cross-lingual keyword spotting in speech", "abstract": "Recent work considered how images paired with speech can be used as\nsupervision for building speech systems when transcriptions are not available.\nWe ask whether visual grounding can be used for cross-lingual keyword spotting:\ngiven a text keyword in one language, the task is to retrieve spoken utterances\ncontaining that keyword in another language. This could enable searching\nthrough speech in a low-resource language using text queries in a high-resource\nlanguage. As a proof-of-concept, we use English speech with German queries: we\nuse a German visual tagger to add keyword labels to each training image, and\nthen train a neural network to map English speech to German keywords. Without\nseeing parallel speech-transcriptions or translations, the model achieves a\nprecision at ten of 58%. We show that most erroneous retrievals contain\nequivalent or semantically relevant keywords; excluding these would improve\nP@10 to 91%.", "published": "2018-06-13 13:37:34", "link": "http://arxiv.org/abs/1806.05030v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Detecting Speech Act Types in Developer Question/Answer Conversations\n  During Bug Repair", "abstract": "This paper targets the problem of speech act detection in conversations about\nbug repair. We conduct a \"Wizard of Oz\" experiment with 30 professional\nprogrammers, in which the programmers fix bugs for two hours, and use a\nsimulated virtual assistant for help. Then, we use an open coding manual\nannotation procedure to identify the speech act types in the conversations.\nFinally, we train and evaluate a supervised learning algorithm to automatically\ndetect the speech act types in the conversations. In 30 two-hour conversations,\nwe made 2459 annotations and uncovered 26 speech act types. Our automated\ndetection achieved 69% precision and 50% recall. The key application of this\nwork is to advance the state of the art for virtual assistants in software\nengineering. Virtual assistant technology is growing rapidly, though\napplications in software engineering are behind those in other areas, largely\ndue to a lack of relevant data and experiments. This paper targets this problem\nin the area of developer Q/A conversations about bug repair.", "published": "2018-06-13 16:26:28", "link": "http://arxiv.org/abs/1806.05130v3", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A Study of Enhancement, Augmentation, and Autoencoder Methods for Domain\n  Adaptation in Distant Speech Recognition", "abstract": "Speech recognizers trained on close-talking speech do not generalize to\ndistant speech and the word error rate degradation can be as large as 40%\nabsolute. Most studies focus on tackling distant speech recognition as a\nseparate problem, leaving little effort to adapting close-talking speech\nrecognizers to distant speech. In this work, we review several approaches from\na domain adaptation perspective. These approaches, including speech\nenhancement, multi-condition training, data augmentation, and autoencoders, all\ninvolve a transformation of the data between domains. We conduct experiments on\nthe AMI data set, where these approaches can be realized under the same\ncontrolled setting. These approaches lead to different amounts of improvement\nunder their respective assumptions. The purpose of this paper is to quantify\nand characterize the performance gap between the two domains, setting up the\nbasis for studying adaptation of speech recognizers from close-talking speech\nto distant speech. Our results also have implications for improving distant\nspeech recognition.", "published": "2018-06-13 04:07:22", "link": "http://arxiv.org/abs/1806.04841v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generative Neural Machine Translation", "abstract": "We introduce Generative Neural Machine Translation (GNMT), a latent variable\narchitecture which is designed to model the semantics of the source and target\nsentences. We modify an encoder-decoder translation model by adding a latent\nvariable as a language agnostic representation which is encouraged to learn the\nmeaning of the sentence. GNMT achieves competitive BLEU scores on pure\ntranslation tasks, and is superior when there are missing words in the source\nsentence. We augment the model to facilitate multilingual translation and\nsemi-supervised learning without adding parameters. This framework\nsignificantly reduces overfitting when there is limited paired data available,\nand is effective for translating between pairs of languages not seen during\ntraining.", "published": "2018-06-13 16:35:32", "link": "http://arxiv.org/abs/1806.05138v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "fMRI Semantic Category Decoding using Linguistic Encoding of Word\n  Embeddings", "abstract": "The dispute of how the human brain represents conceptual knowledge has been\nargued in many scientific fields. Brain imaging studies have shown that the\nspatial patterns of neural activation in the brain are correlated with thinking\nabout different semantic categories of words (for example, tools, animals, and\nbuildings) or when viewing the related pictures. In this paper, we present a\ncomputational model that learns to predict the neural activation captured in\nfunctional magnetic resonance imaging (fMRI) data of test words. Unlike the\nmodels with hand-crafted features that have been used in the literature, in\nthis paper we propose a novel approach wherein decoding models are built with\nfeatures extracted from popular linguistic encodings of Word2Vec, GloVe,\nMeta-Embeddings in conjunction with the empirical fMRI data associated with\nviewing several dozen concrete nouns. We compared these models with several\nother models that use word features extracted from FastText, Randomly-generated\nfeatures, Mitchell's 25 features [1]. The experimental results show that the\npredicted fMRI images using Meta-Embeddings meet the state-of-the-art\nperformance. Although models with features from GloVe and Word2Vec predict fMRI\nimages similar to the state-of-the-art model, model with features from\nMeta-Embeddings predicts significantly better. The proposed scheme that uses\npopular linguistic encoding offers a simple and easy approach for semantic\ndecoding from fMRI experiments.", "published": "2018-06-13 10:59:33", "link": "http://arxiv.org/abs/1806.05177v1", "categories": ["q-bio.NC", "cs.CL", "cs.CV"], "primary_category": "q-bio.NC"}
{"title": "Generating Sentences Using a Dynamic Canvas", "abstract": "We introduce the Attentive Unsupervised Text (W)riter (AUTR), which is a word\nlevel generative model for natural language. It uses a recurrent neural network\nwith a dynamic attention and canvas memory mechanism to iteratively construct\nsentences. By viewing the state of the memory at intermediate stages and where\nthe model is placing its attention, we gain insight into how it constructs\nsentences. We demonstrate that AUTR learns a meaningful latent representation\nfor each sentence, and achieves competitive log-likelihood lower bounds whilst\nbeing computationally efficient. It is effective at generating and\nreconstructing sentences, as well as imputing missing words.", "published": "2018-06-13 12:57:19", "link": "http://arxiv.org/abs/1806.05178v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Retrospective Analysis of the Fake News Challenge Stance Detection\n  Task", "abstract": "The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stance\nclassification task as a crucial first step towards detecting fake news. To\ndate, there is no in-depth analysis paper to critically discuss FNC-1's\nexperimental setup, reproduce the results, and draw conclusions for\nnext-generation stance classification methods. In this paper, we provide such\nan in-depth analysis for the three top-performing systems. We first find that\nFNC-1's proposed evaluation metric favors the majority class, which can be\neasily classified, and thus overestimates the true discriminative power of the\nmethods. Therefore, we propose a new F1-based metric yielding a changed system\nranking. Next, we compare the features and architectures used, which leads to a\nnovel feature-rich stacked LSTM model that performs on par with the best\nsystems, but is superior in predicting minority classes. To understand the\nmethods' ability to generalize, we derive a new dataset and perform both\nin-domain and cross-domain experiments. Our qualitative and quantitative study\nhelps interpreting the original FNC-1 scores and understand which features help\nimproving performance and why. Our new dataset and all source code used during\nthe reproduction study are publicly available for future research.", "published": "2018-06-13 15:38:09", "link": "http://arxiv.org/abs/1806.05180v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Extracting Parallel Sentences with Bidirectional Recurrent Neural\n  Networks to Improve Machine Translation", "abstract": "Parallel sentence extraction is a task addressing the data sparsity problem\nfound in multilingual natural language processing applications. We propose a\nbidirectional recurrent neural network based approach to extract parallel\nsentences from collections of multilingual texts. Our experiments with noisy\nparallel corpora show that we can achieve promising results against a\ncompetitive baseline by removing the need of specific feature engineering or\nadditional external resources. To justify the utility of our approach, we\nextract sentence pairs from Wikipedia articles to train machine translation\nsystems and show significant improvements in translation performance.", "published": "2018-06-13 13:57:13", "link": "http://arxiv.org/abs/1806.05559v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unsupervised Adaptation with Interpretable Disentangled Representations\n  for Distant Conversational Speech Recognition", "abstract": "The current trend in automatic speech recognition is to leverage large\namounts of labeled data to train supervised neural network models.\nUnfortunately, obtaining data for a wide range of domains to train robust\nmodels can be costly. However, it is relatively inexpensive to collect large\namounts of unlabeled data from domains that we want the models to generalize\nto. In this paper, we propose a novel unsupervised adaptation method that\nlearns to synthesize labeled data for the target domain from unlabeled\nin-domain data and labeled out-of-domain data. We first learn without\nsupervision an interpretable latent representation of speech that encodes\nlinguistic and nuisance factors (e.g., speaker and channel) using different\nlatent variables. To transform a labeled out-of-domain utterance without\naltering its transcript, we transform the latent nuisance variables while\nmaintaining the linguistic variables. To demonstrate our approach, we focus on\na channel mismatch setting, where the domain of interest is distant\nconversational speech, and labels are only available for close-talking speech.\nOur proposed method is evaluated on the AMI dataset, outperforming all\nbaselines and bridging the gap between unadapted and in-domain models by over\n77% without using any parallel data.", "published": "2018-06-13 07:14:15", "link": "http://arxiv.org/abs/1806.04872v1", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Model-based Speech Enhancement for Intelligibility Improvement in\n  Binaural Hearing Aids", "abstract": "Speech intelligibility is often severely degraded among hearing impaired\nindividuals in situations such as the cocktail party scenario. The performance\nof the current hearing aid technology has been observed to be limited in these\nscenarios. In this paper, we propose a binaural speech enhancement framework\nthat takes into consideration the speech production model. The enhancement\nframework proposed here is based on the Kalman filter that allows us to take\nthe speech production dynamics into account during the enhancement process. The\nusage of a Kalman filter requires the estimation of clean speech and noise\nshort term predictor (STP) parameters, and the clean speech pitch parameters.\nIn this work, a binaural codebook-based method is proposed for estimating the\nSTP parameters, and a directional pitch estimator based on the harmonic model\nand maximum likelihood principle is used to estimate the pitch parameters. The\nproposed method for estimating the STP and pitch parameters jointly uses the\ninformation from left and right ears, leading to a more robust estimation of\nthe filter parameters. Objective measures such as PESQ and STOI have been used\nto evaluate the enhancement framework in different acoustic scenarios\nrepresentative of the cocktail party scenario. We have also conducted\nsubjective listening tests on a set of nine normal hearing subjects, to\nevaluate the performance in terms of intelligibility and quality improvement.\nThe listening tests show that the proposed algorithm, even with access to only\na single channel noisy observation, significantly improves the overall speech\nquality, and the speech intelligibility by up to 15%.", "published": "2018-06-13 08:19:23", "link": "http://arxiv.org/abs/1806.04885v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A data-driven approach to mid-level perceptual musical feature modeling", "abstract": "Musical features and descriptors could be coarsely divided into three levels\nof complexity. The bottom level contains the basic building blocks of music,\ne.g., chords, beats and timbre. The middle level contains concepts that emerge\nfrom combining the basic blocks: tonal and rhythmic stability, harmonic and\nrhythmic complexity, etc. High-level descriptors (genre, mood, expressive\nstyle) are usually modeled using the lower level ones. The features belonging\nto the middle level can both improve automatic recognition of high-level\ndescriptors, and provide new music retrieval possibilities. Mid-level features\nare subjective and usually lack clear definitions. However, they are very\nimportant for human perception of music, and on some of them people can reach\nhigh agreement, even though defining them and therefore, designing a\nhand-crafted feature extractor for them can be difficult. In this paper, we\nderive the mid-level descriptors from data. We collect and release a\ndataset\\footnote{https://osf.io/5aupt/} of 5000 songs annotated by musicians\nwith seven mid-level descriptors, namely, melodiousness, tonal and rhythmic\nstability, modality, rhythmic complexity, dissonance and articulation. We then\ncompare several approaches to predicting these descriptors from spectrograms\nusing deep-learning. We also demonstrate the usefulness of these mid-level\nfeatures using music emotion recognition as an application.", "published": "2018-06-13 09:10:43", "link": "http://arxiv.org/abs/1806.04903v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multi-View Networks for Denoising of Arbitrary Numbers of Channels", "abstract": "We propose a set of denoising neural networks capable of operating on an\narbitrary number of channels at runtime, irrespective of how many channels they\nwere trained on. We coin the proposed models multi-view networks since they\noperate using multiple views of the same data. We explore two such\narchitectures and show how they outperform traditional denoising models in\nmulti-channel scenarios. Additionally, we demonstrate how multi-view networks\ncan leverage information provided by additional recordings to make better\npredictions, and how they are able to generalize to a number of recordings not\nseen in training.", "published": "2018-06-13 23:09:07", "link": "http://arxiv.org/abs/1806.05296v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
