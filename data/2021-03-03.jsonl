{"title": "Random Feature Attention", "abstract": "Transformers are state-of-the-art models for a variety of sequence modeling\ntasks. At their core is an attention function which models pairwise\ninteractions between the inputs at every timestep. While attention is powerful,\nit does not scale efficiently to long sequences due to its quadratic time and\nspace complexity in the sequence length. We propose RFA, a linear time and\nspace attention that uses random feature methods to approximate the softmax\nfunction, and explore its application in transformers. RFA can be used as a\ndrop-in replacement for conventional softmax attention and offers a\nstraightforward way of learning with recency bias through an optional gating\nmechanism. Experiments on language modeling and machine translation demonstrate\nthat RFA achieves similar or better performance compared to strong transformer\nbaselines. In the machine translation experiment, RFA decodes twice as fast as\na vanilla transformer. Compared to existing efficient transformer variants, RFA\nis competitive in terms of both accuracy and efficiency on three long text\nclassification datasets. Our analysis shows that RFA's efficiency gains are\nespecially notable on long sequences, suggesting that RFA will be particularly\nuseful in tasks that require working with large inputs, fast decoding speed, or\nlow memory footprints.", "published": "2021-03-03 02:48:56", "link": "http://arxiv.org/abs/2103.02143v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradual Fine-Tuning for Low-Resource Domain Adaptation", "abstract": "Fine-tuning is known to improve NLP models by adapting an initial model\ntrained on more plentiful but less domain-salient examples to data in a target\ndomain. Such domain adaptation is typically done using one stage of\nfine-tuning. We demonstrate that gradually fine-tuning in a multi-stage process\ncan yield substantial further gains and can be applied without modifying the\nmodel or learning objective.", "published": "2021-03-03 06:24:54", "link": "http://arxiv.org/abs/2103.02205v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding\n  Transformation", "abstract": "Linear embedding transformation has been shown to be effective for zero-shot\ncross-lingual transfer tasks and achieve surprisingly promising results.\nHowever, cross-lingual embedding space mapping is usually studied in static\nword-level embeddings, where a space transformation is derived by aligning\nrepresentations of translation pairs that are referred from dictionaries. We\nmove further from this line and investigate a contextual embedding alignment\napproach which is sense-level and dictionary-free. To enhance the quality of\nthe mapping, we also provide a deep view of properties of contextual\nembeddings, i.e., anisotropy problem and its solution. Experiments on zero-shot\ndependency parsing through the concept-shared space built by our embedding\ntransformation substantially outperform state-of-the-art methods using\nmultilingual embeddings.", "published": "2021-03-03 06:50:43", "link": "http://arxiv.org/abs/2103.02212v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation with Hierarchical SQL-to-Question Generation for\n  Cross-domain Text-to-SQL Parsing", "abstract": "Data augmentation has attracted a lot of research attention in the deep\nlearning era for its ability in alleviating data sparseness. The lack of\nlabeled data for unseen evaluation databases is exactly the major challenge for\ncross-domain text-to-SQL parsing. Previous works either require human\nintervention to guarantee the quality of generated data, or fail to handle\ncomplex SQL queries. This paper presents a simple yet effective data\naugmentation framework. First, given a database, we automatically produce a\nlarge number of SQL queries based on an abstract syntax tree grammar. For\nbetter distribution matching, we require that at least 80% of SQL patterns in\nthe training data are covered by generated queries. Second, we propose a\nhierarchical SQL-to-question generation model to obtain high-quality natural\nlanguage questions, which is the major contribution of this work. Finally, we\ndesign a simple sampling strategy that can greatly improve training efficiency\ngiven large amounts of generated data. Experiments on three cross-domain\ndatasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that\nour proposed data augmentation framework can consistently improve performance\nover strong baselines, and the hierarchical generation component is the key for\nthe improvement.", "published": "2021-03-03 07:37:38", "link": "http://arxiv.org/abs/2103.02227v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lex2vec: making Explainable Word Embeddings via Lexical Resources", "abstract": "In this technical report, we propose an algorithm, called Lex2vec that\nexploits lexical resources to inject information into word embeddings and name\nthe embedding dimensions by means of knowledge bases. We evaluate the optimal\nparameters to extract a number of informative labels that is readable and has a\ngood coverage for the embedding dimensions.", "published": "2021-03-03 09:11:18", "link": "http://arxiv.org/abs/2103.02269v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study of Compound PCFGs", "abstract": "Compound probabilistic context-free grammars (C-PCFGs) have recently\nestablished a new state of the art for unsupervised phrase-structure grammar\ninduction. However, due to the high space and time complexities of chart-based\nrepresentation and inference, it is difficult to investigate C-PCFGs\ncomprehensively. In this work, we rely on a fast implementation of C-PCFGs to\nconduct an evaluation complementary to that of~\\citet{kim-etal-2019-compound}.\nWe start by analyzing and ablating C-PCFGs on English treebanks. Our findings\nsuggest that (1) C-PCFGs are data-efficient and can generalize to unseen\nsentence/constituent lengths; and (2) C-PCFGs make the best use of\nsentence-level information in generating preterminal rule probabilities. We\nfurther conduct a multilingual evaluation of C-PCFGs. The experimental results\nshow that the best configurations of C-PCFGs, which are tuned on English, do\nnot always generalize to morphology-rich languages.", "published": "2021-03-03 10:24:26", "link": "http://arxiv.org/abs/2103.02298v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Few-shot Learning for Slot Tagging with Attentive Relational Network", "abstract": "Metric-based learning is a well-known family of methods for few-shot\nlearning, especially in computer vision. Recently, they have been used in many\nnatural language processing applications but not for slot tagging. In this\npaper, we explore metric-based learning methods in the slot tagging task and\npropose a novel metric-based learning architecture - Attentive Relational\nNetwork. Our proposed method extends relation networks, making them more\nsuitable for natural language processing applications in general, by leveraging\npretrained contextual embeddings such as ELMO and BERT and by using attention\nmechanism. The results on SNIPS data show that our proposed method outperforms\nother state-of-the-art metric-based learning methods.", "published": "2021-03-03 11:24:24", "link": "http://arxiv.org/abs/2103.02333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OAG-BERT: Towards A Unified Backbone Language Model For Academic\n  Knowledge Services", "abstract": "Academic knowledge services have substantially facilitated the development of\nthe science enterprise by providing a plenitude of efficient research tools.\nHowever, many applications highly depend on ad-hoc models and expensive human\nlabeling to understand scientific contents, hindering deployments into real\nproducts. To build a unified backbone language model for different\nknowledge-intensive academic applications, we pre-train an academic language\nmodel OAG-BERT that integrates both the heterogeneous entity knowledge and\nscientific corpora in the Open Academic Graph (OAG) -- the largest public\nacademic graph to date. In OAG-BERT, we develop strategies for pre-training\ntext and entity data along with zero-shot inference techniques. In OAG-BERT, we\ndevelop strategies for pre-training text and entity data along with zero-shot\ninference techniques. Its zero-shot capability furthers the path to mitigate\nthe need of expensive annotations. OAG-BERT has been deployed for real-world\napplications, such as the reviewer recommendation function for National Nature\nScience Foundation of China (NSFC) -- one of the largest funding agencies in\nChina -- and paper tagging in AMiner. All codes and pre-trained models are\navailable via the CogDL toolkit.", "published": "2021-03-03 14:00:57", "link": "http://arxiv.org/abs/2103.02410v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash\n  Commands", "abstract": "The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of\nnatural language processing to the command line. Participants were tasked with\nbuilding models that can transform descriptions of command line tasks in\nEnglish to their Bash syntax. This is a report on the competition with details\nof the task, metrics, data, attempted solutions, and lessons learned.", "published": "2021-03-03 16:56:18", "link": "http://arxiv.org/abs/2103.02523v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NaturalConv: A Chinese Dialogue Dataset Towards Multi-turn Topic-driven\n  Conversation", "abstract": "In this paper, we propose a Chinese multi-turn topic-driven conversation\ndataset, NaturalConv, which allows the participants to chat anything they want\nas long as any element from the topic is mentioned and the topic shift is\nsmooth. Our corpus contains 19.9K conversations from six domains, and 400K\nutterances with an average turn number of 20.1. These conversations contain\nin-depth discussions on related topics or widely natural transition between\nmultiple topics. We believe either way is normal for human conversation. To\nfacilitate the research on this corpus, we provide results of several benchmark\nmodels. Comparative results show that for this dataset, our current models are\nnot able to provide significant improvement by introducing background\nknowledge/topic. Therefore, the proposed dataset should be a good benchmark for\nfurther research to evaluate the validity and naturalness of multi-turn\nconversation systems. Our dataset is available at\nhttps://ailab.tencent.com/ailab/nlp/dialogue/#datasets.", "published": "2021-03-03 17:38:33", "link": "http://arxiv.org/abs/2103.02548v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Extraneous Content in Podcasts", "abstract": "Podcast episodes often contain material extraneous to the main content, such\nas advertisements, interleaved within the audio and the written descriptions.\nWe present classifiers that leverage both textual and listening patterns in\norder to detect such content in podcast descriptions and audio transcripts. We\ndemonstrate that our models are effective by evaluating them on the downstream\ntask of podcast summarization and show that we can substantively improve ROUGE\nscores and reduce the extraneous content generated in the summaries.", "published": "2021-03-03 18:30:50", "link": "http://arxiv.org/abs/2103.02585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CogNet: Bridging Linguistic Knowledge, World Knowledge and Commonsense\n  Knowledge", "abstract": "In this paper, we present CogNet, a knowledge base (KB) dedicated to\nintegrating three types of knowledge: (1) linguistic knowledge from FrameNet,\nwhich schematically describes situations, objects and events. (2) world\nknowledge from YAGO, Freebase, DBpedia and Wikidata, which provides explicit\nknowledge about specific instances. (3) commonsense knowledge from ConceptNet,\nwhich describes implicit general facts. To model these different types of\nknowledge consistently, we introduce a three-level unified frame-styled\nrepresentation architecture. To integrate free-form commonsense knowledge with\nother structured knowledge, we propose a strategy that combines automated\nlabeling and crowdsourced annotation. At present, CogNet integrates 1,000+\nsemantic frames from linguistic KBs, 20,000,000+ frame instances from world\nKBs, as well as 90,000+ commonsense assertions from commonsense KBs. All these\ndata can be easily queried and explored on our online platform, and free to\ndownload in RDF format for utilization under a CC-BY-SA 4.0 license. The demo\nand data are available at http://cognet.top/.", "published": "2021-03-03 02:47:18", "link": "http://arxiv.org/abs/2103.02141v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Iterative Contextualization Algorithm with Second-Order Attention", "abstract": "Combining the representations of the words that make up a sentence into a\ncohesive whole is difficult, since it needs to account for the order of words,\nand to establish how the words present relate to each other. The solution we\npropose consists in iteratively adjusting the context. Our algorithm starts\nwith a presumably erroneous value of the context, and adjusts this value with\nrespect to the tokens at hand. In order to achieve this, representations of\nwords are built combining their symbolic embedding with a positional encoding\ninto single vectors. The algorithm then iteratively weighs and aggregates these\nvectors using our novel second-order attention mechanism. Our models report\nstrong results in several well-known text classification tasks.", "published": "2021-03-03 05:34:50", "link": "http://arxiv.org/abs/2103.02190v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "An Attention Based Neural Network for Code Switching Detection: English\n  & Roman Urdu", "abstract": "Code-switching is a common phenomenon among people with diverse lingual\nbackground and is widely used on the internet for communication purposes. In\nthis paper, we present a Recurrent Neural Network combined with the Attention\nModel for Language Identification in Code-Switched Data in English and low\nresource Roman Urdu. The attention model enables the architecture to learn the\nimportant features of the languages hence classifying the code switched data.\nWe demonstrated our approach by comparing the results with state of the art\nmodels i.e. Hidden Markov Models, Conditional Random Field and Bidirectional\nLSTM. The models evaluation, using confusion matrix metrics, showed that the\nattention mechanism provides improved the precision and accuracy as compared to\nthe other models.", "published": "2021-03-03 08:36:01", "link": "http://arxiv.org/abs/2103.02252v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Meta-Curriculum Learning for Domain Adaptation in Neural Machine\n  Translation", "abstract": "Meta-learning has been sufficiently validated to be beneficial for\nlow-resource neural machine translation (NMT). However, we find that\nmeta-trained NMT fails to improve the translation performance of the domain\nunseen at the meta-training stage. In this paper, we aim to alleviate this\nissue by proposing a novel meta-curriculum learning for domain adaptation in\nNMT. During meta-training, the NMT first learns the similar curricula from each\ndomain to avoid falling into a bad local optimum early, and finally learns the\ncurricula of individualities to improve the model robustness for learning\ndomain-specific knowledge. Experimental results on 10 different low-resource\ndomains show that meta-curriculum learning can improve the translation\nperformance of both familiar and unfamiliar domains. All the codes and data are\nfreely available at https://github.com/NLP2CT/Meta-Curriculum.", "published": "2021-03-03 08:58:39", "link": "http://arxiv.org/abs/2103.02262v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simplified Data Wrangling with ir_datasets", "abstract": "Managing the data for Information Retrieval (IR) experiments can be\nchallenging. Dataset documentation is scattered across the Internet and once\none obtains a copy of the data, there are numerous different data formats to\nwork with. Even basic formats can have subtle dataset-specific nuances that\nneed to be considered for proper use. To help mitigate these challenges, we\nintroduce a new robust and lightweight tool (ir_datasets) for acquiring,\nmanaging, and performing typical operations over datasets used in IR. We\nprimarily focus on textual datasets used for ad-hoc search. This tool provides\nboth a Python and command line interface to numerous IR datasets and\nbenchmarks. To our knowledge, this is the most extensive tool of its kind.\nIntegrations with popular IR indexing and experimentation toolkits demonstrate\nthe tool's utility. We also provide documentation of these datasets through the\nir_datasets catalog: https://ir-datasets.com/. The catalog acts as a hub for\ninformation on datasets used in IR, providing core information about what data\neach benchmark provides as well as links to more detailed information. We\nwelcome community contributions and intend to continue to maintain and grow\nthis tool.", "published": "2021-03-03 09:38:36", "link": "http://arxiv.org/abs/2103.02280v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Combining Prediction and Interpretation in Decision Trees (PrInDT) -- a\n  Linguistic Example", "abstract": "In this paper, we show that conditional inference trees and ensembles are\nsuitable methods for modeling linguistic variation. As against earlier\nlinguistic applications, however, we claim that their suitability is strongly\nincreased if we combine prediction and interpretation. To that end, we have\ndeveloped a statistical method, PrInDT (Prediction and Interpretation with\nDecision Trees), which we introduce and discuss in the present paper.", "published": "2021-03-03 11:32:20", "link": "http://arxiv.org/abs/2103.02336v2", "categories": ["cs.CL", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Weakly-Supervised Open-Retrieval Conversational Question Answering", "abstract": "Recent studies on Question Answering (QA) and Conversational QA (ConvQA)\nemphasize the role of retrieval: a system first retrieves evidence from a large\ncollection and then extracts answers. This open-retrieval ConvQA setting\ntypically assumes that each question is answerable by a single span of text\nwithin a particular passage (a span answer). The supervision signal is thus\nderived from whether or not the system can recover an exact match of this\nground-truth answer span from the retrieved passages. This method is referred\nto as span-match weak supervision. However, information-seeking conversations\nare challenging for this span-match method since long answers, especially\nfreeform answers, are not necessarily strict spans of any passage. Therefore,\nwe introduce a learned weak supervision approach that can identify a\nparaphrased span of the known answer in a passage. Our experiments on QuAC and\nCoQA datasets show that the span-match weak supervisor can only handle\nconversations with span answers, and has less satisfactory results for freeform\nanswers generated by people. Our method is more flexible as it can handle both\nspan answers and freeform answers. Moreover, our method can be more powerful\nwhen combined with the span-match method which shows it is complementary to the\nspan-match method. We also conduct in-depth analyses to show more insights on\nopen-retrieval ConvQA under a weak supervision setting.", "published": "2021-03-03 17:23:30", "link": "http://arxiv.org/abs/2103.02537v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Novel Context-Aware Multimodal Framework for Persian Sentiment\n  Analysis", "abstract": "Most recent works on sentiment analysis have exploited the text modality.\nHowever, millions of hours of video recordings posted on social media platforms\neveryday hold vital unstructured information that can be exploited to more\neffectively gauge public perception. Multimodal sentiment analysis offers an\ninnovative solution to computationally understand and harvest sentiments from\nvideos by contextually exploiting audio, visual and textual cues. In this\npaper, we, firstly, present a first of its kind Persian multimodal dataset\ncomprising more than 800 utterances, as a benchmark resource for researchers to\nevaluate multimodal sentiment analysis approaches in Persian language.\nSecondly, we present a novel context-aware multimodal sentiment analysis\nframework, that simultaneously exploits acoustic, visual and textual cues to\nmore accurately determine the expressed sentiment. We employ both\ndecision-level (late) and feature-level (early) fusion methods to integrate\naffective cross-modal information. Experimental results demonstrate that the\ncontextual integration of multimodal features such as textual, acoustic and\nvisual features deliver better performance (91.39%) compared to unimodal\nfeatures (89.24%).", "published": "2021-03-03 19:09:01", "link": "http://arxiv.org/abs/2103.02636v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Natural Language Understanding for Argumentative Dialogue Systems in the\n  Opinion Building Domain", "abstract": "This paper introduces a natural language understanding (NLU) framework for\nargumentative dialogue systems in the information-seeking and opinion building\ndomain. The proposed framework consists of two sub-models, namely intent\nclassifier and argument similarity. Intent classifier model stacks BiLSTM with\nattention mechanism on top of the pre-trained BERT model and fine-tune the\nmodel for recognizing the user intent, whereas the argument similarity model\nemploys BERT+BiLSTM for identifying system arguments the user refers to in his\nor her natural language utterances. Our model is evaluated in an argumentative\ndialogue system that engages the user to inform him-/herself about a\ncontroversial topic by exploring pro and con arguments and build his/her\nopinion towards the topic. In order to evaluate the proposed approach, we\ncollect user utterances for the interaction with the respective system labeling\nintent and referenced argument in an extensive online study. The data\ncollection includes multiple topics and two different user types (native\nEnglish speakers from the UK and non-native English speakers from China).\nAdditionally, we evaluate the proposed intent classifier and argument\nsimilarity models separately on the publicly available Banking77 and STS\nbenchmark datasets. The evaluation indicates a clear advantage of the utilized\ntechniques over baseline approaches on several datasets, as well as the\nrobustness of the proposed approach against new topics and different language\nproficiency as well as the cultural background of the user. Furthermore,\nresults show that our intent classifier model outperforms DIET, DistillBERT,\nand BERT fine-tuned models in few-shot setups (i.e., with 10, 20, or 30 labeled\nexamples per intent) and full data setup.", "published": "2021-03-03 21:17:24", "link": "http://arxiv.org/abs/2103.02691v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Compute and memory efficient universal sound source separation", "abstract": "Recent progress in audio source separation lead by deep learning has enabled\nmany neural network models to provide robust solutions to this fundamental\nestimation problem. In this study, we provide a family of efficient neural\nnetwork architectures for general purpose audio source separation while\nfocusing on multiple computational aspects that hinder the application of\nneural networks in real-world scenarios. The backbone structure of this\nconvolutional network is the SUccessive DOwnsampling and Resampling of\nMulti-Resolution Features (SuDoRM-RF) as well as their aggregation which is\nperformed through simple one-dimensional convolutions. This mechanism enables\nour models to obtain high fidelity signal separation in a wide variety of\nsettings where variable number of sources are present and with limited\ncomputational resources (e.g. floating point operations, memory footprint,\nnumber of parameters and latency). Our experiments show that SuDoRM-RF models\nperform comparably and even surpass several state-of-the-art benchmarks with\nsignificantly higher computational resource requirements. The causal variation\nof SuDoRM-RF is able to obtain competitive performance in real-time speech\nseparation of around 10dB scale-invariant signal-to-distortion ratio\nimprovement (SI-SDRi) while remaining up to 20 times faster than real-time on a\nlaptop device.", "published": "2021-03-03 19:16:53", "link": "http://arxiv.org/abs/2103.02644v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Malware Classification with Word Embedding Features", "abstract": "Malware classification is an important and challenging problem in information\nsecurity. Modern malware classification techniques rely on machine learning\nmodels that can be trained on features such as opcode sequences, API calls, and\nbyte $n$-grams, among many others. In this research, we consider opcode\nfeatures. We implement hybrid machine learning techniques, where we engineer\nfeature vectors by training hidden Markov models -- a technique that we refer\nto as HMM2Vec -- and Word2Vec embeddings on these opcode sequences. The\nresulting HMM2Vec and Word2Vec embedding vectors are then used as features for\nclassification algorithms. Specifically, we consider support vector machine\n(SVM), $k$-nearest neighbor ($k$-NN), random forest (RF), and convolutional\nneural network (CNN) classifiers. We conduct substantial experiments over a\nvariety of malware families. Our experiments extend well beyond any previous\nwork in this field.", "published": "2021-03-03 21:57:11", "link": "http://arxiv.org/abs/2103.02711v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Morality, Machines and the Interpretation Problem: A Value-based,\n  Wittgensteinian Approach to Building Moral Agents", "abstract": "We present what we call the Interpretation Problem, whereby any rule in\nsymbolic form is open to infinite interpretation in ways that we might\ndisapprove of and argue that any attempt to build morality into machines is\nsubject to it. We show how the Interpretation Problem in Artificial\nIntelligence is an illustration of Wittgenstein's general claim that no rule\ncan contain the criteria for its own application, and that the risks created by\nthis problem escalate in proportion to the degree to which to machine is\ncausally connected to the world, in what we call the Law of Interpretative\nExposure. Using game theory, we attempt to define the structure of normative\nspaces and argue that any rule-following within a normative space is guided by\nvalues that are external to that space and which cannot themselves be\nrepresented as rules. In light of this, we categorise the types of mistakes an\nartificial moral agent could make into Mistakes of Intention and Instrumental\nMistakes, and we propose ways of building morality into machines by getting\nthem to interpret the rules we give in accordance with these external values,\nthrough explicit moral reasoning, the Show, not Tell paradigm, the adjustment\nof causal power and structure of the agent, and relational values, with the\nultimate aim that the machine develop a virtuous character and that the impact\nof the Interpretation Problem is minimised.", "published": "2021-03-03 22:34:01", "link": "http://arxiv.org/abs/2103.02728v4", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "Stay on Topic, Please: Aligning User Comments to the Content of a News\n  Article", "abstract": "Social scientists have shown that up to 50% if the content posted to a news\narticle have no relation to its journalistic content. In this study we propose\na classification algorithm to categorize user comments posted to a new article\nbase don their alignment to its content. The alignment seek to match user\ncomments to an article based on similarity off content, entities in discussion,\nand topic. We proposed a BERTAC, BAERT-based approach that learn jointly\narticle-comment embeddings and infers the relevance class of comments. We\nintroduce an ordinal classification loss that penalizes the difference between\nthe predicted and true label. We conduct a thorough study to show influence of\nthe proposed loss on the learning process. The results on five representative\nnews outlets show that our approach can learn the comment class with up to 36%\naverage accuracy improvement compering to the baselines, and up to 25%\ncompering to the BA-BC model. BA-BC is out approach that consists of two models\naimed to capture dis-jointly the formal language of news articles and the\ninformal language of comments. We also conduct a user study to evaluate human\nlabeling performance to understand the difficulty of the classification task.\nThe user agreement on comment-article alignment is \"moderate\" per\nKrippendorff's alpha score, which suggests that the classification task is\ndifficult.", "published": "2021-03-03 18:29:00", "link": "http://arxiv.org/abs/2103.06130v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Multi-Channel and Multi-Microphone Acoustic Echo Cancellation Using A\n  Deep Learning Based Approach", "abstract": "Building on the deep learning based acoustic echo cancellation (AEC) in the\nsingle-loudspeaker (single-channel) and single-microphone setup, this paper\ninvestigates multi-channel AEC (MCAEC) and multi-microphone AEC (MMAEC). We\ntrain a deep neural network (DNN) to predict the near-end speech from\nmicrophone signals with far-end signals used as additional information. We find\nthat the deep learning approach avoids the non-uniqueness problem in\ntraditional MCAEC algorithms. For the AEC setup with multiple microphones,\nrather than employing AEC for each microphone, a single DNN is trained to\nachieve echo removal for all microphones. Also, combining deep learning based\nAEC with deep learning based beamforming further improves the system\nperformance. Experimental results show the effectiveness of both bidirectional\nlong short-term memory (BLSTM) and convolutional recurrent network (CRN) based\nmethods for MCAEC and MMAEC. Furthermore, deep learning based methods are\ncapable of removing echo and noise simultaneously and work well in the presence\nof nonlinear distortions.", "published": "2021-03-03 17:47:06", "link": "http://arxiv.org/abs/2103.02552v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reverb Conversion of Mixed Vocal Tracks Using an End-to-end\n  Convolutional Deep Neural Network", "abstract": "Reverb plays a critical role in music production, where it provides listeners\nwith spatial realization, timbre, and texture of the music. Yet, it is\nchallenging to reproduce the musical reverb of a reference music track even by\nskilled engineers. In response, we propose an end-to-end system capable of\nswitching the musical reverb factor of two different mixed vocal tracks. This\nmethod enables us to apply the reverb of the reference track to the source\ntrack to which the effect is desired. Further, our model can perform\nde-reverberation when the reference track is used as a dry vocal source. The\nproposed model is trained in combination with an adversarial objective, which\nmakes it possible to handle high-resolution audio samples. The perceptual\nevaluation confirmed that the proposed model can convert the reverb factor with\nthe preferred rate of 64.8%. To the best of our knowledge, this is the first\nattempt to apply deep neural networks to converting music reverb of vocal\ntracks.", "published": "2021-03-03 03:02:27", "link": "http://arxiv.org/abs/2103.02147v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Auditory Attention Decoding from EEG using Convolutional Recurrent\n  Neural Network", "abstract": "The auditory attention decoding (AAD) approach was proposed to determine the\nidentity of the attended talker in a multi-talker scenario by analyzing\nelectroencephalography (EEG) data. Although the linear model-based method has\nbeen widely used in AAD, the linear assumption was considered oversimplified\nand the decoding accuracy remained lower for shorter decoding windows.\nRecently, nonlinear models based on deep neural networks (DNN) have been\nproposed to solve this problem. However, these models did not fully utilize\nboth the spatial and temporal features of EEG, and the interpretability of DNN\nmodels was rarely investigated. In this paper, we proposed novel convolutional\nrecurrent neural network (CRNN) based regression model and classification\nmodel, and compared them with both the linear model and the state-of-the-art\nDNN models. Results showed that, our proposed CRNN-based classification model\noutperformed others for shorter decoding windows (around 90% for 2 s and 5 s).\nAlthough worse than classification models, the decoding accuracy of the\nproposed CRNN-based regression model was about 5% greater than other regression\nmodels. The interpretability of DNN models was also investigated by visualizing\nlayers' weight.", "published": "2021-03-03 05:09:40", "link": "http://arxiv.org/abs/2103.02183v1", "categories": ["eess.SP", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Open community platform for hearing aid algorithm research: open Master\n  Hearing Aid (openMHA)", "abstract": "open Master Hearing Aid (openMHA) was developed and provided to the hearing\naid research community as an open-source software platform with the aim to\nsupport sustainable and reproducible research towards improvement and new types\nof assistive hearing systems not limited by proprietary software. The software\noffers a flexible framework that allows the users to conduct hearing aid\nresearch using tools and a number of signal processing plugins provided with\nthe software as well as the implementation of own methods. The openMHA software\nis independent of a specific hardware and supports Linux, macOS and Windows\noperating systems as well as 32-bit and 64-bit ARM-based architectures such as\nused in small portable integrated systems. www.openmha.org", "published": "2021-03-03 10:41:22", "link": "http://arxiv.org/abs/2103.02313v3", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Multi-view Audio and Music Classification", "abstract": "We propose in this work a multi-view learning approach for audio and music\nclassification. Considering four typical low-level representations (i.e.\ndifferent views) commonly used for audio and music recognition tasks, the\nproposed multi-view network consists of four subnetworks, each handling one\ninput types. The learned embedding in the subnetworks are then concatenated to\nform the multi-view embedding for classification similar to a simple\nconcatenation network. However, apart from the joint classification branch, the\nnetwork also maintains four classification branches on the single-view\nembedding of the subnetworks. A novel method is then proposed to keep track of\nthe learning behavior on the classification branches and adapt their weights to\nproportionally blend their gradients for network training. The weights are\nadapted in such a way that learning on a branch that is generalizing well will\nbe encouraged whereas learning on a branch that is overfitting will be slowed\ndown. Experiments on three different audio and music classification tasks show\nthat the proposed multi-view network not only outperforms the single-view\nbaselines but also is superior to the multi-view baselines based on\nconcatenation and late fusion.", "published": "2021-03-03 14:18:04", "link": "http://arxiv.org/abs/2103.02420v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The effect of speech and noise levels on the quality perceived by\n  cochlear implant and normal hearing listeners", "abstract": "Electrical hearing by cochlear implants (CIs) may be fundamentally different\nfrom acoustic hearing by normal-hearing (NH) listeners, presumably showing\nunequal speech quality perception in various noise environments. Noise\nreduction (NR) algorithms used in CI reduce the noise in favor of\nsignal-to-noise ratio (SNR), regardless of plausible accompanying distortions\nthat may degrade the speech quality perception. To gain better understanding of\nCI speech quality perception, the present work aimed investigating speech\nquality perception in a diverse noise conditions, including factors of\nspeech/noise levels, type of noise, and distortions caused by NR models.\nFifteen NH and seven CI subjects participated in this study. Speech sentences\nwere set to two different levels (65 and 75 dB SPL). Two types of noise\n(Cafeteria and Babble) at three levels (55, 65, and 75 dB SPL) were used.\nSentences were processed using two NR algorithms to investigate the perceptual\nsensitivity of CI and NH listeners to the distortion. All sentences processed\nwith the combinations of these sets were presented to CI and NH listeners, and\nthey were asked to rate the sound quality of speech as they perceived. The\neffect of each factor on the perceived speech quality was investigated based on\nthe group averaged quality rated by CI and NH listeners. Consistent with\nprevious studies, CI listeners were not as sensitive as NH to the distortion\nmade by NR algorithms. Statistical analysis showed that the speech level has\nsignificant effect on quality perception. At the same SNR, the quality of 65 dB\nspeech was rated higher than that of 75 dB for CI users, but vice versa for NH\nlisteners. Therefore, the present study showed that the perceived speech\nquality patterns were different between CI and NH listeners in terms of their\nsensitivity to distortion and speech level in complex listening environment.", "published": "2021-03-03 14:19:02", "link": "http://arxiv.org/abs/2103.02421v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The Spatial Selective Auditory Attention of Cochlear Implant Users in\n  Different Conversational Sound Levels", "abstract": "In multi speakers environments, cochlear implant (CI) users may attend to a\ntarget sound source in a different manner from the normal hearing (NH)\nindividuals during a conversation. This study attempted to investigate the\neffect of conversational sound levels on the mechanisms adopted by CI and NH\nlisteners in selective auditory attention and how it affects their daily\nconversation. Nine CI users (five bilateral, three unilateral, and one bimodal)\nand eight NH listeners participated in this study. The behavioral speech\nrecognition scores were collected using a matrix sentences test and neural\ntracking to speech envelope was recorded using electroencephalography (EEG).\nSpeech stimuli were presented at three different levels (75, 65, and 55 dB SPL)\nin the presence of two maskers from three spatially separated speakers.\nDifferent combinations of assisted/impaired hearing modes were evaluated for CI\nusers and the outcomes were analyzed in three categories: electric hearing\nonly, acoustic hearing only, and electric+acoustic hearing. Our results showed\nthat increasing the conversational sound level degraded the selective auditory\nattention in electrical hearing. On the other hand, increasing the sound level\nimproved the selective auditory attention for the acoustic hearing group. In NH\nlisteners, however, increasing the sound level did not cause a significant\nchange in the auditory attention. Our result implies that the effect of the\nsound level on the selective auditory attention varies depending on hearing\nmodes and the loudness control is necessary for the ease of attending to the\nconversation by CI users.", "published": "2021-03-03 21:48:58", "link": "http://arxiv.org/abs/2103.02703v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Continuous Speech Separation with Ad Hoc Microphone Arrays", "abstract": "Speech separation has been shown effective for multi-talker speech\nrecognition. Under the ad hoc microphone array setup where the array consists\nof spatially distributed asynchronous microphones, additional challenges must\nbe overcome as the geometry and number of microphones are unknown beforehand.\nPrior studies show, with a spatial-temporalinterleaving structure, neural\nnetworks can efficiently utilize the multi-channel signals of the ad hoc array.\nIn this paper, we further extend this approach to continuous speech separation.\nSeveral techniques are introduced to enable speech separation for real\ncontinuous recordings. First, we apply a transformer-based network for\nspatio-temporal modeling of the ad hoc array signals. In addition, two methods\nare proposed to mitigate a speech duplication problem during single talker\nsegments, which seems more severe in the ad hoc array scenarios. One method is\ndevice distortion simulation for reducing the acoustic mismatch between\nsimulated training data and real recordings. The other is speaker counting to\ndetect the single speaker segments and merge the output signal channels.\nExperimental results for AdHoc-LibiCSS, a new dataset consisting of continuous\nrecordings of concatenated LibriSpeech utterances obtained by multiple\ndifferent devices, show the proposed separation method can significantly\nimprove the ASR accuracy for overlapped speech with little performance\ndegradation for single talker segments.", "published": "2021-03-03 13:01:08", "link": "http://arxiv.org/abs/2103.02378v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
