{"title": "Deep Reinforcement Learning for Dialogue Generation", "abstract": "Recent neural models of dialogue generation offer great promise for\ngenerating responses for conversational agents, but tend to be shortsighted,\npredicting utterances one at a time while ignoring their influence on future\noutcomes. Modeling the future direction of a dialogue is crucial to generating\ncoherent, interesting dialogues, a need which led traditional NLP models of\ndialogue to draw on reinforcement learning. In this paper, we show how to\nintegrate these goals, applying deep reinforcement learning to model future\nreward in chatbot dialogue. The model simulates dialogues between two virtual\nagents, using policy gradient methods to reward sequences that display three\nuseful conversational properties: informativity (non-repetitive turns),\ncoherence, and ease of answering (related to forward-looking function). We\nevaluate our model on diversity, length as well as with human judges, showing\nthat the proposed algorithm generates more interactive responses and manages to\nfoster a more sustained conversation in dialogue simulation. This work marks a\nfirst step towards learning a neural conversational model based on the\nlong-term success of dialogues.", "published": "2016-06-05 17:59:23", "link": "http://arxiv.org/abs/1606.01541v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Net Models for Open-Domain Discourse Coherence", "abstract": "Discourse coherence is strongly associated with text quality, making it\nimportant to natural language generation and understanding. Yet existing models\nof coherence focus on measuring individual aspects of coherence (lexical\noverlap, rhetorical structure, entity centering) in narrow domains.\n  In this paper, we describe domain-independent neural models of discourse\ncoherence that are capable of measuring multiple aspects of coherence in\nexisting sentences and can maintain coherence while generating new sentences.\nWe study both discriminative models that learn to distinguish coherent from\nincoherent discourse, and generative models that produce coherent text,\nincluding a novel neural latent-variable Markovian generative model that\ncaptures the latent discourse dependencies between sentences in a text.\n  Our work achieves state-of-the-art performance on multiple coherence\nevaluations, and marks an initial step in generating coherent texts given\ndiscourse contexts.", "published": "2016-06-05 18:29:45", "link": "http://arxiv.org/abs/1606.01545v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gated-Attention Readers for Text Comprehension", "abstract": "In this paper we study the problem of answering cloze-style questions over\ndocuments. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop\narchitecture with a novel attention mechanism, which is based on multiplicative\ninteractions between the query embedding and the intermediate states of a\nrecurrent neural network document reader. This enables the reader to build\nquery-specific representations of tokens in the document for accurate answer\nselection. The GA Reader obtains state-of-the-art results on three benchmarks\nfor this task--the CNN \\& Daily Mail news stories and the Who Did What dataset.\nThe effectiveness of multiplicative interaction is demonstrated by an ablation\nstudy, and by comparing to alternative compositional operators for implementing\nthe gated-attention. The code is available at\nhttps://github.com/bdhingra/ga-reader.", "published": "2016-06-05 19:30:39", "link": "http://arxiv.org/abs/1606.01549v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Coordination in Categorical Compositional Distributional Semantics", "abstract": "An open problem with categorical compositional distributional semantics is\nthe representation of words that are considered semantically vacuous from a\ndistributional perspective, such as determiners, prepositions, relative\npronouns or coordinators. This paper deals with the topic of coordination\nbetween identical syntactic types, which accounts for the majority of\ncoordination cases in language. By exploiting the compact closed structure of\nthe underlying category and Frobenius operators canonically induced over the\nfixed basis of finite-dimensional vector spaces, we provide a morphism as\nrepresentation of a coordinator tensor, and we show how it lifts from atomic\ntypes to compound types. Linguistic intuitions are provided, and the importance\nof the Frobenius operators as an addition to the compact closed setting with\nregard to language is discussed.", "published": "2016-06-05 14:26:56", "link": "http://arxiv.org/abs/1606.01515v2", "categories": ["cs.CL", "cs.AI", "math.CT"], "primary_category": "cs.CL"}
