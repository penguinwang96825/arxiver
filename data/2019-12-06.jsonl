{"title": "Explaining Sequence-Level Knowledge Distillation as Data-Augmentation\n  for Neural Machine Translation", "abstract": "Sequence-level knowledge distillation (SLKD) is a model compression technique\nthat leverages large, accurate teacher models to train smaller,\nunder-parameterized student models. Why does pre-processing MT data with SLKD\nhelp us train smaller models? We test the common hypothesis that SLKD addresses\na capacity deficiency in students by \"simplifying\" noisy data points and find\nit unlikely in our case. Models trained on concatenations of original and\n\"simplified\" datasets generalize just as well as baseline SLKD. We then propose\nan alternative hypothesis under the lens of data augmentation and\nregularization. We try various augmentation strategies and observe that dropout\nregularization can become unnecessary. Our methods achieve BLEU gains of\n0.7-1.2 on TED Talks.", "published": "2019-12-06 20:27:38", "link": "http://arxiv.org/abs/1912.03334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Decomposing predictability: Semantic feature overlap between words and\n  the dynamics of reading for meaning", "abstract": "The present study uses a computational approach to examine the role of\nsemantic constraints in normal reading. This methodology avoids confounds\ninherent in conventional measures of predictability, allowing for theoretically\ndeeper accounts of semantic processing. We start from a definition of\nassociations between words based on the significant log likelihood that two\nwords co-occur frequently together in the sentences of a large text corpus.\nDirect associations between stimulus words were controlled, and semantic\nfeature overlap between prime and target words was manipulated by their common\nassociates. The stimuli consisted of sentences of the form pronoun, verb,\narticle, adjective and noun, followed by a series of closed class words, e. g.\n\"She rides the grey elephant on one of her many exploratory voyages\". The\nresults showed that verb-noun overlap reduces single and first fixation\ndurations of the target noun and adjective-noun overlap reduces go-past\ndurations. A dynamic spreading of activation account suggests that associates\nof the prime words take some time to become activated: The verb can act on the\ntarget noun's early eye-movement measures presented three words later, while\nthe adjective is presented immediately prior to the target, which induces\nsentence re-examination after a difficult adjective-noun semantic integration.", "published": "2019-12-06 09:36:15", "link": "http://arxiv.org/abs/1912.10164v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A limited-size ensemble of homogeneous CNN/LSTMs for high-performance\n  word classification", "abstract": "In recent years, long short-term memory neural networks (LSTMs) have been\napplied quite successfully to problems in handwritten text recognition.\nHowever, their strength is more located in handling sequences of variable\nlength than in handling geometric variability of the image patterns.\nFurthermore, the best results for LSTMs are often based on large-scale training\nof an ensemble of network instances. In this paper, an end-to-end convolutional\nLSTM Neural Network is used to handle both geometric variation and sequence\nvariability. We show that high performances can be reached on a common\nbenchmark set by using proper data augmentation for just five such networks\nusing a proper coding scheme and a proper voting scheme. The networks have\nsimilar architectures (Convolutional Neural Network (CNN): five layers,\nbidirectional LSTM (BiLSTM): three layers followed by a connectionist temporal\nclassification (CTC) processing step). The approach assumes differently-scaled\ninput images and different feature map sizes. Two datasets are used for\nevaluation of the performance of our algorithm: A standard benchmark RIMES\ndataset (French), and a historical handwritten dataset KdK (Dutch). Final\nperformance obtained for the word-recognition test of RIMES was 96.6%, a clear\nimprovement over other state-of-the-art approaches. On the KdK dataset, our\napproach also shows good results. The proposed approach is deployed in the Monk\nsearch engine for historical-handwriting collections.", "published": "2019-12-06 16:45:52", "link": "http://arxiv.org/abs/1912.03223v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Med2Meta: Learning Representations of Medical Concepts with\n  Meta-Embeddings", "abstract": "Distributed representations of medical concepts have been used to support\ndownstream clinical tasks recently. Electronic Health Records (EHR) capture\ndifferent aspects of patients' hospital encounters and serve as a rich source\nfor augmenting clinical decision making by learning robust medical concept\nembeddings. However, the same medical concept can be recorded in different\nmodalities (e.g., clinical notes, lab results)-with each capturing salient\ninformation unique to that modality-and a holistic representation calls for\nrelevant feature ensemble from all information sources. We hypothesize that\nrepresentations learned from heterogeneous data types would lead to performance\nenhancement on various clinical informatics and predictive modeling tasks. To\nthis end, our proposed approach makes use of meta-embeddings, embeddings\naggregated from learned embeddings. Firstly, modality-specific embeddings for\neach medical concept is learned with graph autoencoders. The ensemble of all\nthe embeddings is then modeled as a meta-embedding learning problem to\nincorporate their correlating and complementary information through a joint\nreconstruction. Empirical results of our model on both quantitative and\nqualitative clinical evaluations have shown improvements over state-of-the-art\nembedding models, thus validating our hypothesis.", "published": "2019-12-06 22:11:37", "link": "http://arxiv.org/abs/1912.03366v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can AI Generate Love Advice?: Toward Neural Answer Generation for\n  Non-Factoid Questions", "abstract": "Deep learning methods that extract answers for non-factoid questions from QA\nsites are seen as critical since they can assist users in reaching their next\ndecisions through conversations with AI systems. The current methods, however,\nhave the following two problems: (1) They can not understand the ambiguous use\nof words in the questions as word usage can strongly depend on the context. As\na result, the accuracies of their answer selections are not good enough. (2)\nThe current methods can only select from among the answers held by QA sites and\ncan not generate new ones. Thus, they can not answer the questions that are\nsomewhat different with those stored in QA sites. Our solution, Neural Answer\nConstruction Model, tackles these problems as it: (1) Incorporates the biases\nof semantics behind questions into word embeddings while also computing them\nregardless of the semantics. As a result, it can extract answers that suit the\ncontexts of words used in the question as well as following the common usage of\nwords across semantics. This improves the accuracy of answer selection. (2)\nUses biLSTM to compute the embeddings of questions as well as those of the\nsentences often used to form answers. It then simultaneously learns the optimum\ncombination of those sentences as well as the closeness between the question\nand those sentences. As a result, our model can construct an answer that\ncorresponds to the situation that underlies the question; it fills the gap\nbetween answer selection and generation and is the first model to move beyond\nthe current simple answer selection model for non-factoid QAs. Evaluations\nusing datasets created for love advice stored in the Japanese QA site, Oshiete\ngoo, indicate that our model achieves 20% higher accuracy in answer creation\nthan the strong baselines. Our model is practical and has already been applied\nto the love advice service in Oshiete goo.", "published": "2019-12-06 09:57:22", "link": "http://arxiv.org/abs/1912.10163v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synchronous Transformers for End-to-End Speech Recognition", "abstract": "For most of the attention-based sequence-to-sequence models, the decoder\npredicts the output sequence conditioned on the entire input sequence processed\nby the encoder. The asynchronous problem between the encoding and decoding\nmakes these models difficult to be applied for online speech recognition. In\nthis paper, we propose a model named synchronous transformer to address this\nproblem, which can predict the output sequence chunk by chunk. Once a\nfixed-length chunk of the input sequence is processed by the encoder, the\ndecoder begins to predict symbols immediately. During training, a\nforward-backward algorithm is introduced to optimize all the possible alignment\npaths. Our model is evaluated on a Mandarin dataset AISHELL-1. The experiments\nshow that the synchronous transformer is able to perform encoding and decoding\nsynchronously, and achieves a character error rate of 8.91% on the test set.", "published": "2019-12-06 03:05:12", "link": "http://arxiv.org/abs/1912.02958v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "abstract": "We describe the Sentiment Analysis in Twitter task, ran as part of\nSemEval-2014. It is a continuation of the last year's task that ran\nsuccessfully as part of SemEval-2013. As in 2013, this was the most popular\nSemEval task; a total of 46 teams contributed 27 submissions for subtask A (21\nteams) and 50 submissions for subtask B (44 teams). This year, we introduced\nthree new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii)\nLiveJournal sentences. We further tested on (iv) 2013 tweets, and (v) 2013 SMS\nmessages. The highest F1-score on (i) was achieved by NRC-Canada at 86.63 for\nsubtask A and by TeamX at 70.96 for subtask B.", "published": "2019-12-06 06:23:19", "link": "http://arxiv.org/abs/1912.02990v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Machine Translation Evaluation Meets Community Question Answering", "abstract": "We explore the applicability of machine translation evaluation (MTE) methods\nto a very different problem: answer ranking in community Question Answering. In\nparticular, we adopt a pairwise neural network (NN) architecture, which\nincorporates MTE features, as well as rich syntactic and semantic embeddings,\nand which efficiently models complex non-linear interactions. The evaluation\nresults show state-of-the-art performance, with sizeable contribution from both\nthe MTE features and from the pairwise NN architecture.", "published": "2019-12-06 06:35:21", "link": "http://arxiv.org/abs/1912.02998v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Semantic Mask for Transformer based End-to-End Speech Recognition", "abstract": "Attention-based encoder-decoder model has achieved impressive results for\nboth automatic speech recognition (ASR) and text-to-speech (TTS) tasks. This\napproach takes advantage of the memorization capacity of neural networks to\nlearn the mapping from the input sequence to the output sequence from scratch,\nwithout the assumption of prior knowledge such as the alignments. However, this\nmodel is prone to overfitting, especially when the amount of training data is\nlimited. Inspired by SpecAugment and BERT, in this paper, we propose a semantic\nmask based regularization for training such kind of end-to-end (E2E) model. The\nidea is to mask the input features corresponding to a particular output token,\ne.g., a word or a word-piece, in order to encourage the model to fill the token\nbased on the contextual information. While this approach is applicable to the\nencoder-decoder framework with any type of neural network architecture, we\nstudy the transformer-based model for ASR in this work. We perform experiments\non Librispeech 960h and TedLium2 data sets, and achieve the state-of-the-art\nperformance on the test set in the scope of E2E models.", "published": "2019-12-06 07:55:04", "link": "http://arxiv.org/abs/1912.03010v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Integrating Deep Learning with Logic Fusion for Information Extraction", "abstract": "Information extraction (IE) aims to produce structured information from an\ninput text, e.g., Named Entity Recognition and Relation Extraction. Various\nattempts have been proposed for IE via feature engineering or deep learning.\nHowever, most of them fail to associate the complex relationships inherent in\nthe task itself, which has proven to be especially crucial. For example, the\nrelation between 2 entities is highly dependent on their entity types. These\ndependencies can be regarded as complex constraints that can be efficiently\nexpressed as logical rules. To combine such logic reasoning capabilities with\nlearning capabilities of deep neural networks, we propose to integrate logical\nknowledge in the form of first-order logic into a deep learning system, which\ncan be trained jointly in an end-to-end manner. The integrated framework is\nable to enhance neural outputs with knowledge regularization via logic rules,\nand at the same time update the weights of logic rules to comply with the\ncharacteristics of the training data. We demonstrate the effectiveness and\ngeneralization of the proposed model on multiple IE tasks.", "published": "2019-12-06 09:38:23", "link": "http://arxiv.org/abs/1912.03041v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Document Network Embedding: Coping for Missing Content and Missing Links", "abstract": "Searching through networks of documents is an important task. A promising\npath to improve the performance of information retrieval systems in this\ncontext is to leverage dense node and content representations learned with\nembedding techniques. However, these techniques cannot learn representations\nfor documents that are either isolated or whose content is missing. To tackle\nthis issue, assuming that the topology of the network and the content of the\ndocuments correlate, we propose to estimate the missing node representations\nfrom the available content representations, and conversely. Inspired by recent\nadvances in machine translation, we detail in this paper how to learn a linear\ntransformation from a set of aligned content and node representations. The\nprojection matrix is efficiently calculated in terms of the singular value\ndecomposition. The usefulness of the proposed method is highlighted by the\nimproved ability to predict the neighborhood of nodes whose links are\nunobserved based on the projected content representations, and to retrieve\nsimilar documents when content is missing, based on the projected node\nrepresentations.", "published": "2019-12-06 10:09:20", "link": "http://arxiv.org/abs/1912.03048v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Weak Supervision helps Emergence of Word-Object Alignment and improves\n  Vision-Language Tasks", "abstract": "The large adoption of the self-attention (i.e. transformer model) and\nBERT-like training principles has recently resulted in a number of high\nperforming models on a large panoply of vision-and-language problems (such as\nVisual Question Answering (VQA), image retrieval, etc.). In this paper we claim\nthat these State-Of-The-Art (SOTA) approaches perform reasonably well in\nstructuring information inside a single modality but, despite their impressive\nperformances , they tend to struggle to identify fine-grained inter-modality\nrelationships. Indeed, such relations are frequently assumed to be implicitly\nlearned during training from application-specific losses, mostly cross-entropy\nfor classification. While most recent works provide inductive bias for\ninter-modality relationships via cross attention modules, in this work, we\ndemonstrate (1) that the latter assumption does not hold, i.e. modality\nalignment does not necessarily emerge automatically, and (2) that adding weak\nsupervision for alignment between visual objects and words improves the quality\nof the learned models on tasks requiring reasoning. In particular , we\nintegrate an object-word alignment loss into SOTA vision-language reasoning\nmodels and evaluate it on two tasks VQA and Language-driven Comparison of\nImages. We show that the proposed fine-grained inter-modality supervision\nsignificantly improves performance on both tasks. In particular, this new\nlearning signal allows obtaining SOTA-level performances on GQA dataset (VQA\ntask) with pre-trained models without finetuning on the task, and a new SOTA on\nNLVR2 dataset (Language-driven Comparison of Images). Finally, we also\nillustrate the impact of the contribution on the models reasoning by\nvisualizing attention distributions.", "published": "2019-12-06 11:04:08", "link": "http://arxiv.org/abs/1912.03063v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CV"}
{"title": "GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions,\n  Semantic Roles, and Reader Perception", "abstract": "Most research on emotion analysis from text focuses on the task of emotion\nclassification or emotion intensity regression. Fewer works address emotions as\na phenomenon to be tackled with structured learning, which can be explained by\nthe lack of relevant datasets. We fill this gap by releasing a dataset of 5000\nEnglish news headlines annotated via crowdsourcing with their associated\nemotions, the corresponding emotion experiencers and textual cues, related\nemotion causes and targets, as well as the reader's perception of the emotion\nof the headline. This annotation task is comparably challenging, given the\nlarge number of classes and roles to be identified. We therefore propose a\nmultiphase annotation procedure in which we first find relevant instances with\nemotional content and then annotate the more fine-grained aspects. Finally, we\ndevelop a baseline for the task of automatic prediction of semantic role\nstructures and discuss the results. The corpus we release enables further\nresearch on emotion classification, emotion intensity prediction, emotion cause\ndetection, and supports further qualitative studies.", "published": "2019-12-06 15:30:58", "link": "http://arxiv.org/abs/1912.03184v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "What Do You Mean I'm Funny? Personalizing the Joke Skill of a\n  Voice-Controlled Virtual Assistant", "abstract": "A considerable part of the success experienced by Voice-controlled virtual\nassistants (VVA) is due to the emotional and personalized experience they\ndeliver, with humor being a key component in providing an engaging interaction.\nIn this paper we describe methods used to improve the joke skill of a VVA\nthrough personalization. The first method, based on traditional NLP techniques,\nis robust and scalable. The others combine self-attentional network and\nmulti-task learning to obtain better results, at the cost of added complexity.\nA significant challenge facing these systems is the lack of explicit user\nfeedback needed to provide labels for the models. Instead, we explore the use\nof two implicit feedback-based labelling strategies. All models were evaluated\non real production data. Online results show that models trained on any of the\nconsidered labels outperform a heuristic method, presenting a positive\nreal-world impact on user satisfaction. Offline results suggest that the\ndeep-learning approaches can improve the joke experience with respect to the\nother considered methods.", "published": "2019-12-06 17:17:39", "link": "http://arxiv.org/abs/1912.03234v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Audio-attention discriminative language model for ASR rescoring", "abstract": "End-to-end approaches for automatic speech recognition (ASR) benefit from\ndirectly modeling the probability of the word sequence given the input audio\nstream in a single neural network. However, compared to conventional ASR\nsystems, these models typically require more data to achieve comparable\nresults. Well-known model adaptation techniques, to account for domain and\nstyle adaptation, are not easily applicable to end-to-end systems. Conventional\nHMM-based systems, on the other hand, have been optimized for various\nproduction environments and use cases. In this work, we propose to combine the\nbenefits of end-to-end approaches with a conventional system using an\nattention-based discriminative language model that learns to rescore the output\nof a first-pass ASR system. We show that learning to rescore a list of\npotential ASR outputs is much simpler than learning to generate the hypothesis.\nThe proposed model results in 8% improvement in word error rate even when the\namount of training data is a fraction of data used for training the first-pass\nsystem.", "published": "2019-12-06 22:09:07", "link": "http://arxiv.org/abs/1912.03363v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Re-Translation Strategies For Long Form, Simultaneous, Spoken Language\n  Translation", "abstract": "We investigate the problem of simultaneous machine translation of long-form\nspeech content. We target a continuous speech-to-text scenario, generating\ntranslated captions for a live audio feed, such as a lecture or play-by-play\ncommentary. As this scenario allows for revisions to our incremental\ntranslations, we adopt a re-translation approach to simultaneous translation,\nwhere the source is repeatedly translated from scratch as it grows. This\napproach naturally exhibits very low latency and high final quality, but at the\ncost of incremental instability as the output is continuously refined. We\nexperiment with a pipeline of industry-grade speech recognition and translation\ntools, augmented with simple inference heuristics to improve stability. We use\nTED Talks as a source of multilingual test data, developing our techniques on\nEnglish-to-German spoken language translation. Our minimalist approach to\nsimultaneous translation allows us to easily scale our final evaluation to six\nmore target languages, dramatically improving incremental stability for all of\nthem.", "published": "2019-12-06 23:46:37", "link": "http://arxiv.org/abs/1912.03393v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visualizing Deep Neural Networks for Speech Recognition with Learned\n  Topographic Filter Maps", "abstract": "The uninformative ordering of artificial neurons in Deep Neural Networks\ncomplicates visualizing activations in deeper layers. This is one reason why\nthe internal structure of such models is very unintuitive. In neuroscience,\nactivity of real brains can be visualized by highlighting active regions.\nInspired by those techniques, we train a convolutional speech recognition\nmodel, where filters are arranged in a 2D grid and neighboring filters are\nsimilar to each other. We show, how those topographic filter maps visualize\nartificial neuron activations more intuitively. Moreover, we investigate,\nwhether this causes phoneme-responsive neurons to be grouped in certain regions\nof the topographic map.", "published": "2019-12-06 10:31:29", "link": "http://arxiv.org/abs/1912.04067v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
