{"title": "A Simple and Efficient Ensemble Classifier Combining Multiple Neural\n  Network Models on Social Media Datasets in Vietnamese", "abstract": "Text classification is a popular topic of natural language processing, which\nhas currently attracted numerous research efforts worldwide. The significant\nincrease of data in social media requires the vast attention of researchers to\nanalyze such data. There are various studies in this field in many languages\nbut limited to the Vietnamese language. Therefore, this study aims to classify\nVietnamese texts on social media from three different Vietnamese benchmark\ndatasets. Advanced deep learning models are used and optimized in this study,\nincluding CNN, LSTM, and their variants. We also implement the BERT, which has\nnever been applied to the datasets. Our experiments find a suitable model for\nclassification tasks on each specific dataset. To take advantage of single\nmodels, we propose an ensemble model, combining the highest-performance models.\nOur single models reach positive results on each dataset. Moreover, our\nensemble model achieves the best performance on all three datasets. We reach\n86.96% of F1- score for the HSD-VLSP dataset, 65.79% of F1-score for the\nUIT-VSMEC dataset, 92.79% and 89.70% for sentiments and topics on the UIT-VSFC\ndataset, respectively. Therefore, our models achieve better performances as\ncompared to previous studies on these datasets.", "published": "2020-09-28 04:28:48", "link": "http://arxiv.org/abs/2009.13060v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graph-based Multi-hop Reasoning for Long Text Generation", "abstract": "Long text generation is an important but challenging task.The main problem\nlies in learning sentence-level semantic dependencies which traditional\ngenerative models often suffer from. To address this problem, we propose a\nMulti-hop Reasoning Generation (MRG) approach that incorporates multi-hop\nreasoning over a knowledge graph to learn semantic dependencies among\nsentences. MRG consists of twoparts, a graph-based multi-hop reasoning module\nand a path-aware sentence realization module. The reasoning module is\nresponsible for searching skeleton paths from a knowledge graph to imitate the\nimagination process in the human writing for semantic transfer. Based on the\ninferred paths, the sentence realization module then generates a complete\nsentence. Unlike previous black-box models, MRG explicitly infers the skeleton\npath, which provides explanatory views tounderstand how the proposed model\nworks. We conduct experiments on three representative tasks, including story\ngeneration, review generation, and product description generation. Automatic\nand manual evaluation show that our proposed method can generate more\ninformative and coherentlong text than strong baselines, such as pre-trained\nmodels(e.g. GPT-2) and knowledge-enhanced models.", "published": "2020-09-28 12:47:59", "link": "http://arxiv.org/abs/2009.13282v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Quantity Hallucinations in Abstractive Summarization", "abstract": "It is well-known that abstractive summaries are subject to\nhallucination---including material that is not supported by the original text.\nWhile summaries can be made hallucination-free by limiting them to general\nphrases, such summaries would fail to be very informative. Alternatively, one\ncan try to avoid hallucinations by verifying that any specific entities in the\nsummary appear in the original text in a similar context. This is the approach\ntaken by our system, Herman. The system learns to recognize and verify quantity\nentities (dates, numbers, sums of money, etc.) in a beam-worth of abstractive\nsummaries produced by state-of-the-art models, in order to up-rank those\nsummaries whose quantity terms are supported by the original text. Experimental\nresults demonstrate that the ROUGE scores of such up-ranked summaries have a\nhigher Precision than summaries that have not been up-ranked, without a\ncomparable loss in Recall, resulting in higher F$_1$. Preliminary human\nevaluation of up-ranked vs. original summaries shows people's preference for\nthe former.", "published": "2020-09-28 13:32:59", "link": "http://arxiv.org/abs/2009.13312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aspects of Terminological and Named Entity Knowledge within Rule-Based\n  Machine Translation Models for Under-Resourced Neural Machine Translation\n  Scenarios", "abstract": "Rule-based machine translation is a machine translation paradigm where\nlinguistic knowledge is encoded by an expert in the form of rules that\ntranslate text from source to target language. While this approach grants\nextensive control over the output of the system, the cost of formalising the\nneeded linguistic knowledge is much higher than training a corpus-based system,\nwhere a machine learning approach is used to automatically learn to translate\nfrom examples. In this paper, we describe different approaches to leverage the\ninformation contained in rule-based machine translation systems to improve a\ncorpus-based one, namely, a neural machine translation model, with a focus on a\nlow-resource scenario. Three different kinds of information were used:\nmorphological information, named entities and terminology. In addition to\nevaluating the general performance of the system, we systematically analysed\nthe performance of the proposed approaches when dealing with the targeted\nphenomena. Our results suggest that the proposed models have limited ability to\nlearn from external information, and most approaches do not significantly alter\nthe results of the automatic evaluation, but our preliminary qualitative\nevaluation shows that in certain cases the hypothesis generated by our system\nexhibit favourable behaviour such as keeping the use of passive voice.", "published": "2020-09-28 15:19:23", "link": "http://arxiv.org/abs/2009.13398v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PIN: A Novel Parallel Interactive Network for Spoken Language\n  Understanding", "abstract": "Spoken Language Understanding (SLU) is an essential part of the spoken\ndialogue system, which typically consists of intent detection (ID) and slot\nfilling (SF) tasks. Recently, recurrent neural networks (RNNs) based methods\nachieved the state-of-the-art for SLU. It is noted that, in the existing\nRNN-based approaches, ID and SF tasks are often jointly modeled to utilize the\ncorrelation information between them. However, we noted that, so far, the\nefforts to obtain better performance by supporting bidirectional and explicit\ninformation exchange between ID and SF are not well studied.In addition, few\nstudies attempt to capture the local context information to enhance the\nperformance of SF. Motivated by these findings, in this paper, Parallel\nInteractive Network (PIN) is proposed to model the mutual guidance between ID\nand SF. Specifically, given an utterance, a Gaussian self-attentive encoder is\nintroduced to generate the context-aware feature embedding of the utterance\nwhich is able to capture local context information. Taking the feature\nembedding of the utterance, Slot2Intent module and Intent2Slot module are\ndeveloped to capture the bidirectional information flow for ID and SF tasks.\nFinally, a cooperation mechanism is constructed to fuse the information\nobtained from Slot2Intent and Intent2Slot modules to further reduce the\nprediction bias.The experiments on two benchmark datasets, i.e., SNIPS and\nATIS, demonstrate the effectiveness of our approach, which achieves a\ncompetitive result with state-of-the-art models. More encouragingly, by using\nthe feature embedding of the utterance generated by the pre-trained language\nmodel BERT, our method achieves the state-of-the-art among all comparison\napproaches.", "published": "2020-09-28 15:59:31", "link": "http://arxiv.org/abs/2009.13431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improve Transformer Models with Better Relative Position Embeddings", "abstract": "Transformer architectures rely on explicit position encodings in order to\npreserve a notion of word order. In this paper, we argue that existing work\ndoes not fully utilize position information. For example, the initial proposal\nof a sinusoid embedding is fixed and not learnable. In this paper, we first\nreview absolute position embeddings and existing methods for relative position\nembeddings. We then propose new techniques that encourage increased interaction\nbetween query, key and relative position embeddings in the self-attention\nmechanism. Our most promising approach is a generalization of the absolute\nposition embedding, improving results on SQuAD1.1 compared to previous position\nembeddings approaches. In addition, we address the inductive property of\nwhether a position embedding can be robust enough to handle long sequences. We\ndemonstrate empirically that our relative position embedding method is\nreasonably generalized and robust from the inductive perspective. Finally, we\nshow that our proposed method can be adopted as a near drop-in replacement for\nimproving the accuracy of large models with a small computational budget.", "published": "2020-09-28 22:18:58", "link": "http://arxiv.org/abs/2009.13658v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fancy Man Lauches Zippo at WNUT 2020 Shared Task-1: A Bert Case Model\n  for Wet Lab Entity Extraction", "abstract": "Automatic or semi-automatic conversion of protocols specifying steps in\nperforming a lab procedure into machine-readable format benefits biological\nresearch a lot. These noisy, dense, and domain-specific lab protocols\nprocessing draws more and more interests with the development of deep learning.\nThis paper presents our teamwork on WNUT 2020 shared task-1: wet lab entity\nextract, that we conducted studies in several models, including a BiLSTM CRF\nmodel and a Bert case model which can be used to complete wet lab entity\nextraction. And we mainly discussed the performance differences of \\textbf{Bert\ncase} under different situations such as \\emph{transformers} versions, case\nsensitivity that may don't get enough attention before.", "published": "2020-09-28 01:05:08", "link": "http://arxiv.org/abs/2009.12997v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer\n  Matching Retrieval", "abstract": "We introduce SPARTA, a novel neural retrieval method that shows great promise\nin performance, generalization, and interpretability for open-domain question\nanswering. Unlike many neural ranking methods that use dense vector nearest\nneighbor search, SPARTA learns a sparse representation that can be efficiently\nimplemented as an Inverted Index. The resulting representation enables scalable\nneural retrieval that does not require expensive approximate vector search and\nleads to better performance than its dense counterpart. We validated our\napproaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval\nquestion answering (ReQA) tasks. SPARTA achieves new state-of-the-art results\nacross a variety of open-domain question answering tasks in both English and\nChinese datasets, including open SQuAD, Natuarl Question, CMRC and etc.\nAnalysis also confirms that the proposed method creates human interpretable\nrepresentation and allows flexible control over the trade-off between\nperformance and efficiency.", "published": "2020-09-28 02:11:02", "link": "http://arxiv.org/abs/2009.13013v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reactive Supervision: A New Method for Collecting Sarcasm Data", "abstract": "Sarcasm detection is an important task in affective computing, requiring\nlarge amounts of labeled data. We introduce reactive supervision, a novel data\ncollection method that utilizes the dynamics of online conversations to\novercome the limitations of existing data collection techniques. We use the new\nmethod to create and release a first-of-its-kind large dataset of tweets with\nsarcasm perspective labels and new contextual features. The dataset is expected\nto advance sarcasm detection research. Our method can be adapted to other\naffective computing domains, thus opening up new research opportunities.", "published": "2020-09-28 05:04:22", "link": "http://arxiv.org/abs/2009.13080v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "What Disease does this Patient Have? A Large-scale Open Domain Question\n  Answering Dataset from Medical Exams", "abstract": "Open domain question answering (OpenQA) tasks have been recently attracting\nmore and more attention from the natural language processing (NLP) community.\nIn this work, we present the first free-form multiple-choice OpenQA dataset for\nsolving medical problems, MedQA, collected from the professional medical board\nexams. It covers three languages: English, simplified Chinese, and traditional\nChinese, and contains 12,723, 34,251, and 14,123 questions for the three\nlanguages, respectively. We implement both rule-based and popular neural\nmethods by sequentially combining a document retriever and a machine\ncomprehension model. Through experiments, we find that even the current best\nmethod can only achieve 36.7\\%, 42.0\\%, and 70.1\\% of test accuracy on the\nEnglish, traditional Chinese, and simplified Chinese questions, respectively.\nWe expect MedQA to present great challenges to existing OpenQA systems and hope\nthat it can serve as a platform to promote much stronger OpenQA models from the\nNLP community in the future.", "published": "2020-09-28 05:07:51", "link": "http://arxiv.org/abs/2009.13081v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep Transformers with Latent Depth", "abstract": "The Transformer model has achieved state-of-the-art performance in many\nsequence modeling tasks. However, how to leverage model capacity with large or\nvariable depths is still an open challenge. We present a probabilistic\nframework to automatically learn which layer(s) to use by learning the\nposterior distributions of layer selection. As an extension of this framework,\nwe propose a novel method to train one shared Transformer network for\nmultilingual machine translation with different layer selection posteriors for\neach language pair. The proposed method alleviates the vanishing gradient issue\nand enables stable training of deep Transformers (e.g. 100 layers). We evaluate\non WMT English-German machine translation and masked language modeling tasks,\nwhere our method outperforms existing approaches for training deeper\nTransformers. Experiments on multilingual machine translation demonstrate that\nthis approach can effectively leverage increased model capacity and bring\nuniversal improvement for both many-to-one and one-to-many translation with\ndiverse language pairs.", "published": "2020-09-28 07:13:23", "link": "http://arxiv.org/abs/2009.13102v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Baselines for Word Alignment", "abstract": "Word alignments identify translational correspondences between words in a\nparallel sentence pair and is used, for instance, to learn bilingual\ndictionaries, to train statistical machine translation systems , or to perform\nquality estimation. In most areas of natural language processing, neural\nnetwork models nowadays constitute the preferred approach, a situation that\nmight also apply to word alignment models. In this work, we study and\ncomprehensively evaluate neural models for unsupervised word alignment for four\nlanguage pairs, contrasting several variants of neural models. We show that in\nmost settings, neural versions of the IBM-1 and hidden Markov models vastly\noutperform their discrete counterparts. We also analyze typical alignment\nerrors of the baselines that our models overcome to illustrate the benefits-and\nthe limitations-of these new models for morphologically rich languages.", "published": "2020-09-28 07:51:03", "link": "http://arxiv.org/abs/2009.13116v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative latent neural models for automatic word alignment", "abstract": "Word alignments identify translational correspondences between words in a\nparallel sentence pair and are used, for instance, to learn bilingual\ndictionaries, to train statistical machine translation systems or to perform\nquality estimation. Variational autoencoders have been recently used in various\nof natural language processing to learn in an unsupervised way latent\nrepresentations that are useful for language generation tasks. In this paper,\nwe study these models for the task of word alignment and propose and assess\nseveral evolutions of a vanilla variational autoencoders. We demonstrate that\nthese techniques can yield competitive results as compared to Giza++ and to a\nstrong neural network alignment system for two language pairs.", "published": "2020-09-28 07:54:09", "link": "http://arxiv.org/abs/2009.13117v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Incomplete Utterance Rewriting as Semantic Segmentation", "abstract": "Recent years the task of incomplete utterance rewriting has raised a large\nattention. Previous works usually shape it as a machine translation task and\nemploy sequence to sequence based architecture with copy mechanism. In this\npaper, we present a novel and extensive approach, which formulates it as a\nsemantic segmentation task. Instead of generating from scratch, such a\nformulation introduces edit operations and shapes the problem as prediction of\na word-level edit matrix. Benefiting from being able to capture both local and\nglobal information, our approach achieves state-of-the-art performance on\nseveral public datasets. Furthermore, our approach is four times faster than\nthe standard approach in inference.", "published": "2020-09-28 09:29:49", "link": "http://arxiv.org/abs/2009.13166v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge-Aware Procedural Text Understanding with Multi-Stage Training", "abstract": "Procedural text describes dynamic state changes during a step-by-step natural\nprocess (e.g., photosynthesis). In this work, we focus on the task of\nprocedural text understanding, which aims to comprehend such documents and\ntrack entities' states and locations during a process. Although recent\napproaches have achieved substantial progress, their results are far behind\nhuman performance. Two challenges, the difficulty of commonsense reasoning and\ndata insufficiency, still remain unsolved, which require the incorporation of\nexternal knowledge bases. Previous works on external knowledge injection\nusually rely on noisy web mining tools and heuristic rules with limited\napplicable scenarios. In this paper, we propose a novel KnOwledge-Aware\nproceduraL text understAnding (KOALA) model, which effectively leverages\nmultiple forms of external knowledge in this task. Specifically, we retrieve\ninformative knowledge triples from ConceptNet and perform knowledge-aware\nreasoning while tracking the entities. Besides, we employ a multi-stage\ntraining schema which fine-tunes the BERT model over unlabeled data collected\nfrom Wikipedia before further fine-tuning it on the final model. Experimental\nresults on two procedural text datasets, ProPara and Recipes, verify the\neffectiveness of the proposed methods, in which our model achieves\nstate-of-the-art performance in comparison to various baselines.", "published": "2020-09-28 10:28:40", "link": "http://arxiv.org/abs/2009.13199v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pchatbot: A Large-Scale Dataset for Personalized Chatbot", "abstract": "Natural language dialogue systems raise great attention recently. As many\ndialogue models are data-driven, high-quality datasets are essential to these\nsystems. In this paper, we introduce Pchatbot, a large-scale dialogue dataset\nthat contains two subsets collected from Weibo and Judicial forums\nrespectively. To adapt the raw dataset to dialogue systems, we elaborately\nnormalize the raw dataset via processes such as anonymization, deduplication,\nsegmentation, and filtering. The scale of Pchatbot is significantly larger than\nexisting Chinese datasets, which might benefit the data-driven models. Besides,\ncurrent dialogue datasets for personalized chatbot usually contain several\npersona sentences or attributes. Different from existing datasets, Pchatbot\nprovides anonymized user IDs and timestamps for both posts and responses. This\nenables the development of personalized dialogue models that directly learn\nimplicit user personality from the user's dialogue history. Our preliminary\nexperimental study benchmarks several state-of-the-art dialogue models to\nprovide a comparison for future work. The dataset can be publicly accessed at\nGithub.", "published": "2020-09-28 12:49:07", "link": "http://arxiv.org/abs/2009.13284v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Similarity Detection Pipeline for Crawling a Topic Related Fake News\n  Corpus", "abstract": "Fake news detection is a challenging task aiming to reduce human time and\neffort to check the truthfulness of news. Automated approaches to combat fake\nnews, however, are limited by the lack of labeled benchmark datasets,\nespecially in languages other than English. Moreover, many publicly available\ncorpora have specific limitations that make them difficult to use. To address\nthis problem, our contribution is threefold. First, we propose a new, publicly\navailable German topic related corpus for fake news detection. To the best of\nour knowledge, this is the first corpus of its kind. In this regard, we\ndeveloped a pipeline for crawling similar news articles. As our third\ncontribution, we conduct different learning experiments to detect fake news.\nThe best performance was achieved using sentence level embeddings from SBERT in\ncombination with a Bi-LSTM (k=0.88).", "published": "2020-09-28 14:35:31", "link": "http://arxiv.org/abs/2009.13367v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Injecting Entity Types into Entity-Guided Text Generation", "abstract": "Recent successes in deep generative modeling have led to significant advances\nin natural language generation (NLG). Incorporating entities into neural\ngeneration models has demonstrated great improvements by assisting to infer the\nsummary topic and to generate coherent content. To enhance the role of entity\nin NLG, in this paper, we aim to model the entity type in the decoding phase to\ngenerate contextual words accurately. We develop a novel NLG model to produce a\ntarget sequence based on a given list of entities. Our model has a multi-step\ndecoder that injects the entity types into the process of entity mention\ngeneration. Experiments on two public news datasets demonstrate type injection\nperforms better than existing type embedding concatenation baselines.", "published": "2020-09-28 15:19:28", "link": "http://arxiv.org/abs/2009.13401v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented\n  Dialogue", "abstract": "A long-standing goal of task-oriented dialogue research is the ability to\nflexibly adapt dialogue models to new domains. To progress research in this\ndirection, we introduce DialoGLUE (Dialogue Language Understanding Evaluation),\na public benchmark consisting of 7 task-oriented dialogue datasets covering 4\ndistinct natural language understanding tasks, designed to encourage dialogue\nresearch in representation-based transfer, domain adaptation, and\nsample-efficient task learning. We release several strong baseline models,\ndemonstrating performance improvements over a vanilla BERT architecture and\nstate-of-the-art results on 5 out of 7 tasks, by pre-training on a large\nopen-domain dialogue corpus and task-adaptive self-supervised training. Through\nthe DialoGLUE benchmark, the baseline methods, and our evaluation scripts, we\nhope to facilitate progress towards the goal of developing more general\ntask-oriented dialogue models.", "published": "2020-09-28 18:36:23", "link": "http://arxiv.org/abs/2009.13570v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Visual Pivoting for (Unsupervised) Entity Alignment", "abstract": "This work studies the use of visual semantic representations to align\nentities in heterogeneous knowledge graphs (KGs). Images are natural components\nof many existing KGs. By combining visual knowledge with other auxiliary\ninformation, we show that the proposed new approach, EVA, creates a holistic\nentity representation that provides strong signals for cross-graph entity\nalignment. Besides, previous entity alignment methods require human labelled\nseed alignment, restricting availability. EVA provides a completely\nunsupervised solution by leveraging the visual similarity of entities to create\nan initial seed dictionary (visual pivots). Experiments on benchmark data sets\nDBP15k and DWY15k show that EVA offers state-of-the-art performance on both\nmonolingual and cross-lingual entity alignment tasks. Furthermore, we discover\nthat images are particularly useful to align long-tail KG entities, which\ninherently lack the structural contexts necessary for capturing the\ncorrespondences.", "published": "2020-09-28 20:09:40", "link": "http://arxiv.org/abs/2009.13603v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Spatio-Textual Reasoning for Answering Tourism Questions", "abstract": "Our goal is to answer real-world tourism questions that seek\nPoints-of-Interest (POI) recommendations. Such questions express various kinds\nof spatial and non-spatial constraints, necessitating a combination of textual\nand spatial reasoning. In response, we develop the first joint spatio-textual\nreasoning model, which combines geo-spatial knowledge with information in\ntextual corpora to answer questions. We first develop a modular\nspatial-reasoning network that uses geo-coordinates of location names mentioned\nin a question, and of candidate answer POIs, to reason over only spatial\nconstraints. We then combine our spatial-reasoner with a textual reasoner in a\njoint model and present experiments on a real world POI recommendation task. We\nreport substantial improvements over existing models with-out joint\nspatio-textual reasoning.", "published": "2020-09-28 20:35:00", "link": "http://arxiv.org/abs/2009.13613v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Conversational Semantic Parsing", "abstract": "The structured representation for semantic parsing in task-oriented assistant\nsystems is geared towards simple understanding of one-turn queries. Due to the\nlimitations of the representation, the session-based properties such as\nco-reference resolution and context carryover are processed downstream in a\npipelined system. In this paper, we propose a semantic representation for such\ntask-oriented conversational systems that can represent concepts such as\nco-reference and context carryover, enabling comprehensive understanding of\nqueries in a session. We release a new session-based, compositional\ntask-oriented parsing dataset of 20k sessions consisting of 60k utterances.\nUnlike Dialog State Tracking Challenges, the queries in the dataset have\ncompositional forms. We propose a new family of Seq2Seq models for the\nsession-based parsing above, which achieve better or comparable performance to\nthe current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve\nthe best known results on DSTC2 by up to 5 points for slot-carryover.", "published": "2020-09-28 22:08:00", "link": "http://arxiv.org/abs/2009.13655v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Knowledge Bases with Parameters for Task-Oriented Dialogue\n  Systems", "abstract": "Task-oriented dialogue systems are either modularized with separate dialogue\nstate tracking (DST) and management steps or end-to-end trainable. In either\ncase, the knowledge base (KB) plays an essential role in fulfilling user\nrequests. Modularized systems rely on DST to interact with the KB, which is\nexpensive in terms of annotation and inference time. End-to-end systems use the\nKB directly as input, but they cannot scale when the KB is larger than a few\nhundred entries. In this paper, we propose a method to embed the KB, of any\nsize, directly into the model parameters. The resulting model does not require\nany DST or template responses, nor the KB as input, and it can dynamically\nupdate its KB via fine-tuning. We evaluate our solution in five task-oriented\ndialogue datasets with small, medium, and large KB size. Our experiments show\nthat end-to-end models can effectively embed knowledge bases in their\nparameters and achieve competitive performance in all evaluated datasets.", "published": "2020-09-28 22:13:54", "link": "http://arxiv.org/abs/2009.13656v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Gender Bias for Neural Dialogue Generation with Adversarial\n  Learning", "abstract": "Dialogue systems play an increasingly important role in various aspects of\nour daily life. It is evident from recent research that dialogue systems\ntrained on human conversation data are biased. In particular, they can produce\nresponses that reflect people's gender prejudice. Many debiasing methods have\nbeen developed for various NLP tasks, such as word embedding. However, they are\nnot directly applicable to dialogue systems because they are likely to force\ndialogue models to generate similar responses for different genders. This\ngreatly degrades the diversity of the generated responses and immensely hurts\nthe performance of the dialogue models. In this paper, we propose a novel\nadversarial learning framework Debiased-Chat to train dialogue models free from\ngender bias while keeping their performance. Extensive experiments on two\nreal-world conversation datasets show that our framework significantly reduces\ngender bias in dialogue models while maintaining the response quality. The\nimplementation of the proposed framework is released.", "published": "2020-09-28 02:46:59", "link": "http://arxiv.org/abs/2009.13028v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visual Exploration and Knowledge Discovery from Biomedical Dark Data", "abstract": "Data visualization techniques proffer efficient means to organize and present\ndata in graphically appealing formats, which not only speeds up the process of\ndecision making and pattern recognition but also enables decision-makers to\nfully understand data insights and make informed decisions. Over time, with the\nrise in technological and computational resources, there has been an\nexponential increase in the world's scientific knowledge. However, most of it\nlacks structure and cannot be easily categorized and imported into regular\ndatabases. This type of data is often termed as Dark Data. Data visualization\ntechniques provide a promising solution to explore such data by allowing quick\ncomprehension of information, the discovery of emerging trends, identification\nof relationships and patterns, etc. In this empirical research study, we use\nthe rich corpus of PubMed comprising of more than 30 million citations from\nbiomedical literature to visually explore and understand the underlying\nkey-insights using various information visualization techniques. We employ a\nnatural language processing based pipeline to discover knowledge out of the\nbiomedical dark data. The pipeline comprises of different lexical analysis\ntechniques like Topic Modeling to extract inherent topics and major focus\nareas, Network Graphs to study the relationships between various entities like\nscientific documents and journals, researchers, and, keywords and terms, etc.\nWith this analytical research, we aim to proffer a potential solution to\novercome the problem of analyzing overwhelming amounts of information and\ndiminish the limitation of human cognition and perception in handling and\nexamining such large volumes of data.", "published": "2020-09-28 04:27:05", "link": "http://arxiv.org/abs/2009.13059v1", "categories": ["cs.DL", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.DL"}
{"title": "Identifying Automatically Generated Headlines using Transformers", "abstract": "False information spread via the internet and social media influences public\nopinion and user activity, while generative models enable fake content to be\ngenerated faster and more cheaply than had previously been possible. In the not\nso distant future, identifying fake content generated by deep learning models\nwill play a key role in protecting users from misinformation. To this end, a\ndataset containing human and computer-generated headlines was created and a\nuser study indicated that humans were only able to identify the fake headlines\nin 47.8% of the cases. However, the most accurate automatic approach,\ntransformers, achieved an overall accuracy of 85.7%, indicating that content\ngenerated from language models can be filtered out accurately.", "published": "2020-09-28 14:48:27", "link": "http://arxiv.org/abs/2009.13375v3", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VIVO: Visual Vocabulary Pre-Training for Novel Object Captioning", "abstract": "It is highly desirable yet challenging to generate image captions that can\ndescribe novel objects which are unseen in caption-labeled training data, a\ncapability that is evaluated in the novel object captioning challenge (nocaps).\nIn this challenge, no additional image-caption training data, other thanCOCO\nCaptions, is allowed for model training. Thus, conventional Vision-Language\nPre-training (VLP) methods cannot be applied. This paper presents VIsual\nVOcabulary pretraining (VIVO) that performs pre-training in the absence of\ncaption annotations. By breaking the dependency of paired image-caption\ntraining data in VLP, VIVO can leverage large amounts of paired image-tag data\nto learn a visual vocabulary. This is done by pre-training a multi-layer\nTransformer model that learns to align image-level tags with their\ncorresponding image region features. To address the unordered nature of image\ntags, VIVO uses a Hungarian matching loss with masked tag prediction to conduct\npre-training. We validate the effectiveness of VIVO by fine-tuning the\npre-trained model for image captioning. In addition, we perform an analysis of\nthe visual-text alignment inferred by our model. The results show that our\nmodel can not only generate fluent image captions that describe novel objects,\nbut also identify the locations of these objects. Our single model has achieved\nnew state-of-the-art results on nocaps and surpassed the human CIDEr score.", "published": "2020-09-28 23:20:02", "link": "http://arxiv.org/abs/2009.13682v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Siamese Capsule Network for End-to-End Speaker Recognition In The Wild", "abstract": "We propose an end-to-end deep model for speaker verification in the wild. Our\nmodel uses thin-ResNet for extracting speaker embeddings from utterances and a\nSiamese capsule network and dynamic routing as the Back-end to calculate a\nsimilarity score between the embeddings. We conduct a series of experiments and\ncomparisons on our model to state-of-the-art solutions, showing that our model\noutperforms all the other models using substantially less amount of training\ndata. We also perform additional experiments to study the impact of different\nspeaker embeddings on the Siamese capsule network. We show that the best\nperformance is achieved by using embeddings obtained directly from the feature\naggregation module of the Front-end and passing them to higher capsules using\ndynamic routing.", "published": "2020-09-28 17:11:11", "link": "http://arxiv.org/abs/2009.13480v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Static and Dynamic Measures of Active Music Listening as Indicators of\n  Depression Risk", "abstract": "Music, an integral part of our lives, which is not only a source of\nentertainment but plays an important role in mental well-being by impacting\nmoods, emotions and other affective states. Music preferences and listening\nstrategies have been shown to be associated with the psychological well-being\nof listeners including internalized symptomatology and depression. However,\ntill date no studies exist that examine time-varying music consumption, in\nterms of acoustic content, and its association with users' well-being. In the\ncurrent study, we aim at unearthing static and dynamic patterns prevalent in\nactive listening behavior of individuals which may be used as indicators of\nrisk for depression. Mental well-being scores and listening histories of 541\nLast.fm users were examined. Static and dynamic acoustic and emotion-related\nfeatures were extracted from each user's listening history and correlated with\ntheir mental well-being scores. Results revealed that individuals with greater\ndepression risk resort to higher dependency on music with greater\nrepetitiveness in their listening activity. Furthermore, the affinity of\ndepressed individuals towards music that can be perceived as sad was found to\nbe resistant to change over time. This study has large implications for future\nwork in the area of assessing mental illness risk by exploiting digital\nfootprints of users via online music streaming platforms.", "published": "2020-09-28 23:29:53", "link": "http://arxiv.org/abs/2009.13685v1", "categories": ["eess.AS", "cs.IR", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
