{"title": "A Hype-Adjusted Probability Measure for NLP Stock Return Forecasting", "abstract": "This article introduces a Hype-Adjusted Probability Measure in the context of\na new Natural Language Processing (NLP) approach for stock return and\nvolatility forecasting. A novel sentiment score equation is proposed to\nrepresent the impact of intraday news on forecasting next-period stock return\nand volatility for selected U.S. semiconductor tickers, a very vibrant industry\nsector. This work improves the forecast accuracy by addressing news bias,\nmemory, and weight, and incorporating shifts in sentiment direction. More\nimportantly, it extends the use of the remarkable tool of change of Probability\nMeasure developed in the finance of Asset Pricing to NLP forecasting by\nconstructing a Hype-Adjusted Probability Measure, obtained from a\nredistribution of the weights in the probability space, meant to correct for\nexcessive or insufficient news.", "published": "2024-12-10 15:23:31", "link": "http://arxiv.org/abs/2412.07587v6", "categories": ["q-fin.CP", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "A Consolidated Volatility Prediction with Back Propagation Neural Network and Genetic Algorithm", "abstract": "This paper provides a unique approach with AI algorithms to predict emerging\nstock markets volatility. Traditionally, stock volatility is derived from\nhistorical volatility,Monte Carlo simulation and implied volatility as well. In\nthis paper, the writer designs a consolidated model with back-propagation\nneural network and genetic algorithm to predict future volatility of emerging\nstock markets and found that the results are quite accurate with low errors.", "published": "2024-12-10 06:19:08", "link": "http://arxiv.org/abs/2412.07223v4", "categories": ["q-fin.CP", "cs.LG", "cs.NE"], "primary_category": "q-fin.CP"}
{"title": "A theory of passive market impact", "abstract": "While the market impact of aggressive orders has been extensively studied,\nthe impact of passive orders, those executed through limit orders, remains less\nunderstood. The goal of this paper is to investigate passive market impact by\ndeveloping a microstructure model connecting liquidity dynamics and price\nmoves. A key innovation of our approach is to replace the traditional\nassumption of constant information content for each trade by a function that\ndepends on the available volume in the limit order book. Within this framework,\nwe explore scaling limits and analyze the market impact of passive metaorders.\nAdditionally, we derive useful approximations for the shape of market impact\ncurves, leading to closed-form formulas that can be easily applied in practice.", "published": "2024-12-10 12:30:27", "link": "http://arxiv.org/abs/2412.07461v1", "categories": ["q-fin.MF", "math.PR", "q-fin.TR", "60G55, 62P05, 91G80"], "primary_category": "q-fin.MF"}
{"title": "How to Choose a Threshold for an Evaluation Metric for Large Language Models", "abstract": "To ensure and monitor large language models (LLMs) reliably, various\nevaluation metrics have been proposed in the literature. However, there is\nlittle research on prescribing a methodology to identify a robust threshold on\nthese metrics even though there are many serious implications of an incorrect\nchoice of the thresholds during deployment of the LLMs. Translating the\ntraditional model risk management (MRM) guidelines within regulated industries\nsuch as the financial industry, we propose a step-by-step recipe for picking a\nthreshold for a given LLM evaluation metric. We emphasize that such a\nmethodology should start with identifying the risks of the LLM application\nunder consideration and risk tolerance of the stakeholders. We then propose\nconcrete and statistically rigorous procedures to determine a threshold for the\ngiven LLM evaluation metric using available ground-truth data. As a concrete\nexample to demonstrate the proposed methodology at work, we employ it on the\nFaithfulness metric, as implemented in various publicly available libraries,\nusing the publicly available HaluBench dataset. We also lay a foundation for\ncreating systematic approaches to select thresholds, not only for LLMs but for\nany GenAI applications.", "published": "2024-12-10 21:57:25", "link": "http://arxiv.org/abs/2412.12148v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "q-fin.ST", "stat.AP"], "primary_category": "stat.ML"}
{"title": "A Joint Energy and Differentially-Private Smart Meter Data Market", "abstract": "Given the vital role that smart meter data could play in handling uncertainty\nin energy markets, data markets have been proposed as a means to enable\nincreased data access. However, most extant literature considers energy markets\nand data markets separately, which ignores the interdependence between them. In\naddition, existing data market frameworks rely on a trusted entity to clear the\nmarket. This paper proposes a joint energy and data market focusing on the\nday-ahead retailer energy procurement problem with uncertain demand. The\nretailer can purchase differentially-private smart meter data from consumers to\nreduce uncertainty. The problem is modelled as an integrated forecasting and\noptimisation problem providing a means of valuing data directly rather than\nvaluing forecasts or forecast accuracy. Value is determined by the Wasserstein\ndistance, enabling privacy to be preserved during the valuation and procurement\nprocess. The value of joint energy and data clearing is highlighted through\nnumerical case studies using both synthetic and real smart meter data.", "published": "2024-12-10 17:25:14", "link": "http://arxiv.org/abs/2412.07688v1", "categories": ["eess.SY", "cs.GT", "cs.SY", "econ.GN", "q-fin.EC", "q-fin.PM", "q-fin.TR"], "primary_category": "eess.SY"}
{"title": "Improving the Natural Language Inference robustness to hard dataset by\n  data augmentation and preprocessing", "abstract": "Natural Language Inference (NLI) is the task of inferring whether the\nhypothesis can be justified by the given premise. Basically, we classify the\nhypothesis into three labels(entailment, neutrality and contradiction) given\nthe premise. NLI was well studied by the previous researchers. A number of\nmodels, especially the transformer based ones, have achieved significant\nimprovement on these tasks. However, it is reported that these models are\nsuffering when they are dealing with hard datasets. Particularly, they perform\nmuch worse when dealing with unseen out-of-distribution premise and hypothesis.\nThey may not understand the semantic content but learn the spurious\ncorrelations. In this work, we propose the data augmentation and preprocessing\nmethods to solve the word overlap, numerical reasoning and length mismatch\nproblems. These methods are general methods that do not rely on the\ndistribution of the testing data and they help improve the robustness of the\nmodels.", "published": "2024-12-10 01:49:23", "link": "http://arxiv.org/abs/2412.07108v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predictable Emergent Abilities of LLMs: Proxy Tasks Are All You Need", "abstract": "While scaling laws optimize training configurations for large language models\n(LLMs) through experiments on smaller or early-stage models, they fail to\npredict emergent abilities due to the absence of such capabilities in these\nmodels. To address this, we propose a method that predicts emergent abilities\nby leveraging proxy tasks. We begin by establishing relevance metrics between\nthe target task and candidate tasks based on performance differences across\nmultiple models. These candidate tasks are then validated for robustness with\nsmall model ensembles, leading to the selection of the most appropriate proxy\ntasks. The predicted performance on the target task is then derived by\nintegrating the evaluation results of these proxies. In a case study on tool\nutilization capabilities, our method demonstrated a strong correlation between\npredicted and actual performance, confirming its effectiveness.", "published": "2024-12-10 01:56:30", "link": "http://arxiv.org/abs/2412.07111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Coding Spot: Understanding Parametric Contributions to LLM\n  Coding Performance", "abstract": "Large Language Models (LLMs) have demonstrated notable proficiency in both\ncode generation and comprehension across multiple programming languages.\nHowever, the mechanisms underlying this proficiency remain underexplored,\nparticularly with respect to whether distinct programming languages are\nprocessed independently or within a shared parametric region. Drawing an\nanalogy to the specialized regions of the brain responsible for distinct\ncognitive functions, we introduce the concept of Coding Spot, a specialized\nparametric region within LLMs that facilitates coding capabilities. Our\nfindings identify this Coding Spot and show that targeted modifications to this\nsubset significantly affect performance on coding tasks, while largely\npreserving non-coding functionalities. This compartmentalization mirrors the\nfunctional specialization observed in cognitive neuroscience, where specific\nbrain regions are dedicated to distinct tasks, suggesting that LLMs may\nsimilarly employ specialized parameter regions for different knowledge domains.", "published": "2024-12-10 02:03:24", "link": "http://arxiv.org/abs/2412.07113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Breaking the Stage Barrier: A Novel Single-Stage Approach to Long\n  Context Extension for Large Language Models", "abstract": "Recently, Large language models (LLMs) have revolutionized Natural Language\nProcessing (NLP). Pretrained LLMs, due to limited training context size,\nstruggle with handling long token sequences, limiting their performance on\nvarious downstream tasks. Current solutions toward long context modeling often\nemploy multi-stage continual pertaining, which progressively increases the\neffective context length through several continual pretraining stages. However,\nthose approaches require extensive manual tuning and human expertise. In this\npaper, we introduce a novel single-stage continual pretraining method,\nHead-Adaptive Rotary Position Encoding (HARPE), to equip LLMs with long context\nmodeling capabilities while simplifying the training process. Our HARPE\nleverages different Rotary Position Encoding (RoPE) base frequency values\nacross different attention heads and directly trains LLMs on the target context\nlength. Extensive experiments on 4 language modeling benchmarks, including the\nlatest RULER benchmark, demonstrate that HARPE excels in understanding and\nintegrating long-context tasks with single-stage training, matching and even\noutperforming existing multi-stage methods. Our results highlight that HARPE\nsuccessfully breaks the stage barrier for training LLMs with long context\nmodeling capabilities.", "published": "2024-12-10 04:09:29", "link": "http://arxiv.org/abs/2412.07171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Comateformer: Combined Attention Transformer for Semantic Sentence\n  Matching", "abstract": "The Transformer-based model have made significant strides in semantic\nmatching tasks by capturing connections between phrase pairs. However, to\nassess the relevance of sentence pairs, it is insufficient to just examine the\ngeneral similarity between the sentences. It is crucial to also consider the\ntiny subtleties that differentiate them from each other. Regrettably, attention\nsoftmax operations in transformers tend to miss these subtle differences. To\nthis end, in this work, we propose a novel semantic sentence matching model\nnamed Combined Attention Network based on Transformer model (Comateformer). In\nComateformer model, we design a novel transformer-based quasi-attention\nmechanism with compositional properties. Unlike traditional attention\nmechanisms that merely adjust the weights of input tokens, our proposed method\nlearns how to combine, subtract, or resize specific vectors when building a\nrepresentation. Moreover, our proposed approach builds on the intuition of\nsimilarity and dissimilarity (negative affinity) when calculating dual affinity\nscores. This allows for a more meaningful representation of relationships\nbetween sentences. To evaluate the performance of our proposed model, we\nconducted extensive experiments on ten public real-world datasets and\nrobustness testing. Experimental results show that our method achieves\nconsistent improvements.", "published": "2024-12-10 06:18:07", "link": "http://arxiv.org/abs/2412.07220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean\n  Cultural Context", "abstract": "Large language models have exhibited significant enhancements in performance\nacross various tasks. However, the complexity of their evaluation increases as\nthese models generate more fluent and coherent content. Current multilingual\nbenchmarks often use translated English versions, which may incorporate Western\ncultural biases that do not accurately assess other languages and cultures. To\naddress this research gap, we introduce KULTURE Bench, an evaluation framework\nspecifically designed for Korean culture that features datasets of cultural\nnews, idioms, and poetry. It is designed to assess language models' cultural\ncomprehension and reasoning capabilities at the word, sentence, and paragraph\nlevels. Using the KULTURE Bench, we assessed the capabilities of models trained\nwith different language corpora and analyzed the results comprehensively. The\nresults show that there is still significant room for improvement in the\nmodels' understanding of texts related to the deeper aspects of Korean culture.", "published": "2024-12-10 07:20:51", "link": "http://arxiv.org/abs/2412.07251v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Rise and Down of Babel Tower: Investigating the Evolution Process of\n  Multilingual Code Large Language Model", "abstract": "Large language models (LLMs) have shown significant multilingual\ncapabilities. However, the mechanisms underlying the development of these\ncapabilities during pre-training are not well understood. In this paper, we use\ncode LLMs as an experimental platform to explore the evolution of multilingual\ncapabilities in LLMs during the pre-training process. Based on our\nobservations, we propose the Babel Tower Hypothesis, which describes the entire\nprocess of LLMs acquiring new language capabilities. During the learning\nprocess, multiple languages initially share a single knowledge system dominated\nby the primary language and gradually develop language-specific knowledge\nsystems. We then validate the above hypothesis by tracking the internal states\nof the LLMs through identifying working languages and language transferring\nneurons. Experimental results show that the internal state changes of the LLM\nare consistent with our Babel Tower Hypothesis. Building on these insights, we\npropose a novel method to construct an optimized pre-training corpus for\nmultilingual code LLMs, which significantly outperforms LLMs trained on the\noriginal corpus. The proposed Babel Tower Hypothesis provides new insights into\ndesigning pre-training data distributions to achieve optimal multilingual\ncapabilities in LLMs.", "published": "2024-12-10 08:28:57", "link": "http://arxiv.org/abs/2412.07298v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Filipino Benchmarks for Measuring Sexist and Homophobic Bias in\n  Multilingual Language Models from Southeast Asia", "abstract": "Bias studies on multilingual models confirm the presence of gender-related\nstereotypes in masked models processing languages with high NLP resources. We\nexpand on this line of research by introducing Filipino CrowS-Pairs and\nFilipino WinoQueer: benchmarks that assess both sexist and anti-queer biases in\npretrained language models (PLMs) handling texts in Filipino, a low-resource\nlanguage from the Philippines. The benchmarks consist of 7,074 new challenge\npairs resulting from our cultural adaptation of English bias evaluation\ndatasets, a process that we document in detail to guide similar forthcoming\nefforts. We apply the Filipino benchmarks on masked and causal multilingual\nmodels, including those pretrained on Southeast Asian data, and find that they\ncontain considerable amounts of bias. We also find that for multilingual\nmodels, the extent of bias learned for a particular language is influenced by\nhow much pretraining data in that language a model was exposed to. Our\nbenchmarks and insights can serve as a foundation for future work analyzing and\nmitigating bias in multilingual models.", "published": "2024-12-10 08:31:52", "link": "http://arxiv.org/abs/2412.07303v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Frame Representation Hypothesis: Multi-Token LLM Interpretability and\n  Concept-Guided Text Generation", "abstract": "Interpretability is a key challenge in fostering trust for Large Language\nModels (LLMs), which stems from the complexity of extracting reasoning from\nmodel's parameters. We present the Frame Representation Hypothesis, a\ntheoretically robust framework grounded in the Linear Representation Hypothesis\n(LRH) to interpret and control LLMs by modeling multi-token words. Prior\nresearch explored LRH to connect LLM representations with linguistic concepts,\nbut was limited to single token analysis. As most words are composed of several\ntokens, we extend LRH to multi-token words, thereby enabling usage on any\ntextual data with thousands of concepts. To this end, we propose words can be\ninterpreted as frames, ordered sequences of vectors that better capture\ntoken-word relationships. Then, concepts can be represented as the average of\nword frames sharing a common concept. We showcase these tools through Top-k\nConcept-Guided Decoding, which can intuitively steer text generation using\nconcepts of choice. We verify said ideas on Llama 3.1, Gemma 2, and Phi 3\nfamilies, demonstrating gender and language biases, exposing harmful content,\nbut also potential to remediate them, leading to safer and more transparent\nLLMs. Code is available at\nhttps://github.com/phvv-me/frame-representation-hypothesis.git", "published": "2024-12-10 09:25:39", "link": "http://arxiv.org/abs/2412.07334v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "My Words Imply Your Opinion: Reader Agent-Based Propagation Enhancement\n  for Personalized Implicit Emotion Analysis", "abstract": "The subtlety of emotional expressions makes implicit emotion analysis (IEA)\nparticularly sensitive to user-specific characteristics. Current studies\npersonalize emotion analysis by focusing on the author but neglect the impact\nof the intended reader on implicit emotional feedback. In this paper, we\nintroduce Personalized IEA (PIEA) and present the RAPPIE model, which addresses\nsubjective variability by incorporating reader feedback. In particular, (1) we\ncreate reader agents based on large language models to simulate reader\nfeedback, overcoming the issue of ``spiral of silence effect'' and data\nincompleteness of real reader reaction. (2) We develop a role-aware multi-view\ngraph learning to model the emotion interactive propagation process in\nscenarios with sparse reader information. (3) We construct two new PIEA\ndatasets covering English and Chinese social media with detailed user metadata,\naddressing the text-centric limitation of existing datasets. Extensive\nexperiments show that RAPPIE significantly outperforms state-of-the-art\nbaselines, demonstrating the value of incorporating reader feedback in PIEA.", "published": "2024-12-10 10:06:46", "link": "http://arxiv.org/abs/2412.07367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Algorithmic Phase Transitions in Language Models: A Mechanistic Case\n  Study of Arithmetic", "abstract": "Zero-shot capabilities of large language models make them powerful tools for\nsolving a range of tasks without explicit training. It remains unclear,\nhowever, how these models achieve such performance, or why they can zero-shot\nsome tasks but not others. In this paper, we shed some light on this phenomenon\nby defining and investigating algorithmic stability in language models --\nchanges in problem-solving strategy employed by the model as a result of\nchanges in task specification. We focus on a task where algorithmic stability\nis needed for generalization: two-operand arithmetic. Surprisingly, we find\nthat Gemma-2-2b employs substantially different computational models on closely\nrelated subtasks, i.e. four-digit versus eight-digit addition. Our findings\nsuggest that algorithmic instability may be a contributing factor to language\nmodels' poor zero-shot performance across certain logical reasoning tasks, as\nthey struggle to abstract different problem-solving strategies and smoothly\ntransition between them.", "published": "2024-12-10 10:32:01", "link": "http://arxiv.org/abs/2412.07386v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Composing or Not Composing? Towards Distributional Construction Grammars", "abstract": "The mechanisms of comprehension during language processing remains an open\nquestion. Classically, building the meaning of a linguistic utterance is said\nto be incremental, step-by-step, based on a compositional process. However,\nmany different works have shown for a long time that non-compositional\nphenomena are also at work. It is therefore necessary to propose a framework\nbringing together both approaches. We present in this paper an approach based\non Construction Grammars and completing this framework in order to account for\nthese different mechanisms. We propose first a formal definition of this\nframework by completing the feature structure representation proposed in\nSign-Based Construction Grammars. In a second step, we present a general\nrepresentation of the meaning based on the interaction of constructions, frames\nand events. This framework opens the door to a processing mechanism for\nbuilding the meaning based on the notion of activation evaluated in terms of\nsimilarity and unification. This new approach integrates features from\ndistributional semantics into the constructionist framework, leading to what we\ncall Distributional Construction Grammars.", "published": "2024-12-10 11:17:02", "link": "http://arxiv.org/abs/2412.07419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoPrUS: Consistency Preserving Utterance Synthesis towards more\n  realistic benchmark dialogues", "abstract": "Large-scale Wizard-Of-Oz dialogue datasets have enabled the training of deep\nlearning-based dialogue systems. While they are successful as benchmark\ndatasets, they lack certain types of utterances, which would make them more\nrealistic. In this work, we investigate the creation of synthetic communication\nerrors in an automatic pipeline. Based on linguistic theory, we propose and\nfollow a simple error taxonomy. We focus on three types of miscommunications\nthat could happen in real-world dialogues but are underrepresented in the\nbenchmark dataset: misunderstandings, non-understandings and vaguely related\nquestions. Our two-step approach uses a state-of-the-art Large Language Model\n(LLM) to first create the error and secondly the repairing utterance. We\nperform Language Model-based evaluation to ensure the quality of the generated\nutterances. We apply the method to the MultiWOZ dataset and evaluate it both\nqualitatively and empirically as well as with human judges. Our results\nindicate that current LLMs can aid in adding post-hoc miscommunications to\nbenchmark datasets as a form of data augmentation. We publish the resulting\ndataset, in which nearly 1900 dialogues have been modified, as CoPrUS-MultiWOZ\nto facilitate future work on dialogue systems.", "published": "2024-12-10 13:51:55", "link": "http://arxiv.org/abs/2412.07515v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DRUM: Learning Demonstration Retriever for Large MUlti-modal Models", "abstract": "Recently, large language models (LLMs) have demonstrated impressive\ncapabilities in dealing with new tasks with the help of in-context learning\n(ICL). In the study of Large Vision-Language Models (LVLMs), when implementing\nICL, researchers usually adopts the naive strategies like fixed demonstrations\nacross different samples, or selecting demonstrations directly via a\nvisual-language embedding model. These methods does not guarantee the\nconfigured demonstrations fit the need of the LVLMs. To address this issue, we\nnow propose a novel framework, \\underline{d}emonstration \\underline{r}etriever\nfor large m\\underline{u}lti-modal \\underline{m}odel (DRUM), which fine-tunes\nthe visual-language embedding model to better meet the LVLM's needs. First, we\ndiscuss the retrieval strategies for a visual-language task, assuming an\nembedding model is given. And we propose to concate the image and text\nembeddings to enhance the retrieval performance. Second, we propose to re-rank\nthe demonstrations retrieved by the embedding model via the LVLM's feedbacks,\nand calculate a list-wise ranking loss for training the embedding model. Third,\nwe propose an iterative demonstration mining strategy to improve the training\nof the embedding model. Through extensive experiments on 3 types of\nvisual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be\neffective in boosting the LVLM's in-context learning performance via retrieving\nmore proper demonstrations.", "published": "2024-12-10 15:56:12", "link": "http://arxiv.org/abs/2412.07619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChocoLlama: Lessons Learned From Teaching Llamas Dutch", "abstract": "While Large Language Models (LLMs) have shown remarkable capabilities in\nnatural language understanding and generation, their performance often lags in\nlower-resource, non-English languages due to biases in the training data. In\nthis work, we explore strategies for adapting the primarily English LLMs\n(Llama-2 and Llama-3) to Dutch, a language spoken by 30 million people\nworldwide yet often underrepresented in LLM development. We collect 104GB of\nDutch text ($32$B tokens) from various sources to first apply continued\npretraining using low-rank adaptation (LoRA), complemented with Dutch\nposttraining strategies provided by prior work. For Llama-2, we consider using\n(i) the tokenizer of the original model, and (ii) training a new,\nDutch-specific tokenizer combined with embedding reinitialization. We evaluate\nour adapted models, ChocoLlama-2, both on standard benchmarks and a novel Dutch\nbenchmark, ChocoLlama-Bench. Our results demonstrate that LoRA can effectively\nscale for language adaptation, and that tokenizer modification with careful\nweight reinitialization can improve performance. Notably, Llama-3 was released\nduring the course of this project and, upon evaluation, demonstrated superior\nDutch capabilities compared to our Dutch-adapted versions of Llama-2. We hence\napply the same adaptation technique to Llama-3, using its original tokenizer.\nWhile our adaptation methods enhanced Llama-2's Dutch capabilities, we found\nlimited gains when applying the same techniques to Llama-3. This suggests that\nfor ever improving, multilingual foundation models, language adaptation\ntechniques may benefit more from focusing on language-specific posttraining\nrather than on continued pretraining. We hope this work contributes to the\nbroader understanding of adapting LLMs to lower-resource languages, and to the\ndevelopment of Dutch LLMs in particular.", "published": "2024-12-10 16:13:58", "link": "http://arxiv.org/abs/2412.07633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models", "abstract": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.", "published": "2024-12-10 16:32:19", "link": "http://arxiv.org/abs/2412.07646v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRIM: Token Reduction and Inference Modeling for Cost-Effective Language\n  Generation", "abstract": "The inference cost of Large Language Models (LLMs) is a significant challenge\ndue to their computational demands, specially on tasks requiring long outputs.\nHowever, natural language often contains redundancy, which presents an\nopportunity for optimization. We have observed that LLMs can generate distilled\nlanguage-concise outputs that retain essential meaning, when prompted\nappropriately. We propose TRIM, a pipeline for saving computational cost in\nwhich a shorter distilled output from the LLM is reconstructed into a full\nnarrative by a smaller model with lower inference costs. Our experiments show\npromising results, particularly in general knowledge domains with 20.58% saved\ntokens on average with tiny decrease in evaluation metrics, hinting that this\napproach can effectively balance efficiency and accuracy in language processing\ntasks.", "published": "2024-12-10 17:13:35", "link": "http://arxiv.org/abs/2412.07682v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Granite Guardian", "abstract": "We introduce the Granite Guardian models, a suite of safeguards designed to\nprovide risk detection for prompts and responses, enabling safe and responsible\nuse in combination with any large language model (LLM). These models offer\ncomprehensive coverage across multiple risk dimensions, including social bias,\nprofanity, violence, sexual content, unethical behavior, jailbreaking, and\nhallucination-related risks such as context relevance, groundedness, and answer\nrelevance for retrieval-augmented generation (RAG). Trained on a unique dataset\ncombining human annotations from diverse sources and synthetic data, Granite\nGuardian models address risks typically overlooked by traditional risk\ndetection models, such as jailbreaks and RAG-specific issues. With AUC scores\nof 0.871 and 0.854 on harmful content and RAG-hallucination-related benchmarks\nrespectively, Granite Guardian is the most generalizable and competitive model\navailable in the space. Released as open-source, Granite Guardian aims to\npromote responsible AI development across the community.\n  https://github.com/ibm-granite/granite-guardian", "published": "2024-12-10 18:17:02", "link": "http://arxiv.org/abs/2412.07724v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot ATC Coding with Large Language Models for Clinical Assessments", "abstract": "Manual assignment of Anatomical Therapeutic Chemical (ATC) codes to\nprescription records is a significant bottleneck in healthcare research and\noperations at Ontario Health and InterRAI Canada, requiring extensive expert\ntime and effort. To automate this process while maintaining data privacy, we\ndevelop a practical approach using locally deployable large language models\n(LLMs). Inspired by recent advances in automatic International Classification\nof Diseases (ICD) coding, our method frames ATC coding as a hierarchical\ninformation extraction task, guiding LLMs through the ATC ontology level by\nlevel. We evaluate our approach using GPT-4o as an accuracy ceiling and focus\ndevelopment on open-source Llama models suitable for privacy-sensitive\ndeployment. Testing across Health Canada drug product data, the RABBITS\nbenchmark, and real clinical notes from Ontario Health, our method achieves 78%\nexact match accuracy with GPT-4o and 60% with Llama 3.1 70B. We investigate\nknowledge grounding through drug definitions, finding modest improvements in\naccuracy. Further, we show that fine-tuned Llama 3.1 8B matches zero-shot Llama\n3.1 70B accuracy, suggesting that effective ATC coding is feasible with smaller\nmodels. Our results demonstrate the feasibility of automatic ATC coding in\nprivacy-sensitive healthcare environments, providing a foundation for future\ndeployments.", "published": "2024-12-10 18:43:02", "link": "http://arxiv.org/abs/2412.07743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Emotion Annotations in the Era of Large Language Models", "abstract": "Modern affective computing systems rely heavily on datasets with\nhuman-annotated emotion labels, for training and evaluation. However, human\nannotations are expensive to obtain, sensitive to study design, and difficult\nto quality control, because of the subjective nature of emotions. Meanwhile,\nLarge Language Models (LLMs) have shown remarkable performance on many Natural\nLanguage Understanding tasks, emerging as a promising tool for text annotation.\nIn this work, we analyze the complexities of emotion annotation in the context\nof LLMs, focusing on GPT-4 as a leading model. In our experiments, GPT-4\nachieves high ratings in a human evaluation study, painting a more positive\npicture than previous work, in which human labels served as the only ground\ntruth. On the other hand, we observe differences between human and GPT-4\nemotion perception, underscoring the importance of human input in annotation\nstudies. To harness GPT-4's strength while preserving human perspective, we\nexplore two ways of integrating GPT-4 into emotion annotation pipelines,\nshowing its potential to flag low-quality labels, reduce the workload of human\nannotators, and improve downstream model learning performance and efficiency.\nTogether, our findings highlight opportunities for new emotion labeling\npractices and suggest the use of LLMs as a promising tool to aid human\nannotation.", "published": "2024-12-10 20:30:51", "link": "http://arxiv.org/abs/2412.07906v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Asking Again and Again: Exploring LLM Robustness to Repeated Questions", "abstract": "This study investigates whether repeating questions within prompts influences\nthe performance of large language models (LLMs). We hypothesize that\nreiterating a question within a single prompt might enhance the model's focus\non key elements of the query. We evaluate five recent LLMs -- including\nGPT-4o-mini, DeepSeek-V3, and smaller open-source models -- on three reading\ncomprehension datasets under different prompt settings, varying question\nrepetition levels (1, 3, or 5 times per prompt). Our results demonstrate that\nquestion repetition can increase models' accuracy by up to $6\\%$. However,\nacross all models, settings, and datasets, we do not find the result\nstatistically significant. These findings provide insights into prompt design\nand LLM behavior, suggesting that repetition alone does not significantly\nimpact output quality.", "published": "2024-12-10 21:09:12", "link": "http://arxiv.org/abs/2412.07923v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Style-agnostic evaluation of ASR using multiple reference transcripts", "abstract": "Word error rate (WER) as a metric has a variety of limitations that have\nplagued the field of speech recognition. Evaluation datasets suffer from\nvarying style, formality, and inherent ambiguity of the transcription task. In\nthis work, we attempt to mitigate some of these differences by performing\nstyle-agnostic evaluation of ASR systems using multiple references transcribed\nunder opposing style parameters. As a result, we find that existing WER reports\nare likely significantly over-estimating the number of contentful errors made\nby state-of-the-art ASR systems. In addition, we have found our multireference\nmethod to be a useful mechanism for comparing the quality of ASR models that\ndiffer in the stylistic makeup of their training data and target task.", "published": "2024-12-10 21:47:15", "link": "http://arxiv.org/abs/2412.07937v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HalluCana: Fixing LLM Hallucination with A Canary Lookahead", "abstract": "In this paper, we present HalluCana, a canary lookahead to detect and correct\nfactuality hallucinations of Large Language Models (LLMs) in long-form\ngeneration. HalluCana detects and intervenes as soon as traces of hallucination\nemerge, during and even before generation. To support timely detection, we\nexploit the internal factuality representation in the LLM hidden space, where\nwe investigate various proxies to the LLMs' factuality self-assessment, and\ndiscuss its relation to the models' context familiarity from their\npre-training. On biography generation, our method improves generation quality\nby up to 2.5x, while consuming over 6 times less compute.", "published": "2024-12-10 23:09:02", "link": "http://arxiv.org/abs/2412.07965v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Defensive Dual Masking for Robust Adversarial Defense", "abstract": "The field of textual adversarial defenses has gained considerable attention\nin recent years due to the increasing vulnerability of natural language\nprocessing (NLP) models to adversarial attacks, which exploit subtle\nperturbations in input text to deceive models. This paper introduces the\nDefensive Dual Masking (DDM) algorithm, a novel approach designed to enhance\nmodel robustness against such attacks. DDM utilizes a unique adversarial\ntraining strategy where [MASK] tokens are strategically inserted into training\nsamples to prepare the model to handle adversarial perturbations more\neffectively. During inference, potentially adversarial tokens are dynamically\nreplaced with [MASK] tokens to neutralize potential threats while preserving\nthe core semantics of the input. The theoretical foundation of our approach is\nexplored, demonstrating how the selective masking mechanism strengthens the\nmodel's ability to identify and mitigate adversarial manipulations. Our\nempirical evaluation across a diverse set of benchmark datasets and attack\nmechanisms consistently shows that DDM outperforms state-of-the-art defense\ntechniques, improving model accuracy and robustness. Moreover, when applied to\nLarge Language Models (LLMs), DDM also enhances their resilience to adversarial\nattacks, providing a scalable defense mechanism for large-scale NLP\napplications.", "published": "2024-12-10 00:41:25", "link": "http://arxiv.org/abs/2412.07078v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QAPyramid: Fine-grained Evaluation of Content Selection for Text\n  Summarization", "abstract": "How to properly conduct human evaluations for text summarization is a\nlongstanding challenge. The Pyramid human evaluation protocol, which assesses\ncontent selection by breaking the reference summary into sub-units and\nverifying their presence in the system summary, has been widely adopted.\nHowever, it suffers from a lack of systematicity in the definition and\ngranularity of the sub-units. We address these problems by proposing QAPyramid,\nwhich decomposes each reference summary into finer-grained question-answer (QA)\npairs according to the QA-SRL framework. We collect QA-SRL annotations for\nreference summaries from CNN/DM and evaluate 10 summarization systems,\nresulting in 8.9K QA-level annotations. We show that, compared to Pyramid,\nQAPyramid provides more systematic and fine-grained content selection\nevaluation while maintaining high inter-annotator agreement without needing\nexpert annotations. Furthermore, we propose metrics that automate the\nevaluation pipeline and achieve higher correlations with QAPyramid than other\nwidely adopted metrics, allowing future work to accurately and efficiently\nbenchmark summarization systems.", "published": "2024-12-10 01:29:51", "link": "http://arxiv.org/abs/2412.07096v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Maya: An Instruction Finetuned Multilingual Multimodal Model", "abstract": "The rapid development of large Vision-Language Models (VLMs) has led to\nimpressive results on academic benchmarks, primarily in widely spoken\nlanguages. However, significant gaps remain in the ability of current VLMs to\nhandle low-resource languages and varied cultural contexts, largely due to a\nlack of high-quality, diverse, and safety-vetted data. Consequently, these\nmodels often struggle to understand low-resource languages and cultural nuances\nin a manner free from toxicity. To address these limitations, we introduce\nMaya, an open-source Multimodal Multilingual model. Our contributions are\nthreefold: 1) a multilingual image-text pretraining dataset in eight languages,\nbased on the LLaVA pretraining dataset; 2) a thorough analysis of toxicity\nwithin the LLaVA dataset, followed by the creation of a novel toxicity-free\nversion across eight languages; and 3) a multilingual image-text model\nsupporting these languages, enhancing cultural and linguistic comprehension in\nvision-language tasks. Code available at https://github.com/nahidalam/maya.", "published": "2024-12-10 01:57:17", "link": "http://arxiv.org/abs/2412.07112v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Bridging the Gap for Test-Time Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis (MSA) is an emerging research topic that aims\nto understand and recognize human sentiment or emotions through multiple\nmodalities. However, in real-world dynamic scenarios, the distribution of\ntarget data is always changing and different from the source data used to train\nthe model, which leads to performance degradation. Common adaptation methods\nusually need source data, which could pose privacy issues or storage overheads.\nTherefore, test-time adaptation (TTA) methods are introduced to improve the\nperformance of the model at inference time. Existing TTA methods are always\nbased on probabilistic models and unimodal learning, and thus can not be\napplied to MSA which is often considered as a multimodal regression task. In\nthis paper, we propose two strategies: Contrastive Adaptation and Stable\nPseudo-label generation (CASP) for test-time adaptation for multimodal\nsentiment analysis. The two strategies deal with the distribution shifts for\nMSA by enforcing consistency and minimizing empirical risk, respectively.\nExtensive experiments show that CASP brings significant and consistent\nimprovements to the performance of the model across various distribution shift\nsettings and with different backbones, demonstrating its effectiveness and\nversatility. Our codes are available at https://github.com/zrguo/CASP.", "published": "2024-12-10 02:26:33", "link": "http://arxiv.org/abs/2412.07121v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Political Actor Agent: Simulating Legislative System for Roll Call Votes\n  Prediction with Large Language Models", "abstract": "Predicting roll call votes through modeling political actors has emerged as a\nfocus in quantitative political science and computer science. Widely used\nembedding-based methods generate vectors for legislators from diverse data sets\nto predict legislative behaviors. However, these methods often contend with\nchallenges such as the need for manually predefined features, reliance on\nextensive training data, and a lack of interpretability. Achieving more\ninterpretable predictions under flexible conditions remains an unresolved\nissue. This paper introduces the Political Actor Agent (PAA), a novel\nagent-based framework that utilizes Large Language Models to overcome these\nlimitations. By employing role-playing architectures and simulating legislative\nsystem, PAA provides a scalable and interpretable paradigm for predicting\nroll-call votes. Our approach not only enhances the accuracy of predictions but\nalso offers multi-view, human-understandable decision reasoning, providing new\ninsights into political actor behaviors. We conducted comprehensive experiments\nusing voting records from the 117-118th U.S. House of Representatives,\nvalidating the superior performance and interpretability of PAA. This study not\nonly demonstrates PAA's effectiveness but also its potential in political\nscience research.", "published": "2024-12-10 03:06:28", "link": "http://arxiv.org/abs/2412.07144v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Review on the Applications of Transformer-based language models for\n  Nucleotide Sequence Analysis", "abstract": "In recent times, Transformer-based language models are making quite an impact\nin the field of natural language processing. As relevant parallels can be drawn\nbetween biological sequences and natural languages, the models used in NLP can\nbe easily extended and adapted for various applications in bioinformatics. In\nthis regard, this paper introduces the major developments of Transformer-based\nmodels in the recent past in the context of nucleotide sequences. We have\nreviewed and analysed a large number of application-based papers on this\nsubject, giving evidence of the main characterizing features and to different\napproaches that may be adopted to customize such powerful computational\nmachines. We have also provided a structured description of the functioning of\nTransformers, that may enable even first time users to grab the essence of such\ncomplex architectures. We believe this review will help the scientific\ncommunity in understanding the various applications of Transformer-based\nlanguage models to nucleotide sequences. This work will motivate the readers to\nbuild on these methodologies to tackle also various other problems in the field\nof bioinformatics.", "published": "2024-12-10 05:33:09", "link": "http://arxiv.org/abs/2412.07201v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speaker effects in spoken language comprehension", "abstract": "The identity of a speaker significantly influences spoken language\ncomprehension by affecting both perception and expectation. This review\nexplores speaker effects, focusing on how speaker information impacts language\nprocessing. We propose an integrative model featuring the interplay between\nbottom-up perception-based processes driven by acoustic details and top-down\nexpectation-based processes driven by a speaker model. The acoustic details\ninfluence lower-level perception, while the speaker model modulates both\nlower-level and higher-level processes such as meaning interpretation and\npragmatic inferences. We define speaker-idiosyncrasy and speaker-demographics\neffects and demonstrate how bottom-up and top-down processes interact at\nvarious levels in different scenarios. This framework contributes to\npsycholinguistic theory by offering a comprehensive account of how speaker\ninformation interacts with linguistic content to shape message construction. We\nsuggest that speaker effects can serve as indices of a language learner's\nproficiency and an individual's characteristics of social cognition. We\nencourage future research to extend these findings to AI speakers, probing the\nuniversality of speaker effects across humans and artificial agents.", "published": "2024-12-10 07:03:06", "link": "http://arxiv.org/abs/2412.07238v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Filling Memory Gaps: Enhancing Continual Semantic Parsing via SQL Syntax\n  Variance-Guided LLMs without Real Data Replay", "abstract": "Continual Semantic Parsing (CSP) aims to train parsers to convert natural\nlanguage questions into SQL across tasks with limited annotated examples,\nadapting to the real-world scenario of dynamically updated databases. Previous\nstudies mitigate this challenge by replaying historical data or employing\nparameter-efficient tuning (PET), but they often violate data privacy or rely\non ideal continual learning settings. To address these problems, we propose a\nnew Large Language Model (LLM)-Enhanced Continuous Semantic Parsing method,\nnamed LECSP, which alleviates forgetting while encouraging generalization,\nwithout requiring real data replay or ideal settings. Specifically, it first\nanalyzes the commonalities and differences between tasks from the SQL syntax\nperspective to guide LLMs in reconstructing key memories and improving memory\naccuracy through a calibration strategy. Then, it uses a task-aware\ndual-teacher distillation framework to promote the accumulation and transfer of\nknowledge during sequential training. Experimental results on two CSP\nbenchmarks show that our method significantly outperforms existing methods,\neven those utilizing data replay or ideal settings. Additionally, we achieve\ngeneralization performance beyond the upper limits, better adapting to unseen\ntasks.", "published": "2024-12-10 07:11:49", "link": "http://arxiv.org/abs/2412.07246v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Label-Confidence-Aware Uncertainty Estimation in Natural Language\n  Generation", "abstract": "Large Language Models (LLMs) display formidable capabilities in generative\ntasks but also pose potential risks due to their tendency to generate\nhallucinatory responses. Uncertainty Quantification (UQ), the evaluation of\nmodel output reliability, is crucial for ensuring the safety and robustness of\nAI systems. Recent studies have concentrated on model uncertainty by analyzing\nthe relationship between output entropy under various sampling conditions and\nthe corresponding labels. However, these methods primarily focus on measuring\nmodel entropy with precision to capture response characteristics, often\nneglecting the uncertainties associated with greedy decoding results-the\nsources of model labels, which can lead to biased classification outcomes. In\nthis paper, we explore the biases introduced by greedy decoding and propose a\nlabel-confidence-aware (LCA) uncertainty estimation based on Kullback-Leibler\n(KL) divergence bridging between samples and label source, thus enhancing the\nreliability and stability of uncertainty assessments. Our empirical evaluations\nacross a range of popular LLMs and NLP datasets reveal that different label\nsources can indeed affect classification, and that our approach can effectively\ncapture differences in sampling results and label sources, demonstrating more\neffective uncertainty estimation.", "published": "2024-12-10 07:35:23", "link": "http://arxiv.org/abs/2412.07255v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Relation Extraction via Supervised Rationale Verification and\n  Feedback", "abstract": "Despite the rapid progress that existing automated feedback methods have made\nin correcting the output of large language models (LLMs), these methods cannot\nbe well applied to the relation extraction (RE) task due to their designated\nfeedback objectives and correction manner. To address this problem, we propose\na novel automated feedback framework for RE, which presents a rationale\nsupervisor to verify the rationale and provides re-selected demonstrations as\nfeedback to correct the initial prediction. Specifically, we first design a\ncausal intervention and observation method to collect biased/unbiased\nrationales for contrastive training the rationale supervisor. Then, we present\na verification-feedback-correction procedure to iteratively enhance LLMs'\ncapability of handling the RE task. Extensive experiments prove that our\nproposed framework significantly outperforms existing methods.", "published": "2024-12-10 08:18:29", "link": "http://arxiv.org/abs/2412.07289v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multimodal Sentiment Analysis Based on Causal Reasoning", "abstract": "With the rapid development of multimedia, the shift from unimodal textual\nsentiment analysis to multimodal image-text sentiment analysis has obtained\nacademic and industrial attention in recent years. However, multimodal\nsentiment analysis is affected by unimodal data bias, e.g., text sentiment is\nmisleading due to explicit sentiment semantic, leading to low accuracy in the\nfinal sentiment classification. In this paper, we propose a novel\nCounterFactual Multimodal Sentiment Analysis framework (CF-MSA) using causal\ncounterfactual inference to construct multimodal sentiment causal inference.\nCF-MSA mitigates the direct effect from unimodal bias and ensures heterogeneity\nacross modalities by differentiating the treatment variables between\nmodalities. In addition, considering the information complementarity and bias\ndifferences between modalities, we propose a new optimisation objective to\neffectively integrate different modalities and reduce the inherent bias from\neach modality. Experimental results on two public datasets, MVSA-Single and\nMVSA-Multiple, demonstrate that the proposed CF-MSA has superior debiasing\ncapability and achieves new state-of-the-art performances. We will release the\ncode and datasets to facilitate future research.", "published": "2024-12-10 08:21:19", "link": "http://arxiv.org/abs/2412.07292v1", "categories": ["cs.MM", "cs.CL"], "primary_category": "cs.MM"}
{"title": "Towards Predictive Communication with Brain-Computer Interfaces\n  integrating Large Language Models", "abstract": "This perspective article aims at providing an outline of the state of the art\nand future developments towards the integration of cutting-edge predictive\nlanguage models with BCI. A synthetic overview of early and more recent\nlinguistic models, from natural language processing (NLP) models to recent LLM,\nthat to a varying extent improved predictive writing systems, is first\nprovided. Second, a summary of previous BCI implementations integrating\nlanguage models is presented. The few preliminary studies investigating the\npossible combination of LLM with BCI spellers to efficiently support fast\ncommunication and control are then described. Finally, current challenges and\nlimitations towards the full integration of LLM with BCI systems are discussed.\nRecent investigations suggest that the combination of LLM with BCI might\ndrastically improve human-computer interaction in patients with motor or\nlanguage disorders as well as in healthy individuals. In particular, the\npretrained autoregressive transformer models, such as GPT, that capitalize from\nparallelization, learning through pre-training and fine-tuning, promise a\nsubstantial improvement of BCI for communication with respect to previous\nsystems incorporating simpler language models. Indeed, among various models,\nthe GPT-2 was shown to represent an excellent candidate for its integration\ninto BCI although testing was only perfomed on simulated conversations and not\non real BCI scenarios. Prospectively, the full integration of LLM with advanced\nBCI systems might lead to a big leap forward towards fast, efficient and\nuser-adaptive neurotechnology.", "published": "2024-12-10 09:48:07", "link": "http://arxiv.org/abs/2412.07355v2", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction", "abstract": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance.", "published": "2024-12-10 10:27:41", "link": "http://arxiv.org/abs/2412.07380v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CMT: A Memory Compression Method for Continual Knowledge Learning of\n  Large Language Models", "abstract": "Large Language Models (LLMs) need to adapt to the continuous changes in data,\ntasks, and user preferences. Due to their massive size and the high costs\nassociated with training, LLMs are not suitable for frequent retraining.\nHowever, updates are necessary to keep them in sync with rapidly evolving human\nknowledge. To address these challenges, this paper proposes the Compression\nMemory Training (CMT) method, an efficient and effective online adaptation\nframework for LLMs that features robust knowledge retention capabilities.\nInspired by human memory mechanisms, CMT compresses and extracts information\nfrom new documents to be stored in a memory bank. When answering to queries\nrelated to these new documents, the model aggregates these document memories\nfrom the memory bank to better answer user questions. The parameters of the LLM\nitself do not change during training and inference, reducing the risk of\ncatastrophic forgetting. To enhance the encoding, retrieval, and aggregation of\nmemory, we further propose three new general and flexible techniques, including\nmemory-aware objective, self-matching and top-aggregation. Extensive\nexperiments conducted on three continual learning datasets (i.e., StreamingQA,\nSQuAD and ArchivalQA) demonstrate that the proposed method improves model\nadaptability and robustness across multiple base LLMs (e.g., +4.07 EM & +4.19\nF1 in StreamingQA with Llama-2-7b).", "published": "2024-12-10 10:35:19", "link": "http://arxiv.org/abs/2412.07393v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RAG-based Question Answering over Heterogeneous Data and Text", "abstract": "This article presents the QUASAR system for question answering over\nunstructured text, structured tables, and knowledge graphs, with unified\ntreatment of all sources. The system adopts a RAG-based architecture, with a\npipeline of evidence retrieval followed by answer generation, with the latter\npowered by a moderate-sized language model. Additionally and uniquely, QUASAR\nhas components for question understanding, to derive crisper input for evidence\nretrieval, and for re-ranking and filtering the retrieved evidence before\nfeeding the most informative pieces into the answer generation. Experiments\nwith three different benchmarks demonstrate the high answering quality of our\napproach, being on par with or better than large GPT models, while keeping the\ncomputational cost and energy consumption orders of magnitude lower.", "published": "2024-12-10 11:18:29", "link": "http://arxiv.org/abs/2412.07420v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Optimizing Alignment with Less: Leveraging Data Augmentation for\n  Personalized Evaluation", "abstract": "Automatic evaluation by large language models (LLMs) is a prominent topic\ntoday; however, judgment and evaluation tasks are often subjective and\ninfluenced by various factors, making adaptation challenging. While many\nstudies demonstrate the capabilities of state-of-the-art proprietary LLMs in\ncomparison to human evaluators, they often struggle to adapt to reference\nevaluators over time, a requirement for achieving personalized judgment.\nAdditionally, numerous works have attempted to apply open LLMs as judges or\nevaluators, but these efforts frequently overlook the limitations of working\nwith scarce data. Personalized judgment is inherently associated with limited\ndata scenarios, which are common in many real-world problems. Our work aims to\npresent a data augmentation technique to select a more effective sample from\nlimited data in order to align an open LLM with human preference. Our work\nachieves approximately 7% improvements in Pearson correlation with a reference\njudge over the baseline,and 30% improvement over the base model\n(Llama3.1-8B-Instruct) in the mathematical reasoning evaluation task.\ndemonstrating that augmenting selecting more effective preference data enables\nour approach to surpass baseline methods.", "published": "2024-12-10 11:40:11", "link": "http://arxiv.org/abs/2412.07429v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Guided Evaluation of Abstention Techniques", "abstract": "To deploy language models safely, it is crucial that they abstain from\nresponding to inappropriate requests. Several prior studies test the safety\npromises of models based on their effectiveness in blocking malicious requests.\nIn this work, we focus on evaluating the underlying techniques that cause\nmodels to abstain. We create SELECT, a benchmark derived from a set of benign\nconcepts (e.g., \"rivers\") from a knowledge graph. Focusing on benign concepts\nisolates the effect of safety training, and grounding these concepts in a\nknowledge graph allows us to study the generalization and specificity of\nabstention techniques. Using SELECT, we benchmark different abstention\ntechniques over six open-weight and closed-source models. We find that the\nexamined techniques indeed cause models to abstain with over $80\\%$ abstention\nrates. However, these techniques are not as effective for descendants of the\ntarget concepts, where abstention rates drop by $19\\%$. We also characterize\nthe generalization-specificity trade-offs for different techniques. Overall, no\nsingle technique is invariably better than others, and our findings inform\npractitioners of the various trade-offs involved.", "published": "2024-12-10 11:40:47", "link": "http://arxiv.org/abs/2412.07430v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Bilingual BSARD: Extending Statutory Article Retrieval to Dutch", "abstract": "Statutory article retrieval plays a crucial role in making legal information\nmore accessible to both laypeople and legal professionals. Multilingual\ncountries like Belgium present unique challenges for retrieval models due to\nthe need for handling legal issues in multiple languages. Building on the\nBelgian Statutory Article Retrieval Dataset (BSARD) in French, we introduce the\nbilingual version of this dataset, bBSARD. The dataset contains parallel\nBelgian statutory articles in both French and Dutch, along with legal questions\nfrom BSARD and their Dutch translation. Using bBSARD, we conduct extensive\nbenchmarking of retrieval models available for Dutch and French. Our\nbenchmarking setup includes lexical models, zero-shot dense models, and\nfine-tuned small foundation models. Our experiments show that BM25 remains a\ncompetitive baseline compared to many zero-shot dense models in both languages.\nWe also observe that while proprietary models outperform open alternatives in\nthe zero-shot setting, they can be matched or surpassed by fine-tuning small\nlanguage-specific models. Our dataset and evaluation code are publicly\navailable.", "published": "2024-12-10 12:31:33", "link": "http://arxiv.org/abs/2412.07462v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Subtopic-aware View Sampling and Temporal Aggregation for Long-form\n  Document Matching", "abstract": "Long-form document matching aims to judge the relevance between two documents\nand has been applied to various scenarios. Most existing works utilize\nhierarchical or long context models to process documents, which achieve coarse\nunderstanding but may ignore details. Some researchers construct a document\nview with similar sentences about aligned document subtopics to focus on\ndetailed matching signals. However, a long document generally contains multiple\nsubtopics. The matching signals are heterogeneous from multiple topics.\nConsidering only the homologous aligned subtopics may not be representative\nenough and may cause biased modeling. In this paper, we introduce a new\nframework to model representative matching signals. First, we propose to\ncapture various matching signals through subtopics of document pairs. Next, We\nconstruct multiple document views based on subtopics to cover heterogeneous and\nvaluable details. However, existing spatial aggregation methods like attention,\nwhich integrate all these views simultaneously, are hard to integrate\nheterogeneous information. Instead, we propose temporal aggregation, which\neffectively integrates different views gradually as the training progresses.\nExperimental results show that our learning framework is effective on several\ndocument-matching tasks, including news duplication and legal case retrieval.", "published": "2024-12-10 15:06:48", "link": "http://arxiv.org/abs/2412.07573v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Adapting to Non-Stationary Environments: Multi-Armed Bandit Enhanced\n  Retrieval-Augmented Generation on Knowledge Graphs", "abstract": "Despite the superior performance of Large language models on many NLP tasks,\nthey still face significant limitations in memorizing extensive world\nknowledge. Recent studies have demonstrated that leveraging the\nRetrieval-Augmented Generation (RAG) framework, combined with Knowledge Graphs\nthat encapsulate extensive factual data in a structured format, robustly\nenhances the reasoning capabilities of LLMs. However, deploying such systems in\nreal-world scenarios presents challenges: the continuous evolution of\nnon-stationary environments may lead to performance degradation and user\nsatisfaction requires a careful balance of performance and responsiveness. To\naddress these challenges, we introduce a Multi-objective Multi-Armed Bandit\nenhanced RAG framework, supported by multiple retrieval methods with diverse\ncapabilities under rich and evolving retrieval contexts in practice. Within\nthis framework, each retrieval method is treated as a distinct ``arm''. The\nsystem utilizes real-time user feedback to adapt to dynamic environments, by\nselecting the appropriate retrieval method based on input queries and the\nhistorical multi-objective performance of each arm. Extensive experiments\nconducted on two benchmark KGQA datasets demonstrate that our method\nsignificantly outperforms baseline methods in non-stationary settings while\nachieving state-of-the-art performance in stationary environments. Code and\ndata are available at https://github.com/FUTUREEEEEE/Dynamic-RAG.git", "published": "2024-12-10 15:56:03", "link": "http://arxiv.org/abs/2412.07618v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Piece of Table: A Divide-and-Conquer Approach for Selecting Subtables in\n  Table Question Answering", "abstract": "Applying language models (LMs) to tables is challenging due to the inherent\nstructural differences between two-dimensional tables and one-dimensional text\nfor which the LMs were originally designed. Furthermore, when applying\nlinearized tables to LMs, the maximum token lengths often imposed in\nself-attention calculations make it difficult to comprehensively understand the\ncontext spread across large tables. To address these challenges, we present\nPieTa (Piece of Table), a new framework for subtable-based question answering\n(QA). PieTa operates through an iterative process of dividing tables into\nsmaller windows, using LMs to select relevant cells within each window, and\nmerging these cells into a subtable. This multi-resolution approach captures\ndependencies across multiple rows and columns while avoiding the limitations\ncaused by long context inputs. Instantiated as a simple iterative subtable\nunion algorithm, PieTa demonstrates improved performance over previous\nsubtable-based QA approaches.", "published": "2024-12-10 16:08:14", "link": "http://arxiv.org/abs/2412.07629v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FlexLLM: Exploring LLM Customization for Moving Target Defense on\n  Black-Box LLMs Against Jailbreak Attacks", "abstract": "Defense in large language models (LLMs) is crucial to counter the numerous\nattackers exploiting these systems to generate harmful content through\nmanipulated prompts, known as jailbreak attacks. Although many defense\nstrategies have been proposed, they often require access to the model's\ninternal structure or need additional training, which is impractical for\nservice providers using LLM APIs, such as OpenAI APIs or Claude APIs. In this\npaper, we propose a moving target defense approach that alters decoding\nhyperparameters to enhance model robustness against various jailbreak attacks.\nOur approach does not require access to the model's internal structure and\nincurs no additional training costs. The proposed defense includes two key\ncomponents: (1) optimizing the decoding strategy by identifying and adjusting\ndecoding hyperparameters that influence token generation probabilities, and (2)\ntransforming the decoding hyperparameters and model system prompts into dynamic\ntargets, which are continuously altered during each runtime. By continuously\nmodifying decoding strategies and prompts, the defense effectively mitigates\nthe existing attacks. Our results demonstrate that our defense is the most\neffective against jailbreak attacks in three of the models tested when using\nLLMs as black-box APIs. Moreover, our defense offers lower inference costs and\nmaintains comparable response quality, making it a potential layer of\nprotection when used alongside other defense methods.", "published": "2024-12-10 17:02:28", "link": "http://arxiv.org/abs/2412.07672v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting", "abstract": "Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.", "published": "2024-12-10 17:02:58", "link": "http://arxiv.org/abs/2412.07675v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can linguists better understand DNA?", "abstract": "Multilingual transfer ability, which reflects how well models fine-tuned on\none source language can be applied to other languages, has been well studied in\nmultilingual pre-trained models. However, the existence of such capability\ntransfer between natural language and gene sequences/languages remains under\nexplored.This study addresses this gap by drawing inspiration from the\nsentence-pair classification task used for evaluating sentence similarity in\nnatural language. We constructed two analogous tasks: DNA-pair\nclassification(DNA sequence similarity) and DNA-protein-pair\nclassification(gene coding determination). These tasks were designed to\nvalidate the transferability of capabilities from natural language to gene\nsequences. Even a small-scale pre-trained model like GPT-2-small, which was\npre-trained on English, achieved an accuracy of 78% on the DNA-pair\nclassification task after being fine-tuned on English sentence-pair\nclassification data(XTREME PAWS-X). While training a BERT model on multilingual\ntext, the precision reached 89%. On the more complex DNA-protein-pair\nclassification task, however, the model's output was barely distinguishable\nfrom random output.Experimental validation has confirmed that the transfer of\ncapabilities from natural language to biological language is unequivocally\npresent. Building on this foundation, we have also investigated the impact of\nmodel parameter scale and pre-training on this capability transfer. We provide\nrecommendations for facilitating the transfer of capabilities from natural\nlanguage to genetic language,as well as new approaches for conducting\nbiological research based on this capability.This study offers an intriguing\nnew perspective on exploring the relationship between natural language and\ngenetic language.", "published": "2024-12-10 17:06:33", "link": "http://arxiv.org/abs/2412.07678v3", "categories": ["cs.CL", "q-bio.GN", "92-10", "J.3"], "primary_category": "cs.CL"}
{"title": "Multi-Response Preference Optimization with Augmented Ranking Dataset", "abstract": "Recent advancements in Large Language Models (LLMs) have been remarkable,\nwith new models consistently surpassing their predecessors. These advancements\nare underpinned by extensive research on various training mechanisms. Among\nthese, Preference Optimization has played a significant role in improving the\nperformance of LLMs by incorporating human preferences into the training\nprocess. However, constructing preference optimization datasets is challenging\nand the optimization process is highly sensitive to the dataset quality. In\nthis study, we propose a novel approach to augment Preference Optimization\ndatasets. Additionally, we introduce a Multi-response-based Preference\nOptimization training method that enables the simultaneous learning of multiple\nresponses.", "published": "2024-12-10 05:45:36", "link": "http://arxiv.org/abs/2412.07812v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HEDS 3.0: The Human Evaluation Data Sheet Version 3.0", "abstract": "This paper presents version 3.0 of the Human Evaluation Datasheet (HEDS).\nThis update is the result of our experience using HEDS in the context of\nnumerous recent human evaluation experiments, including reproduction studies,\nand of feedback received. Our main overall goal was to improve clarity, and to\nenable users to complete the datasheet more consistently and comparably. The\nHEDS 3.0 package consists of the digital data sheet, documentation, and code\nfor exporting completed data sheets as latex files, all available from the HEDS\nGitHub.", "published": "2024-12-10 21:57:32", "link": "http://arxiv.org/abs/2412.07940v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework", "abstract": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-aware data preparation involves specific tasks such as column\naugmentation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multi-agent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-of-Clauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation.", "published": "2024-12-10 11:03:49", "link": "http://arxiv.org/abs/2412.10422v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Look Before You Leap: Enhancing Attention and Vigilance Regarding\n  Harmful Content with GuidelineLLM", "abstract": "Despite being empowered with alignment mechanisms, large language models\n(LLMs) are increasingly vulnerable to emerging jailbreak attacks that can\ncompromise their alignment mechanisms. This vulnerability poses significant\nrisks to the real-world applications. Existing work faces challenges in both\ntraining efficiency and generalization capabilities (i.e., Reinforcement\nLearning from Human Feedback and Red-Teaming). Developing effective strategies\nto enable LLMs to resist continuously evolving jailbreak attempts represents a\nsignificant challenge. To address this challenge, we propose a novel defensive\nparadigm called GuidelineLLM, which assists LLMs in recognizing queries that\nmay have harmful content. Before LLMs respond to a query, GuidelineLLM first\nidentifies potential risks associated with the query, summarizes these risks\ninto guideline suggestions, and then feeds these guidelines to the responding\nLLMs. Importantly, our approach eliminates the necessity for additional safety\nfine-tuning of the LLMs themselves; only the GuidelineLLM requires fine-tuning.\nThis characteristic enhances the general applicability of GuidelineLLM across\nvarious LLMs. Experimental results demonstrate that GuidelineLLM can\nsignificantly reduce the attack success rate (ASR) against the LLMs (an average\nreduction of 34.17\\% ASR) while maintaining the helpfulness of the LLMs in\nhandling benign queries. Code is available at\nhttps://github.com/sqzhang-lazy/GuidelineLLM.", "published": "2024-12-10 12:42:33", "link": "http://arxiv.org/abs/2412.10423v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM\n  Evaluation", "abstract": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/.", "published": "2024-12-10 15:00:32", "link": "http://arxiv.org/abs/2412.10424v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Active Inference for Self-Organizing Multi-LLM Systems: A Bayesian\n  Thermodynamic Approach to Adaptation", "abstract": "This paper introduces a novel approach to creating adaptive language agents\nby integrating active inference with large language models (LLMs). While LLMs\ndemonstrate remarkable capabilities, their reliance on static prompts limits\nadaptation to new information and changing environments. We address this by\nimplementing an active inference framework that acts as a cognitive layer above\nan LLM-based agent, dynamically adjusting prompts and search strategies through\nprincipled information-seeking behavior. Our framework models the environment\nusing three state factors (prompt, search, and information states) with seven\nobservation modalities capturing quality metrics. By framing the agent's\nlearning through the free energy principle, we enable systematic exploration of\nprompt combinations and search strategies. Experimental results demonstrate the\neffectiveness of this approach, with the agent developing accurate models of\nenvironment dynamics evidenced by emergent structure in observation matrices.\nAction selection patterns reveal sophisticated exploration-exploitation\nbehavior, transitioning from initial information-gathering to targeted prompt\ntesting. The integration of thermodynamic principles with language model\ncapabilities provides a principled framework for creating robust, adaptable\nagents, extending active inference beyond traditional low-dimensional control\nproblems to high-dimensional, language-driven environments.", "published": "2024-12-10 16:34:47", "link": "http://arxiv.org/abs/2412.10425v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying and Manipulating Personality Traits in LLMs Through\n  Activation Engineering", "abstract": "The field of large language models (LLMs) has grown rapidly in recent years,\ndriven by the desire for better efficiency, interpretability, and safe use.\nBuilding on the novel approach of \"activation engineering,\" this study explores\npersonality modification in LLMs, drawing inspiration from research like\nRefusal in LLMs Is Mediated by a Single Direction (arXiv:2406.11717) and\nSteering Llama 2 via Contrastive Activation Addition (arXiv:2312.06681). We\nleverage activation engineering to develop a method for identifying and\nadjusting activation directions related to personality traits, which may allow\nfor dynamic LLM personality fine-tuning. This work aims to further our\nunderstanding of LLM interpretability while examining the ethical implications\nof such developments.", "published": "2024-12-10 23:15:25", "link": "http://arxiv.org/abs/2412.10427v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automatic Item Generation for Personality Situational Judgment Tests\n  with Large Language Models", "abstract": "Personality assessment, particularly through situational judgment tests\n(SJTs), is a vital tool for psychological research, talent selection, and\neducational evaluation. This study explores the potential of GPT-4, a\nstate-of-the-art large language model (LLM), to automate the generation of\npersonality situational judgment tests (PSJTs) in Chinese. Traditional SJT\ndevelopment is labor-intensive and prone to biases, while GPT-4 offers a\nscalable, efficient alternative. Two studies were conducted: Study 1 evaluated\nthe impact of prompt design and temperature settings on content validity,\nfinding that optimized prompts with a temperature of 1.0 produced creative and\naccurate items. Study 2 assessed the psychometric properties of GPT-4-generated\nPSJTs, revealing that they demonstrated satisfactory reliability and validity,\nsurpassing the performance of manually developed tests in measuring the Big\nFive personality traits. This research highlights GPT-4's effectiveness in\ndeveloping high-quality PSJTs, providing a scalable and innovative method for\npsychometric test development. These findings expand the possibilities of\nautomatic item generation and the application of LLMs in psychology, and offer\npractical implications for streamlining test development processes in\nresource-limited settings.", "published": "2024-12-10 09:13:32", "link": "http://arxiv.org/abs/2412.12144v1", "categories": ["cs.CL", "cs.AI", "I.2.1; J.4"], "primary_category": "cs.CL"}
{"title": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars", "abstract": "Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}", "published": "2024-12-10 10:14:03", "link": "http://arxiv.org/abs/2412.12145v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Review of Human Emotion Synthesis Based on Generative Technology", "abstract": "Human emotion synthesis is a crucial aspect of affective computing. It\ninvolves using computational methods to mimic and convey human emotions through\nvarious modalities, with the goal of enabling more natural and effective\nhuman-computer interactions. Recent advancements in generative models, such as\nAutoencoders, Generative Adversarial Networks, Diffusion Models, Large Language\nModels, and Sequence-to-Sequence Models, have significantly contributed to the\ndevelopment of this field. However, there is a notable lack of comprehensive\nreviews in this field. To address this problem, this paper aims to address this\ngap by providing a thorough and systematic overview of recent advancements in\nhuman emotion synthesis based on generative models. Specifically, this review\nwill first present the review methodology, the emotion models involved, the\nmathematical principles of generative models, and the datasets used. Then, the\nreview covers the application of different generative models to emotion\nsynthesis based on a variety of modalities, including facial images, speech,\nand text. It also examines mainstream evaluation metrics. Additionally, the\nreview presents some major findings and suggests future research directions,\nproviding a comprehensive understanding of the role of generative technology in\nthe nuanced domain of emotion synthesis.", "published": "2024-12-10 02:06:10", "link": "http://arxiv.org/abs/2412.07116v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MM-PoE: Multiple Choice Reasoning via. Process of Elimination using\n  Multi-Modal Models", "abstract": "This paper introduces Multiple Choice Reasoning via. Process of Elimination\nusing Multi-Modal models, herein referred to as Multi-Modal Process of\nElimination (MM-PoE). This novel methodology is engineered to augment the\nefficacy of Vision-Language Models (VLMs) in multiple-choice visual reasoning\ntasks. Diverging from conventional approaches that evaluate each option\nindependently, MM-PoE employs a dual-step scoring paradigm that initially\nidentifies and excludes implausible choices, subsequently concentrating on the\nmost probable remaining options. This method emulates human test-taking\nstrategies, where individuals typically eliminate clearly incorrect answers\nprior to selecting the optimal response. Our empirical evaluations, conducted\nacross three benchmark datasets, reveal that MM-PoE significantly improves both\nzero-shot and few-shot performance of contemporary state-of-the-art VLMs.\nCritically, this approach not only broadens the application of the elimination\nprocess to multi-modal contexts but also allows few-shot experiments, thereby\naddressing two principal limitations concerning usage of PoE only in zero-shot\nsettings and only with a language-only framework. As a result, MM-PoE not only\nrefines the reasoning capabilities of VLMs but also broadens their\napplicability to complex visual question-answering scenarios. All code and\ndocumentation supporting our work are available at\nhttps://pypi.org/project/mm-poe/, enabling researchers and practitioners to\neasily integrate and further develop these techniques.", "published": "2024-12-10 03:13:41", "link": "http://arxiv.org/abs/2412.07148v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "PrisonBreak: Jailbreaking Large Language Models with Fewer Than\n  Twenty-Five Targeted Bit-flips", "abstract": "We introduce a new class of attacks on commercial-scale (human-aligned)\nlanguage models that induce jailbreaking through targeted bitwise corruptions\nin model parameters. Our adversary can jailbreak billion-parameter language\nmodels with fewer than 25 bit-flips in all cases$-$and as few as 5 in\nsome$-$using up to 40$\\times$ less bit-flips than existing attacks on computer\nvision models at least 100$\\times$ smaller. Unlike prompt-based jailbreaks, our\nattack renders these models in memory 'uncensored' at runtime, allowing them to\ngenerate harmful responses without any input modifications. Our attack\nalgorithm efficiently identifies target bits to flip, offering up to 20$\\times$\nmore computational efficiency than previous methods. This makes it practical\nfor language models with billions of parameters. We show an end-to-end\nexploitation of our attack using software-induced fault injection, Rowhammer\n(RH). Our work examines 56 DRAM RH profiles from DDR4 and LPDDR4X devices with\ndifferent RH vulnerabilities. We show that our attack can reliably induce\njailbreaking in systems similar to those affected by prior bit-flip attacks.\nMoreover, our approach remains effective even against highly RH-secure systems\n(e.g., 46$\\times$ more secure than previously tested systems). Our analyses\nfurther reveal that: (1) models with less post-training alignment require fewer\nbit flips to jailbreak; (2) certain model components, such as value projection\nlayers, are substantially more vulnerable than others; and (3) our method is\nmechanistically different than existing jailbreaks. Our findings highlight a\npressing, practical threat to the language model ecosystem and underscore the\nneed for research to protect these models from bit-flip attacks.", "published": "2024-12-10 05:00:01", "link": "http://arxiv.org/abs/2412.07192v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Modifying AI, Enhancing Essays: How Active Engagement with Generative AI\n  Boosts Writing Quality", "abstract": "Students are increasingly relying on Generative AI (GAI) to support their\nwriting-a key pedagogical practice in education. In GAI-assisted writing,\nstudents can delegate core cognitive tasks (e.g., generating ideas and turning\nthem into sentences) to GAI while still producing high-quality essays. This\ncreates new challenges for teachers in assessing and supporting student\nlearning, as they often lack insight into whether students are engaging in\nmeaningful cognitive processes during writing or how much of the essay's\nquality can be attributed to those processes. This study aimed to help teachers\nbetter assess and support student learning in GAI-assisted writing by examining\nhow different writing behaviors, especially those indicative of meaningful\nlearning versus those that are not, impact essay quality. Using a dataset of\n1,445 GAI-assisted writing sessions, we applied the cutting-edge method,\nX-Learner, to quantify the causal impact of three GAI-assisted writing\nbehavioral patterns (i.e., seeking suggestions but not accepting them, seeking\nsuggestions and accepting them as they are, and seeking suggestions and\naccepting them with modification) on four measures of essay quality (i.e.,\nlexical sophistication, syntactic complexity, text cohesion, and linguistic\nbias). Our analysis showed that writers who frequently modified GAI-generated\ntext-suggesting active engagement in higher-order cognitive\nprocesses-consistently improved the quality of their essays in terms of lexical\nsophistication, syntactic complexity, and text cohesion. In contrast, those who\noften accepted GAI-generated text without changes, primarily engaging in\nlower-order processes, saw a decrease in essay quality. Additionally, while\nhuman writers tend to introduce linguistic bias when writing independently,\nincorporating GAI-generated text-even without modification-can help mitigate\nthis bias.", "published": "2024-12-10 05:32:57", "link": "http://arxiv.org/abs/2412.07200v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "MAPLE: A Framework for Active Preference Learning Guided by Large\n  Language Models", "abstract": "The advent of large language models (LLMs) has sparked significant interest\nin using natural language for preference learning. However, existing methods\noften suffer from high computational burdens, taxing human supervision, and\nlack of interpretability. To address these issues, we introduce MAPLE, a\nframework for large language model-guided Bayesian active preference learning.\nMAPLE leverages LLMs to model the distribution over preference functions,\nconditioning it on both natural language feedback and conventional preference\nlearning feedback, such as pairwise trajectory rankings. MAPLE also employs\nactive learning to systematically reduce uncertainty in this distribution and\nincorporates a language-conditioned active query selection mechanism to\nidentify informative and easy-to-answer queries, thus reducing human burden. We\nevaluate MAPLE's sample efficiency and preference inference quality across two\nbenchmarks, including a real-world vehicle route planning benchmark using\nOpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning\nprocess and effectively improves humans' ability to answer queries.", "published": "2024-12-10 05:55:14", "link": "http://arxiv.org/abs/2412.07207v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HARP: Hesitation-Aware Reframing in Transformer Inference Pass", "abstract": "This paper aims to improve the performance of large language models by\naddressing the variable computational demands in inference steps, where some\ntokens require more computational resources than others. We present HARP, a\nsimple modification to \"off-the-shelf\" Transformer forward pass. Drawing from\nhesitation and the framing effect in decision-making, HARP selectively applies\nadditional computation when the model encounters uncertainty during token\ngeneration. Our method mimics human cognitive processes by pausing at difficult\ndecision points and reframing inputs for a different perspective. Unlike other\napproaches, HARP is model-agnostic, training-free, and easy to implement. We\nthoroughly evaluate our method across various downstream tasks and model sizes,\ndemonstrating performance improvements up to +5.16%. Notably, HARP achieves\nthese gains while maintaining inference times twice faster than beam search.\nSimple and yet with significant gains, HARP offers a practical solution for\nenhancing the performance of Transformer-based language models with minimal\ncomputational impact.", "published": "2024-12-10 08:12:22", "link": "http://arxiv.org/abs/2412.07282v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Review of Challenges in Speech-based Conversational AI for Elderly\n  Care", "abstract": "Artificially intelligent systems optimized for speech conversation are\nappearing at a fast pace. Such models are interesting from a healthcare\nperspective, as these voice-controlled assistants may support the elderly and\nenable remote health monitoring. The bottleneck for efficacy, however, is how\nwell these devices work in practice and how the elderly experience them, but\nresearch on this topic is scant. We review elderly use of voice-controlled AI\nand highlight various user- and technology-centered issues, that need to be\nconsidered before effective speech-controlled AI for elderly care can be\nrealized.", "published": "2024-12-10 10:32:22", "link": "http://arxiv.org/abs/2412.07388v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.ET"], "primary_category": "cs.CL"}
{"title": "Generating Knowledge Graphs from Large Language Models: A Comparative\n  Study of GPT-4, LLaMA 2, and BERT", "abstract": "Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a\nform of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks\nrequiring structured reasoning and semantic understanding. However, creating\nKGs for GraphRAGs remains a significant challenge due to accuracy and\nscalability limitations of traditional methods. This paper introduces a novel\napproach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and\nBERT to generate KGs directly from unstructured data, bypassing traditional\npipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit\nDistance, and Semantic Similarity, we evaluate the models' ability to generate\nhigh-quality KGs. Results demonstrate that GPT-4 achieves superior semantic\nfidelity and structural accuracy, LLaMA 2 excels in lightweight,\ndomain-specific graphs, and BERT provides insights into challenges in\nentity-relationship modeling. This study underscores the potential of LLMs to\nstreamline KG creation and enhance GraphRAG accessibility for real-world\napplications, while setting a foundation for future advancements.", "published": "2024-12-10 11:05:26", "link": "http://arxiv.org/abs/2412.07412v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "A Causal World Model Underlying Next Token Prediction in GPT", "abstract": "Are generative pre-trained transformer (GPT) models only trained to predict\nthe next token, or do they implicitly learn a world model from which a sequence\nis generated one token at a time? We examine this question by deriving a causal\ninterpretation of the attention mechanism in GPT, and suggesting a causal world\nmodel that arises from this interpretation. Furthermore, we propose that\nGPT-models, at inference time, can be utilized for zero-shot causal structure\nlearning for in-distribution sequences. Empirical evaluation is conducted in a\ncontrolled synthetic environment using the setup and rules of the Othello board\ngame. A GPT, pre-trained on real-world games played with the intention of\nwinning, is tested on synthetic data that only adheres to the game rules,\noblivious to the goal of winning. We find that the GPT model is likely to\ngenerate moves that adhere to the game rules for sequences for which a causal\nstructure is encoded in the attention mechanism with high confidence. In\ngeneral, in cases for which the GPT model generates moves that do not adhere to\nthe game rules, it also fails to capture any causal structure.", "published": "2024-12-10 12:05:03", "link": "http://arxiv.org/abs/2412.07446v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Identifying Quantum Mechanical Statistics in Italian Corpora", "abstract": "We present a theoretical and empirical investigation of the statistical\nbehaviour of the words in a text produced by human language. To this aim, we\nanalyse the word distribution of various texts of Italian language selected\nfrom a specific literary corpus. We firstly generalise a theoretical framework\nelaborated by ourselves to identify 'quantum mechanical statistics' in\nlarge-size texts. Then, we show that, in all analysed texts, words distribute\naccording to 'Bose--Einstein statistics' and show significant deviations from\n'Maxwell--Boltzmann statistics'. Next, we introduce an effect of 'word\nrandomization' which instead indicates that the difference between the two\nstatistical models is not as pronounced as in the original cases. These results\nconfirm the empirical patterns obtained in texts of English language and\nstrongly indicate that identical words tend to 'clump together' as a\nconsequence of their meaning, which can be explained as an effect of 'quantum\nentanglement' produced through a phenomenon of 'contextual updating'. More,\nword randomization can be seen as the linguistic-conceptual equivalent of an\nincrease of temperature which destroys 'coherence' and makes classical\nstatistics prevail over quantum statistics. Some insights into the origin of\nquantum statistics in physics are finally provided.", "published": "2024-12-10 21:04:56", "link": "http://arxiv.org/abs/2412.07919v1", "categories": ["q-bio.NC", "cs.CL", "quant-ph"], "primary_category": "q-bio.NC"}
{"title": "Forking Paths in Neural Text Generation", "abstract": "Estimating uncertainty in Large Language Models (LLMs) is important for\nproperly evaluating LLMs, and ensuring safety for users. However, prior\napproaches to uncertainty estimation focus on the final answer in generated\ntext, ignoring intermediate steps that might dramatically impact the outcome.\nWe hypothesize that there exist key forking tokens, such that re-sampling the\nsystem at those specific tokens, but not others, leads to very different\noutcomes. To test this empirically, we develop a novel approach to representing\nuncertainty dynamics across individual tokens of text generation, and applying\nstatistical models to test our hypothesis. Our approach is highly flexible: it\ncan be applied to any dataset and any LLM, without fine tuning or accessing\nmodel weights. We use our method to analyze LLM responses on 7 different tasks\nacross 4 domains, spanning a wide range of typical use cases. We find many\nexamples of forking tokens, including surprising ones such as punctuation\nmarks, suggesting that LLMs are often just a single token away from saying\nsomething very different.", "published": "2024-12-10 22:57:57", "link": "http://arxiv.org/abs/2412.07961v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machines of Meaning", "abstract": "One goal of Artificial Intelligence is to learn meaningful representations\nfor natural language expressions, but what this entails is not always clear. A\nvariety of new linguistic behaviours present themselves embodied as computers,\nenhanced humans, and collectives with various kinds of integration and\ncommunication. But to measure and understand the behaviours generated by such\nsystems, we must clarify the language we use to talk about them. Computational\nmodels are often confused with the phenomena they try to model and shallow\nmetaphors are used as justifications for (or to hype) the success of\ncomputational techniques on many tasks related to natural language; thus\nimplying their progress toward human-level machine intelligence without ever\nclarifying what that means.\n  This paper discusses the challenges in the specification of \"machines of\nmeaning\", machines capable of acquiring meaningful semantics from natural\nlanguage in order to achieve their goals. We characterize \"meaning\" in a\ncomputational setting, while highlighting the need for detachment from\nanthropocentrism in the study of the behaviour of machines of meaning. The\npressing need to analyse AI risks and ethics requires a proper measurement of\nits capabilities which cannot be productively studied and explained while using\nambiguous language. We propose a view of \"meaning\" to facilitate the discourse\naround approaches such as neural language models and help broaden the research\nperspectives for technology that facilitates dialogues between humans and\nmachines.", "published": "2024-12-10 23:23:28", "link": "http://arxiv.org/abs/2412.07975v1", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "I.2.0; I.2.7; A.1"], "primary_category": "cs.AI"}
{"title": "Evaluation Agent: Efficient and Promptable Evaluation Framework for\n  Visual Generative Models", "abstract": "Recent advancements in visual generative models have enabled high-quality\nimage and video generation, opening diverse applications. However, evaluating\nthese models often demands sampling hundreds or thousands of images or videos,\nmaking the process computationally expensive, especially for diffusion-based\nmodels with inherently slow sampling. Moreover, existing evaluation methods\nrely on rigid pipelines that overlook specific user needs and provide numerical\nresults without clear explanations. In contrast, humans can quickly form\nimpressions of a model's capabilities by observing only a few samples. To mimic\nthis, we propose the Evaluation Agent framework, which employs human-like\nstrategies for efficient, dynamic, multi-round evaluations using only a few\nsamples per round, while offering detailed, user-tailored analyses. It offers\nfour key advantages: 1) efficiency, 2) promptable evaluation tailored to\ndiverse user needs, 3) explainability beyond single numerical scores, and 4)\nscalability across various models and tools. Experiments show that Evaluation\nAgent reduces evaluation time to 10% of traditional methods while delivering\ncomparable results. The Evaluation Agent framework is fully open-sourced to\nadvance research in visual generative models and their efficient evaluation.", "published": "2024-12-10 18:52:39", "link": "http://arxiv.org/abs/2412.09645v2", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CAP: Evaluation of Persuasive and Creative Image Generation", "abstract": "We address the task of advertisement image generation and introduce three\nevaluation metrics to assess Creativity, prompt Alignment, and Persuasiveness\n(CAP) in generated advertisement images. Despite recent advancements in\nText-to-Image (T2I) generation and their performance in generating high-quality\nimages for explicit descriptions, evaluating these models remains challenging.\nExisting evaluation methods focus largely on assessing alignment with explicit,\ndetailed descriptions, but evaluating alignment with visually implicit prompts\nremains an open problem. Additionally, creativity and persuasiveness are\nessential qualities that enhance the effectiveness of advertisement images, yet\nare seldom measured. To address this, we propose three novel metrics for\nevaluating the creativity, alignment, and persuasiveness of generated images.\nOur findings reveal that current T2I models struggle with creativity,\npersuasiveness, and alignment when the input text is implicit messages. We\nfurther introduce a simple yet effective approach to enhance T2I models'\ncapabilities in producing images that are better aligned, more creative, and\nmore persuasive.", "published": "2024-12-10 19:54:59", "link": "http://arxiv.org/abs/2412.10426v1", "categories": ["cs.CV", "cs.CL", "cs.GR"], "primary_category": "cs.CV"}
{"title": "Observing Micromotives and Macrobehavior of Large Language Models", "abstract": "Thomas C. Schelling, awarded the 2005 Nobel Memorial Prize in Economic\nSciences, pointed out that ``individuals decisions (micromotives), while often\npersonal and localized, can lead to societal outcomes (macrobehavior) that are\nfar more complex and different from what the individuals intended.'' The\ncurrent research related to large language models' (LLMs') micromotives, such\nas preferences or biases, assumes that users will make more appropriate\ndecisions once LLMs are devoid of preferences or biases. Consequently, a series\nof studies has focused on removing bias from LLMs. In the NLP community, while\nthere are many discussions on LLMs' micromotives, previous studies have seldom\nconducted a systematic examination of how LLMs may influence society's\nmacrobehavior. In this paper, we follow the design of Schelling's model of\nsegregation to observe the relationship between the micromotives and\nmacrobehavior of LLMs. Our results indicate that, regardless of the level of\nbias in LLMs, a highly segregated society will emerge as more people follow\nLLMs' suggestions. We hope our discussion will spark further consideration of\nthe fundamental assumption regarding the mitigation of LLMs' micromotives and\nencourage a reevaluation of how LLMs may influence users and society.", "published": "2024-12-10 23:25:14", "link": "http://arxiv.org/abs/2412.10428v1", "categories": ["physics.soc-ph", "cs.AI", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Personalized and Sequential Text-to-Image Generation", "abstract": "We address the problem of personalized, interactive text-to-image (T2I)\ngeneration, designing a reinforcement learning (RL) agent which iteratively\nimproves a set of generated images for a user through a sequence of prompt\nexpansions. Using human raters, we create a novel dataset of sequential\npreferences, which we leverage, together with large-scale open-source\n(non-sequential) datasets. We construct user-preference and user-choice models\nusing an EM strategy and identify varying user preference types. We then\nleverage a large multimodal language model (LMM) and a value-based RL approach\nto suggest a personalized and diverse slate of prompt expansions to the user.\nOur Personalized And Sequential Text-to-image Agent (PASTA) extends T2I models\nwith personalized multi-turn capabilities, fostering collaborative co-creation\nand addressing uncertainty or underspecification in a user's intent. We\nevaluate PASTA using human raters, showing significant improvement compared to\nbaseline methods. We also release our sequential rater dataset and simulated\nuser-rater interactions to support future research in personalized, multi-turn\nT2I generation.", "published": "2024-12-10 01:47:40", "link": "http://arxiv.org/abs/2412.10419v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.CV"}
{"title": "Spatio-temporal Latent Representations for the Analysis of Acoustic\n  Scenes in-the-wild", "abstract": "In the field of acoustic scene analysis, this paper presents a novel approach\nto find spatio-temporal latent representations from in-the-wild audio data. By\nusing WE-LIVE, an in-house collected dataset that includes audio recordings in\ndiverse real-world environments together with sparse GPS coordinates,\nself-annotated emotional and situational labels, we tackle the challenging task\nof associating each audio segment with its corresponding location as a pretext\ntask, with the final aim of acoustically detecting violent (anomalous)\ncontexts, left as further work. By generating acoustic embeddings and using the\nself-supervised learning paradigm, we aim to use the model-generated latent\nspace to acoustically characterize the spatio-temporal context. We use YAMNet,\nan acoustic events classifier trained in AudioSet to temporally locate and\nidentify acoustic events in WE-LIVE. In order to transform the discrete\nacoustic events into embeddings, we compare the information-retrieval-based\nTF-IDF algorithm and Node2Vec as an analogy to Natural Language Processing\ntechniques. A VAE is then trained to provide a further adapted latent space.\nThe analysis was carried out by measuring the cosine distance and visualizing\ndata distribution via t-Distributed Stochastic Neighbor Embedding, revealing\ndistinct acoustic scenes. Specifically, we discern variations between indoor\nand subway environments. Notably, these distinctions emerge within the latent\nspace of the VAE, a stark contrast to the random distribution of data points\nbefore encoding. In summary, our research contributes a pioneering approach for\nextracting spatio-temporal latent representations from in-the-wild audio data.", "published": "2024-12-10 16:34:08", "link": "http://arxiv.org/abs/2412.07648v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Preserving Speaker Information in Direct Speech-to-Speech Translation\n  with Non-Autoregressive Generation and Pretraining", "abstract": "Speech-to-Speech Translation (S2ST) refers to the conversion of speech in one\nlanguage into semantically equivalent speech in another language, facilitating\ncommunication between speakers of different languages. Speech-to-Discrete Unit\nTranslation (S2UT), a mainstream approach for end-to-end S2ST, addresses\nchallenges such as error propagation across modules and slow inference speed\noften encountered in traditional cascade systems. However, as discrete units\nprimarily capture content information, conventional S2UT methods fail to retain\nspeaker-specific characteristics from the source. Our previous work, SC-S2UT,\nintroduced a speaker adapter and a unit-to-mel structure, enabling the\npreservation of speaker information and non-autoregressive speech generation.\nBuilding on this foundation, this study proposes a self-supervised pretraining\nmethod to enrich the information extracted by both the speaker adapter and the\nunit-to-mel structure. Additionally, we investigate different feature fusion\nstrategies to further improve the integration of speaker and content features.\nExperiments conducted on the CVSS-T dataset for ES-EN and FR-EN tasks\ndemonstrate that our proposed method achieves a BLEU score improvement of 1.14\ncompared to SC-S2UT, along with significant enhancements in MOS and speaker\nsimilarity. Furthermore, our approach achieves translation quality comparable\nto traditional S2UT, with only a minimal increase of 0.04s per utterance in\ninference time, while maintaining high speaker similarity. These results\nvalidate the effectiveness of the proposed method.", "published": "2024-12-10 08:58:51", "link": "http://arxiv.org/abs/2412.07316v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interfacing with history: Curating with audio augmented objects", "abstract": "This article presents and discusses the results from visitors' interactions\nwith two audio augmented reality experiences containing audio augmented\nobjects; physical, real-world objects to which virtual audio sources have been\nattached. It then proceeds to discusses the commonly identified themes arising\nfrom the observation of visitors' behaviour within these experiences and the\nanalysis of their verbal and written feedback. The curatorial potential of\naudio augmented objects is discussed and, by way of conclusion, their\nfunctionality as interfaces to digital audio archival content is proposed,\nalong with their ability to reframe, re-contextualise and create renewed\nexperiences with existing collections of silenced museum exhibits.", "published": "2024-12-10 09:37:59", "link": "http://arxiv.org/abs/2412.07345v1", "categories": ["cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "Learning Self-Supervised Audio-Visual Representations for Sound\n  Recommendations", "abstract": "We propose a novel self-supervised approach for learning audio and visual\nrepresentations from unlabeled videos, based on their correspondence. The\napproach uses an attention mechanism to learn the relative importance of\nconvolutional features extracted at different resolutions from the audio and\nvisual streams and uses the attention features to encode the audio and visual\ninput based on their correspondence. We evaluated the representations learned\nby the model to classify audio-visual correlation as well as to recommend sound\neffects for visual scenes. Our results show that the representations generated\nby the attention model improves the correlation accuracy compared to the\nbaseline, by 18% and the recommendation accuracy by 10% for VGG-Sound, which is\na public video dataset. Additionally, audio-visual representations learned by\ntraining the attention model with cross-modal contrastive learning further\nimproves the recommendation performance, based on our evaluation using\nVGG-Sound and a more challenging dataset consisting of gameplay video\nrecordings.", "published": "2024-12-10 10:56:02", "link": "http://arxiv.org/abs/2412.07406v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Frechet Music Distance: A Metric For Generative Symbolic Music\n  Evaluation", "abstract": "In this paper we introduce the Frechet Music Distance (FMD), a novel\nevaluation metric for generative symbolic music models, inspired by the Frechet\nInception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in\ngenerative audio. FMD calculates the distance between distributions of\nreference and generated symbolic music embeddings, capturing abstract musical\nfeatures. We validate FMD across several datasets and models. Results indicate\nthat FMD effectively differentiates model quality, providing a domain-specific\nmetric for evaluating symbolic music generation, and establishing a\nreproducible standard for future research in symbolic music modeling.", "published": "2024-12-10 22:22:19", "link": "http://arxiv.org/abs/2412.07948v2", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
