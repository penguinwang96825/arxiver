{"title": "Automatic Prediction of Discourse Connectives", "abstract": "Accurate prediction of suitable discourse connectives (however, furthermore,\netc.) is a key component of any system aimed at building coherent and fluent\ndiscourses from shorter sentences and passages. As an example, a dialog system\nmight assemble a long and informative answer by sampling passages extracted\nfrom different documents retrieved from the Web. We formulate the task of\ndiscourse connective prediction and release a dataset of 2.9M sentence pairs\nseparated by discourse connectives for this task. Then, we evaluate the\nhardness of the task for human raters, apply a recently proposed decomposable\nattention (DA) model to this task and observe that the automatic predictor has\na higher F1 than human raters (32 vs. 30). Nevertheless, under specific\nconditions the raters still outperform the DA model, suggesting that there is\nheadroom for future improvements.", "published": "2017-02-03 13:06:25", "link": "http://arxiv.org/abs/1702.00992v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Multi-modal Embeddings for Natural Language Processing", "abstract": "We propose a novel discriminative model that learns embeddings from\nmultilingual and multi-modal data, meaning that our model can take advantage of\nimages and descriptions in multiple languages to improve embedding quality. To\nthat end, we introduce a modification of a pairwise contrastive estimation\noptimisation function as our training objective. We evaluate our embeddings on\nan image-sentence ranking (ISR), a semantic textual similarity (STS), and a\nneural machine translation (NMT) task. We find that the additional multilingual\nsignals lead to improvements on both the ISR and STS tasks, and the\ndiscriminative cost can also be used in re-ranking $n$-best lists produced by\nNMT models, yielding strong improvements.", "published": "2017-02-03 18:19:47", "link": "http://arxiv.org/abs/1702.01101v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Predicting Target Language CCG Supertags Improves Neural Machine\n  Translation", "abstract": "Neural machine translation (NMT) models are able to partially learn syntactic\ninformation from sequential lexical information. Still, some complex syntactic\nphenomena such as prepositional phrase attachment are poorly modeled. This work\naims to answer two questions: 1) Does explicitly modeling target language\nsyntax help NMT? 2) Is tight integration of words and syntax better than\nmultitask training? We introduce syntactic information in the form of CCG\nsupertags in the decoder, by interleaving the target supertags with the word\nsequence. Our results on WMT data show that explicitly modeling target-syntax\nimproves machine translation quality for German->English, a high-resource pair,\nand for Romanian->English, a low-resource pair and also several syntactic\nphenomena including prepositional phrase attachment. Furthermore, a tight\ncoupling of words and syntax improves translation quality more than multitask\ntraining. By combining target-syntax with adding source-side dependency labels\nin the embedding layer, we obtain a total improvement of 0.9 BLEU for\nGerman->English and 1.2 BLEU for Romanian->English.", "published": "2017-02-03 20:31:34", "link": "http://arxiv.org/abs/1702.01147v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KU-ISPL Speaker Recognition Systems under Language mismatch condition\n  for NIST 2016 Speaker Recognition Evaluation", "abstract": "Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed\nspeaker recognition system for SRE16 fixed training condition. Data for\nevaluation trials are collected from outside North America, spoken in Tagalog\nand Cantonese while training data only is spoken English. Thus, main issue for\nSRE16 is compensating the discrepancy between different languages. As\ndevelopment dataset which is spoken in Cebuano and Mandarin, we could prepare\nthe evaluation trials through preliminary experiments to compensate the\nlanguage mismatched condition. Our team developed 4 different approaches to\nextract i-vectors and applied state-of-the-art techniques as backend. To\ncompensate language mismatch, we investigated and endeavored unique method such\nas unsupervised language clustering, inter language variability compensation\nand gender/language dependent score normalization.", "published": "2017-02-03 10:15:29", "link": "http://arxiv.org/abs/1702.00956v2", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Insights into Entity Name Evolution on Wikipedia", "abstract": "Working with Web archives raises a number of issues caused by their temporal\ncharacteristics. Depending on the age of the content, additional knowledge\nmight be needed to find and understand older texts. Especially facts about\nentities are subject to change. Most severe in terms of information retrieval\nare name changes. In order to find entities that have changed their name over\ntime, search engines need to be aware of this evolution. We tackle this problem\nby analyzing Wikipedia in terms of entity evolutions mentioned in articles\nregardless the structural elements. We gathered statistics and automatically\nextracted minimum excerpts covering name changes by incorporating lists\ndedicated to that subject. In future work, these excerpts are going to be used\nto discover patterns and detect changes in other sources. In this work we\ninvestigate whether or not Wikipedia is a suitable source for extracting the\nrequired knowledge.", "published": "2017-02-03 21:36:46", "link": "http://arxiv.org/abs/1702.01172v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Named Entity Evolution Analysis on Wikipedia", "abstract": "Accessing Web archives raises a number of issues caused by their temporal\ncharacteristics. Additional knowledge is needed to find and understand older\ntexts. Especially entities mentioned in texts are subject to change. Most\nsevere in terms of information retrieval are name changes. In order to find\nentities that have changed their name over time, search engines need to be\naware of this evolution. We tackle this problem by analyzing Wikipedia in terms\nof entity evolutions mentioned in articles. We present statistical data on\nexcerpts covering name changes, which will be used to discover similar text\npassages and extract evolution knowledge in future work.", "published": "2017-02-03 21:44:26", "link": "http://arxiv.org/abs/1702.01176v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Extraction of Evolution Descriptions from the Web", "abstract": "The evolution of named entities affects exploration and retrieval tasks in\ndigital libraries. An information retrieval system that is aware of name\nchanges can actively support users in finding former occurrences of evolved\nentities. However, current structured knowledge bases, such as DBpedia or\nFreebase, do not provide enough information about evolutions, even though the\ndata is available on their resources, like Wikipedia. Our \\emph{Evolution Base}\nprototype will demonstrate how excerpts describing name evolutions can be\nidentified on these websites with a promising precision. The descriptions are\nclassified by means of models that we trained based on a recent analysis of\nnamed entity evolutions on Wikipedia.", "published": "2017-02-03 21:48:18", "link": "http://arxiv.org/abs/1702.01179v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Named Entity Evolution Recognition on the Blogosphere", "abstract": "Advancements in technology and culture lead to changes in our language. These\nchanges create a gap between the language known by users and the language\nstored in digital archives. It affects user's possibility to firstly find\ncontent and secondly interpret that content. In previous work we introduced our\napproach for Named Entity Evolution Recognition~(NEER) in newspaper\ncollections. Lately, increasing efforts in Web preservation lead to increased\navailability of Web archives covering longer time spans. However, language on\nthe Web is more dynamic than in traditional media and many of the basic\nassumptions from the newspaper domain do not hold for Web data. In this paper\nwe discuss the limitations of existing methodology for NEER. We approach these\nby adapting an existing NEER method to work on noisy data like the Web and the\nBlogosphere in particular. We develop novel filters that reduce the noise and\nmake use of Semantic Web resources to obtain more information about terms. Our\nevaluation shows the potentials of the proposed approach.", "published": "2017-02-03 22:07:57", "link": "http://arxiv.org/abs/1702.01187v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Structured Attention Networks", "abstract": "Attention networks have proven to be an effective approach for embedding\ncategorical inference within a deep neural network. However, for many tasks we\nmay want to model richer structural dependencies without abandoning end-to-end\ntraining. In this work, we experiment with incorporating richer structural\ndistributions, encoded using graphical models, within deep networks. We show\nthat these structured attention networks are simple extensions of the basic\nattention procedure, and that they allow for extending attention beyond the\nstandard soft-selection approach, such as attending to partial segmentations or\nto subtrees. We experiment with two different classes of structured attention\nnetworks: a linear-chain conditional random field and a graph-based parsing\nmodel, and describe how these models can be practically implemented as neural\nnetwork layers. Experiments show that this approach is effective for\nincorporating structural biases, and structured attention networks outperform\nbaseline attention models on a variety of synthetic and real tasks: tree\ntransduction, neural machine translation, question answering, and natural\nlanguage inference. We further find that models trained in this way learn\ninteresting unsupervised hidden representations that generalize simple\nattention.", "published": "2017-02-03 01:40:45", "link": "http://arxiv.org/abs/1702.00887v3", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Multi-level computational methods for interdisciplinary research in the\n  HathiTrust Digital Library", "abstract": "We show how faceted search using a combination of traditional classification\nsystems and mixed-membership topic models can go beyond keyword search to\ninform resource discovery, hypothesis formulation, and argument extraction for\ninterdisciplinary research. Our test domain is the history and philosophy of\nscientific work on animal mind and cognition. The methods can be generalized to\nother research areas and ultimately support a system for semi-automatic\nidentification of argument structures. We provide a case study for the\napplication of the methods to the problem of identifying and extracting\narguments about anthropomorphism during a critical period in the development of\ncomparative psychology. We show how a combination of classification systems and\nmixed-membership models trained over large digital libraries can inform\nresource discovery in this domain. Through a novel approach of \"drill-down\"\ntopic modeling---simultaneously reducing both the size of the corpus and the\nunit of analysis---we are able to reduce a large collection of fulltext volumes\nto a much smaller set of pages within six focal volumes containing arguments of\ninterest to historians and philosophers of comparative psychology. The volumes\nidentified in this way did not appear among the first ten results of the\nkeyword search in the HathiTrust digital library and the pages bear the kind of\n\"close reading\" needed to generate original interpretations that is the heart\nof scholarly work in the humanities. Zooming back out, we provide a way to\nplace the books onto a map of science originally constructed from very\ndifferent data and for different purposes. The multilevel approach advances\nunderstanding of the intellectual and societal contexts in which writings are\ninterpreted.", "published": "2017-02-03 17:36:19", "link": "http://arxiv.org/abs/1702.01090v2", "categories": ["cs.DL", "cs.CL", "cs.IR"], "primary_category": "cs.DL"}
