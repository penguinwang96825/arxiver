{"title": "GNN-LM: Language Modeling based on Global Contexts via GNN", "abstract": "Inspired by the notion that ``{\\it to copy is easier than to memorize}``, in\nthis work, we introduce GNN-LM, which extends the vanilla neural language model\n(LM) by allowing to reference similar contexts in the entire training corpus.\nWe build a directed heterogeneous graph between an input context and its\nsemantically related neighbors selected from the training corpus, where nodes\nare tokens in the input context and retrieved neighbor contexts, and edges\nrepresent connections between nodes. Graph neural networks (GNNs) are\nconstructed upon the graph to aggregate information from similar contexts to\ndecode the token. This learning paradigm provides direct access to the\nreference contexts and helps improve a model's generalization ability. We\nconduct comprehensive experiments to validate the effectiveness of the GNN-LM:\nGNN-LM achieves a new state-of-the-art perplexity of 14.8 on WikiText-103 (a\n3.9 point improvement over its counterpart of the vanilla LM model), and shows\nsubstantial improvement on One Billion Word and Enwiki8 datasets against strong\nbaselines. In-depth ablation studies are performed to understand the mechanics\nof GNN-LM. \\footnote{The code can be found at\nhttps://github.com/ShannonAI/GNN-LM", "published": "2021-10-17 07:18:21", "link": "http://arxiv.org/abs/2110.08743v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Opinion Summarization with Minimal Supervision", "abstract": "Opinion summarization aims to profile a target by extracting opinions from\nmultiple documents. Most existing work approaches the task in a semi-supervised\nmanner due to the difficulty of obtaining high-quality annotation from\nthousands of documents. Among them, some use aspect and sentiment analysis as a\nproxy for identifying opinions. In this work, we propose a new framework,\nFineSum, which advances this frontier in three aspects: (1) minimal\nsupervision, where only aspect names and a few aspect/sentiment keywords are\navailable; (2) fine-grained opinion analysis, where sentiment analysis drills\ndown to the sub-aspect level; and (3) phrase-based summarization, where opinion\nis summarized in the form of phrases. FineSum automatically identifies opinion\nphrases from the raw corpus, classifies them into different aspects and\nsentiments, and constructs multiple fine-grained opinion clusters under each\naspect/sentiment. Each cluster consists of semantically coherent phrases,\nexpressing uniform opinions towards certain sub-aspect or characteristics\n(e.g., positive feelings for ``burgers'' in the ``food'' aspect). An\nopinion-oriented spherical word embedding space is trained to provide weak\nsupervision for the phrase classifier, and phrase clustering is performed using\nthe aspect-aware contextualized embedding generated from the phrase classifier.\nBoth automatic evaluation on the benchmark and quantitative human evaluation\nvalidate the effectiveness of our approach.", "published": "2021-10-17 15:16:34", "link": "http://arxiv.org/abs/2110.08845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Schr\u00f6dinger's Tree -- On Syntax and Neural Language Models", "abstract": "In the last half-decade, the field of natural language processing (NLP) has\nundergone two major transitions: the switch to neural networks as the primary\nmodeling paradigm and the homogenization of the training regime (pre-train,\nthen fine-tune). Amidst this process, language models have emerged as NLP's\nworkhorse, displaying increasingly fluent generation capabilities and proving\nto be an indispensable means of knowledge transfer downstream. Due to the\notherwise opaque, black-box nature of such models, researchers have employed\naspects of linguistic theory in order to characterize their behavior. Questions\ncentral to syntax -- the study of the hierarchical structure of language --\nhave factored heavily into such work, shedding invaluable insights about\nmodels' inherent biases and their ability to make human-like generalizations.\nIn this paper, we attempt to take stock of this growing body of literature. In\ndoing so, we observe a lack of clarity across numerous dimensions, which\ninfluences the hypotheses that researchers form, as well as the conclusions\nthey draw from their findings. To remedy this, we urge researchers make careful\nconsiderations when investigating coding properties, selecting representations,\nand evaluating via downstream tasks. Furthermore, we outline the implications\nof the different types of research questions exhibited in studies on syntax, as\nwell as the inherent pitfalls of aggregate metrics. Ultimately, we hope that\nour discussion adds nuance to the prospect of studying language models and\npaves the way for a less monolithic perspective on syntax in this context.", "published": "2021-10-17 18:25:23", "link": "http://arxiv.org/abs/2110.08887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying the Task-Specific Information in Text-Based Classifications", "abstract": "Recently, neural natural language models have attained state-of-the-art\nperformance on a wide variety of tasks, but the high performance can result\nfrom superficial, surface-level cues (Bender and Koller, 2020; Niven and Kao,\n2020). These surface cues, as the ``shortcuts'' inherent in the datasets, do\nnot contribute to the *task-specific information* (TSI) of the classification\ntasks. While it is essential to look at the model performance, it is also\nimportant to understand the datasets. In this paper, we consider this question:\nApart from the information introduced by the shortcut features, how much\ntask-specific information is required to classify a dataset? We formulate this\nquantity in an information-theoretic framework. While this quantity is hard to\ncompute, we approximate it with a fast and stable method. TSI quantifies the\namount of linguistic knowledge modulo a set of predefined shortcuts -- that\ncontributes to classifying a sample from each dataset. This framework allows us\nto compare across datasets, saying that, apart from a set of ``shortcut\nfeatures'', classifying each sample in the Multi-NLI task involves around 0.4\nnats more TSI than in the Quora Question Pair.", "published": "2021-10-17 21:54:38", "link": "http://arxiv.org/abs/2110.08931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Predicting the Performance of Multilingual NLP Models", "abstract": "Recent advancements in NLP have given us models like mBERT and XLMR that can\nserve over 100 languages. The languages that these models are evaluated on,\nhowever, are very few in number, and it is unlikely that evaluation datasets\nwill cover all the languages that these models support. Potential solutions to\nthe costly problem of dataset creation are to translate datasets to new\nlanguages or use template-filling based techniques for creation. This paper\nproposes an alternate solution for evaluating a model across languages which\nmake use of the existing performance scores of the model on languages that a\nparticular task has test sets for. We train a predictor on these performance\nscores and use this predictor to predict the model's performance in different\nevaluation settings. Our results show that our method is effective in filling\nthe gaps in the evaluation for an existing set of languages, but might require\nadditional improvements if we want it to generalize to unseen languages.", "published": "2021-10-17 17:36:53", "link": "http://arxiv.org/abs/2110.08875v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Awesome Korean NLP Datasets", "abstract": "English based datasets are commonly available from Kaggle, GitHub, or\nrecently published papers. Although benchmark tests with English datasets are\nsufficient to show off the performances of new models and methods, still a\nresearcher need to train and validate the models on Korean based datasets to\nproduce a technology or product, suitable for Korean processing. This paper\nintroduces 15 popular Korean based NLP datasets with summarized details such as\nvolume, license, repositories, and other research results inspired by the\ndatasets. Also, I provide high-resolution instructions with sample or\nstatistics of datasets. The main characteristics of datasets are presented on a\nsingle table to provide a rapid summarization of datasets for researchers.", "published": "2021-10-17 03:24:05", "link": "http://arxiv.org/abs/2112.01624v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reminding the Incremental Language Model via Data-Free Self-Distillation", "abstract": "Incremental language learning with pseudo-data can alleviate catastrophic\nforgetting in neural networks. However, to obtain better performance, former\nmethods have higher demands for pseudo-data of the previous tasks. The\nperformance dramatically decreases when fewer pseudo-data are employed. In\naddition, the distribution of pseudo-data gradually deviates from the real data\nwith the sequential learning of different tasks. The deviation will be greater\nwith more tasks learned, which results in more serious catastrophic forgetting.\nTo address these issues, we propose reminding incremental language model via\ndata-free self-distillation (DFSD), which includes self-distillation based on\nthe Earth Mover's Distance and hidden data augmentation. By estimating the\nknowledge distribution in all layers of GPT-2 and transforming it from teacher\nmodel to student model, the Self-distillation based on the Earth Mover's\nDistance can significantly reduce the demand for pseudo-data. Hidden data\naugmentation can greatly alleviate the catastrophic forgetting caused by\ndeviations via modeling the generation of pseudo-data as a hidden data\naugmentation process, where each sample is a mixture of all trained task data.\nThe experimental results demonstrate that our DFSD can exceed the previous\nstate-of-the-art methods even if the maximum decrease in pseudo-data is 90%.", "published": "2021-10-17 07:27:43", "link": "http://arxiv.org/abs/2110.08745v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Prioritization of COVID-19-related literature via unsupervised keyphrase\n  extraction and document representation learning", "abstract": "The COVID-19 pandemic triggered a wave of novel scientific literature that is\nimpossible to inspect and study in a reasonable time frame manually. Current\nmachine learning methods offer to project such body of literature into the\nvector space, where similar documents are located close to each other, offering\nan insightful exploration of scientific papers and other knowledge sources\nassociated with COVID-19. However, to start searching, such texts need to be\nappropriately annotated, which is seldom the case due to the lack of human\nresources. In our system, the current body of COVID-19-related literature is\nannotated using unsupervised keyphrase extraction, facilitating the initial\nqueries to the latent space containing the learned document embeddings\n(low-dimensional representations). The solution is accessible through a web\nserver capable of interactive search, term ranking, and exploration of\npotentially interesting literature. We demonstrate the usefulness of the\napproach via case studies from the medicinal chemistry domain.", "published": "2021-10-17 17:35:09", "link": "http://arxiv.org/abs/2110.08874v1", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "DECAR: Deep Clustering for learning general-purpose Audio\n  Representations", "abstract": "We introduce DECAR, a self-supervised pre-training approach for learning\ngeneral-purpose audio representations. Our system is based on clustering: it\nutilizes an offline clustering step to provide target labels that act as\npseudo-labels for solving a prediction task. We develop on top of recent\nadvances in self-supervised learning for computer vision and design a\nlightweight, easy-to-use self-supervised pre-training scheme. We pre-train\nDECAR embeddings on a balanced subset of the large-scale Audioset dataset and\ntransfer those representations to 9 downstream classification tasks, including\nspeech, music, animal sounds, and acoustic scenes. Furthermore, we conduct\nablation studies identifying key design choices and also make all our code and\npre-trained models publicly available.", "published": "2021-10-17 19:03:51", "link": "http://arxiv.org/abs/2110.08895v4", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VISinger: Variational Inference with Adversarial Learning for End-to-End\n  Singing Voice Synthesis", "abstract": "In this paper, we propose VISinger, a complete end-to-end high-quality\nsinging voice synthesis (SVS) system that directly generates audio waveform\nfrom lyrics and musical score. Our approach is inspired by VITS, which adopts\nVAE-based posterior encoder augmented with normalizing flow-based prior encoder\nand adversarial decoder to realize complete end-to-end speech generation.\nVISinger follows the main architecture of VITS, but makes substantial\nimprovements to the prior encoder based on the characteristics of singing.\nFirst, instead of using phoneme-level mean and variance of acoustic features,\nwe introduce a length regulator and a frame prior network to get the\nframe-level mean and variance on acoustic features, modeling the rich acoustic\nvariation in singing. Second, we further introduce an F0 predictor to guide the\nframe prior network, leading to stabler singing performance. Finally, to\nimprove the singing rhythm, we modify the duration predictor to specifically\npredict the phoneme to note duration ratio, helped with singing note\nnormalization. Experiments on a professional Mandarin singing corpus show that\nVISinger significantly outperforms FastSpeech+Neural-Vocoder two-stage approach\nand the oracle VITS; ablation study demonstrates the effectiveness of different\ncontributions.", "published": "2021-10-17 12:51:52", "link": "http://arxiv.org/abs/2110.08813v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving End-To-End Modeling for Mispronunciation Detection with\n  Effective Augmentation Mechanisms", "abstract": "Recently, end-to-end (E2E) models, which allow to take spectral vector\nsequences of L2 (second-language) learners' utterances as input and produce the\ncorresponding phone-level sequences as output, have attracted much research\nattention in developing mispronunciation detection (MD) systems. However, due\nto the lack of sufficient labeled speech data of L2 speakers for model\nestimation, E2E MD models are prone to overfitting in relation to conventional\nones that are built on DNN-HMM acoustic models. To alleviate this critical\nissue, we in this paper propose two modeling strategies to enhance the\ndiscrimination capability of E2E MD models, each of which can implicitly\nleverage the phonetic and phonological traits encoded in a pretrained acoustic\nmodel and contained within reference transcripts of the training data,\nrespectively. The first one is input augmentation, which aims to distill\nknowledge about phonetic discrimination from a DNN-HMM acoustic model. The\nsecond one is label augmentation, which manages to capture more phonological\npatterns from the transcripts of training data. A series of empirical\nexperiments conducted on the L2-ARCTIC English dataset seem to confirm the\nefficacy of our E2E MD model when compared to some top-of-the-line E2E MD\nmodels and a classic pronunciation-scoring based method built on a DNN-HMM\nacoustic model.", "published": "2021-10-17 06:11:15", "link": "http://arxiv.org/abs/2110.08731v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Storage and Authentication of Audio Footage for IoAuT Devices Using\n  Distributed Ledger Technology", "abstract": "Detection of fabricated or manipulated audio content to prevent, e.g.,\ndistribution of forgeries in digital media, is crucial, especially in political\nand reputational contexts. Better tools for protecting the integrity of media\ncreation are desired. Within the paradigm of the Internet of Audio\nThings(IoAuT), we discuss the ability of the IoAuT network to verify the\nauthenticity of original audio using distributed ledger technology. By storing\naudio recordings in combination with associated recording-specific metadata\nobtained by the IoAuT capturing device, this architecture enables secure\ndistribution of original audio footage, authentication of unknown audio\ncontent, and referencing of original audio material in future derivative works.\nBy developing a proof-of-concept system, the feasibility of the proposed\narchitecture is evaluated and discussed.", "published": "2021-10-17 13:47:10", "link": "http://arxiv.org/abs/2110.08821v1", "categories": ["cs.SD", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning Based EDM Subgenre Classification using Mel-Spectrogram\n  and Tempogram Features", "abstract": "Along with the evolution of music technology, a large number of styles, or\n\"subgenres,\" of Electronic Dance Music(EDM) have emerged in recent years. While\nthe classification task of distinguishing between EDM and non-EDM has been\noften studied in the context of music genre classification, little work has\nbeen done on the more challenging EDM subgenre classification. The state-of-art\nmodel is based on extremely randomized trees and could be improved by deep\nlearning methods. In this paper, we extend the state-of-art music auto-tagging\nmodel \"short-chunkCNN+Resnet\" to EDM subgenre classification, with the addition\nof two mid-level tempo-related feature representations, called the Fourier\ntempogram and autocorrelation tempogram. And, we explore two fusion strategies,\nearly fusion and late fusion, to aggregate the two types of tempograms. We\nevaluate the proposed models using a large dataset consisting of 75,000 songs\nfor 30 different EDM subgenres, and show that the adoption of deep learning\nmodels and tempo features indeed leads to higher classification accuracy.", "published": "2021-10-17 16:25:33", "link": "http://arxiv.org/abs/2110.08862v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Taming Visually Guided Sound Generation", "abstract": "Recent advances in visually-induced audio generation are based on sampling\nshort, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio\nfrom the state-of-the-art model takes minutes on a high-end GPU. In this work,\nwe propose a single model capable of generating visually relevant,\nhigh-fidelity sounds prompted with a set of frames from open-domain videos in\nless time than it takes to play it on a single GPU.\n  We train a transformer to sample a new spectrogram from the pre-trained\nspectrogram codebook given the set of video features. The codebook is obtained\nusing a variant of VQGAN trained to produce a compact sampling space with a\nnovel spectrogram-based perceptual loss. The generated spectrogram is\ntransformed into a waveform using a window-based GAN that significantly speeds\nup generation. Considering the lack of metrics for automatic evaluation of\ngenerated spectrograms, we also build a family of metrics called FID and MKL.\nThese metrics are based on a novel sound classifier, called Melception, and\ndesigned to evaluate the fidelity and relevance of open-domain samples.\n  Both qualitative and quantitative studies are conducted on small- and\nlarge-scale datasets to evaluate the fidelity and relevance of generated\nsamples. We also compare our model to the state-of-the-art and observe a\nsubstantial improvement in quality, size, and computation time. Code, demo, and\nsamples: v-iashin.github.io/SpecVQGAN", "published": "2021-10-17 11:14:00", "link": "http://arxiv.org/abs/2110.08791v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
