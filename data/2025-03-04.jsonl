{"title": "Improving LLM-as-a-Judge Inference with the Judgment Distribution", "abstract": "Using language models to scalably approximate human preferences on text\nquality (LLM-as-a-judge) has become a standard practice applicable to many\ntasks. A judgment is often extracted from the judge's textual output alone,\ntypically with greedy decoding. However, LLM judges naturally provide\ndistributions over judgment tokens, inviting a breadth of inference methods for\nextracting fine-grained preferences. We find that taking the mean of the\njudgment distribution consistently outperforms taking the mode (i.e. greedy\ndecoding) in all evaluation settings (i.e. pointwise, pairwise, and listwise).\nWe further explore novel methods of deriving preferences from judgment\ndistributions, and find that methods incorporating risk aversion often improve\nperformance. Lastly, we analyze LLM-as-a-judge paired with chain-of-thought\n(CoT) prompting, showing that CoT can collapse the spread of the judgment\ndistribution, often harming performance. Our findings suggest leveraging\ndistributional output can improve LLM-as-a-judge, as opposed to using the text\ninterface alone.", "published": "2025-03-04 23:59:08", "link": "http://arxiv.org/abs/2503.03064v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semi-Supervised In-Context Learning: A Baseline Study", "abstract": "Most existing work in data selection for In-Context Learning (ICL) has\nfocused on constructing demonstrations from ground truth annotations, with\nlimited attention given to selecting reliable self-generated annotations. In\nthis work, we propose a three-step semi-supervised ICL framework: annotation\ngeneration, demonstration selection, and semi-supervised inference. Our\nbaseline, Naive-SemiICL, which prompts select high-confidence self-generated\ndemonstrations for ICL prompting, outperforms a 16-shot baseline by an average\nof 9.94% across 16 datasets. We further introduce IterPSD, an annotation\napproach that refines pseudo-demonstrations iteratively, achieving up to 6.8%\nadditional gains in classification tasks. Lastly, we reveal a scaling law for\nsemi-supervised ICL, where models achieve optimal performance with over 1,000\ndemonstrations.", "published": "2025-03-04 23:52:49", "link": "http://arxiv.org/abs/2503.03062v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "QE4PE: Word-level Quality Estimation for Human Post-Editing", "abstract": "Word-level quality estimation (QE) detects erroneous spans in machine\ntranslations, which can direct and facilitate human post-editing. While the\naccuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. Our QE4PE study investigates the impact\nof word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated by behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows.", "published": "2025-03-04 22:50:17", "link": "http://arxiv.org/abs/2503.03044v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "SAGE: Steering and Refining Dialog Generation with State-Action Augmentation", "abstract": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel.", "published": "2025-03-04 22:45:24", "link": "http://arxiv.org/abs/2503.03040v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SAFE: A Sparse Autoencoder-Based Framework for Robust Query Enrichment and Hallucination Mitigation in LLMs", "abstract": "Despite the state-of-the-art performance of Large Language Models (LLMs),\nthese models often suffer from hallucinations, which can undermine their\nperformance in critical applications. In this work, we propose SAFE, a novel\nmethod for detecting and mitigating hallucinations by leveraging Sparse\nAutoencoders (SAEs). While hallucination detection techniques and SAEs have\nbeen explored independently, their synergistic application in a comprehensive\nsystem, particularly for hallucination-aware query enrichment, has not been\nfully investigated. To validate the effectiveness of SAFE, we evaluate it on\ntwo models with available SAEs across three diverse cross-domain datasets\ndesigned to assess hallucination problems. Empirical results demonstrate that\nSAFE consistently improves query generation accuracy and mitigates\nhallucinations across all datasets, achieving accuracy improvements of up to\n29.45%.", "published": "2025-03-04 22:19:52", "link": "http://arxiv.org/abs/2503.03032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting Science Report 1: Prompt Engineering is Complicated and Contingent", "abstract": "This is the first of a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we demonstrate two things:\n  - There is no single standard for measuring whether a Large Language Model\n(LLM) passes a benchmark, and that choosing a standard has a big impact on how\nwell the LLM does on that benchmark. The standard you choose will depend on\nyour goals for using an LLM in a particular case.\n  - It is hard to know in advance whether a particular prompting approach will\nhelp or harm the LLM's ability to answer any particular question. Specifically,\nwe find that sometimes being polite to the LLM helps performance, and sometimes\nit lowers performance. We also find that constraining the AI's answers helps\nperformance in some cases, though it may lower performance in other cases.\n  Taken together, this suggests that benchmarking AI performance is not\none-size-fits-all, and also that particular prompting formulas or approaches,\nlike being polite to the AI, are not universally valuable.", "published": "2025-03-04 21:09:12", "link": "http://arxiv.org/abs/2503.04818v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "One Model to Train them All: Hierarchical Self-Distillation for Enhanced Early Layer Embeddings", "abstract": "Deploying language models often requires handling model size vs. performance\ntrade-offs to satisfy downstream latency constraints while preserving the\nmodel's usefulness. Model distillation is commonly employed to reduce model\nsize while maintaining acceptable performance. However, distillation can be\ninefficient since it involves multiple training steps. In this work, we\nintroduce MODULARSTARENCODER, a modular multi-exit encoder with 1B parameters,\nuseful for multiple tasks within the scope of code retrieval.\nMODULARSTARENCODER is trained with a novel self-distillation mechanism that\nsignificantly improves lower-layer representations-allowing different portions\nof the model to be used while still maintaining a good trade-off in terms of\nperformance. Our architecture focuses on enhancing text-to-code and\ncode-to-code search by systematically capturing syntactic and semantic\nstructures across multiple levels of representation. Specific encoder layers\nare targeted as exit heads, allowing higher layers to guide earlier layers\nduring training. This self-distillation effect improves intermediate\nrepresentations, increasing retrieval recall at no extra training cost. In\naddition to the multi-exit scheme, our approach integrates a repository-level\ncontextual loss that maximally utilizes the training context window, further\nenhancing the learned representations. We also release a new dataset\nconstructed via code translation, seamlessly expanding traditional text-to-code\nbenchmarks with code-to-code pairs across diverse programming languages.\nExperimental results highlight the benefits of self-distillation through\nmulti-exit supervision.", "published": "2025-03-04 21:08:17", "link": "http://arxiv.org/abs/2503.03008v1", "categories": ["cs.CL", "cs.AI", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Will I Get Hate Speech Predicting the Volume of Abusive Replies before Posting in Social Media", "abstract": "Despite the growing body of research tackling offensive language in social\nmedia, this research is predominantly reactive, determining if content already\nposted in social media is abusive. There is a gap in predictive approaches,\nwhich we address in our study by enabling to predict the volume of abusive\nreplies a tweet will receive after being posted. We formulate the problem from\nthe perspective of a social media user asking: ``if I post a certain message on\nsocial media, is it possible to predict the volume of abusive replies it might\nreceive?'' We look at four types of features, namely text, text metadata, tweet\nmetadata, and account features, which also help us understand the extent to\nwhich the user or the content helps predict the number of abusive replies.\nThis, in turn, helps us develop a model to support social media users in\nfinding the best way to post content. One of our objectives is also to\ndetermine the extent to which the volume of abusive replies that a tweet will\nget are motivated by the content of the tweet or by the identity of the user\nposting it. Our study finds that one can build a model that performs\ncompetitively by developing a comprehensive set of features derived from the\ncontent of the message that is going to be posted. In addition, our study\nsuggests that features derived from the user's identity do not impact model\nperformance, hence suggesting that it is especially the content of a post that\ntriggers abusive replies rather than who the user is.", "published": "2025-03-04 21:04:21", "link": "http://arxiv.org/abs/2503.03005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Multi-Label Classification of Bangla Documents: Large Decoders Vs. Classic Encoders", "abstract": "Bangla, a language spoken by over 300 million native speakers and ranked as\nthe sixth most spoken language worldwide, presents unique challenges in natural\nlanguage processing (NLP) due to its complex morphological characteristics and\nlimited resources. While recent Large Decoder Based models (LLMs), such as GPT,\nLLaMA, and DeepSeek, have demonstrated excellent performance across many NLP\ntasks, their effectiveness in Bangla remains largely unexplored. In this paper,\nwe establish the first benchmark comparing decoder-based LLMs with classic\nencoder-based models for Zero-Shot Multi-Label Classification (Zero-Shot-MLC)\ntask in Bangla. Our evaluation of 32 state-of-the-art models reveals that,\nexisting so-called powerful encoders and decoders still struggle to achieve\nhigh accuracy on the Bangla Zero-Shot-MLC task, suggesting a need for more\nresearch and resources for Bangla NLP.", "published": "2025-03-04 20:39:07", "link": "http://arxiv.org/abs/2503.02993v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Effectively Steer LLM To Follow Preference via Building Confident Directions", "abstract": "Having an LLM that aligns with human preferences is essential for\naccommodating individual needs, such as maintaining writing style or generating\nspecific topics of interest. The majority of current alignment methods rely on\nfine-tuning or prompting, which can be either costly or difficult to control.\nModel steering algorithms, which modify the model output by constructing\nspecific steering directions, are typically easy to implement and\noptimization-free. However, their capabilities are typically limited to\nsteering the model into one of the two directions (i.e., bidirectional\nsteering), and there has been no theoretical understanding to guarantee their\nperformance. In this work, we propose a theoretical framework to understand and\nquantify the model steering methods. Inspired by the framework, we propose a\nconfident direction steering method (CONFST) that steers LLMs via modifying\ntheir activations at inference time. More specifically, CONFST builds a\nconfident direction that is closely aligned with users' preferences, and this\ndirection is then added to the activations of the LLMs to effectively steer the\nmodel output. Our approach offers three key advantages over popular\nbidirectional model steering methods: 1) It is more powerful, since multiple\n(i.e. more than two) users' preferences can be aligned simultaneously; 2) It is\nsimple to implement, since there is no need to determine which layer to add the\nsteering vector to; 3) No explicit user instruction is required. We validate\nour method on GPT-2 XL (1.5B), Mistral (7B) and Gemma-it (9B) models for tasks\nthat require shifting the output of LLMs across various topics and styles,\nachieving superior performance over competing methods.", "published": "2025-03-04 20:32:27", "link": "http://arxiv.org/abs/2503.02989v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Agent System for AI-Assisted Extraction of Narrative Arcs in TV Series", "abstract": "Serialized TV shows are built on complex storylines that can be hard to track\nand evolve in ways that defy straightforward analysis. This paper introduces a\nmulti-agent system designed to extract and analyze these narrative arcs. Tested\non the first season of Grey's Anatomy (ABC 2005-), the system identifies three\ntypes of arcs: Anthology (self-contained), Soap (relationship-focused), and\nGenre-Specific (strictly related to the series' genre). Episodic progressions\nof these arcs are stored in both relational and semantic (vectorial) databases,\nenabling structured analysis and comparison. To bridge the gap between\nautomation and critical interpretation, the system is paired with a graphical\ninterface that allows for human refinement using tools to enhance and visualize\nthe data. The system performed strongly in identifying Anthology Arcs and\ncharacter entities, but its reliance on textual paratexts (such as episode\nsummaries) revealed limitations in recognizing overlapping arcs and subtler\ndynamics. This approach highlights the potential of combining computational and\nhuman expertise in narrative analysis. Beyond television, it offers promise for\nserialized written formats, where the narrative resides entirely in the text.\nFuture work will explore the integration of multimodal inputs, such as dialogue\nand visuals, and expand testing across a wider range of genres to refine the\nsystem further.", "published": "2025-03-04 20:27:14", "link": "http://arxiv.org/abs/2503.04817v1", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.MM"], "primary_category": "cs.CL"}
{"title": "LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic Templatisation and Orthographic Obfuscation", "abstract": "Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.", "published": "2025-03-04 19:57:47", "link": "http://arxiv.org/abs/2503.02972v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingual Relative Clause Attachment Ambiguity Resolution in Large Language Models", "abstract": "This study examines how large language models (LLMs) resolve relative clause\n(RC) attachment ambiguities and compares their performance to human sentence\nprocessing. Focusing on two linguistic factors, namely the length of RCs and\nthe syntactic position of complex determiner phrases (DPs), we assess whether\nLLMs can achieve human-like interpretations amid the complexities of language.\nIn this study, we evaluated several LLMs, including Claude, Gemini and Llama,\nin multiple languages: English, Spanish, French, German, Japanese, and Korean.\nWhile these models performed well in Indo-European languages (English, Spanish,\nFrench, and German), they encountered difficulties in Asian languages (Japanese\nand Korean), often defaulting to incorrect English translations. The findings\nunderscore the variability in LLMs' handling of linguistic ambiguities and\nhighlight the need for model improvements, particularly for non-European\nlanguages. This research informs future enhancements in LLM design to improve\naccuracy and human-like processing in diverse linguistic environments.", "published": "2025-03-04 19:56:56", "link": "http://arxiv.org/abs/2503.02971v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model", "abstract": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST", "published": "2025-03-04 19:51:29", "link": "http://arxiv.org/abs/2503.02969v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OWLViz: An Open-World Benchmark for Visual Question Answering", "abstract": "We present a challenging benchmark for the Open WorLd VISual question\nanswering (OWLViz) task. OWLViz presents concise, unambiguous queries that\nrequire integrating multiple capabilities, including visual understanding, web\nexploration, and specialized tool usage. While humans achieve 69.2% accuracy on\nthese intuitive tasks, even state-of-the-art VLMs struggle, with the best\nmodel, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which\nrely on limited vision and vision-language models as tools, perform even worse.\nThis performance gap reveals significant limitations in multimodal systems'\nability to select appropriate tools and execute complex reasoning sequences,\nestablishing new directions for advancing practical AI research.", "published": "2025-03-04 19:37:33", "link": "http://arxiv.org/abs/2503.07631v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding", "abstract": "We introduce KodCode, a synthetic dataset that addresses the persistent\nchallenge of acquiring high-quality, verifiable training data across diverse\ndifficulties and domains for training Large Language Models for coding.\nExisting code-focused resources typically fail to ensure either the breadth of\ncoverage (e.g., spanning simple coding tasks to advanced algorithmic problems)\nor verifiable correctness (e.g., unit tests). In contrast, KodCode comprises\nquestion-solution-test triplets that are systematically validated via a\nself-verification procedure. Our pipeline begins by synthesizing a broad range\nof coding questions, then generates solutions and test cases with additional\nattempts allocated to challenging problems. Finally, post-training data\nsynthesis is done by rewriting questions into diverse formats and generating\nresponses under a test-based reject sampling procedure from a reasoning model\n(DeepSeek R1). This pipeline yields a large-scale, robust and diverse coding\ndataset. KodCode is suitable for supervised fine-tuning and the paired unit\ntests also provide great potential for RL tuning. Fine-tuning experiments on\ncoding benchmarks (HumanEval(+), MBPP(+), BigCodeBench, and LiveCodeBench)\ndemonstrate that KodCode-tuned models achieve state-of-the-art performance,\nsurpassing models like Qwen2.5-Coder-32B-Instruct and\nDeepSeek-R1-Distill-Llama-70B.", "published": "2025-03-04 19:17:36", "link": "http://arxiv.org/abs/2503.02951v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications", "abstract": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.", "published": "2025-03-04 19:13:10", "link": "http://arxiv.org/abs/2503.02950v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "ExpertGenQA: Open-ended QA generation in Specialized Domains", "abstract": "Generating high-quality question-answer pairs for specialized technical\ndomains remains challenging, with existing approaches facing a tradeoff between\nleveraging expert examples and achieving topical diversity. We present\nExpertGenQA, a protocol that combines few-shot learning with structured topic\nand style categorization to generate comprehensive domain-specific QA pairs.\nUsing U.S. Federal Railroad Administration documents as a test bed, we\ndemonstrate that ExpertGenQA achieves twice the efficiency of baseline few-shot\napproaches while maintaining $94.4\\%$ topic coverage. Through systematic\nevaluation, we show that current LLM-based judges and reward models exhibit\nstrong bias toward superficial writing styles rather than content quality. Our\nanalysis using Bloom's Taxonomy reveals that ExpertGenQA better preserves the\ncognitive complexity distribution of expert-written questions compared to\ntemplate-based approaches. When used to train retrieval models, our generated\nqueries improve top-1 accuracy by $13.02\\%$ over baseline performance,\ndemonstrating their effectiveness for downstream applications in technical\ndomains.", "published": "2025-03-04 19:09:48", "link": "http://arxiv.org/abs/2503.02948v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "FourierNAT: A Fourier-Mixing-Based Non-Autoregressive Transformer for Parallel Sequence Generation", "abstract": "We present FourierNAT, a novel non-autoregressive Transformer (NAT)\narchitecture that employs Fourier-based mixing in the decoder to generate\noutput sequences in parallel. While traditional NAT approaches often face\nchallenges with capturing global dependencies, our method leverages a discrete\nFourier transform to mix token embeddings across the entire sequence dimension,\ncoupled with learned frequency-domain gating. This allows the model to\nefficiently propagate context without explicit autoregressive steps.\nEmpirically, FourierNAT achieves competitive results against leading NAT\nbaselines on standard benchmarks like WMT machine translation and CNN/DailyMail\nsummarization, providing significant speed advantages over autoregressive\nTransformers. We further demonstrate that learned frequency-domain parameters\nallow the model to adaptively focus on long-range or short-range dependencies,\npartially mitigating the well-known coherence gaps in one-pass NAT generation.\nOverall, FourierNAT highlights the potential of integrating spectral-domain\noperations to accelerate and improve parallel text generation. This approach\ncan potentially provide great computational and time savings in inference tasks\nLLMs.", "published": "2025-03-04 19:08:39", "link": "http://arxiv.org/abs/2503.07630v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Wikipedia in the Era of LLMs: Evolution and Risks", "abstract": "In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.", "published": "2025-03-04 18:58:13", "link": "http://arxiv.org/abs/2503.02879v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models can Self-Improve at State-Value Estimation for Better Search", "abstract": "Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.", "published": "2025-03-04 18:58:11", "link": "http://arxiv.org/abs/2503.02878v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models", "abstract": "Improving the reasoning capabilities of large language models (LLMs)\ntypically requires supervised fine-tuning with labeled data or computationally\nexpensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which\nleverages the observation of Prefix Self-Consistency -- the shared initial\nreasoning steps across diverse solution trajectories -- to enhance LLM\nreasoning efficiency. By training exclusively on the initial prefix substrings\n(as few as 8 tokens), UPFT removes the need for labeled data or exhaustive\nsampling. Experiments on reasoning benchmarks show that UPFT matches the\nperformance of supervised methods such as Rejection Sampling Fine-Tuning, while\nreducing training time by 75% and sampling cost by 99%. Further analysis\nreveals that errors tend to appear in later stages of the reasoning process and\nthat prefix-based training preserves the model's structural knowledge. This\nwork demonstrates how minimal unsupervised fine-tuning can unlock substantial\nreasoning gains in LLMs, offering a scalable and resource-efficient alternative\nto conventional approaches.", "published": "2025-03-04 18:56:03", "link": "http://arxiv.org/abs/2503.02875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FairSense-AI: Responsible AI Meets Sustainability", "abstract": "In this paper, we introduce FairSense-AI: a multimodal framework designed to\ndetect and mitigate bias in both text and images. By leveraging Large Language\nModels (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle\nforms of prejudice or stereotyping that can appear in content, providing users\nwith bias scores, explanatory highlights, and automated recommendations for\nfairness enhancements. In addition, FairSense-AI integrates an AI risk\nassessment component that aligns with frameworks like the MIT AI Risk\nRepository and NIST AI Risk Management Framework, enabling structured\nidentification of ethical and safety concerns. The platform is optimized for\nenergy efficiency via techniques such as model pruning and mixed-precision\ncomputation, thereby reducing its environmental footprint. Through a series of\ncase studies and applications, we demonstrate how FairSense-AI promotes\nresponsible AI use by addressing both the social dimension of fairness and the\npressing need for sustainability in large-scale AI deployments.\nhttps://vectorinstitute.github.io/FairSense-AI,\nhttps://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,\nLarge Language Models , Vision Language Models , Ethical AI , Green AI)", "published": "2025-03-04 18:43:57", "link": "http://arxiv.org/abs/2503.02865v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt Aggregation Framework", "abstract": "Large Language Models (LLMs) often exhibit misaligned confidence scores,\nusually overestimating the reliability of their predictions. While verbalized\nconfidence in Large Language Models (LLMs) has gained attention, prior work\nremains divided on whether confidence scores can be systematically steered\nthrough prompting. Recent studies even argue that such prompt-induced\nconfidence shifts are negligible, suggesting LLMs' confidence calibration is\nrigid to linguistic interventions. Contrary to these claims, we first\nrigorously confirm the existence of directional confidence shifts by probing\nthree models (including GPT3.5, LLAMA3-70b, GPT4) across 7 benchmarks,\ndemonstrating that explicit instructions can inflate or deflate confidence\nscores in a regulated manner. Based on this observation, we propose a novel\nframework containing three components: confidence steering, steered confidence\naggregation and steered answers selection, named SteeringConf. Our method,\nSteeringConf, leverages a confidence manipulation mechanism to steer the\nconfidence scores of LLMs in several desired directions, followed by a\nsummarization module that aggregates the steered confidence scores to produce a\nfinal prediction. We evaluate our method on 7 benchmarks and it consistently\noutperforms the baselines in terms of calibration metrics in task of confidence\ncalibration and failure detection.", "published": "2025-03-04 18:40:49", "link": "http://arxiv.org/abs/2503.02863v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "(How) Do Language Models Track State?", "abstract": "Transformer language models (LMs) exhibit behaviors -- from storytelling to\ncode generation -- that appear to require tracking the unobserved state of an\nevolving world. How do they do so? We study state tracking in LMs trained or\nfine-tuned to compose permutations (i.e., to compute the order of a set of\nobjects after a sequence of swaps). Despite the simple algebraic structure of\nthis problem, many other tasks (e.g., simulation of finite automata and\nevaluation of boolean expressions) can be reduced to permutation composition,\nmaking it a natural model for state tracking in general. We show that LMs\nconsistently learn one of two state tracking mechanisms for this task. The\nfirst closely resembles the \"associative scan\" construction used in recent\ntheoretical work by Liu et al. (2023) and Merrill et al. (2024). The second\nuses an easy-to-compute feature (permutation parity) to partially prune the\nspace of outputs, then refines this with an associative scan. The two\nmechanisms exhibit markedly different robustness properties, and we show how to\nsteer LMs toward one or the other with intermediate training tasks that\nencourage or suppress the heuristics. Our results demonstrate that transformer\nLMs, whether pretrained or fine-tuned, can learn to implement efficient and\ninterpretable state tracking mechanisms, and the emergence of these mechanisms\ncan be predicted and controlled.", "published": "2025-03-04 18:31:02", "link": "http://arxiv.org/abs/2503.02854v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers", "abstract": "Large language models (LLMs) are known to hallucinate, a phenomenon often\nlinked to creativity. While previous research has primarily explored this\nconnection through theoretical or qualitative lenses, our work takes a\nquantitative approach to systematically examine the relationship between\nhallucination and creativity in LLMs. Given the complex nature of creativity,\nwe propose a narrow definition tailored to LLMs and introduce an evaluation\nframework, HCL, which quantifies Hallucination and Creativity across different\nLayers of LLMs during decoding. Our empirical analysis reveals a tradeoff\nbetween hallucination and creativity that is consistent across layer depth,\nmodel type, and model size. Notably, across different model architectures, we\nidentify a specific layer at each model size that optimally balances this\ntradeoff. Additionally, the optimal layer tends to appear in the early layers\nof larger models, and the confidence of the model is also significantly higher\nat this layer. These findings provide a quantitative perspective that offers\nnew insights into the interplay between LLM creativity and hallucination. The\ncode and data for our experiments are available at\nhttps://github.com/ZicongHe2002/HCL-Spark.", "published": "2025-03-04 18:27:00", "link": "http://arxiv.org/abs/2503.02851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs", "abstract": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.", "published": "2025-03-04 18:20:24", "link": "http://arxiv.org/abs/2503.02846v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation", "abstract": "In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.", "published": "2025-03-04 17:57:09", "link": "http://arxiv.org/abs/2503.02832v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression", "abstract": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.", "published": "2025-03-04 17:37:49", "link": "http://arxiv.org/abs/2503.02812v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging", "abstract": "Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.", "published": "2025-03-04 16:56:34", "link": "http://arxiv.org/abs/2503.02783v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Implicit Bias in LLMs: A Survey", "abstract": "Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.", "published": "2025-03-04 16:49:37", "link": "http://arxiv.org/abs/2503.02776v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training", "abstract": "Recent advancements in speech large language models (SpeechLLMs) have\nattracted considerable attention. Nonetheless, current methods exhibit\nsuboptimal performance in adhering to speech instructions. Notably, the\nintelligence of models significantly diminishes when processing speech-form\ninput as compared to direct text-form input. Prior work has attempted to\nmitigate this semantic inconsistency between speech and text representations\nthrough techniques such as representation and behavior alignment, which involve\nthe meticulous design of data pairs during the post-training phase. In this\npaper, we introduce a simple and scalable training method called InSerter,\nwhich stands for Interleaved Speech-Text Representation Pre-training. InSerter\nis designed to pre-train large-scale unsupervised speech-text sequences, where\nthe speech is synthesized from randomly selected segments of an extensive text\ncorpus using text-to-speech conversion. Consequently, the model acquires the\nability to generate textual continuations corresponding to the provided speech\nsegments, obviating the need for intensive data design endeavors. To\nsystematically evaluate speech instruction-following capabilities, we introduce\nSpeechInstructBench, the first comprehensive benchmark specifically designed\nfor speech-oriented instruction-following tasks. Our proposed InSerter achieves\nSOTA performance in SpeechInstructBench and demonstrates superior or\ncompetitive results across diverse speech processing tasks.", "published": "2025-03-04 16:34:14", "link": "http://arxiv.org/abs/2503.02769v1", "categories": ["cs.SD", "cs.CL", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine Symbolic Language for Modern Clinical Relevance", "abstract": "Metaphorical expressions are abundant in Traditional Chinese Medicine (TCM),\nconveying complex disease mechanisms and holistic health concepts through\nculturally rich and often abstract terminology. Bridging these metaphors to\nanatomically driven Western medical (WM) concepts poses significant challenges\nfor both automated language processing and real-world clinical practice. To\naddress this gap, we propose a novel multi-agent and chain-of-thought (CoT)\nframework designed to interpret TCM metaphors accurately and map them to WM\npathophysiology. Specifically, our approach combines domain-specialized agents\n(TCM Expert, WM Expert) with a Coordinator Agent, leveraging stepwise\nchain-of-thought prompts to ensure transparent reasoning and conflict\nresolution. We detail a methodology for building a metaphor-rich TCM dataset,\ndiscuss strategies for effectively integrating multi-agent collaboration and\nCoT reasoning, and articulate the theoretical underpinnings that guide metaphor\ninterpretation across distinct medical paradigms. We present a comprehensive\nsystem design and highlight both the potential benefits and limitations of our\napproach, while leaving placeholders for future experimental validation. Our\nwork aims to support clinical decision-making, cross-system educational\ninitiatives, and integrated healthcare research, ultimately offering a robust\nscaffold for reconciling TCM's symbolic language with the mechanistic focus of\nWestern medicine.", "published": "2025-03-04 16:22:49", "link": "http://arxiv.org/abs/2503.02760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BatchGEMBA: Token-Efficient Machine Translation Evaluation with Batched Prompting and Prompt Compression", "abstract": "Recent advancements in Large Language Model (LLM)-based Natural Language\nGeneration evaluation have largely focused on single-example prompting,\nresulting in significant token overhead and computational inefficiencies. In\nthis work, we introduce BatchGEMBA-MQM, a framework that integrates batched\nprompting with the GEMBA-MQM metric for machine translation evaluation. Our\napproach aggregates multiple translation examples into a single prompt,\nreducing token usage by 2-4 times (depending on the batch size) relative to\nsingle-example prompting. Furthermore, we propose a batching-aware prompt\ncompression model that achieves an additional token reduction of 13-15% on\naverage while also showing ability to help mitigate batching-induced quality\ndegradation. Evaluations across several LLMs (GPT-4o, GPT-4o-mini, Mistral\nSmall, Phi4, and CommandR7B) and varying batch sizes reveal that while batching\ngenerally negatively affects quality (but sometimes not substantially), prompt\ncompression does not degrade further, and in some cases, recovers quality loss.\nFor instance, GPT-4o retains over 90% of its baseline performance at a batch\nsize of 4 when compression is applied, compared to a 44.6% drop without\ncompression. We plan to release our code and trained models at\nhttps://github.com/NL2G/batchgemba to support future research in this domain.", "published": "2025-03-04 16:20:52", "link": "http://arxiv.org/abs/2503.02756v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeded Poisson Factorization: Leveraging domain knowledge to fit topic models", "abstract": "Topic models are widely used for discovering latent thematic structures in\nlarge text corpora, yet traditional unsupervised methods often struggle to\nalign with predefined conceptual domains. This paper introduces Seeded Poisson\nFactorization (SPF), a novel approach that extends the Poisson Factorization\nframework by incorporating domain knowledge through seed words. SPF enables a\nmore interpretable and structured topic discovery by modifying the prior\ndistribution of topic-specific term intensities, assigning higher initial rates\nto predefined seed words. The model is estimated using variational inference\nwith stochastic gradient optimization, ensuring scalability to large datasets.\n  We apply SPF to an Amazon customer feedback dataset, leveraging predefined\nproduct categories as guiding structures. Our evaluation demonstrates that SPF\nachieves superior classification performance compared to alternative guided\ntopic models, particularly in terms of computational efficiency and predictive\nperformance. Furthermore, robustness checks highlight SPF's ability to\nadaptively balance domain knowledge and data-driven topic discovery, even in\ncases of imperfect seed word selection. These results establish SPF as a\npowerful and scalable alternative for integrating expert knowledge into topic\nmodeling, enhancing both interpretability and efficiency in real-world\napplications.", "published": "2025-03-04 16:05:13", "link": "http://arxiv.org/abs/2503.02741v1", "categories": ["stat.ME", "cs.CL", "cs.LG", "econ.GN", "q-fin.EC"], "primary_category": "stat.ME"}
{"title": "Large Language Models for Multilingual Previously Fact-Checked Claim Detection", "abstract": "In our era of widespread false information, human fact-checkers often face\nthe challenge of duplicating efforts when verifying claims that may have\nalready been addressed in other countries or languages. As false information\ntranscends linguistic boundaries, the ability to automatically detect\npreviously fact-checked claims across languages has become an increasingly\nimportant task. This paper presents the first comprehensive evaluation of large\nlanguage models (LLMs) for multilingual previously fact-checked claim\ndetection. We assess seven LLMs across 20 languages in both monolingual and\ncross-lingual settings. Our results show that while LLMs perform well for\nhigh-resource languages, they struggle with low-resource languages. Moreover,\ntranslating original texts into English proved to be beneficial for\nlow-resource languages. These findings highlight the potential of LLMs for\nmultilingual previously fact-checked claim detection and provide a foundation\nfor further research on this promising application of LLMs.", "published": "2025-03-04 15:56:43", "link": "http://arxiv.org/abs/2503.02737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Knowledge Generation and Self-Refinement Strategies for LLM-based Column Type Annotation", "abstract": "Understanding the semantics of columns in relational tables is an important\npre-processing step for indexing data lakes in order to provide rich data\nsearch. An approach to establishing such understanding is column type\nannotation (CTA) where the goal is to annotate table columns with terms from a\ngiven vocabulary. This paper experimentally compares different knowledge\ngeneration and self-refinement strategies for LLM-based column type annotation.\nThe strategies include using LLMs to generate term definitions, error-based\nrefinement of term definitions, self-correction, and fine-tuning using examples\nand term definitions. We evaluate these strategies along two dimensions:\neffectiveness measured as F1 performance and efficiency measured in terms of\ntoken usage and cost. Our experiments show that the best performing strategy\ndepends on the model/dataset combination. We find that using training data to\ngenerate label definitions outperforms using the same data as demonstrations\nfor in-context learning for two out of three datasets using OpenAI models. The\nexperiments further show that using the LLMs to refine label definitions brings\nan average increase of 3.9% F1 in 10 out of 12 setups compared to the\nperformance of the non-refined definitions. Combining fine-tuned models with\nself-refined term definitions results in the overall highest performance,\noutperforming zero-shot prompting fine-tuned models by at least 3% in F1 score.\nThe costs analysis shows that while reaching similar F1 score, self-refinement\nvia prompting is more cost efficient for use cases requiring smaller amounts of\ntables to be annotated while fine-tuning is more efficient for large amounts of\ntables.", "published": "2025-03-04 15:32:59", "link": "http://arxiv.org/abs/2503.02718v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Normalization through Fine-tuning: Understanding Wav2vec 2.0 Embeddings for Phonetic Analysis", "abstract": "Phonetic normalization plays a crucial role in speech recognition and\nanalysis, ensuring the comparability of features derived from raw audio data.\nHowever, in the current paradigm of fine-tuning pre-trained large transformer\nmodels, phonetic normalization is not deemed a necessary step; instead, it is\nimplicitly executed within the models. This study investigates the\nnormalization process within transformer models, especially wav2vec 2.0.\nThrough a comprehensive analysis of embeddings from models fine-tuned for\nvarious tasks, our results demonstrate that fine-tuning wav2vec 2.0 effectively\nachieves phonetic normalization by selectively suppressing task-irrelevant\ninformation. We found that models fine-tuned for multiple tasks retain\ninformation for both tasks without compromising performance, and that\nsuppressing task-irrelevant information is not necessary for effective\nclassification. These findings provide new insights into how phonetic\nnormalization can be flexibly achieved in speech models and how it is realized\nin human speech perception.", "published": "2025-03-04 15:28:10", "link": "http://arxiv.org/abs/2503.04814v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multilingualism, Transnationality, and K-pop in the Online #StopAsianHate Movement", "abstract": "The #StopAsianHate (SAH) movement is a broad social movement against violence\ntargeting Asians and Asian Americans, beginning in 2021 in response to racial\ndiscrimination related to COVID-19 and sparking worldwide conversation about\nanti-Asian hate. However, research on the online SAH movement has focused on\nEnglish-speaking participants so the spread of the movement outside of the\nUnited States is largely unknown. In addition, there have been no long-term\nstudies of SAH so the extent to which it has been successfully sustained over\ntime is not well understood. We present an analysis of 6.5 million\n\"#StopAsianHate\" tweets from 2.2 million users all over the globe and spanning\n60 different languages, constituting the first study of the non-English and\ntransnational component of the online SAH movement. Using a combination of\ntopic modeling, user modeling, and hand annotation, we identify and\ncharacterize the dominant discussions and users participating in the movement\nand draw comparisons of English versus non-English topics and users. We\ndiscover clear differences in events driving topics, where spikes in English\ntweets are driven by violent crimes in the US but spikes in non-English tweets\nare driven by transnational incidents of anti-Asian sentiment towards symbolic\nrepresentatives of Asian nations. We also find that global K-pop fans were\nquick to adopt the SAH movement and, in fact, sustained it for longer than any\nother user group. Our work contributes to understanding the transnationality\nand evolution of the SAH movement, and more generally to exploring upward scale\nshift and public attention in large-scale multilingual online activism.", "published": "2025-03-04 15:21:22", "link": "http://arxiv.org/abs/2503.02707v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "MPO: Boosting LLM Agents with Meta Plan Optimization", "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.", "published": "2025-03-04 14:54:45", "link": "http://arxiv.org/abs/2503.02682v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Are some books better than others?", "abstract": "Scholars, awards committees, and laypeople frequently discuss the merit of\nwritten works. Literary professionals and journalists differ in how much\nperspectivism they concede in their book reviews. Here, we quantify how\nstrongly book reviews are determined by the actual book contents vs.\nidiosyncratic reader tendencies. In our analysis of 624,320 numerical and\ntextual book reviews, we find that the contents of professionally published\nbooks are not predictive of a random reader's reading enjoyment. Online reviews\nof popular fiction and non-fiction books carry up to ten times more information\nabout the reviewer than about the book. For books of a preferred genre, readers\nmight be less likely to give low ratings, but still struggle to converge in\ntheir relative assessments. We find that book evaluations generalize more\nacross experienced review writers than casual readers. When discussing specific\nissues with a book, one review text had poor predictability of issues brought\nup in another review of the same book. We conclude that extreme perspectivism\nis a justifiable position when researching literary quality, bestowing literary\nawards, and designing recommendation systems.", "published": "2025-03-04 14:43:43", "link": "http://arxiv.org/abs/2503.02671v1", "categories": ["cs.DL", "cs.CL", "cs.IR"], "primary_category": "cs.DL"}
{"title": "Self-Evolved Preference Optimization for Enhancing Mathematical Reasoning in Small Language Models", "abstract": "Large language models (LLMs) have significantly improved their reasoning\ncapabilities; however, they still struggle with complex multi-step mathematical\nproblem-solving due to error propagation, lack of self-correction, and limited\nadaptability to diverse reasoning styles. Existing methods rely on static\nfine-tuning or prompt engineering, which fail to generalize across problem\ncomplexities, while the scarcity of high-quality preference data further\nhinders reliable reasoning.\n  We introduce SPHERE, a self-evolving data generation pipeline that enhances\nreasoning in small language models (SLMs) by iteratively generating,\ncorrecting, and diversifying reasoning chains. SPHERE operates in three stages:\n(i) Self-Generation, where the model autonomously constructs problem-solving\nsteps; (ii) Self-Correction, enabling it to identify and rectify errors; and\n(iii) Diversity Induction, improving robustness through multiple valid\nreasoning trajectories. This self-evolution mechanism strengthens mathematical\nreasoning and enhances model reliability. Evaluations on MATH 500, GSM8K, AIME,\nAMC, and Olympiad show that SPHERE-trained models achieve significant gains\nover their base versions and match/surpass GPT-4o on certain benchmarks. Our\nfindings demonstrate that self-evolving models can close the reasoning gap\nbetween SLMs and state-of-the-art LLMs, making mathematical AI more reliable,\nscalable, and efficient.", "published": "2025-03-04 14:43:25", "link": "http://arxiv.org/abs/2503.04813v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multidimensional Consistency Improves Reasoning in Language Models", "abstract": "While Large language models (LLMs) have proved able to address some complex\nreasoning tasks, we also know that they are highly sensitive to input\nvariation, which can lead to different solution paths and final answers. Answer\nconsistency across input variations can thus be taken as a sign of stronger\nconfidence. Leveraging this insight, we introduce a framework, {\\em\nMultidimensional Reasoning Consistency} where, focusing on math problems,\nmodels are systematically pushed to diversify solution paths towards a final\nanswer, thereby testing them for answer consistency across multiple input\nvariations. We induce variations in (i) order of shots in prompt, (ii) problem\nphrasing, and (iii) languages used. Extensive experiments on a large range of\nopen-source state-of-the-art LLMs of various sizes show that reasoning\nconsistency differs by variation dimension, and that by aggregating consistency\nacross dimensions, our framework consistently enhances mathematical reasoning\nperformance on both monolingual dataset GSM8K and multilingual dataset MGSM,\nespecially for smaller models.", "published": "2025-03-04 14:41:05", "link": "http://arxiv.org/abs/2503.02670v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models", "abstract": "Low-Rank Adaptation (LoRA) is the leading parameter-efficient fine-tuning\nmethod for Large Language Models (LLMs). However, the fine-tuned LLMs encounter\nthe issue of catastrophic forgetting of the pre-trained world knowledge. To\naddress this issue, inspired by theoretical insights of null space, we propose\nLoRA-Null, i.e., Low-Rank Adaptation via null space, which builds adapters\ninitialized from the null space of the pre-trained knowledge activation.\nConcretely, we randomly collect a few data samples and capture their\nactivations after passing through the LLM layer. We perform Singular Value\nDecomposition on the input activations to obtain their null space. We use the\nprojection of the pre-trained weights onto the null space as the initialization\nfor adapters. Experimental results demonstrate that this initialization\napproach can effectively preserve the original pre-trained world knowledge of\nthe LLMs during fine-tuning. Additionally, if we freeze the values of the\ndown-projection matrices during fine-tuning, it achieves even better\npreservation of the pre-trained world knowledge. LoRA-Null effectively\npreserves pre-trained world knowledge while maintaining strong fine-tuning\nperformance, as validated by extensive experiments on LLaMA series (LLaMA2,\nLLaMA3, LLaMA3.1, and LLaMA3.2) across Code, Math, and Instruction Following\ntasks. We also provide a theoretical guarantee for the capacity of LoRA-Null to\nretain pre-trained knowledge. Code is in\nhttps://github.com/HungerPWAY/LoRA-Null.", "published": "2025-03-04 14:21:08", "link": "http://arxiv.org/abs/2503.02659v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "abstract": "Decoder-based transformers, while revolutionizing language modeling and\nscaling to immense sizes, have not completely overtaken encoder-heavy\narchitectures in natural language processing. Specifically, encoder-only models\nremain dominant in tasks like classification, regression, and ranking. This is\nprimarily due to the inherent structure of decoder-based models, which limits\ntheir direct applicability to these tasks. In this paper, we introduce Gemma\nEncoder, adapting the powerful Gemma decoder model to an encoder architecture,\nthereby unlocking its potential for a wider range of non-generative\napplications. To optimize the adaptation from decoder to encoder, we\nsystematically analyze various pooling strategies, attention mechanisms, and\nhyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder\nagainst established approaches on the GLUE benchmarks, and MS MARCO ranking\nbenchmark, demonstrating its effectiveness and versatility.", "published": "2025-03-04 14:17:00", "link": "http://arxiv.org/abs/2503.02656v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats", "abstract": "The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.", "published": "2025-03-04 14:14:28", "link": "http://arxiv.org/abs/2503.02650v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Towards Event Extraction with Massive Types: LLM-based Collaborative Annotation and Partitioning Extraction", "abstract": "Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.", "published": "2025-03-04 13:53:43", "link": "http://arxiv.org/abs/2503.02628v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ttta: Tools for Temporal Text Analysis", "abstract": "Text data is inherently temporal. The meaning of words and phrases changes\nover time, and the context in which they are used is constantly evolving. This\nis not just true for social media data, where the language used is rapidly\ninfluenced by current events, memes and trends, but also for journalistic,\neconomic or political text data. Most NLP techniques however consider the\ncorpus at hand to be homogenous in regard to time. This is a simplification\nthat can lead to biased results, as the meaning of words and phrases can change\nover time. For instance, running a classic Latent Dirichlet Allocation on a\ncorpus that spans several years is not enough to capture changes in the topics\nover time, but only portraits an \"average\" topic distribution over the whole\ntime span. Researchers have developed a number of tools for analyzing text data\nover time. However, these tools are often scattered across different packages\nand libraries, making it difficult for researchers to use them in a consistent\nand reproducible way. The ttta package is supposed to serve as a collection of\ntools for analyzing text data over time.", "published": "2025-03-04 13:50:21", "link": "http://arxiv.org/abs/2503.02625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rewarding Doubt: A Reinforcement Learning Approach to Confidence Calibration of Large Language Models", "abstract": "A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.", "published": "2025-03-04 13:48:50", "link": "http://arxiv.org/abs/2503.02623v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query Processing", "abstract": "Large Language Models (LLMs) encounter challenges in efficiently processing\nlong-text queries, as seen in applications like enterprise document analysis\nand financial report comprehension. While conventional solutions employ\nlong-context processing or Retrieval-Augmented Generation (RAG), they suffer\nfrom prohibitive input expenses or incomplete information. Recent advancements\nadopt context compression and dynamic retrieval loops, but still sacrifice\ncritical details or incur iterative costs. To address these limitations, we\npropose OkraLong, a novel framework that flexibly optimizes the entire\nprocessing workflow. Unlike prior static or coarse-grained adaptive strategies,\nOkraLong adopts fine-grained orchestration through three synergistic\ncomponents: analyzer, organizer and executor. The analyzer characterizes the\ntask states, which guide the organizer in dynamically scheduling the workflow.\nThe executor carries out the execution and generates the final answer.\nExperimental results demonstrate that OkraLong not only enhances answer\naccuracy but also achieves cost-effectiveness across a variety of datasets.", "published": "2025-03-04 13:21:47", "link": "http://arxiv.org/abs/2503.02603v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MCiteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs", "abstract": "Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, overlooking the challenges and opportunities\nof multimodal contexts. To address this gap, we introduce MCiteBench, the first\nbenchmark designed to evaluate and analyze the multimodal citation text\ngeneration ability of MLLMs. Our benchmark comprises data derived from academic\npapers and review-rebuttal interactions, featuring diverse information sources\nand multimodal content. We comprehensively evaluate models from multiple\ndimensions, including citation quality, source reliability, and answer\naccuracy. Through extensive experiments, we observe that MLLMs struggle with\nmultimodal citation text generation. We also conduct deep analyses of models'\nperformance, revealing that the bottleneck lies in attributing the correct\nsources rather than understanding the multimodal content.", "published": "2025-03-04 13:12:39", "link": "http://arxiv.org/abs/2503.02589v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Use Me Wisely: AI-Driven Assessment for LLM Prompting Skills Development", "abstract": "The use of large language model (LLM)-powered chatbots, such as ChatGPT, has\nbecome popular across various domains, supporting a range of tasks and\nprocesses. However, due to the intrinsic complexity of LLMs, effective\nprompting is more challenging than it may seem. This highlights the need for\ninnovative educational and support strategies that are both widely accessible\nand seamlessly integrated into task workflows. Yet, LLM prompting is highly\ntask- and domain-dependent, limiting the effectiveness of generic approaches.\nIn this study, we explore whether LLM-based methods can facilitate learning\nassessments by using ad-hoc guidelines and a minimal number of annotated prompt\nsamples. Our framework transforms these guidelines into features that can be\nidentified within learners' prompts. Using these feature descriptions and\nannotated examples, we create few-shot learning detectors. We then evaluate\ndifferent configurations of these detectors, testing three state-of-the-art\nLLMs and ensembles. We run experiments with cross-validation on a sample of\noriginal prompts, as well as tests on prompts collected from task-naive\nlearners. Our results show how LLMs perform on feature detection. Notably, GPT-\n4 demonstrates strong performance on most features, while closely related\nmodels, such as GPT-3 and GPT-3.5 Turbo (Instruct), show inconsistent behaviors\nin feature classification. These differences highlight the need for further\nresearch into how design choices impact feature selection and prompt detection.\nOur findings contribute to the fields of generative AI literacy and\ncomputer-supported learning assessment, offering valuable insights for both\nresearchers and practitioners.", "published": "2025-03-04 11:56:33", "link": "http://arxiv.org/abs/2503.02532v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent", "abstract": "Large language model (LLM) agents typically adopt a step-by-step reasoning\nframework, in which they interleave the processes of thinking and acting to\naccomplish the given task. However, this paradigm faces a deep-rooted one-pass\nissue whereby each generated intermediate thought is plugged into the\ntrajectory regardless of its correctness, which can cause irreversible error\npropagation. To address the issue, this paper proposes a novel framework called\nGenerator-Assistant Stepwise Rollback (GA-Rollback) to induce better\ndecision-making for LLM agents. Particularly, GA-Rollback utilizes a generator\nto interact with the environment and an assistant to examine each action\nproduced by the generator, where the assistant triggers a rollback operation\nupon detection of incorrect actions. Moreover, we introduce two additional\nstrategies tailored for the rollback scenario to further improve its\neffectiveness. Extensive experiments show that GA-Rollback achieves significant\nimprovements over several strong baselines on three widely used benchmarks. Our\nanalysis further reveals that GA-Rollback can function as a robust\nplug-and-play module, integrating seamlessly with other methods.", "published": "2025-03-04 11:31:05", "link": "http://arxiv.org/abs/2503.02519v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs", "abstract": "Long-context modeling has drawn more and more attention in the area of Large\nLanguage Models (LLMs). Continual training with long-context data becomes the\nde-facto method to equip LLMs with the ability to process long inputs. However,\nit still remains an open challenge to measure the quality of long-context\ntraining data. To address this issue, we propose a Long-context data selection\nframework with Attention-based Dependency Measurement (LADM), which can\nefficiently identify high-quality long-context data from a large-scale,\nmulti-domain pre-training corpus. LADM leverages the retrieval capabilities of\nthe attention mechanism to capture contextual dependencies, ensuring a\ncomprehensive quality measurement of long-context data. Experimental results\nshow that our LADM framework significantly boosts the performance of LLMs on\nmultiple long-context tasks with only 1B tokens for continual training.", "published": "2025-03-04 11:10:13", "link": "http://arxiv.org/abs/2503.02502v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer", "abstract": "We propose Union-of-Experts (UoE), which decomposes transformer into an\nequitant group of experts, and then implement selective routing on input data\nand experts. Our approach advances MoE design with four key innovations: (1) We\nconducted equitant expert decomposition on both MLP blocks and attention blocks\nbased on matrix partition in tensor parallelism. (2) We developed two routing\nparadigms: patch-wise data selection and expert selection, to apply routing\nacross different levels. (3) We design the architecture of UoE model, including\nSelective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We\ndevelop parallel implementation of UoE's routing and computation operation, and\noptimize efficiency based on the hardware processing analysis. The experiments\ndemonstrate that the UoE model surpass Full Attention, state-of-art MoEs and\nefficient transformers (including the model architecture of recently proposed\nDeepSeek-V3) in several tasks across image and natural language domains. In\nlanguage modeling tasks, we achieve an average reduction of 2.38 in perplexity\ncompared to the best-performed MoE method with an average of 76% FLOPs. In Long\nRange Arena benchmark, we recorded an average score that is at least 0.68%\nhigher than all comparison models including Full Attention, MoEs, and\ntransformer variants, with only 50% FLOPs of the best MoE method. In image\nclassification, our model yielded an average accuracy improvement of 1.75% than\nthe best model while maintaining comparable FLOPs. The source codes are\navailable at https://github.com/YujiaoYang-work/UoE.", "published": "2025-03-04 11:01:25", "link": "http://arxiv.org/abs/2503.02495v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07", "I.5.1; I.2.0"], "primary_category": "cs.LG"}
{"title": "LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning", "abstract": "Universal multimodal embedding models play a critical role in tasks such as\ninterleaved image-text retrieval, multimodal RAG, and multimodal clustering.\nHowever, our empirical results indicate that existing LMM-based embedding\nmodels trained with the standard InfoNCE loss exhibit a high degree of overlap\nin similarity distribution between positive and negative pairs, making it\nchallenging to distinguish hard negative pairs effectively. To deal with this\nissue, we propose a simple yet effective framework that dynamically improves\nthe embedding model's representation learning for negative pairs based on their\ndiscriminative difficulty. Within this framework, we train a series of models,\nnamed LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasks\nand 36 datasets. Experimental results show that LLaVE establishes stronger\nbaselines that achieve state-of-the-art (SOTA) performance while demonstrating\nstrong scalability and efficiency. Specifically, LLaVE-2B surpasses the\nprevious SOTA 7B models, while LLaVE-7B achieves a further performance\nimprovement of 6.2 points. Although LLaVE is trained on image-text data, it can\ngeneralize to text-video retrieval tasks in a zero-shot manner and achieve\nstrong performance, demonstrating its remarkable potential for transfer to\nother embedding tasks.", "published": "2025-03-04 10:21:57", "link": "http://arxiv.org/abs/2503.04812v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate Mutually via Selective Rationale Optimisation", "abstract": "Very large language models (LLMs) such as GPT-4 have shown the ability to\nhandle complex tasks by generating and self-refining step-by-step rationales.\nSmaller language models (SLMs), typically with < 13B parameters, have been\nimproved by using the data generated from very-large LMs through knowledge\ndistillation. However, various practical constraints such as API costs,\ncopyright, legal and ethical policies restrict using large (often opaque)\nmodels to train smaller models for commercial use. Limited success has been\nachieved at improving the ability of an SLM to explore the space of possible\nrationales and evaluate them by itself through self-deliberation. To address\nthis, we propose COALITION, a trainable framework that facilitates interaction\nbetween two variants of the same SLM and trains them to generate and refine\nrationales optimized for the end-task. The variants exhibit different behaviors\nto produce a set of diverse candidate rationales during the generation and\nrefinement steps. The model is then trained via Selective Rationale\nOptimization (SRO) to prefer generating rationale candidates that maximize the\nlikelihood of producing the ground-truth answer. During inference, COALITION\nemploys a controller to select the suitable variant for generating and refining\nthe rationales. On five different datasets covering mathematical problems,\ncommonsense reasoning, and natural language inference, COALITION outperforms\nseveral baselines by up to 5%. Our ablation studies reveal that\ncross-communication between the two variants performs better than using the\nsingle model to self-refine the rationales. We also demonstrate the\napplicability of COALITION for LMs of varying scales (4B to 14B parameters) and\nmodel families (Mistral, Llama, Qwen, Phi). We release the code for this work\nat https://github.com/Sohanpatnaik106/coalition.", "published": "2025-03-04 10:17:29", "link": "http://arxiv.org/abs/2503.02463v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization", "abstract": "Personalizing Large Language Models (LLMs) has become a critical step in\nfacilitating their widespread application to enhance individual life\nexperiences. In pursuit of personalization, distilling key preference\ninformation from an individual's historical data as instructional preference\ncontext to customize LLM generation has emerged as a promising direction.\nHowever, these methods face a fundamental limitation by overlooking the\ninter-user comparative analysis, which is essential for identifying the\ninter-user differences that truly shape preferences. To address this\nlimitation, we propose Difference-aware Personalization Learning (DPL), a novel\napproach that emphasizes extracting inter-user differences to enhance LLM\npersonalization. DPL strategically selects representative users for comparison\nand establishes a structured standard to extract meaningful, task-relevant\ndifferences for customizing LLM generation. Extensive experiments on real-world\ndatasets demonstrate that DPL significantly enhances LLM personalization. We\nrelease our code at https://github.com/SnowCharmQ/DPL.", "published": "2025-03-04 09:53:26", "link": "http://arxiv.org/abs/2503.02450v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling", "abstract": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.", "published": "2025-03-04 09:40:00", "link": "http://arxiv.org/abs/2503.02445v2", "categories": ["cs.LG", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Flipping Matchings is Hard", "abstract": "Given a point set $\\mathcal{P}$ and a plane perfect matching $\\mathcal{M}$ on\n$\\mathcal{P}$, a flip is an operation that replaces two edges of $\\mathcal{M}$\nsuch that another plane perfect matching on $\\mathcal{P}$ is obtained. Given\ntwo plane perfect matchings on $\\mathcal{P}$, we show that it is NP-hard to\nminimize the number of flips that are needed to transform one matching into the\nother.", "published": "2025-03-04 18:13:39", "link": "http://arxiv.org/abs/2503.02842v1", "categories": ["cs.CG", "cs.DM"], "primary_category": "cs.CG"}
{"title": "Hierarchy of Hub Covering Problems", "abstract": "Hub Covering Problems arise in various practical domains, such as urban\nplanning, cargo delivery systems, airline networks, telecommunication network\ndesign, and e-mobility. The task is to select a set of hubs that enable tours\nbetween designated origin-destination pairs while ensuring that any tour\nincludes no more than two hubs and that either the overall tour length or the\nlongest individual edge is kept within prescribed limits. In literature, three\nprimary variants of this problem are distinguished by their specific\nconstraints. Each version exists in a single and multi allocation version,\nresulting in multiple distinct problem statements. Furthermore, the capacitated\nversions of these problems introduce additional restrictions on the maximum\nnumber of hubs that can be opened. It is currently unclear whether some\nvariants are more complex than others, and no approximation bound is known. In\nthis paper, we establish a hierarchy among these problems, demonstrating that\ncertain variants are indeed special cases of others. For each problem, we\neither determine the absence of any approximation bound or provide both upper\nand lower bounds on the approximation guarantee.", "published": "2025-03-04 12:44:58", "link": "http://arxiv.org/abs/2503.02566v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "3-Majority and 2-Choices with Many Opinions", "abstract": "We present the first nearly-optimal bounds on the consensus time for the\nwell-known synchronous consensus dynamics, specifically 3-Majority and\n2-Choices, for an arbitrary number of opinions. In synchronous consensus\ndynamics, we consider an $n$-vertex complete graph with self-loops, where each\nvertex holds an opinion from $\\{1,\\dots,k\\}$. At each discrete-time round, all\nvertices update their opinions simultaneously according to a given protocol.\nThe goal is to reach a consensus, where all vertices support the same opinion.\nIn 3-Majority, each vertex chooses three random neighbors with replacement and\nupdates its opinion to match the majority, with ties broken randomly. In\n2-Choices, each vertex chooses two random neighbors with replacement. If the\nselected vertices hold the same opinion, the vertex adopts that opinion.\nOtherwise, it retains its current opinion for that round. Improving upon a line\nof work [Becchetti et al., SPAA'14], [Becchetti et al., SODA'16], [Berenbrink\net al., PODC'17], [Ghaffari and Lengler, PODC'18], we prove that, for every\n$2\\le k \\le n$, 3-Majority (resp.\\ 2-Choices) reaches consensus within\n$\\widetilde{\\Theta}(\\min\\{k,\\sqrt{n}\\})$ (resp.\\ $\\widetilde{\\Theta}(k)$)\nrounds with high probability. Prior to this work, the best known upper bound on\nthe consensus time of 3-Majority was $\\widetilde{O}(k)$ if $k \\ll n^{1/3}$ and\n$\\widetilde{O}(n^{2/3})$ otherwise, and for 2-Choices, the consensus time was\nknown to be $\\widetilde{O}(k)$ for $k\\ll \\sqrt{n}$.", "published": "2025-03-04 09:13:39", "link": "http://arxiv.org/abs/2503.02426v1", "categories": ["cs.DC", "cs.DM"], "primary_category": "cs.DC"}
{"title": "Boosting Rectilinear Steiner Minimum Tree Algorithms with Augmented Bounding Volume Hierarchy", "abstract": "The rectilinear Steiner minimum tree (RSMT) problem computes the shortest\nnetwork connecting a given set of points using only horizontal and vertical\nlines, possibly adding extra points (Steiner points) to minimize the total\nlength. RSMT solvers seek to balance speed and accuracy. In this work, we\ndesign a framework to boost existing RSMT solvers, extending the Pareto front.\nCombined with GeoSteiner, our algorithm reaches 5.16\\% length error on nets\nwith 1000 pins. The average time needed is 0.46 seconds. This provides an\neffective way to solve large-scale RSMT problems with small-scale solvers.", "published": "2025-03-04 06:19:07", "link": "http://arxiv.org/abs/2503.02319v1", "categories": ["cs.DS", "cs.CG", "cs.DM"], "primary_category": "cs.DS"}
{"title": "On graphs coverable by chubby shortest paths", "abstract": "Dumas, Foucaud, Perez, and Todinca [SIAM J. Disc. Math., 2024] proved that if\nthe vertex set of a graph $G$ can be covered by $k$ shortest paths, then the\npathwidth of $G$ is bounded by $\\mathcal{O}(k \\cdot 3^k)$.\n  We prove a coarse variant of this theorem: if in a graph $G$ one can find~$k$\nshortest paths such that every vertex is at distance at most $\\rho$ from one of\nthem, then $G$ is $(3,12\\rho)$-quasi-isometric to a graph of pathwidth\n$k^{\\mathcal{O}(k)}$ and maximum degree $\\mathcal{O}(k)$, and $G$ admits a\npath-partition-decomposition whose bags are coverable by $k^{\\mathcal{O}(k)}$\nballs of radius at most $2\\rho$ and vertices from non-adjacent bags are at\ndistance larger than $2\\rho$.\n  We also discuss applications of such decompositions in the context of\nalgorithms for finding maximum distance independent sets and minimum distance\ndominating sets in graphs.", "published": "2025-03-04 00:45:47", "link": "http://arxiv.org/abs/2503.02160v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "Accelerating Focal Search in Multi-Agent Path Finding with Tighter Lower Bounds", "abstract": "Multi-Agent Path Finding (MAPF) involves finding collision-free paths for\nmultiple agents while minimizing a cost function--an NP-hard problem. Bounded\nsuboptimal methods like Enhanced Conflict-Based Search (ECBS) and Explicit\nEstimation CBS (EECBS) balance solution quality with computational efficiency\nusing focal search mechanisms. While effective, traditional focal search faces\na limitation: the lower bound (LB) value determining which nodes enter the\nFOCAL list often increases slowly in early search stages, resulting in a\nconstrained search space that delays finding valid solutions. In this paper, we\npropose a novel bounded suboptimal algorithm, double-ECBS (DECBS), to address\nthis issue by first determining the maximum LB value and then employing a\nbest-first search guided by this LB to find a collision-free path. Experimental\nresults demonstrate that DECBS outperforms ECBS in most test cases and is\ncompatible with existing optimization techniques. DECBS can reduce nearly 30%\nhigh-level CT nodes and 50% low-level focal search nodes. When agent density is\nmoderate to high, DECBS achieves a 23.5% average runtime improvement over ECBS\nwith identical suboptimality bounds and optimizations.", "published": "2025-03-04 20:39:00", "link": "http://arxiv.org/abs/2503.03779v1", "categories": ["cs.MA", "cs.AI", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders", "abstract": "Multi-agent coordination is crucial for reliable multi-robot navigation in\nshared spaces such as automated warehouses. In regions of dense robot traffic,\nlocal coordination methods may fail to find a deadlock-free solution. In these\nscenarios, it is appropriate to let a central unit generate a global schedule\nthat decides the passing order of robots. However, the runtime of such\ncentralized coordination methods increases significantly with the problem\nscale. In this paper, we propose to leverage Graph Neural Network Variational\nAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale\nfaster than through centralized optimization. We formulate the coordination\nproblem as a graph problem and collect ground truth data using a Mixed-Integer\nLinear Program (MILP) solver. During training, our learning framework encodes\ngood quality solutions of the graph problem into a latent space. At inference\ntime, solution samples are decoded from the sampled latent variables, and the\nlowest-cost sample is selected for coordination. Finally, the feasible proposal\nwith the highest performance index is selected for the deployment. By\nconstruction, our GNN-VAE framework returns solutions that always respect the\nconstraints of the considered coordination problem. Numerical results show that\nour approach trained on small-scale problems can achieve high-quality solutions\neven for large-scale problems with 250 robots, being much faster than other\nbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord", "published": "2025-03-04 19:20:11", "link": "http://arxiv.org/abs/2503.02954v1", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.RO"}
{"title": "Quantitative Resilience Modeling for Autonomous Cyber Defense", "abstract": "Cyber resilience is the ability of a system to recover from an attack with\nminimal impact on system operations. However, characterizing a network's\nresilience under a cyber attack is challenging, as there are no formal\ndefinitions of resilience applicable to diverse network topologies and attack\npatterns. In this work, we propose a quantifiable formulation of resilience\nthat considers multiple defender operational goals, the criticality of various\nnetwork resources for daily operations, and provides interpretability to\nsecurity operators about their system's resilience under attack. We evaluate\nour approach within the CybORG environment, a reinforcement learning (RL)\nframework for autonomous cyber defense, analyzing trade-offs between\nresilience, costs, and prioritization of operational goals. Furthermore, we\nintroduce methods to aggregate resilience metrics across time-variable attack\npatterns and multiple network topologies, comprehensively characterizing system\nresilience. Using insights gained from our resilience metrics, we design RL\nautonomous defensive agents and compare them against several heuristic\nbaselines, showing that proactive network hardening techniques and prompt\nrecovery of compromised machines are critical for effective cyber defenses.", "published": "2025-03-04 16:52:25", "link": "http://arxiv.org/abs/2503.02780v1", "categories": ["cs.CR", "cs.LG", "cs.MA"], "primary_category": "cs.CR"}
{"title": "Federated Learning for Privacy-Preserving Feedforward Control in Multi-Agent Systems", "abstract": "Feedforward control (FF) is often combined with feedback control (FB) in many\ncontrol systems, improving tracking performance, efficiency, and stability.\nHowever, designing effective data-driven FF controllers in multi-agent systems\nrequires significant data collection, including transferring private or\nproprietary data, which raises privacy concerns and incurs high communication\ncosts. Therefore, we propose a novel approach integrating Federated Learning\n(FL) into FF control to address these challenges. This approach enables\nprivacy-preserving, communication-efficient, and decentralized continuous\nimprovement of FF controllers across multiple agents without sharing personal\nor proprietary data. By leveraging FL, each agent learns a local, neural FF\ncontroller using its data and contributes only model updates to a global\naggregation process, ensuring data privacy and scalability. We demonstrate the\neffectiveness of our method in an autonomous driving use case. Therein,\nvehicles equipped with a trajectory-tracking feedback controller are enhanced\nby FL-based neural FF control. Simulations highlight significant improvements\nin tracking performance compared to pure FB control, analogous to model-based\nFF control. We achieve comparable tracking performance without exchanging\nprivate vehicle-specific data compared to a centralized neural FF control. Our\nresults underscore the potential of FL-based neural FF control to enable\nprivacy-preserving learning in multi-agent control systems, paving the way for\nscalable and efficient autonomous systems applications.", "published": "2025-03-04 15:07:25", "link": "http://arxiv.org/abs/2503.02693v1", "categories": ["cs.LG", "cs.MA"], "primary_category": "cs.LG"}
{"title": "Bayesian Estimation of Corporate Default Spreads", "abstract": "Risk-averse investors often wish to exclude stocks from their portfolios that\nbear high credit risk, which is a measure of a firm's likelihood of bankruptcy.\nThis risk is commonly estimated by constructing signals from quarterly\naccounting items, such as debt and income volatility. While such information\nmay provide a rich description of a firm's credit risk, the low-frequency with\nwhich the data is released implies that investors may be operating with\noutdated information. In this paper we circumvent this problem by developing a\nhigh-frequency credit risk proxy via corporate default spreads which are\nestimated from daily bond price data. We accomplish this by adapting classic\nyield curve estimation methods to a corporate bond setting, leveraging advances\nin Bayesian estimation to ensure higher model stability when working with small\nsample data which also allows us to directly model the uncertainty of our\npredictions.", "published": "2025-03-04 20:34:37", "link": "http://arxiv.org/abs/2503.02991v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Complex discontinuities of $\\surd\\overline{\\text{Fredholm determinants}}$ in the Volterra Stein-Stein model", "abstract": "We study complex discontinuities arising from the miscomputation of the\nFourier-Laplace transform in the Volterra Stein-Stein model, which involves the\ncomplex square root of a Fredholm determinant. Discontinuities occur when the\ndeterminant crosses the negative real axis. We characterize these crossings for\nthe joint Fourier-Laplace transform of the integrated variance and log-price.\nAdditionally, we derive a corrected formula for the Fourier-Laplace transform\nand develop efficient numerical techniques to detect and compute these\ncrossings. Applying our algorithms to Fourier-based pricing in the rough\nStein-Stein model, we achieve a significant increase in accuracy while\ndrastically reducing computational cost compared to existing methods.", "published": "2025-03-04 19:47:09", "link": "http://arxiv.org/abs/2503.02965v1", "categories": ["q-fin.MF", "q-fin.CP", "91G20, 45P05"], "primary_category": "q-fin.MF"}
{"title": "Beyond the Leland strategies", "abstract": "In the Black and Scholes model with proportional transaction costs, the\nLeland strategy allows to asymptotically super-replicate the European Call\noption as the number of revision dates converges to + infinity and the\ntransaction costs rate tends rapidly to 0. This method relies heavily on the\nexplicit expression of the delta-hedging strategy in the Black and Scholes\nmodel where the volatility is enlarged to compensate for the transaction costs.\nWe solve the same problem of super-hedging but for a general model with an\narbitrary fixed number of revision dates and arbitrary fixed transaction costs\nrates. Moreover, our approach does not need the existence of a risk-neutral\nprobability measure and is (almost) model free and easily implementable from\nreal data.", "published": "2025-03-04 09:03:58", "link": "http://arxiv.org/abs/2503.02419v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Numerical methods for two-dimensional G-heat equation", "abstract": "The G-expectation is a sublinear expectation. It is an important tool for\npricing financial products and managing risk thanks to its ability to deal with\nmodel uncertainty. The problem is how to efficiently quantify it since the\ncommonly used Monte Carlo method does not work. Fortunately, the expectation of\na G-normal random variable can be linked to the viscosity solution of a fully\nnonlinear G-heat equation. In this paper, we propose a novel numerical scheme\nfor the two-dimensional G-heat equation and pay more attention to the case that\nthere exists uncertainty on the correlationship, especially to the case that\nthe correlationship ranges from negative to positive. The scheme is monotonic,\nstable, and convergent. The numerical tests show that the scheme is highly\nefficient.", "published": "2025-03-04 08:35:41", "link": "http://arxiv.org/abs/2503.02395v1", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "VWAP Execution with Signature-Enhanced Transformers: A Multi-Asset Learning Approach", "abstract": "In this paper I propose a novel approach to Volume Weighted Average Price\n(VWAP) execution that addresses two key practical challenges: the need for\nasset-specific model training and the capture of complex temporal dependencies.\nBuilding upon my recent work in dynamic VWAP execution arXiv:2502.18177, I\ndemonstrate that a single neural network trained across multiple assets can\nachieve performance comparable to or better than traditional asset-specific\nmodels. The proposed architecture combines a transformer-based design inspired\nby arXiv:2406.02486 with path signatures for capturing geometric features of\nprice-volume trajectories, as in arXiv:2406.17890. The empirical analysis,\nconducted on hourly cryptocurrency trading data from 80 trading pairs, shows\nthat the globally-fitted model with signature features (GFT-Sig) achieves\nsuperior performance in both absolute and quadratic VWAP loss metrics compared\nto asset-specific approaches. Notably, these improvements persist for\nout-of-sample assets, demonstrating the model's ability to generalize across\ndifferent market conditions. The results suggest that combining global\nparameter sharing with signature-based feature extraction provides a scalable\nand robust approach to VWAP execution, offering significant practical\nadvantages over traditional asset-specific implementations.", "published": "2025-03-04 14:50:20", "link": "http://arxiv.org/abs/2503.02680v1", "categories": ["q-fin.ST", "cs.LG"], "primary_category": "q-fin.ST"}
{"title": "Extrapolating the long-term seasonal component of electricity prices for forecasting in the day-ahead market", "abstract": "Recent studies provide evidence that decomposing the electricity price into\nthe long-term seasonal component (LTSC) and the remaining part, predicting both\nseparately, and then combining their forecasts can bring significant accuracy\ngains in day-ahead electricity price forecasting. However, not much attention\nhas been paid to predicting the LTSC, and the last 24 hourly values of the\nestimated pattern are typically copied for the target day. To address this gap,\nwe introduce a novel approach which extracts the trend-seasonal pattern from a\nprice series extrapolated using price forecasts for the next 24 hours. We\nassess it using two 5-year long test periods from the German and Spanish power\nmarkets, covering the Covid-19 pandemic, the 2021/2022 energy crisis, and the\nwar in Ukraine. Considering parsimonious autoregressive and LASSO-estimated\nmodels, we find that improvements in predictive accuracy range from 3\\% to 15\\%\nin terms of the root mean squared error and exceed 1\\% in terms of profits from\na realistic trading strategy involving day-ahead bidding and battery storage.", "published": "2025-03-04 11:30:24", "link": "http://arxiv.org/abs/2503.02518v1", "categories": ["q-fin.ST", "econ.EM", "q-fin.RM"], "primary_category": "q-fin.ST"}
{"title": "To Hedge or Not to Hedge: Optimal Strategies for Stochastic Trade Flow Management", "abstract": "This paper addresses the trade-off between internalisation and\nexternalisation in the management of stochastic trade flows. We consider agents\nwho must absorb flows and manage risk by deciding whether to warehouse it or\nhedge in the market, thereby incurring transaction costs and market impact.\nUnlike market makers, these agents cannot skew their quotes to attract\noffsetting flows and deter risk-increasing ones, leading to a fundamentally\ndifferent problem. Within the Almgren-Chriss framework, we derive\nalmost-closed-form solutions in the case of quadratic execution costs, while\nmore general cases require numerical methods. In particular, we discuss the\nchallenges posed by artificial boundary conditions when using classical\ngrid-based numerical PDE techniques and propose reinforcement learning methods\nas an alternative.", "published": "2025-03-04 11:03:10", "link": "http://arxiv.org/abs/2503.02496v1", "categories": ["q-fin.TR"], "primary_category": "q-fin.TR"}
