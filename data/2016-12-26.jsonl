{"title": "Text Summarization using Deep Learning and Ridge Regression", "abstract": "We develop models and extract relevant features for automatic text\nsummarization and investigate the performance of different models on the DUC\n2001 dataset. Two different models were developed, one being a ridge regressor\nand the other one was a multi-layer perceptron. The hyperparameters were varied\nand their performance were noted. We segregated the summarization task into 2\nmain steps, the first being sentence ranking and the second step being sentence\nselection. In the first step, given a document, we sort the sentences based on\ntheir Importance, and in the second step, in order to obtain non-redundant\nsentences, we weed out the sentences that are have high similarity with the\npreviously selected sentences.", "published": "2016-12-26 07:17:30", "link": "http://arxiv.org/abs/1612.08333v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Abstractive Headline Generation for Spoken Content by Attentive\n  Recurrent Neural Networks with ASR Error Modeling", "abstract": "Headline generation for spoken content is important since spoken content is\ndifficult to be shown on the screen and browsed by the user. It is a special\ntype of abstractive summarization, for which the summaries are generated word\nby word from scratch without using any part of the original content. Many deep\nlearning approaches for headline generation from text document have been\nproposed recently, all requiring huge quantities of training data, which is\ndifficult for spoken document summarization. In this paper, we propose an ASR\nerror modeling approach to learn the underlying structure of ASR error patterns\nand incorporate this model in an Attentive Recurrent Neural Network (ARNN)\narchitecture. In this way, the model for abstractive headline generation for\nspoken content can be learned from abundant text data and the ASR data for some\nrecognizers. Experiments showed very encouraging results and verified that the\nproposed ASR error model works well even when the input spoken content is\nrecognized by a recognizer very different from the one the model learned from.", "published": "2016-12-26 13:13:38", "link": "http://arxiv.org/abs/1612.08375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Image-Text Multi-Modal Representation Learning by Adversarial\n  Backpropagation", "abstract": "We present novel method for image-text multi-modal representation learning.\nIn our knowledge, this work is the first approach of applying adversarial\nlearning concept to multi-modal learning and not exploiting image-text pair\ninformation to learn multi-modal feature. We only use category information in\ncontrast with most previous methods using image-text pair information for\nmulti-modal embedding. In this paper, we show that multi-modal feature can be\nachieved without image-text pair information and our method makes more similar\ndistribution with image and text in multi-modal feature space than other\nmethods which use image-text pair information. And we show our multi-modal\nfeature has universal semantic information, even though it was trained for\ncategory prediction. Our model is end-to-end backpropagation, intuitive and\neasily extended to other multi-modal learning work.", "published": "2016-12-26 09:51:18", "link": "http://arxiv.org/abs/1612.08354v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
