{"title": "A Morphology-aware Network for Morphological Disambiguation", "abstract": "Agglutinative languages such as Turkish, Finnish and Hungarian require\nmorphological disambiguation before further processing due to the complex\nmorphology of words. A morphological disambiguator is used to select the\ncorrect morphological analysis of a word. Morphological disambiguation is\nimportant because it generally is one of the first steps of natural language\nprocessing and its performance affects subsequent analyses. In this paper, we\npropose a system that uses deep learning techniques for morphological\ndisambiguation. Many of the state-of-the-art results in computer vision, speech\nrecognition and natural language processing have been obtained through deep\nlearning models. However, applying deep learning techniques to morphologically\nrich languages is not well studied. In this work, while we focus on Turkish\nmorphological disambiguation we also present results for French and German in\norder to show that the proposed architecture achieves high accuracy with no\nlanguage-specific feature engineering or additional resource. In the\nexperiments, we achieve 84.12, 88.35 and 93.78 morphological disambiguation\naccuracy among the ambiguous words for Turkish, German and French respectively.", "published": "2017-02-13 07:08:28", "link": "http://arxiv.org/abs/1702.03654v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Learning with Deep Neural Networks for Community Question\n  Answering", "abstract": "In this paper, we developed a deep neural network (DNN) that learns to solve\nsimultaneously the three tasks of the cQA challenge proposed by the\nSemEval-2016 Task 3, i.e., question-comment similarity, question-question\nsimilarity and new question-comment similarity. The latter is the main task,\nwhich can exploit the previous two for achieving better results. Our DNN is\ntrained jointly on all the three cQA tasks and learns to encode questions and\ncomments into a single vector representation shared across the multiple tasks.\nThe results on the official challenge test set show that our approach produces\nhigher accuracy and faster convergence rates than the individual neural\nnetworks. Additionally, our method, which does not use any manual feature\nengineering, approaches the state of the art established with methods that make\nheavy use of it.", "published": "2017-02-13 10:24:55", "link": "http://arxiv.org/abs/1702.03706v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards speech-to-text translation without speech recognition", "abstract": "We explore the problem of translating speech to text in low-resource\nscenarios where neither automatic speech recognition (ASR) nor machine\ntranslation (MT) are available, but we have training data in the form of audio\npaired with text translations. We present the first system for this problem\napplied to a realistic multi-speaker dataset, the CALLHOME Spanish-English\nspeech translation corpus. Our approach uses unsupervised term discovery (UTD)\nto cluster repeated patterns in the audio, creating a pseudotext, which we pair\nwith translations to create a parallel text and train a simple bag-of-words MT\nmodel. We identify the challenges faced by the system, finding that the\ndifficulty of cross-speaker UTD results in low recall, but that our system is\nstill able to correctly translate some content words in test data.", "published": "2017-02-13 16:30:23", "link": "http://arxiv.org/abs/1702.03856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations\n  Annotated with Compositional Meaning Representations", "abstract": "The Parallel Meaning Bank is a corpus of translations annotated with shared,\nformal meaning representations comprising over 11 million words divided over\nfour languages (English, German, Italian, and Dutch). Our approach is based on\ncross-lingual projection: automatically produced (and manually corrected)\nsemantic annotations for English sentences are mapped onto their word-aligned\ntranslations, assuming that the translations are meaning-preserving. The\nsemantic annotation consists of five main steps: (i) segmentation of the text\nin sentences and lexical items; (ii) syntactic parsing with Combinatory\nCategorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and\n(v) compositional semantic analysis based on Discourse Representation Theory.\nThese steps are performed using statistical models trained in a semi-supervised\nmanner. The employed annotation models are all language-neutral. Our first\nresults are promising.", "published": "2017-02-13 19:52:02", "link": "http://arxiv.org/abs/1702.03964v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bilateral Multi-Perspective Matching for Natural Language Sentences", "abstract": "Natural language sentence matching is a fundamental technology for a variety\nof tasks. Previous approaches either match sentences from a single direction or\nonly apply single granular (word-by-word or sentence-by-sentence) matching. In\nthis work, we propose a bilateral multi-perspective matching (BiMPM) model\nunder the \"matching-aggregation\" framework. Given two sentences $P$ and $Q$,\nour model first encodes them with a BiLSTM encoder. Next, we match the two\nencoded sentences in two directions $P \\rightarrow Q$ and $P \\leftarrow Q$. In\neach matching direction, each time step of one sentence is matched against all\ntime-steps of the other sentence from multiple perspectives. Then, another\nBiLSTM layer is utilized to aggregate the matching results into a fix-length\nmatching vector. Finally, based on the matching vector, the decision is made\nthrough a fully connected layer. We evaluate our model on three tasks:\nparaphrase identification, natural language inference and answer sentence\nselection. Experimental results on standard benchmark datasets show that our\nmodel achieves the state-of-the-art performance on all tasks.", "published": "2017-02-13 15:26:27", "link": "http://arxiv.org/abs/1702.03814v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Offline bilingual word vectors, orthogonal transformations and the\n  inverted softmax", "abstract": "Usually bilingual word vectors are trained \"online\". Mikolov et al. showed\nthey can also be found \"offline\", whereby two pre-trained embeddings are\naligned with a linear transformation, using dictionaries compiled from expert\nknowledge. In this work, we prove that the linear transformation between two\nspaces should be orthogonal. This transformation can be obtained using the\nsingular value decomposition. We introduce a novel \"inverted softmax\" for\nidentifying translation pairs, with which we improve the precision @1 of\nMikolov's original mapping from 34% to 43%, when translating a test set\ncomposed of both common and rare English words into Italian. Orthogonal\ntransformations are more robust to noise, enabling us to learn the\ntransformation without expert bilingual signal by constructing a\n\"pseudo-dictionary\" from the identical character strings which appear in both\nlanguages, achieving 40% precision on the same test set. Finally, we extend our\nmethod to retrieve the true translations of English sentences from a corpus of\n200k Italian sentences with a precision @1 of 68%.", "published": "2017-02-13 16:31:06", "link": "http://arxiv.org/abs/1702.03859v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
