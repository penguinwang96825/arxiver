{"title": "Linguistic generalization and compositionality in modern artificial\n  neural networks", "abstract": "In the last decade, deep artificial neural networks have achieved astounding\nperformance in many natural language processing tasks. Given the high\nproductivity of language, these models must possess effective generalization\nabilities. It is widely assumed that humans handle linguistic productivity by\nmeans of algebraic compositional rules: Are deep networks similarly\ncompositional? After reviewing the main innovations characterizing current deep\nlanguage processing networks, I discuss a set of studies suggesting that deep\nnetworks are capable of subtle grammar-dependent generalizations, but also that\nthey do not rely on systematic compositional rules. I argue that the intriguing\nbehaviour of these devices (still awaiting a full understanding) should be of\ninterest to linguists and cognitive scientists, as it offers a new perspective\non possible computational strategies to deal with linguistic productivity\nbeyond rule-based compositionality, and it might lead to new insights into the\nless systematic generalization patterns that also appear in natural language.", "published": "2019-03-30 06:48:32", "link": "http://arxiv.org/abs/1904.00157v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag\n  Attentions", "abstract": "This paper presents a neural relation extraction method to deal with the\nnoisy training data generated by distant supervision. Previous studies mainly\nfocus on sentence-level de-noising by designing neural networks with intra-bag\nattentions. In this paper, both intra-bag and inter-bag attentions are\nconsidered in order to deal with the noise at sentence-level and bag-level\nrespectively. First, relation-aware bag representations are calculated by\nweighting sentence embeddings using intra-bag attentions. Here, each possible\nrelation is utilized as the query for attention calculation instead of only\nusing the target relation in conventional methods. Furthermore, the\nrepresentation of a group of bags in the training set which share the same\nrelation label is calculated by weighting bag representations using a\nsimilarity-based inter-bag attention module. Finally, a bag group is utilized\nas a training sample when building our relation extractor. Experimental results\non the New York Times dataset demonstrate the effectiveness of our proposed\nintra-bag and inter-bag attention modules. Our method also achieves better\nrelation extraction accuracy than state-of-the-art methods on this dataset.", "published": "2019-03-30 03:55:20", "link": "http://arxiv.org/abs/1904.00143v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Machine translation considering context information using\n  Encoder-Decoder model", "abstract": "In the task of machine translation, context information is one of the\nimportant factor. But considering the context information model dose not\nproposed. The paper propose a new model which can integrate context information\nand make translation. In this paper, we create a new model based Encoder\nDecoder model. When translating current sentence, the model integrates output\nfrom preceding encoder with current encoder. The model can consider context\ninformation and the result score is higher than existing model.", "published": "2019-03-30 07:13:38", "link": "http://arxiv.org/abs/1904.00160v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ANA at SemEval-2019 Task 3: Contextual Emotion detection in\n  Conversations through hierarchical LSTMs and BERT", "abstract": "This paper describes the system submitted by ANA Team for the SemEval-2019\nTask 3: EmoContext. We propose a novel Hierarchical LSTMs for Contextual\nEmotion Detection (HRLCE) model. It classifies the emotion of an utterance\ngiven its conversational context. The results show that, in this task, our\nHRCLE outperforms the most recent state-of-the-art text classification\nframework: BERT. We combine the results generated by BERT and HRCLE to achieve\nan overall score of 0.7709 which ranked 5th on the final leader board of the\ncompetition among 165 Teams.", "published": "2019-03-30 01:51:24", "link": "http://arxiv.org/abs/1904.00132v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Static Visual Spatial Priors for DoA Estimation", "abstract": "As we interact with the world, for example when we communicate with our\ncolleagues in a large open space or meeting room, we continuously analyse the\nsurrounding environment and, in particular, localise and recognise acoustic\nevents. While we largely take such abilities for granted, they represent a\nchallenging problem for current robots or smart voice assistants as they can be\neasily fooled by high degree of sound interference in acoustically complex\nenvironments. Preventing such failures when using solely audio data is\nchallenging, if not impossible since the algorithms need to take into account\nwider context and often understand the scene on a semantic level. In this\npaper, we propose what to our knowledge is the first multi-modal direction of\narrival (DoA) of sound, which uses static visual spatial prior providing an\nauxiliary information about the environment to suppress some of the false DoA\ndetections. We validate our approach on a newly collected real-world dataset,\nand show that our approach consistently improves over classic DoA baselines", "published": "2019-03-30 11:34:35", "link": "http://arxiv.org/abs/1904.00202v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Learning Affective Correspondence between Music and Image", "abstract": "We introduce the problem of learning affective correspondence between audio\n(music) and visual data (images). For this task, a music clip and an image are\nconsidered similar (having true correspondence) if they have similar emotion\ncontent. In order to estimate this crossmodal, emotion-centric similarity, we\npropose a deep neural network architecture that learns to project the data from\nthe two modalities to a common representation space, and performs a binary\nclassification task of predicting the affective correspondence (true or false).\nTo facilitate the current study, we construct a large scale database containing\nmore than $3,500$ music clips and $85,000$ images with three emotion classes\n(positive, neutral, negative). The proposed approach achieves $61.67\\%$\naccuracy for the affective correspondence prediction task on this database,\noutperforming two relevant and competitive baselines. We also demonstrate that\nour network learns modality-specific representations of emotion (without\nexplicitly being trained with emotion labels), which are useful for emotion\nrecognition in individual modalities.", "published": "2019-03-30 05:17:27", "link": "http://arxiv.org/abs/1904.00150v2", "categories": ["cs.MM", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
