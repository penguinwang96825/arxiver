{"title": "MIDAS at SemEval-2020 Task 10: Emphasis Selection using Label\n  Distribution Learning and Contextual Embeddings", "abstract": "This paper presents our submission to the SemEval 2020 - Task 10 on emphasis\nselection in written text. We approach this emphasis selection problem as a\nsequence labeling task where we represent the underlying text with various\ncontextual embedding models. We also employ label distribution learning to\naccount for annotator disagreements. We experiment with the choice of model\narchitectures, trainability of layers, and different contextual embeddings. Our\nbest performing architecture is an ensemble of different models, which achieved\nan overall matching score of 0.783, placing us 15th out of 31 participating\nteams. Lastly, we analyze the results in terms of parts of speech tags,\nsentence lengths, and word ordering.", "published": "2020-09-06 00:15:33", "link": "http://arxiv.org/abs/2009.02619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Once Upon A Time In Visualization: Understanding the Use of Textual\n  Narratives for Causality", "abstract": "Causality visualization can help people understand temporal chains of events,\nsuch as messages sent in a distributed system, cause and effect in a historical\nconflict, or the interplay between political actors over time. However, as the\nscale and complexity of these event sequences grows, even these visualizations\ncan become overwhelming to use. In this paper, we propose the use of textual\nnarratives as a data-driven storytelling method to augment causality\nvisualization. We first propose a design space for how textual narratives can\nbe used to describe causal data. We then present results from a crowdsourced\nuser study where participants were asked to recover causality information from\ntwo causality visualizations--causal graphs and Hasse diagrams--with and\nwithout an associated textual narrative. Finally, we describe CAUSEWORKS, a\ncausality visualization system for understanding how specific interventions\ninfluence a causal model. The system incorporates an automatic textual\nnarrative mechanism based on our design space. We validate CAUSEWORKS through\ninterviews with experts who used the system for understanding complex events.", "published": "2020-09-06 05:46:24", "link": "http://arxiv.org/abs/2009.02649v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Dialect Adaptation in Finnish and its Effect on Perceived\n  Creativity", "abstract": "We present a novel approach for adapting text written in standard Finnish to\ndifferent dialects. We experiment with character level NMT models both by using\na multi-dialectal and transfer learning approaches. The models are tested with\nover 20 different dialects. The results seem to favor transfer learning,\nalthough not strongly over the multi-dialectal approach. We study the influence\ndialectal adaptation has on perceived creativity of computer generated poetry.\nOur results suggest that the more the dialect deviates from the standard\nFinnish, the lower scores people tend to give on an existing evaluation metric.\nHowever, on a word association test, people associate creativity and\noriginality more with dialect and fluency more with standard Finnish.", "published": "2020-09-06 09:28:44", "link": "http://arxiv.org/abs/2009.02685v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UPB at SemEval-2020 Task 8: Joint Textual and Visual Modeling in a\n  Multi-Task Learning Architecture for Memotion Analysis", "abstract": "Users from the online environment can create different ways of expressing\ntheir thoughts, opinions, or conception of amusement. Internet memes were\ncreated specifically for these situations. Their main purpose is to transmit\nideas by using combinations of images and texts such that they will create a\ncertain state for the receptor, depending on the message the meme has to send.\nThese posts can be related to various situations or events, thus adding a funny\nside to any circumstance our world is situated in. In this paper, we describe\nthe system developed by our team for SemEval-2020 Task 8: Memotion Analysis.\nMore specifically, we introduce a novel system to analyze these posts, a\nmultimodal multi-task learning architecture that combines ALBERT for text\nencoding with VGG-16 for image representation. In this manner, we show that the\ninformation behind them can be properly revealed. Our approach achieves good\nperformance on each of the three subtasks of the current competition, ranking\n11th for Subtask A (0.3453 macro F1-score), 1st for Subtask B (0.5183 macro\nF1-score), and 3rd for Subtask C (0.3171 macro F1-score) while exceeding the\nofficial baseline results by high margins.", "published": "2020-09-06 17:17:41", "link": "http://arxiv.org/abs/2009.02779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UPB at SemEval-2020 Task 9: Identifying Sentiment in Code-Mixed Social\n  Media Texts using Transformers and Multi-Task Learning", "abstract": "Sentiment analysis is a process widely used in opinion mining campaigns\nconducted today. This phenomenon presents applications in a variety of fields,\nespecially in collecting information related to the attitude or satisfaction of\nusers concerning a particular subject. However, the task of managing such a\nprocess becomes noticeably more difficult when it is applied in cultures that\ntend to combine two languages in order to express ideas and thoughts. By\ninterleaving words from two languages, the user can express with ease, but at\nthe cost of making the text far less intelligible for those who are not\nfamiliar with this technique, but also for standard opinion mining algorithms.\nIn this paper, we describe the systems developed by our team for SemEval-2020\nTask 9 that aims to cover two well-known code-mixed languages: Hindi-English\nand Spanish-English.\n  We intend to solve this issue by introducing a solution that takes advantage\nof several neural network approaches, as well as pre-trained word embeddings.\nOur approach (multlingual BERT) achieves promising performance on the\nHindi-English task, with an average F1-score of 0.6850, registered on the\ncompetition leaderboard, ranking our team 16th out of 62 participants. For the\nSpanish-English task, we obtained an average F1-score of 0.7064 ranking our\nteam 17th out of 29 participants by using another multilingual\nTransformer-based model, XLM-RoBERTa.", "published": "2020-09-06 17:19:18", "link": "http://arxiv.org/abs/2009.02780v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duluth at SemEval-2020 Task 7: Using Surprise as a Key to Unlock\n  Humorous Headlines", "abstract": "We use pretrained transformer-based language models in SemEval-2020 Task 7:\nAssessing the Funniness of Edited News Headlines. Inspired by the incongruity\ntheory of humor, we use a contrastive approach to capture the surprise in the\nedited headlines. In the official evaluation, our system gets 0.531 RMSE in\nSubtask 1, 11th among 49 submissions. In Subtask 2, our system gets 0.632\naccuracy, 9th among 32 submissions.", "published": "2020-09-06 18:34:54", "link": "http://arxiv.org/abs/2009.02795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QiaoNing at SemEval-2020 Task 4: Commonsense Validation and Explanation\n  system based on ensemble of language model", "abstract": "In this paper, we present language model system submitted to SemEval-2020\nTask 4 competition: \"Commonsense Validation and Explanation\". We participate in\ntwo subtasks for subtask A: validation and subtask B: Explanation. We\nimplemented with transfer learning using pretrained language models (BERT,\nXLNet, RoBERTa, and ALBERT) and fine-tune them on this task. Then we compared\ntheir characteristics in this task to help future researchers understand and\nuse these models more properly. The ensembled model better solves this problem,\nmaking the model's accuracy reached 95.9% on subtask A, which just worse than\nhuman's by only 3% accuracy.", "published": "2020-09-06 05:12:50", "link": "http://arxiv.org/abs/2009.02645v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BANANA at WNUT-2020 Task 2: Identifying COVID-19 Information on Twitter\n  by Combining Deep Learning and Transfer Learning Models", "abstract": "The outbreak COVID-19 virus caused a significant impact on the health of\npeople all over the world. Therefore, it is essential to have a piece of\nconstant and accurate information about the disease with everyone. This paper\ndescribes our prediction system for WNUT-2020 Task 2: Identification of\nInformative COVID-19 English Tweets. The dataset for this task contains size\n10,000 tweets in English labeled by humans. The ensemble model from our three\ntransformer and deep learning models is used for the final prediction. The\nexperimental result indicates that we have achieved F1 for the INFORMATIVE\nlabel on our systems at 88.81% on the test set.", "published": "2020-09-06 08:24:55", "link": "http://arxiv.org/abs/2009.02671v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SemEval-2020 Task 11: Detection of Propaganda Techniques in News\n  Articles", "abstract": "We present the results and the main findings of SemEval-2020 Task 11 on\nDetection of Propaganda Techniques in News Articles. The task featured two\nsubtasks. Subtask SI is about Span Identification: given a plain-text document,\nspot the specific text fragments containing propaganda. Subtask TC is about\nTechnique Classification: given a specific text fragment, in the context of a\nfull document, determine the propaganda technique it uses, choosing from an\ninventory of 14 possible propaganda techniques. The task attracted a large\nnumber of participants: 250 teams signed up to participate and 44 made a\nsubmission on the test set. In this paper, we present the task, analyze the\nresults, and discuss the system submissions and the methods they used. For both\nsubtasks, the best systems used pre-trained Transformers and ensembles.", "published": "2020-09-06 10:05:43", "link": "http://arxiv.org/abs/2009.02696v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Romanian Diacritics Restoration Using Recurrent Neural Networks", "abstract": "Diacritics restoration is a mandatory step for adequately processing Romanian\ntexts, and not a trivial one, as you generally need context in order to\nproperly restore a character. Most previous methods which were experimented for\nRomanian restoration of diacritics do not use neural networks. Among those that\ndo, there are no solutions specifically optimized for this particular language\n(i.e., they were generally designed to work on many different languages).\nTherefore we propose a novel neural architecture based on recurrent neural\nnetworks that can attend information at different levels of abstractions in\norder to restore diacritics.", "published": "2020-09-06 14:20:35", "link": "http://arxiv.org/abs/2009.02743v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence\n  Modeling", "abstract": "This paper proposes an any-to-many location-relative, sequence-to-sequence\n(seq2seq), non-parallel voice conversion approach, which utilizes text\nsupervision during training. In this approach, we combine a bottle-neck feature\nextractor (BNE) with a seq2seq synthesis module. During the training stage, an\nencoder-decoder-based hybrid connectionist-temporal-classification-attention\n(CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck\nlayer. A BNE is obtained from the phoneme recognizer and is utilized to extract\nspeaker-independent, dense and rich spoken content representations from\nspectral features. Then a multi-speaker location-relative attention based\nseq2seq synthesis model is trained to reconstruct spectral features from the\nbottle-neck features, conditioning on speaker representations for speaker\nidentity control in the generated speech. To mitigate the difficulties of using\nseq2seq models to align long sequences, we down-sample the input spectral\nfeature along the temporal dimension and equip the synthesis model with a\ndiscretized mixture of logistic (MoL) attention mechanism. Since the phoneme\nrecognizer is trained with large speech recognition data corpus, the proposed\napproach can conduct any-to-many voice conversion. Objective and subjective\nevaluations show that the proposed any-to-many approach has superior voice\nconversion performance in terms of both naturalness and speaker similarity.\nAblation studies are conducted to confirm the effectiveness of feature\nselection and model design strategies in the proposed approach. The proposed VC\napproach can readily be extended to support any-to-any VC (also known as\none/few-shot VC), and achieve high performance according to objective and\nsubjective evaluations.", "published": "2020-09-06 13:01:06", "link": "http://arxiv.org/abs/2009.02725v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with\n  Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets", "abstract": "Twitter and, in general, social media has become an indispensable\ncommunication channel in times of emergency. The ubiquitousness of smartphone\ngadgets enables people to declare an emergency observed in real-time. As a\nresult, more agencies are interested in programmatically monitoring Twitter\n(disaster relief organizations and news agencies). Therefore, recognizing the\ninformativeness of a Tweet can help filter noise from the large volumes of\nTweets. In this paper, we present our submission for WNUT-2020 Task 2:\nIdentification of informative COVID-19 English Tweets. Our most successful\nmodel is an ensemble of transformers, including RoBERTa, XLNet, and BERTweet\ntrained in a Semi-Supervised Learning (SSL) setting. The proposed system\nachieves an F1 score of 0.9011 on the test set (ranking 7th on the leaderboard)\nand shows significant gains in performance compared to a baseline system using\nFastText embeddings.", "published": "2020-09-06 15:57:28", "link": "http://arxiv.org/abs/2009.06375v3", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Overview and Evaluation of Sound Event Localization and Detection in\n  DCASE 2019", "abstract": "Sound event localization and detection is a novel area of research that\nemerged from the combined interest of analyzing the acoustic scene in terms of\nthe spatial and temporal activity of sounds of interest. This paper presents an\noverview of the first international evaluation on sound event localization and\ndetection, organized as a task of the DCASE 2019 Challenge. A large-scale\nrealistic dataset of spatialized sound events was generated for the challenge,\nto be used for training of learning-based approaches, and for evaluation of the\nsubmissions in an unlabeled subset. The overview presents in detail how the\nsystems were evaluated and ranked and the characteristics of the\nbest-performing systems. Common strategies in terms of input features, model\narchitectures, training approaches, exploitation of prior knowledge, and data\naugmentation are discussed. Since ranking in the challenge was based on\nindividually evaluating localization and event classification performance, part\nof the overview focuses on presenting metrics for the joint measurement of the\ntwo, together with a reevaluation of submissions using these new metrics. The\nnew analysis reveals submissions that performed better on the joint task of\ndetecting the correct type of event close to its original location than some of\nthe submissions that were ranked higher in the challenge. Consequently, ranking\nof submissions which performed strongly when evaluated separately on detection\nor localization, but not jointly on both, was affected negatively.", "published": "2020-09-06 18:15:14", "link": "http://arxiv.org/abs/2009.02792v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Non causal deep learning based dereverberation", "abstract": "In this paper we demonstrate the effectiveness of non-causal context for\nmitigating the effects of reverberation in deep-learning-based automatic speech\nrecognition (ASR) systems. First, the value of non-causal context using a\nnon-causal FIR filter is shown by comparing the contributions of previous vs.\nfuture information. Second, MLP- and LSTM-based dereverberation networks were\ntrained to confirm the effects of causal and non-causal context when used in\nASR systems trained with clean speech. The non-causal deep-learning-based\ndereverberation provides a 45% relative reduction in word error rate (WER)\ncompared to the popular weighted prediction error (WPE) method in experiments\nwith clean training in the REVERB challenge. Finally, an expanded\nmulticondition training procedure used in combination with a semi-enhanced test\nutterance generation based on combinations of reverberated and dereverberated\nsignals is proposed to reduce any artifacts or distortion that may be\nintroduced by the non-causal dereverberation methods. The combination of both\napproaches provided average relative reductions in WER equal to 10.9% and 6.0%\nwhen compared to the baseline system obtained with the most recent REVERB\nchallenge recipe without and with WPE, respectively.", "published": "2020-09-06 23:52:59", "link": "http://arxiv.org/abs/2009.02832v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Comparison of Virtual Analog Modelling Techniques for Desktop and\n  Embedded Implementations", "abstract": "We develop a virtual analog model of the Klon Centaur guitar pedal circuit,\ncomparing various circuit modelling techniques. The techniques analyzed include\ntraditional modelling techniques such as nodal analysis and Wave Digital\nFilters, as well as a machine learning technique using recurrent neural\nnetworks. We examine these techniques in the contexts of two use cases: an\naudio plug-in designed to be run on a consumer-grade desktop computer, and a\nguitar pedal-style effect running on an embedded device. Finally, we discuss\nthe advantages and disdvantages of each technique for modelling different\ncircuits, and targeting different platforms.", "published": "2020-09-06 23:56:41", "link": "http://arxiv.org/abs/2009.02833v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Libri-Adapt: A New Speech Dataset for Unsupervised Domain Adaptation", "abstract": "This paper introduces a new dataset, Libri-Adapt, to support unsupervised\ndomain adaptation research on speech recognition models. Built on top of the\nLibriSpeech corpus, Libri-Adapt contains English speech recorded on mobile and\nembedded-scale microphones, and spans 72 different domains that are\nrepresentative of the challenging practical scenarios encountered by ASR\nmodels. More specifically, Libri-Adapt facilitates the study of domain shifts\nin ASR models caused by a) different acoustic environments, b) variations in\nspeaker accents, c) heterogeneity in the hardware and platform software of the\nmicrophones, and d) a combination of the aforementioned three shifts. We also\nprovide a number of baseline results quantifying the impact of these domain\nshifts on the Mozilla DeepSpeech2 ASR model.", "published": "2020-09-06 20:47:20", "link": "http://arxiv.org/abs/2009.02814v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
