{"title": "Creating Suspenseful Stories: Iterative Planning with Large Language\n  Models", "abstract": "Automated story generation has been one of the long-standing challenges in\nNLP. Among all dimensions of stories, suspense is very common in human-written\nstories but relatively under-explored in AI-generated stories. While recent\nadvances in large language models (LLMs) have greatly promoted language\ngeneration in general, state-of-the-art LLMs are still unreliable when it comes\nto suspenseful story generation. We propose a novel iterative-prompting-based\nplanning method that is grounded in two theoretical foundations of story\nsuspense from cognitive psychology and narratology. This theory-grounded method\nworks in a fully zero-shot manner and does not rely on any supervised story\ncorpora. To the best of our knowledge, this paper is the first attempt at\nsuspenseful story generation with LLMs. Extensive human evaluations of the\ngenerated suspenseful stories demonstrate the effectiveness of our method.", "published": "2024-02-27 01:25:52", "link": "http://arxiv.org/abs/2402.17119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large\n  Language Models", "abstract": "For a LLM to be trustworthy, its confidence level should be well-calibrated\nwith its actual performance. While it is now common sense that LLM performances\nare greatly impacted by prompts, the confidence calibration in prompting LLMs\nhas yet to be thoroughly explored. In this paper, we explore how different\nprompting strategies influence LLM confidence calibration and how it could be\nimproved. We conduct extensive experiments on six prompting methods in the\nquestion-answering context and we observe that, while these methods help\nimprove the expected LLM calibration, they also trigger LLMs to be\nover-confident when responding to some instances. Inspired by human cognition,\nwe propose Fact-and-Reflection (FaR) prompting, which improves the LLM\ncalibration in two steps. First, FaR elicits the known \"facts\" that are\nrelevant to the input prompt from the LLM. And then it asks the model to\n\"reflect\" over them to generate the final answer. Experiments show that FaR\nprompting achieves significantly better calibration; it lowers the Expected\nCalibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR\nprompting even elicits the capability of verbally expressing concerns in less\nconfident scenarios, which helps trigger retrieval augmentation for solving\nthese harder instances.", "published": "2024-02-27 01:37:23", "link": "http://arxiv.org/abs/2402.17124v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering Document Parts: Detecting and Characterizing Influence\n  Campaigns from Documents", "abstract": "We propose a novel clustering pipeline to detect and characterize influence\ncampaigns from documents. This approach clusters parts of document, detects\nclusters that likely reflect an influence campaign, and then identifies\ndocuments linked to an influence campaign via their association with the\nhigh-influence clusters. Our approach outperforms both the direct\ndocument-level classification and the direct document-level clustering approach\nin predicting if a document is part of an influence campaign. We propose\nvarious novel techniques to enhance our pipeline, including using an existing\nevent factuality prediction system to obtain document parts, and aggregating\nmultiple clustering experiments to improve the performance of both cluster and\ndocument classification. Classifying documents after clustering not only\naccurately extracts the parts of the documents that are relevant to influence\ncampaigns, but also captures influence campaigns as a coordinated and holistic\nphenomenon. Our approach makes possible more fine-grained and interpretable\ncharacterizations of influence campaigns from documents.", "published": "2024-02-27 02:36:43", "link": "http://arxiv.org/abs/2402.17151v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue\n  Simulation for Large Language Models", "abstract": "Large Language Models (LLMs) have achieved remarkable performance in\nobjective tasks such as open-domain question answering and mathematical\nreasoning, which can often be solved through recalling learned factual\nknowledge or chain-of-thought style reasoning. However, we find that the\nperformance of LLMs in subjective tasks is still unsatisfactory, such as\nmetaphor recognition, dark humor detection, etc. Compared to objective tasks,\nsubjective tasks focus more on interpretation or emotional response rather than\na universally accepted reasoning pathway. Based on the characteristics of the\ntasks and the strong dialogue-generation capabilities of LLMs, we propose RiC\n(Reasoning in Conversation), a method that focuses on solving subjective tasks\nthrough dialogue simulation. The motivation of RiC is to mine useful contextual\ninformation by simulating dialogues instead of supplying chain-of-thought style\nrationales, thereby offering potential useful knowledge behind dialogues for\ngiving the final answers. We evaluate both API-based and open-source LLMs\nincluding GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental\nresults show that RiC can yield significant improvement compared with various\nbaselines.", "published": "2024-02-27 05:37:10", "link": "http://arxiv.org/abs/2402.17226v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical\n  Reasoning", "abstract": "Tool-augmented Large Language Models (TALMs) are known to enhance the\nskillset of large language models (LLMs), thereby, leading to their improved\nreasoning abilities across many tasks. While, TALMs have been successfully\nemployed in different question-answering benchmarks, their efficacy on complex\nmathematical reasoning benchmarks, and the potential complementary benefits\noffered by tools for knowledge retrieval and mathematical equation solving are\nopen research questions. In this work, we present MathSensei, a tool-augmented\nlarge language model for mathematical reasoning. We study the complementary\nbenefits of the tools - knowledge retriever (Bing Web Search), program\ngenerator + executor (Python), and symbolic equation solver (Wolfram-Alpha API)\nthrough evaluations on mathematical reasoning datasets. We perform exhaustive\nablations on MATH, a popular dataset for evaluating mathematical reasoning on\ndiverse mathematical disciplines. We also conduct experiments involving\nwell-known tool planners to study the impact of tool sequencing on the model\nperformance. MathSensei achieves 13.5% better accuracy over gpt-3.5-turbo with\nChain-of-Thought on the MATH dataset. We further observe that TALMs are not as\neffective for simpler math word problems (in GSM-8K), and the benefit increases\nas the complexity and required knowledge increases (progressively over AQuA,\nMMLU-Math, and higher level complex questions in MATH). The code and data are\navailable at https://github.com/Debrup-61/MathSensei.", "published": "2024-02-27 05:50:35", "link": "http://arxiv.org/abs/2402.17231v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent\n  Detection", "abstract": "Out-of-domain (OOD) intent detection aims to examine whether the user's query\nfalls outside the predefined domain of the system, which is crucial for the\nproper functioning of task-oriented dialogue (TOD) systems. Previous methods\naddress it by fine-tuning discriminative models. Recently, some studies have\nbeen exploring the application of large language models (LLMs) represented by\nChatGPT to various downstream tasks, but it is still unclear for their ability\non OOD detection task.This paper conducts a comprehensive evaluation of LLMs\nunder various experimental settings, and then outline the strengths and\nweaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot\ncapabilities, but is still at a disadvantage compared to models fine-tuned with\nfull resource. More deeply, through a series of additional analysis\nexperiments, we discuss and summarize the challenges faced by LLMs and provide\nguidance for future work including injecting domain knowledge, strengthening\nknowledge transfer from IND(In-domain) to OOD, and understanding long\ninstructions.", "published": "2024-02-27 07:02:10", "link": "http://arxiv.org/abs/2402.17256v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient\n  Fine-Tuning", "abstract": "Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring\npre-trained large language models (LLMs), especially as the models' scale and\nthe diversity of tasks increase. Low-rank adaptation (LoRA) is based on the\nidea that the adaptation process is intrinsically low-dimensional, i.e.,\nsignificant model changes can be represented with relatively few parameters.\nHowever, decreasing the rank encounters challenges with generalization errors\nfor specific tasks when compared to full-parameter fine-tuning. We present\nMELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters\nwhile maintaining a higher rank, thereby offering improved performance\npotential. The core idea is to freeze original pretrained weights and train a\ngroup of mini LoRAs with only a small number of parameters. This can capture a\nsignificant degree of diversity among mini LoRAs, thus promoting better\ngeneralization ability. We conduct a theoretical analysis and empirical studies\non various NLP tasks. Our experimental results show that, compared to LoRA,\nMELoRA achieves better performance with 8 times fewer trainable parameters on\nnatural language understanding tasks and 36 times fewer trainable parameters on\ninstruction following tasks, which demonstrates the effectiveness of MELoRA.", "published": "2024-02-27 07:14:12", "link": "http://arxiv.org/abs/2402.17263v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in\n  Indonesian and Sundanese", "abstract": "Large Language Models (LLMs) are increasingly being used to generate\nsynthetic data for training and evaluating models. However, it is unclear\nwhether they can generate a good quality of question answering (QA) dataset\nthat incorporates knowledge and cultural nuance embedded in a language,\nespecially for low-resource languages. In this study, we investigate the\neffectiveness of using LLMs in generating culturally relevant commonsense QA\ndatasets for Indonesian and Sundanese languages. To do so, we create datasets\nfor these languages using various methods involving both LLMs and human\nannotators, resulting in ~4.5K questions per language (~9K in total), making\nour dataset the largest of its kind. Our experiments show that automatic data\nadaptation from an existing English dataset is less effective for Sundanese.\nInterestingly, using the direct generation method on the target language, GPT-4\nTurbo can generate questions with adequate general knowledge in both languages,\nalbeit not as culturally 'deep' as humans. We also observe a higher occurrence\nof fluency errors in the Sundanese dataset, highlighting the discrepancy\nbetween medium- and lower-resource languages.", "published": "2024-02-27 08:24:32", "link": "http://arxiv.org/abs/2402.17302v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SKT5SciSumm -- Revisiting Extractive-Generative Approach for\n  Multi-Document Scientific Summarization", "abstract": "Summarization for scientific text has shown significant benefits both for the\nresearch community and human society. Given the fact that the nature of\nscientific text is distinctive and the input of the multi-document\nsummarization task is substantially long, the task requires sufficient\nembedding generation and text truncation without losing important information.\nTo tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid\nframework for multi-document scientific summarization (MDSS). We leverage the\nSentence-Transformer version of Scientific Paper Embeddings using\nCitation-Informed Transformers (SPECTER) to encode and represent textual\nsentences, allowing for efficient extractive summarization using k-means\nclustering. We employ the T5 family of models to generate abstractive summaries\nusing extracted sentences. SKT5SciSumm achieves state-of-the-art performance on\nthe Multi-XScience dataset. Through extensive experiments and evaluation, we\nshowcase the benefits of our model by using less complicated models to achieve\nremarkable results, thereby highlighting its potential in advancing the field\nof multi-document summarization for scientific text.", "published": "2024-02-27 08:33:31", "link": "http://arxiv.org/abs/2402.17311v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised multiple choices question answering via universal corpus", "abstract": "Unsupervised question answering is a promising yet challenging task, which\nalleviates the burden of building large-scale annotated data in a new domain.\nIt motivates us to study the unsupervised multiple-choice question answering\n(MCQA) problem. In this paper, we propose a novel framework designed to\ngenerate synthetic MCQA data barely based on contexts from the universal domain\nwithout relying on any form of manual annotation. Possible answers are\nextracted and used to produce related questions, then we leverage both named\nentities (NE) and knowledge graphs to discover plausible distractors to form\ncomplete synthetic samples. Experiments on multiple MCQA datasets demonstrate\nthe effectiveness of our method.", "published": "2024-02-27 09:10:28", "link": "http://arxiv.org/abs/2402.17333v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RECOST: External Knowledge Guided Data-efficient Instruction Tuning", "abstract": "In the current landscape of large language models (LLMs), the process of\ninstruction tuning serves as an essential step. Considering the high computing\npower overhead, data-efficient instruction tuning was proposed to reduce the\ntraining data size in this process, aiming at selecting high-quality\ninstructional data. Nevertheless, we argue that most current data-efficient\ninstruction-tuning methods are highly dependent on the quality of the original\ninstruction-tuning dataset. When it comes to datasets synthesized by LLMs, a\ncommon scenario in this field, dirty samples will even be selected with a\nhigher probability than other samples. To address these challenges, we utilized\nexternal knowledge (relevant examples or paragraphs) to evaluate those samples\nsynthesized by LLMs with an in-context-based relative predictive entropy. Based\non the new metric, we proposed a framework, dubbed as \\textbf{RECOST}, which\nintegrates external-knowledge-base re-ranking and diversity-consistent sampling\ninto a single pipeline. Through extensive experiments on several synthetic\ndatasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our\nmethod and achieve even better results with only \\textbf{1\\%} of the full\ndataset.", "published": "2024-02-27 09:47:36", "link": "http://arxiv.org/abs/2402.17355v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SoFA: Shielded On-the-fly Alignment via Priority Rule Following", "abstract": "The alignment problem in Large Language Models (LLMs) involves adapting them\nto the broad spectrum of human values. This requirement challenges existing\nalignment methods due to diversity of preferences and regulatory standards.\nThis paper introduces a novel alignment paradigm, priority rule following,\nwhich defines rules as the primary control mechanism in each dialog,\nprioritizing them over user instructions. Our preliminary analysis reveals that\neven the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding\nand prioritizing the rules. Therefore, we present PriorityDistill, a\nsemi-automated approach for distilling priority following signals from LLM\nsimulations to ensure robust rule integration and adherence. Our experiments\nshow that this method not only effectively minimizes misalignments utilizing\nonly one general rule but also adapts smoothly to various unseen rules,\nensuring they are shielded from hijacking and that the model responds\nappropriately.", "published": "2024-02-27 09:52:27", "link": "http://arxiv.org/abs/2402.17358v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry", "abstract": "There is a large volume of late antique and medieval Hebrew texts. They\nrepresent a crucial linguistic and cultural bridge between Biblical and modern\nHebrew. Poetry is prominent in these texts and one of its main haracteristics\nis the frequent use of metaphor. Distinguishing figurative and literal language\nuse is a major task for scholars of the Humanities, especially in the fields of\nliterature, linguistics, and hermeneutics. This paper presents a new,\nchallenging dataset of late antique and medieval Hebrew poetry with expert\nannotations of metaphor, as well as some baseline results, which we hope will\nfacilitate further research in this area.", "published": "2024-02-27 10:09:40", "link": "http://arxiv.org/abs/2402.17371v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "KoDialogBench: Evaluating Conversational Understanding of Language\n  Models with Korean Dialogue Benchmark", "abstract": "As language models are often deployed as chatbot assistants, it becomes a\nvirtue for models to engage in conversations in a user's first language. While\nthese models are trained on a wide range of languages, a comprehensive\nevaluation of their proficiency in low-resource languages such as Korean has\nbeen lacking. In this work, we introduce KoDialogBench, a benchmark designed to\nassess language models' conversational capabilities in Korean. To this end, we\ncollect native Korean dialogues on daily topics from public sources, or\ntranslate dialogues from other languages. We then structure these conversations\ninto diverse test datasets, spanning from dialogue comprehension to response\nselection tasks. Leveraging the proposed benchmark, we conduct extensive\nevaluations and analyses of various language models to measure a foundational\nunderstanding of Korean dialogues. Experimental results indicate that there\nexists significant room for improvement in models' conversation skills.\nFurthermore, our in-depth comparisons across different language models\nhighlight the effectiveness of recent training techniques in enhancing\nconversational proficiency. We anticipate that KoDialogBench will promote the\nprogress towards conversation-aware Korean language models.", "published": "2024-02-27 10:14:57", "link": "http://arxiv.org/abs/2402.17377v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and\n  Humans", "abstract": "Nowadays, technology is rapidly advancing: bots are writing comments,\narticles, and reviews. Due to this fact, it is crucial to know if the text was\nwritten by a human or by a bot. This paper focuses on comparing structures of\nthe coarse-grained partitions of semantic paths for human-written and\nbot-generated texts. We compare the clusterizations of datasets of n-grams from\nliterary texts and texts generated by several bots. The hypothesis is that the\nstructures and clusterizations are different. Our research supports the\nhypothesis. As the semantic structure may be different for different languages,\nwe investigate Russian, English, German, and Vietnamese languages.", "published": "2024-02-27 10:38:37", "link": "http://arxiv.org/abs/2402.17392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Continual Pretraining in Large Language Models: Insights\n  and Implications", "abstract": "Continual learning (CL) in large language models (LLMs) is an evolving domain\nthat focuses on developing efficient and sustainable training strategies to\nadapt models to emerging knowledge and achieve robustness in dynamic\nenvironments. Our primary emphasis is on continual domain-adaptive pretraining,\na process designed to equip LLMs with the ability to integrate new information\nfrom various domains while retaining previously learned knowledge. Since\nexisting works concentrate mostly on continual fine-tuning for a limited\nselection of downstream tasks or training domains, we introduce a new benchmark\ndesigned to measure the adaptability of LLMs to changing pretraining data\nlandscapes. We further examine the impact of model size on learning efficacy\nand forgetting, as well as how the progression and similarity of emerging\ndomains affect the knowledge transfer within these models.\n  Our findings uncover several key insights: (i) continual pretraining\nconsistently improves <1.5B models studied in this work and is also superior to\ndomain adaptation, (ii) larger models always achieve better perplexity than\nsmaller ones when continually pretrained on the same corpus, (iii) smaller\nmodels are particularly sensitive to continual pretraining, showing the most\nsignificant rates of both learning and forgetting, (iv) continual pretraining\nboosts downstream task performance of GPT-2 family, (v) continual pretraining\nenables LLMs to specialize better when the sequence of domains shows semantic\nsimilarity while randomizing training domains leads to better transfer and\nfinal performance otherwise. We posit that our research establishes a new\nbenchmark for CL in LLMs, providing a more realistic evaluation of knowledge\nretention and transfer across diverse domains.", "published": "2024-02-27 10:47:24", "link": "http://arxiv.org/abs/2402.17400v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistency Matters: Explore LLMs Consistency From a Black-Box\n  Perspective", "abstract": "Nowadays both commercial and open-source academic LLM have become the\nmainstream models of NLP. However, there is still a lack of research on LLM\nconsistency, meaning that throughout the various stages of LLM research and\ndeployment, its internal parameters and capabilities should remain unchanged.\nThis issue exists in both the industrial and academic sectors. The solution to\nthis problem is often time-consuming and labor-intensive, and there is also an\nadditional cost of secondary deployment, resulting in economic and time losses.\nTo fill this gap, we build an LLM consistency task dataset and design several\nbaselines. Additionally, we choose models of diverse scales for the main\nexperiments. Specifically, in the LightGBM experiment, we used traditional NLG\nmetrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training.\nThe final result exceeds the manual evaluation and GPT3.5 as well as other\nmodels in the main experiment, achieving the best performance. In the end, we\nuse the best performing LightGBM model as the base model to build the\nevaluation tool, which can effectively assist in the deployment of business\nmodels. Our code and tool demo are available at\nhttps://github.com/heavenhellchen/Consistency.git", "published": "2024-02-27 11:02:12", "link": "http://arxiv.org/abs/2402.17411v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing EEG-to-Text Decoding through Transferable Representations from\n  Pre-trained Contrastive EEG-Text Masked Autoencoder", "abstract": "Reconstructing natural language from non-invasive electroencephalography\n(EEG) holds great promise as a language decoding technology for brain-computer\ninterfaces (BCIs). However, EEG-based language decoding is still in its nascent\nstages, facing several technical issues such as: 1) Absence of a hybrid\nstrategy that can effectively integrate cross-modality (between EEG and text)\nself-learning with intra-modality self-reconstruction of EEG features or\ntextual sequences; 2) Under-utilization of large language models (LLMs) to\nenhance EEG-based language decoding. To address above issues, we propose the\nContrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that\norchestrates compound self-supervised learning across and within EEG and text\nthrough a dedicated multi-stream encoder. Furthermore, we develop a framework\ncalled E2T-PTR (EEG-to-Text decoding using Pretrained Transferable\nRepresentations), which leverages pre-trained modules alongside the EEG stream\nfrom CET-MAE and further enables an LLM (specifically BART) to decode text from\nEEG sequences. Comprehensive experiments conducted on the popular text-evoked\nEEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms\nthe state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,\nrespectively. These results indicate significant advancements in the field and\nunderscores the proposed framework's potential to enable more powerful and\nwidespread BCI applications.", "published": "2024-02-27 11:45:21", "link": "http://arxiv.org/abs/2402.17433v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training-Free Long-Context Scaling of Large Language Models", "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent\ntext is markedly weakened when the number of input tokens exceeds their\npretraining length. Given the expensive overhead of finetuning large-scale\nmodels with longer sequences, we propose Dual Chunk Attention (DCA), which\nenables Llama2 70B to support context windows of more than 100k tokens without\ncontinual training. By decomposing the attention computation for long sequences\ninto chunk-based modules, DCA manages to effectively capture the relative\npositional information of tokens within the same chunk (Intra-Chunk) and across\ndistinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash\nAttention. In addition to its impressive extrapolation capability, DCA achieves\nperformance on practical long-context tasks that is comparable to or even\nbetter than that of finetuned models. When compared with proprietary models,\nour training-free 70B model attains 94% of the performance of gpt-3.5-16k,\nindicating it is a viable open-source alternative. All code and data used in\nthis work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.", "published": "2024-02-27 12:39:23", "link": "http://arxiv.org/abs/2402.17463v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda\n  Spans in News Articles", "abstract": "The use of propaganda has spiked on mainstream and social media, aiming to\nmanipulate or mislead users. While efforts to automatically detect propaganda\ntechniques in textual, visual, or multimodal content have increased, most of\nthem primarily focus on English content. The majority of the recent initiatives\ntargeting medium to low-resource languages produced relatively small annotated\ndatasets, with a skewed distribution, posing challenges for the development of\nsophisticated propaganda detection models. To address this challenge, we\ncarefully develop the largest propaganda dataset to date, ArPro, comprised of\n8K paragraphs from newspaper articles, labeled at the text span level following\na taxonomy of 23 propagandistic techniques. Furthermore, our work offers the\nfirst attempt to understand the performance of large language models (LLMs),\nusing GPT-4, for fine-grained propaganda detection from text. Results showed\nthat GPT-4's performance degrades as the task moves from simply classifying a\nparagraph as propagandistic or not, to the fine-grained task of detecting\npropaganda techniques and their manifestation in text. Compared to models\nfine-tuned on the dataset for propaganda detection at different classification\ngranularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a\ndataset consisting of six other languages for span detection, and results\nsuggest that the model struggles with the task across languages. Our dataset\nand resources will be released to the community.", "published": "2024-02-27 13:02:19", "link": "http://arxiv.org/abs/2402.17478v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Foundational Capabilities of Large Language Models in Predicting\n  Postoperative Risks Using Clinical Notes", "abstract": "Clinical notes recorded during a patient's perioperative journey holds\nimmense informational value. Advances in large language models (LLMs) offer\nopportunities for bridging this gap. Using 84,875 pre-operative notes and its\nassociated surgical cases from 2018 to 2021, we examine the performance of LLMs\nin predicting six postoperative risks using various fine-tuning strategies.\nPretrained LLMs outperformed traditional word embeddings by an absolute AUROC\nof 38.3% and AUPRC of 33.2%. Self-supervised fine-tuning further improved\nperformance by 3.2% and 1.5%. Incorporating labels into training further\nincreased AUROC by 1.8% and AUPRC by 2%. The highest performance was achieved\nwith a unified foundation model, with improvements of 3.6% for AUROC and 2.6%\nfor AUPRC compared to self-supervision, highlighting the foundational\ncapabilities of LLMs in predicting postoperative risks, which could be\npotentially beneficial when deployed for perioperative care", "published": "2024-02-27 13:18:00", "link": "http://arxiv.org/abs/2402.17493v5", "categories": ["cs.CL", "J.3; I.2.7"], "primary_category": "cs.CL"}
{"title": "Extreme Miscalibration and the Illusion of Adversarial Robustness", "abstract": "Deep learning-based Natural Language Processing (NLP) models are vulnerable\nto adversarial attacks, where small perturbations can cause a model to\nmisclassify. Adversarial Training (AT) is often used to increase model\nrobustness. However, we have discovered an intriguing phenomenon: deliberately\nor accidentally miscalibrating models masks gradients in a way that interferes\nwith adversarial attack search methods, giving rise to an apparent increase in\nrobustness. We show that this observed gain in robustness is an illusion of\nrobustness (IOR), and demonstrate how an adversary can perform various forms of\ntest-time temperature calibration to nullify the aforementioned interference\nand allow the adversarial attack to find adversarial examples. Hence, we urge\nthe NLP community to incorporate test-time temperature scaling into their\nrobustness evaluations to ensure that any observed gains are genuine. Finally,\nwe show how the temperature can be scaled during \\textit{training} to improve\ngenuine robustness.", "published": "2024-02-27 13:49:12", "link": "http://arxiv.org/abs/2402.17509v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval is Accurate Generation", "abstract": "Standard language models generate text by selecting tokens from a fixed,\nfinite, and standalone vocabulary. We introduce a novel method that selects\ncontext-aware phrases from a collection of supporting documents. One of the\nmost significant challenges for this paradigm shift is determining the training\noracles, because a string of text can be segmented in various ways and each\nsegment can be retrieved from numerous possible documents. To address this, we\npropose to initialize the training oracles using linguistic heuristics and,\nmore importantly, bootstrap the oracles through iterative self-reinforcement.\nExtensive experiments show that our model not only outperforms standard\nlanguage models on a variety of knowledge-intensive tasks but also demonstrates\nimproved generation quality in open-ended text generation. For instance,\ncompared to the standard language model counterpart, our model raises the\naccuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from\n42.61% to 81.58% in open-ended text generation. Remarkably, our model also\nachieves the best performance and the lowest latency among several\nretrieval-augmented baselines. In conclusion, we assert that retrieval is more\naccurate generation and hope that our work will encourage further research on\nthis new paradigm shift.", "published": "2024-02-27 14:16:19", "link": "http://arxiv.org/abs/2402.17532v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unleashing the Potential of Large Language Models as Prompt Optimizers:\n  Analogical Analysis with Gradient-based Model Optimizers", "abstract": "Automatic prompt optimization is an important approach to improving the\nperformance of large language models (LLMs). Recent research demonstrates the\npotential of using LLMs as prompt optimizers, which can generate improved task\nprompts via iterative refinement. In this paper, we propose a novel perspective\nto investigate the design of LLM-based prompt optimizers, by drawing an analogy\nwith gradient-based model optimizers. To connect these two approaches, we\nidentify two pivotal factors in model parameter learning: update direction and\nupdate method. By systematically analyzing a rich set of improvement strategies\non the two aspects, we further develop a capable Gradient-inspired LLM-based\nPrompt Optimizer called GPO. At each step, it first retrieves relevant prompts\nfrom the optimization trajectory as the update direction. Then, it utilizes the\ngeneration-based refinement strategy to perform the update, while controlling\nthe edit distance through a cosine-based decay strategy. Extensive experiments\ndemonstrate the effectiveness and efficiency of GPO. In particular, GPO brings\nan additional improvement of up to 56.8% on Big-Bench Hard and 62.6% on MMLU\ncompared to baseline methods. The code is available at\nhttps://github.com/RUCAIBox/GPO.", "published": "2024-02-27 15:05:32", "link": "http://arxiv.org/abs/2402.17564v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)", "abstract": "In this paper, we explore the impact of augmenting pre-trained\nEncoder-Decoder models, specifically T5, with linguistic knowledge for the\nprediction of a target task. In particular, we investigate whether fine-tuning\na T5 model on an intermediate task that predicts structural linguistic\nproperties of sentences modifies its performance in the target task of\npredicting sentence-level complexity. Our study encompasses diverse experiments\nconducted on Italian and English datasets, employing both monolingual and\nmultilingual T5 models at various sizes. Results obtained for both languages\nand in cross-lingual configurations show that linguistically motivated\nintermediate fine-tuning has generally a positive impact on target task\nperformance, especially when applied to smaller models and in scenarios with\nlimited data availability.", "published": "2024-02-27 15:34:15", "link": "http://arxiv.org/abs/2402.17608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Automated Writing Evaluation with Corrective Feedback", "abstract": "The utilization of technology in second language learning and teaching has\nbecome ubiquitous. For the assessment of writing specifically, automated\nwriting evaluation (AWE) and grammatical error correction (GEC) have become\nimmensely popular and effective methods for enhancing writing proficiency and\ndelivering instant and individualized feedback to learners. By leveraging the\npower of natural language processing (NLP) and machine learning algorithms, AWE\nand GEC systems have been developed separately to provide language learners\nwith automated corrective feedback and more accurate and unbiased scoring that\nwould otherwise be subject to examiners. In this paper, we propose an\nintegrated system for automated writing evaluation with corrective feedback as\na means of bridging the gap between AWE and GEC results for second language\nlearners. This system enables language learners to simulate the essay writing\ntests: a student writes and submits an essay, and the system returns the\nassessment of the writing along with suggested grammatical error corrections.\nGiven that automated scoring and grammatical correction are more efficient and\ncost-effective than human grading, this integrated system would also alleviate\nthe burden of manually correcting innumerable essays.", "published": "2024-02-27 15:42:33", "link": "http://arxiv.org/abs/2402.17613v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Natural Language Inference Based Faithfulness Evaluation\n  for Diverse Summarisation Tasks", "abstract": "We study existing approaches to leverage off-the-shelf Natural Language\nInference (NLI) models for the evaluation of summary faithfulness and argue\nthat these are sub-optimal due to the granularity level considered for premises\nand hypotheses. That is, the smaller content unit considered as hypothesis is a\nsentence and premises are made up of a fixed number of document sentences. We\npropose a novel approach, namely InFusE, that uses a variable premise size and\nsimplifies summary sentences into shorter hypotheses. Departing from previous\nstudies which focus on single short document summarisation, we analyse NLI\nbased faithfulness evaluation for diverse summarisation tasks. We introduce\nDiverSumm, a new benchmark comprising long form summarisation (long documents\nand summaries) and diverse summarisation tasks (e.g., meeting and\nmulti-document summarisation). In experiments, InFusE obtains superior\nperformance across the different summarisation tasks. Our code and data are\navailable at https://github.com/HJZnlp/infuse.", "published": "2024-02-27 15:57:11", "link": "http://arxiv.org/abs/2402.17630v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Text Segmentation to Smart Chaptering: A Novel Benchmark for\n  Structuring Video Transcriptions", "abstract": "Text segmentation is a fundamental task in natural language processing, where\ndocuments are split into contiguous sections. However, prior research in this\narea has been constrained by limited datasets, which are either small in scale,\nsynthesized, or only contain well-structured documents. In this paper, we\naddress these limitations by introducing a novel benchmark YTSeg focusing on\nspoken content that is inherently more unstructured and both topically and\nstructurally diverse. As part of this work, we introduce an efficient\nhierarchical segmentation model MiniSeg, that outperforms state-of-the-art\nbaselines. Lastly, we expand the notion of text segmentation to a more\npractical \"smart chaptering\" task that involves the segmentation of\nunstructured content, the generation of meaningful segment titles, and a\npotential real-time application of the models.", "published": "2024-02-27 15:59:37", "link": "http://arxiv.org/abs/2402.17633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NextLevelBERT: Masked Language Modeling with Higher-Level\n  Representations for Long Documents", "abstract": "While (large) language models have significantly improved over the last\nyears, they still struggle to sensibly process long sequences found, e.g., in\nbooks, due to the quadratic scaling of the underlying attention mechanism. To\naddress this, we propose NextLevelBERT, a Masked Language Model operating not\non tokens, but on higher-level semantic representations in the form of text\nembeddings. We pretrain NextLevelBERT to predict the vector representation of\nentire masked text chunks and evaluate the effectiveness of the resulting\ndocument vectors on three types of tasks: 1) Semantic Textual Similarity via\nzero-shot document embeddings, 2) Long document classification, 3)\nMultiple-choice question answering. We find that next-level Masked Language\nModeling is an effective technique to tackle long-document use cases and can\noutperfor much larger embedding models as long as the required level of detail\nof semantic information is not too fine. Our models and code are publicly\navailable online.", "published": "2024-02-27 16:56:30", "link": "http://arxiv.org/abs/2402.17682v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG", "abstract": "We introduce AmbigNLG, a novel task designed to tackle the challenge of task\nambiguity in instructions for Natural Language Generation (NLG). Ambiguous\ninstructions often impede the performance of Large Language Models (LLMs),\nespecially in complex NLG tasks. To tackle this issue, we propose an ambiguity\ntaxonomy that categorizes different types of instruction ambiguities and\nrefines initial instructions with clearer specifications. Accompanying this\ntask, we present AmbigSNI-NLG, a dataset comprising 2,500 instances annotated\nto facilitate research in AmbigNLG. Through comprehensive experiments with\nstate-of-the-art LLMs, we demonstrate that our method significantly enhances\nthe alignment of generated text with user expectations, achieving up to a\n15.02-point increase in ROUGE scores. Our findings highlight the critical\nimportance of addressing task ambiguity to fully harness the capabilities of\nLLMs in NLG tasks. Furthermore, we confirm the effectiveness of our method in\npractical settings involving interactive ambiguity mitigation with users,\nunderscoring the benefits of leveraging LLMs for interactive clarification.", "published": "2024-02-27 17:52:33", "link": "http://arxiv.org/abs/2402.17717v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tower: An Open Multilingual Large Language Model for Translation-Related\n  Tasks", "abstract": "While general-purpose large language models (LLMs) demonstrate proficiency on\nmultiple tasks within the domain of translation, approaches based on open LLMs\nare competitive only when specializing on a single task. In this paper, we\npropose a recipe for tailoring LLMs to multiple tasks present in translation\nworkflows. We perform continued pretraining on a multilingual mixture of\nmonolingual and parallel data, creating TowerBase, followed by finetuning on\ninstructions relevant for translation processes, creating TowerInstruct. Our\nfinal model surpasses open alternatives on several tasks relevant to\ntranslation workflows and is competitive with general-purpose closed LLMs. To\nfacilitate future research, we release the Tower models, our specialization\ndataset, an evaluation framework for LLMs focusing on the translation\necosystem, and a collection of model generations, including ours, on our\nbenchmark.", "published": "2024-02-27 18:09:36", "link": "http://arxiv.org/abs/2402.17733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Optimal Learning of Language Models", "abstract": "This work studies the general principles of improving the learning of\nlanguage models (LMs), which aims at reducing the necessary training steps for\nachieving superior performance. Specifically, we present a theory for the\noptimal learning of LMs. We first propose an objective that optimizes LM\nlearning by maximizing the data compression ratio in an\n\"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named\nLearning Law, to reveal the properties of the dynamics in the optimal learning\nprocess under our objective. The theorem is then validated by experiments on a\nlinear classification and a real-world language modeling task. Finally, we\nempirically verify that the optimal learning of LMs essentially stems from the\nimprovement of the coefficients in the scaling law of LMs, indicating great\npromise and significance for designing practical learning acceleration methods.\nOur code can be found at https://aka.ms/LearningLaw.", "published": "2024-02-27 18:52:19", "link": "http://arxiv.org/abs/2402.17759v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in\n  Relational Algebra", "abstract": "Many existing end-to-end systems for hybrid question answering tasks can\noften be boiled down to a \"prompt-and-pray\" paradigm, where the user has\nlimited control and insight into the intermediate reasoning steps used to\nachieve the final result. Additionally, due to the context size limitation of\nmany transformer-based LLMs, it is often not reasonable to expect that the full\nstructured and unstructured context will fit into a given prompt in a zero-shot\nsetting, let alone a few-shot setting. We introduce BlendSQL, a superset of\nSQLite to act as a unified dialect for orchestrating reasoning across both\nunstructured and structured data. For hybrid question answering tasks involving\nmulti-hop reasoning, we encode the full decomposed reasoning roadmap into a\nsingle interpretable BlendSQL query. Notably, we show that BlendSQL can scale\nto massive datasets and improve the performance of end-to-end systems while\nusing 35% fewer tokens. Our code is available and installable as a package at\nhttps://github.com/parkervg/blendsql.", "published": "2024-02-27 20:48:24", "link": "http://arxiv.org/abs/2402.17882v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Acquiring Linguistic Knowledge from Multimodal Input", "abstract": "In contrast to children, language models (LMs) exhibit considerably inferior\ndata efficiency when acquiring language. In this submission to the BabyLM\nChallenge (Warstadt et al., 2023), we test the hypothesis that this data\nefficiency gap is partly caused by a lack of multimodal input and grounding in\nthe learning environment of typical language models. Although previous work\nlooking into this question found that multimodal training can even harm\nlanguage-only performance, we speculate that these findings can be attributed\nto catastrophic forgetting of complex language due to fine-tuning on captions\ndata. To test our hypothesis, we perform an ablation study on FLAVA (Singh et\nal., 2022), a multimodal vision-and-language model, independently varying the\nvolume of text and vision input to quantify how much text data (if any) can be\noffset by vision at different data scales. We aim to limit catastrophic\nforgetting through a multitask pretraining regime that includes unimodal\ntext-only tasks and data sampled from WiT, the relatively diverse\nWikipedia-based dataset (Srinivasan et al., 2021). Our results are largely\nnegative: Multimodal pretraining does not harm our models' language performance\nbut does not consistently help either. That said, our conclusions are limited\nby our having been able to conduct only a small number of runs. While we must\nleave open the possibility that multimodal input explains some of the gap in\ndata efficiency between LMs and humans, positive evidence for this hypothesis\nwill require better architectures and techniques for multimodal training.", "published": "2024-02-27 23:29:10", "link": "http://arxiv.org/abs/2402.17936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and\n  Understanding -- A Survey", "abstract": "Recent breakthroughs in large language modeling have facilitated rigorous\nexploration of their application in diverse tasks related to tabular data\nmodeling, such as prediction, tabular data synthesis, question answering, and\ntable understanding. Each task presents unique challenges and opportunities.\nHowever, there is currently a lack of comprehensive review that summarizes and\ncompares the key techniques, metrics, datasets, models, and optimization\napproaches in this research domain. This survey aims to address this gap by\nconsolidating recent progress in these areas, offering a thorough survey and\ntaxonomy of the datasets, metrics, and methodologies utilized. It identifies\nstrengths, limitations, unexplored territories, and gaps in the existing\nliterature, while providing some insights for future research directions in\nthis vital and rapidly evolving field. It also provides relevant code and\ndatasets references. Through this comprehensive review, we hope to provide\ninterested readers with pertinent references and insightful perspectives,\nempowering them with the necessary tools and knowledge to effectively navigate\nand address the prevailing challenges in the field.", "published": "2024-02-27 23:59:01", "link": "http://arxiv.org/abs/2402.17944v4", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Comparing effectiveness of regularization methods on text\n  classification: Simple and complex model in data shortage situation", "abstract": "Text classification is the task of assigning a document to a predefined\nclass. However, it is expensive to acquire enough labeled documents or to label\nthem. In this paper, we study the regularization methods' effects on various\nclassification models when only a few labeled data are available. We compare a\nsimple word embedding-based model, which is simple but effective, with complex\nmodels (CNN and BiLSTM). In supervised learning, adversarial training can\nfurther regularize the model. When an unlabeled dataset is available, we can\nregularize the model using semi-supervised learning methods such as the Pi\nmodel and virtual adversarial training. We evaluate the regularization effects\non four text classification datasets (AG news, DBpedia, Yahoo! Answers, Yelp\nPolarity), using only 0.1% to 0.5% of the original labeled training documents.\nThe simple model performs relatively well in fully supervised learning, but\nwith the help of adversarial training and semi-supervised learning, both simple\nand complex models can be regularized, showing better results for complex\nmodels. Although the simple model is robust to overfitting, a complex model\nwith well-designed prior beliefs can be also robust to overfitting.", "published": "2024-02-27 07:26:16", "link": "http://arxiv.org/abs/2403.00825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM\n  Responses", "abstract": "Mitigating hallucination issues is a key challenge that must be overcome to\nreliably deploy large language models (LLMs) in real-world scenarios. Recently,\nvarious methods have been proposed to detect and revise factual errors in\nLLM-generated texts, in order to reduce hallucination. In this paper, we\npropose Re-Ex, a method for post-editing LLM-generated responses. Re-Ex\nintroduces a novel reasoning step dubbed as the factual error explanation step.\nRe-Ex revises the initial response of LLMs using 3-steps : first, external\ntools are used to retrieve the evidences of the factual errors in the initial\nLLM response; next, LLM is instructed to explain the problematic parts of the\nresponse based on the gathered evidence; finally, LLM revises the initial\nresponse using the explanations provided in the previous step. In addition to\nthe explanation step, Re-Ex also incorporates new prompting techniques to\nreduce the token count and inference time required for the response revision\nprocess. Compared with existing methods including FacTool, CoVE, and RARR,\nRe-Ex provides better detection and revision performance with less inference\ntime and fewer tokens in multiple benchmarks.", "published": "2024-02-27 00:22:18", "link": "http://arxiv.org/abs/2402.17097v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sinkhorn Distance Minimization for Knowledge Distillation", "abstract": "Knowledge distillation (KD) has been widely adopted to compress large\nlanguage models (LLMs). Existing KD methods investigate various divergence\nmeasures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL),\nand Jensen-Shannon (JS) divergences. However, due to limitations inherent in\ntheir assumptions and definitions, these measures fail to deliver effective\nsupervision when few distribution overlap exists between the teacher and the\nstudent. In this paper, we show that the aforementioned KL, RKL, and JS\ndivergences respectively suffer from issues of mode-averaging, mode-collapsing,\nand mode-underestimation, which deteriorates logits-based KD for diverse NLP\ntasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the\nSinkhorn distance to ensure a nuanced and precise assessment of the disparity\nbetween teacher and student distributions. Besides, profit by properties of the\nSinkhorn metric, we can get rid of sample-wise KD that restricts the perception\nof divergence in each teacher-student sample pair. Instead, we propose a\nbatch-wise reformulation to capture geometric intricacies of distributions\nacross samples in the high-dimensional space. Comprehensive evaluation on GLUE\nand SuperGLUE, in terms of comparability, validity, and generalizability,\nhighlights our superiority over state-of-the-art methods on all kinds of LLMs\nwith encoder-only, encoder-decoder, and decoder-only architectures.", "published": "2024-02-27 01:13:58", "link": "http://arxiv.org/abs/2402.17110v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Benchmarking Data Science Agents", "abstract": "In the era of data-driven decision-making, the complexity of data analysis\nnecessitates advanced expertise and tools of data science, presenting\nsignificant challenges even for specialists. Large Language Models (LLMs) have\nemerged as promising aids as data science agents, assisting humans in data\nanalysis and processing. Yet their practical efficacy remains constrained by\nthe varied demands of real-world applications and complicated analytical\nprocess. In this paper, we introduce DSEval -- a novel evaluation paradigm, as\nwell as a series of innovative benchmarks tailored for assessing the\nperformance of these agents throughout the entire data science lifecycle.\nIncorporating a novel bootstrapped annotation method, we streamline dataset\npreparation, improve the evaluation coverage, and expand benchmarking\ncomprehensiveness. Our findings uncover prevalent obstacles and provide\ncritical insights to inform future advancements in the field.", "published": "2024-02-27 03:03:06", "link": "http://arxiv.org/abs/2402.17168v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and\n  Finetuning Method", "abstract": "While large language models (LLMs) often adopt finetuning to unlock their\ncapabilities for downstream applications, our understanding on the inductive\nbiases (especially the scaling properties) of different finetuning methods is\nstill limited. To fill this gap, we conduct systematic experiments studying\nwhether and how different scaling factors, including LLM model size,\npretraining data size, new finetuning parameter size and finetuning data size,\naffect the finetuning performance. We consider two types of finetuning --\nfull-model tuning (FMT) and parameter efficient tuning (PET, including prompt\ntuning and LoRA), and explore their scaling behaviors in the data-limited\nregime where the LLM model size substantially outweighs the finetuning data\nsize. Based on two sets of pretrained bilingual LLMs from 1B to 16B and\nexperiments on bilingual machine translation and multilingual summarization\nbenchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative\njoint scaling law between finetuning data size and each other scaling factor;\n2) LLM finetuning benefits more from LLM model scaling than pretraining data\nscaling, and PET parameter scaling is generally ineffective; and 3) the optimal\nfinetuning method is highly task- and finetuning data-dependent. We hope our\nfindings could shed light on understanding, selecting and developing LLM\nfinetuning methods.", "published": "2024-02-27 04:18:49", "link": "http://arxiv.org/abs/2402.17193v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MVAM: Multi-View Attention Method for Fine-grained Image-Text Matching", "abstract": "Existing two-stream models, such as CLIP, encode images and text through\nindependent representations, showing good performance while ensuring retrieval\nspeed, have attracted attention from industry and academia. However, the single\nrepresentation often struggles to capture complex content fully. Such models\nmay ignore fine-grained information during matching, resulting in suboptimal\nretrieval results. To overcome this limitation and enhance the performance of\ntwo-stream models, we propose a Multi-view Attention Method (MVAM) for\nimage-text matching. This approach leverages diverse attention heads with\nunique view codes to learn multiple representations for images and text, which\nare then concatenated for matching. We also incorporate a diversity objective\nto explicitly encourage attention heads to focus on distinct aspects of the\ninput data, capturing complementary fine-grained details. This diversity\nenables the model to represent image-text pairs from multiple perspectives,\nensuring a more comprehensive understanding and alignment of critical content.\nOur method allows models to encode images and text from different perspectives\nand focus on more critical details, leading to better matching performance. Our\nexperiments on MSCOCO and Flickr30K demonstrate enhancements over existing\nmodels, and further case studies reveal that different attention heads can\nfocus on distinct content, achieving more comprehensive representations.", "published": "2024-02-27 06:11:54", "link": "http://arxiv.org/abs/2402.17237v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in\n  Multi-turn Dialogue", "abstract": "Large Language Models (LLMs) have been demonstrated to generate illegal or\nunethical responses, particularly when subjected to \"jailbreak.\" Research on\njailbreak has highlighted the safety issues of LLMs. However, prior studies\nhave predominantly focused on single-turn dialogue, ignoring the potential\ncomplexities and risks presented by multi-turn dialogue, a crucial mode through\nwhich humans derive information from LLMs. In this paper, we argue that humans\ncould exploit multi-turn dialogue to induce LLMs into generating harmful\ninformation. LLMs may not intend to reject cautionary or borderline unsafe\nqueries, even if each turn is closely served for one malicious purpose in a\nmulti-turn dialogue. Therefore, by decomposing an unsafe query into several\nsub-queries for multi-turn dialogue, we induced LLMs to answer harmful\nsub-questions incrementally, culminating in an overall harmful response. Our\nexperiments, conducted across a wide range of LLMs, indicate current\ninadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our\nfindings expose vulnerabilities of LLMs in complex scenarios involving\nmulti-turn dialogue, presenting new challenges for the safety of LLMs.", "published": "2024-02-27 07:11:59", "link": "http://arxiv.org/abs/2402.17262v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations", "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.", "published": "2024-02-27 08:27:15", "link": "http://arxiv.org/abs/2402.17304v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FairBelief -- Assessing Harmful Beliefs in Language Models", "abstract": "Language Models (LMs) have been shown to inherit undesired biases that might\nhurt minorities and underrepresented groups if such systems were integrated\ninto real-world applications without careful fairness auditing. This paper\nproposes FairBelief, an analytical approach to capture and assess beliefs,\ni.e., propositions that an LM may embed with different degrees of confidence\nand that covertly influence its predictions. With FairBelief, we leverage\nprompting to study the behavior of several state-of-the-art LMs across\ndifferent previously neglected axes, such as model scale and likelihood,\nassessing predictions on a fairness dataset specifically designed to quantify\nLMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative\nassessment of the beliefs emitted by the models. We apply FairBelief to English\nLMs, revealing that, although these architectures enable high performances on\ndiverse natural language processing tasks, they show hurtful beliefs about\nspecific genders. Interestingly, training procedure and dataset, model scale,\nand architecture induce beliefs of different degrees of hurtfulness.", "published": "2024-02-27 10:31:00", "link": "http://arxiv.org/abs/2402.17389v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploiting Emotion-Semantic Correlations for Empathetic Response\n  Generation", "abstract": "Empathetic response generation aims to generate empathetic responses by\nunderstanding the speaker's emotional feelings from the language of dialogue.\nRecent methods capture emotional words in the language of communicators and\nconstruct them as static vectors to perceive nuanced emotions. However,\nlinguistic research has shown that emotional words in language are dynamic and\nhave correlations with other grammar semantic roles, i.e., words with semantic\nmeanings, in grammar. Previous methods overlook these two characteristics,\nwhich easily lead to misunderstandings of emotions and neglect of key\nsemantics. To address this issue, we propose a dynamical Emotion-Semantic\nCorrelation Model (ESCM) for empathetic dialogue generation tasks. ESCM\nconstructs dynamic emotion-semantic vectors through the interaction of context\nand emotions. We introduce dependency trees to reflect the correlations between\nemotions and semantics. Based on dynamic emotion-semantic vectors and\ndependency trees, we propose a dynamic correlation graph convolutional network\nto guide the model in learning context meanings in dialogue and generating\nempathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset\nshow that ESCM understands semantics and emotions more accurately and expresses\nfluent and informative empathetic responses. Our analysis results also indicate\nthat the correlations between emotions and semantics are frequently used in\ndialogues, which is of great significance for empathetic perception and\nexpression.", "published": "2024-02-27 11:50:05", "link": "http://arxiv.org/abs/2402.17437v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain\n  Question Answering", "abstract": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (eg., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness regarding the reliability of external knowledge for LLMs, so\nas to adaptively utilize external knowledge in RAG systems. Specially, we\ndevelop a novel architecture for LLM-based RAG systems, by incorporating a\nspecially designed assessment module that precisely assesses the relevance of\nretrieved documents. Furthermore, we propose an improved training method based\non bi-granularity relevance fusion and noise-resistant training. By combining\nthe improvements in both architecture and training, our proposed REAR can\nbetter utilize external knowledge by effectively perceiving the relevance of\nretrieved documents. Experiments on four open-domain QA tasks show that REAR\nsignificantly outperforms previous a number of competitive RAG approaches. Our\ncodes can be accessed at https://github.com/RUCAIBox/REAR.", "published": "2024-02-27 13:22:51", "link": "http://arxiv.org/abs/2402.17497v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "BASES: Large-scale Web Search User Simulation with Large Language Model\n  based Agents", "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes\nfeasible to develop LLM-based agents for reliable user simulation. Considering\nthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,\nwe conduct large-scale user simulation for web search, to improve the analysis\nand modeling of user search behavior. Specially, we propose BASES, a novel user\nsimulation framework with LLM-based agents, designed to facilitate\ncomprehensive simulations of web search user behaviors. Our simulation\nframework can generate unique user profiles at scale, which subsequently leads\nto diverse search behaviors. To demonstrate the effectiveness of BASES, we\nconduct evaluation experiments based on two human benchmarks in both Chinese\nand English, demonstrating that BASES can effectively simulate large-scale\nhuman-like search behaviors. To further accommodate the research on web search,\nwe develop WARRIORS, a new large-scale dataset encompassing web search user\nbehaviors, including both Chinese and English versions, which can greatly\nbolster research in the field of information retrieval. Our code and data will\nbe publicly released soon.", "published": "2024-02-27 13:44:09", "link": "http://arxiv.org/abs/2402.17505v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Latte: Latent Attention for Linear Time Transformers", "abstract": "The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference.", "published": "2024-02-27 13:54:48", "link": "http://arxiv.org/abs/2402.17512v4", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Predict the Next Word: Humans exhibit uncertainty in this task and\n  language models _____", "abstract": "Language models (LMs) are statistical models trained to assign probability to\nhuman-generated text. As such, it is reasonable to question whether they\napproximate linguistic variability exhibited by humans well. This form of\nstatistical assessment is difficult to perform at the passage level, for it\nrequires acceptability judgements (i.e., human evaluation) or a robust\nautomated proxy (which is non-trivial). At the word level, however, given some\ncontext, samples from an LM can be assessed via exact matching against a\nprerecorded dataset of alternative single-word continuations of the available\ncontext. We exploit this fact and evaluate the LM's ability to reproduce\nvariability that humans (in particular, a population of English speakers)\nexhibit in the 'next word prediction' task. This can be seen as assessing a\nform of calibration, which, in the context of text classification, Baan et al.\n(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and\nChatGPT and find that they exhibit fairly low calibration to human uncertainty.\nWe also verify the failure of expected calibration error (ECE) to reflect this,\nand as such, advise the community against relying on it in this setting.", "published": "2024-02-27 14:11:32", "link": "http://arxiv.org/abs/2402.17527v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "COCOA: CBT-based Conversational Counseling Agent using Memory\n  Specialized in Cognitive Distortions and Dynamic Prompt", "abstract": "The demand for conversational agents that provide mental health care is\nconsistently increasing. In this work, we develop a psychological counseling\nagent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT)\ntechniques to identify and address cognitive distortions inherent in the\nclient's statements. Specifically, we construct a memory system to efficiently\nmanage information necessary for counseling while extracting high-level\ninsights about the client from their utterances. Additionally, to ensure that\nthe counseling agent generates appropriate responses, we introduce dynamic\nprompting to flexibly apply CBT techniques and facilitate the appropriate\nretrieval of information. We conducted dialogues between CoCoA and characters\nfrom Character.ai, creating a dataset for evaluation. Then, we asked GPT to\nevaluate the constructed counseling dataset, and our model demonstrated a\nstatistically significant difference from other models.", "published": "2024-02-27 14:38:47", "link": "http://arxiv.org/abs/2402.17546v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization", "abstract": "Large Language Models (LLMs) exhibit robust problem-solving capabilities for\ndiverse tasks. However, most LLM-based agents are designed as specific task\nsolvers with sophisticated prompt engineering, rather than agents capable of\nlearning and evolving through interactions. These task solvers necessitate\nmanually crafted prompts to inform task rules and regulate LLM behaviors,\ninherently incapacitating to address complex dynamic scenarios e.g., large\ninteractive games. In light of this, we propose Agent-Pro: an LLM-based Agent\nwith Policy-level Reflection and Optimization that can learn a wealth of\nexpertise from interactive experiences and progressively elevate its behavioral\npolicy. Specifically, it involves a dynamic belief generation and reflection\nprocess for policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.", "published": "2024-02-27 15:09:20", "link": "http://arxiv.org/abs/2402.17574v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Are LLMs Capable of Data-based Statistical and Causal Reasoning?\n  Benchmarking Advanced Quantitative Reasoning with Data", "abstract": "Quantitative reasoning is a critical skill to analyze data, yet the\nassessment of such ability remains limited. To address this gap, we introduce\nthe Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate\nLarge Language Models' capability in statistical and causal reasoning with\nreal-world data. The benchmark comprises a carefully constructed dataset of 411\nquestions accompanied by data sheets from textbooks, online learning materials,\nand academic papers. To compare models' quantitative reasoning abilities on\ndata and text, we enrich the benchmark with an auxiliary set of 290 text-only\nquestions, namely QRText. We evaluate natural language reasoning, program-based\nreasoning, and agent reasoning methods including Chain-of-Thought,\nProgram-of-Thoughts, ReAct, and code interpreter assistants on diverse models.\nThe strongest model GPT-4 achieves an accuracy of 58%, which has much room for\nimprovement. Among open-source models, Deepseek-coder-instruct, a code LLM\npretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals\nthat models encounter difficulties in data analysis and causal reasoning, and\nstruggle in using causal knowledge and provided data simultaneously. Code and\ndata are in https://github.com/xxxiaol/QRData.", "published": "2024-02-27 16:15:03", "link": "http://arxiv.org/abs/2402.17644v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond prompt brittleness: Evaluating the reliability and consistency of\n  political worldviews in LLMs", "abstract": "Due to the widespread use of large language models (LLMs), we need to\nunderstand whether they embed a specific \"worldview\" and what these views\nreflect. Recent studies report that, prompted with political questionnaires,\nLLMs show left-liberal leanings (Feng et al., 2023; Motoki et al., 2024).\nHowever, it is as yet unclear whether these leanings are reliable (robust to\nprompt variations) and whether the leaning is consistent across policies and\npolitical leaning. We propose a series of tests which assess the reliability\nand consistency of LLMs' stances on political statements based on a dataset of\nvoting-advice questionnaires collected from seven EU countries and annotated\nfor policy issues. We study LLMs ranging in size from 7B to 70B parameters and\nfind that their reliability increases with parameter count. Larger models show\noverall stronger alignment with left-leaning parties but differ among policy\nprograms: They show a (left-wing) positive stance towards environment\nprotection, social welfare state and liberal society but also (right-wing) law\nand order, with no consistent preferences in the areas of foreign policy and\nmigration.", "published": "2024-02-27 16:19:37", "link": "http://arxiv.org/abs/2402.17649v3", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "RAVEL: Evaluating Interpretability Methods on Disentangling Language\n  Model Representations", "abstract": "Individual neurons participate in the representation of multiple high-level\nconcepts. To what extent can different interpretability methods successfully\ndisentangle these roles? To help address this question, we introduce RAVEL\n(Resolving Attribute-Value Entanglements in Language Models), a dataset that\nenables tightly controlled, quantitative comparisons between a variety of\nexisting interpretability methods. We use the resulting conceptual framework to\ndefine the new method of Multi-task Distributed Alignment Search (MDAS), which\nallows us to find distributed representations satisfying multiple causal\ncriteria. With Llama2-7B as the target language model, MDAS achieves\nstate-of-the-art results on RAVEL, demonstrating the importance of going beyond\nneuron-level analyses to identify features distributed across activations. We\nrelease our benchmark at https://github.com/explanare/ravel.", "published": "2024-02-27 17:25:37", "link": "http://arxiv.org/abs/2402.17700v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Case-Based or Rule-Based: How Do Transformers Do the Math?", "abstract": "Despite the impressive performance in a variety of complex tasks, modern\nlarge language models (LLMs) still have trouble dealing with some math problems\nthat are simple and intuitive for humans, such as addition. While we can easily\nlearn basic rules of addition and apply them to new problems of any length,\nLLMs struggle to do the same. Instead, they may rely on similar cases seen in\nthe training corpus for help. We define these two different reasoning\nmechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since\nrule-based reasoning is essential for acquiring systematic generalization\nability, we aim to explore exactly whether transformers use rule-based or\ncase-based reasoning for math problems. Through carefully designed intervention\nexperiments on five math tasks, we confirm that transformers are performing\ncase-based reasoning, no matter whether scratchpad is used, which aligns with\nthe previous observations that transformers use subgraph matching/shortcut\nlearning to reason. To mitigate such problems, we propose a Rule-Following\nFine-Tuning (RFFT) technique to teach transformers to perform rule-based\nreasoning. Specifically, we provide explicit rules in the input and then\ninstruct transformers to recite and follow the rules step by step. Through\nRFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to\ngeneralize to up to 12-digit addition with over 95% accuracy, which is over 40%\nhigher than scratchpad. The significant improvement demonstrates that teaching\nLLMs to use rules explicitly helps them learn rule-based reasoning and\ngeneralize better in length.", "published": "2024-02-27 17:41:58", "link": "http://arxiv.org/abs/2402.17709v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Massive Activations in Large Language Models", "abstract": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations.", "published": "2024-02-27 18:55:17", "link": "http://arxiv.org/abs/2402.17762v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits", "abstract": "Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.", "published": "2024-02-27 18:56:19", "link": "http://arxiv.org/abs/2402.17764v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping\n  Backward Propagation", "abstract": "Large language models (LLMs) have achieved significant success across various\ndomains. However, training these LLMs typically involves substantial memory and\ncomputational costs during both forward and backward propagation. While\nparameter-efficient fine-tuning (PEFT) considerably reduces the training memory\nassociated with parameters, it does not address the significant computational\ncosts and activation memory. In this paper, we propose Dropping Backward\nPropagation (DropBP), a novel approach designed to reduce computational costs\nand activation memory while maintaining accuracy. DropBP randomly drops layers\nduring backward propagation, which is essentially equivalent to training\nshallow submodules generated by undropped layers and residual connections.\nAdditionally, DropBP calculates the sensitivity of each layer to assign an\nappropriate drop rate, thereby stabilizing the training process. DropBP is not\nonly applicable to full fine-tuning but can also be orthogonally integrated\nwith all types of PEFT by dropping layers during backward propagation.\nSpecifically, DropBP can reduce training time by 44% with comparable accuracy\nto the baseline, accelerate convergence to the same perplexity by 1.5x, and\nenable training with a sequence length 6.2x larger on a single NVIDIA-A100 GPU.\nFurthermore, our DropBP enabled a throughput increase of 79% on a NVIDIA A100\nGPU and 117% on an Intel Gaudi2 HPU. The code is available at\nhttps://github.com/WooSunghyeon/dropbp.", "published": "2024-02-27 14:51:11", "link": "http://arxiv.org/abs/2402.17812v4", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Stable LM 2 1.6B Technical Report", "abstract": "We introduce StableLM 2 1.6B, the first in a new generation of our language\nmodel series. In this technical report, we present in detail the data and\ntraining procedure leading to the base and instruction-tuned versions of\nStableLM 2 1.6B. The weights for both models are available via Hugging Face for\nanyone to download and use. The report contains thorough evaluations of these\nmodels, including zero- and few-shot benchmarks, multilingual benchmarks, and\nthe MT benchmark focusing on multi-turn dialogues. At the time of publishing\nthis report, StableLM 2 1.6B was the state-of-the-art open model under 2B\nparameters by a significant margin. Given its appealing small size, we also\nprovide throughput measurements on a number of edge devices. In addition, we\nopen source several quantized checkpoints and provide their performance metrics\ncompared to the original model.", "published": "2024-02-27 19:00:07", "link": "http://arxiv.org/abs/2402.17834v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Automated Statistical Model Discovery with Language Models", "abstract": "Statistical model discovery is a challenging search over a vast space of\nmodels subject to domain-specific constraints. Efficiently searching over this\nspace requires expertise in modeling and the problem domain. Motivated by the\ndomain knowledge and programming capabilities of large language models (LMs),\nwe introduce a method for language model driven automated statistical model\ndiscovery. We cast our automated procedure within the principled framework of\nBox's Loop: the LM iterates between proposing statistical models represented as\nprobabilistic programs, acting as a modeler, and critiquing those models,\nacting as a domain expert. By leveraging LMs, we do not have to define a\ndomain-specific language of models or design a handcrafted search procedure,\nwhich are key restrictions of previous systems. We evaluate our method in three\nsettings in probabilistic modeling: searching within a restricted space of\nmodels, searching over an open-ended space, and improving expert models under\nnatural language constraints (e.g., this model should be interpretable to an\necologist). Our method identifies models on par with human expert designed\nmodels and extends classic models in interpretable ways. Our results highlight\nthe promise of LM-driven model discovery.", "published": "2024-02-27 20:33:22", "link": "http://arxiv.org/abs/2402.17879v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning\n  and Professional Question Answering Capability", "abstract": "Large Language Models (LLMs) have demonstrated a remarkable potential in\nmedical knowledge acquisition and question-answering. However, LLMs can\npotentially hallucinate and yield factually incorrect outcomes, even with\ndomain-specific pretraining. Previously, retrieval augmented generation (RAG)\nhas limited success in addressing hallucinations. Unlike previous methods in\nRAG where the retrieval model was trained separately from the LLM, we introduce\nJMLR (for Jointly trains LLM and information Retrieval) during the fine-tuning\nphase. The synchronized training mechanism enhances JMLR's ability to retrieve\nclinical guidelines and leverage medical knowledge to reason and answer\nquestions and reduces the demand for computational resources. We evaluated JMLR\non the important medical question-answering application. Our experimental\nresults demonstrate that JMLR-13B (70.5%) outperforms a previous\nstate-of-the-art open-source model using conventional pre-training and\nfine-tuning Meditron-70B (68.9%) and Llama2-13B with RAG (67.7%) on a medical\nquestion-answering dataset. Comprehensive evaluations reveal JMLR-13B enhances\nreasoning quality and reduces hallucinations better than Claude3-Opus.\nAdditionally, JMLR-13B (148 GPU hours) also trains much faster than\nMeditron-70B (42630 GPU hours). Through this work, we provide a new and\nefficient knowledge enhancement method for healthcare, demonstrating the\npotential of integrating retrieval and LLM training for medical\nquestion-answering systems.", "published": "2024-02-27 21:01:41", "link": "http://arxiv.org/abs/2402.17887v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Researchy Questions: A Dataset of Multi-Perspective, Decompositional\n  Questions for LLM Web Agents", "abstract": "Existing question answering (QA) datasets are no longer challenging to most\npowerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA,\nNaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear\nindications of both what information is missing, and how to find it to answer\nthe question. Hence, good performance on these benchmarks provides a false\nsense of security. A yet unmet need of the NLP community is a bank of\nnon-factoid, multi-perspective questions involving a great deal of unclear\ninformation needs, i.e. ``unknown uknowns''. We claim we can find such\nquestions in search engine logs, which is surprising because most\nquestion-intent queries are indeed factoid. We present Researchy Questions, a\ndataset of search engine queries tediously filtered to be non-factoid,\n``decompositional'' and multi-perspective. We show that users spend a lot of\n``effort'' on these questions in terms of signals like clicks and session\nlength, and that they are also challenging for GPT-4. We also show that ``slow\nthinking'' answering techniques, like decomposition into sub-questions shows\nbenefit over answering directly. We release $\\sim$ 100k Researchy Questions,\nalong with the Clueweb22 URLs that were clicked.", "published": "2024-02-27 21:27:16", "link": "http://arxiv.org/abs/2402.17896v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Language Model based Framework for New Concept Placement in Ontologies", "abstract": "We investigate the task of inserting new concepts extracted from texts into\nan ontology using language models. We explore an approach with three steps:\nedge search which is to find a set of candidate locations to insert (i.e.,\nsubsumptions between concepts), edge formation and enrichment which leverages\nthe ontological structure to produce and enhance the edge candidates, and edge\nselection which eventually locates the edge to be placed into. In all steps, we\npropose to leverage neural methods, where we apply embedding-based methods and\ncontrastive learning with Pre-trained Language Models (PLMs) such as BERT for\nedge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder,\nand Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for\nedge selection. We evaluate the methods on recent datasets created using the\nSNOMED CT ontology and the MedMentions entity linking benchmark. The best\nsettings in our framework use fine-tuned PLM for search and a multi-label\nCross-encoder for selection. Zero-shot prompting of LLMs is still not adequate\nfor the task, and we propose explainable instruction tuning of LLMs for\nimproved performance. Our study shows the advantages of PLMs and highlights the\nencouraging performance of LLMs that motivates future studies.", "published": "2024-02-27 21:27:35", "link": "http://arxiv.org/abs/2402.17897v2", "categories": ["cs.CL", "cs.IR", "I.2.7; I.2.4"], "primary_category": "cs.CL"}
{"title": "Extracting Lexical Features from Dialects via Interpretable Dialect\n  Classifiers", "abstract": "Identifying linguistic differences between dialects of a language often\nrequires expert knowledge and meticulous human analysis. This is largely due to\nthe complexity and nuance involved in studying various dialects. We present a\nnovel approach to extract distinguishing lexical features of dialects by\nutilizing interpretable dialect classifiers, even in the absence of human\nexperts. We explore both post-hoc and intrinsic approaches to interpretability,\nconduct experiments on Mandarin, Italian, and Low Saxon, and experimentally\ndemonstrate that our method successfully identifies key language-specific\nlexical features that contribute to dialectal variations.", "published": "2024-02-27 22:06:55", "link": "http://arxiv.org/abs/2402.17914v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Adversarial Math Word Problem Generation", "abstract": "Large language models (LLMs) have significantly transformed the educational\nlandscape. As current plagiarism detection tools struggle to keep pace with\nLLMs' rapid advancements, the educational community faces the challenge of\nassessing students' true problem-solving abilities in the presence of LLMs. In\nthis work, we explore a new paradigm for ensuring fair evaluation -- generating\nadversarial examples which preserve the structure and difficulty of the\noriginal questions aimed for assessment, but are unsolvable by LLMs. Focusing\non the domain of math word problems, we leverage abstract syntax trees to\nstructurally generate adversarial examples that cause LLMs to produce incorrect\nanswers by simply editing the numeric values in the problems. We conduct\nexperiments on various open- and closed-source LLMs, quantitatively and\nqualitatively demonstrating that our method significantly degrades their math\nproblem-solving ability. We identify shared vulnerabilities among LLMs and\npropose a cost-effective approach to attack high-cost models. Additionally, we\nconduct automatic analysis to investigate the cause of failure, providing\nfurther insights into the limitations of LLMs.", "published": "2024-02-27 22:07:52", "link": "http://arxiv.org/abs/2402.17916v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Inducing Generalization across Languages and Tasks using Featurized\n  Low-Rank Mixtures", "abstract": "Adapting pretrained large language models (LLMs) to various downstream tasks\nin tens or hundreds of human languages is computationally expensive.\nParameter-efficient fine-tuning (PEFT) significantly reduces the adaptation\ncost, by tuning only a small amount of parameters. However, common PEFT methods\nLoRA (Hu et al., 2022) suffer from suboptimal performance on diverse dataset\nmixtures, due to aggressive parameter tying and negative interference among\ndifferent datasets. In this work, we propose Featurized Low-rank Mixtures\n(FLix), a novel PEFT method designed for effective multitask multilingual\nadaptation. FLix associates each unique dataset feature, such as the dataset's\nlanguage or task, with its own low-rank weight update parameters. By composing\nfeature-specific parameters for each dataset, FLix can accommodate diverse\ndataset mixtures and generalize better to unseen datasets. Our experiments show\nthat FLix leads to significant improvements over a variety of tasks for both\nsupervised learning and zero-shot settings with gains of up to $14.2$ inexact\nmatch points in zero-shot semantic parsing.", "published": "2024-02-27 23:12:45", "link": "http://arxiv.org/abs/2402.17934v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large\n  Language Models", "abstract": "This paper introduces EmMark,a novel watermarking framework for protecting\nthe intellectual property (IP) of embedded large language models deployed on\nresource-constrained edge devices. To address the IP theft risks posed by\nmalicious end-users, EmMark enables proprietors to authenticate ownership by\nquerying the watermarked model weights and matching the inserted signatures.\nEmMark's novelty lies in its strategic watermark weight parameters selection,\nnsuring robustness and maintaining model quality. Extensive proof-of-concept\nevaluations of models from OPT and LLaMA-2 families demonstrate EmMark's\nfidelity, achieving 100% success in watermark extraction with model performance\npreservation. EmMark also showcased its resilience against watermark removal\nand forging attacks.", "published": "2024-02-27 23:30:17", "link": "http://arxiv.org/abs/2402.17938v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Information Flow Routes: Automatically Interpreting Language Models at\n  Scale", "abstract": "Information flows by routes inside the network via mechanisms implemented in\nthe model. These routes can be represented as graphs where nodes correspond to\ntoken representations and edges to operations inside the network. We\nautomatically build these graphs in a top-down manner, for each prediction\nleaving only the most important nodes and edges. In contrast to the existing\nworkflows relying on activation patching, we do this through attribution: this\nallows us to efficiently uncover existing circuits with just a single forward\npass. Additionally, the applicability of our method is far beyond patching: we\ndo not need a human to carefully design prediction templates, and we can\nextract information flow routes for any prediction (not just the ones among the\nallowed templates). As a result, we can talk about model behavior in general,\nfor specific types of predictions, or different domains. We experiment with\nLlama 2 and show that the role of some attention heads is overall important,\ne.g. previous token heads and subword merging heads. Next, we find similarities\nin Llama 2 behavior when handling tokens of the same part of speech. Finally,\nwe show that some model components can be specialized on domains such as coding\nor multilingual texts.", "published": "2024-02-27 00:24:42", "link": "http://arxiv.org/abs/2403.00824v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained\n  Identification of Expressions", "abstract": "Media bias detection poses a complex, multifaceted problem traditionally\ntackled using single-task models and small in-domain datasets, consequently\nlacking generalizability. To address this, we introduce MAGPIE, the first\nlarge-scale multi-task pre-training approach explicitly tailored for media bias\ndetection. To enable pre-training at scale, we present Large Bias Mixture\n(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous\napproaches in media bias detection on the Bias Annotation By Experts (BABE)\ndataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs\nbetter than previous models on 5 out of 8 tasks in the Media Bias\nIdentification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%\nof finetuning steps compared to single-task approaches. Our evaluation shows,\nfor instance, that tasks like sentiment and emotionality boost all learning,\nall tasks enhance fake news detection, and scaling tasks leads to the best\nresults. MAGPIE confirms that MTL is a promising approach for addressing media\nbias detection, enhancing the accuracy and efficiency of existing models.\nFurthermore, LBM is the first available resource collection focused on media\nbias MTL.", "published": "2024-02-27 02:00:28", "link": "http://arxiv.org/abs/2403.07910v2", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "A Synergistic Approach to Wildfire Prevention and Management Using AI,\n  ML, and 5G Technology in the United States", "abstract": "Over the past few years, wildfires have become a worldwide environmental\nemergency, resulting in substantial harm to natural habitats and playing a part\nin the acceleration of climate change. Wildfire management methods involve\nprevention, response, and recovery efforts. Despite improvements in detection\ntechniques, the rising occurrence of wildfires demands creative solutions for\nprompt identification and effective control. This research investigates\nproactive methods for detecting and handling wildfires in the United States,\nutilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G\ntechnology. The specific objective of this research covers proactive detection\nand prevention of wildfires using advanced technology; Active monitoring and\nmapping with remote sensing and signaling leveraging on 5G technology; and\nAdvanced response mechanisms to wildfire using drones and IOT devices. This\nstudy was based on secondary data collected from government databases and\nanalyzed using descriptive statistics. In addition, past publications were\nreviewed through content analysis, and narrative synthesis was used to present\nthe observations from various studies. The results showed that developing new\ntechnology presents an opportunity to detect and manage wildfires proactively.\nUtilizing advanced technology could save lives and prevent significant economic\nlosses caused by wildfires. Various methods, such as AI-enabled remote sensing\nand 5G-based active monitoring, can enhance proactive wildfire detection and\nmanagement. In addition, super intelligent drones and IOT devices can be used\nfor safer responses to wildfires. This forms the core of the recommendation to\nthe fire Management Agencies and the government.", "published": "2024-02-27 04:09:30", "link": "http://arxiv.org/abs/2403.14657v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection", "abstract": "Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged\nas an effective approach for scaling Transformer models. However, they often\nsuffer from computational inefficiency since a significant number of parameters\nare unnecessarily involved in computations via multiplying values by zero or\nlow activation values. To address this issue, we present \\tool, a novel MoE\ndesigned to enhance both the efficacy and efficiency of sparse MoE models.\n\\tool leverages small experts and a threshold-based router to enable tokens to\nselectively engage only essential parameters. Our extensive experiments on\nlanguage modeling and machine translation tasks demonstrate that \\tool can\nenhance model performance while decreasing the computation load at MoE layers\nby over 50\\% without sacrificing performance. Furthermore, we present the\nversatility of \\tool by applying it to dense models, enabling sparse\ncomputation during inference. We provide a comprehensive analysis and make our\ncode available at https://github.com/ysngki/XMoE.", "published": "2024-02-27 08:18:02", "link": "http://arxiv.org/abs/2403.18926v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "OSCaR: Object State Captioning and State Change Representation", "abstract": "The capability of intelligent models to extrapolate and comprehend changes in\nobject states is a crucial yet demanding aspect of AI research, particularly\nthrough the lens of human interaction in real-world settings. This task\ninvolves describing complex visual environments, identifying active objects,\nand interpreting their changes as conveyed through language. Traditional\nmethods, which isolate object captioning and state change detection, offer a\nlimited view of dynamic environments. Moreover, relying on a small set of\nsymbolic words to represent changes has restricted the expressiveness of the\nlanguage. To address these challenges, in this paper, we introduce the Object\nState Captioning and State Change Representation (OSCaR) dataset and benchmark.\nOSCaR consists of 14,084 annotated video segments with nearly 1,000 unique\nobjects from various egocentric video collections. It sets a new testbed for\nevaluating multimodal large language models (MLLMs). Our experiments\ndemonstrate that while MLLMs show some skill, they lack a full understanding of\nobject state changes. The benchmark includes a fine-tuned model that, despite\ninitial capabilities, requires significant improvements in accuracy and\ngeneralization ability for effective understanding of these changes. Our code\nand dataset are available at https://github.com/nguyennm1024/OSCaR.", "published": "2024-02-27 01:48:19", "link": "http://arxiv.org/abs/2402.17128v4", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Extreme Encoder Output Frame Rate Reduction: Improving Computational\n  Latencies of Large End-to-End Models", "abstract": "The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models\ncontinues to improve as they are scaled to larger sizes, with some now reaching\nbillions of parameters. Widespread deployment and adoption of these models,\nhowever, requires computationally efficient strategies for decoding. In the\npresent work, we study one such strategy: applying multiple frame reduction\nlayers in the encoder to compress encoder outputs into a small number of output\nframes. While similar techniques have been investigated in previous work, we\nachieve dramatically more reduction than has previously been demonstrated\nthrough the use of multiple funnel reduction layers. Through ablations, we\nstudy the impact of various architectural choices in the encoder to identify\nthe most effective strategies. We demonstrate that we can generate one encoder\noutput frame for every 2.56 sec of input speech, without significantly\naffecting word error rate on a large-scale voice search task, while improving\nencoder and decoder latencies by 48% and 92% respectively, relative to a strong\nbut computationally expensive baseline.", "published": "2024-02-27 03:40:44", "link": "http://arxiv.org/abs/2402.17184v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Effective Mixture-Of-Experts Approach For Code-Switching Speech\n  Recognition Leveraging Encoder Disentanglement", "abstract": "With the massive developments of end-to-end (E2E) neural networks, recent\nyears have witnessed unprecedented breakthroughs in automatic speech\nrecognition (ASR). However, the codeswitching phenomenon remains a major\nobstacle that hinders ASR from perfection, as the lack of labeled data and the\nvariations between languages often lead to degradation of ASR performance. In\nthis paper, we focus exclusively on improving the acoustic encoder of E2E ASR\nto tackle the challenge caused by the codeswitching phenomenon. Our main\ncontributions are threefold: First, we introduce a novel disentanglement loss\nto enable the lower-layer of the encoder to capture inter-lingual acoustic\ninformation while mitigating linguistic confusion at the higher-layer of the\nencoder. Second, through comprehensive experiments, we verify that our proposed\nmethod outperforms the prior-art methods using pretrained dual-encoders,\nmeanwhile having access only to the codeswitching corpus and consuming half of\nthe parameterization. Third, the apparent differentiation of the encoders'\noutput features also corroborates the complementarity between the\ndisentanglement loss and the mixture-of-experts (MoE) architecture.", "published": "2024-02-27 04:08:59", "link": "http://arxiv.org/abs/2402.17189v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Measuring Vision-Language STEM Skills of Neural Models", "abstract": "We introduce a new challenge to test the STEM skills of neural models. The\nproblems in the real world often require solutions, combining knowledge from\nSTEM (science, technology, engineering, and math). Unlike existing datasets,\nour dataset requires the understanding of multimodal vision-language\ninformation of STEM. Our dataset features one of the largest and most\ncomprehensive datasets for the challenge. It includes 448 skills and 1,073,146\nquestions spanning all STEM subjects. Compared to existing datasets that often\nfocus on examining expert-level ability, our dataset includes fundamental\nskills and questions designed based on the K-12 curriculum. We also add\nstate-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our\nbenchmark. Results show that the recent model advances only help master a very\nlimited number of lower grade-level skills (2.5% in the third grade) in our\ndataset. In fact, these models are still well below (averaging 54.7%) the\nperformance of elementary students, not to mention near expert-level\nperformance. To understand and increase the performance on our dataset, we\nteach the models on a training split of our dataset. Even though we observe\nimproved performance, the model performance remains relatively low compared to\naverage elementary students. To solve STEM problems, we will need novel\nalgorithmic innovations from the community.", "published": "2024-02-27 04:55:03", "link": "http://arxiv.org/abs/2402.17205v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of\n  Prompting Strategies", "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing thanks to their ability to reuse knowledge acquired on\nmassive text corpora on a wide variety of downstream tasks, with minimal (if\nany) tuning steps. At the same time, it has been repeatedly shown that LLMs\nlack systematic generalization, which allows to extrapolate the learned\nstatistical regularities outside the training distribution. In this work, we\noffer a systematic benchmarking of GPT-4, one of the most advanced LLMs\navailable, on three algorithmic tasks characterized by the possibility to\ncontrol the problem difficulty with two parameters. We compare the performance\nof GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the\nTransformer-Encoder architecture recently introduced to solve similar tasks,\nthe Neural Data Router. We find that the deployment of advanced prompting\ntechniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating\nthat state-of-the-art LLMs constitute a very strong baseline also in\nchallenging tasks that require systematic generalization.", "published": "2024-02-27 10:44:52", "link": "http://arxiv.org/abs/2402.17396v2", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
{"title": "A Neural Rewriting System to Solve Algorithmic Problems", "abstract": "Modern neural network architectures still struggle to learn algorithmic\nprocedures that require to systematically apply compositional rules to solve\nout-of-distribution problem instances. In this work, we focus on formula\nsimplification problems, a class of synthetic benchmarks used to study the\nsystematic generalization capabilities of neural architectures. We propose a\nmodular architecture designed to learn a general procedure for solving nested\nmathematical formulas by only relying on a minimal set of training examples.\nInspired by rewriting systems, a classic framework in symbolic artificial\nintelligence, we include in the architecture three specialized and interacting\nmodules: the Selector, trained to identify solvable sub-expressions; the\nSolver, mapping sub-expressions to their values; and the Combiner, replacing\nsub-expressions in the original formula with the solution provided by the\nSolver. We benchmark our system against the Neural Data Router, a recent model\nspecialized for systematic generalization, and a state-of-the-art large\nlanguage model (GPT-4) probed with advanced prompting strategies. We\ndemonstrate that our approach achieves a higher degree of out-of-distribution\ngeneralization compared to these alternative approaches on three different\ntypes of formula simplification problems, and we discuss its limitations by\nanalyzing its failures.", "published": "2024-02-27 10:57:07", "link": "http://arxiv.org/abs/2402.17407v2", "categories": ["cs.NE", "cs.AI", "cs.CL"], "primary_category": "cs.NE"}
{"title": "Deep Learning Based Named Entity Recognition Models for Recipes", "abstract": "Food touches our lives through various endeavors, including flavor,\nnourishment, health, and sustainability. Recipes are cultural capsules\ntransmitted across generations via unstructured text. Automated protocols for\nrecognizing named entities, the building blocks of recipe text, are of immense\nvalue for various applications ranging from information extraction to novel\nrecipe generation. Named entity recognition is a technique for extracting\ninformation from unstructured or semi-structured data with known labels.\nStarting with manually-annotated data of 6,611 ingredient phrases, we created\nan augmented dataset of 26,445 phrases cumulatively. Simultaneously, we\nsystematically cleaned and analyzed ingredient phrases from RecipeDB, the\ngold-standard recipe data repository, and annotated them using the Stanford\nNER. Based on the analysis, we sampled a subset of 88,526 phrases using a\nclustering-based approach while preserving the diversity to create the\nmachine-annotated dataset. A thorough investigation of NER approaches on these\nthree datasets involving statistical, fine-tuning of deep learning-based\nlanguage models and few-shot prompting on large language models (LLMs) provides\ndeep insights. We conclude that few-shot prompting on LLMs has abysmal\nperformance, whereas the fine-tuned spaCy-transformer emerges as the best model\nwith macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,\naugmented, and machine-annotated datasets, respectively.", "published": "2024-02-27 12:03:56", "link": "http://arxiv.org/abs/2402.17447v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to\n  Assist Adolescent Cyberbullying Education", "abstract": "Cyberbullying harms teenagers' mental health, and teaching them upstanding\nintervention is crucial. Wizard-of-Oz studies show chatbots can scale up\npersonalized and interactive cyberbullying education, but implementing such\nchatbots is a challenging and delicate task. We created a no-code chatbot\ndesign tool for K-12 teachers. Using large language models and prompt chaining,\nour tool allows teachers to prototype bespoke dialogue flows and chatbot\nutterances. In offering this tool, we explore teachers' distinctive needs when\ndesigning chatbots to assist their teaching, and how chatbot design tools might\nbetter support them. Our findings reveal that teachers welcome the tool\nenthusiastically. Moreover, they see themselves as playwrights guiding both the\nstudents' and the chatbot's behaviors, while allowing for some improvisation.\nTheir goal is to enable students to rehearse both desirable and undesirable\nreactions to cyberbullying in a safe environment. We discuss the design\nopportunities LLM-Chains offer for empowering teachers and the research\nopportunities this work opens up.", "published": "2024-02-27 12:27:51", "link": "http://arxiv.org/abs/2402.17456v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Emotional Voice Messages (EMOVOME) database: emotion recognition in\n  spontaneous voice messages", "abstract": "Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing\n999 audio messages from real conversations on a messaging app from 100 Spanish\nspeakers, gender balanced. Voice messages were produced in-the-wild conditions\nbefore participants were recruited, avoiding any conscious bias due to\nlaboratory environment. Audios were labeled in valence and arousal dimensions\nby three non-experts and two experts, which were then combined to obtain a\nfinal label per dimension. The experts also provided an extra label\ncorresponding to seven emotion categories. To set a baseline for future\ninvestigations using EMOVOME, we implemented emotion recognition models using\nboth speech and audio transcriptions. For speech, we used the standard eGeMAPS\nfeature set and support vector machines, obtaining 49.27% and 44.71% unweighted\naccuracy for valence and arousal respectively. For text, we fine-tuned a\nmultilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for\nvalence and arousal respectively. This database will significantly contribute\nto research on emotion recognition in the wild, while also providing a unique\nnatural and freely accessible resource for Spanish.", "published": "2024-02-27 13:22:47", "link": "http://arxiv.org/abs/2402.17496v2", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS", "I.5.1; I.5.4; I.2.7"], "primary_category": "cs.SD"}
{"title": "Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides", "abstract": "Effective incident management is pivotal for the smooth operation of\nenterprises-level cloud services. In order to expedite incident mitigation,\nservice teams compile troubleshooting knowledge into Troubleshooting Guides\n(TSGs) accessible to on-call engineers (OCEs). While automated pipelines are\nenabled to resolve the most frequent and easy incidents, there still exist\ncomplex incidents that require OCEs' intervention. However, TSGs are often\nunstructured and incomplete, which requires manual interpretation by OCEs,\nleading to on-call fatigue and decreased productivity, especially among\nnew-hire OCEs. In this work, we propose Nissist which leverages TSGs and\nincident mitigation histories to provide proactive suggestions, reducing human\nintervention. Leveraging Large Language Models (LLM), Nissist extracts insights\nfrom unstructured TSGs and historical incident mitigation discussions, forming\na comprehensive knowledge base. Its multi-agent system design enhances\nproficiency in precisely discerning user queries, retrieving relevant\ninformation, and delivering systematic plans consecutively. Through our user\ncase and experiment, we demonstrate that Nissist significant reduce Time to\nMitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs\nand improving service reliability. Our demo is available at\nhttps://aka.ms/nissist_demo.", "published": "2024-02-27 14:14:23", "link": "http://arxiv.org/abs/2402.17531v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist\n  Autonomous Agents for Desktop and Web", "abstract": "For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.", "published": "2024-02-27 14:47:53", "link": "http://arxiv.org/abs/2402.17553v3", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "cs.AI"}
{"title": "FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in\n  Large-scale Cloud Systems", "abstract": "Postmortem analysis is essential in the management of incidents within cloud\nsystems, which provides valuable insights to improve system's reliability and\nrobustness. At CloudA, fault pattern profiling is performed during the\npostmortem phase, which involves the classification of incidents' faults into\nunique categories, referred to as fault pattern. By aggregating and analyzing\nthese fault patterns, engineers can discern common faults, vulnerable\ncomponents and emerging fault trends. However, this process is currently\nconducted by manual labeling, which has inherent drawbacks. On the one hand,\nthe sheer volume of incidents means only the most severe ones are analyzed,\ncausing a skewed overview of fault patterns. On the other hand, the complexity\nof the task demands extensive domain knowledge, which leads to errors and\ninconsistencies. To address these limitations, we propose an automated\napproach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.\nIt leverages hierarchy-guided contrastive learning to train a hierarchy-aware\nincident encoder and predicts fault patterns with enhanced incident\nrepresentations. We evaluate FaultProfIT using the production incidents from\nCloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art\nmethods. Our ablation study and analysis also verify the effectiveness of\nhierarchy-guided contrastive learning. Additionally, we have deployed\nFaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+\nincidents from 30+ cloud services, successfully revealing several fault trends\nthat have informed system improvements.", "published": "2024-02-27 15:14:19", "link": "http://arxiv.org/abs/2402.17583v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "SongComposer: A Large Language Model for Lyric and Melody Composition in\n  Song Generation", "abstract": "We present SongComposer, an innovative LLM designed for song composition. It\ncould understand and generate melodies and lyrics in symbolic song\nrepresentations, by leveraging the capability of LLM. Existing music-related\nLLM treated the music as quantized audio signals, while such implicit encoding\nleads to inefficient encoding and poor flexibility. In contrast, we resort to\nsymbolic song representation, the mature and efficient way humans designed for\nmusic, and enable LLM to explicitly compose songs like humans. In practice, we\ndesign a novel tuple design to format lyric and three note attributes (pitch,\nduration, and rest duration) in the melody, which guarantees the correct LLM\nunderstanding of musical symbols and realizes precise alignment between lyrics\nand melody. To impart basic music understanding to LLM, we carefully collected\nSongCompose-PT, a large-scale song pretraining dataset that includes lyrics,\nmelodies, and paired lyrics-melodies in either Chinese or English. After\nadequate pre-training, 10K carefully crafted QA pairs are used to empower the\nLLM with the instruction-following capability and solve diverse tasks. With\nextensive experiments, SongComposer demonstrates superior performance in\nlyric-to-melody generation, melody-to-lyric generation, song continuation, and\ntext-to-song creation, outperforming advanced LLMs like GPT-4.", "published": "2024-02-27 16:15:28", "link": "http://arxiv.org/abs/2402.17645v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating Very Long-Term Conversational Memory of LLM Agents", "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.", "published": "2024-02-27 18:42:31", "link": "http://arxiv.org/abs/2402.17753v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in\n  Truthful Space", "abstract": "Large Language Models (LLMs) sometimes suffer from producing hallucinations,\nespecially LLMs may generate untruthful responses despite knowing the correct\nknowledge. Activating the truthfulness within LLM is the key to fully unlocking\nLLM's knowledge potential. In this paper, we propose TruthX, an inference-time\nintervention method to activate the truthfulness of LLM by identifying and\nediting the features within LLM's internal representations that govern the\ntruthfulness. TruthX employs an auto-encoder to map LLM's representations into\nsemantic and truthful latent spaces respectively, and applies contrastive\nlearning to identify a truthful editing direction within the truthful space.\nDuring inference, by editing LLM's internal representations in truthful space,\nTruthX effectively enhances the truthfulness of LLM. Experiments show that\nTruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on\nTruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to\nproduce truthful or hallucinatory responses via editing only one vector in\nLLM's internal representations.", "published": "2024-02-27 14:45:04", "link": "http://arxiv.org/abs/2402.17811v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Follow My Instruction and Spill the Beans: Scalable Data Extraction from\n  Retrieval-Augmented Generation Systems", "abstract": "Retrieval-Augmented Generation (RAG) improves pre-trained models by\nincorporating external knowledge at test time to enable customized adaptation.\nWe study the risk of datastore leakage in Retrieval-In-Context RAG Language\nModels (LMs). We show that an adversary can exploit LMs' instruction-following\ncapabilities to easily extract text data verbatim from the datastore of RAG\nsystems built with instruction-tuned LMs via prompt injection. The\nvulnerability exists for a wide range of modern LMs that span Llama2,\nMistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the\nexploitability exacerbates as the model size scales up. We also study multiple\neffects of RAG setup on the extractability of data, indicating that following\nunexpected instructions to regurgitate data can be an outcome of failure in\neffectively utilizing contexts for modern LMs, and further show that such\nvulnerability can be greatly mitigated by position bias elimination strategies.\nExtending our study to production RAG models GPTs, we design an attack that can\ncause datastore leakage with a 100% success rate on 25 randomly selected\ncustomized GPTs with at most 2 queries, and we extract text data verbatim at a\nrate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words\nby prompting the GPTs with only 100 queries generated by themselves.", "published": "2024-02-27 19:08:05", "link": "http://arxiv.org/abs/2402.17840v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pragmatic Instruction Following and Goal Assistance via Cooperative\n  Language-Guided Inverse Planning", "abstract": "People often give instructions whose meaning is ambiguous without further\ncontext, expecting that their actions or goals will disambiguate their\nintentions. How can we build assistive agents that follow such instructions in\na flexible, context-sensitive manner? This paper introduces cooperative\nlanguage-guided inverse plan search (CLIPS), a Bayesian agent architecture for\npragmatic instruction following and goal assistance. Our agent assists a human\nby modeling them as a cooperative planner who communicates joint plans to the\nassistant, then performs multimodal Bayesian inference over the human's goal\nfrom actions and language, using large language models (LLMs) to evaluate the\nlikelihood of an instruction given a hypothesized plan. Given this posterior,\nour assistant acts to minimize expected goal achievement cost, enabling it to\npragmatically follow ambiguous instructions and provide effective assistance\neven when uncertain about the goal. We evaluate these capabilities in two\ncooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that\nCLIPS significantly outperforms GPT-4V, LLM-based literal instruction following\nand unimodal inverse planning in both accuracy and helpfulness, while closely\nmatching the inferences and assistive judgments provided by human raters.", "published": "2024-02-27 23:06:53", "link": "http://arxiv.org/abs/2402.17930v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "LLMGuard: Guarding Against Unsafe LLM Behavior", "abstract": "Although the rise of Large Language Models (LLMs) in enterprise settings\nbrings new opportunities and capabilities, it also brings challenges, such as\nthe risk of generating inappropriate, biased, or misleading content that\nviolates regulations and can have legal concerns. To alleviate this, we present\n\"LLMGuard\", a tool that monitors user interactions with an LLM application and\nflags content against specific behaviours or conversation topics. To do this\nrobustly, LLMGuard employs an ensemble of detectors.", "published": "2024-02-27 10:22:45", "link": "http://arxiv.org/abs/2403.00826v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-Refinement of Language Models from External Proxy Metrics Feedback", "abstract": "It is often desirable for Large Language Models (LLMs) to capture multiple\nobjectives when providing a response. In document-grounded response generation,\nfor example, agent responses are expected to be relevant to a user's query\nwhile also being grounded in a given document. In this paper, we introduce\nProxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine\nits own initial response along key dimensions of quality guided by external\nmetrics feedback, yielding an overall better final response. ProMiSe leverages\nfeedback on response quality through principle-specific proxy metrics, and\niteratively refines its response one principle at a time. We apply ProMiSe to\nopen source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its\nperformance on document-grounded question answering datasets, MultiDoc2Dial and\nQuAC, demonstrating that self-refinement improves response quality. We further\nshow that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated\nby ProMiSe yields significant performance improvements over the zero-shot\nbaseline as well as a supervised fine-tuned model on human annotated data.", "published": "2024-02-27 19:13:01", "link": "http://arxiv.org/abs/2403.00827v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning Detection Method for Large Language Models-Generated\n  Scientific Content", "abstract": "Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual\ncontent is written and communicated. These models have the potential to\ngenerate scientific content that is indistinguishable from that written by\nhumans. Hence, LLMs carry severe consequences for the scientific community,\nwhich relies on the integrity and reliability of publications. This research\npaper presents a novel ChatGPT-generated scientific text detection method,\nAI-Catcher. AI-Catcher integrates two deep learning models, multilayer\nperceptron (MLP) and convolutional neural networks (CNN). The MLP learns the\nfeature representations of the linguistic and statistical features. The CNN\nextracts high-level representations of the sequential patterns from the textual\ncontent. AI-Catcher is a multimodal model that fuses hidden patterns derived\nfrom MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset\nis collected to enhance AI-generated text detection tools, AIGTxt. AIGTxt\ncontains 3000 records collected from published academic articles across ten\ndomains and divided into three classes: Human-written, ChatGPT-generated, and\nMixed text. Several experiments are conducted to evaluate the performance of\nAI-Catcher. The comparative results demonstrate the capability of AI-Catcher to\ndistinguish between human-written and ChatGPT-generated scientific text more\naccurately than alternative methods. On average, AI-Catcher improved accuracy\nby 37.4%.", "published": "2024-02-27 19:16:39", "link": "http://arxiv.org/abs/2403.00828v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Variational Learning is Effective for Large Deep Networks", "abstract": "We give extensive empirical evidence against the common belief that\nvariational learning is ineffective for large neural networks. We show that an\noptimizer called Improved Variational Online Newton (IVON) consistently matches\nor outperforms Adam for training large networks such as GPT-2 and ResNets from\nscratch. IVON's computational costs are nearly identical to Adam but its\npredictive uncertainty is better. We show several new use cases of IVON where\nwe improve finetuning and model merging in Large Language Models, accurately\npredict generalization error, and faithfully estimate sensitivity to data. We\nfind overwhelming evidence that variational learning is effective.", "published": "2024-02-27 16:11:05", "link": "http://arxiv.org/abs/2402.17641v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Prediction-Powered Ranking of Large Language Models", "abstract": "Large language models are often ranked according to their level of alignment\nwith human preferences -- a model is better than other models if its outputs\nare more frequently preferred by humans. One of the popular ways to elicit\nhuman preferences utilizes pairwise comparisons between the outputs provided by\ndifferent models to the same inputs. However, since gathering pairwise\ncomparisons by humans is costly and time-consuming, it has become a common\npractice to gather pairwise comparisons by a strong large language model -- a\nmodel strongly aligned with human preferences. Surprisingly, practitioners\ncannot currently measure the uncertainty that any mismatch between human and\nmodel preferences may introduce in the constructed rankings. In this work, we\ndevelop a statistical framework to bridge this gap. Given a (small) set of\npairwise comparisons by humans and a large set of pairwise comparisons by a\nmodel, our framework provides a rank-set -- a set of possible ranking positions\n-- for each of the models under comparison. Moreover, it guarantees that, with\na probability greater than or equal to a user-specified value, the rank-sets\ncover the true ranking consistent with the distribution of human pairwise\npreferences asymptotically. Using pairwise comparisons made by humans in the\nLMSYS Chatbot Arena platform and pairwise comparisons made by three strong\nlarge language models, we empirically demonstrate the effectivity of our\nframework and show that the rank-sets constructed using only pairwise\ncomparisons by the strong large language models are often inconsistent with\n(the distribution of) human pairwise preferences.", "published": "2024-02-27 19:00:01", "link": "http://arxiv.org/abs/2402.17826v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "cs.HC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Target Speaker Extraction by Directly Exploiting Contextual Information\n  in the Time-Frequency Domain", "abstract": "In target speaker extraction, many studies rely on the speaker embedding\nwhich is obtained from an enrollment of the target speaker and employed as the\nguidance. However, solely using speaker embedding may not fully utilize the\ncontextual information contained in the enrollment. In this paper, we directly\nexploit this contextual information in the time-frequency (T-F) domain.\nSpecifically, the T-F representations of the enrollment and the mixed signal\nare interacted to compute the weighting matrices through an attention\nmechanism. These weighting matrices reflect the similarity among different\nframes of the T-F representations and are further employed to obtain the\nconsistent T-F representations of the enrollment. These consistent\nrepresentations are served as the guidance, allowing for better exploitation of\nthe contextual information. Furthermore, the proposed method achieves the\nstate-of-the-art performance on the benchmark dataset and shows its\neffectiveness in the complex scenarios.", "published": "2024-02-27 02:18:55", "link": "http://arxiv.org/abs/2402.17146v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Ambisonics Encoding For Arbitrary Microphone Arrays Incorporating\n  Residual Channels For Binaural Reproduction", "abstract": "In the rapidly evolving fields of virtual and augmented reality, accurate\nspatial audio capture and reproduction are essential. For these applications,\nAmbisonics has emerged as a standard format. However, existing methods for\nencoding Ambisonics signals from arbitrary microphone arrays face challenges,\nsuch as errors due to the irregular array configurations and limited spatial\nresolution resulting from a typically small number of microphones. To address\nthese limitations and challenges, a mathematical framework for studying\nAmbisonics encoding is presented, highlighting the importance of incorporating\nthe full steering function, and providing a novel measure for predicting the\naccuracy of encoding each Ambisonics channel from the steering functions alone.\nFurthermore, novel residual channels are formulated supplementing the\nAmbisonics channels. A simulation study for several array configurations\ndemonstrates a reduction in binaural error for this approach.", "published": "2024-02-27 09:54:40", "link": "http://arxiv.org/abs/2402.17362v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "CLAPSep: Leveraging Contrastive Pre-trained Model for Multi-Modal\n  Query-Conditioned Target Sound Extraction", "abstract": "Universal sound separation (USS) aims to extract arbitrary types of sounds\nfrom real-world recordings. This can be achieved by language-queried target\nsound extraction (TSE), which typically consists of two components: a query\nnetwork that converts user queries into conditional embeddings, and a\nseparation network that extracts the target sound accordingly. Existing methods\ncommonly train models from scratch. As a consequence, substantial data and\ncomputational resources are required to make the randomly initialized model\ncomprehend sound events and perform separation accordingly. In this paper, we\npropose to integrate pre-trained models into TSE models to address the above\nissue. To be specific, we tailor and adapt the powerful contrastive\nlanguage-audio pre-trained model (CLAP) for USS, denoted as CLAPSep. CLAPSep\nalso accepts flexible user inputs, taking both positive and negative user\nprompts of uni- and/or multi-modalities for target sound extraction. These key\nfeatures of CLAPSep can not only enhance the extraction performance but also\nimprove the versatility of its application. We provide extensive experiments on\n5 diverse datasets to demonstrate the superior performance and zero- and\nfew-shot generalizability of our proposed CLAPSep with fast training\nconvergence, surpassing previous methods by a significant margin. Full codes\nand some audio examples are released for reproduction and evaluation.", "published": "2024-02-27 12:27:18", "link": "http://arxiv.org/abs/2402.17455v5", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Experimental Study: Enhancing Voice Spoofing Detection Models with\n  wav2vec 2.0", "abstract": "Conventional spoofing detection systems have heavily relied on the use of\nhandcrafted features derived from speech data. However, a notable shift has\nrecently emerged towards the direct utilization of raw speech waveforms, as\ndemonstrated by methods like SincNet filters. This shift underscores the demand\nfor more sophisticated audio sample features. Moreover, the success of deep\nlearning models, particularly those utilizing large pretrained wav2vec 2.0 as a\nfeaturization front-end, highlights the importance of refined feature encoders.\nIn response, this research assessed the representational capability of wav2vec\n2.0 as an audio feature extractor, modifying the size of its pretrained\nTransformer layers through two key adjustments: (1) selecting a subset of\nlayers starting from the leftmost one and (2) fine-tuning a portion of the\nselected layers from the rightmost one. We complemented this analysis with five\nspoofing detection back-end models, with a primary focus on AASIST, enabling us\nto pinpoint the optimal configuration for the selection and fine-tuning\nprocess. In contrast to conventional handcrafted features, our investigation\nidentified several spoofing detection systems that achieve state-of-the-art\nperformance in the ASVspoof 2019 LA dataset. This comprehensive exploration\noffers valuable insights into feature selection strategies, advancing the field\nof spoofing detection.", "published": "2024-02-27 01:45:51", "link": "http://arxiv.org/abs/2402.17127v1", "categories": ["cs.SD", "eess.AS", "00A71", "I.2.6"], "primary_category": "cs.SD"}
{"title": "EDTC: enhance depth of text comprehension in automated audio captioning", "abstract": "Modality discrepancies have perpetually posed significant challenges within\nthe realm of Automated Audio Captioning (AAC) and across all multi-modal\ndomains. Facilitating models in comprehending text information plays a pivotal\nrole in establishing a seamless connection between the two modalities of text\nand audio. While recent research has focused on closing the gap between these\ntwo modalities through contrastive learning, it is challenging to bridge the\ndifference between both modalities using only simple contrastive loss. This\npaper introduces Enhance Depth of Text Comprehension (EDTC), which enhances the\nmodel's understanding of text information from three different perspectives.\nFirst, we propose a novel fusion module, FUSER, which aims to extract shared\nsemantic information from different audio features through feature fusion. We\nthen introduced TRANSLATOR, a novel alignment module designed to align audio\nfeatures and text features along the tensor level. Finally, the weights are\nupdated by adding momentum to the twin structure so that the model can learn\ninformation about both modalities at the same time. The resulting method\nachieves state-of-the-art performance on AudioCaps datasets and demonstrates\nresults comparable to the state-of-the-art on Clotho datasets.", "published": "2024-02-27 07:05:22", "link": "http://arxiv.org/abs/2402.17259v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "High-Fidelity Neural Phonetic Posteriorgrams", "abstract": "A phonetic posteriorgram (PPG) is a time-varying categorical distribution\nover acoustic units of speech (e.g., phonemes). PPGs are a popular\nrepresentation in speech generation due to their ability to disentangle\npronunciation features from speaker identity, allowing accurate reconstruction\nof pronunciation (e.g., voice conversion) and coarse-grained pronunciation\nediting (e.g., foreign accent conversion). In this paper, we demonstrably\nimprove the quality of PPGs to produce a state-of-the-art interpretable PPG\nrepresentation. We train an off-the-shelf speech synthesizer using our PPG\nrepresentation and show that high-quality PPGs yield independent control over\npitch and pronunciation. We further demonstrate novel uses of PPGs, such as an\nacoustic pronunciation distance and fine-grained pronunciation control.", "published": "2024-02-27 18:12:43", "link": "http://arxiv.org/abs/2402.17735v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NIIRF: Neural IIR Filter Field for HRTF Upsampling and Personalization", "abstract": "Head-related transfer functions (HRTFs) are important for immersive audio,\nand their spatial interpolation has been studied to upsample finite\nmeasurements. Recently, neural fields (NFs) which map from sound source\ndirection to HRTF have gained attention. Existing NF-based methods focused on\nestimating the magnitude of the HRTF from a given sound source direction, and\nthe magnitude is converted to a finite impulse response (FIR) filter. We\npropose the neural infinite impulse response filter field (NIIRF) method that\ninstead estimates the coefficients of cascaded IIR filters. IIR filters mimic\nthe modal nature of HRTFs, thus needing fewer coefficients to approximate them\nwell compared to FIR filters. We find that our method can match the performance\nof existing NF-based methods on multiple datasets, even outperforming them when\nmeasurements are sparse. We also explore approaches to personalize the NF to a\nsubject and experimentally find low-rank adaptation to be effective.", "published": "2024-02-27 21:48:41", "link": "http://arxiv.org/abs/2402.17907v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Natural Language Processing Methods for Symbolic Music Generation and\n  Information Retrieval: a Survey", "abstract": "Several adaptations of Transformers models have been developed in various\ndomains since its breakthrough in Natural Language Processing (NLP). This trend\nhas spread into the field of Music Information Retrieval (MIR), including\nstudies processing music data. However, the practice of leveraging NLP tools\nfor symbolic music data is not novel in MIR. Music has been frequently compared\nto language, as they share several similarities, including sequential\nrepresentations of text and music. These analogies are also reflected through\nsimilar tasks in MIR and NLP. This survey reviews NLP methods applied to\nsymbolic music generation and information retrieval studies following two axes.\nWe first propose an overview of representations of symbolic music adapted from\nnatural language sequential representations. Such representations are designed\nby considering the specificities of symbolic music. These representations are\nthen processed by models. Such models, possibly originally developed for text\nand adapted for symbolic music, are trained on various tasks. We describe these\nmodels, in particular deep learning models, through different prisms,\nhighlighting music-specialized mechanisms. We finally present a discussion\nsurrounding the effective use of NLP tools for symbolic music data. This\nincludes technical issues regarding NLP methods and fundamental differences\nbetween text and music, which may open several doors for further research into\nmore effectively adapting NLP tools to symbolic MIR.", "published": "2024-02-27 12:48:01", "link": "http://arxiv.org/abs/2402.17467v1", "categories": ["cs.IR", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Automated Classification of Phonetic Segments in Child Speech Using Raw\n  Ultrasound Imaging", "abstract": "Speech sound disorder (SSD) is defined as a persistent impairment in speech\nsound production leading to reduced speech intelligibility and hindered verbal\ncommunication. Early recognition and intervention of children with SSD and\ntimely referral to speech and language therapists (SLTs) for treatment are\ncrucial. Automated detection of speech impairment is regarded as an efficient\nmethod for examining and screening large populations. This study focuses on\nadvancing the automatic diagnosis of SSD in early childhood by proposing a\ntechnical solution that integrates ultrasound tongue imaging (UTI) with\ndeep-learning models. The introduced FusionNet model combines UTI data with the\nextracted texture features to classify UTI. The overarching aim is to elevate\nthe accuracy and efficiency of UTI analysis, particularly for classifying\nspeech sounds associated with SSD. This study compared the FusionNet approach\nwith standard deep-learning methodologies, highlighting the excellent\nimprovement results of the FusionNet model in UTI classification and the\npotential of multi-learning in improving UTI classification in speech therapy\nclinics.", "published": "2024-02-27 13:08:34", "link": "http://arxiv.org/abs/2402.17482v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Real-time Low-latency Music Source Separation using Hybrid\n  Spectrogram-TasNet", "abstract": "There have been significant advances in deep learning for music demixing in\nrecent years. However, there has been little attention given to how these\nneural networks can be adapted for real-time low-latency applications, which\ncould be helpful for hearing aids, remixing audio streams and live shows. In\nthis paper, we investigate the various challenges involved in adapting current\ndemixing models in the literature for this use case. Subsequently, inspired by\nthe Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain\nAudio Separation Network HS-TasNet, which utilises the advantages of spectral\nand waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall\nsignal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases\nto 5.55 with additional training data. These results demonstrate the potential\nof efficient demixing for real-time low-latency music applications.", "published": "2024-02-27 17:26:33", "link": "http://arxiv.org/abs/2402.17701v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "I.5.1; I.5.4"], "primary_category": "eess.AS"}
{"title": "Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion\n  Latent Aligners", "abstract": "Video and audio content creation serves as the core technique for the movie\nindustry and professional users. Recently, existing diffusion-based methods\ntackle video and audio generation separately, which hinders the technique\ntransfer from academia to industry. In this work, we aim at filling the gap,\nwith a carefully designed optimization-based framework for cross-visual-audio\nand joint-visual-audio generation. We observe the powerful generation ability\nof off-the-shelf video or audio generation models. Thus, instead of training\nthe giant models from scratch, we propose to bridge the existing strong models\nwith a shared latent representation space. Specifically, we propose a\nmultimodality latent aligner with the pre-trained ImageBind model. Our latent\naligner shares a similar core as the classifier guidance that guides the\ndiffusion denoising process during inference time. Through carefully designed\noptimization strategy and loss functions, we show the superior performance of\nour method on joint video-audio generation, visual-steered audio generation,\nand audio-steered visual generation tasks. The project website can be found at\nhttps://yzxing87.github.io/Seeing-and-Hearing/", "published": "2024-02-27 17:57:04", "link": "http://arxiv.org/abs/2402.17723v1", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
