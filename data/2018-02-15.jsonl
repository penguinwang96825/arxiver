{"title": "Deep contextualized word representations", "abstract": "We introduce a new type of deep contextualized word representation that\nmodels both (1) complex characteristics of word use (e.g., syntax and\nsemantics), and (2) how these uses vary across linguistic contexts (i.e., to\nmodel polysemy). Our word vectors are learned functions of the internal states\nof a deep bidirectional language model (biLM), which is pre-trained on a large\ntext corpus. We show that these representations can be easily added to existing\nmodels and significantly improve the state of the art across six challenging\nNLP problems, including question answering, textual entailment and sentiment\nanalysis. We also present an analysis showing that exposing the deep internals\nof the pre-trained network is crucial, allowing downstream models to mix\ndifferent types of semi-supervision signals.", "published": "2018-02-15 00:05:11", "link": "http://arxiv.org/abs/1802.05365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Neural Machine Translation for Extremely Low Resource\n  Languages", "abstract": "In this paper, we propose a new universal machine translation approach\nfocusing on languages with a limited amount of parallel data. Our proposed\napproach utilizes a transfer-learning approach to share lexical and sentence\nlevel representations across multiple source languages into one target\nlanguage. The lexical part is shared through a Universal Lexical Representation\nto support multilingual word-level sharing. The sentence-level sharing is\nrepresented by a model of experts from all source languages that share the\nsource encoders with all other languages. This enables the low-resource\nlanguage to utilize the lexical and sentence representations of the higher\nresource languages. Our approach is able to achieve 23 BLEU on Romanian-English\nWMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU\nof strong baseline system which uses multilingual training and\nback-translation. Furthermore, we show that the proposed approach can achieve\nalmost 20 BLEU on the same dataset through fine-tuning a pre-trained\nmulti-lingual system in a zero-shot setting.", "published": "2018-02-15 00:35:08", "link": "http://arxiv.org/abs/1802.05368v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Retrieval Modeling Using Cross Convolution Networks And Multi\n  Frequency Word Embedding", "abstract": "To build a satisfying chatbot that has the ability of managing a\ngoal-oriented multi-turn dialogue, accurate modeling of human conversation is\ncrucial. In this paper we concentrate on the task of response selection for\nmulti-turn human-computer conversation with a given context. Previous\napproaches show weakness in capturing information of rare keywords that appear\nin either or both context and correct response, and struggle with long input\nsequences. We propose Cross Convolution Network (CCN) and Multi Frequency word\nembedding to address both problems. We train several models using the Ubuntu\nDialogue dataset which is the largest freely available multi-turn based\ndialogue corpus. We further build an ensemble model by averaging predictions of\nmultiple models. We achieve a new state-of-the-art on this dataset with\nconsiderable improvements compared to previous best results.", "published": "2018-02-15 00:49:52", "link": "http://arxiv.org/abs/1802.05373v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Open Information Extraction on Scientific Text: An Evaluation", "abstract": "Open Information Extraction (OIE) is the task of the unsupervised creation of\nstructured information from text. OIE is often used as a starting point for a\nnumber of downstream tasks including knowledge base construction, relation\nextraction, and question answering. While OIE methods are targeted at being\ndomain independent, they have been evaluated primarily on newspaper,\nencyclopedic or general web text. In this article, we evaluate the performance\nof OIE on scientific texts originating from 10 different disciplines. To do so,\nwe use two state-of-the-art OIE systems applying a crowd-sourcing approach. We\nfind that OIE systems perform significantly worse on scientific text than\nencyclopedic text. We also provide an error analysis and suggest areas of work\nto reduce errors. Our corpus of sentences and judgments are made available.", "published": "2018-02-15 14:38:46", "link": "http://arxiv.org/abs/1802.05574v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language\n  Inference", "abstract": "We present a novel deep learning architecture to address the natural language\ninference (NLI) task. Existing approaches mostly rely on simple reading\nmechanisms for independent encoding of the premise and hypothesis. Instead, we\npropose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to\nefficiently model the relationship between a premise and a hypothesis during\nencoding and inference. We also introduce a sophisticated ensemble strategy to\ncombine our proposed models, which noticeably improves final predictions.\nFinally, we demonstrate how the results can be improved further with an\nadditional preprocessing step. Our evaluation shows that DR-BiLSTM obtains the\nbest single model and ensemble model results achieving the new state-of-the-art\nscores on the Stanford NLI dataset.", "published": "2018-02-15 14:39:47", "link": "http://arxiv.org/abs/1802.05577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tools and resources for Romanian text-to-speech and speech-to-text\n  applications", "abstract": "In this paper we introduce a set of resources and tools aimed at providing\nsupport for natural language processing, text-to-speech synthesis and speech\nrecognition for Romanian. While the tools are general purpose and can be used\nfor any language (we successfully trained our system for more than 50 languages\nand participated in the Universal Dependencies Shared Task), the resources are\nonly relevant for Romanian language processing.", "published": "2018-02-15 14:50:54", "link": "http://arxiv.org/abs/1802.05583v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calculating the similarity between words and sentences using a lexical\n  database and corpus statistics", "abstract": "Calculating the semantic similarity between sentences is a long dealt problem\nin the area of natural language processing. The semantic analysis field has a\ncrucial role to play in the research related to the text analytics. The\nsemantic similarity differs as the domain of operation differs. In this paper,\nwe present a methodology which deals with this issue by incorporating semantic\nsimilarity and corpus statistics. To calculate the semantic similarity between\nwords and sentences, the proposed method follows an edge-based approach using a\nlexical database. The methodology can be applied in a variety of domains. The\nmethodology has been tested on both benchmark standards and mean human\nsimilarity dataset. When tested on these two datasets, it gives highest\ncorrelation value for both word and sentence similarity outperforming other\nsimilar models. For word similarity, we obtained Pearson correlation\ncoefficient of 0.8753 and for sentence similarity, the correlation obtained is\n0.8794.", "published": "2018-02-15 17:15:25", "link": "http://arxiv.org/abs/1802.05667v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Nugget Detection with Forward-Backward Recurrent Neural Networks", "abstract": "Traditional event detection methods heavily rely on manually engineered rich\nfeatures. Recent deep learning approaches alleviate this problem by automatic\nfeature engineering. But such efforts, like tradition methods, have so far only\nfocused on single-token event mentions, whereas in practice events can also be\na phrase. We instead use forward-backward recurrent neural networks (FBRNNs) to\ndetect events that can be either words or phrases. To the best our knowledge,\nthis is one of the first efforts to handle multi-word events and also the first\nattempt to use RNNs for event detection. Experimental results demonstrate that\nFBRNN is competitive with the state-of-the-art methods on the ACE 2005 and the\nRich ERE 2015 event detection tasks.", "published": "2018-02-15 17:28:46", "link": "http://arxiv.org/abs/1802.05672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed\n  Social Media Texts", "abstract": "This paper reports about our work in the NLP Tool Contest @ICON-2017, shared\ntask on Sentiment Analysis for Indian Languages (SAIL) (code mixed). To\nimplement our system, we have used a machine learning algo-rithm called\nMultinomial Na\\\"ive Bayes trained using n-gram and SentiWordnet features. We\nhave also used a small SentiWordnet for English and a small SentiWordnet for\nBengali. But we have not used any SentiWordnet for Hindi language. We have\ntested our system on Hindi-English and Bengali-English code mixed social media\ndata sets released for the contest. The performance of our system is very close\nto the best system participated in the contest. For both Bengali-English and\nHindi-English runs, our system was ranked at the 3rd position out of all\nsubmitted runs and awarded the 3rd prize in the contest.", "published": "2018-02-15 20:02:43", "link": "http://arxiv.org/abs/1802.05737v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Cross-topic Argument Mining from Heterogeneous Sources Using\n  Attention-based Neural Networks", "abstract": "Argument mining is a core technology for automating argument search in large\ndocument collections. Despite its usefulness for this task, most current\napproaches to argument mining are designed for use only with specific text\ntypes and fall short when applied to heterogeneous texts. In this paper, we\npropose a new sentential annotation scheme that is reliably applicable by crowd\nworkers to arbitrary Web texts. We source annotations for over 25,000 instances\ncovering eight controversial topics. The results of cross-topic experiments\nshow that our attention-based neural network generalizes best to unseen topics\nand outperforms vanilla BiLSTM models by 6% in accuracy and 11% in F-score.", "published": "2018-02-15 20:49:19", "link": "http://arxiv.org/abs/1802.05758v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NtMalDetect: A Machine Learning Approach to Malware Detection Using\n  Native API System Calls", "abstract": "As computing systems become increasingly advanced and as users increasingly\nengage themselves in technology, security has never been a greater concern. In\nmalware detection, static analysis, the method of analyzing potentially\nmalicious files, has been the prominent approach. This approach, however,\nquickly falls short as malicious programs become more advanced and adopt the\ncapabilities of obfuscating its binaries to execute the same malicious\nfunctions, making static analysis extremely difficult for newer variants. The\napproach assessed in this paper is a novel dynamic malware analysis method,\nwhich may generalize better than static analysis to newer variants. Inspired by\nrecent successes in Natural Language Processing (NLP), widely used document\nclassification techniques were assessed in detecting malware by doing such\nanalysis on system calls, which contain useful information about the operation\nof a program as requests that the program makes of the kernel. Features\nconsidered are extracted from system call traces of benign and malicious\nprograms, and the task to classify these traces is treated as a binary document\nclassification task of system call traces. The system call traces were\nprocessed to remove the parameters to only leave the system call function\nnames. The features were grouped into various n-grams and weighted with Term\nFrequency-Inverse Document Frequency. This paper shows that Linear Support\nVector Machines (SVM) optimized by Stochastic Gradient Descent and the\ntraditional Coordinate Descent on the Wolfe Dual form of the SVM are effective\nin this approach, achieving a highest of 96% accuracy with 95% recall score.\nAdditional contributions include the identification of significant system call\nsequences that could be avenues for further research.", "published": "2018-02-15 05:34:21", "link": "http://arxiv.org/abs/1802.05412v2", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Learning to Count Objects in Natural Images for Visual Question\n  Answering", "abstract": "Visual Question Answering (VQA) models have struggled with counting objects\nin natural images so far. We identify a fundamental problem due to soft\nattention in these models as a cause. To circumvent this problem, we propose a\nneural network component that allows robust counting from object proposals.\nExperiments on a toy task show the effectiveness of this component and we\nobtain state-of-the-art accuracy on the number category of the VQA v2 dataset\nwithout negatively affecting other categories, even outperforming ensemble\nmodels with our single model. On a difficult balanced pair metric, the\ncomponent gives a substantial improvement in counting over a strong baseline by\n6.6%.", "published": "2018-02-15 21:16:59", "link": "http://arxiv.org/abs/1802.05766v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Teaching Machines to Code: Neural Markup Generation with Visual\n  Attention", "abstract": "We present a neural transducer model with visual attention that learns to\ngenerate LaTeX markup of a real-world math formula given its image. Applying\nsequence modeling and transduction techniques that have been very successful\nacross modalities such as natural language, image, handwriting, speech and\naudio; we construct an image-to-markup model that learns to produce\nsyntactically and semantically correct LaTeX markup code over 150 words long\nand achieves a BLEU score of 89%; improving upon the previous state-of-art for\nthe Im2Latex problem. We also demonstrate with heat-map visualization how\nattention helps in interpreting the model and can pinpoint (detect and\nlocalize) symbols on the image accurately despite having been trained without\nany bounding box data.", "published": "2018-02-15 06:17:51", "link": "http://arxiv.org/abs/1802.05415v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.NE"], "primary_category": "cs.LG"}
{"title": "CNN+LSTM Architecture for Speech Emotion Recognition with Data\n  Augmentation", "abstract": "In this work we design a neural network for recognizing emotions in speech,\nusing the IEMOCAP dataset. Following the latest advances in audio analysis, we\nuse an architecture involving both convolutional layers, for extracting\nhigh-level features from raw spectrograms, and recurrent ones for aggregating\nlong-term dependencies. We examine the techniques of data augmentation with\nvocal track length perturbation, layer-wise optimizer adjustment, batch\nnormalization of recurrent layers and obtain highly competitive results of\n64.5% for weighted accuracy and 61.7% for unweighted accuracy on four emotions.", "published": "2018-02-15 16:05:02", "link": "http://arxiv.org/abs/1802.05630v2", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multinomial Adversarial Networks for Multi-Domain Text Classification", "abstract": "Many text classification tasks are known to be highly domain-dependent.\nUnfortunately, the availability of training data can vary drastically across\ndomains. Worse still, for some domains there may not be any annotated data at\nall. In this work, we propose a multinomial adversarial network (MAN) to tackle\nthe text classification problem in this real-world multidomain setting (MDTC).\nWe provide theoretical justifications for the MAN framework, proving that\ndifferent instances of MANs are essentially minimizers of various f-divergence\nmetrics (Ali and Silvey, 1966) among multiple probability distributions. MANs\nare thus a theoretically sound generalization of traditional adversarial\nnetworks that discriminate over two distributions. More specifically, for the\nMDTC task, MAN learns features that are invariant across multiple domains by\nresorting to its ability to reduce the divergence among the feature\ndistributions of each domain. We present experimental results showing that MANs\nsignificantly outperform the prior art on the MDTC task. We also show that MANs\nachieve state-of-the-art performance for domains with no labeled data.", "published": "2018-02-15 18:22:58", "link": "http://arxiv.org/abs/1802.05694v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Explainable Prediction of Medical Codes from Clinical Text", "abstract": "Clinical notes are text documents that are created by clinicians for each\npatient encounter. They are typically accompanied by medical codes, which\ndescribe the diagnosis and treatment. Annotating these codes is labor intensive\nand error prone; furthermore, the connection between the codes and the text is\nnot annotated, obscuring the reasons and details behind specific diagnoses and\ntreatments. We present an attentional convolutional network that predicts\nmedical codes from clinical text. Our method aggregates information across the\ndocument using a convolutional neural network, and uses an attention mechanism\nto select the most relevant segments for each of the thousands of possible\ncodes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of\n0.54, which are both better than the prior state of the art. Furthermore,\nthrough an interpretability evaluation by a physician, we show that the\nattention mechanism identifies meaningful explanations for each code assignment", "published": "2018-02-15 18:25:32", "link": "http://arxiv.org/abs/1802.05695v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Multimodal Explanations: Justifying Decisions and Pointing to the\n  Evidence", "abstract": "Deep models that are both effective and explainable are desirable in many\nsettings; prior explainable models have been unimodal, offering either\nimage-based visualization of attention weights or text-based generation of\npost-hoc justifications. We propose a multimodal approach to explanation, and\nargue that the two modalities provide complementary explanatory strengths. We\ncollect two new datasets to define and evaluate this task, and propose a novel\nmodel which can provide joint textual rationale generation and attention\nvisualization. Our datasets define visual and textual justifications of a\nclassification decision for activity recognition tasks (ACT-X) and for visual\nquestion answering tasks (VQA-X). We quantitatively show that training with the\ntextual explanations not only yields better textual justification models, but\nalso better localizes the evidence that supports the decision. We also\nqualitatively show cases where visual explanation is more insightful than\ntextual explanation, and vice versa, supporting our thesis that multimodal\nexplanation models offer significant benefits over unimodal approaches.", "published": "2018-02-15 19:12:03", "link": "http://arxiv.org/abs/1802.08129v1", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Deep Learning Based Speech Beamforming", "abstract": "Multi-channel speech enhancement with ad-hoc sensors has been a challenging\ntask. Speech model guided beamforming algorithms are able to recover natural\nsounding speech, but the speech models tend to be oversimplified or the\ninference would otherwise be too complicated. On the other hand, deep learning\nbased enhancement approaches are able to learn complicated speech distributions\nand perform efficient inference, but they are unable to deal with variable\nnumber of input channels. Also, deep learning approaches introduce a lot of\nerrors, particularly in the presence of unseen noise types and settings. We\nhave therefore proposed an enhancement framework called DEEPBEAM, which\ncombines the two complementary classes of algorithms. DEEPBEAM introduces a\nbeamforming filter to produce natural sounding speech, but the filter\ncoefficients are determined with the help of a monaural speech enhancement\nneural network. Experiments on synthetic and real-world data show that DEEPBEAM\nis able to produce clean, dry and natural sounding speech, and is robust\nagainst unseen noise.", "published": "2018-02-15 02:00:54", "link": "http://arxiv.org/abs/1802.05383v1", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.CL"}
{"title": "Enhancement of Noisy Speech Exploiting an Exponential Model Based\n  Threshold and a Custom Thresholding Function in Perceptual Wavelet Packet\n  Domain", "abstract": "For enhancement of noisy speech, a method of threshold determination based on\nmodeling of Teager energy (TE) operated perceptual wavelet packet (PWP)\ncoefficients of the noisy speech by exponential distribution is presented. A\ncustom thresholding function based on the combination of mu-law and semisoft\nthresholding functions is designed and exploited to apply the statistically\nderived threshold upon the PWP coefficients. The effectiveness of the proposed\nmethod is evaluated for car and multi-talker babble noise corrupted speech\nsignals through performing extensive simulations using the NOIZEUS database.\nThe proposed method outperforms some of the state-of-the-art speech enhancement\nmethods both at high and low levels of SNRs in terms of the standard objective\nmeasures and the subjective evaluations including formal listening tests.", "published": "2018-02-15 02:35:37", "link": "http://arxiv.org/abs/1802.05962v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Blind Source Separation with Optimal Transport Non-negative Matrix\n  Factorization", "abstract": "Optimal transport as a loss for machine learning optimization problems has\nrecently gained a lot of attention. Building upon recent advances in\ncomputational optimal transport, we develop an optimal transport non-negative\nmatrix factorization (NMF) algorithm for supervised speech blind source\nseparation (BSS). Optimal transport allows us to design and leverage a cost\nbetween short-time Fourier transform (STFT) spectrogram frequencies, which\ntakes into account how humans perceive sound. We give empirical evidence that\nusing our proposed optimal transport NMF leads to perceptually better results\nthan Euclidean NMF, for both isolated voice reconstruction and BSS tasks.\nFinally, we demonstrate how to use optimal transport for cross domain sound\nprocessing tasks, where frequencies represented in the input spectrograms may\nbe different from one spectrogram to another.", "published": "2018-02-15 08:01:48", "link": "http://arxiv.org/abs/1802.05429v1", "categories": ["cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Deep Learning for Lip Reading using Audio-Visual Information for Urdu\n  Language", "abstract": "Human lip-reading is a challenging task. It requires not only knowledge of\nunderlying language but also visual clues to predict spoken words. Experts need\ncertain level of experience and understanding of visual expressions learning to\ndecode spoken words. Now-a-days, with the help of deep learning it is possible\nto translate lip sequences into meaningful words. The speech recognition in the\nnoisy environments can be increased with the visual information [1]. To\ndemonstrate this, in this project, we have tried to train two different\ndeep-learning models for lip-reading: first one for video sequences using\nspatiotemporal convolution neural network, Bi-gated recurrent neural network\nand Connectionist Temporal Classification Loss, and second for audio that\ninputs the MFCC features to a layer of LSTM cells and output the sequence. We\nhave also collected a small audio-visual dataset to train and test our model.\nOur target is to integrate our both models to improve the speech recognition in\nthe noisy environment", "published": "2018-02-15 13:28:19", "link": "http://arxiv.org/abs/1802.05521v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Masked Conditional Neural Networks for Automatic Sound Events\n  Recognition", "abstract": "Deep neural network architectures designed for application domains other than\nsound, especially image recognition, may not optimally harness the\ntime-frequency representation when adapted to the sound recognition problem. In\nthis work, we explore the ConditionaL Neural Network (CLNN) and the Masked\nConditionaL Neural Network (MCLNN) for multi-dimensional temporal signal\nrecognition. The CLNN considers the inter-frame relationship, and the MCLNN\nenforces a systematic sparseness over the network's links to enable learning in\nfrequency bands rather than bins allowing the network to be frequency shift\ninvariant mimicking a filterbank. The mask also allows considering several\ncombinations of features concurrently, which is usually handcrafted through\nexhaustive manual search. We applied the MCLNN to the environmental sound\nrecognition problem using the ESC-10 and ESC-50 datasets. MCLNN achieved\ncompetitive performance, using 12% of the parameters and without augmentation,\ncompared to state-of-the-art Convolutional Neural Networks.", "published": "2018-02-15 23:24:39", "link": "http://arxiv.org/abs/1802.05792v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
