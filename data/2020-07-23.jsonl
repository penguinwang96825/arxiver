{"title": "AI4D -- African Language Dataset Challenge", "abstract": "As language and speech technologies become more advanced, the lack of\nfundamental digital resources for African languages, such as data, spell\ncheckers and Part of Speech taggers, means that the digital divide between\nthese languages and others keeps growing. This work details the organisation of\nthe AI4D - African Language Dataset Challenge, an effort to incentivize the\ncreation, organization and discovery of African language datasets through a\ncompetitive challenge. We particularly encouraged the submission of annotated\ndatasets which can be used for training task-specific supervised machine\nlearning models.", "published": "2020-07-23 08:48:06", "link": "http://arxiv.org/abs/2007.11865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning based, end-to-end metaphor detection in Greek language\n  with Recurrent and Convolutional Neural Networks", "abstract": "This paper presents and benchmarks a number of end-to-end Deep Learning based\nmodels for metaphor detection in Greek. We combine Convolutional Neural\nNetworks and Recurrent Neural Networks with representation learning to bear on\nthe metaphor detection problem for the Greek language. The models presented\nachieve exceptional accuracy scores, significantly improving the previous state\nof the art results, which had already achieved accuracy 0.82. Furthermore, no\nspecial preprocessing, feature engineering or linguistic knowledge is used in\nthis work. The methods presented achieve accuracy of 0.92 and F-score 0.92 with\nConvolutional Neural Networks (CNNs) and bidirectional Long Short Term Memory\nnetworks (LSTMs). Comparable results of 0.91 accuracy and 0.91 F-score are also\nachieved with bidirectional Gated Recurrent Units (GRUs) and Convolutional\nRecurrent Neural Nets (CRNNs). The models are trained and evaluated only on the\nbasis of the training tuples, the sentences and their labels. The outcome is a\nstate of the art collection of metaphor detection models, trained on limited\nlabelled resources, which can be extended to other languages and similar tasks.", "published": "2020-07-23 12:02:40", "link": "http://arxiv.org/abs/2007.11949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NITS-Hinglish-SentiMix at SemEval-2020 Task 9: Sentiment Analysis For\n  Code-Mixed Social Media Text Using an Ensemble Model", "abstract": "Sentiment Analysis is the process of deciphering what a sentence emotes and\nclassifying them as either positive, negative, or neutral. In recent times,\nIndia has seen a huge influx in the number of active social media users and\nthis has led to a plethora of unstructured text data. Since the Indian\npopulation is generally fluent in both Hindi and English, they end up\ngenerating code-mixed Hinglish social media text i.e. the expressions of Hindi\nlanguage, written in the Roman script alongside other English words. The\nability to adequately comprehend the notions in these texts is truly necessary.\nOur team, rns2020 participated in Task 9 at SemEval2020 intending to design a\nsystem to carry out the sentiment analysis of code-mixed social media text.\nThis work proposes a system named NITS-Hinglish-SentiMix to viably complete the\nsentiment analysis of such code-mixed Hinglish text. The proposed framework has\nrecorded an F-Score of 0.617 on the test data.", "published": "2020-07-23 15:45:12", "link": "http://arxiv.org/abs/2007.12081v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embeddings: Stability and Semantic Change", "abstract": "Word embeddings are computed by a class of techniques within natural language\nprocessing (NLP), that create continuous vector representations of words in a\nlanguage from a large text corpus. The stochastic nature of the training\nprocess of most embedding techniques can lead to surprisingly strong\ninstability, i.e. subsequently applying the same technique to the same data\ntwice, can produce entirely different results. In this work, we present an\nexperimental study on the instability of the training process of three of the\nmost influential embedding techniques of the last decade: word2vec, GloVe and\nfastText. Based on the experimental results, we propose a statistical model to\ndescribe the instability of embedding techniques and introduce a novel metric\nto measure the instability of the representation of an individual word.\nFinally, we propose a method to minimize the instability - by computing a\nmodified average over multiple runs - and apply it to a specific linguistic\nproblem: The detection and quantification of semantic change, i.e. measuring\nchanges in the meaning and usage of words over time.", "published": "2020-07-23 16:03:50", "link": "http://arxiv.org/abs/2007.16006v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Clustering of Social Media Messages for Humanitarian Aid Response during\n  Crisis", "abstract": "Social media has quickly grown into an essential tool for people to\ncommunicate and express their needs during crisis events. Prior work in\nanalyzing social media data for crisis management has focused primarily on\nautomatically identifying actionable (or, informative) crisis-related messages.\nIn this work, we show that recent advances in Deep Learning and Natural\nLanguage Processing outperform prior approaches for the task of classifying\ninformativeness and encourage the field to adopt them for their research or\neven deployment. We also extend these methods to two sub-tasks of\ninformativeness and find that the Deep Learning methods are effective here as\nwell.", "published": "2020-07-23 02:18:05", "link": "http://arxiv.org/abs/2007.11756v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Product Title Generation for Conversational Systems using BERT", "abstract": "Through recent advancements in speech technology and introduction of smart\ndevices, such as Amazon Alexa and Google Home, increasing number of users are\ninteracting with applications through voice. E-commerce companies typically\ndisplay short product titles on their webpages, either human-curated or\nalgorithmically generated, when brevity is required, but these titles are\ndissimilar from natural spoken language. For example, \"Lucky Charms Gluten Free\nBreak-fast Cereal, 20.5 oz a box Lucky Charms Gluten Free\" is acceptable to\ndisplay on a webpage, but \"a 20.5 ounce box of lucky charms gluten free cereal\"\nis easier to comprehend over a conversational system. As compared to display\ndevices, where images and detailed product information can be presented to\nusers, short titles for products are necessary when interfacing with voice\nassistants. We propose a sequence-to-sequence approach using BERT to generate\nshort, natural, spoken language titles from input web titles. Our extensive\nexperiments on a real-world industry dataset and human evaluation of model\noutputs, demonstrate that BERT summarization outperforms comparable baseline\nmodels.", "published": "2020-07-23 03:15:19", "link": "http://arxiv.org/abs/2007.11768v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Applying GPGPU to Recurrent Neural Network Language Model based Fast\n  Network Search in the Real-Time LVCSR", "abstract": "Recurrent Neural Network Language Models (RNNLMs) have started to be used in\nvarious fields of speech recognition due to their outstanding performance.\nHowever, the high computational complexity of RNNLMs has been a hurdle in\napplying the RNNLM to a real-time Large Vocabulary Continuous Speech\nRecognition (LVCSR). In order to accelerate the speed of RNNLM-based network\nsearches during decoding, we apply the General Purpose Graphic Processing Units\n(GPGPUs). This paper proposes a novel method of applying GPGPUs to RNNLM-based\ngraph traversals. We have achieved our goal by reducing redundant computations\non CPUs and amount of transfer between GPGPUs and CPUs. The proposed approach\nwas evaluated on both WSJ corpus and in-house data. Experiments shows that the\nproposed approach achieves the real-time speed in various circumstances while\nmaintaining the Word Error Rate (WER) to be relatively 10% lower than that of\nn-gram models.", "published": "2020-07-23 05:15:14", "link": "http://arxiv.org/abs/2007.11794v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revealing semantic and emotional structure of suicide notes with\n  cognitive network science", "abstract": "Understanding the cognitive and emotional perceptions of people who commit\nsuicide is one of the most sensitive scientific challenges. There are\ncircumstances where people feel the need to leave something written, an\nartifact where they express themselves, registering their last words and\nfeelings. These suicide notes are of utmost importance for better understanding\nthe psychology of suicidal ideation. This work gives structure to the\nlinguistic content of suicide notes, revealing interconnections between\ncognitive and emotional states of people who committed suicide. We build upon\ncognitive network science, psycholinguistics and semantic frame theory to\nintroduce a network representation of the mindset expressed in suicide notes.\nOur cognitive network representation enables the quantitative analysis of the\nlanguage in suicide notes through structural balance theory, semantic\nprominence and emotional profiling. Our results indicate that the emotional\nsyntax connecting positively- and negatively-valenced terms gives rise to a\ndegree of structural balance that is significantly higher than null models\nwhere the affective structure was randomized. We show that suicide notes are\naffectively compartmentalized such that positive concepts tend to cluster\ntogether and dominate the overall network structure. A key positive concept is\n\"love\", which integrates information relating the self to others in ways that\nare semantically prominent across suicide notes. The emotions populating the\nsemantic frame of \"love\" combine joy and trust with anticipation and sadness,\nwhich connects with psychological theories about meaning-making and narrative\npsychology. Our results open new ways for understanding the structure of\ngenuine suicide notes informing future research for suicide prevention.", "published": "2020-07-23 15:11:32", "link": "http://arxiv.org/abs/2007.12053v3", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Exploring Swedish & English fastText Embeddings for NER with the\n  Transformer", "abstract": "In this paper, our main contributions are that embeddings from relatively\nsmaller corpora can outperform ones from larger corpora and we make the new\nSwedish analogy test set publicly available. To achieve a good network\nperformance in natural language processing (NLP) downstream tasks, several\nfactors play important roles: dataset size, the right hyper-parameters, and\nwell-trained embeddings. We show that, with the right set of hyper-parameters,\ngood network performance can be reached even on smaller datasets. We evaluate\nthe embeddings at both the intrinsic and extrinsic levels. The embeddings are\ndeployed with the Transformer in named entity recognition (NER) task and\nsignificance tests conducted. This is done for both Swedish and English. We\nobtain better performance in both languages on the downstream task with smaller\ntraining data, compared to recently released, Common Crawl versions; and\ncharacter n-grams appear useful for Swedish, a morphologically rich language.", "published": "2020-07-23 08:51:09", "link": "http://arxiv.org/abs/2007.16007v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SBAT: Video Captioning with Sparse Boundary-Aware Transformer", "abstract": "In this paper, we focus on the problem of applying the transformer structure\nto video captioning effectively. The vanilla transformer is proposed for\nuni-modal language generation task such as machine translation. However, video\ncaptioning is a multimodal learning problem, and the video features have much\nredundancy between different time steps. Based on these concerns, we propose a\nnovel method called sparse boundary-aware transformer (SBAT) to reduce the\nredundancy in video representation. SBAT employs boundary-aware pooling\noperation for scores from multihead attention and selects diverse features from\ndifferent scenarios. Also, SBAT includes a local correlation scheme to\ncompensate for the local information loss brought by sparse operation. Based on\nSBAT, we further propose an aligned cross-modal encoding scheme to boost the\nmultimodal interaction. Experimental results on two benchmark datasets show\nthat SBAT outperforms the state-of-the-art methods under most of the metrics.", "published": "2020-07-23 09:57:25", "link": "http://arxiv.org/abs/2007.11888v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "HCMS at SemEval-2020 Task 9: A Neural Approach to Sentiment Analysis for\n  Code-Mixed Texts", "abstract": "Problems involving code-mixed language are often plagued by a lack of\nresources and an absence of materials to perform sophisticated transfer\nlearning with. In this paper we describe our submission to the Sentimix\nHindi-English task involving sentiment classification of code-mixed texts, and\nwith an F1 score of 67.1%, we demonstrate that simple convolution and attention\nmay well produce reasonable results.", "published": "2020-07-23 15:39:53", "link": "http://arxiv.org/abs/2007.12076v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.NE", "I.2.6; I.2.7; I.7.1"], "primary_category": "cs.CL"}
{"title": "Health, Psychosocial, and Social issues emanating from COVID-19 pandemic\n  based on Social Media Comments using Natural Language Processing", "abstract": "The COVID-19 pandemic has caused a global health crisis that affects many\naspects of human lives. In the absence of vaccines and antivirals, several\nbehavioural change and policy initiatives, such as physical distancing, have\nbeen implemented to control the spread of the coronavirus. Social media data\ncan reveal public perceptions toward how governments and health agencies across\nthe globe are handling the pandemic, as well as the impact of the disease on\npeople regardless of their geographic locations in line with various factors\nthat hinder or facilitate the efforts to control the spread of the pandemic\nglobally. This paper aims to investigate the impact of the COVID-19 pandemic on\npeople globally using social media data. We apply natural language processing\n(NLP) and thematic analysis to understand public opinions, experiences, and\nissues with respect to the COVID-19 pandemic using social media data. First, we\ncollect over 47 million COVID-19-related comments from Twitter, Facebook,\nYouTube, and three online discussion forums. Second, we perform data\npreprocessing which involves applying NLP techniques to clean and prepare the\ndata for automated theme extraction. Third, we apply context-aware NLP approach\nto extract meaningful keyphrases or themes from over 1 million randomly\nselected comments, as well as compute sentiment scores for each theme and\nassign sentiment polarity based on the scores using lexicon-based technique.\nFourth, we categorize related themes into broader themes. A total of 34\nnegative themes emerged, out of which 15 are health-related issues,\npsychosocial issues, and social issues related to the COVID-19 pandemic from\nthe public perspective. In addition, 20 positive themes emerged from our\nresults. Finally, we recommend interventions that can help address the negative\nissues based on the positive themes and other remedial ideas rooted in\nresearch.", "published": "2020-07-23 17:19:50", "link": "http://arxiv.org/abs/2007.12144v1", "categories": ["cs.CL", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot\n  Retrieval of Images from Textual Descriptions", "abstract": "Most existing algorithms for cross-modal Information Retrieval are based on a\nsupervised train-test setup, where a model learns to align the mode of the\nquery (e.g., text) to the mode of the documents (e.g., images) from a given\ntraining set. Such a setup assumes that the training set contains an exhaustive\nrepresentation of all possible classes of queries. In reality, a retrieval\nmodel may need to be deployed on previously unseen classes, which implies a\nzero-shot IR setup. In this paper, we propose a novel GAN-based model for\nzero-shot text to image retrieval. When given a textual description as the\nquery, our model can retrieve relevant images in a zero-shot setup. The\nproposed model is trained using an Expectation-Maximization framework.\nExperiments on multiple benchmark datasets show that our proposed model\ncomfortably outperforms several state-of-the-art zero-shot text to image\nretrieval models, as well as zero-shot classification and hashing models\nsuitably used for retrieval.", "published": "2020-07-23 18:50:03", "link": "http://arxiv.org/abs/2007.12212v3", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks", "abstract": "In natural language processing (NLP), enormous pre-trained models like BERT\nhave become the standard starting point for training on a range of downstream\ntasks, and similar trends are emerging in other areas of deep learning. In\nparallel, work on the lottery ticket hypothesis has shown that models for NLP\nand computer vision contain smaller matching subnetworks capable of training in\nisolation to full accuracy and transferring to other tasks. In this work, we\ncombine these observations to assess whether such trainable, transferrable\nsubnetworks exist in pre-trained BERT models. For a range of downstream tasks,\nwe indeed find matching subnetworks at 40% to 90% sparsity. We find these\nsubnetworks at (pre-trained) initialization, a deviation from prior NLP\nresearch where they emerge only after some amount of training. Subnetworks\nfound on the masked language modeling task (the same task used to pre-train the\nmodel) transfer universally; those found on other tasks transfer in a limited\nfashion if at all. As large-scale pre-training becomes an increasingly central\nparadigm in deep learning, our results demonstrate that the main lottery ticket\nobservations remain relevant in this context. Codes available at\nhttps://github.com/VITA-Group/BERT-Tickets.", "published": "2020-07-23 19:35:39", "link": "http://arxiv.org/abs/2007.12223v2", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Sound Field Translation and Mixed Source Model for Virtual Applications\n  with Perceptual Validation", "abstract": "Non-interactive and linear experiences like cinema film offer high quality\nsurround sound audio to enhance immersion, however the listener's experience is\nusually fixed to a single acoustic perspective. With the rise of virtual\nreality, there is a demand for recording and recreating real-world experiences\nin a way that allows for the user to interact and move within the reproduction.\nConventional sound field translation techniques take a recording and expand it\ninto an equivalent environment of virtual sources. However, the finite sampling\nof a commercial higher order microphone produces an acoustic sweet-spot in the\nvirtual reproduction. As a result, the technique remains to restrict the\nlistener's navigable region. In this paper, we propose a method for listener\ntranslation in an acoustic reproduction that incorporates a mixture of\nnear-field and far-field sources in a sparsely expanded virtual environment. We\nperceptually validate the method through a Multiple Stimulus with Hidden\nReference and Anchor (MUSHRA) experiment. Compared to the planewave benchmark,\nthe proposed method offers both improved source localizability and robustness\nto spectral distortions at translated positions. A cross-examination with\nnumerical simulations demonstrated that the sparse expansion relaxes the\ninherent sweet-spot constraint, leading to the improved localizability for\nsparse environments. Additionally, the proposed method is seen to better\nreproduce the intensity and binaural room impulse response spectra of\nnear-field environments, further supporting the strong perceptual results.", "published": "2020-07-23 05:16:01", "link": "http://arxiv.org/abs/2007.11795v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Sequential Routing Framework: Fully Capsule Network-based Speech\n  Recognition", "abstract": "Capsule networks (CapsNets) have recently gotten attention as a novel neural\narchitecture. This paper presents the sequential routing framework which we\nbelieve is the first method to adapt a CapsNet-only structure to\nsequence-to-sequence recognition. Input sequences are capsulized then sliced by\na window size. Each slice is classified to a label at the corresponding time\nthrough iterative routing mechanisms. Afterwards, losses are computed by\nconnectionist temporal classification (CTC). During routing, the required\nnumber of parameters can be controlled by the window size regardless of the\nlength of sequences by sharing learnable weights across the slices. We\nadditionally propose a sequential dynamic routing algorithm to replace\ntraditional dynamic routing. The proposed technique can minimize decoding speed\ndegradation caused by the routing iterations since it can operate in a\nnon-iterative manner without dropping accuracy. The method achieves a 1.1%\nlower word error rate at 16.9% on the Wall Street Journal corpus compared to\nbidirectional long short-term memory-based CTC networks. On the TIMIT corpus,\nit attains a 0.7% lower phone error rate at 17.5% compared to convolutional\nneural network-based CTC networks (Zhang et al., 2016).", "published": "2020-07-23 01:51:41", "link": "http://arxiv.org/abs/2007.11747v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Version Control of Speaker Recognition Systems", "abstract": "This paper discusses one of the most challenging practical engineering\nproblems in speaker recognition systems - the version control of models and\nuser profiles. A typical speaker recognition system consists of two stages: the\nenrollment stage, where a profile is generated from user-provided enrollment\naudio; and the runtime stage, where the voice identity of the runtime audio is\ncompared against the stored profiles. As technology advances, the speaker\nrecognition system needs to be updated for better performance. However, if the\nstored user profiles are not updated accordingly, version mismatch will result\nin meaningless recognition results. In this paper, we describe different\nversion control strategies for speaker recognition systems that had been\ncarefully studied at Google from years of engineering practice. These\nstrategies are categorized into three groups according to how they are deployed\nin the production environment: device-side deployment, server-side deployment,\nand hybrid deployment. To compare different strategies with quantitative\nmetrics under various network configurations, we present SpeakerVerSim, an\neasily-extensible Python-based simulation framework for different server-side\ndeployment strategies of speaker recognition systems.", "published": "2020-07-23 15:28:58", "link": "http://arxiv.org/abs/2007.12069v8", "categories": ["eess.AS", "cs.DC", "cs.NI", "cs.SE"], "primary_category": "eess.AS"}
{"title": "Augmentation adversarial training for self-supervised speaker\n  recognition", "abstract": "The goal of this work is to train robust speaker recognition models without\nspeaker labels. Recent works on unsupervised speaker representations are based\non contrastive learning in which they encourage within-utterance embeddings to\nbe similar and across-utterance embeddings to be dissimilar. However, since the\nwithin-utterance segments share the same acoustic characteristics, it is\ndifficult to separate the speaker information from the channel information. To\nthis end, we propose augmentation adversarial training strategy that trains the\nnetwork to be discriminative for the speaker information, while invariant to\nthe augmentation applied. Since the augmentation simulates the acoustic\ncharacteristics, training the network to be invariant to augmentation also\nencourages the network to be invariant to the channel information in general.\nExtensive experiments on the VoxCeleb and VOiCES datasets show significant\nimprovements over previous works using self-supervision, and the performance of\nour self-supervised models far exceed that of humans.", "published": "2020-07-23 15:49:52", "link": "http://arxiv.org/abs/2007.12085v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound2Sight: Generating Visual Dynamics from Sound and Context", "abstract": "Learning associations across modalities is critical for robust multimodal\nreasoning, especially when a modality may be missing during inference. In this\npaper, we study this problem in the context of audio-conditioned visual\nsynthesis -- a task that is important, for example, in occlusion reasoning.\nSpecifically, our goal is to generate future video frames and their motion\ndynamics conditioned on audio and a few past frames. To tackle this problem, we\npresent Sound2Sight, a deep variational framework, that is trained to learn a\nper frame stochastic prior conditioned on a joint embedding of audio and past\nframes. This embedding is learned via a multi-head attention-based audio-visual\ntransformer encoder. The learned prior is then sampled to further condition a\nvideo forecasting module to generate future frames. The stochastic prior allows\nthe model to sample multiple plausible futures that are consistent with the\nprovided audio and the past context. Moreover, to improve the quality and\ncoherence of the generated frames, we propose a multimodal discriminator that\ndifferentiates between a synthesized and a real audio-visual clip. We\nempirically evaluate our approach, vis-\\'a-vis closely-related prior methods,\non two new datasets viz. (i) Multimodal Stochastic Moving MNIST with a Surprise\nObstacle, (ii) Youtube Paintings; as well as on the existing Audio-Set Drums\ndataset. Our extensive experiments demonstrate that Sound2Sight significantly\noutperforms the state of the art in the generated video quality, while also\nproducing diverse video content.", "published": "2020-07-23 16:57:44", "link": "http://arxiv.org/abs/2007.12130v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
