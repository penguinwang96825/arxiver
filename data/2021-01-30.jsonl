{"title": "Can We Automate Scientific Reviewing?", "abstract": "The rapid development of science and technology has been accompanied by an\nexponential growth in peer-reviewed scientific publications. At the same time,\nthe review of each paper is a laborious process that must be carried out by\nsubject matter experts. Thus, providing high-quality reviews of this growing\nnumber of papers is a significant challenge. In this work, we ask the question\n\"can we automate scientific reviewing?\", discussing the possibility of using\nstate-of-the-art natural language processing (NLP) models to generate\nfirst-pass peer reviews for scientific papers. Arguably the most difficult part\nof this is defining what a \"good\" review is in the first place, so we first\ndiscuss possible evaluation measures for such reviews. We then collect a\ndataset of papers in the machine learning domain, annotate them with different\naspects of content covered in each review, and train targeted summarization\nmodels that take in papers to generate reviews. Comprehensive experimental\nresults show that system-generated reviews tend to touch upon more aspects of\nthe paper than human-written reviews, but the generated text can suffer from\nlower constructiveness for all aspects except the explanation of the core ideas\nof the papers, which are largely factually correct. We finally summarize eight\nchallenges in the pursuit of a good review generation system together with\npotential solutions, which, hopefully, will inspire more future research on\nthis subject. We make all code, and the dataset publicly available:\nhttps://github.com/neulab/ReviewAdvisor, as well as a ReviewAdvisor system:\nhttp://review.nlpedia.ai/.", "published": "2021-01-30 07:16:53", "link": "http://arxiv.org/abs/2102.00176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning From How Humans Correct", "abstract": "In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we need to relabel\nthe noisy data in our dataset for our industry application. The experiment\nresult shows that our learn-on-correction method improve the classification\naccuracy from 91.7% to 92.5% in test dataset. The 91.7% accuracy is trained on\nthe corrected dataset, which improve the baseline from 83.3% to 91.7% in test\ndataset. The accuracy under human evaluation achieves more than 97%.", "published": "2021-01-30 13:13:50", "link": "http://arxiv.org/abs/2102.00225v20", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "If you've got it, flaunt it: Making the most of fine-grained sentiment\n  annotations", "abstract": "Fine-grained sentiment analysis attempts to extract sentiment holders,\ntargets and polar expressions and resolve the relationship between them, but\nprogress has been hampered by the difficulty of annotation. Targeted sentiment\nanalysis, on the other hand, is a more narrow task, focusing on extracting\nsentiment targets and classifying their polarity.In this paper, we explore\nwhether incorporating holder and expression information can improve target\nextraction and classification and perform experiments on eight English\ndatasets. We conclude that jointly predicting target and polarity BIO labels\nimproves target extraction, and that augmenting the input text with gold\nexpressions generally improves targeted polarity classification. This\nhighlights the potential importance of annotating expressions for fine-grained\nsentiment datasets. At the same time, our results show that performance of\ncurrent models for predicting polar expressions is poor, hampering the benefit\nof this information in practice.", "published": "2021-01-30 19:47:58", "link": "http://arxiv.org/abs/2102.00299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taxonomic survey of Hindi Language NLP systems", "abstract": "Natural Language processing (NLP) represents the task of automatic handling\nof natural human language by machines.There is large spectrum of possible\napplications of NLP which help in automating tasks like translating text from\none language to other, retrieving and summarizing data from very huge\nrepositories, spam email filtering, identifying fake news in digital media,\nfind sentiment and feedback of people, find political opinions and views of\npeople on various government policies, provide effective medical assistance\nbased on past history records of patient etc. Hindi is the official language of\nIndia with nearly 691 million users in India and 366 million in rest of world.\nAt present, a number of government and private sector projects and researchers\nin India and abroad, are working towards developing NLP applications and\nresources for Indian languages. This survey gives a report of the resources and\napplications available for Hindi language NLP.", "published": "2021-01-30 11:53:56", "link": "http://arxiv.org/abs/2102.00214v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ShufText: A Simple Black Box Approach to Evaluate the Fragility of Text\n  Classification Models", "abstract": "Text classification is the most basic natural language processing task. It\nhas a wide range of applications ranging from sentiment analysis to topic\nclassification. Recently, deep learning approaches based on CNN, LSTM, and\nTransformers have been the de facto approach for text classification. In this\nwork, we highlight a common issue associated with these approaches. We show\nthat these systems are over-reliant on the important words present in the text\nthat are useful for classification. With limited training data and\ndiscriminative training strategy, these approaches tend to ignore the semantic\nmeaning of the sentence and rather just focus on keywords or important n-grams.\nWe propose a simple black box technique ShutText to present the shortcomings of\nthe model and identify the over-reliance of the model on keywords. This\ninvolves randomly shuffling the words in a sentence and evaluating the\nclassification accuracy. We see that on common text classification datasets\nthere is very little effect of shuffling and with high probability these models\npredict the original class. We also evaluate the effect of language model\npretraining on these models and try to answer questions around model robustness\nto out of domain sentences. We show that simple models based on CNN or LSTM as\nwell as complex models like BERT are questionable in terms of their syntactic\nand semantic understanding.", "published": "2021-01-30 15:18:35", "link": "http://arxiv.org/abs/2102.00238v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Triple M: A Practical Text-to-speech Synthesis System With\n  Multi-guidance Attention And Multi-band Multi-time LPCNet", "abstract": "In this work, a robust and efficient text-to-speech (TTS) synthesis system\nnamed Triple M is proposed for large-scale online application. The key\ncomponents of Triple M are: 1) A sequence-to-sequence model adopts a novel\nmulti-guidance attention to transfer complementary advantages from guiding\nattention mechanisms to the basic attention mechanism without in-domain\nperformance loss and online service modification. Compared with single\nattention mechanism, multi-guidance attention not only brings better\nnaturalness to long sentence synthesis, but also reduces the word error rate by\n26.8%. 2) A new efficient multi-band multi-time vocoder framework, which\nreduces the computational complexity from 2.8 to 1.0 GFLOP and speeds up LPCNet\nby 2.75x on a single CPU.", "published": "2021-01-30 15:38:36", "link": "http://arxiv.org/abs/2102.00247v4", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EmpathBERT: A BERT-based Framework for Demographic-aware Empathy\n  Prediction", "abstract": "Affect preferences vary with user demographics, and tapping into demographic\ninformation provides important cues about the users' language preferences. In\nthis paper, we utilize the user demographics, and propose EmpathBERT, a\ndemographic-aware framework for empathy prediction based on BERT. Through\nseveral comparative experiments, we show that EmpathBERT surpasses traditional\nmachine learning and deep learning models, and illustrate the importance of\nuser demographics to predict empathy and distress in user responses to\nstimulative news articles. We also highlight the importance of affect\ninformation in the responses by developing affect-aware models to predict user\ndemographic attributes.", "published": "2021-01-30 16:57:40", "link": "http://arxiv.org/abs/2102.00272v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Fake it Till You Make it: Self-Supervised Semantic Shifts for\n  Monolingual Word Embedding Tasks", "abstract": "The use of language is subject to variation over time as well as across\nsocial groups and knowledge domains, leading to differences even in the\nmonolingual scenario. Such variation in word usage is often called lexical\nsemantic change (LSC). The goal of LSC is to characterize and quantify language\nvariations with respect to word meaning, to measure how distinct two language\nsources are (that is, people or language models). Because there is hardly any\ndata available for such a task, most solutions involve unsupervised methods to\nalign two embeddings and predict semantic change with respect to a distance\nmeasure. To that end, we propose a self-supervised approach to model lexical\nsemantic change by generating training samples by introducing perturbations of\nword vectors in the input corpora. We show that our method can be used for the\ndetection of semantic change with any alignment method. Furthermore, it can be\nused to choose the landmark words to use in alignment and can lead to\nsubstantial improvements over the existing techniques for alignment.\n  We illustrate the utility of our techniques using experimental results on\nthree different datasets, involving words with the same or different meanings.\nOur methods not only provide significant improvements but also can lead to\nnovel findings for the LSC problem.", "published": "2021-01-30 18:59:43", "link": "http://arxiv.org/abs/2102.00290v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Machine Translationese: Effects of Algorithmic Bias on Linguistic\n  Complexity in Machine Translation", "abstract": "Recent studies in the field of Machine Translation (MT) and Natural Language\nProcessing (NLP) have shown that existing models amplify biases observed in the\ntraining data. The amplification of biases in language technology has mainly\nbeen examined with respect to specific phenomena, such as gender bias. In this\nwork, we go beyond the study of gender in MT and investigate how bias\namplification might affect language in a broader sense. We hypothesize that the\n'algorithmic bias', i.e. an exacerbation of frequently observed patterns in\ncombination with a loss of less frequent ones, not only exacerbates societal\nbiases present in current datasets but could also lead to an artificially\nimpoverished language: 'machine translationese'. We assess the linguistic\nrichness (on a lexical and morphological level) of translations created by\ndifferent data-driven MT paradigms - phrase-based statistical (PB-SMT) and\nneural MT (NMT). Our experiments show that there is a loss of lexical and\nmorphological richness in the translations produced by all investigated MT\nparadigms for two language pairs (EN<=>FR and EN<=>ES).", "published": "2021-01-30 18:49:11", "link": "http://arxiv.org/abs/2102.00287v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Speech Recognition by Simply Fine-tuning BERT", "abstract": "We propose a simple method for automatic speech recognition (ASR) by\nfine-tuning BERT, which is a language model (LM) trained on large-scale\nunlabeled text data and can generate rich contextual representations. Our\nassumption is that given a history context sequence, a powerful LM can narrow\nthe range of possible choices and the speech signal can be used as a simple\nclue. Hence, comparing to conventional ASR systems that train a powerful\nacoustic model (AM) from scratch, we believe that speech recognition is\npossible by simply fine-tuning a BERT model. As an initial study, we\ndemonstrate the effectiveness of the proposed idea on the AISHELL dataset and\nshow that stacking a very simple AM on top of BERT can yield reasonable\nperformance.", "published": "2021-01-30 19:06:14", "link": "http://arxiv.org/abs/2102.00291v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "LSSED: a large-scale dataset and benchmark for speech emotion\n  recognition", "abstract": "Speech emotion recognition is a vital contributor to the next generation of\nhuman-computer interaction (HCI). However, current existing small-scale\ndatabases have limited the development of related research. In this paper, we\npresent LSSED, a challenging large-scale english speech emotion dataset, which\nhas data collected from 820 subjects to simulate real-world distribution. In\naddition, we release some pre-trained models based on LSSED, which can not only\npromote the development of speech emotion recognition, but can also be\ntransferred to related downstream tasks such as mental health analysis where\ndata is extremely difficult to collect. Finally, our experiments show the\nnecessity of large-scale datasets and the effectiveness of pre-trained models.\nThe dateset will be released on https://github.com/tobefans/LSSED.", "published": "2021-01-30 11:15:32", "link": "http://arxiv.org/abs/2102.01754v1", "categories": ["cs.SD", "cs.AI", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Enhancing the Intelligibility of Cleft Lip and Palate Speech using\n  Cycle-consistent Adversarial Networks", "abstract": "Cleft lip and palate (CLP) refer to a congenital craniofacial condition that\ncauses various speech-related disorders. As a result of structural and\nfunctional deformities, the affected subjects' speech intelligibility is\nsignificantly degraded, limiting the accessibility and usability of\nspeech-controlled devices. Towards addressing this problem, it is desirable to\nimprove the CLP speech intelligibility. Moreover, it would be useful during\nspeech therapy. In this study, the cycle-consistent adversarial network\n(CycleGAN) method is exploited for improving CLP speech intelligibility. The\nmodel is trained on native Kannada-speaking childrens' speech data. The\neffectiveness of the proposed approach is also measured using automatic speech\nrecognition performance. Further, subjective evaluation is performed, and those\nresults also confirm the intelligibility improvement in the enhanced speech\nover the original.", "published": "2021-01-30 16:49:44", "link": "http://arxiv.org/abs/2102.00270v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "End-to-End Language Identification using Multi-Head Self-Attention and\n  1D Convolutional Neural Networks", "abstract": "In this work, we propose a new approach for language identification using\nmulti-head self-attention combined with raw waveform based 1D convolutional\nneural networks for Indian languages. Our approach uses an encoder, multi-head\nselfattention, and a statistics pooling layer. The encoder learns features\ndirectly from raw waveforms using 1D convolution kernels and an LSTM layer. The\nLSTM layer captures temporal information between the features extracted by the\n1D convolutional layer. The multi-head self-attention layer takes outputs of\nthe LSTM layer and applies self-attention mechanisms on these features with M\ndifferent heads. This process helps the model give more weightage to the more\nuseful features and less weightage to the less relevant features. Finally, the\nframe-level features are combined using a statistics pooling layer to extract\nthe utterance-level feature vector label prediction. We conduct all our\nexperiments on the 373 hrs of audio data for eight different Indian languages.\nOur experiments show that our approach outperforms the baseline model by an\nabsolute 3.69% improvement in F1-score and achieves the best F1-score of\n95.90%. Our approach also shows that using raw waveform models gets a 1.7%\nimprovement in performance compared to the models built using handcrafted\nfeatures.", "published": "2021-01-30 20:32:51", "link": "http://arxiv.org/abs/2102.00306v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Semi-supervised Sound Event Detection using Random Augmentation and\n  Consistency Regularization", "abstract": "Sound event detection is a core module for acoustic environmental analysis.\nSemi-supervised learning technique allows to largely scale up the dataset\nwithout increasing the annotation budget, and recently attracts lots of\nresearch attention. In this work, we study on two advanced semi-supervised\nlearning techniques for sound event detection. Data augmentation is important\nfor the success of recent deep learning systems. This work studies the\naudio-signal random augmentation method, which provides an augmentation\nstrategy that can handle a large number of different audio transformations. In\naddition, consistency regularization is widely adopted in recent\nstate-of-the-art semi-supervised learning methods, which exploits the\nunlabelled data by constraining the prediction of different transformations of\none sample to be identical to the prediction of this sample. This work finds\nthat, for semi-supervised sound event detection, consistency regularization is\nan effective strategy, especially the best performance is achieved when it is\ncombined with the MeanTeacher model.", "published": "2021-01-30 05:22:13", "link": "http://arxiv.org/abs/2102.00154v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Expressive Neural Voice Cloning", "abstract": "Voice cloning is the task of learning to synthesize the voice of an unseen\nspeaker from a few samples. While current voice cloning methods achieve\npromising results in Text-to-Speech (TTS) synthesis for a new voice, these\napproaches lack the ability to control the expressiveness of synthesized audio.\nIn this work, we propose a controllable voice cloning method that allows\nfine-grained control over various style aspects of the synthesized speech for\nan unseen speaker. We achieve this by explicitly conditioning the speech\nsynthesis model on a speaker encoding, pitch contour and latent style tokens\nduring training. Through both quantitative and qualitative evaluations, we show\nthat our framework can be used for various expressive voice cloning tasks using\nonly a few transcribed or untranscribed speech samples for a new speaker. These\ncloning tasks include style transfer from a reference speech, synthesizing\nspeech directly from text, and fine-grained style control by manipulating the\nstyle conditioning variables during inference.", "published": "2021-01-30 05:09:57", "link": "http://arxiv.org/abs/2102.00151v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adversarially learning disentangled speech representations for robust\n  multi-factor voice conversion", "abstract": "Factorizing speech as disentangled speech representations is vital to achieve\nhighly controllable style transfer in voice conversion (VC). Conventional\nspeech representation learning methods in VC only factorize speech as speaker\nand content, lacking controllability on other prosody-related factors.\nState-of-the-art speech representation learning methods for more speechfactors\nare using primary disentangle algorithms such as random resampling and ad-hoc\nbottleneck layer size adjustment,which however is hard to ensure robust speech\nrepresentationdisentanglement. To increase the robustness of highly\ncontrollable style transfer on multiple factors in VC, we propose a\ndisentangled speech representation learning framework based on adversarial\nlearning. Four speech representations characterizing content, timbre, rhythm\nand pitch are extracted, and further disentangled by an adversarial\nMask-And-Predict (MAP)network inspired by BERT. The adversarial network is used\ntominimize the correlations between the speech representations,by randomly\nmasking and predicting one of the representationsfrom the others. Experimental\nresults show that the proposedframework significantly improves the robustness\nof VC on multiple factors by increasing the speech quality MOS from 2.79 to3.30\nand decreasing the MCD from 3.89 to 3.58.", "published": "2021-01-30 08:29:55", "link": "http://arxiv.org/abs/2102.00184v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Directional Sparse Filtering using Weighted Lehmer Mean for Blind\n  Separation of Unbalanced Speech Mixtures", "abstract": "In blind source separation of speech signals, the inherent imbalance in the\nsource spectrum poses a challenge for methods that rely on single-source\ndominance for the estimation of the mixing matrix. We propose an algorithm\nbased on the directional sparse filtering (DSF) framework that utilizes the\nLehmer mean with learnable weights to adaptively account for source imbalance.\nPerformance evaluation in multiple real acoustic environments show improvements\nin source separation compared to the baseline methods.", "published": "2021-01-30 09:36:36", "link": "http://arxiv.org/abs/2102.00196v3", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Cortical Features for Defense Against Adversarial Audio Attacks", "abstract": "We propose using a computational model of the auditory cortex as a defense\nagainst adversarial attacks on audio. We apply several white-box iterative\noptimization-based adversarial attacks to an implementation of Amazon Alexa's\nHW network, and a modified version of this network with an integrated cortical\nrepresentation, and show that the cortical features help defend against\nuniversal adversarial examples. At the same level of distortion, the\nadversarial noises found for the cortical network are always less effective for\nuniversal audio attacks. We make our code publicly available at\nhttps://github.com/ilyakava/py3fst.", "published": "2021-01-30 21:21:46", "link": "http://arxiv.org/abs/2102.00313v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Melon Playlist Dataset: a public dataset for audio-based playlist\n  generation and music tagging", "abstract": "One of the main limitations in the field of audio signal processing is the\nlack of large public datasets with audio representations and high-quality\nannotations due to restrictions of copyrighted commercial music. We present\nMelon Playlist Dataset, a public dataset of mel-spectrograms for 649,091tracks\nand 148,826 associated playlists annotated by 30,652 different tags. All the\ndata is gathered from Melon, a popular Korean streaming service. The dataset is\nsuitable for music information retrieval tasks, in particular, auto-tagging and\nautomatic playlist continuation. Even though the latter can be addressed by\ncollaborative filtering approaches, audio provides opportunities for research\non track suggestions and building systems resistant to the cold-start problem,\nfor which we provide a baseline. Moreover, the playlists and the annotations\nincluded in the Melon Playlist Dataset make it suitable for metric learning and\nrepresentation learning.", "published": "2021-01-30 10:13:10", "link": "http://arxiv.org/abs/2102.00201v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
