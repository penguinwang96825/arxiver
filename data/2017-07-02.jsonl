{"title": "DAG-based Long Short-Term Memory for Neural Word Segmentation", "abstract": "Neural word segmentation has attracted more and more research interests for\nits ability to alleviate the effort of feature engineering and utilize the\nexternal resource by the pre-trained character or word embeddings. In this\npaper, we propose a new neural model to incorporate the word-level information\nfor Chinese word segmentation. Unlike the previous word-based models, our model\nstill adopts the framework of character-based sequence labeling, which has\nadvantages on both effectiveness and efficiency at the inference stage. To\nutilize the word-level information, we also propose a new long short-term\nmemory (LSTM) architecture over directed acyclic graph (DAG). Experimental\nresults demonstrate that our model leads to better performances than the\nbaseline models.", "published": "2017-07-02 07:41:10", "link": "http://arxiv.org/abs/1707.00248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grammatical Error Correction with Neural Reinforcement Learning", "abstract": "We propose a neural encoder-decoder model with reinforcement learning (NRL)\nfor grammatical error correction (GEC). Unlike conventional maximum likelihood\nestimation (MLE), the model directly optimizes towards an objective that\nconsiders a sentence-level, task-specific evaluation metric, avoiding the\nexposure bias issue in MLE. We demonstrate that NRL outperforms MLE both in\nhuman and automated evaluation metrics, achieving the state-of-the-art on a\nfluency-oriented GEC corpus.", "published": "2017-07-02 14:39:21", "link": "http://arxiv.org/abs/1707.00299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modulating early visual processing by language", "abstract": "It is commonly assumed that language refers to high-level visual concepts\nwhile leaving low-level visual processing unaffected. This view dominates the\ncurrent literature in computational models for language-vision tasks, where\nvisual and linguistic input are mostly processed independently before being\nfused into a single representation. In this paper, we deviate from this classic\npipeline and propose to modulate the \\emph{entire visual processing} by\nlinguistic input. Specifically, we condition the batch normalization parameters\nof a pretrained residual network (ResNet) on a language embedding. This\napproach, which we call MOdulated RESnet (\\MRN), significantly improves strong\nbaselines on two visual question answering tasks. Our ablation study shows that\nmodulating from the early stages of the visual processing is beneficial.", "published": "2017-07-02 04:06:01", "link": "http://arxiv.org/abs/1707.00683v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
