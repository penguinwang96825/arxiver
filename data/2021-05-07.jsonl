{"title": "A Dataset of Information-Seeking Questions and Answers Anchored in\n  Research Papers", "abstract": "Readers of academic research papers often read with the goal of answering\nspecific questions. Question Answering systems that can answer those questions\ncan make consumption of the content much more efficient. However, building such\ntools requires data that reflect the difficulty of the task arising from\ncomplex reasoning about claims made in multiple parts of a paper. In contrast,\nexisting information-seeking question answering datasets usually contain\nquestions about generic factoid-type information. We therefore present QASPER,\na dataset of 5,049 questions over 1,585 Natural Language Processing papers.\nEach question is written by an NLP practitioner who read only the title and\nabstract of the corresponding paper, and the question seeks information present\nin the full text. The questions are then answered by a separate set of NLP\npractitioners who also provide supporting evidence to answers. We find that\nexisting models that do well on other QA tasks do not perform well on answering\nthese questions, underperforming humans by at least 27 F1 points when answering\nthem from entire papers, motivating further research in document-grounded,\ninformation-seeking QA, which our dataset is designed to facilitate.", "published": "2021-05-07 00:12:34", "link": "http://arxiv.org/abs/2105.03011v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DExperts: Decoding-Time Controlled Text Generation with Experts and\n  Anti-Experts", "abstract": "Despite recent advances in natural language generation, it remains\nchallenging to control attributes of generated text. We propose DExperts:\nDecoding-time Experts, a decoding-time method for controlled text generation\nthat combines a pretrained language model with \"expert\" LMs and/or\n\"anti-expert\" LMs in a product of experts. Intuitively, under the ensemble,\ntokens only get high probability if they are considered likely by the experts,\nand unlikely by the anti-experts. We apply DExperts to language detoxification\nand sentiment-controlled generation, where we outperform existing controllable\ngeneration methods on both automatic and human evaluations. Moreover, because\nDExperts operates only on the output of the pretrained LM, it is effective with\n(anti-)experts of smaller size, including when operating on GPT-3. Our work\nhighlights the promise of tuning small LMs on text with (un)desirable\nattributes for efficient decoding-time steering.", "published": "2021-05-07 01:19:38", "link": "http://arxiv.org/abs/2105.03023v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing\n  Regressions In NLP Model Updates", "abstract": "Behavior of deep neural networks can be inconsistent between different\nversions. Regressions during model update are a common cause of concern that\noften over-weigh the benefits in accuracy or efficiency gain. This work focuses\non quantifying, reducing and analyzing regression errors in the NLP model\nupdates. Using negative flip rate as regression measure, we show that\nregression has a prevalent presence across tasks in the GLUE benchmark. We\nformulate the regression-free model updates into a constrained optimization\nproblem, and further reduce it into a relaxed form which can be approximately\noptimized through knowledge distillation training method. We empirically\nanalyze how model ensemble reduces regression. Finally, we conduct CheckList\nbehavioral testing to understand the distribution of regressions across\nlinguistic phenomena, and the efficacy of ensemble and distillation methods.", "published": "2021-05-07 03:33:00", "link": "http://arxiv.org/abs/2105.03048v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Shared Semantic Space for Speech-to-Text Translation", "abstract": "Having numerous potential applications and great impact, end-to-end speech\ntranslation (ST) has long been treated as an independent task, failing to fully\ndraw strength from the rapid advances of its sibling - text machine translation\n(MT). With text and audio inputs represented differently, the modality gap has\nrendered MT data and its end-to-end models incompatible with their ST\ncounterparts. In observation of this obstacle, we propose to bridge this\nrepresentation gap with Chimera. By projecting audio and text features to a\ncommon semantic representation, Chimera unifies MT and ST tasks and boosts the\nperformance on ST benchmarks, MuST-C and Augmented Librispeech, to a new\nstate-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,\nimproving the SOTA by a +1.9 BLEU margin. Further experimental analyses\ndemonstrate that the shared semantic space indeed conveys common knowledge\nbetween these two tasks and thus paves a new way for augmenting training\nresources across modalities. Code, data, and resources are available at\nhttps://github.com/Glaciohound/Chimera-ST.", "published": "2021-05-07 07:49:56", "link": "http://arxiv.org/abs/2105.03095v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CO-NNECT: A Framework for Revealing Commonsense Knowledge Paths as\n  Explicitations of Implicit Knowledge in Texts", "abstract": "In this work we leverage commonsense knowledge in form of knowledge paths to\nestablish connections between sentences, as a form of explicitation of implicit\nknowledge. Such connections can be direct (singlehop paths) or require\nintermediate concepts (multihop paths). To construct such paths we combine two\nmodel types in a joint framework we call Co-nnect: a relation classifier that\npredicts direct connections between concepts; and a target prediction model\nthat generates target or intermediate concepts given a source concept and a\nrelation, which we use to construct multihop paths. Unlike prior work that\nrelies exclusively on static knowledge sources, we leverage language models\nfinetuned on knowledge stored in ConceptNet, to dynamically generate knowledge\npaths, as explanations of implicit knowledge that connects sentences in texts.\nAs a central contribution we design manual and automatic evaluation settings\nfor assessing the quality of the generated paths. We conduct evaluations on two\nargumentative datasets and show that a combination of the two model types\ngenerates meaningful, high-quality knowledge paths between sentences that\nreveal implicit knowledge conveyed in text.", "published": "2021-05-07 10:43:43", "link": "http://arxiv.org/abs/2105.03157v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identity Signals in Emoji Do not Influence Perception of Factual Truth\n  on Twitter", "abstract": "Prior work has shown that Twitter users use skin-toned emoji as an act of\nself-representation to express their racial/ethnic identity. We test whether\nthis signal of identity can influence readers' perceptions about the content of\na post containing that signal. In a large scale (n=944) pre-registered\ncontrolled experiment, we manipulate the presence of skin-toned emoji and\nprofile photos in a task where readers rate obscure trivia facts (presented as\ntweets) as true or false. Using a Bayesian statistical analysis, we find that\nneither emoji nor profile photo has an effect on how readers rate these facts.\nThis result will be of some comfort to anyone concerned about the manipulation\nof online users through the crafting of fake profiles.", "published": "2021-05-07 10:56:19", "link": "http://arxiv.org/abs/2105.03160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Grounded Approach to Modeling Generic Knowledge Acquisition", "abstract": "We introduce and implement a cognitively plausible model for learning from\ngeneric language, statements that express generalizations about members of a\ncategory and are an important aspect of concept development in language\nacquisition (Carlson & Pelletier, 1995; Gelman, 2009). We extend a\ncomputational framework designed to model grounded language acquisition by\nintroducing the concept network. This new layer of abstraction enables the\nsystem to encode knowledge learned from generic statements and represent the\nassociations between concepts learned by the system. Through three tasks that\nutilize the concept network, we demonstrate that our extensions to ADAM can\nacquire generic information and provide an example of how ADAM can be used to\nmodel language acquisition.", "published": "2021-05-07 12:27:55", "link": "http://arxiv.org/abs/2105.03207v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VAULT: VAriable Unified Long Text Representation for Machine Reading\n  Comprehension", "abstract": "Existing models on Machine Reading Comprehension (MRC) require complex model\narchitecture for effectively modeling long texts with paragraph representation\nand classification, thereby making inference computationally inefficient for\nproduction use. In this work, we propose VAULT: a light-weight and\nparallel-efficient paragraph representation for MRC based on contextualized\nrepresentation from long document input, trained using a new Gaussian\ndistribution-based objective that pays close attention to the partially correct\ninstances that are close to the ground-truth. We validate our VAULT\narchitecture showing experimental results on two benchmark MRC datasets that\nrequire long context modeling; one Wikipedia-based (Natural Questions (NQ)) and\nthe other on TechNotes (TechQA). VAULT can achieve comparable performance on NQ\nwith a state-of-the-art (SOTA) complex document modeling approach while being\n16 times faster, demonstrating the efficiency of our proposed model. We also\ndemonstrate that our model can also be effectively adapted to a completely\ndifferent domain -- TechQA -- with large improvement over a model fine-tuned on\na previously published large PLM.", "published": "2021-05-07 13:03:43", "link": "http://arxiv.org/abs/2105.03229v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duplex Sequence-to-Sequence Learning for Reversible Machine Translation", "abstract": "Sequence-to-sequence learning naturally has two directions. How to\neffectively utilize supervision signals from both directions? Existing\napproaches either require two separate models, or a multitask-learned model but\nwith inferior performance. In this paper, we propose REDER (Reversible Duplex\nTransformer), a parameter-efficient model and apply it to machine translation.\nEither end of REDER can simultaneously input and output a distinct language.\nThus REDER enables reversible machine translation by simply flipping the input\nand output ends. Experiments verify that REDER achieves the first success of\nreversible machine translation, which helps outperform its multitask-trained\nbaselines by up to 1.3 BLEU.", "published": "2021-05-07 18:21:57", "link": "http://arxiv.org/abs/2105.03458v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring and Increasing Context Usage in Context-Aware Machine\n  Translation", "abstract": "Recent work in neural machine translation has demonstrated both the necessity\nand feasibility of using inter-sentential context -- context from sentences\nother than those currently being translated. However, while many current\nmethods present model architectures that theoretically can use this extra\ncontext, it is often not clear how much they do actually utilize it at\ntranslation time. In this paper, we introduce a new metric, conditional\ncross-mutual information, to quantify the usage of context by these models.\nUsing this metric, we measure how much document-level machine translation\nsystems use particular varieties of context. We find that target context is\nreferenced more than source context, and that conditioning on a longer context\nhas a diminishing effect on results. We then introduce a new, simple training\nmethod, context-aware word dropout, to increase the usage of context by\ncontext-aware models. Experiments show that our method increases context usage\nand that this reflects on the translation quality according to metrics such as\nBLEU and COMET, as well as performance on anaphoric pronoun resolution and\nlexical cohesion contrastive datasets.", "published": "2021-05-07 19:55:35", "link": "http://arxiv.org/abs/2105.03482v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Evaluation of Pre-trained Transformers for Human-Level NLP:\n  The Role of Sample Size and Dimensionality", "abstract": "In human-level NLP tasks, such as predicting mental health, personality, or\ndemographics, the number of observations is often smaller than the standard\n768+ hidden state sizes of each layer within modern transformer-based language\nmodels, limiting the ability to effectively leverage transformers. Here, we\nprovide a systematic study on the role of dimension reduction methods\n(principal components analysis, factorization techniques, or multi-layer\nauto-encoders) as well as the dimensionality of embedding vectors and sample\nsizes as a function of predictive performance. We first find that fine-tuning\nlarge models with a limited amount of data pose a significant difficulty which\ncan be overcome with a pre-trained dimension reduction regime. RoBERTa\nconsistently achieves top performance in human-level tasks, with PCA giving\nbenefit over other reduction methods in better handling users that write longer\ntexts. Finally, we observe that a majority of the tasks achieve results\ncomparable to the best performance with just $\\frac{1}{12}$ of the embedding\ndimensions.", "published": "2021-05-07 20:06:24", "link": "http://arxiv.org/abs/2105.03484v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction\n  from Language Models", "abstract": "Coherent discourse is distinguished from a mere collection of utterances by\nthe satisfaction of a diverse set of constraints, for example choice of\nexpression, logical relation between denoted events, and implicit compatibility\nwith world-knowledge. Do neural language models encode such constraints? We\ndesign an extendable set of test suites addressing different aspects of\ndiscourse and dialogue coherence. Unlike most previous coherence evaluation\nstudies, we address specific linguistic devices beyond sentence order\nperturbations, allowing for a more fine-grained analysis of what constitutes\ncoherence and what neural models trained on a language modelling objective do\nencode. Extending the targeted evaluation paradigm for neural language models\n(Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this\nparadigm is equally suited to evaluate linguistic qualities that contribute to\nthe notion of coherence.", "published": "2021-05-07 20:28:33", "link": "http://arxiv.org/abs/2105.03495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Cross-Domain Prerequisite Chain Learning using Variational\n  Graph Autoencoders", "abstract": "Learning prerequisite chains is an essential task for efficiently acquiring\nknowledge in both known and unknown domains. For example, one may be an expert\nin the natural language processing (NLP) domain but want to determine the best\norder to learn new concepts in an unfamiliar Computer Vision domain (CV). Both\ndomains share some common concepts, such as machine learning basics and deep\nlearning models. In this paper, we propose unsupervised cross-domain concept\nprerequisite chain learning using an optimized variational graph autoencoder.\nOur model learns to transfer concept prerequisite relations from an\ninformation-rich domain (source domain) to an information-poor domain (target\ndomain), substantially surpassing other baseline models. Also, we expand an\nexisting dataset by introducing two new domains: CV and Bioinformatics (BIO).\nThe annotated data and resources, as well as the code, will be made publicly\navailable.", "published": "2021-05-07 21:02:41", "link": "http://arxiv.org/abs/2105.03505v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding by Understanding Not: Modeling Negation in Language Models", "abstract": "Negation is a core construction in natural language. Despite being very\nsuccessful on many tasks, state-of-the-art pre-trained language models often\nhandle negation incorrectly. To improve language models in this regard, we\npropose to augment the language modeling objective with an unlikelihood\nobjective that is based on negated generic sentences from a raw text corpus. By\ntraining BERT with the resulting combined objective we reduce the mean top~1\nerror rate to 4% on the negated LAMA dataset. We also see some improvements on\nthe negated NLI benchmarks.", "published": "2021-05-07 21:58:35", "link": "http://arxiv.org/abs/2105.03519v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AraCOVID19-MFH: Arabic COVID-19 Multi-label Fake News and Hate Speech\n  Detection Dataset", "abstract": "Along with the COVID-19 pandemic, an \"infodemic\" of false and misleading\ninformation has emerged and has complicated the COVID-19 response efforts.\nSocial networking sites such as Facebook and Twitter have contributed largely\nto the spread of rumors, conspiracy theories, hate, xenophobia, racism, and\nprejudice. To combat the spread of fake news, researchers around the world have\nand are still making considerable efforts to build and share COVID-19 related\nresearch articles, models, and datasets. This paper releases \"AraCOVID19-MFH\" a\nmanually annotated multi-label Arabic COVID-19 fake news and hate speech\ndetection dataset. Our dataset contains 10,828 Arabic tweets annotated with 10\ndifferent labels. The labels have been designed to consider some aspects\nrelevant to the fact-checking task, such as the tweet's check worthiness,\npositivity/negativity, and factuality. To confirm our annotated dataset's\npractical utility, we used it to train and evaluate several classification\nmodels and reported the obtained results. Though the dataset is mainly designed\nfor fake news detection, it can also be used for hate speech detection,\nopinion/news classification, dialect identification, and many other tasks.", "published": "2021-05-07 09:52:44", "link": "http://arxiv.org/abs/2105.03143v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "The Shadowy Lives of Emojis: An Analysis of a Hacktivist Collective's\n  Use of Emojis on Twitter", "abstract": "Emojis have established themselves as a popular means of communication in\nonline messaging. Despite the apparent ubiquity in these image-based tokens,\nhowever, interpretation and ambiguity may allow for unique uses of emojis to\nappear. In this paper, we present the first examination of emoji usage by\nhacktivist groups via a study of the Anonymous collective on Twitter. This\nresearch aims to identify whether Anonymous affiliates have evolved their own\napproach to using emojis. To do this, we compare a large dataset of Anonymous\ntweets to a baseline tweet dataset from randomly sampled Twitter users using\ncomputational and qualitative analysis to compare their emoji usage. We utilise\nWord2Vec language models to examine the semantic relationships between emojis,\nidentifying clear distinctions in the emoji-emoji relationships of Anonymous\nusers. We then explore how emojis are used as a means of conveying emotions,\nfinding that despite little commonality in emoji-emoji semantic ties, Anonymous\nemoji usage displays similar patterns of emotional purpose to the emojis of\nbaseline Twitter users. Finally, we explore the textual context in which these\nemojis occur, finding that although similarities exist between the emoji usage\nof our Anonymous and baseline Twitter datasets, Anonymous users appear to have\nadopted more specific interpretations of certain emojis. This includes the use\nof emojis as a means of expressing adoration and infatuation towards notable\nAnonymous affiliates. These findings indicate that emojis appear to retain a\nconsiderable degree of similarity within Anonymous accounts as compared to more\ntypical Twitter users. However, their are signs that emoji usage in Anonymous\naccounts has evolved somewhat, gaining additional group-specific associations\nthat reveal new insights into the behaviours of this unusual collective.", "published": "2021-05-07 11:21:04", "link": "http://arxiv.org/abs/2105.03168v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Order in the Court: Explainable AI Methods Prone to Disagreement", "abstract": "By computing the rank correlation between attention weights and\nfeature-additive explanation methods, previous analyses either invalidate or\nsupport the role of attention-based explanations as a faithful and plausible\nmeasure of salience. To investigate whether this approach is appropriate, we\ncompare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and\nattention-based explanations, applied to two neural architectures trained on\nsingle- and pair-sequence language tasks. In most cases, we find that none of\nour chosen methods agree. Based on our empirical observations and theoretical\nobjections, we conclude that rank correlation does not measure the quality of\nfeature-additive methods. Practitioners should instead use the numerous and\nrigorous diagnostic methods proposed by the community.", "published": "2021-05-07 14:27:37", "link": "http://arxiv.org/abs/2105.03287v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Are Pre-trained Convolutions Better than Pre-trained Transformers?", "abstract": "In the era of pre-trained language models, Transformers are the de facto\nchoice of model architectures. While recent research has shown promise in\nentirely convolutional, or CNN, architectures, they have not been explored\nusing the pre-train-fine-tune paradigm. In the context of language models, are\nconvolutional models competitive to Transformers when pre-trained? This paper\ninvestigates this research question and presents several interesting findings.\nAcross an extensive set of experiments on 8 datasets/tasks, we find that\nCNN-based pre-trained models are competitive and outperform their Transformer\ncounterpart in certain scenarios, albeit with caveats. Overall, the findings\noutlined in this paper suggest that conflating pre-training and architectural\nadvances is misguided and that both advances should be considered\nindependently. We believe our research paves the way for a healthy amount of\noptimism in alternative architectures.", "published": "2021-05-07 15:13:30", "link": "http://arxiv.org/abs/2105.03322v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting by Pruning: A Case Study on BERT", "abstract": "Adapting pre-trained neural models to downstream tasks has become the\nstandard practice for obtaining high-quality models. In this work, we propose a\nnovel model adaptation paradigm, adapting by pruning, which prunes neural\nconnections in the pre-trained model to optimise the performance on the target\ntask; all remaining connections have their weights intact. We formulate\nadapting-by-pruning as an optimisation problem with a differentiable loss and\npropose an efficient algorithm to prune the model. We prove that the algorithm\nis near-optimal under standard assumptions and apply the algorithm to adapt\nBERT to some GLUE tasks. Results suggest that our method can prune up to 50%\nweights in BERT while yielding similar performance compared to the fine-tuned\nfull model. We also compare our method with other state-of-the-art pruning\nmethods and study the topological differences of their obtained sub-networks.", "published": "2021-05-07 15:51:08", "link": "http://arxiv.org/abs/2105.03343v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Executable Interval Temporal Logic Specifications", "abstract": "In this paper the reversibility of executable Interval Temporal Logic (ITL)\nspecifications is investigated. ITL allows for the reasoning about systems in\nterms of behaviours which are represented as non-empty sequences of states. It\nallows for the specification of systems at different levels of abstraction. At\na high level this specification is in terms of properties, for instance safety\nand liveness properties. At concrete level one can specify a system in terms of\nprogramming constructs. One can execute these concrete specification, i.e.,\ntest and simulate the behaviour of the system. In this paper we will formalise\nthis notion of executability of ITL specifications. ITL also has a reflection\noperator which allows for the reasoning about reversed behaviours. We will\ninvestigate the reversibility of executable ITL specifications, i.e., how one\ncan use this reflection operator to reverse the concrete behaviour of a\nparticular system.", "published": "2021-05-07 16:36:24", "link": "http://arxiv.org/abs/2105.03375v1", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Diff-Explainer: Differentiable Convex Optimization for Explainable\n  Multi-hop Inference", "abstract": "This paper presents Diff-Explainer, the first hybrid framework for\nexplainable multi-hop inference that integrates explicit constraints with\nneural architectures through differentiable convex optimization. Specifically,\nDiff-Explainer allows for the fine-tuning of neural representations within a\nconstrained optimization framework to answer and explain multi-hop questions in\nnatural language. To demonstrate the efficacy of the hybrid framework, we\ncombine existing ILP-based solvers for multi-hop Question Answering (QA) with\nTransformer-based representations. An extensive empirical evaluation on\nscientific and commonsense QA tasks demonstrates that the integration of\nexplicit constraints in an end-to-end differentiable framework can\nsignificantly improve the performance of non-differentiable ILP solvers (8.91%\n- 13.3%). Moreover, additional analysis reveals that Diff-Explainer is able to\nachieve strong performance when compared to standalone Transformers and\nprevious multi-hop approaches while still providing structured explanations in\nsupport of its predictions.", "published": "2021-05-07 17:49:19", "link": "http://arxiv.org/abs/2105.03417v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generalising Multilingual Concept-to-Text NLG with Language Agnostic\n  Delexicalisation", "abstract": "Concept-to-text Natural Language Generation is the task of expressing an\ninput meaning representation in natural language. Previous approaches in this\ntask have been able to generalise to rare or unseen instances by relying on a\ndelexicalisation of the input. However, this often requires that the input\nappears verbatim in the output text. This poses challenges in multilingual\nsettings, where the task expands to generate the output text in multiple\nlanguages given the same input. In this paper, we explore the application of\nmultilingual models in concept-to-text and propose Language Agnostic\nDelexicalisation, a novel delexicalisation method that uses multilingual\npretrained embeddings, and employs a character-level post-editing model to\ninflect words in their correct form during relexicalisation. Our experiments\nacross five datasets and five languages show that multilingual models\noutperform monolingual models in concept-to-text and that our framework\noutperforms previous approaches, especially for low resource languages.", "published": "2021-05-07 17:48:53", "link": "http://arxiv.org/abs/2105.03432v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Weight factorization for Multilingual Speech Recognition", "abstract": "End-to-end multilingual speech recognition involves using a single model\ntraining on a compositional speech corpus including many languages, resulting\nin a single neural network to handle transcribing different languages. Due to\nthe fact that each language in the training data has different characteristics,\nthe shared network may struggle to optimize for all various languages\nsimultaneously. In this paper we propose a novel multilingual architecture that\ntargets the core operation in neural networks: linear transformation functions.\nThe key idea of the method is to assign fast weight matrices for each language\nby decomposing each weight matrix into a shared component and a language\ndependent component. The latter is then factorized into vectors using rank-1\nassumptions to reduce the number of parameters per language. This efficient\nfactorization scheme is proved to be effective in two multilingual settings\nwith $7$ and $27$ languages, reducing the word error rates by $26\\%$ and $27\\%$\nrel. for two popular architectures LSTM and Transformer, respectively.", "published": "2021-05-07 00:12:02", "link": "http://arxiv.org/abs/2105.03010v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SpeechMoE: Scaling to Large Acoustic Models with Dynamic Routing Mixture\n  of Experts", "abstract": "Recently, Mixture of Experts (MoE) based Transformer has shown promising\nresults in many domains. This is largely due to the following advantages of\nthis architecture: firstly, MoE based Transformer can increase model capacity\nwithout computational cost increasing both at training and inference time.\nBesides, MoE based Transformer is a dynamic network which can adapt to the\nvarying complexity of input instances in realworld applications. In this work,\nwe explore the MoE based model for speech recognition, named SpeechMoE. To\nfurther control the sparsity of router activation and improve the diversity of\ngate values, we propose a sparsity L1 loss and a mean importance loss\nrespectively. In addition, a new router architecture is used in SpeechMoE which\ncan simultaneously utilize the information from a shared embedding network and\nthe hierarchical representation of different MoE layers. Experimental results\nshow that SpeechMoE can achieve lower character error rate (CER) with\ncomparable computation cost than traditional static networks, providing\n7.0%-23.0% relative CER improvements on four evaluation datasets.", "published": "2021-05-07 02:38:23", "link": "http://arxiv.org/abs/2105.03036v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SpeechNet: A Universal Modularized Model for Speech Processing Tasks", "abstract": "There is a wide variety of speech processing tasks ranging from extracting\ncontent information from speech signals to generating speech signals. For\ndifferent tasks, model networks are usually designed and tuned separately. If a\nuniversal model can perform multiple speech processing tasks, some tasks might\nbe improved with the related abilities learned from other tasks. The multi-task\nlearning of a wide variety of speech processing tasks with a universal model\nhas not been studied. This paper proposes a universal modularized model,\nSpeechNet, which treats all speech processing tasks into a speech/text input\nand speech/text output format. We select five essential speech processing tasks\nfor multi-task learning experiments with SpeechNet. We show that SpeechNet\nlearns all of the above tasks, and we further analyze which tasks can be\nimproved by other tasks. SpeechNet is modularized and flexible for\nincorporating more modules, tasks, or training approaches in the future. We\nrelease the code and experimental settings to facilitate the research of\nmodularized universal models and multi-task learning of speech processing\ntasks.", "published": "2021-05-07 05:31:34", "link": "http://arxiv.org/abs/2105.03070v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Survey of Data Augmentation Approaches for NLP", "abstract": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP", "published": "2021-05-07 06:03:45", "link": "http://arxiv.org/abs/2105.03075v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Benchmarking on Cloud based Speech-To-Text Services for French Speech\n  and Background Noise Effect", "abstract": "This study presents a large scale benchmarking on cloud based Speech-To-Text\nsystems: {Google Cloud Speech-To-Text}, {Microsoft Azure Cognitive Services},\n{Amazon Transcribe}, {IBM Watson Speech to Text}. For each systems, 40158 clean\nand noisy speech files about 101 hours are tested. Effect of background noise\non STT quality is also evaluated with 5 different Signal-to-noise ratios from\n40dB to 0dB. Results showed that {Microsoft Azure} provided lowest\ntranscription error rate $9.09\\%$ on clean speech, with high robustness to\nnoisy environment. {Google Cloud} and {Amazon Transcribe} gave similar\nperformance, but the latter is very limited for time-constraint usage. Though\n{IBM Watson} could work correctly in quiet conditions, it is highly sensible to\nnoisy speech which could strongly limit its application in real life\nsituations.", "published": "2021-05-07 17:41:34", "link": "http://arxiv.org/abs/2105.03409v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Lambek pregroups are Frobenius spiders in preorders", "abstract": "\"Spider\" is a nickname of special Frobenius algebras, a fundamental structure\nfrom mathematics, physics, and computer science. Pregroups are a fundamental\nstructure from linguistics. Pregroups and spiders have been used together in\nnatural language processing: one for syntax, the other for semantics. It turns\nout that pregroups themselves can be characterized as pointed spiders in the\ncategory of preordered relations, where they naturally arise from grammars. The\nother way around, preordered spider algebras in general can be characterized as\nunions of pregroups. This extends the characterization of relational spider\nalgebras as disjoint unions of groups. The compositional framework that emerged\nwith the results suggests new ways to understand and apply the basis structures\nin machine learning and data analysis.", "published": "2021-05-07 02:42:03", "link": "http://arxiv.org/abs/2105.03038v4", "categories": ["math.CT", "cs.CL", "cs.FL", "cs.LO", "math.LO", "68Q42, 03B47, 68T50, 03B65, 18A15, 18B35, 18D10, 18B10, 03B70, 91F20", "F.4.2; I.2.7; I.2.6"], "primary_category": "math.CT"}
{"title": "Online Acoustic System Identification Exploiting Kalman Filtering and an\n  Adaptive Impulse Response Subspace Model", "abstract": "We introduce a novel algorithm for online estimation of acoustic impulse\nresponses (AIRs) which allows for fast convergence by exploiting prior\nknowledge about the fundamental structure of AIRs. The proposed method assumes\nthat the variability of AIRs of an acoustic scene is confined to a\nlow-dimensional manifold which is embedded in a high-dimensional space of\npossible AIR estimates. We discuss various approaches to locally approximate\nthe AIR manifold by affine subspaces which are assumed to be tangential\nhyperplanes to the manifold. The validity of these model assumptions is\nverified for simulated data. Subsequently, we describe how the learned models\ncan be used to improve online AIR estimates by projecting them onto an\nadaptively estimated subspace. The parameters determining the subspace are\nlearned from training samples in a local neighbourhood to the current AIR\nestimate. This allows the system identification algorithm to benefit from\npreceding estimates in the acoustic scene. To assess the proximity of training\ndata AIRs to the current AIR estimate, we introduce a probabilistic extension\nof the Euclidean distance which improves the performance for applications with\ncorrelated excitation signals. Furthermore, we describe how model imperfections\ncan be tackled by a soft projection of the AIR estimates. The proposed\nalgorithm exhibits significantly faster convergence properties in comparison to\na high-performance state-of-the-art algorithm. Furthermore, we show an improved\nsteady-state performance for speech-excited system identification scenarios\nsuffering from high-level interfering noise and nonunique solutions.", "published": "2021-05-07 15:41:55", "link": "http://arxiv.org/abs/2105.03337v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
