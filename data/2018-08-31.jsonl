{"title": "Gromov-Wasserstein Alignment of Word Embedding Spaces", "abstract": "Cross-lingual or cross-domain correspondences play key roles in tasks ranging\nfrom machine translation to transfer learning. Recently, purely unsupervised\nmethods operating on monolingual embeddings have become effective alignment\ntools. Current state-of-the-art methods, however, involve multiple steps,\nincluding heuristic post-hoc refinement strategies. In this paper, we cast the\ncorrespondence problem directly as an optimal transport (OT) problem, building\non the idea that word embeddings arise from metric recovery algorithms. Indeed,\nwe exploit the Gromov-Wasserstein distance that measures how similarities\nbetween pairs of words relate across languages. We show that our OT objective\ncan be estimated efficiently, requires little or no tuning, and results in\nperformance comparable with the state-of-the-art in various unsupervised word\ntranslation tasks.", "published": "2018-08-31 18:00:27", "link": "http://arxiv.org/abs/1809.00013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do RNN Language Models Learn about Filler-Gap Dependencies?", "abstract": "RNN language models have achieved state-of-the-art perplexity results and\nhave proven useful in a suite of NLP tasks, but it is as yet unclear what\nsyntactic generalizations they learn. Here we investigate whether\nstate-of-the-art RNN language models represent long-distance filler-gap\ndependencies and constraints on them. Examining RNN behavior on experimentally\ncontrolled sentences designed to expose filler-gap dependencies, we show that\nRNNs can represent the relationship in multiple syntactic positions and over\nlarge spans of text. Furthermore, we show that RNNs learn a subset of the known\nrestrictions on filler-gap dependencies, known as island constraints: RNNs show\nevidence for wh-islands, adjunct islands, and complex NP islands. These studies\ndemonstrates that state-of-the-art RNN models are able to learn and generalize\nabout empty syntactic positions.", "published": "2018-08-31 20:04:42", "link": "http://arxiv.org/abs/1809.00042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalizing Procrustes Analysis for Better Bilingual Dictionary\n  Induction", "abstract": "Most recent approaches to bilingual dictionary induction find a linear\nalignment between the word vector spaces of two languages. We show that\nprojecting the two languages onto a third, latent space, rather than directly\nonto each other, while equivalent in terms of expressivity, makes it easier to\nlearn approximate alignments. Our modified approach also allows for supporting\nlanguages to be included in the alignment process, to obtain an even better\nperformance in low resource settings.", "published": "2018-08-31 21:20:00", "link": "http://arxiv.org/abs/1809.00064v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Indicatements that character language models learn English\n  morpho-syntactic units and regularities", "abstract": "Character language models have access to surface morphological patterns, but\nit is not clear whether or how they learn abstract morphological regularities.\nWe instrument a character language model with several probes, finding that it\ncan develop a specific unit to identify word boundaries and, by extension,\nmorpheme boundaries, which allows it to capture linguistic properties and\nregularities of these units. Our language model proves surprisingly good at\nidentifying the selectional restrictions of English derivational morphemes, a\ntask that requires both morphological and syntactic awareness. Thus we conclude\nthat, when morphemes overlap extensively with the words of a language, a\ncharacter language model can perform morphological abstraction.", "published": "2018-08-31 21:27:54", "link": "http://arxiv.org/abs/1809.00066v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When to Finish? Optimal Beam Search for Neural Text Generation (modulo\n  beam size)", "abstract": "In neural text generation such as neural machine translation, summarization,\nand image captioning, beam search is widely used to improve the output text\nquality. However, in the neural generation setting, hypotheses can finish in\ndifferent steps, which makes it difficult to decide when to end beam search to\nensure optimality. We propose a provably optimal beam search algorithm that\nwill always return the optimal-score complete hypothesis (modulo beam size),\nand finish as soon as the optimality is established (finishing no later than\nthe baseline). To counter neural generation's tendency for shorter hypotheses,\nwe also introduce a bounded length reward mechanism which allows a modified\nversion of our beam search algorithm to remain optimal. Experiments on neural\nmachine translation demonstrate that our principled beam search algorithm leads\nto improvement in BLEU score over previously proposed alternatives.", "published": "2018-08-31 22:01:48", "link": "http://arxiv.org/abs/1809.00069v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Nightmare at test time: How punctuation prevents parsers from\n  generalizing", "abstract": "Punctuation is a strong indicator of syntactic structure, and parsers trained\non text with punctuation often rely heavily on this signal. Punctuation is a\ndiversion, however, since human language processing does not rely on\npunctuation to the same extent, and in informal texts, we therefore often leave\nout punctuation. We also use punctuation ungrammatically for emphatic or\ncreative purposes, or simply by mistake. We show that (a) dependency parsers\nare sensitive to both absence of punctuation and to alternative uses; (b)\nneural parsers tend to be more sensitive than vintage parsers; (c) training\nneural parsers without punctuation outperforms all out-of-the-box parsers\nacross all scenarios where punctuation departs from standard punctuation. Our\nmain experiments are on synthetically corrupted data to study the effect of\npunctuation in isolation and avoid potential confounds, but we also show\neffects on out-of-domain data.", "published": "2018-08-31 22:07:19", "link": "http://arxiv.org/abs/1809.00070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural DrugNet", "abstract": "In this paper, we describe the system submitted for the shared task on Social\nMedia Mining for Health Applications by the team Light. Previous works\ndemonstrate that LSTMs have achieved remarkable performance in natural language\nprocessing tasks. We deploy an ensemble of two LSTM models. The first one is a\npretrained language model appended with a classifier and takes words as input,\nwhile the second one is a LSTM model with an attention unit over it which takes\ncharacter tri-gram as input. We call the ensemble of these two models:\nNeural-DrugNet. Our system ranks 2nd in the second shared task: Automatic\nclassification of posts describing medication intake.", "published": "2018-08-31 10:16:19", "link": "http://arxiv.org/abs/1809.01500v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale", "abstract": "AISHELL-1 is by far the largest open-source speech corpus available for\nMandarin speech recognition research. It was released with a baseline system\ncontaining solid training and testing pipelines for Mandarin ASR. In AISHELL-2,\n1000 hours of clean read-speech data from iOS is published, which is free for\nacademic usage. On top of AISHELL-2 corpus, an improved recipe is developed and\nreleased, containing key components for industrial applications, such as\nChinese word segmentation, flexible vocabulary expension and phone set\ntransformation etc. Pipelines support various state-of-the-art techniques, such\nas time-delayed neural networks and Lattic-Free MMI objective funciton. In\naddition, we also release dev and test data from other channels(Android and\nMic). For research community, we hope that AISHELL-2 corpus can be a solid\nresource for topics like transfer learning and robust ASR. For industry, we\nhope AISHELL-2 recipe can be a helpful reference for building meaningful\nindustrial systems and products.", "published": "2018-08-31 03:11:08", "link": "http://arxiv.org/abs/1808.10583v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ensemble Sequence Level Training for Multimodal MT: OSU-Baidu WMT18\n  Multimodal Machine Translation System Report", "abstract": "This paper describes multimodal machine translation systems developed jointly\nby Oregon State University and Baidu Research for WMT 2018 Shared Task on\nmultimodal translation. In this paper, we introduce a simple approach to\nincorporate image information by feeding image features to the decoder side. We\nalso explore different sequence level training methods including scheduled\nsampling and reinforcement learning which lead to substantial improvements. Our\nsystems ensemble several models using different architectures and training\nmethods and achieve the best performance for three subtasks: En-De and En-Cs in\ntask 1 and (En+De+Fr)-Cs task 1B.", "published": "2018-08-31 04:14:40", "link": "http://arxiv.org/abs/1808.10592v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explicit State Tracking with Semi-Supervision for Neural Dialogue\n  Generation", "abstract": "The task of dialogue generation aims to automatically provide responses given\nprevious utterances. Tracking dialogue states is an important ingredient in\ndialogue generation for estimating users' intention. However, the\n\\emph{expensive nature of state labeling} and the \\emph{weak interpretability}\nmake the dialogue state tracking a challenging problem for both task-oriented\nand non-task-oriented dialogue generation: For generating responses in\ntask-oriented dialogues, state tracking is usually learned from manually\nannotated corpora, where the human annotation is expensive for training; for\ngenerating responses in non-task-oriented dialogues, most of existing work\nneglects the explicit state tracking due to the unlimited number of dialogue\nstates.\n  In this paper, we propose the \\emph{semi-supervised explicit dialogue state\ntracker} (SEDST) for neural dialogue generation. To this end, our approach has\ntwo core ingredients: \\emph{CopyFlowNet} and \\emph{posterior regularization}.\nSpecifically, we propose an encoder-decoder architecture, named\n\\emph{CopyFlowNet}, to represent an explicit dialogue state with a\nprobabilistic distribution over the vocabulary space. To optimize the training\nprocedure, we apply a posterior regularization strategy to integrate indirect\nsupervision. Extensive experiments conducted on both task-oriented and\nnon-task-oriented dialogue corpora demonstrate the effectiveness of our\nproposed model. Moreover, we find that our proposed semi-supervised dialogue\nstate tracker achieves a comparable performance as state-of-the-art supervised\nlearning baselines in state tracking procedure.", "published": "2018-08-31 04:27:41", "link": "http://arxiv.org/abs/1808.10596v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Language Models Understand Anything? On the Ability of LSTMs to\n  Understand Negative Polarity Items", "abstract": "In this paper, we attempt to link the inner workings of a neural language\nmodel to linguistic theory, focusing on a complex phenomenon well discussed in\nformal linguis- tics: (negative) polarity items. We briefly discuss the leading\nhypotheses about the licensing contexts that allow negative polarity items and\nevaluate to what extent a neural language model has the ability to correctly\nprocess a subset of such constructions. We show that the model finds a relation\nbetween the licensing context and the negative polarity item and appears to be\naware of the scope of this context, which we extract from a parse tree of the\nsentence. With this research, we hope to pave the way for other studies linking\nformal linguistics to deep learning.", "published": "2018-08-31 08:21:45", "link": "http://arxiv.org/abs/1808.10627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Analysis of the Role of Amplifiers, Downtoners, and\n  Negations in Emotion Classification in Microblogs", "abstract": "The effect of amplifiers, downtoners, and negations has been studied in\ngeneral and particularly in the context of sentiment analysis. However, there\nis only limited work which aims at transferring the results and methods to\ndiscrete classes of emotions, e. g., joy, anger, fear, sadness, surprise, and\ndisgust. For instance, it is not straight-forward to interpret which emotion\nthe phrase \"not happy\" expresses. With this paper, we aim at obtaining a better\nunderstanding of such modifiers in the context of emotion-bearing words and\ntheir impact on document-level emotion classification, namely, microposts on\nTwitter. We select an appropriate scope detection method for modifiers of\nemotion words, incorporate it in a document-level emotion classification model\nas additional bag of words and show that this approach improves the performance\nof emotion classification. In addition, we build a term weighting approach\nbased on the different modifiers into a lexical model for the analysis of the\nsemantics of modifiers and their impact on emotion meaning. We show that\namplifiers separate emotions expressed with an emotion- bearing word more\nclearly from other secondary connotations. Downtoners have the opposite effect.\nIn addition, we discuss the meaning of negations of emotion-bearing words. For\ninstance we show empirically that \"not happy\" is closer to sadness than to\nanger and that fear-expressing words in the scope of downtoners often express\nsurprise.", "published": "2018-08-31 09:55:06", "link": "http://arxiv.org/abs/1808.10653v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural\n  Machine Translation", "abstract": "Tying the weights of the target word embeddings with the target word\nclassifiers of neural machine translation models leads to faster training and\noften to better translation quality. Given the success of this parameter\nsharing, we investigate other forms of sharing in between no sharing and hard\nequality of parameters. In particular, we propose a structure-aware output\nlayer which captures the semantic structure of the output space of words within\na joint input-output embedding. The model is a generalized form of weight tying\nwhich shares parameters but allows learning a more flexible relationship with\ninput word embeddings and allows the effective capacity of the output layer to\nbe controlled. In addition, the model shares weights across output classifiers\nand translation contexts which allows it to better leverage prior knowledge\nabout them. Our evaluation on English-to-Finnish and English-to-German datasets\nshows the effectiveness of the method against strong encoder-decoder baselines\ntrained with or without weight tying.", "published": "2018-08-31 11:14:36", "link": "http://arxiv.org/abs/1808.10681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Keywords from Open-Ended Business Survey Questions", "abstract": "Open-ended survey data constitute an important basis in research as well as\nfor making business decisions. Collecting and manually analysing free-text\nsurvey data is generally more costly than collecting and analysing survey data\nconsisting of answers to multiple-choice questions. Yet free-text data allow\nfor new content to be expressed beyond predefined categories and are a very\nvaluable source of new insights into people's opinions. At the same time,\nsurveys always make ontological assumptions about the nature of the entities\nthat are researched, and this has vital ethical consequences. Human\ninterpretations and opinions can only be properly ascertained in their richness\nusing textual data sources; if these sources are analyzed appropriately, the\nessential linguistic nature of humans and social entities is safeguarded.\nNatural Language Processing (NLP) offers possibilities for meeting this ethical\nbusiness challenge by automating the analysis of natural language and thus\nallowing for insightful investigations of human judgements. We present a\ncomputational pipeline for analysing large amounts of responses to open-ended\nquestions in surveys and extract keywords that appropriately represent people's\nopinions. This pipeline addresses the need to perform such tasks outside the\nscope of both commercial software and bespoke analysis, exceeds the performance\nto state-of-the-art systems, and performs this task in a transparent way that\nallows for scrutinising and exposing potential biases in the analysis.\nFollowing the principle of Open Data Science, our code is open-source and\ngeneralizable to other datasets.", "published": "2018-08-31 11:20:53", "link": "http://arxiv.org/abs/1808.10685v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Imitation Learning for Neural Morphological String Transduction", "abstract": "We employ imitation learning to train a neural transition-based string\ntransducer for morphological tasks such as inflection generation and\nlemmatization. Previous approaches to training this type of model either rely\non an external character aligner for the production of gold action sequences,\nwhich results in a suboptimal model due to the unwarranted dependence on a\nsingle gold action sequence despite spurious ambiguity, or require warm\nstarting with an MLE model. Our approach only requires a simple expert policy,\neliminating the need for a character aligner or warm start. It also addresses\nfamiliar MLE training biases and leads to strong and state-of-the-art\nperformance on several benchmarks.", "published": "2018-08-31 12:08:55", "link": "http://arxiv.org/abs/1808.10701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Continuous Turn-Taking Prediction Using Multiscale RNNs", "abstract": "In human conversational interactions, turn-taking exchanges can be\ncoordinated using cues from multiple modalities. To design spoken dialog\nsystems that can conduct fluid interactions it is desirable to incorporate cues\nfrom separate modalities into turn-taking models. We propose that there is an\nappropriate temporal granularity at which modalities should be modeled. We\ndesign a multiscale RNN architecture to model modalities at separate timescales\nin a continuous manner. Our results show that modeling linguistic and acoustic\nfeatures at separate temporal rates can be beneficial for turn-taking modeling.\nWe also show that our approach can be used to incorporate gaze features into\nturn-taking models.", "published": "2018-08-31 14:38:50", "link": "http://arxiv.org/abs/1808.10785v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cognate-aware morphological segmentation for multilingual neural\n  translation", "abstract": "This article describes the Aalto University entry to the WMT18 News\nTranslation Shared Task. We participate in the multilingual subtrack with a\nsystem trained under the constrained condition to translate from English to\nboth Finnish and Estonian. The system is based on the Transformer model. We\nfocus on improving the consistency of morphological segmentation for words that\nare similar orthographically, semantically, and distributionally; such words\ninclude etymological cognates, loan words, and proper names. For this, we\nintroduce Cognate Morfessor, a multilingual variant of the Morfessor method. We\nshow that our approach improves the translation quality particularly for\nEstonian, which has less resources for training the translation model.", "published": "2018-08-31 14:54:49", "link": "http://arxiv.org/abs/1808.10791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The MeMAD Submission to the WMT18 Multimodal Translation Task", "abstract": "This paper describes the MeMAD project entry to the WMT Multimodal Machine\nTranslation Shared Task.\n  We propose adapting the Transformer neural machine translation (NMT)\narchitecture to a multi-modal setting. In this paper, we also describe the\npreliminary experiments with text-only translation systems leading us up to\nthis choice.\n  We have the top scoring system for both English-to-German and\nEnglish-to-French, according to the automatic metrics for flickr18.\n  Our experiments show that the effect of the visual features in our system is\nsmall. Our largest gains come from the quality of the underlying text-only NMT\nsystem. We find that appropriate use of additional data is effective.", "published": "2018-08-31 15:14:59", "link": "http://arxiv.org/abs/1808.10802v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spherical Latent Spaces for Stable Variational Autoencoders", "abstract": "A hallmark of variational autoencoders (VAEs) for text processing is their\ncombination of powerful encoder-decoder models, such as LSTMs, with simple\nlatent distributions, typically multivariate Gaussians. These models pose a\ndifficult optimization problem: there is an especially bad local optimum where\nthe variational posterior always equals the prior and the model does not use\nthe latent variable at all, a kind of \"collapse\" which is encouraged by the KL\ndivergence term of the objective. In this work, we experiment with another\nchoice of latent distribution, namely the von Mises-Fisher (vMF) distribution,\nwhich places mass on the surface of the unit hypersphere. With this choice of\nprior and posterior, the KL divergence term now only depends on the variance of\nthe vMF distribution, giving us the ability to treat it as a fixed\nhyperparameter. We show that doing so not only averts the KL collapse, but\nconsistently gives better likelihoods than Gaussians across a range of modeling\nconditions, including recurrent language modeling and bag-of-words document\nmodeling. An analysis of the properties of our vMF representations shows that\nthey learn richer and more nuanced structures in their latent representations\nthan their Gaussian counterparts.", "published": "2018-08-31 15:21:05", "link": "http://arxiv.org/abs/1808.10805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical CVAE for Fine-Grained Hate Speech Classification", "abstract": "Existing work on automated hate speech detection typically focuses on binary\nclassification or on differentiating among a small set of categories. In this\npaper, we propose a novel method on a fine-grained hate speech classification\ntask, which focuses on differentiating among 40 hate groups of 13 different\nhate group categories. We first explore the Conditional Variational Autoencoder\n(CVAE) as a discriminative model and then extend it to a hierarchical\narchitecture to utilize the additional hate category information for more\naccurate prediction. Experimentally, we show that incorporating the hate\ncategory information for training can significantly improve the classification\nperformance and our proposed model outperforms commonly-used discriminative\nmodels.", "published": "2018-08-31 23:53:18", "link": "http://arxiv.org/abs/1809.00088v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning to Describe Differences Between Pairs of Similar Images", "abstract": "In this paper, we introduce the task of automatically generating text to\ndescribe the differences between two similar images. We collect a new dataset\nby crowd-sourcing difference descriptions for pairs of image frames extracted\nfrom video-surveillance footage. Annotators were asked to succinctly describe\nall the differences in a short paragraph. As a result, our novel dataset\nprovides an opportunity to explore models that align language and vision, and\ncapture visual salience. The dataset may also be a useful benchmark for\ncoherent multi-sentence generation. We perform a firstpass visual analysis that\nexposes clusters of differing pixels as a proxy for object-level differences.\nWe propose a model that captures visual salience by using a latent variable to\nalign clusters of differing pixels with output sentences. We find that, for\nboth single-sentence generation and as well as multi-sentence generation, the\nproposed model outperforms the models that use attention alone.", "published": "2018-08-31 03:15:28", "link": "http://arxiv.org/abs/1808.10584v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Retrieve-and-Read: Multi-task Learning of Information Retrieval and\n  Reading Comprehension", "abstract": "This study considers the task of machine reading at scale (MRS) wherein,\ngiven a question, a system first performs the information retrieval (IR) task\nof finding relevant passages in a knowledge source and then carries out the\nreading comprehension (RC) task of extracting an answer span from the passages.\nPrevious MRS studies, in which the IR component was trained without considering\nanswer spans, struggled to accurately find a small number of relevant passages\nfrom a large set of passages. In this paper, we propose a simple and effective\napproach that incorporates the IR and RC tasks by using supervised multi-task\nlearning in order that the IR component can be trained by considering answer\nspans. Experimental results on the standard benchmark, answering SQuAD\nquestions using the full Wikipedia as the knowledge source, showed that our\nmodel achieved state-of-the-art performance. Moreover, we thoroughly evaluated\nthe individual contributions of our model components with our new Japanese\ndataset and SQuAD. The results showed significant improvements in the IR task\nand provided a new perspective on IR for RC: it is effective to teach which\npart of the passage answers the question rather than to give only a relevance\nscore to the whole passage.", "published": "2018-08-31 08:22:12", "link": "http://arxiv.org/abs/1808.10628v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "How agents see things: On visual representations in an emergent language\n  game", "abstract": "There is growing interest in the language developed by agents interacting in\nemergent-communication settings. Earlier studies have focused on the agents'\nsymbol usage, rather than on their representation of visual input. In this\npaper, we consider the referential games of Lazaridou et al. (2017) and\ninvestigate the representations the agents develop during their evolving\ninteraction. We find that the agents establish successful communication by\ninducing visual representations that almost perfectly align with each other,\nbut, surprisingly, do not capture the conceptual properties of the objects\ndepicted in the input images. We conclude that, if we are interested in\ndeveloping language-like communication systems, we must pay more attention to\nthe visual semantics agents associate to the symbols they use.", "published": "2018-08-31 11:56:10", "link": "http://arxiv.org/abs/1808.10696v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Denoising Neural Machine Translation Training with Trusted Data and\n  Online Data Selection", "abstract": "Measuring domain relevance of data and identifying or selecting well-fit\ndomain data for machine translation (MT) is a well-studied topic, but denoising\nis not yet. Denoising is concerned with a different type of data quality and\ntries to reduce the negative impact of data noise on MT training, in\nparticular, neural MT (NMT) training. This paper generalizes methods for\nmeasuring and selecting data for domain MT and applies them to denoising NMT\ntraining. The proposed approach uses trusted data and a denoising curriculum\nrealized by online data selection. Intrinsic and extrinsic evaluations of the\napproach show its significant effectiveness for NMT to train on data with\nsevere noise.", "published": "2018-08-31 22:01:45", "link": "http://arxiv.org/abs/1809.00068v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Deep Neural Network Sentence Level Classification Method with Context\n  Information", "abstract": "In the sentence classification task, context formed from sentences adjacent\nto the sentence being classified can provide important information for\nclassification. This context is, however, often ignored. Where methods do make\nuse of context, only small amounts are considered, making it difficult to\nscale. We present a new method for sentence classification, Context-LSTM-CNN,\nthat makes use of potentially large contexts. The method also utilizes\nlong-range dependencies within the sentence being classified, using an LSTM,\nand short-span features, using a stacked CNN. Our experiments demonstrate that\nthis approach consistently improves over previous methods on two different\ndatasets.", "published": "2018-08-31 14:45:33", "link": "http://arxiv.org/abs/1809.00934v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "A Supervised Learning Approach For Heading Detection", "abstract": "As the Portable Document Format (PDF) file format increases in popularity,\nresearch in analysing its structure for text extraction and analysis is\nnecessary. Detecting headings can be a crucial component of classifying and\nextracting meaningful data. This research involves training a supervised\nlearning model to detect headings with features carefully selected through\nrecursive feature elimination. The best performing classifier had an accuracy\nof 96.95%, sensitivity of 0.986 and a specificity of 0.953. This research into\nheading detection contributes to the field of PDF based text extraction and can\nbe applied to the automation of large scale PDF text analysis in a variety of\nprofessional and policy based contexts.", "published": "2018-08-31 19:31:05", "link": "http://arxiv.org/abs/1809.01477v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Speaker Fluency Level Classification Using Machine Learning Techniques", "abstract": "Level assessment for foreign language students is necessary for putting them\nin the right level group, furthermore, interviewing students is a very\ntime-consuming task, so we propose to automate the evaluation of speaker\nfluency level by implementing machine learning techniques. This work presents\nan audio processing system capable of classifying the level of fluency of\nnon-native English speakers using five different machine learning models. As a\nfirst step, we have built our own dataset, which consists of labeled audio\nconversations in English between people ranging in different fluency\ndomains/classes (low, intermediate, high). We segment the audio conversations\ninto 5s non-overlapped audio clips to perform feature extraction on them. We\nstart by extracting Mel cepstral coefficients from the audios, selecting 20\ncoefficients is an appropriate quantity for our data. We thereafter extracted\nzero-crossing rate, root mean square energy and spectral flux features, proving\nthat this improves model performance. Out of a total of 1424 audio segments,\nwith 70% training data and 30% test data, one of our trained models (support\nvector machine) achieved a classification accuracy of 94.39%, whereas the other\nfour models passed an 89% classification accuracy threshold.", "published": "2018-08-31 00:15:53", "link": "http://arxiv.org/abs/1808.10556v1", "categories": ["stat.ML", "cs.CL", "cs.LG", "I.2.1"], "primary_category": "stat.ML"}
{"title": "Multi-Hop Knowledge Graph Reasoning with Reward Shaping", "abstract": "Multi-hop reasoning is an effective approach for query answering (QA) over\nincomplete knowledge graphs (KGs). The problem can be formulated in a\nreinforcement learning (RL) setup, where a policy-based agent sequentially\nextends its inference path until it reaches a target. However, in an incomplete\nKG environment, the agent receives low-quality rewards corrupted by false\nnegatives in the training data, which harms generalization at test time.\nFurthermore, since no golden action sequence is used for training, the agent\ncan be misled by spurious search trajectories that incidentally lead to the\ncorrect answer. We propose two modeling advances to address both issues: (1) we\nreduce the impact of false negative supervision by adopting a pretrained\none-hop embedding model to estimate the reward of unobserved facts; (2) we\ncounter the sensitivity to spurious paths of on-policy RL by forcing the agent\nto explore a diverse set of paths using randomly generated edge masks. Our\napproach significantly improves over existing path-based KGQA models on several\nbenchmark datasets and is comparable or better than embedding-based models.", "published": "2018-08-31 01:55:09", "link": "http://arxiv.org/abs/1808.10568v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Bottom-Up Abstractive Summarization", "abstract": "Neural network-based methods for abstractive summarization produce outputs\nthat are more fluent than other techniques, but which can be poor at content\nselection. This work proposes a simple technique for addressing this issue: use\na data-efficient content selector to over-determine phrases in a source\ndocument that should be part of the summary. We use this selector as a\nbottom-up attention step to constrain the model to likely phrases. We show that\nthis approach improves the ability to compress text, while still generating\nfluent summaries. This two-step process is both simpler and higher performing\nthan other end-to-end content selection models, leading to significant\nimprovements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the\ncontent selector can be trained with as little as 1,000 sentences, making it\neasy to transfer a trained summarizer to a new domain.", "published": "2018-08-31 14:55:52", "link": "http://arxiv.org/abs/1808.10792v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Single-Microphone Speech Enhancement and Separation Using Deep Learning", "abstract": "The cocktail party problem comprises the challenging task of understanding a\nspeech signal in a complex acoustic environment, where multiple speakers and\nbackground noise signals simultaneously interfere with the speech signal of\ninterest. A signal processing algorithm that can effectively increase the\nspeech intelligibility and quality of speech signals in such complicated\nacoustic situations is highly desirable. Especially for applications involving\nmobile communication devices and hearing assistive devices. Due to the\nre-emergence of machine learning techniques, today, known as deep learning, the\nchallenges involved with such algorithms might be overcome. In this PhD thesis,\nwe study and develop deep learning-based techniques for two sub-disciplines of\nthe cocktail party problem: single-microphone speech enhancement and\nsingle-microphone multi-talker speech separation. Specifically, we conduct\nin-depth empirical analysis of the generalizability capability of modern deep\nlearning-based single-microphone speech enhancement algorithms. We show that\nperformance of such algorithms is closely linked to the training data, and good\ngeneralizability can be achieved with carefully designed training data.\nFurthermore, we propose uPIT, a deep learning-based algorithm for\nsingle-microphone speech separation and we report state-of-the-art results on a\nspeaker-independent multi-talker speech separation task. Additionally, we show\nthat uPIT works well for joint speech separation and enhancement without\nexplicit prior knowledge about the noise type or number of speakers. Finally,\nwe show that deep learning-based speech enhancement algorithms designed to\nminimize the classical short-time spectral amplitude mean squared error leads\nto enhanced speech signals which are essentially optimal in terms of STOI, a\nstate-of-the-art speech intelligibility estimator.", "published": "2018-08-31 07:55:20", "link": "http://arxiv.org/abs/1808.10620v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Content-based feature exploration for transparent music recommendation\n  using self-attentive genre classification", "abstract": "Interpretation of retrieved results is an important issue in music\nrecommender systems, particularly from a user perspective. In this study, we\ninvestigate the methods for providing interpretability of content features\nusing self-attention. We extract lyric features with the self-attentive genre\nclassification model trained on 140,000 tracks of lyrics. Likewise, we extract\nacoustic features using the acoustic model with self-attention trained on\n120,000 tracks of acoustic signals. The experimental results show that the\nproposed methods provide the characteristics that are interpretable in terms of\nboth lyrical and musical contents. We demonstrate this by visualizing the\nattention weights, and by presenting the most similar songs found using lyric\nor audio features.", "published": "2018-08-31 05:05:05", "link": "http://arxiv.org/abs/1808.10600v2", "categories": ["cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.IR"}
{"title": "Self-Attention Linguistic-Acoustic Decoder", "abstract": "The conversion from text to speech relies on the accurate mapping from\nlinguistic to acoustic symbol sequences, for which current practice employs\nrecurrent statistical models like recurrent neural networks. Despite the good\nperformance of such models (in terms of low distortion in the generated\nspeech), their recursive structure tends to make them slow to train and to\nsample from. In this work, we try to overcome the limitations of recursive\nstructure by using a module based on the transformer decoder network, designed\nwithout recurrent connections but emulating them with attention and positioning\ncodes. Our results show that the proposed decoder network is competitive in\nterms of distortion when compared to a recurrent baseline, whilst being\nsignificantly faster in terms of CPU inference time. On average, it increases\nMel cepstral distortion between 0.1 and 0.3 dB, but it is over an order of\nmagnitude faster on average. Fast inference is important for the deployment of\nspeech synthesis systems on devices with restricted resources, like mobile\nphones or embedded systems, where speaking virtual assistants are gaining\nimportance.", "published": "2018-08-31 11:08:41", "link": "http://arxiv.org/abs/1808.10678v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Whispered-to-voiced Alaryngeal Speech Conversion with Generative\n  Adversarial Networks", "abstract": "Most methods of voice restoration for patients suffering from aphonia either\nproduce whispered or monotone speech. Apart from intelligibility, this type of\nspeech lacks expressiveness and naturalness due to the absence of pitch\n(whispered speech) or artificial generation of it (monotone speech). Existing\ntechniques to restore prosodic information typically combine a vocoder, which\nparameterises the speech signal, with machine learning techniques that predict\nprosodic information. In contrast, this paper describes an end-to-end neural\napproach for estimating a fully-voiced speech waveform from whispered\nalaryngeal speech. By adapting our previous work in speech enhancement with\ngenerative adversarial networks, we develop a speaker-dependent model to\nperform whispered-to-voiced speech conversion. Preliminary qualitative results\nshow effectiveness in re-generating voiced speech, with the creation of\nrealistic pitch contours.", "published": "2018-08-31 11:23:56", "link": "http://arxiv.org/abs/1808.10687v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
