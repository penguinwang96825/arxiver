{"title": "Polyglot: Distributed Word Representations for Multilingual NLP", "abstract": "Distributed word representations (word embeddings) have recently contributed\nto competitive performance in language modeling and several NLP tasks. In this\nwork, we train word embeddings for more than 100 languages using their\ncorresponding Wikipedias. We quantitatively demonstrate the utility of our word\nembeddings by using them as the sole features for training a part of speech\ntagger for a subset of these languages. We find their performance to be\ncompetitive with near state-of-art methods in English, Danish and Swedish.\nMoreover, we investigate the semantic features captured by these embeddings\nthrough the proximity of word groupings. We will release these embeddings\npublicly to help researchers in the development and enhancement of multilingual\napplications.", "published": "2013-07-05 16:52:09", "link": "http://arxiv.org/abs/1307.1662v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
