{"title": "Multi-Pair Text Style Transfer on Unbalanced Data", "abstract": "Text-style transfer aims to convert text given in one domain into another by\nparaphrasing the sentence or substituting the keywords without altering the\ncontent. By necessity, state-of-the-art methods have evolved to accommodate\nnonparallel training data, as it is frequently the case there are multiple data\nsources of unequal size, with a mixture of labeled and unlabeled sentences.\nMoreover, the inherent style defined within each source might be distinct. A\ngeneric bidirectional (e.g., formal $\\Leftrightarrow$ informal) style transfer\nregardless of different groups may not generalize well to different\napplications. In this work, we developed a task adaptive meta-learning\nframework that can simultaneously perform a multi-pair text-style transfer\nusing a single model. The proposed method can adaptively balance the difference\nof meta-knowledge across multiple tasks. Results show that our method leads to\nbetter quantitative performance as well as coherent style variations. Common\nchallenges of unbalanced data and mismatched domains are handled well by this\nmethod.", "published": "2021-06-20 03:20:43", "link": "http://arxiv.org/abs/2106.10608v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Brief Study on the Effects of Training Generative Dialogue Models with\n  a Semantic loss", "abstract": "Neural models trained for next utterance generation in dialogue task learn to\nmimic the n-gram sequences in the training set with training objectives like\nnegative log-likelihood (NLL) or cross-entropy. Such commonly used training\nobjectives do not foster generating alternate responses to a context. But, the\neffects of minimizing an alternate training objective that fosters a model to\ngenerate alternate response and score it on semantic similarity has not been\nwell studied. We hypothesize that a language generation model can improve on\nits diversity by learning to generate alternate text during training and\nminimizing a semantic loss as an auxiliary objective. We explore this idea on\ntwo different sized data sets on the task of next utterance generation in goal\noriented dialogues. We make two observations (1) minimizing a semantic\nobjective improved diversity in responses in the smaller data set (Frames) but\nonly as-good-as minimizing the NLL in the larger data set (MultiWoZ) (2) large\nlanguage model embeddings can be more useful as a semantic loss objective than\nas initialization for token embeddings.", "published": "2021-06-20 04:39:29", "link": "http://arxiv.org/abs/2106.10619v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do Encoder Representations of Generative Dialogue Models Encode\n  Sufficient Information about the Task ?", "abstract": "Predicting the next utterance in dialogue is contingent on encoding of users'\ninput text to generate appropriate and relevant response in data-driven\napproaches. Although the semantic and syntactic quality of the language\ngenerated is evaluated, more often than not, the encoded representation of\ninput is not evaluated. As the representation of the encoder is essential for\npredicting the appropriate response, evaluation of encoder representation is a\nchallenging yet important problem. In this work, we showcase evaluating the\ntext generated through human or automatic metrics is not sufficient to\nappropriately evaluate soundness of the language understanding of dialogue\nmodels and, to that end, propose a set of probe tasks to evaluate encoder\nrepresentation of different language encoders commonly used in dialogue models.\nFrom experiments, we observe that some of the probe tasks are easier and some\nare harder for even sophisticated model architectures to learn. And, through\nexperiments we observe that RNN based architectures have lower performance on\nautomatic metrics on text generation than transformer model but perform better\nthan the transformer model on the probe tasks indicating that RNNs might\npreserve task information better than the Transformers.", "published": "2021-06-20 04:52:37", "link": "http://arxiv.org/abs/2106.10622v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CPM-2: Large-scale Cost-effective Pre-trained Language Models", "abstract": "In recent years, the size of pre-trained language models (PLMs) has grown by\nleaps and bounds. However, efficiency issues of these large-scale PLMs limit\ntheir utilization in real-world scenarios. We present a suite of cost-effective\ntechniques for the use of PLMs to deal with the efficiency issues of\npre-training, fine-tuning, and inference. (1) We introduce knowledge\ninheritance to accelerate the pre-training process by exploiting existing PLMs\ninstead of training models from scratch. (2) We explore the best practice of\nprompt tuning with large-scale PLMs. Compared with conventional fine-tuning,\nprompt tuning significantly reduces the number of task-specific parameters. (3)\nWe implement a new inference toolkit, namely InfMoE, for using large-scale PLMs\nwith limited computational resources. Based on our cost-effective pipeline, we\npre-train two models: an encoder-decoder bilingual model with 11 billion\nparameters (CPM-2) and its corresponding MoE version with 198 billion\nparameters. In our experiments, we compare CPM-2 with mT5 on downstream tasks.\nExperimental results show that CPM-2 has excellent general language\nintelligence. Moreover, we validate the efficiency of InfMoE when conducting\ninference of large-scale models having tens of billions of parameters on a\nsingle GPU. All source code and model parameters are available at\nhttps://github.com/TsinghuaAI/CPM.", "published": "2021-06-20 15:43:54", "link": "http://arxiv.org/abs/2106.10715v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges in Translation of Emotions in Multilingual User-Generated\n  Content: Twitter as a Case Study", "abstract": "Although emotions are universal concepts, transferring the different shades\nof emotion from one language to another may not always be straightforward for\nhuman translators, let alone for machine translation systems. Moreover, the\ncognitive states are established by verbal explanations of experience which is\nshaped by both the verbal and cultural contexts. There are a number of verbal\ncontexts where expression of emotions constitutes the pivotal component of the\nmessage. This is particularly true for User-Generated Content (UGC) which can\nbe in the form of a review of a product or a service, a tweet, or a social\nmedia post. Recently, it has become common practice for multilingual websites\nsuch as Twitter to provide an automatic translation of UGC to reach out to\ntheir linguistically diverse users. In such scenarios, the process of\ntranslating the user's emotion is entirely automatic with no human\nintervention, neither for post-editing nor for accuracy checking. In this\nresearch, we assess whether automatic translation tools can be a successful\nreal-life utility in transferring emotion in user-generated multilingual data\nsuch as tweets. We show that there are linguistic phenomena specific of Twitter\ndata that pose a challenge in translation of emotions in different languages.\nWe summarise these challenges in a list of linguistic features and show how\nfrequent these features are in different language pairs. We also assess the\ncapacity of commonly used methods for evaluating the performance of an MT\nsystem with respect to the preservation of emotion in the source text.", "published": "2021-06-20 16:12:48", "link": "http://arxiv.org/abs/2106.10719v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Calliar: An Online Handwritten Dataset for Arabic Calligraphy", "abstract": "Calligraphy is an essential part of the Arabic heritage and culture. It has\nbeen used in the past for the decoration of houses and mosques. Usually, such\ncalligraphy is designed manually by experts with aesthetic insights. In the\npast few years, there has been a considerable effort to digitize such type of\nart by either taking a photo of decorated buildings or drawing them using\ndigital devices. The latter is considered an online form where the drawing is\ntracked by recording the apparatus movement, an electronic pen for instance, on\na screen. In the literature, there are many offline datasets collected with a\ndiversity of Arabic styles for calligraphy. However, there is no available\nonline dataset for Arabic calligraphy. In this paper, we illustrate our\napproach for the collection and annotation of an online dataset for Arabic\ncalligraphy called Calliar that consists of 2,500 sentences. Calliar is\nannotated for stroke, character, word and sentence level prediction.", "published": "2021-06-20 20:04:51", "link": "http://arxiv.org/abs/2106.10745v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Context-Aware Legal Citation Recommendation using Deep Learning", "abstract": "Lawyers and judges spend a large amount of time researching the proper legal\nauthority to cite while drafting decisions. In this paper, we develop a\ncitation recommendation tool that can help improve efficiency in the process of\nopinion drafting. We train four types of machine learning models, including a\ncitation-list based method (collaborative filtering) and three context-based\nmethods (text similarity, BiLSTM and RoBERTa classifiers). Our experiments show\nthat leveraging local textual context improves recommendation, and that deep\nneural models achieve decent performance. We show that non-deep text-based\nmethods benefit from access to structured case metadata, but deep models only\nbenefit from such access when predicting from context of insufficient length.\nWe also find that, even after extensive training, RoBERTa does not outperform a\nrecurrent neural model, despite its benefits of pretraining. Our behavior\nanalysis of the RoBERTa model further shows that predictive performance is\nstable across time and citation classes.", "published": "2021-06-20 23:23:11", "link": "http://arxiv.org/abs/2106.10776v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Encoder-Decoder Based Attractors for End-to-End Neural Diarization", "abstract": "This paper investigates an end-to-end neural diarization (EEND) method for an\nunknown number of speakers. In contrast to the conventional cascaded approach\nto speaker diarization, EEND methods are better in terms of speaker overlap\nhandling. However, EEND still has a disadvantage in that it cannot deal with a\nflexible number of speakers. To remedy this problem, we introduce\nencoder-decoder-based attractor calculation module (EDA) to EEND. Once\nframe-wise embeddings are obtained, EDA sequentially generates speaker-wise\nattractors on the basis of a sequence-to-sequence method using an LSTM\nencoder-decoder. The attractor generation continues until a stopping condition\nis satisfied; thus, the number of attractors can be flexible. Diarization\nresults are then estimated as dot products of the attractors and embeddings.\nThe embeddings from speaker overlaps result in larger dot product values with\nmultiple attractors; thus, this method can deal with speaker overlaps. Because\nthe maximum number of output speakers is still limited by the training set, we\nalso propose an iterative inference method to remove this restriction. Further,\nwe propose a method that aligns the estimated diarization results with the\nresults of an external speech activity detector, which enables fair comparison\nagainst cascaded approaches. Extensive evaluations on simulated and real\ndatasets show that EEND-EDA outperforms the conventional cascaded approach.", "published": "2021-06-20 08:46:12", "link": "http://arxiv.org/abs/2106.10654v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving Ultrasound Tongue Image Reconstruction from Lip Images Using\n  Self-supervised Learning and Attention Mechanism", "abstract": "Speech production is a dynamic procedure, which involved multi human organs\nincluding the tongue, jaw and lips. Modeling the dynamics of the vocal tract\ndeformation is a fundamental problem to understand the speech, which is the\nmost common way for human daily communication. Researchers employ several\nsensory streams to describe the process simultaneously, which are\nincontrovertibly statistically related to other streams. In this paper, we\naddress the following question: given an observable image sequences of lips,\ncan we picture the corresponding tongue motion. We formulated this problem as\nthe self-supervised learning problem, and employ the two-stream convolutional\nnetwork and long-short memory network for the learning task, with the attention\nmechanism. We evaluate the performance of the proposed method by leveraging the\nunlabeled lip videos to predict an upcoming ultrasound tongue image sequence.\nThe results show that our model is able to generate images that close to the\nreal ultrasound tongue images, and results in the matching between two imaging\nmodalities.", "published": "2021-06-20 10:51:23", "link": "http://arxiv.org/abs/2106.11769v1", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD", "eess.IV"], "primary_category": "eess.AS"}
