{"title": "Code Generation for Unknown Libraries via Reading API Documentations", "abstract": "Open-domain code generation is a challenging problem because the set of\nfunctions and classes that we use are frequently changed and extended in\nprogramming communities. We consider the challenge of code generation for\nunknown libraries without additional training. In this paper, we explore a\nframework of code generation that can refer to relevant API documentations like\nhuman programmers to handle unknown libraries. As a first step of this\ndirection, we implement a model that can extract relevant code signatures from\nAPI documentations based on a natural language intent and copy primitives from\nthe extracted signatures. Moreover, to evaluate code generation for unknown\nlibraries and our framework, we extend an existing dataset of open-domain code\ngeneration and resplit it so that the evaluation data consist of only examples\nusing the libraries that do not appear in the training data. Experiments on our\nnew split show that baseline encoder-decoder models cannot generate code using\nprimitives of unknown libraries as expected. In contrast, our model outperforms\nthe baseline on the new split and can properly generate unknown primitives when\nextracted code signatures are noiseless.", "published": "2022-02-16 00:36:33", "link": "http://arxiv.org/abs/2202.07806v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\rm{C {\\small IS}}^2$: A Simplified Commonsense Inference Evaluation\n  for Story Prose", "abstract": "Transformers have been showing near-human performance on a variety of tasks,\nbut they are not without their limitations. We discuss the issue of conflating\nresults of transformers that are instructed to do multiple tasks\nsimultaneously. In particular, we focus on the domain of commonsense reasoning\nwithin story prose, which we call contextual commonsense inference (CCI). We\nlook at the GLUCOSE (Mostafazadeh et al. 2020) dataset and task for predicting\nimplicit commonsense inferences between story sentences. Since the GLUCOSE task\nsimultaneously generates sentences and predicts the CCI relation, there is a\nconflation in the results. Is the model really measuring CCI or is its ability\nto generate grammatical text carrying the results? In this paper, we introduce\nthe task contextual commonsense inference in sentence selection ($\\rm{C {\\small\nIS}}^2$), a simplified task that avoids conflation by eliminating language\ngeneration altogether. Our findings emphasize the necessity of future work to\ndisentangle language generation from the desired NLP tasks at hand.", "published": "2022-02-16 06:14:37", "link": "http://arxiv.org/abs/2202.07880v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq\n  Generation", "abstract": "We introduce EdgeFormer -- a parameter-efficient Transformer for on-device\nseq2seq generation under the strict computation and memory constraints.\nCompared with the previous parameter-efficient Transformers, EdgeFormer applies\ntwo novel principles for cost-effective parameterization, allowing it to\nperform better given the same parameter budget; moreover, EdgeFormer is further\nenhanced by layer adaptation innovation that is proposed for improving the\nnetwork with shared layers.\n  Extensive experiments show EdgeFormer can effectively outperform previous\nparameter-efficient Transformer baselines and achieve competitive results under\nboth the computation and memory constraints. Given the promising results, we\nrelease EdgeLM -- the pretrained version of EdgeFormer, which is the first\npublicly available pretrained on-device seq2seq model that can be easily\nfine-tuned for seq2seq tasks with strong results, facilitating on-device\nseq2seq generation in practice.", "published": "2022-02-16 10:10:00", "link": "http://arxiv.org/abs/2202.07959v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Parameter-Efficient Tuning: Are We Really There Yet?", "abstract": "Parameter-Efficient Tuning (PETuning) methods have been deemed by many as the\nnew paradigm for using pretrained language models (PLMs). By tuning just a\nfraction amount of parameters comparing to full model finetuning, PETuning\nmethods claim to have achieved performance on par with or even better than\nfinetuning. In this work, we take a step back and re-examine these PETuning\nmethods by conducting the first comprehensive investigation into the training\nand evaluation of them. We found the problematic validation and testing\npractice in current studies, when accompanied by the instability nature of\nPETuning methods, has led to unreliable conclusions. When being compared under\na truly fair evaluation protocol, PETuning cannot yield consistently\ncompetitive performance while finetuning remains to be the best-performing\nmethod in medium- and high-resource settings. We delve deeper into the cause of\nthe instability and observed that the number of trainable parameters and\ntraining iterations are two main factors: reducing trainable parameters and\nprolonging training iterations may lead to higher stability in PETuning\nmethods.", "published": "2022-02-16 10:11:19", "link": "http://arxiv.org/abs/2202.07962v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and\n  Benchmarks", "abstract": "The research of open-domain dialog systems has been greatly prospered by\nneural models trained on large-scale corpora, however, such corpora often\nintroduce various safety problems (e.g., offensive languages, biases, and toxic\nbehaviors) that significantly hinder the deployment of dialog systems in\npractice. Among all these unsafe issues, addressing social bias is more complex\nas its negative impact on marginalized populations is usually expressed\nimplicitly, thus requiring normative reasoning and rigorous analysis. In this\npaper, we focus our investigation on social bias detection of dialog safety\nproblems. We first propose a novel Dial-Bias Frame for analyzing the social\nbias in conversations pragmatically, which considers more comprehensive\nbias-related analyses rather than simple dichotomy annotations. Based on the\nproposed framework, we further introduce CDail-Bias Dataset that, to our\nknowledge, is the first well-annotated Chinese social bias dialog dataset. In\naddition, we establish several dialog bias detection benchmarks at different\nlabel granularities and input types (utterance-level and context-level). We\nshow that the proposed in-depth analyses together with these benchmarks in our\nDial-Bias Frame are necessary and essential to bias detection tasks and can\nbenefit building safe dialog systems in practice.", "published": "2022-02-16 11:59:29", "link": "http://arxiv.org/abs/2202.08011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XFBoost: Improving Text Generation with Controllable Decoders", "abstract": "Multimodal conditionality in transformer-based natural language models has\ndemonstrated state-of-the-art performance in the task of product description\ngeneration. Recent approaches condition a language model on one or more images\nand other textual metadata to achieve near-human performance for describing\nproducts from e-commerce stores. However, generated descriptions may exhibit\ndegrees of inaccuracy or even contradictory claims relative to the inputs of a\ngiven product. In this paper, we propose a controllable language generation\nframework called Extract-Finetune-Boost (XFBoost), which addresses the problem\nof inaccurate low-quality inference. By using visual semantic attributes as\nconstraints at the decoding stage of the generation process and finetuning the\nlanguage model with policy gradient techniques, the XFBoost framework is found\nto produce significantly more descriptive text with higher image relevancy,\noutperforming baselines and lowering the frequency of factually inaccurate\ndescriptions. We further demonstrate the application of XFBoost to online\nlearning wherein human-in-the-loop critics improve language models with active\nfeedback.", "published": "2022-02-16 15:00:25", "link": "http://arxiv.org/abs/2202.08124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Processing the structure of documents: Logical Layout Analysis of\n  historical newspapers in French", "abstract": "Background. In recent years, libraries and archives led important\ndigitisation campaigns that opened the access to vast collections of historical\ndocuments. While such documents are often available as XML ALTO documents, they\nlack information about their logical structure. In this paper, we address the\nproblem of Logical Layout Analysis applied to historical documents in French.\nWe propose a rule-based method, that we evaluate and compare with two\nMachine-Learning models, namely RIPPER and Gradient Boosting. Our data set\ncontains French newspapers, periodicals and magazines, published in the first\nhalf of the twentieth century in the Franche-Comt\\'e Region. Results. Our\nrule-based system outperforms the two other models in nearly all evaluations.\nIt has especially better Recall results, indicating that our system covers more\ntypes of every logical label than the other two models. When comparing RIPPER\nwith Gradient Boosting, we can observe that Gradient Boosting has better\nPrecision scores but RIPPER has better Recall scores. Conclusions. The\nevaluation shows that our system outperforms the two Machine Learning models,\nand provides significantly higher Recall. It also confirms that our system can\nbe used to produce annotated data sets that are large enough to envisage\nMachine Learning or Deep Learning approaches for the task of Logical Layout\nAnalysis. Combining rules and Machine Learning models into hybrid systems could\npotentially provide even better performances. Furthermore, as the layout in\nhistorical documents evolves rapidly, one possible solution to overcome this\nproblem would be to apply Rule Learning algorithms to bootstrap rule sets\nadapted to different publication periods.", "published": "2022-02-16 15:05:13", "link": "http://arxiv.org/abs/2202.08125v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FAMIE: A Fast Active Learning Framework for Multilingual Information\n  Extraction", "abstract": "This paper presents FAMIE, a comprehensive and efficient active learning (AL)\ntoolkit for multilingual information extraction. FAMIE is designed to address a\nfundamental problem in existing AL frameworks where annotators need to wait for\na long time between annotation batches due to the time-consuming nature of\nmodel training and data selection at each AL iteration. This hinders the\nengagement, productivity, and efficiency of annotators. Based on the idea of\nusing a small proxy network for fast data selection, we introduce a novel\nknowledge distillation mechanism to synchronize the proxy network with the main\nlarge model (i.e., BERT-based) to ensure the appropriateness of the selected\nannotation examples for the main model. Our AL framework can support multiple\nlanguages. The experiments demonstrate the advantages of FAMIE in terms of\ncompetitive performance and time efficiency for sequence labeling with AL. We\npublicly release our code (\\url{https://github.com/nlp-uoregon/famie}) and demo\nwebsite (\\url{http://nlp.uoregon.edu:9000/}). A demo video for FAMIE is\nprovided at: \\url{https://youtu.be/I2i8n_jAyrY}.", "published": "2022-02-16 20:11:31", "link": "http://arxiv.org/abs/2202.08316v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The NLP Task Effectiveness of Long-Range Transformers", "abstract": "Transformer models cannot easily scale to long sequences due to their O(N^2)\ntime and space complexity. This has led to Transformer variants seeking to\nlower computational complexity, such as Longformer and Performer. While such\nmodels have theoretically greater efficiency, their effectiveness on real NLP\ntasks has not been well studied. We benchmark 7 variants of Transformer models\non 5 difficult NLP tasks and 7 datasets. We design experiments to isolate the\neffect of pretraining and hyperparameter settings, to focus on their capacity\nfor long-range attention. Moreover, we present various methods to investigate\nattention behaviors to illuminate model details beyond metric scores. We find\nthat the modified attention in long-range transformers has advantages on\ncontent selection and query-guided decoding, but they come with previously\nunrecognized drawbacks such as insufficient attention to distant tokens and\naccumulated approximation error.", "published": "2022-02-16 04:39:35", "link": "http://arxiv.org/abs/2202.07856v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ITTC @ TREC 2021 Clinical Trials Track", "abstract": "This paper describes the submissions of the Natural Language Processing (NLP)\nteam from the Australian Research Council Industrial Transformation Training\nCentre (ITTC) for Cognitive Computing in Medical Technologies to the TREC 2021\nClinical Trials Track. The task focuses on the problem of matching eligible\nclinical trials to topics constituting a summary of a patient's admission\nnotes. We explore different ways of representing trials and topics using NLP\ntechniques, and then use a common retrieval model to generate the ranked list\nof relevant trials for each topic. The results from all our submitted runs are\nwell above the median scores for all topics, but there is still plenty of scope\nfor improvement.", "published": "2022-02-16 04:56:47", "link": "http://arxiv.org/abs/2202.07858v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "ZeroGen: Efficient Zero-shot Learning via Dataset Generation", "abstract": "There is a growing interest in dataset generation recently due to the\nsuperior generative capacity of large pre-trained language models (PLMs). In\nthis paper, we study a flexible and efficient zero-short learning method,\n\\textsc{ZeroGen}. Given a zero-shot task, we first generate a dataset from\nscratch using PLMs in an unsupervised manner. Then, we train a tiny task model\n(e.g., LSTM) under the supervision of the synthesized dataset. This approach\nallows highly efficient inference as the final task model only has orders of\nmagnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being\nannotation-free and efficient, we argue that \\textsc{ZeroGen} can also provide\nuseful insights from the perspective of data-free model-agnostic knowledge\ndistillation, and unreferenced text generation evaluation. Experiments and\nanalysis on different NLP tasks, namely, text classification, question\nanswering, and natural language inference, show the effectiveness of\n\\textsc{ZeroGen}.", "published": "2022-02-16 08:18:02", "link": "http://arxiv.org/abs/2202.07922v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "m-Nearly k-Universal Words -- Investigating Simon Congruence", "abstract": "Determining the index of the Simon congruence is a long outstanding open\nproblem. Two words $u$ and $v$ are called Simon congruent if they have the same\nset of scattered factors, which are parts of the word in the correct order but\nnot necessarily consecutive, e.g., $\\mathtt{oath}$ is a scattered factor of\n$\\mathtt{logarithm}$. Following the idea of scattered factor $k$-universality,\nwe investigate $m$-nearly $k$-universality, i.e., words where $m$ scattered\nfactors of length $k$ are absent, w.r.t. Simon congruence. We present a full\ncharacterisation as well as the index of the congruence for $m=1$. For $m\\neq\n1$, we show some results if in addition $w$ is $(k-1)$-universal as well as\nsome further insights for different $m$.", "published": "2022-02-16 10:45:10", "link": "http://arxiv.org/abs/2202.07981v1", "categories": ["math.CO", "cs.CL", "14J60", "F.2.2; I.2.7"], "primary_category": "math.CO"}
{"title": "On the Self Shuffle Language", "abstract": "The shuffle product \\(u\\shuffle v\\) of two words \\(u\\) and \\(v\\) is the set\nof all words which can be obtained by interleaving \\(u\\) and \\(v\\). Motivated\nby the paper \\emph{The Shuffle Product: New Research Directions} by Restivo\n(2015) we investigate a special case of the shuffle product. In this work we\nconsider the shuffle of a word with itself called the \\emph{self shuffle} or\n\\emph{shuffle square}, showing first that the self shuffle language and the\nshuffle of the language are in general different sets. We prove that the\nlanguage of all words arising as a self shuffle of some word is context\nsensitive but not context free. Furthermore, we show that the self shuffle \\(w\n\\shuffle w\\) uniquely determines \\(w\\).", "published": "2022-02-16 10:59:13", "link": "http://arxiv.org/abs/2202.07988v2", "categories": ["math.CO", "cs.CL", "14J60", "F.2.2; I.2.7"], "primary_category": "math.CO"}
{"title": "Should You Mask 15% in Masked Language Modeling?", "abstract": "Masked language models (MLMs) conventionally mask 15% of tokens due to the\nbelief that more masking would leave insufficient context to learn good\nrepresentations; this masking rate has been widely used, regardless of model\nsizes or masking strategies. In this work, we revisit this important choice of\nMLM pre-training. We first establish that 15% is not universally optimal, and\nlarger models should adopt a higher masking rate. Specifically, we find that\nmasking 40% outperforms 15% for BERT-large size models on GLUE and SQuAD.\nInterestingly, an extremely high masking rate of 80% can still preserve 95%\nfine-tuning performance and most of the accuracy in linguistic probing,\nchallenging the conventional wisdom about the role of the masking rate. We then\nexamine the interplay between masking rates and masking strategies and find\nthat uniform masking requires a higher masking rate compared to sophisticated\nmasking strategies such as span or PMI masking. Finally, we argue that\nincreasing the masking rate has two distinct effects: it leads to more\ncorruption, which makes the prediction task more difficult; it also enables\nmore predictions, which benefits optimization. Using this framework, we revisit\nBERT's 80-10-10 corruption strategy. Together, our results contribute to a\nbetter understanding of MLM pre-training.", "published": "2022-02-16 11:42:34", "link": "http://arxiv.org/abs/2202.08005v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When Did It Happen? Duration-informed Temporal Localization of Narrated\n  Actions in Vlogs", "abstract": "We consider the task of temporal human action localization in lifestyle\nvlogs. We introduce a novel dataset consisting of manual annotations of\ntemporal localization for 13,000 narrated actions in 1,200 video clips. We\npresent an extensive analysis of this data, which allows us to better\nunderstand how the language and visual modalities interact throughout the\nvideos. We propose a simple yet effective method to localize the narrated\nactions based on their expected duration. Through several experiments and\nanalyses, we show that our method brings complementary information with respect\nto previous methods, and leads to improvements over previous work for the task\nof temporal action localization.", "published": "2022-02-16 15:26:12", "link": "http://arxiv.org/abs/2202.08138v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Capitalization Normalization for Language Modeling with an Accurate and\n  Efficient Hierarchical RNN Model", "abstract": "Capitalization normalization (truecasing) is the task of restoring the\ncorrect case (uppercase or lowercase) of noisy text. We propose a fast,\naccurate and compact two-level hierarchical word-and-character-based recurrent\nneural network model. We use the truecaser to normalize user-generated text in\na Federated Learning framework for language modeling. A case-aware language\nmodel trained on this normalized text achieves the same perplexity as a model\ntrained on text with gold capitalization. In a real user A/B experiment, we\ndemonstrate that the improvement translates to reduced prediction error rates\nin a virtual keyboard application. Similarly, in an ASR language model fusion\nexperiment, we show reduction in uppercase character error rate and word error\nrate.", "published": "2022-02-16 16:21:53", "link": "http://arxiv.org/abs/2202.08171v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "GraphNLI: A Graph-based Natural Language Inference Model for Polarity\n  Prediction in Online Debates", "abstract": "Online forums that allow participatory engagement between users have been\ntransformative for public discussion of important issues. However, debates on\nsuch forums can sometimes escalate into full blown exchanges of hate or\nmisinformation. An important tool in understanding and tackling such problems\nis to be able to infer the argumentative relation of whether a reply is\nsupporting or attacking the post it is replying to. This so called polarity\nprediction task is difficult because replies may be based on external context\nbeyond a post and the reply whose polarity is being predicted. We propose\nGraphNLI, a novel graph-based deep learning architecture that uses graph walk\ntechniques to capture the wider context of a discussion thread in a principled\nfashion. Specifically, we propose methods to perform root-seeking graph walks\nthat start from a post and captures its surrounding context to generate\nadditional embeddings for the post. We then use these embeddings to predict the\npolarity relation between a reply and the post it is replying to. We evaluate\nthe performance of our models on a curated debate dataset from Kialo, an online\ndebating platform. Our model outperforms relevant baselines, including S-BERT,\nwith an overall accuracy of 83%.", "published": "2022-02-16 16:26:21", "link": "http://arxiv.org/abs/2202.08175v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Question-Answer Sentence Graph for Joint Modeling Answer Selection", "abstract": "This research studies graph-based approaches for Answer Sentence Selection\n(AS2), an essential component for retrieval-based Question Answering (QA)\nsystems. During offline learning, our model constructs a small-scale relevant\ntraining graph per question in an unsupervised manner, and integrates with\nGraph Neural Networks. Graph nodes are question sentence to answer sentence\npairs. We train and integrate state-of-the-art (SOTA) models for computing\nscores between question-question, question-answer, and answer-answer pairs, and\nuse thresholding on relevance scores for creating graph edges. Online inference\nis then performed to solve the AS2 task on unseen queries. Experiments on two\nwell-known academic benchmarks and a real-world dataset show that our approach\nconsistently outperforms SOTA QA baseline models.", "published": "2022-02-16 05:59:53", "link": "http://arxiv.org/abs/2203.03549v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in\n  Text-to-Speech", "abstract": "Expressive text-to-speech (TTS) has become a hot research topic recently,\nmainly focusing on modeling prosody in speech. Prosody modeling has several\nchallenges: 1) the extracted pitch used in previous prosody modeling works have\ninevitable errors, which hurts the prosody modeling; 2) different attributes of\nprosody (e.g., pitch, duration and energy) are dependent on each other and\nproduce the natural prosody together; and 3) due to high variability of prosody\nand the limited amount of high-quality data for TTS training, the distribution\nof prosody cannot be fully shaped. To tackle these issues, we propose\nProsoSpeech, which enhances the prosody using quantized latent vectors\npre-trained on large-scale unpaired and low-quality text and speech data.\nSpecifically, we first introduce a word-level prosody encoder, which quantizes\nthe low-frequency band of the speech and compresses prosody attributes in the\nlatent prosody vector (LPV). Then we introduce an LPV predictor, which predicts\nLPV given word sequence. We pre-train the LPV predictor on large-scale text and\nlow-quality speech data and fine-tune it on the high-quality TTS dataset.\nFinally, our model can generate expressive speech conditioned on the predicted\nLPV. Experimental results show that ProsoSpeech can generate speech with richer\nprosody compared with baseline methods.", "published": "2022-02-16 01:42:32", "link": "http://arxiv.org/abs/2202.07816v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Conversational Speech Recognition By Learning Conversation-level\n  Characteristics", "abstract": "Conversational automatic speech recognition (ASR) is a task to recognize\nconversational speech including multiple speakers. Unlike sentence-level ASR,\nconversational ASR can naturally take advantages from specific characteristics\nof conversation, such as role preference and topical coherence. This paper\nproposes a conversational ASR model which explicitly learns conversation-level\ncharacteristics under the prevalent end-to-end neural framework. The highlights\nof the proposed model are twofold. First, a latent variational module (LVM) is\nattached to a conformer-based encoder-decoder ASR backbone to learn role\npreference and topical coherence. Second, a topic model is specifically adopted\nto bias the outputs of the decoder to words in the predicted topics.\nExperiments on two Mandarin conversational ASR tasks show that the proposed\nmodel achieves a maximum 12% relative character error rate (CER) reduction.", "published": "2022-02-16 04:33:05", "link": "http://arxiv.org/abs/2202.07855v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge Transfer from Large-scale Pretrained Language Models to\n  End-to-end Speech Recognizers", "abstract": "End-to-end speech recognition is a promising technology for enabling compact\nautomatic speech recognition (ASR) systems since it can unify the acoustic and\nlanguage model into a single neural network. However, as a drawback, training\nof end-to-end speech recognizers always requires transcribed utterances. Since\nend-to-end models are also known to be severely data hungry, this constraint is\ncrucial especially because obtaining transcribed utterances is costly and can\npossibly be impractical or impossible. This paper proposes a method for\nalleviating this issue by transferring knowledge from a language model neural\nnetwork that can be pretrained with text-only data. Specifically, this paper\nattempts to transfer semantic knowledge acquired in embedding vectors of\nlarge-scale language models. Since embedding vectors can be assumed as implicit\nrepresentations of linguistic information such as part-of-speech, intent, and\nso on, those are also expected to be useful modeling cues for ASR decoders.\nThis paper extends two types of ASR decoders, attention-based decoders and\nneural transducers, by modifying training loss functions to include embedding\nprediction terms. The proposed systems were shown to be effective for error\nrate reduction without incurring extra computational costs in the decoding\nphase.", "published": "2022-02-16 07:02:24", "link": "http://arxiv.org/abs/2202.07894v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ADIMA: Abuse Detection In Multilingual Audio", "abstract": "Abusive content detection in spoken text can be addressed by performing\nAutomatic Speech Recognition (ASR) and leveraging advancements in natural\nlanguage processing. However, ASR models introduce latency and often perform\nsub-optimally for profane words as they are underrepresented in training\ncorpora and not spoken clearly or completely. Exploration of this problem\nentirely in the audio domain has largely been limited by the lack of audio\ndatasets. Building on these challenges, we propose ADIMA, a novel,\nlinguistically diverse, ethically sourced, expert annotated and well-balanced\nmultilingual profanity detection audio dataset comprising of 11,775 audio\nsamples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique\nusers. Through quantitative experiments across monolingual and cross-lingual\nzero-shot settings, we take the first step in democratizing audio based content\nmoderation in Indic languages and set forth our dataset to pave future work.", "published": "2022-02-16 11:09:50", "link": "http://arxiv.org/abs/2202.07991v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Decorrelate Irrelevant, Purify Relevant: Overcome Textual Spurious\n  Correlations from a Feature Perspective", "abstract": "Natural language understanding (NLU) models tend to rely on spurious\ncorrelations (i.e., dataset bias) to achieve high performance on\nin-distribution datasets but poor performance on out-of-distribution ones. Most\nof the existing debiasing methods often identify and weaken these samples with\nbiased features (i.e., superficial surface features that cause such spurious\ncorrelations). However, down-weighting these samples obstructs the model in\nlearning from the non-biased parts of these samples. To tackle this challenge,\nin this paper, we propose to eliminate spurious correlations in a fine-grained\nmanner from a feature space perspective. Specifically, we introduce Random\nFourier Features and weighted re-sampling to decorrelate the dependencies\nbetween features to mitigate spurious correlations. After obtaining\ndecorrelated features, we further design a mutual-information-based method to\npurify them, which forces the model to learn features that are more relevant to\ntasks. Extensive experiments on two well-studied NLU tasks demonstrate that our\nmethod is superior to other comparative approaches.", "published": "2022-02-16 13:23:14", "link": "http://arxiv.org/abs/2202.08048v2", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Information Extraction in Low-Resource Scenarios: Survey and Perspective", "abstract": "Information Extraction (IE) seeks to derive structured information from\nunstructured texts, often facing challenges in low-resource scenarios due to\ndata scarcity and unseen classes. This paper presents a review of neural\napproaches to low-resource IE from \\emph{traditional} and \\emph{LLM-based}\nperspectives, systematically categorizing them into a fine-grained taxonomy.\nThen we conduct empirical study on LLM-based methods compared with previous\nstate-of-the-art models, and discover that (1) well-tuned LMs are still\npredominant; (2) tuning open-resource LLMs and ICL with GPT family is promising\nin general; (3) the optimal LLM-based technical solution for low-resource IE\ncan be task-dependent. In addition, we discuss low-resource IE with LLMs,\nhighlight promising applications, and outline potential research directions.\nThis survey aims to foster understanding of this field, inspire new ideas, and\nencourage widespread applications in both academia and industry.", "published": "2022-02-16 13:44:00", "link": "http://arxiv.org/abs/2202.08063v6", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Voice Filter: Few-shot text-to-speech speaker adaptation using voice\n  conversion as a post-processing module", "abstract": "State-of-the-art text-to-speech (TTS) systems require several hours of\nrecorded speech data to generate high-quality synthetic speech. When using\nreduced amounts of training data, standard TTS models suffer from speech\nquality and intelligibility degradations, making training low-resource TTS\nsystems problematic. In this paper, we propose a novel extremely low-resource\nTTS method called Voice Filter that uses as little as one minute of speech from\na target speaker. It uses voice conversion (VC) as a post-processing module\nappended to a pre-existing high-quality TTS system and marks a conceptual shift\nin the existing TTS paradigm, framing the few-shot TTS problem as a VC task.\nFurthermore, we propose to use a duration-controllable TTS system to create a\nparallel speech corpus to facilitate the VC task. Results show that the Voice\nFilter outperforms state-of-the-art few-shot speech synthesis techniques in\nterms of objective and subjective metrics on one minute of speech on a diverse\nset of voices, while being competitive against a TTS model built on 30 times\nmore data.", "published": "2022-02-16 16:12:21", "link": "http://arxiv.org/abs/2202.08164v1", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Probing Pretrained Models of Source Code", "abstract": "Deep learning models are widely used for solving challenging code processing\ntasks, such as code generation or code summarization. Traditionally, a specific\nmodel architecture was carefully built to solve a particular code processing\ntask. However, recently general pretrained models such as CodeBERT or CodeT5\nhave been shown to outperform task-specific models in many applications. While\npretrained models are known to learn complex patterns from data, they may fail\nto understand some properties of source code. To test diverse aspects of code\nunderstanding, we introduce a set of diagnosting probing tasks. We show that\npretrained models of code indeed contain information about code syntactic\nstructure and correctness, the notions of identifiers, data flow and\nnamespaces, and natural language naming. We also investigate how probing\nresults are affected by using code-specific pretraining objectives, varying the\nmodel size, or finetuning.", "published": "2022-02-16 10:26:14", "link": "http://arxiv.org/abs/2202.08975v3", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Learning Deep Direct-Path Relative Transfer Function for Binaural Sound\n  Source Localization", "abstract": "Direct-path relative transfer function (DP-RTF) refers to the ratio between\nthe direct-path acoustic transfer functions of two microphone channels. Though\nDP-RTF fully encodes the sound spatial cues and serves as a reliable\nlocalization feature, it is often erroneously estimated in the presence of\nnoise and reverberation. This paper proposes to learn DP-RTF with deep neural\nnetworks for robust binaural sound source localization. A DP-RTF learning\nnetwork is designed to regress the binaural sensor signals to a real-valued\nrepresentation of DP-RTF. It consists of a branched convolutional neural\nnetwork module to separately extract the inter-channel magnitude and phase\npatterns, and a convolutional recurrent neural network module for joint feature\nlearning. To better explore the speech spectra to aid the DP-RTF estimation, a\nmonaural speech enhancement network is used to recover the direct-path\nspectrograms from the noisy ones. The enhanced spectrograms are stacked onto\nthe noisy spectrograms to act as the input of the DP-RTF learning network. We\ntrain one unique DP-RTF learning network using many different binaural arrays\nto enable the generalization of DP-RTF learning across arrays. This way avoids\ntime-consuming training data collection and network retraining for a new array,\nwhich is very useful in practical application. Experimental results on both\nsimulated and real-world data show the effectiveness of the proposed method for\ndirection of arrival (DOA) estimation in the noisy and reverberant environment,\nand a good generalization ability to unseen binaural arrays.", "published": "2022-02-16 03:30:31", "link": "http://arxiv.org/abs/2202.07841v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SRP-DNN: Learning Direct-Path Phase Difference for Multiple Moving Sound\n  Source Localization", "abstract": "Multiple moving sound source localization in real-world scenarios remains a\nchallenging issue due to interaction between sources, time-varying\ntrajectories, distorted spatial cues, etc. In this work, we propose to use deep\nlearning techniques to learn competing and time-varying direct-path phase\ndifferences for localizing multiple moving sound sources. A causal\nconvolutional recurrent neural network is designed to extract the direct-path\nphase difference sequence from signals of each microphone pair. To avoid the\nassignment ambiguity and the problem of uncertain output-dimension encountered\nwhen simultaneously predicting multiple targets, the learning target is\ndesigned in a weighted sum format, which encodes source activity in the weight\nand direct-path phase differences in the summed value. The learned direct-path\nphase differences for all microphone pairs can be directly used to construct\nthe spatial spectrum according to the formulation of steered response power\n(SRP). This deep neural network (DNN) based SRP method is referred to as\nSRP-DNN. The locations of sources are estimated by iteratively detecting and\nremoving the dominant source from the spatial spectrum, in which way the\ninteraction between sources is reduced. Experimental results on both simulated\nand real-world data show the superiority of the proposed method in the presence\nof noise and reverberation.", "published": "2022-02-16 05:00:22", "link": "http://arxiv.org/abs/2202.07859v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DBT-Net: Dual-branch federative magnitude and phase estimation with\n  attention-in-attention transformer for monaural speech enhancement", "abstract": "The decoupling-style concept begins to ignite in the speech enhancement area,\nwhich decouples the original complex spectrum estimation task into multiple\neasier sub-tasks i.e., magnitude-only recovery and the residual complex\nspectrum estimation)}, resulting in better performance and easier\ninterpretability. In this paper, we propose a dual-branch federative magnitude\nand phase estimation framework, dubbed DBT-Net, for monaural speech\nenhancement, aiming at recovering the coarse- and fine-grained regions of the\noverall spectrum in parallel. From the complementary perspective, the magnitude\nestimation branch is designed to filter out dominant noise components in the\nmagnitude domain, while the complex spectrum purification branch is elaborately\ndesigned to inpaint the missing spectral details and implicitly estimate the\nphase information in the complex-valued spectral domain. To facilitate the\ninformation flow between each branch, interaction modules are introduced to\nleverage features learned from one branch, so as to suppress the undesired\nparts and recover the missing components of the other branch. Instead of\nadopting the conventional RNNs and temporal convolutional networks for sequence\nmodeling, we employ a novel attention-in-attention transformer-based network\nwithin each branch for better feature learning. More specially, it is composed\nof several adaptive spectro-temporal attention transformer-based modules and an\nadaptive hierarchical attention module, aiming to capture long-term\ntime-frequency dependencies and further aggregate intermediate hierarchical\ncontextual information. Comprehensive evaluations on the WSJ0-SI84 +\nDNS-Challenge and VoiceBank + DEMAND dataset demonstrate that the proposed\napproach consistently outperforms previous advanced systems and yields\nstate-of-the-art performance in terms of speech quality and intelligibility.", "published": "2022-02-16 08:44:38", "link": "http://arxiv.org/abs/2202.07931v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Increasing loudness in audio signals: a perceptually motivated approach\n  to preserve audio quality", "abstract": "We present a method to maintain the subjective perception of volume of audio\nsignals and, at the same time, reduce their absolute peak value. We focus on\nachieving this without compromising the perceived audio quality. This is\nspecially useful, for example, to maximize the perceived reproduction level of\nloudspeakers where simply amplifying the signal amplitude, and hence their peak\nvalue, is limited due to already constrained physical designs. In particular,\nwe minimize the absolute peak value subject to a constraint based on auditory\nmasking. This limits the perceptual difference between the original and the\nmodified signals. Moreover, this constraint can be tuned and allows to control\nthe resulting audio quality. We show results comparing loudness and audio\nquality as a function of peak reduction. These results suggest that our method\npresents the best trade-off between loudness and audio quality when compared\nagainst classical methods based on compression and clipping.", "published": "2022-02-16 16:41:07", "link": "http://arxiv.org/abs/2202.08183v1", "categories": ["eess.SP", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Singing-Tacotron: Global duration control attention and dynamic filter\n  for End-to-end singing voice synthesis", "abstract": "End-to-end singing voice synthesis (SVS) is attractive due to the avoidance\nof pre-aligned data. However, the auto learned alignment of singing voice with\nlyrics is difficult to match the duration information in musical score, which\nwill lead to the model instability or even failure to synthesize voice. To\nlearn accurate alignment information automatically, this paper proposes an\nend-to-end SVS framework, named Singing-Tacotron. The main difference between\nthe proposed framework and Tacotron is that the speech can be controlled\nsignificantly by the musical score's duration information. Firstly, we propose\na global duration control attention mechanism for the SVS model. The attention\nmechanism can control each phoneme's duration. Secondly, a duration encoder is\nproposed to learn a set of global transition tokens from the musical score.\nThese transition tokens can help the attention mechanism decide whether moving\nto the next phoneme or staying at each decoding step. Thirdly, to further\nimprove the model's stability, a dynamic filter is designed to help the model\novercome noise interference and pay more attention to local context\ninformation. Subjective and objective evaluation verify the effectiveness of\nthe method. Furthermore, the role of global transition tokens and the effect of\nduration control are explored. Examples of experiments can be found at\nhttps://hairuo55.github.io/SingingTacotron.", "published": "2022-02-16 07:35:17", "link": "http://arxiv.org/abs/2202.07907v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On loss functions and evaluation metrics for music source separation", "abstract": "We investigate which loss functions provide better separations via\nbenchmarking an extensive set of those for music source separation. To that\nend, we first survey the most representative audio source separation losses we\nidentified, to later consistently benchmark them in a controlled experimental\nsetup. We also explore using such losses as evaluation metrics, via\ncross-correlating them with the results of a subjective test. Based on the\nobservation that the standard signal-to-distortion ratio metric can be\nmisleading in some scenarios, we study alternative evaluation metrics based on\nthe considered losses.", "published": "2022-02-16 10:22:40", "link": "http://arxiv.org/abs/2202.07968v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "APPLADE: Adjustable Plug-and-play Audio Declipper Combining DNN with\n  Sparse Optimization", "abstract": "In this paper, we propose an audio declipping method that takes advantages of\nboth sparse optimization and deep learning. Since sparsity-based audio\ndeclipping methods have been developed upon constrained optimization, they are\nadjustable and well-studied in theory. However, they always uniformly promote\nsparsity and ignore the individual properties of a signal. Deep neural network\n(DNN)-based methods can learn the properties of target signals and use them for\naudio declipping. Still, they cannot perform well if the training data have\nmismatches and/or constraints in the time domain are not imposed. In the\nproposed method, we use a DNN in an optimization algorithm. It is inspired by\nan idea called plug-and-play (PnP) and enables us to promote sparsity based on\nthe learned information of data, considering constraints in the time domain.\nOur experiments confirmed that the proposed method is stable and robust to\nmismatches between training and test data.", "published": "2022-02-16 12:44:01", "link": "http://arxiv.org/abs/2202.08028v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Multimodal Emotion Recognition using Transfer Learning from Speaker\n  Recognition and BERT-based models", "abstract": "Automatic emotion recognition plays a key role in computer-human interaction\nas it has the potential to enrich the next-generation artificial intelligence\nwith emotional intelligence. It finds applications in customer and/or\nrepresentative behavior analysis in call centers, gaming, personal assistants,\nand social robots, to mention a few. Therefore, there has been an increasing\ndemand to develop robust automatic methods to analyze and recognize the various\nemotions. In this paper, we propose a neural network-based emotion recognition\nframework that uses a late fusion of transfer-learned and fine-tuned models\nfrom speech and text modalities. More specifically, we i) adapt a residual\nnetwork (ResNet) based model trained on a large-scale speaker recognition task\nusing transfer learning along with a spectrogram augmentation approach to\nrecognize emotions from speech, and ii) use a fine-tuned bidirectional encoder\nrepresentations from transformers (BERT) based model to represent and recognize\nemotions from the text. The proposed system then combines the ResNet and\nBERT-based model scores using a late fusion strategy to further improve the\nemotion recognition performance. The proposed multimodal solution addresses the\ndata scarcity limitation in emotion recognition using transfer learning, data\naugmentation, and fine-tuning, thereby improving the generalization performance\nof the emotion recognition models. We evaluate the effectiveness of our\nproposed multimodal approach on the interactive emotional dyadic motion capture\n(IEMOCAP) dataset. Experimental results indicate that both audio and text-based\nmodels improve the emotion recognition performance and that the proposed\nmultimodal solution achieves state-of-the-art results on the IEMOCAP benchmark.", "published": "2022-02-16 00:23:42", "link": "http://arxiv.org/abs/2202.08974v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
