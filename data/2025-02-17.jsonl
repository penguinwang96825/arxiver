{"title": "Parameterised algorithms for temporal reconfiguration problems", "abstract": "Given a static vertex-selection problem (e.g. independent set, dominating\nset) on a graph, we can define a corresponding temporal reconfiguration problem\non a temporal graph which asks for a sequence of solutions to the\nvertex-selection problem at each time such that we can reconfigure from one\nsolution to the next. We can think of each solution in the sequence as a set of\nvertices with tokens placed on them; our reconfiguration model allows us to\nslide tokens along active edges of a temporal graph.\n  We show that it is possible to efficiently check whether one solution can be\nreconfigured to another, and show that approximation results on the static\nvertex-selection problem can be adapted with a lifetime factor to the\nreconfiguration version. Our main contributions are fixed-parameter tractable\nalgorithms with respect to: enumeration time of the related static problem; the\ncombination of temporal neighbourhood diversity and lifetime of the input\ngraph; and the combination of lifetime and treewidth of the footprint graph.", "published": "2025-02-17 16:09:00", "link": "http://arxiv.org/abs/2502.11961v1", "categories": ["cs.DS", "cs.DM", "math.CO"], "primary_category": "cs.DS"}
{"title": "On a tree-based variant of bandwidth and forbidding simple topological minors", "abstract": "We obtain structure theorems for graphs excluding a fan (a path with a\nuniversal vertex) or a dipole ($K_{2,k}$) as a topological minor. The\ncorresponding decompositions can be computed in FPT linear time. This is\nmotivated by the study of a graph parameter we call treebandwidth which extends\nthe graph parameter bandwidth by replacing the linear layout by a rooted tree\nsuch that neighbours in the graph are in ancestor-descendant relation in the\ntree.\n  We deduce an approximation algorithm for treebandwidth running in FPT linear\ntime from our structure theorems. We complement this result with a precise\ncharacterisation of the parameterised complexity of computing the parameter\nexactly.", "published": "2025-02-17 11:07:14", "link": "http://arxiv.org/abs/2502.11674v1", "categories": ["cs.DM", "cs.CC", "cs.DS", "math.CO"], "primary_category": "cs.DM"}
{"title": "On rigid regular graphs and a problem of Babai and Pultr", "abstract": "A graph is \\textit{rigid} if it only admits the identity endomorphism. We\nshow that for every $d\\ge 3$ there exist infinitely many mutually rigid\n$d$-regular graphs of arbitrary odd girth $g\\geq 7$. Moreover, we determine the\nminimum order of a rigid $d$-regular graph for every $d\\ge 3$. This provides\nstrong positive answers to a question of van der Zypen\n[https://mathoverflow.net/q/296483, https://mathoverflow.net/q/321108].\nFurther, we use our construction to show that every finite monoid is isomorphic\nto the endomorphism monoid of a regular graph. This solves a problem of Babai\nand Pultr [J. Comb.~Theory, Ser.~B, 1980].", "published": "2025-02-17 04:21:03", "link": "http://arxiv.org/abs/2502.11421v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "A deep BSDE approach for the simultaneous pricing and delta-gamma hedging of large portfolios consisting of high-dimensional multi-asset Bermudan options", "abstract": "A deep BSDE approach is presented for the pricing and delta-gamma hedging of\nhigh-dimensional Bermudan options, with applications in portfolio risk\nmanagement. Large portfolios of a mixture of multi-asset European and Bermudan\nderivatives are cast into the framework of discretely reflected BSDEs. This\nsystem is discretized by the One Step Malliavin scheme (Negyesi et al. [2024,\n2025]) of discretely reflected Markovian BSDEs, which involves a $\\Gamma$\nprocess, corresponding to second-order sensitivities of the associated option\nprices. The discretized system is solved by a neural network regression Monte\nCarlo method, efficiently for a large number of underlyings. The resulting\noption Deltas and Gammas are used to discretely rebalance the corresponding\nreplicating strategies. Numerical experiments are presented on both\nhigh-dimensional basket options and large portfolios consisting of multiple\noptions with varying early exercise rights, moneyness and volatility. These\nexamples demonstrate the robustness and accuracy of the method up to $100$ risk\nfactors. The resulting hedging strategies significantly outperform benchmark\nmethods both in the case of standard delta- and delta-gamma hedging.", "published": "2025-02-17 11:46:40", "link": "http://arxiv.org/abs/2502.11706v1", "categories": ["q-fin.CP", "q-fin.RM", "91G20, 68T07, 91G60, 65C30"], "primary_category": "q-fin.CP"}
{"title": "A Cholesky decomposition-based asset selection heuristic for sparse tangent portfolio optimization", "abstract": "In practice, including large number of assets in mean-variance portfolios can\nlead to higher transaction costs and management fees. To address this, one\ncommon approach is to select a smaller subset of assets from the larger pool,\nconstructing more efficient portfolios. As a solution, we propose a new asset\nselection heuristic which generates a pre-defined list of asset candidates\nusing a surrogate formulation and re-optimizes the cardinality-constrained\ntangent portfolio with these selected assets. This method enables faster\noptimization and effectively constructs portfolios with fewer assets, as\ndemonstrated by numerical analyses on historical stock returns. Finally, we\ndiscuss a quantitative metric that can provide a initial assessment of the\nperformance of the proposed heuristic based on asset covariance.", "published": "2025-02-17 11:39:50", "link": "http://arxiv.org/abs/2502.11701v1", "categories": ["q-fin.MF", "q-fin.PM"], "primary_category": "q-fin.MF"}
{"title": "Market-Derived Financial Sentiment Analysis: Context-Aware Language Models for Crypto Forecasting", "abstract": "Financial Sentiment Analysis (FSA) traditionally relies on human-annotated\nsentiment labels to infer investor sentiment and forecast market movements.\nHowever, inferring the potential market impact of words based on their\nhuman-perceived intentions is inherently challenging. We hypothesize that the\nhistorical market reactions to words, offer a more reliable indicator of their\npotential impact on markets than subjective sentiment interpretations by human\nannotators. To test this hypothesis, a market-derived labeling approach is\nproposed to assign tweet labels based on ensuing short-term price trends,\nenabling the language model to capture the relationship between textual signals\nand market dynamics directly. A domain-specific language model was fine-tuned\non these labels, achieving up to an 11% improvement in short-term trend\nprediction accuracy over traditional sentiment-based benchmarks. Moreover, by\nincorporating market and temporal context through prompt-tuning, the proposed\ncontext-aware language model demonstrated an accuracy of 89.6% on a curated\ndataset of 227 impactful Bitcoin-related news events with significant market\nimpacts. Aggregating daily tweet predictions into trading signals, our method\noutperformed traditional fusion models (which combine sentiment-based and\nprice-based predictions). It challenged the assumption that sentiment-based\nsignals are inferior to price-based predictions in forecasting market\nmovements. Backtesting these signals across three distinct market regimes\nyielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in\nneutral markets. Our findings demonstrate that language models can serve as\neffective short-term market predictors. This paradigm shift underscores the\nuntapped capabilities of language models in financial decision-making and opens\nnew avenues for market prediction applications.", "published": "2025-02-17 21:35:18", "link": "http://arxiv.org/abs/2502.14897v2", "categories": ["cs.CE", "cs.CL", "cs.LG", "q-fin.ST", "68T50", "H.3.1; I.2.7; J.1"], "primary_category": "cs.CE"}
{"title": "FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading", "abstract": "Large language models (LLMs) fine-tuned on multimodal financial data have\ndemonstrated impressive reasoning capabilities in various financial tasks.\nHowever, they often struggle with multi-step, goal-oriented scenarios in\ninteractive financial markets, such as trading, where complex agentic\napproaches are required to improve decision-making. To address this, we propose\n\\textsc{FLAG-Trader}, a unified architecture integrating linguistic processing\n(via LLMs) with gradient-driven reinforcement learning (RL) policy\noptimization, in which a partially fine-tuned LLM acts as the policy network,\nleveraging pre-trained knowledge while adapting to the financial domain through\nparameter-efficient fine-tuning. Through policy gradient optimization driven by\ntrading rewards, our framework not only enhances LLM performance in trading but\nalso improves results on other financial-domain tasks. We present extensive\nempirical evidence to validate these enhancements.", "published": "2025-02-17 04:45:53", "link": "http://arxiv.org/abs/2502.11433v3", "categories": ["cs.AI", "cs.CE", "q-fin.TR"], "primary_category": "cs.AI"}
{"title": "HedgeAgents: A Balanced-aware Multi-agent Financial Trading System", "abstract": "As automated trading gains traction in the financial market, algorithmic\ninvestment strategies are increasingly prominent. While Large Language Models\n(LLMs) and Agent-based models exhibit promising potential in real-time market\nanalysis and trading decisions, they still experience a significant -20% loss\nwhen confronted with rapid declines or frequent fluctuations, impeding their\npractical application. Hence, there is an imperative to explore a more robust\nand resilient framework. This paper introduces an innovative multi-agent\nsystem, HedgeAgents, aimed at bolstering system robustness via ``hedging''\nstrategies. In this well-balanced system, an array of hedging agents has been\ntailored, where HedgeAgents consist of a central fund manager and multiple\nhedging experts specializing in various financial asset classes. These agents\nleverage LLMs' cognitive capabilities to make decisions and coordinate through\nthree types of conferences. Benefiting from the powerful understanding of LLMs,\nour HedgeAgents attained a 70% annualized return and a 400% total return over a\nperiod of 3 years. Moreover, we have observed with delight that HedgeAgents can\neven formulate investment experience comparable to those of human experts\n(https://hedgeagents.github.io/).", "published": "2025-02-17 04:13:19", "link": "http://arxiv.org/abs/2502.13165v1", "categories": ["cs.MA", "cs.AI", "q-fin.TR"], "primary_category": "cs.MA"}
{"title": "ExaGPT: Example-Based Machine-Generated Text Detection for Human\n  Interpretability", "abstract": "Detecting texts generated by Large Language Models (LLMs) could cause grave\nmistakes due to incorrect decisions, such as undermining student's academic\ndignity. LLM text detection thus needs to ensure the interpretability of the\ndecision, which can help users judge how reliably correct its prediction is.\nWhen humans verify whether a text is human-written or LLM-generated, they\nintuitively investigate with which of them it shares more similar spans.\nHowever, existing interpretable detectors are not aligned with the human\ndecision-making process and fail to offer evidence that users easily\nunderstand. To bridge this gap, we introduce ExaGPT, an interpretable detection\napproach grounded in the human decision-making process for verifying the origin\nof a text. ExaGPT identifies a text by checking whether it shares more similar\nspans with human-written vs. with LLM-generated texts from a datastore. This\napproach can provide similar span examples that contribute to the decision for\neach span in the text as evidence. Our human evaluation demonstrates that\nproviding similar span examples contributes more effectively to judging the\ncorrectness of the decision than existing interpretable methods. Moreover,\nextensive experiments in four domains and three generators show that ExaGPT\nmassively outperforms prior powerful detectors by up to +40.9 points of\naccuracy at a false positive rate of 1%.", "published": "2025-02-17 01:15:07", "link": "http://arxiv.org/abs/2502.11336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hierarchical Graph Topic Modeling with Topic Tree-based Transformer", "abstract": "Textual documents are commonly connected in a hierarchical graph structure\nwhere a central document links to others with an exponentially growing\nconnectivity. Though Hyperbolic Graph Neural Networks (HGNNs) excel at\ncapturing such graph hierarchy, they cannot model the rich textual semantics\nwithin documents. Moreover, text contents in documents usually discuss topics\nof different specificity. Hierarchical Topic Models (HTMs) discover such latent\ntopic hierarchy within text corpora. However, most of them focus on the textual\ncontent within documents, and ignore the graph adjacency across interlinked\ndocuments. We thus propose a Hierarchical Graph Topic Modeling Transformer to\nintegrate both topic hierarchy within documents and graph hierarchy across\ndocuments into a unified Transformer. Specifically, to incorporate topic\nhierarchy within documents, we design a topic tree and infer a hierarchical\ntree embedding for hierarchical topic modeling. To preserve both topic and\ngraph hierarchies, we design our model in hyperbolic space and propose\nHyperbolic Doubly Recurrent Neural Network, which models ancestral and\nfraternal tree structure. Both hierarchies are inserted into each Transformer\nlayer to learn unified representations. Both supervised and unsupervised\nexperiments verify the effectiveness of our model.", "published": "2025-02-17 01:55:29", "link": "http://arxiv.org/abs/2502.11345v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VLDBench: Vision Language Models Disinformation Detection Benchmark", "abstract": "The rapid rise of AI-generated content has made detecting disinformation\nincreasingly challenging. In particular, multimodal disinformation, i.e.,\nonline posts-articles that contain images and texts with fabricated information\nare specially designed to deceive. While existing AI safety benchmarks\nprimarily address bias and toxicity, multimodal disinformation detection\nremains largely underexplored. To address this challenge, we present the\nVision-Language Disinformation Detection Benchmark VLDBench, the first\ncomprehensive benchmark for detecting disinformation across both unimodal\n(text-only) and multimodal (text and image) content, comprising 31,000} news\narticle-image pairs, spanning 13 distinct categories, for robust evaluation.\nVLDBench features a rigorous semi-automated data curation pipeline, with 22\ndomain experts dedicating 300 plus hours} to annotation, achieving a strong\ninter-annotator agreement (Cohen kappa = 0.78). We extensively evaluate\nstate-of-the-art Large Language Models (LLMs) and Vision-Language Models\n(VLMs), demonstrating that integrating textual and visual cues in multimodal\nnews posts improves disinformation detection accuracy by 5 - 35 % compared to\nunimodal models. Developed in alignment with AI governance frameworks such as\nthe EU AI Act, NIST guidelines, and the MIT AI Risk Repository 2024, VLDBench\nis expected to become a benchmark for detecting disinformation in online\nmulti-modal contents. Our code and data will be publicly available.", "published": "2025-02-17 02:18:47", "link": "http://arxiv.org/abs/2502.11361v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Blessing of Multilinguality: A Systematic Analysis of Multilingual\n  In-Context Learning", "abstract": "While multilingual large language models generally perform adequately, and\nsometimes even rival English performance on high-resource languages (HRLs),\nthey often significantly underperform on low-resource languages (LRLs). Among\nseveral prompting strategies aiming at bridging the gap, multilingual\nin-context learning (ICL) has been particularly effective when demonstration in\ntarget languages is unavailable. However, there lacks a systematic\nunderstanding of when and why it works well.\n  In this work, we systematically analyze multilingual ICL, using\ndemonstrations in HRLs to enhance cross-lingual transfer. We show that\ndemonstrations in mixed HRLs consistently outperform English-only ones across\nthe board, particularly for tasks written in LRLs. Surprisingly, our ablation\nstudy shows that the presence of irrelevant non-English sentences in the prompt\nyields measurable gains, suggesting the effectiveness of multilingual exposure\nitself. Our results highlight the potential of strategically leveraging\nmultilingual resources to bridge the performance gap for underrepresented\nlanguages.", "published": "2025-02-17 02:27:35", "link": "http://arxiv.org/abs/2502.11364v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Small World of Word Embeddings: A Comparative Study on\n  Conceptual Spaces from LLMs of Different Scales", "abstract": "A conceptual space represents concepts as nodes and semantic relatedness as\nedges. Word embeddings, combined with a similarity metric, provide an effective\napproach to constructing such a space. Typically, embeddings are derived from\ntraditional distributed models or encoder-only pretrained models, whose\nobjectives directly capture the meaning of the current token. In contrast,\ndecoder-only models, including large language models (LLMs), predict the next\ntoken, making their embeddings less directly tied to the current token's\nsemantics. Moreover, comparative studies on LLMs of different scales remain\nunderexplored. In this paper, we construct a conceptual space using word\nembeddings from LLMs of varying scales and comparatively analyze their\nproperties. We establish a network based on a linguistic typology-inspired\nconnectivity hypothesis, examine global statistical properties, and compare\nLLMs of varying scales. Locally, we analyze conceptual pairs, WordNet\nrelations, and a cross-lingual semantic network for qualitative words. Our\nresults indicate that the constructed space exhibits small-world properties,\ncharacterized by a high clustering coefficient and short path lengths. Larger\nLLMs generate more intricate spaces, with longer paths reflecting richer\nrelational structures and connections. Furthermore, the network serves as an\nefficient bridge for cross-lingual semantic mapping.", "published": "2025-02-17 02:52:07", "link": "http://arxiv.org/abs/2502.11380v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RoleMRC: A Fine-Grained Composite Benchmark for Role-Playing and\n  Instruction-Following", "abstract": "Role-playing is important for Large Language Models (LLMs) to follow diverse\ninstructions while maintaining role identity and the role's pre-defined ability\nlimits. Existing role-playing datasets mostly contribute to controlling role\nstyle and knowledge boundaries, but overlook role-playing in\ninstruction-following scenarios. We introduce a fine-grained role-playing and\ninstruction-following composite benchmark, named RoleMRC, including: (1)\nMulti-turn dialogues between ideal roles and humans, including free chats or\ndiscussions upon given passages; (2) Role-playing machine reading\ncomprehension, involving response, refusal, and attempts according to passage\nanswerability and role ability; (3) More complex scenarios with nested,\nmulti-turn and prioritized instructions. The final RoleMRC features a 10.2k\nrole profile meta-pool, 37.9k well-synthesized role-playing instructions, and\n1.4k testing samples. We develop a pipeline to quantitatively evaluate the\nfine-grained role-playing and instruction-following capabilities of several\nmainstream LLMs, as well as models that are fine-tuned on our data. Moreover,\ncross-evaluation on external role-playing datasets confirms that models\nfine-tuned on RoleMRC enhances instruction-following without compromising\ngeneral role-playing and reasoning capabilities. We also probe the neural-level\nactivation maps of different capabilities over post-tuned LLMs. Access to our\nRoleMRC, RoleMRC-mix and Codes: https://github.com/LuJunru/RoleMRC.", "published": "2025-02-17 03:08:37", "link": "http://arxiv.org/abs/2502.11387v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the\n  Robustness of LLMs in Commonsense Reasoning", "abstract": "Large language models (LLMs) have shown remarkable capabilities in\ncommonsense reasoning; however, some variations in questions can trigger\nincorrect responses. Do these models truly understand commonsense knowledge, or\njust memorize expression patterns? To investigate this question, we present the\nfirst extensive robustness evaluation of LLMs in commonsense reasoning. We\nintroduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200\ncases, by designing and compiling seven types of question variants. To\nconstruct this benchmark, we propose a two-stage method to develop Chinese\nHellaSwag, a finely annotated dataset comprising 12,000 instances across 56\ncategories. We conduct extensive experiments on 41 representative LLMs,\nrevealing that these LLMs are far from robust in commonsense reasoning.\nFurthermore, this robustness varies depending on the language in which the LLM\nis tested. This work establishes a high-quality evaluation benchmark, with\nextensive experiments offering valuable insights to the community in\ncommonsense reasoning for LLMs.", "published": "2025-02-17 03:24:02", "link": "http://arxiv.org/abs/2502.11393v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Robust RAG: Do We Still Need Complex Robust Training in the\n  Era of Powerful LLMs?", "abstract": "Retrieval-augmented generation (RAG) systems often suffer from performance\ndegradation when encountering noisy or irrelevant documents, driving\nresearchers to develop sophisticated training strategies to enhance their\nrobustness against such retrieval noise. However, as large language models\n(LLMs) continue to advance, the necessity of these complex training methods is\nincreasingly questioned. In this paper, we systematically investigate whether\ncomplex robust training strategies remain necessary as model capacity grows.\nThrough comprehensive experiments spanning multiple model architectures and\nparameter scales, we evaluate various document selection methods and\nadversarial training techniques across diverse datasets. Our extensive\nexperiments consistently demonstrate that as models become more powerful, the\nperformance gains brought by complex robust training methods drop off\ndramatically. We delve into the rationale and find that more powerful models\ninherently exhibit superior confidence calibration, better generalization\nacross datasets (even when trained with randomly selected documents), and\noptimal attention mechanisms learned with simpler strategies. Our findings\nsuggest that RAG systems can benefit from simpler architectures and training\nstrategies as models become more powerful, enabling more scalable applications\nwith minimal complexity.", "published": "2025-02-17 03:34:31", "link": "http://arxiv.org/abs/2502.11400v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Following the Autoregressive Nature of LLM Embeddings via Compression\n  and Alignment", "abstract": "A new trend uses LLMs as dense text encoders via contrastive learning.\nHowever, since LLM embeddings predict the probability distribution of the next\ntoken, they are inherently generative and distributive, conflicting with\ncontrastive learning, which requires embeddings to capture full-text semantics\nand align via cosine similarity. This discrepancy hinders the full utilization\nof LLMs' pre-training capabilities, resulting in inefficient learning. In\nresponse to this issue, we propose AutoRegEmbed, a new contrastive learning\nmethod built on embedding conditional probability distributions, which\nintegrates two core tasks: information compression and conditional distribution\nalignment. The information compression task encodes text into the embedding\nspace, ensuring that the embedding vectors capture global semantics. The\nconditional distribution alignment task focuses on aligning text embeddings\nwith positive samples embeddings by leveraging the conditional distribution of\nembeddings while simultaneously reducing the likelihood of generating negative\nsamples from text embeddings, thereby achieving embedding alignment and\nuniformity. Experimental results demonstrate that our method significantly\noutperforms traditional contrastive learning approaches and achieves\nperformance comparable to state-of-the-art models when using the same amount of\ndata.", "published": "2025-02-17 03:36:25", "link": "http://arxiv.org/abs/2502.11401v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large\n  Language Models", "abstract": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning.", "published": "2025-02-17 03:42:28", "link": "http://arxiv.org/abs/2502.11404v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LayAlign: Enhancing Multilingual Reasoning in Large Language Models via\n  Layer-Wise Adaptive Fusion and Alignment Strategy", "abstract": "Despite being pretrained on multilingual corpora, large language models\n(LLMs) exhibit suboptimal performance on low-resource languages. Recent\napproaches have leveraged multilingual encoders alongside LLMs by introducing\ntrainable parameters connecting the two models. However, these methods\ntypically focus on the encoder's output, overlooking valuable information from\nother layers. We propose \\aname (\\mname), a framework that integrates\nrepresentations from all encoder layers, coupled with the \\attaname mechanism\nto enable layer-wise interaction between the LLM and the multilingual encoder.\nExtensive experiments on multilingual reasoning tasks, along with analyses of\nlearned representations, show that our approach consistently outperforms\nexisting baselines.", "published": "2025-02-17 03:45:03", "link": "http://arxiv.org/abs/2502.11405v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InsBank: Evolving Instruction Subset for Ongoing Alignment", "abstract": "Large language models (LLMs) typically undergo instruction tuning to enhance\nalignment. Recent studies emphasize that quality and diversity of instruction\ndata are more crucial than quantity, highlighting the need to select diverse,\nhigh-quality subsets to reduce training costs. However, how to evolve these\nselected subsets alongside the development of new instruction data remains\ninsufficiently explored. To achieve LLMs' ongoing alignment, we introduce\nInstruction Bank (InsBank), a continuously updated repository that integrates\nthe latest valuable instruction data. We further propose Progressive\nInstruction Bank Evolution (PIBE), a novel framework designed to evolve InsBank\neffectively and efficiently over time. PIBE employs a gradual data selection\nstrategy to maintain long-term efficiency, leveraging a representation-based\ndiversity score to capture relationships between data points and retain\nhistorical information for comprehensive diversity evaluation. This also allows\nfor flexible combination of diversity and quality scores during data selection\nand ranking. Extensive experiments demonstrate that PIBE significantly\noutperforms baselines in InsBank evolution and is able to extract\nbudget-specific subsets, demonstrating its effectiveness and adaptability.", "published": "2025-02-17 04:17:53", "link": "http://arxiv.org/abs/2502.11419v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Persona Sentiment Sensitivity in Personalized Dialogue\n  Generation", "abstract": "Personalized dialogue systems have advanced considerably with the integration\nof user-specific personas into large language models (LLMs). However, while\nLLMs can effectively generate personalized responses, the influence of persona\nsentiment on dialogue quality remains underexplored. In this work, we conduct a\nlarge-scale analysis of dialogues generated using a range of polarized user\nprofiles. Our experiments reveal that dialogues involving negatively polarized\nusers tend to overemphasize persona attributes, leading to increased entailment\nand contradiction instances and lower overall coherence. In contrast,\npositively polarized profiles yield dialogues that selectively incorporate\npersona information, resulting in smoother and more coherent interactions.\nFurthermore, we find that personas with weak or neutral sentiment generally\nproduce lower-quality dialogues. Motivated by these findings, we propose a\ndialogue generation approach that explicitly accounts for persona polarity by\ncombining a turn-based generation strategy with a profile ordering mechanism.\nOur study provides new insights into the sensitivity of LLMs to persona\nsentiment and offers guidance for developing more robust and nuanced\npersonalized dialogue systems.", "published": "2025-02-17 04:36:53", "link": "http://arxiv.org/abs/2502.11423v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Any Information Is Just Worth One Single Screenshot: Unifying Search\n  With Visualized Information Retrieval", "abstract": "With the popularity of multimodal techniques, it receives growing interests\nto acquire useful information in visual forms. In this work, we formally define\nan emerging IR paradigm called \\textit{Visualized Information Retrieval}, or\n\\textbf{Vis-IR}, where multimodal information, such as texts, images, tables\nand charts, is jointly represented by a unified visual format called\n\\textbf{Screenshots}, for various retrieval applications. We further make three\nkey contributions for Vis-IR. First, we create \\textbf{VIRA} (Vis-IR\nAggregation), a large-scale dataset comprising a vast collection of screenshots\nfrom diverse sources, carefully curated into captioned and question-answer\nformats. Second, we develop \\textbf{UniSE} (Universal Screenshot Embeddings), a\nfamily of retrieval models that enable screenshots to query or be queried\nacross arbitrary data modalities. Finally, we construct \\textbf{MVRB} (Massive\nVisualized IR Benchmark), a comprehensive benchmark covering a variety of task\nforms and application scenarios. Through extensive evaluations on MVRB, we\nhighlight the deficiency from existing multimodal retrievers and the\nsubstantial improvements made by UniSE. Our work will be shared with the\ncommunity, laying a solid foundation for this emerging field.", "published": "2025-02-17 04:40:15", "link": "http://arxiv.org/abs/2502.11431v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example\n  Selection for Text-to-SQL", "abstract": "Text-to-SQL aims to convert natural language questions into executable SQL\nqueries. While previous approaches, such as skeleton-masked selection, have\ndemonstrated strong performance by retrieving similar training examples to\nguide large language models (LLMs), they struggle in real-world scenarios where\nsuch examples are unavailable. To overcome this limitation, we propose\nSelf-Augmentation in-context learning with Fine-grained Example selection for\nText-to-SQL (SAFE-SQL), a novel framework that improves SQL generation by\ngenerating and filtering self-augmented examples. SAFE-SQL first prompts an LLM\nto generate multiple Text-to-SQL examples relevant to the test input. Then\nSAFE-SQL filters these examples through three relevance assessments,\nconstructing high-quality in-context learning examples. Using self-generated\nexamples, SAFE-SQL surpasses the previous zero-shot, and few-shot Text-to-SQL\nframeworks, achieving higher execution accuracy. Notably, our approach provides\nadditional performance gains in extra hard and unseen scenarios, where\nconventional methods often fail.", "published": "2025-02-17 04:52:24", "link": "http://arxiv.org/abs/2502.11438v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which Retain Set Matters for LLM Unlearning? A Case Study on Entity\n  Unlearning", "abstract": "Large language models (LLMs) risk retaining unauthorized or sensitive\ninformation from their training data, which raises privacy concerns. LLM\nunlearning seeks to mitigate these risks by selectively removing specified data\nwhile maintaining overall model performance. However, most existing work focus\non methods to achieve effective forgetting and does not provide a detailed\nanalysis of the retain set, the portion of training data that is not targeted\nfor removal. In this paper, we investigate the effects of unlearning on various\nsubsets of the retain set through a case study on entity unlearning. We\nintroduce the Syntactically Similar Neighbor Set, a group of queries that share\nsimilar syntactic structures with the data targeted for removal, and show that\nthis subset suffers the greatest performance drop during unlearning. Moreover,\nwhen used for regularization, this set not only preserves performance on\nsyntactically similar queries but also delivers comparable or improved results\nacross other data subsets. Our results highlight that syntactic similarity is a\ncritical factor, potentially more so than domain or entity relationships, in\nachieving effective and practical LLM unlearning.", "published": "2025-02-17 04:55:02", "link": "http://arxiv.org/abs/2502.11441v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does RAG Really Perform Bad For Long-Context Processing?", "abstract": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.", "published": "2025-02-17 05:02:25", "link": "http://arxiv.org/abs/2502.11444v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Personas to Talks: Revisiting the Impact of Personas on\n  LLM-Synthesized Emotional Support Conversations", "abstract": "The rapid advancement of Large Language Models (LLMs) has revolutionized the\ngeneration of emotional support conversations (ESC), offering scalable\nsolutions with reduced costs and enhanced data privacy. This paper explores the\nrole of personas in the creation of ESC by LLMs. Our research utilizes\nestablished psychological frameworks to measure and infuse persona traits into\nLLMs, which then generate dialogues in the emotional support scenario. We\nconduct extensive evaluations to understand the stability of persona traits in\ndialogues, examining shifts in traits post-generation and their impact on\ndialogue quality and strategy distribution. Experimental results reveal several\nnotable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in\nemotionality and extraversion occur, influencing the dialogue dynamics, and 3)\nthe application of persona traits modifies the distribution of emotional\nsupport strategies, enhancing the relevance and empathetic quality of the\nresponses. These findings highlight the potential of persona-driven LLMs in\ncrafting more personalized, empathetic, and effective emotional support\ndialogues, which has significant implications for the future design of\nAI-driven emotional support systems.", "published": "2025-02-17 05:24:30", "link": "http://arxiv.org/abs/2502.11451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with\n  Unified Multi-Objective Optimization", "abstract": "Human preference plays a significant role in measuring large language models\nand guiding them to align with human values. Unfortunately, current\ncomparing-based evaluation (CBE) methods typically focus on a single\noptimization objective, failing to effectively utilize scarce yet valuable\npreference signals. To address this, we delve into key factors that can enhance\nthe accuracy, convergence, and scalability of CBE: suppressing sampling bias,\nbalancing descending process of uncertainty, and mitigating updating\nuncertainty. Following the derived guidelines, we propose UniCBE, a unified\nuniformity-driven CBE framework which simultaneously optimize these core\nobjectives by constructing and integrating three decoupled sampling probability\nmatrices, each designed to ensure uniformity in specific aspects. We further\nablate the optimal tuple sampling and preference aggregation strategies to\nachieve efficient CBE. On the AlpacaEval benchmark, UniCBE saves over 17% of\nevaluation budgets while achieving a Pearson correlation with ground truth\nexceeding 0.995, demonstrating excellent accuracy and convergence. In scenarios\nwhere new models are continuously introduced, UniCBE can even save over 50% of\nevaluation costs, highlighting its improved scalability.", "published": "2025-02-17 05:28:12", "link": "http://arxiv.org/abs/2502.11454v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What\n  is the Plausible Memory Representation?", "abstract": "Recent work in computational psycholinguistics has revealed intriguing\nparallels between attention mechanisms and human memory retrieval, focusing\nprimarily on Transformer architectures that operate on token-level\nrepresentations. However, computational psycholinguistic research has also\nestablished that syntactic structures provide compelling explanations for human\nsentence processing that word-level factors alone cannot fully account for. In\nthis study, we investigate whether the attention mechanism of Transformer\nGrammar (TG), which uniquely operates on syntactic structures as\nrepresentational units, can serve as a cognitive model of human memory\nretrieval, using Normalized Attention Entropy (NAE) as a linking hypothesis\nbetween model behavior and human processing difficulty. Our experiments\ndemonstrate that TG's attention achieves superior predictive power for\nself-paced reading times compared to vanilla Transformer's, with further\nanalyses revealing independent contributions from both models. These findings\nsuggest that human sentence processing involves dual memory representations --\none based on syntactic structures and another on token sequences -- with\nattention serving as the general retrieval algorithm, while highlighting the\nimportance of incorporating syntactic structures as representational units.", "published": "2025-02-17 05:58:25", "link": "http://arxiv.org/abs/2502.11469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FastMCTS: A Simple Sampling Strategy for Data Synthesis", "abstract": "Synthetic high-quality multi-step reasoning data can significantly enhance\nthe performance of large language models on various tasks. However, most\nexisting methods rely on rejection sampling, which generates trajectories\nindependently and suffers from inefficiency and imbalanced sampling across\nproblems of varying difficulty. In this work, we introduce FastMCTS, an\ninnovative data synthesis strategy inspired by Monte Carlo Tree Search.\nFastMCTS provides a more efficient sampling method for multi-step reasoning\ndata, offering step-level evaluation signals and promoting balanced sampling\nacross problems of different difficulty levels. Experiments on both English and\nChinese reasoning datasets demonstrate that FastMCTS generates over 30\\% more\ncorrect reasoning paths compared to rejection sampling as the number of\ngenerated tokens scales up. Furthermore, under comparable synthetic data\nbudgets, models trained on FastMCTS-generated data outperform those trained on\nrejection sampling data by 3.9\\% across multiple benchmarks. As a lightweight\nsampling strategy, FastMCTS offers a practical and efficient alternative for\nsynthesizing high-quality reasoning data. Our code will be released soon.", "published": "2025-02-17 06:27:57", "link": "http://arxiv.org/abs/2502.11476v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAST: Context-Aware Compression in LLMs via Dynamic Allocation of Soft\n  Tokens", "abstract": "Large Language Models (LLMs) face computational inefficiencies and redundant\nprocessing when handling long context inputs, prompting a focus on compression\ntechniques. While existing semantic vector-based compression methods achieve\npromising performance, these methods fail to account for the intrinsic\ninformation density variations between context chunks, instead allocating soft\ntokens uniformly across context chunks. This uniform distribution inevitably\ndiminishes allocation to information-critical regions. To address this, we\npropose Dynamic Allocation of Soft Tokens (DAST), a simple yet effective method\nthat leverages the LLM's intrinsic understanding of contextual relevance to\nguide compression. DAST combines perplexity-based local information with\nattention-driven global information to dynamically allocate soft tokens to the\ninformative-rich chunks, enabling effective, context-aware compression.\nExperimental results across multiple benchmarks demonstrate that DAST surpasses\nstate-of-the-art methods.", "published": "2025-02-17 06:55:13", "link": "http://arxiv.org/abs/2502.11493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balanced Multi-Factor In-Context Learning for Multilingual Large\n  Language Models", "abstract": "Multilingual large language models (MLLMs) are able to leverage in-context\nlearning (ICL) to achieve high performance by leveraging cross-lingual\nknowledge transfer without parameter updates. However, their effectiveness is\nhighly sensitive to example selection, particularly in multilingual settings.\nBased on the findings of existing work, three key factors influence\nmultilingual ICL: (1) semantic similarity, (2) linguistic alignment, and (3)\nlanguage-specific performance. However, existing approaches address these\nfactors independently, without explicitly disentangling their combined impact,\nleaving optimal example selection underexplored. To address this gap, we\npropose balanced multi-factor ICL (\\textbf{BMF-ICL}), a method that quantifies\nand optimally balances these factors for improved example selection.\nExperiments on mCSQA and TYDI across four MLLMs demonstrate that BMF-ICL\noutperforms existing methods. Further analysis highlights the importance of\nincorporating all three factors and the importance of selecting examples from\nmultiple languages.", "published": "2025-02-17 06:56:33", "link": "http://arxiv.org/abs/2502.11495v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Investigating Inference-time Scaling for Chain of Multi-modal Thought: A\n  Preliminary Study", "abstract": "Recently, inference-time scaling of chain-of-thought (CoT) has been\ndemonstrated as a promising approach for addressing multi-modal reasoning\ntasks. While existing studies have predominantly centered on text-based\nthinking, the integration of both visual and textual modalities within the\nreasoning process remains unexplored. In this study, we pioneer the exploration\nof inference-time scaling with multi-modal thought, aiming to bridge this gap.\nTo provide a comprehensive analysis, we systematically investigate popular\nsampling-based and tree search-based inference-time scaling methods on 10\nchallenging tasks spanning various domains. Besides, we uniformly adopt a\nconsistency-enhanced verifier to ensure effective guidance for both methods\nacross different thought paradigms. Results show that multi-modal thought\npromotes better performance against conventional text-only thought, and\nblending the two types of thought fosters more diverse thinking. Despite these\nadvantages, multi-modal thoughts necessitate higher token consumption for\nprocessing richer visual inputs, which raises concerns in practical\napplications. We hope that our findings on the merits and drawbacks of this\nresearch line will inspire future works in the field.", "published": "2025-02-17 07:29:01", "link": "http://arxiv.org/abs/2502.11514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AURORA:Automated Training Framework of Universal Process Reward Models\n  via Ensemble Prompting and Reverse Verification", "abstract": "The reasoning capabilities of advanced large language models (LLMs) like o1\nhave revolutionized artificial intelligence applications. Nevertheless,\nevaluating and optimizing complex reasoning processes remain significant\nchallenges due to diverse policy distributions and the inherent limitations of\nhuman effort and accuracy. In this paper, we present AURORA, a novel automated\nframework for training universal process reward models (PRMs) using ensemble\nprompting and reverse verification. The framework employs a two-phase approach:\nFirst, it uses diverse prompting strategies and ensemble methods to perform\nautomated annotation and evaluation of processes, ensuring robust assessments\nfor reward learning. Second, it leverages practical reference answers for\nreverse verification, enhancing the model's ability to validate outputs and\nimproving training accuracy. To assess the framework's performance, we extend\nbeyond the existing ProcessBench benchmark by introducing UniversalBench, which\nevaluates reward predictions across full trajectories under diverse policy\ndistribtion with long Chain-of-Thought (CoT) outputs. Experimental results\ndemonstrate that AURORA enhances process evaluation accuracy, improves PRMs'\naccuracy for diverse policy distributions and long-CoT responses. The project\nwill be open-sourced at https://auroraprm.github.io/. The Universal-PRM-7B is\navailable at https://huggingface.co/infly/Universal-PRM-7B.", "published": "2025-02-17 07:41:27", "link": "http://arxiv.org/abs/2502.11520v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Large Language Models to be Better Rule Followers", "abstract": "Large language models (LLMs) have shown impressive performance across a wide\nrange of tasks. However, they often exhibit unexpected failures in seemingly\nstraightforward tasks, suggesting a reliance on case-based reasoning rather\nthan rule-based reasoning. While the vast training corpus of LLMs contains\nnumerous textual \"rules\", current training methods fail to leverage these rules\neffectively. Crucially, the relationships between these \"rules\" and their\ncorresponding \"instances\" are not explicitly modeled. As a result, while LLMs\ncan often recall rules with ease, they fail to apply these rules strictly and\nconsistently in relevant reasoning scenarios. In this paper, we investigate the\nrule-following capabilities of LLMs and propose Meta Rule-Following Fine-Tuning\n(Meta-RFFT) to enhance the cross-task transferability of rule-following\nabilities. We first construct a dataset of 88 tasks requiring following rules,\nencompassing diverse reasoning domains. We demonstrate through extensive\nexperiments that models trained on large-scale rule-following tasks are better\nrule followers, outperforming the baselines in both downstream fine-tuning and\nfew-shot prompting scenarios. This highlights the cross-task transferability of\nmodels with the aid of Meta-RFFT. Furthermore, we examine the influence of\nfactors such as dataset size, rule formulation, and in-context learning.", "published": "2025-02-17 07:54:50", "link": "http://arxiv.org/abs/2502.11525v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of\n  Stealing Privacy", "abstract": "Model merging is a widespread technology in large language models (LLMs) that\nintegrates multiple task-specific LLMs into a unified one, enabling the merged\nmodel to inherit the specialized capabilities of these LLMs. Most task-specific\nLLMs are sourced from open-source communities and have not undergone rigorous\nauditing, potentially imposing risks in model merging. This paper highlights an\noverlooked privacy risk: \\textit{an unsafe model could compromise the privacy\nof other LLMs involved in the model merging.} Specifically, we propose PhiMM, a\nprivacy attack approach that trains a phishing model capable of stealing\nprivacy using a crafted privacy phishing instruction dataset. Furthermore, we\nintroduce a novel model cloaking method that mimics a specialized capability to\nconceal attack intent, luring users into merging the phishing model. Once\nvictims merge the phishing model, the attacker can extract personally\nidentifiable information (PII) or infer membership information (MI) by querying\nthe merged model with the phishing instruction. Experimental results show that\nmerging a phishing model increases the risk of privacy breaches. Compared to\nthe results before merging, PII leakage increased by 3.9\\% and MI leakage\nincreased by 17.4\\% on average. We release the code of PhiMM through a link.", "published": "2025-02-17 08:04:52", "link": "http://arxiv.org/abs/2502.11533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating o1-Like LLMs: Unlocking Reasoning for Translation through\n  Comprehensive Analysis", "abstract": "The o1-Like LLMs are transforming AI by simulating human cognitive processes,\nbut their performance in multilingual machine translation (MMT) remains\nunderexplored. This study examines: (1) how o1-Like LLMs perform in MMT tasks\nand (2) what factors influence their translation quality. We evaluate multiple\no1-Like LLMs and compare them with traditional models like ChatGPT and GPT-4o.\nResults show that o1-Like LLMs establish new multilingual translation\nbenchmarks, with DeepSeek-R1 surpassing GPT-4o in contextless tasks. They\ndemonstrate strengths in historical and cultural translation but exhibit a\ntendency for rambling issues in Chinese-centric outputs. Further analysis\nreveals three key insights: (1) High inference costs and slower processing\nspeeds make complex translation tasks more resource-intensive. (2) Translation\nquality improves with model size, enhancing commonsense reasoning and cultural\ntranslation. (3) The temperature parameter significantly impacts output\nquality-lower temperatures yield more stable and accurate translations, while\nhigher temperatures reduce coherence and precision.", "published": "2025-02-17 08:23:46", "link": "http://arxiv.org/abs/2502.11544v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data\n  Cleaning as Anomaly Detection", "abstract": "The rapid development of multilingual large language models (LLMs) highlights\nthe need for high-quality, diverse, and clean multilingual datasets. In this\npaper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), a\nlarge-scale multilingual corpus built using newly extracted Common Crawl data\nand existing multilingual datasets. DCAD-2000 includes over 2,282 languages,\n46.72TB of data, and 8.63 billion documents, spanning 155 high- and\nmedium-resource languages and 159 writing scripts. To overcome the limitations\nof current data cleaning methods, which rely on manual heuristic thresholds, we\npropose reframing data cleaning as an anomaly detection task. This dynamic\nfiltering approach significantly enhances data quality by identifying and\nremoving noisy or anomalous content. We evaluate the quality of DCAD-2000 on\nthe FineTask benchmark, demonstrating substantial improvements in multilingual\ndataset quality and task performance.", "published": "2025-02-17 08:28:29", "link": "http://arxiv.org/abs/2502.11546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforced Information Retrieval", "abstract": "While retrieval techniques are widely used in practice, they still face\nsignificant challenges in cross-domain scenarios. Recently,\ngeneration-augmented methods have emerged as a promising solution to this\nproblem. These methods enhance raw queries by incorporating additional\ninformation from an LLM-based generator, facilitating more direct retrieval of\nrelevant documents. However, existing methods struggle with highly specialized\nsituations that require extensive domain expertise. To address this problem, we\npresent \\textbf{Reinforced-IR}, a novel approach that jointly adapts a\npre-trained retriever and generator for precise cross-domain retrieval. A key\ninnovation of Reinforced-IR is its \\textbf{Self-Boosting} framework, which\nenables retriever and generator to learn from each other's feedback.\nSpecifically, the generator is reinforced to generate query augmentations that\nenhance the retriever's performance, while the retriever is trained to better\ndiscriminate the relevant documents identified by the generator. This iterative\nprocess allows the end-to-end retrieval performance to be progressively\noptimized using an unlabeled corpus from the target domain. In our experiment,\nReinforced-IR outperforms existing domain adaptation methods by a large margin,\nleading to substantial improvements in retrieval quality across a wide range of\napplication scenarios.", "published": "2025-02-17 08:52:39", "link": "http://arxiv.org/abs/2502.11562v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?", "abstract": "The radioactive nature of Large Language Model (LLM) watermarking enables the\ndetection of watermarks inherited by student models when trained on the outputs\nof watermarked teacher models, making it a promising tool for preventing\nunauthorized knowledge distillation. However, the robustness of watermark\nradioactivity against adversarial actors remains largely unexplored. In this\npaper, we investigate whether student models can acquire the capabilities of\nteacher models through knowledge distillation while avoiding watermark\ninheritance. We propose two categories of watermark removal approaches:\npre-distillation removal through untargeted and targeted training data\nparaphrasing (UP and TP), and post-distillation removal through inference-time\nwatermark neutralization (WN). Extensive experiments across multiple model\npairs, watermarking schemes and hyper-parameter settings demonstrate that both\nTP and WN thoroughly eliminate inherited watermarks, with WN achieving this\nwhile maintaining knowledge transfer efficiency and low computational overhead.\nGiven the ongoing deployment of watermarking techniques in production LLMs,\nthese findings emphasize the urgent need for more robust defense strategies.\nOur code is available at\nhttps://github.com/THU-BPM/Watermark-Radioactivity-Attack.", "published": "2025-02-17 09:34:19", "link": "http://arxiv.org/abs/2502.11598v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "CLASS: Enhancing Cross-Modal Text-Molecule Retrieval Performance and\n  Training Efficiency", "abstract": "Cross-modal text-molecule retrieval task bridges molecule structures and\nnatural language descriptions. Existing methods predominantly focus on aligning\ntext modality and molecule modality, yet they overlook adaptively adjusting the\nlearning states at different training stages and enhancing training efficiency.\nTo tackle these challenges, this paper proposes a Curriculum Learning-bAsed\ncroSS-modal text-molecule training framework (CLASS), which can be integrated\nwith any backbone to yield promising performance improvement. Specifically, we\nquantify the sample difficulty considering both text modality and molecule\nmodality, and design a sample scheduler to introduce training samples via an\neasy-to-difficult paradigm as the training advances, remarkably reducing the\nscale of training samples at the early stage of training and improving training\nefficiency. Moreover, we introduce adaptive intensity learning to increase the\ntraining intensity as the training progresses, which adaptively controls the\nlearning intensity across all curriculum stages. Experimental results on the\nChEBI-20 dataset demonstrate that our proposed method gains superior\nperformance, simultaneously achieving prominent time savings.", "published": "2025-02-17 10:24:07", "link": "http://arxiv.org/abs/2502.11633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Fully Exploiting LLM Internal States to Enhance Knowledge\n  Boundary Perception", "abstract": "Large language models (LLMs) exhibit impressive performance across diverse\ntasks but often struggle to accurately gauge their knowledge boundaries,\nleading to confident yet incorrect responses. This paper explores leveraging\nLLMs' internal states to enhance their perception of knowledge boundaries from\nefficiency and risk perspectives. We investigate whether LLMs can estimate\ntheir confidence using internal states before response generation, potentially\nsaving computational resources. Our experiments on datasets like Natural\nQuestions, HotpotQA, and MMLU reveal that LLMs demonstrate significant\npre-generation perception, which is further refined post-generation, with\nperception gaps remaining stable across varying conditions. To mitigate risks\nin critical domains, we introduce Consistency-based Confidence Calibration\n($C^3$), which assesses confidence consistency through question reformulation.\n$C^3$ significantly improves LLMs' ability to recognize their knowledge gaps,\nenhancing the unknown perception rate by 5.6\\% on NQ and 4.9\\% on HotpotQA. Our\nfindings suggest that pre-generation confidence estimation can optimize\nefficiency, while $C^3$ effectively controls output risks, advancing the\nreliability of LLMs in practical applications.", "published": "2025-02-17 11:11:09", "link": "http://arxiv.org/abs/2502.11677v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Isolates to Families: Using Neural Networks for Automated Language\n  Affiliation", "abstract": "In historical linguistics, the affiliation of languages to a common language\nfamily is traditionally carried out using a complex workflow that relies on\nmanually comparing individual languages. Large-scale standardized collections\nof multilingual wordlists and grammatical language structures might help to\nimprove this and open new avenues for developing automated language affiliation\nworkflows. Here, we present neural network models that use lexical and\ngrammatical data from a worldwide sample of more than 1,000 languages with\nknown affiliations to classify individual languages into families. In line with\nthe traditional assumption of most linguists, our results show that models\ntrained on lexical data alone outperform models solely based on grammatical\ndata, whereas combining both types of data yields even better performance. In\nadditional experiments, we show how our models can identify long-ranging\nrelations between entire subgroups, how they can be employed to investigate\npotential relatives of linguistic isolates, and how they can help us to obtain\nfirst hints on the affiliation of so far unaffiliated languages. We conclude\nthat models for automated language affiliation trained on lexical and\ngrammatical data provide comparative linguists with a valuable tool for\nevaluating hypotheses about deep and unknown language relations.", "published": "2025-02-17 11:25:32", "link": "http://arxiv.org/abs/2502.11688v1", "categories": ["cs.CL", "J.5"], "primary_category": "cs.CL"}
{"title": "Improve LLM-as-a-Judge Ability as a General Ability", "abstract": "LLM-as-a-Judge leverages the generative and reasoning capabilities of large\nlanguage models (LLMs) to evaluate LLM responses across diverse scenarios,\nproviding accurate preference signals. This approach plays a vital role in\naligning LLMs with human values, ensuring ethical and reliable AI outputs that\nalign with societal norms. Recent studies have raised many methods to train LLM\nas generative judges, but most of them are data consuming or lack accuracy, and\nonly focus on LLM's judge ability. In this work, we regard judge ability as a\ngeneral ability of LLM and implement a two-stage training approach, comprising\nsupervised fine-tuning (SFT) warm-up and direct preference optimization (DPO)\nenhancement, to achieve judge style adaptation and improve judgment accuracy.\nAdditionally, we introduce an efficient data synthesis method to generate\njudgmental content. Experimental results demonstrate that our approach,\nutilizing only about 2% to 40% of the data required by other methods, achieves\nSOTA performance on RewardBench. Furthermore, our training method enhances the\ngeneral capabilities of the model by constructing complicated judge task, and\nthe judge signals provided by our model have significantly enhanced the\ndownstream DPO training performance of our internal models in our test to\noptimize policy model with Judge Model. We also open-source our model weights\nand training data to facilitate further research.", "published": "2025-02-17 11:28:43", "link": "http://arxiv.org/abs/2502.11689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models\n  in Medical Quality Control Indicator Calculation", "abstract": "Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepo https://anonymous.4open.science/r/C-MQCIC-1151.", "published": "2025-02-17 11:40:48", "link": "http://arxiv.org/abs/2502.11703v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ad-hoc Concept Forming in the Game Codenames as a Means for Evaluating\n  Large Language Models", "abstract": "This study utilizes the game Codenames as a benchmarking tool to evaluate\nlarge language models (LLMs) with respect to specific linguistic and cognitive\nskills. LLMs play each side of the game, where one side generates a clue word\ncovering several target words and the other guesses those target words. We\ndesigned various experiments by controlling the choice of words (abstract vs.\nconcrete words, ambiguous vs. monosemic) or the opponent (programmed to be\nfaster or slower in revealing words). Recent commercial and open-weight models\nwere compared side-by-side to find out factors affecting their performance. The\nevaluation reveals details about their strategies, challenging cases, and\nlimitations of LLMs.", "published": "2025-02-17 11:46:46", "link": "http://arxiv.org/abs/2502.11707v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plant in Cupboard, Orange on Table, Book on Shelf. Benchmarking\n  Practical Reasoning and Situation Modelling in a Text-Simulated Situated\n  Environment", "abstract": "Large language models (LLMs) have risen to prominence as 'chatbots' for users\nto interact via natural language. However, their abilities to capture\ncommon-sense knowledge make them seem promising as language-based planners of\nsituated or embodied action as well. We have implemented a simple text-based\nenvironment -- similar to others that have before been used for\nreinforcement-learning of agents -- that simulates, very abstractly, a\nhousehold setting. We use this environment and the detailed error-tracking\ncapabilities we implemented for targeted benchmarking of LLMs on the problem of\npractical reasoning: Going from goals and observations to actions. Our findings\nshow that environmental complexity and game restrictions hamper performance,\nand concise action planning is demanding for current LLMs.", "published": "2025-02-17 12:20:39", "link": "http://arxiv.org/abs/2502.11733v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MT-RAIG: Novel Benchmark and Evaluation Framework for\n  Retrieval-Augmented Insight Generation over Multiple Tables", "abstract": "Recent advancements in table-based reasoning have expanded beyond\nfactoid-level QA to address insight-level tasks, where systems should\nsynthesize implicit knowledge in the table to provide explainable analyses.\nAlthough effective, existing studies remain confined to scenarios where a\nsingle gold table is given alongside the user query, failing to address cases\nwhere users seek comprehensive insights from multiple unknown tables. To bridge\nthese gaps, we propose MT-RAIG Bench, design to evaluate systems on\nRetrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to\ntackle the suboptimality of existing automatic evaluation methods in the table\ndomain, we further introduce a fine-grained evaluation framework MT-RAIG Eval,\nwhich achieves better alignment with human quality judgments on the generated\ninsights. We conduct extensive experiments and reveal that even frontier LLMs\nstill struggle with complex multi-table reasoning, establishing our MT-RAIG\nBench as a challenging testbed for future research.", "published": "2025-02-17 12:21:13", "link": "http://arxiv.org/abs/2502.11735v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Warmup-Distill: Bridge the Distribution Mismatch between Teacher and\n  Student before Knowledge Distillation", "abstract": "The widespread deployment of Large Language Models (LLMs) is hindered by the\nhigh computational demands, making knowledge distillation (KD) crucial for\ndeveloping compact smaller ones. However, the conventional KD methods endure\nthe distribution mismatch issue between the teacher and student models, leading\nto the poor performance of distillation. For instance, the widely-used KL-based\nmethods suffer the mode-averaging and mode-collapsing problems, since the\nmismatched probabitliy distribution between both models. Previous studies\nmainly optimize this issue via different distance calculations towards the\ndistribution of both models. Unfortunately, the distribution mismatch issue\nstill exists in the early stage of the distillation. Hence, to reduce the\nimpact of distribution mismatch, we propose a simple yet efficient method,\nnamed Warmup-Distill, which aligns the distillation of the student to that of\nthe teacher in advance of distillation. Specifically, we first detect the\ndistribution of the student model in practical scenarios with its internal\nknowledge, and then modify the knowledge with low probability via the teacher\nas the checker. Consequently, Warmup-Distill aligns the internal student's\nknowledge to that of the teacher, which expands the distribution of the student\nwith the teacher's, and assists the student model to learn better in the\nsubsequent distillation. Experiments on the seven benchmarks demonstrate that\nWarmup-Distill could provide a warmup student more suitable for distillation,\nwhich outperforms the vanilla student by as least +0.4 averaged score among all\nbenchmarks. Noteably, with the assistance of Warmup-Distill, the distillation\non the math task could yield a further improvement, at most +1.9% accuracy.", "published": "2025-02-17 12:58:12", "link": "http://arxiv.org/abs/2502.11766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Response Generation Strategy Selection for Fine-Tuning Large\n  Language Models Through Self-Aligned Perplexity", "abstract": "Fine-tuning large language models (LLMs) typically relies on producing large\nsets of input-output pairs. Yet for a given question, there can be many valid\noutputs. In practice, these outputs are often derived by distilling knowledge\nfrom teacher models, and they can vary depending on the specific teacher model\nor prompting strategy employed. Recent findings show that how these training\noutputs are generated can significantly affect the performance of the\nfine-tuned model, raising an important question: how do we pick the best data\ngeneration method from among numerous possibilities? Rather than exhaustively\ntraining and evaluating on each candidate, this paper proposes a scalable\napproximate method that assesses a small subset of generated data to estimate\nits suitability for a specific target LLM. Our central idea is that effective\noutputs should be familiar to the target LLM. While previous work measures\nfamiliarity with perplexity, we find that perplexity might be suboptimal in\ncharacterizing 'familiarity' through theoretical analysis and practical\nobservations. To address this, we introduce self-aligned perplexity, a novel\nmetric capturing how closely candidate outputs adhere to the target LLM's own\nstyle and reasoning patterns. In this way, we can identify the most effective\ngeneration strategy on a small sample, then apply it to produce the complete\ntraining set. We demonstrate that training on data generated by the chosen\nmethod yields significant improvements across diverse reasoning-focused\nbenchmarks.", "published": "2025-02-17 13:14:11", "link": "http://arxiv.org/abs/2502.11779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personality Editing for Language Models through Relevant Knowledge\n  Editing", "abstract": "Large Language Models (LLMs) play a vital role in applications like\nconversational agents and content creation, where controlling a model's\npersonality is crucial for maintaining tone, consistency, and engagement.\nHowever, traditional prompt-based techniques for controlling personality often\nfall short, as they do not effectively mitigate the model's inherent biases. In\nthis paper, we introduce a novel method PALETTE that enhances personality\ncontrol through knowledge editing. By generating adjustment queries inspired by\npsychological assessments, our approach systematically adjusts responses to\npersonality-related queries similar to modifying factual knowledge, thereby\nachieving controlled shifts in personality traits. Experimental results from\nboth automatic and human evaluations demonstrate that our method enables more\nstable and well-balanced personality control in LLMs.", "published": "2025-02-17 13:28:14", "link": "http://arxiv.org/abs/2502.11789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Translation Mechanism of Large Language Models", "abstract": "Large language models (LLMs) have succeeded remarkably in multilingual\ntranslation tasks. However, the inherent translation mechanisms of LLMs remain\npoorly understood, largely due to sophisticated architectures and vast\nparameter scales. In response to this issue, this study explores the\ntranslation mechanism of LLM from the perspective of computational components\n(e.g., attention heads and MLPs). Path patching is utilized to explore causal\nrelationships between components, detecting those crucial for translation tasks\nand subsequently analyzing their behavioral patterns in human-interpretable\nterms. Comprehensive analysis reveals that translation is predominantly\nfacilitated by a sparse subset of specialized attention heads (less than 5\\%),\nwhich extract source language, indicator, and positional features. MLPs\nsubsequently integrate and process these features by transiting towards\nEnglish-centric latent representations. Notably, building on the above\nfindings, targeted fine-tuning of only 64 heads achieves translation\nimprovement comparable to full-parameter tuning while preserving general\ncapabilities.", "published": "2025-02-17 13:50:29", "link": "http://arxiv.org/abs/2502.11806v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FineFilter: A Fine-grained Noise Filtering Mechanism for\n  Retrieval-Augmented Large Language Models", "abstract": "Retrieved documents containing noise will hinder Retrieval-Augmented\nGeneration (RAG) from detecting answer clues, necessitating noise filtering\nmechanisms to enhance accuracy. Existing methods use re-ranking or\nsummarization to identify the most relevant sentences, but directly and\naccurately locating answer clues from these large-scale and complex documents\nremains challenging. Unlike these document-level operations, we treat noise\nfiltering as a sentence-level MinMax optimization problem: first identifying\nthe potential clues from multiple documents using contextual information, then\nranking them by relevance, and finally retaining the least clues through\ntruncation. In this paper, we propose FineFilter, a novel fine-grained noise\nfiltering mechanism for RAG consisting of a clue extractor, a re-ranker, and a\ntruncator. We optimize each module to tackle complex reasoning challenges: (1)\nClue extractor firstly uses sentences containing the answer and similar ones as\nfine-tuned targets, aiming at extracting sufficient potential clues; (2)\nRe-ranker is trained to prioritize effective clues based on the real feedback\nfrom generation module, with clues capable of generating correct answer as\npositive samples and others as negative; (3) Truncator takes the minimum clues\nneeded to answer the question (truncation point) as fine-tuned targets, and\nperforms truncation on the re-ranked clues to achieve fine-grained noise\nfiltering. Experiments on three QA datasets demonstrate that FineFilter\nsignificantly outperforms baselines in terms of performance and inference cost.\nFurther analysis on each module shows the effectiveness of our optimizations\nfor complex reasoning.", "published": "2025-02-17 13:55:42", "link": "http://arxiv.org/abs/2502.11811v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) is a crucial task in information\nextraction and sentiment analysis, aiming to identify aspects with associated\nsentiment elements in text. However, existing ABSA datasets are predominantly\nEnglish-centric, limiting the scope for multilingual evaluation and research.\nTo bridge this gap, we present M-ABSA, a comprehensive dataset spanning 7\ndomains and 21 languages, making it the most extensive multilingual parallel\ndataset for ABSA to date. Our primary focus is on triplet extraction, which\ninvolves identifying aspect terms, aspect categories, and sentiment polarities.\nThe dataset is constructed through an automatic translation process with human\nreview to ensure quality. We perform extensive experiments using various\nbaselines to assess performance and compatibility on M-ABSA. Our empirical\nfindings highlight that the dataset enables diverse evaluation tasks, such as\nmultilingual and multi-domain transfer learning, and large language model\nevaluation, underscoring its inclusivity and its potential to drive\nadvancements in multilingual ABSA research.", "published": "2025-02-17 14:16:01", "link": "http://arxiv.org/abs/2502.11824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Classification in the LLM Era -- Where do we stand?", "abstract": "Large Language Models revolutionized NLP and showed dramatic performance\nimprovements across several tasks. In this paper, we investigated the role of\nsuch language models in text classification and how they compare with other\napproaches relying on smaller pre-trained language models. Considering 32\ndatasets spanning 8 languages, we compared zero-shot classification, few-shot\nfine-tuning and synthetic data based classifiers with classifiers built using\nthe complete human labeled dataset. Our results show that zero-shot approaches\ndo well for sentiment classification, but are outperformed by other approaches\nfor the rest of the tasks, and synthetic data sourced from multiple LLMs can\nbuild better classifiers than zero-shot open LLMs. We also see wide performance\ndisparities across languages in all the classification scenarios. We expect\nthat these findings would guide practitioners working on developing text\nclassification systems across languages.", "published": "2025-02-17 14:25:54", "link": "http://arxiv.org/abs/2502.11830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs as a synthesis between symbolic and continuous approaches to\n  language", "abstract": "Since the middle of the 20th century, a fierce battle is being fought between\nsymbolic and continuous approaches to language and cognition. The success of\ndeep learning models, and LLMs in particular, has been alternatively taken as\nshowing that the continuous camp has won, or dismissed as an irrelevant\nengineering development. However, in this position paper I argue that deep\nlearning models for language actually represent a synthesis between the two\ntraditions. This is because 1) deep learning architectures allow for both\ncontinuous/distributed and symbolic/discrete-like representations and\ncomputations; 2) models trained on language make use this flexibility. In\nparticular, I review recent research in mechanistic interpretability that\nshowcases how a substantial part of morphosyntactic knowledge is encoded in a\nnear-discrete fashion in LLMs. This line of research suggests that different\nbehaviors arise in an emergent fashion, and models flexibly alternate between\nthe two modes (and everything in between) as needed. This is possibly one of\nthe main reasons for their wild success; and it is also what makes them\nparticularly interesting for the study of language and cognition. Is it time\nfor peace?", "published": "2025-02-17 14:48:18", "link": "http://arxiv.org/abs/2502.11856v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Large Language Models in Healthcare: Insights into Corpora\n  Sources, Customization Strategies, and Evaluation Metrics", "abstract": "This study reviewed the use of Large Language Models (LLMs) in healthcare,\nfocusing on their training corpora, customization techniques, and evaluation\nmetrics. A systematic search of studies from 2021 to 2024 identified 61\narticles. Four types of corpora were used: clinical resources, literature,\nopen-source datasets, and web-crawled data. Common construction techniques\nincluded pre-training, prompt engineering, and retrieval-augmented generation,\nwith 44 studies combining multiple methods. Evaluation metrics were categorized\ninto process, usability, and outcome metrics, with outcome metrics divided into\nmodel-based and expert-assessed outcomes. The study identified critical gaps in\ncorpus fairness, which contributed to biases from geographic, cultural, and\nsocio-economic factors. The reliance on unverified or unstructured data\nhighlighted the need for better integration of evidence-based clinical\nguidelines. Future research should focus on developing a tiered corpus\narchitecture with vetted sources and dynamic weighting, while ensuring model\ntransparency. Additionally, the lack of standardized evaluation frameworks for\ndomain-specific models called for comprehensive validation of LLMs in\nreal-world healthcare settings.", "published": "2025-02-17 14:53:23", "link": "http://arxiv.org/abs/2502.11861v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding In-Context Machine Translation for Low-Resource Languages:\n  A Case Study on Manchu", "abstract": "In-context machine translation (MT) with large language models (LLMs) is a\npromising approach for low-resource MT, as it can readily take advantage of\nlinguistic resources such as grammar books and dictionaries. Such resources are\nusually selectively integrated into the prompt so that LLMs can directly\nperform translation without any specific training, via their in-context\nlearning capability (ICL). However, the relative importance of each type of\nresource e.g., dictionary, grammar book, and retrieved parallel examples, is\nnot entirely clear. To address this gap, this study systematically investigates\nhow each resource and its quality affects the translation performance, with the\nManchu language as our case study. To remove any prior knowledge of Manchu\nencoded in the LLM parameters and single out the effect of ICL, we also\nexperiment with an encrypted version of Manchu texts. Our results indicate that\nhigh-quality dictionaries and good parallel examples are very helpful, while\ngrammars hardly help. In a follow-up study, we showcase a promising application\nof in-context MT: parallel data augmentation as a way to bootstrap the\nconventional MT model. When monolingual data abound, generating synthetic\nparallel data through in-context MT offers a pathway to mitigate data scarcity\nand build effective and efficient low-resource neural MT systems.", "published": "2025-02-17 14:53:49", "link": "http://arxiv.org/abs/2502.11862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Southern Newswire Corpus: A Large-Scale Dataset of Mid-Century Wire\n  Articles Beyond the Front Page", "abstract": "I introduce a new large-scale dataset of historical wire articles from U.S.\nSouthern newspapers, spanning 1960-1975 and covering multiple wire services:\nThe Associated Press, United Press International, Newspaper Enterprise\nAssociation. Unlike prior work focusing on front-page content, this dataset\ncaptures articles across the entire newspaper, offering broader insight into\nmid-century Southern coverage. The dataset includes a version that has\nundergone an LLM-based text cleanup pipeline to reduce OCR noise, enhancing its\nsuitability for quantitative text analysis. Additionally, duplicate versions of\narticles are retained to enable analysis of editorial differences in language\nand framing across newspapers. Each article is tagged by wire service,\nfacilitating comparative studies of editorial patterns across agencies. This\nresource opens new avenues for research in computational social science,\ndigital humanities, and historical linguistics, providing a detailed\nperspective on how Southern newspapers relayed national and international news\nduring a transformative period in American history. The dataset will be made\navailable upon publication or request for research purposes.", "published": "2025-02-17 14:57:47", "link": "http://arxiv.org/abs/2502.11866v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VAQUUM: Are Vague Quantifiers Grounded in Visual Data?", "abstract": "Vague quantifiers such as \"a few\" and \"many\" are influenced by many\ncontextual factors, including how many objects are present in a given context.\nIn this work, we evaluate the extent to which vision-and-language models (VLMs)\nare compatible with humans when producing or judging the appropriateness of\nvague quantifiers in visual contexts. We release a novel dataset, VAQUUM,\ncontaining 20300 human ratings on quantified statements across a total of 1089\nimages. Using this dataset, we compare human judgments and VLM predictions\nusing three different evaluation methods. Our findings show that VLMs, like\nhumans, are influenced by object counts in vague quantifier use. However, we\nfind significant inconsistencies across models in different evaluation\nsettings, suggesting that judging and producing vague quantifiers rely on two\ndifferent processes.", "published": "2025-02-17 15:02:09", "link": "http://arxiv.org/abs/2502.11874v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revisiting Classification Taxonomy for Grammatical Errors", "abstract": "Grammatical error classification plays a crucial role in language learning\nsystems, but existing classification taxonomies often lack rigorous validation,\nleading to inconsistencies and unreliable feedback. In this paper, we revisit\nprevious classification taxonomies for grammatical errors by introducing a\nsystematic and qualitative evaluation framework. Our approach examines four\naspects of a taxonomy, i.e., exclusivity, coverage, balance, and usability.\nThen, we construct a high-quality grammatical error classification dataset\nannotated with multiple classification taxonomies and evaluate them grounding\non our proposed evaluation framework. Our experiments reveal the drawbacks of\nexisting taxonomies. Our contributions aim to improve the precision and\neffectiveness of error analysis, providing more understandable and actionable\nfeedback for language learners.", "published": "2025-02-17 15:16:44", "link": "http://arxiv.org/abs/2502.11890v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMRC: A Large-Scale Benchmark for Understanding Multimodal Large\n  Language Model in Real-World Conversation", "abstract": "Recent multimodal large language models (MLLMs) have demonstrated significant\npotential in open-ended conversation, generating more accurate and personalized\nresponses. However, their abilities to memorize, recall, and reason in\nsustained interactions within real-world scenarios remain underexplored. This\npaper introduces MMRC, a Multi-Modal Real-world Conversation benchmark for\nevaluating six core open-ended abilities of MLLMs: information extraction,\nmulti-turn reasoning, information update, image management, memory recall, and\nanswer refusal. With data collected from real-world scenarios, MMRC comprises\n5,120 conversations and 28,720 corresponding manually labeled questions, posing\na significant challenge to existing MLLMs. Evaluations on 20 MLLMs in MMRC\nindicate an accuracy drop during open-ended interactions. We identify four\ncommon failure patterns: long-term memory degradation, inadequacies in updating\nfactual knowledge, accumulated assumption of error propagation, and reluctance\nto say no. To mitigate these issues, we propose a simple yet effective\nNOTE-TAKING strategy, which can record key information from the conversation\nand remind the model during its responses, enhancing conversational\ncapabilities. Experiments across six MLLMs demonstrate significant performance\nimprovements.", "published": "2025-02-17 15:24:49", "link": "http://arxiv.org/abs/2502.11903v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion\n  Recognition Datasets for 28 Languages", "abstract": "People worldwide use language in subtle and complex ways to express emotions.\nWhile emotion recognition -- an umbrella term for several NLP tasks --\nsignificantly impacts different applications in NLP and other fields, most work\nin the area is focused on high-resource languages. Therefore, this has led to\nmajor disparities in research and proposed solutions, especially for\nlow-resource languages that suffer from the lack of high-quality datasets. In\nthis paper, we present BRIGHTER -- a collection of multilabeled\nemotion-annotated datasets in 28 different languages. BRIGHTER covers\npredominantly low-resource languages from Africa, Asia, Eastern Europe, and\nLatin America, with instances from various domains annotated by fluent\nspeakers. We describe the data collection and annotation processes and the\nchallenges of building these datasets. Then, we report different experimental\nresults for monolingual and crosslingual multi-label emotion identification, as\nwell as intensity-level emotion recognition. We investigate results with and\nwithout using LLMs and analyse the large variability in performance across\nlanguages and text domains. We show that BRIGHTER datasets are a step towards\nbridging the gap in text-based emotion recognition and discuss their impact and\nutility.", "published": "2025-02-17 15:39:50", "link": "http://arxiv.org/abs/2502.11926v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Representational Dissociation of Language and Arithmetic in Large\n  Language Models", "abstract": "The association between language and (non-linguistic) thinking ability in\nhumans has long been debated, and recently, neuroscientific evidence of brain\nactivity patterns has been considered. Such a scientific context naturally\nraises an interdisciplinary question -- what about such a language-thought\ndissociation in large language models (LLMs)? In this paper, as an initial\nforay, we explore this question by focusing on simple arithmetic skills (e.g.,\n$1+2=$ ?) as a thinking ability and analyzing the geometry of their encoding in\nLLMs' representation space. Our experiments with linear classifiers and cluster\nseparability tests demonstrate that simple arithmetic equations and general\nlanguage input are encoded in completely separated regions in LLMs' internal\nrepresentation space across all the layers, which is also supported with more\ncontrolled stimuli (e.g., spelled-out equations). These tentatively suggest\nthat arithmetic reasoning is mapped into a distinct region from general\nlanguage input, which is in line with the neuroscientific observations of human\nbrain activations, while we also point out their somewhat cognitively\nimplausible geometric properties.", "published": "2025-02-17 15:42:01", "link": "http://arxiv.org/abs/2502.11932v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Your Uncertainty Scores Detect Hallucinated Entity?", "abstract": "To mitigate the impact of hallucination nature of LLMs, many studies propose\ndetecting hallucinated generation through uncertainty estimation. However,\nthese approaches predominantly operate at the sentence or paragraph level,\nfailing to pinpoint specific spans or entities responsible for hallucinated\ncontent. This lack of granularity is especially problematic for long-form\noutputs that mix accurate and fabricated information. To address this\nlimitation, we explore entity-level hallucination detection. We propose a new\ndata set, HalluEntity, which annotates hallucination at the entity level. Based\non the dataset, we comprehensively evaluate uncertainty-based hallucination\ndetection approaches across 17 modern LLMs. Our experimental results show that\nuncertainty estimation approaches focusing on individual token probabilities\ntend to over-predict hallucinations, while context-aware methods show better\nbut still suboptimal performance. Through an in-depth qualitative study, we\nidentify relationships between hallucination tendencies and linguistic\nproperties and highlight important directions for future research.", "published": "2025-02-17 16:01:41", "link": "http://arxiv.org/abs/2502.11948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Text from Uniform Meaning Representation", "abstract": "Uniform Meaning Representation (UMR) is a recently developed graph-based\nsemantic representation, which expands on Abstract Meaning Representation (AMR)\nin a number of ways, in particular through the inclusion of document-level\ninformation and multilingual flexibility. In order to effectively adopt and\nleverage UMR for downstream tasks, efforts must be placed toward developing a\nUMR technological ecosystem. Though still limited amounts of UMR annotations\nhave been produced to date, in this work, we investigate the first approaches\nto producing text from multilingual UMR graphs: (1) a pipeline conversion of\nUMR to AMR, then using AMR-to-text generation models, (2) fine-tuning large\nlanguage models with UMR data, and (3) fine-tuning existing AMR-to-text\ngeneration models with UMR data. Our best performing model achieves a\nmultilingual BERTscore of 0.825 for English and 0.882 for Chinese when compared\nto the reference, which is a promising indication of the effectiveness of\nfine-tuning approaches for UMR-to-text generation with even limited amounts of\nUMR data.", "published": "2025-02-17 16:20:22", "link": "http://arxiv.org/abs/2502.11973v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dual-Perspective NLG Meta-Evaluation Framework with Automatic\n  Benchmark and Better Interpretability", "abstract": "In NLG meta-evaluation, evaluation metrics are typically assessed based on\ntheir consistency with humans. However, we identify some limitations in\ntraditional NLG meta-evaluation approaches, such as issues in handling human\nratings and ambiguous selections of correlation measures, which undermine the\neffectiveness of meta-evaluation. In this work, we propose a dual-perspective\nNLG meta-evaluation framework that focuses on different evaluation\ncapabilities, thereby providing better interpretability. In addition, we\nintroduce a method of automatically constructing the corresponding benchmarks\nwithout requiring new human annotations. Furthermore, we conduct experiments\nwith 16 representative LLMs as the evaluators based on our proposed framework,\ncomprehensively analyzing their evaluation performance from different\nperspectives.", "published": "2025-02-17 17:22:49", "link": "http://arxiv.org/abs/2502.12052v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Designing Role Vectors to Improve LLM Inference Behaviour", "abstract": "The influence of personas on Large Language Models (LLMs) has been widely\nstudied, yet their direct impact on performance remains uncertain. This work\nexplores a novel approach to guiding LLM behaviour through role vectors, an\nalternative to persona-based prompting. We construct 29 role vectors derived\nfrom model activations and evaluate their impact on benchmark performance\nacross multiple domains. Our analysis investigates whether these vectors can\neffectively steer models toward domain-specific expertise. We measure two key\ninterventions: (i) activation addition, which reinforces role-specific\ndirections, and (ii) directional ablation, which removes them. Results on\nwell-established benchmarks indicate that role vectors do, in fact, influence\nmodel behaviour, improving task performance in relevant domains while\nmarginally affecting unrelated tasks. This, in turn, suggests that manipulating\ninternal model representations has a greater impact on outcomes than\npersona-based prompting.", "published": "2025-02-17 17:24:37", "link": "http://arxiv.org/abs/2502.12055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Simulate Social Media Engagement? A Study on Action-Guided\n  Response Generation", "abstract": "Social media enables dynamic user engagement with trending topics, and recent\nresearch has explored the potential of large language models (LLMs) for\nresponse generation. While some studies investigate LLMs as agents for\nsimulating user behavior on social media, their focus remains on practical\nviability and scalability rather than a deeper understanding of how well LLM\naligns with human behavior. This paper analyzes LLMs' ability to simulate\nsocial media engagement through action guided response generation, where a\nmodel first predicts a user's most likely engagement action-retweet, quote, or\nrewrite-towards a trending post before generating a personalized response\nconditioned on the predicted action. We benchmark GPT-4o-mini, O1-mini, and\nDeepSeek-R1 in social media engagement simulation regarding a major societal\nevent discussed on X. Our findings reveal that zero-shot LLMs underperform BERT\nin action prediction, while few-shot prompting initially degrades the\nprediction accuracy of LLMs with limited examples. However, in response\ngeneration, few-shot LLMs achieve stronger semantic alignment with ground truth\nposts.", "published": "2025-02-17 17:43:08", "link": "http://arxiv.org/abs/2502.12073v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VLM$^2$-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit\n  Matching Visual Cues", "abstract": "Visually linking matching cues is a crucial ability in daily life, such as\nidentifying the same person in multiple photos based on their cues, even\nwithout knowing who they are. Despite the extensive knowledge that\nvision-language models (VLMs) possess, it remains largely unexplored whether\nthey are capable of performing this fundamental task. To address this, we\nintroduce VLM$^2$-Bench, a benchmark designed to assess whether VLMs can\nVisually Link Matching cues, with 9 subtasks and over 3,000 test cases.\nComprehensive evaluation across eight open-source VLMs and GPT-4o, along with\nfurther analysis of various language-side and vision-side prompting methods,\nleads to a total of eight key findings. We identify critical challenges in\nmodels' ability to link visual cues, highlighting a significant performance gap\nwhere even GPT-4o lags 34.80% behind humans. Based on these insights, we\nadvocate for (i) enhancing core visual capabilities to improve adaptability and\nreduce reliance on prior knowledge, (ii) establishing clearer principles for\nintegrating language-based reasoning in vision-centric tasks to prevent\nunnecessary biases, and (iii) shifting vision-text training paradigms toward\nfostering models' ability to independently structure and infer relationships\namong visual cues.", "published": "2025-02-17 17:57:50", "link": "http://arxiv.org/abs/2502.12084v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RA-MTR: A Retrieval Augmented Multi-Task Reader based Approach for\n  Inspirational Quote Extraction from Long Documents", "abstract": "Inspirational quotes from famous individuals are often used to convey\nthoughts in news articles, essays, and everyday conversations. In this paper,\nwe propose a novel context-based quote extraction system that aims to extract\nthe most relevant quote from a long text. We formulate this quote extraction as\nan open domain question answering problem first by employing a vector-store\nbased retriever and then applying a multi-task reader. We curate three\ncontext-based quote extraction datasets and introduce a novel multi-task\nframework RA-MTR that improves the state-of-the-art performance, achieving a\nmaximum improvement of 5.08% in BoW F1-score.", "published": "2025-02-17 18:46:46", "link": "http://arxiv.org/abs/2502.12124v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs", "abstract": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to\nsolve complex reasoning tasks by generating intermediate reasoning steps.\nHowever, most existing approaches focus on hard token decoding, which\nconstrains reasoning within the discrete vocabulary space and may not always be\noptimal. While recent efforts explore continuous-space reasoning, they often\nsuffer from catastrophic forgetting, limiting their applicability to\nstate-of-the-art LLMs that already perform well in zero-shot settings with a\nproper instruction. To address this challenge, we propose a novel approach for\ncontinuous-space reasoning that does not require modifying the underlying LLM.\nSpecifically, we employ a lightweight assistant model to generate\ninstance-specific soft thought tokens speculatively as the initial chain of\nthoughts, which are then mapped into the LLM's representation space via a\nprojection module. Experimental results on five reasoning benchmarks\ndemonstrate that our method enhances LLM reasoning performance through\nsupervised, parameter-efficient fine-tuning.", "published": "2025-02-17 18:52:29", "link": "http://arxiv.org/abs/2502.12134v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Idiosyncrasies in Large Language Models", "abstract": "In this work, we unveil and study idiosyncrasies in Large Language Models\n(LLMs) -- unique patterns in their outputs that can be used to distinguish the\nmodels. To do so, we consider a simple classification task: given a particular\ntext output, the objective is to predict the source LLM that generates the\ntext. We evaluate this synthetic task across various groups of LLMs and find\nthat simply fine-tuning existing text embedding models on LLM-generated texts\nyields excellent classification accuracy. Notably, we achieve 97.1% accuracy on\nheld-out validation data in the five-way classification problem involving\nChatGPT, Claude, Grok, Gemini, and DeepSeek. Our further investigation reveals\nthat these idiosyncrasies are rooted in word-level distributions. These\npatterns persist even when the texts are rewritten, translated, or summarized\nby an external LLM, suggesting that they are also encoded in the semantic\ncontent. Additionally, we leverage LLM as judges to generate detailed,\nopen-ended descriptions of each model's idiosyncrasies. Finally, we discuss the\nbroader implications of our findings, particularly for training on synthetic\ndata and inferring model similarity. Code is available at\nhttps://github.com/locuslab/llm-idiosyncrasies.", "published": "2025-02-17 18:59:02", "link": "http://arxiv.org/abs/2502.12150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Story Grammar Semantic Matching for Literary Study", "abstract": "In Natural Language Processing (NLP), semantic matching algorithms have\ntraditionally relied on the feature of word co-occurrence to measure semantic\nsimilarity. While this feature approach has proven valuable in many contexts,\nits simplistic nature limits its analytical and explanatory power when used to\nunderstand literary texts. To address these limitations, we propose a more\ntransparent approach that makes use of story structure and related elements.\nUsing a BERT language model pipeline, we label prose and epic poetry with story\nelement labels and perform semantic matching by only considering these labels\nas features. This new method, Story Grammar Semantic Matching, guides literary\nscholars to allusions and other semantic similarities across texts in a way\nthat allows for characterizing patterns and literary technique.", "published": "2025-02-17 19:20:39", "link": "http://arxiv.org/abs/2502.12276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Step-by-step Reasoning Traces: A Survey", "abstract": "Step-by-step reasoning is widely used to enhance the reasoning ability of\nlarge language models (LLMs) in complex problems. Evaluating the quality of\nreasoning traces is crucial for understanding and improving LLM reasoning.\nHowever, the evaluation criteria remain highly unstandardized, leading to\nfragmented efforts in developing metrics and meta-evaluation benchmarks. To\naddress this gap, this survey provides a comprehensive overview of step-by-step\nreasoning evaluation, proposing a taxonomy of evaluation criteria with four\ntop-level categories (groundedness, validity, coherence, and utility). We then\ncategorize metrics based on their implementations, survey which metrics are\nused for assessing each criterion, and explore whether evaluator models can\ntransfer across different criteria. Finally, we identify key directions for\nfuture research.", "published": "2025-02-17 19:58:31", "link": "http://arxiv.org/abs/2502.12289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMOL: Professionally translated parallel data for 115 under-represented\n  languages", "abstract": "We open-source SMOL (Set of Maximal Overall Leverage), a suite of training\ndata to unlock translation for low-resource languages (LRLs). SMOL has been\ntranslated into 115 under-resourced languages, including many for which there\nexist no previous public resources, for a total of 6.1M translated tokens. SMOL\ncomprises two sub-datasets, each carefully chosen for maximum impact given its\nsize: SMOL-Sent, a set of sentences chosen for broad unique token coverage, and\nSMOL-Doc, a document-level source focusing on a broad topic coverage. They join\nthe already released GATITOS for a trifecta of paragraph, sentence, and\ntoken-level content. We demonstrate that using SMOL to prompt or fine-tune\nLarge Language Models yields robust ChrF improvements. In addition to\ntranslation, we provide factuality ratings and rationales for all documents in\nSMOL-Doc, yielding the first factuality datasets for most of these languages.", "published": "2025-02-17 20:22:08", "link": "http://arxiv.org/abs/2502.12301v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Dense to Dynamic: Token-Difficulty Driven MoEfication of\n  Pre-Trained LLMs", "abstract": "Training large language models (LLMs) for different inference constraints is\ncomputationally expensive, limiting control over efficiency-accuracy\ntrade-offs. Moreover, once trained, these models typically process tokens\nuniformly, regardless of their complexity, leading to static and inflexible\nbehavior. In this paper, we introduce a post-training optimization framework,\nDynaMoE, that adapts a pre-trained dense LLM to a token-difficulty-driven\nMixture-of-Experts model with minimal fine-tuning cost. This adaptation makes\nthe model dynamic, with sensitivity control to customize the balance between\nefficiency and accuracy. DynaMoE features a token-difficulty-aware router that\npredicts the difficulty of tokens and directs them to the appropriate\nsub-networks or experts, enabling larger experts to handle more complex tokens\nand smaller experts to process simpler ones. Our experiments demonstrate that\nDynaMoE can generate a range of adaptive model variants of the existing trained\nLLM with a single fine-tuning step, utilizing only $10B$ tokens, a minimal cost\ncompared to the base model's training. Each variant offers distinct trade-offs\nbetween accuracy and performance. Compared to the baseline post-training\noptimization framework, Flextron, our method achieves similar aggregated\naccuracy across downstream tasks, despite using only $\\frac{1}{9}\\text{th}$ of\ntheir fine-tuning cost.", "published": "2025-02-17 21:12:57", "link": "http://arxiv.org/abs/2502.12325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ConFit v2: Improving Resume-Job Matching using Hypothetical Resume\n  Embedding and Runner-Up Hard-Negative Mining", "abstract": "A reliable resume-job matching system helps a company recommend suitable\ncandidates from a pool of resumes and helps a job seeker find relevant jobs\nfrom a list of job posts. However, since job seekers apply only to a few jobs,\ninteraction labels in resume-job datasets are sparse. We introduce ConFit v2,\nan improvement over ConFit to tackle this sparsity problem. We propose two\ntechniques to enhance the encoder's contrastive training process: augmenting\njob data with hypothetical reference resume generated by a large language\nmodel; and creating high-quality hard negatives from unlabeled resume/job pairs\nusing a novel hard-negative mining strategy. We evaluate ConFit v2 on two\nreal-world datasets and demonstrate that it outperforms ConFit and prior\nmethods (including BM25 and OpenAI text-embedding-003), achieving an average\nabsolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking\nand resume-ranking tasks.", "published": "2025-02-17 22:56:42", "link": "http://arxiv.org/abs/2502.12361v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UltraGen: Extremely Fine-grained Controllable Generation via Attribute\n  Reconstruction and Global Preference Optimization", "abstract": "Fine granularity is an essential requirement for controllable text\ngeneration, which has seen rapid growth with the ability of LLMs. However,\nexisting methods focus mainly on a small set of attributes like 3 to 5, and\ntheir performance degrades significantly when the number of attributes\nincreases to the next order of magnitude. To address this challenge, we propose\na novel zero-shot approach for extremely fine-grained controllable generation\n(EFCG), proposing auto-reconstruction (AR) and global preference optimization\n(GPO). In the AR phase, we leverage LLMs to extract soft attributes (e.g.,\nEmphasis on simplicity and minimalism in design) from raw texts, and combine\nthem with programmatically derived hard attributes (e.g., The text should be\nbetween 300 and 400 words) to construct massive (around 45) multi-attribute\nrequirements, which guide the fine-grained text reconstruction process under\nweak supervision. In the GPO phase, we apply direct preference optimization\n(DPO) to refine text generation under diverse attribute combinations, enabling\nefficient exploration of the global combination space. Additionally, we\nintroduce an efficient attribute sampling strategy to identify and correct\npotentially erroneous attributes, further improving global optimization. Our\nframework significantly improves the constraint satisfaction rate (CSR) and\ntext quality for EFCG by mitigating position bias and alleviating attention\ndilution.", "published": "2025-02-17 23:28:58", "link": "http://arxiv.org/abs/2502.12375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pragmatics in the Era of Large Language Models: A Survey on Datasets,\n  Evaluation, Opportunities and Challenges", "abstract": "Understanding pragmatics-the use of language in context-is crucial for\ndeveloping NLP systems capable of interpreting nuanced language use. Despite\nrecent advances in language technologies, including large language models,\nevaluating their ability to handle pragmatic phenomena such as implicatures and\nreferences remains challenging. To advance pragmatic abilities in models, it is\nessential to understand current evaluation trends and identify existing\nlimitations. In this survey, we provide a comprehensive review of resources\ndesigned for evaluating pragmatic capabilities in NLP, categorizing datasets by\nthe pragmatics phenomena they address. We analyze task designs, data collection\nmethods, evaluation approaches, and their relevance to real-world applications.\nBy examining these resources in the context of modern language models, we\nhighlight emerging trends, challenges, and gaps in existing benchmarks. Our\nsurvey aims to clarify the landscape of pragmatic evaluation and guide the\ndevelopment of more comprehensive and targeted benchmarks, ultimately\ncontributing to more nuanced and context-aware NLP models.", "published": "2025-02-17 23:31:38", "link": "http://arxiv.org/abs/2502.12378v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Are They Filtering Out? A Survey of Filtering Strategies for Harm\n  Reduction in Pretraining Datasets", "abstract": "Data filtering strategies are a crucial component to develop safe Large\nLanguage Models (LLM), since they support the removal of harmful contents from\npretraining datasets. There is a lack of research on the actual impact of these\nstrategies on vulnerable groups to discrimination, though, and their\neffectiveness has not been yet systematically addressed. In this paper we\npresent a benchmark study of data filtering strategies for harm reduction aimed\nat providing a systematic overview on these approaches. We survey 55 technical\nreports of English LMs and LLMs to identify the existing filtering strategies\nin literature and implement an experimental setting to test their impact\nagainst vulnerable groups. Our results show that the positive impact that\nstrategies have in reducing harmful contents from documents has the side effect\nof increasing the underrepresentation of vulnerable groups to discrimination in\ndatasets.", "published": "2025-02-17 13:10:57", "link": "http://arxiv.org/abs/2503.05721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "System Message Generation for User Preferences using Open-Source Models", "abstract": "System messages play a crucial role in interactions with large language\nmodels (LLMs), often serving as prompts to initiate conversations. Through\nsystem messages, users can assign specific roles, perform intended tasks,\nincorporate background information, specify various output formats and\ncommunication styles. Despite such versatility, publicly available data are\noften lack system messages and subject to strict license constraints in the\nindustry field. Manual labeling of publicly available data with system messages\nthat align with user instructions demands significant resources. In view of\nsuch challenges, our work introduces SysGen, a pipeline for generating system\nmessages with better aligned assistant responses from the supervised\nfine-tuning dataset without system messages. Training on SysGen data has\ndemonstrated substantial improvements in the alignment of model responses with\nsystem messages and user instructions, as demonstrated across various\nopen-source models on the Multifacet benchmark, while maintaining minimal\nimpact on other unseen benchmarks such as Open LLM Leaderboard 2. Our\nqualitative analysis highlights the importance of diverse system messages to\nensure better adaptability across different contexts.", "published": "2025-02-17 01:05:31", "link": "http://arxiv.org/abs/2502.11330v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GeoDANO: Geometric VLM with Domain Agnostic Vision Encoder", "abstract": "We introduce GeoDANO, a geometric vision-language model (VLM) with a\ndomain-agnostic vision encoder, for solving plane geometry problems. Although\nVLMs have been employed for solving geometry problems, their ability to\nrecognize geometric features remains insufficiently analyzed. To address this\ngap, we propose a benchmark that evaluates the recognition of visual geometric\nfeatures, including primitives such as dots and lines, and relations such as\northogonality. Our preliminary study shows that vision encoders often used in\ngeneral-purpose VLMs, e.g., OpenCLIP, fail to detect these features and\nstruggle to generalize across domains. We develop GeoCLIP, a CLIP based model\ntrained on synthetic geometric diagram-caption pairs to overcome the\nlimitation. Benchmark results show that GeoCLIP outperforms existing vision\nencoders in recognizing geometric features. We then propose our VLM, GeoDANO,\nwhich augments GeoCLIP with a domain adaptation strategy for unseen diagram\nstyles. GeoDANO outperforms specialized methods for plane geometry problems and\nGPT-4o on MathVerse.", "published": "2025-02-17 02:18:33", "link": "http://arxiv.org/abs/2502.11360v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case\n  Study of L2 Graduate-Level Academic English Writing", "abstract": "The paper explores the performance of LLMs in the context of\nmulti-dimensional analytic writing assessments, i.e. their ability to provide\nboth scores and comments based on multiple assessment criteria. Using a corpus\nof literature reviews written by L2 graduate students and assessed by human\nexperts against 9 analytic criteria, we prompt several popular LLMs to perform\nthe same task under various conditions. To evaluate the quality of feedback\ncomments, we apply a novel feedback comment quality evaluation framework. This\nframework is interpretable, cost-efficient, scalable, and reproducible,\ncompared to existing methods that rely on manual judgments. We find that LLMs\ncan generate reasonably good and generally reliable multi-dimensional analytic\nassessments. We release our corpus for reproducibility.", "published": "2025-02-17 02:31:56", "link": "http://arxiv.org/abs/2502.11368v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Counterfactual-Consistency Prompting for Relative Temporal Understanding\n  in Large Language Models", "abstract": "Despite the advanced capabilities of large language models (LLMs), their\ntemporal reasoning ability remains underdeveloped. Prior works have highlighted\nthis limitation, particularly in maintaining temporal consistency when\nunderstanding events. For example, models often confuse mutually exclusive\ntemporal relations like ``before'' and ``after'' between events and make\ninconsistent predictions. In this work, we tackle the issue of temporal\ninconsistency in LLMs by proposing a novel counterfactual prompting approach.\nOur method generates counterfactual questions and enforces collective\nconstraints, enhancing the model's consistency. We evaluate our method on\nmultiple datasets, demonstrating significant improvements in event ordering for\nexplicit and implicit events and temporal commonsense understanding by\neffectively addressing temporal inconsistencies.", "published": "2025-02-17 04:37:07", "link": "http://arxiv.org/abs/2502.11425v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do we Really Need Visual Instructions? Towards Visual Instruction-Free\n  Fine-tuning for Large Vision-Language Models", "abstract": "Visual instruction tuning has become the predominant technology in eliciting\nthe multimodal task-solving capabilities of large vision-language models\n(LVLMs). Despite the success, as visual instructions require images as the\ninput, it would leave the gap in inheriting the task-solving capabilities from\nthe backbone LLMs, and make it costly to collect a large-scale dataset. To\naddress it, we propose ViFT, a visual instruction-free fine-tuning framework\nfor LVLMs. In ViFT, we only require the text-only instructions and image\ncaption data during training, to separately learn the task-solving and visual\nperception abilities. During inference, we extract and combine the\nrepresentations of the text and image inputs, for fusing the two abilities to\nfulfill multimodal tasks. Experimental results demonstrate that ViFT can\nachieve state-of-the-art performance on several visual reasoning and visual\ninstruction following benchmarks, with rather less training data. Our code and\ndata will be publicly released.", "published": "2025-02-17 04:38:12", "link": "http://arxiv.org/abs/2502.11427v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Aligning Sentence Simplification with ESL Learner's Proficiency for\n  Language Acquisition", "abstract": "Text simplification is crucial for improving accessibility and comprehension\nfor English as a Second Language (ESL) learners. This study goes a step further\nand aims to facilitate ESL learners' language acquisition by simplification.\nSpecifically, we propose simplifying complex sentences to appropriate levels\nfor learners while also increasing vocabulary coverage of the target level in\nthe simplifications. We achieve this without a parallel corpus by conducting\nreinforcement learning on a large language model. Our method employs\ntoken-level and sentence-level rewards, and iteratively trains the model on its\nself-generated outputs to guide the model to search for simplification\nhypotheses that satisfy the target attributes. Experiment results on CEFR-SP\nand TurkCorpus datasets show that the proposed method can effectively increase\nthe frequency and diversity of vocabulary of the target level by more than\n$20\\%$ compared to baseline models, while maintaining high simplification\nquality.", "published": "2025-02-17 05:32:56", "link": "http://arxiv.org/abs/2502.11457v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UnitCoder: Scalable Iterative Code Synthesis with Unit Test Guidance", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, yet code generation remains a major challenge. Current\napproaches for obtaining high-quality code data primarily focus on (i)\ncollecting large-scale pre-training data and (ii) synthesizing instruction data\nthrough prompt engineering with powerful models. While pre-training data faces\nquality consistency issues, instruction-based synthesis suffers from limited\ninstruction diversity and inherent biases of LLMs. To address this gap, we\nintroduce UnitCoder, a systematic pipeline leveraging model-generated unit\ntests to both guide and validate the code generation process. Combined with\nlarge-scale package-based retrieval from pre-training corpus, we generate a\ndataset of 500K+ verifiable programs containing diverse API calls. Evaluations\non multiple Python benchmarks (BigCodeBench, HumanEval, MBPP) demonstrate that\nmodels fine-tuned on our synthetic data exhibit consistent performance\nimprovements. Notably, Llama3.1-8B and InternLM2.5-7B improve from 31\\% and\n28\\% to 40\\% and 39\\% success rates on BigCodeBench, respectively. Our work\npresents a scalable approach that leverages model-generated unit tests to guide\nthe synthesis of high-quality code data from pre-training corpora,\ndemonstrating the potential for producing diverse and high-quality\npost-training data at scale. All code and data will be released\n(https://github.com).", "published": "2025-02-17 05:37:02", "link": "http://arxiv.org/abs/2502.11460v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language\n  for Knowledge Graph Completion", "abstract": "Knowledge Graph Completion (KGC), which aims to infer missing or incomplete\nfacts, is a crucial task for KGs. However, integrating the vital structural\ninformation of KGs into Large Language Models (LLMs) and outputting predictions\ndeterministically remains challenging. To address this, we propose a new method\ncalled GLTW, which encodes the structural information of KGs and merges it with\nLLMs to enhance KGC performance. Specifically, we introduce an improved Graph\nTransformer (iGT) that effectively encodes subgraphs with both local and global\nstructural information and inherits the characteristics of language model,\nbypassing training from scratch. Also, we develop a subgraph-based\nmulti-classification training objective, using all entities within KG as\nclassification objects, to boost learning efficiency.Importantly, we combine\niGT with an LLM that takes KG language prompts as input.Our extensive\nexperiments on various KG datasets show that GLTW achieves significant\nperformance gains compared to SOTA baselines.", "published": "2025-02-17 06:02:59", "link": "http://arxiv.org/abs/2502.11471v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on\n  Knowledge Graph Question Answering", "abstract": "Large language models (LLMs) have shown remarkable capabilities in natural\nlanguage processing. However, in knowledge graph question answering tasks\n(KGQA), there remains the issue of answering questions that require multi-hop\nreasoning. Existing methods rely on entity vector matching, but the purpose of\nthe question is abstract and difficult to match with specific entities. As a\nresult, it is difficult to establish reasoning paths to the purpose, which\nleads to information loss and redundancy. To address this issue, inspired by\nhuman reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a\nnovel framework that constructs reasoning paths from purposes back to\nconditions. ORT operates in three key phases: (1) using LLM to extract purpose\nlabels and condition labels, (2) constructing label reasoning paths based on\nthe KG ontology, and (3) using the label reasoning paths to guide knowledge\nretrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves\nstate-of-the-art performance and significantly enhances the capability of LLMs\nfor KGQA.", "published": "2025-02-17 06:53:15", "link": "http://arxiv.org/abs/2502.11491v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Stop Looking for Important Tokens in Multimodal Language Models:\n  Duplication Matters More", "abstract": "Vision tokens in multimodal large language models often dominate huge\ncomputational overhead due to their excessive length compared to linguistic\nmodality. Abundant recent methods aim to solve this problem with token pruning,\nwhich first defines an importance criterion for tokens and then prunes the\nunimportant vision tokens during inference. However, in this paper, we show\nthat the importance is not an ideal indicator to decide whether a token should\nbe pruned. Surprisingly, it usually results in inferior performance than random\ntoken pruning and leading to incompatibility to efficient attention computation\noperators.Instead, we propose DART (Duplication-Aware Reduction of Tokens),\nwhich prunes tokens based on its duplication with other tokens, leading to\nsignificant and training-free acceleration. Concretely, DART selects a small\nsubset of pivot tokens and then retains the tokens with low duplication to the\npivots, ensuring minimal information loss during token pruning. Experiments\ndemonstrate that DART can prune 88.9% vision tokens while maintaining\ncomparable performance, leading to a 1.99$\\times$ and 2.99$\\times$ speed-up in\ntotal time and prefilling stage, respectively, with good compatibility to\nefficient attention operators. Our codes are available at\nhttps://github.com/ZichenWen1/DART.", "published": "2025-02-17 06:56:28", "link": "http://arxiv.org/abs/2502.11494v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?", "abstract": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.", "published": "2025-02-17 07:05:36", "link": "http://arxiv.org/abs/2502.11501v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Chinese Spelling Correction: A Comprehensive Survey of Progress,\n  Challenges, and Opportunities", "abstract": "Chinese Spelling Correction (CSC) is a critical task in natural language\nprocessing, aimed at detecting and correcting spelling errors in Chinese text.\nThis survey provides a comprehensive overview of CSC, tracing its evolution\nfrom pre-trained language models to large language models, and critically\nanalyzing their respective strengths and weaknesses in this domain. Moreover,\nwe further present a detailed examination of existing benchmark datasets,\nhighlighting their inherent challenges and limitations. Finally, we propose\npromising future research directions, particularly focusing on leveraging the\npotential of LLMs and their reasoning capabilities for improved CSC\nperformance. To the best of our knowledge, this is the first comprehensive\nsurvey dedicated to the field of CSC. We believe this work will serve as a\nvaluable resource for researchers, fostering a deeper understanding of the\nfield and inspiring future advancements.", "published": "2025-02-17 07:17:27", "link": "http://arxiv.org/abs/2502.11508v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MuSC: Improving Complex Instruction Following with Multi-granularity\n  Self-Contrastive Training", "abstract": "Complex instruction-following with elaborate constraints is imperative for\nLarge Language Models (LLMs). While existing methods have constructed data for\ncomplex instruction alignment, they all rely on a more advanced model,\nespecially GPT-4, limiting their application. In this paper, we propose a\nMulti-granularity Self-Contrastive Training (MuSC) framework, to improve the\ncomplex instruction alignment without relying on a stronger model. Our method\nis conducted on both coarse and fine granularity. On coarse-granularity, we\nconstruct constraint-aware preference data based on instruction decomposition\nand recombination. On fine-granularity, we perform token-aware preference\noptimization with dynamic token-level supervision. Our method is evaluated on\nopen-sourced models, and experiment results show our method achieves\nsignificant improvement on both complex and general instruction-following\nbenchmarks, surpassing previous self-alignment methods.", "published": "2025-02-17 08:12:49", "link": "http://arxiv.org/abs/2502.11541v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Auto-Search and Refinement: An Automated Framework for Gender Bias\n  Mitigation in Large Language Models", "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances\nnatural language processing capabilities but risks encoding social biases,\nparticularly gender bias. While parameter-modification methods like fine-tuning\nmitigate bias, they are resource-intensive, unsuitable for closed-source\nmodels, and lack adaptability to evolving societal norms. Instruction-based\napproaches offer flexibility but often compromise task performance. To address\nthese limitations, we propose $\\textit{FaIRMaker}$, an automated and\nmodel-independent framework that employs an $\\textbf{auto-search and\nrefinement}$ paradigm to adaptively generate Fairwords, which act as\ninstructions integrated into input queries to reduce gender bias and enhance\nresponse quality. Extensive experiments demonstrate that $\\textit{FaIRMaker}$\nautomatically searches for and dynamically refines Fairwords, effectively\nmitigating gender bias while preserving task integrity and ensuring\ncompatibility with both API-based and open-source LLMs.", "published": "2025-02-17 08:44:04", "link": "http://arxiv.org/abs/2502.11559v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InfiR : Crafting Effective Small Language Models and Multimodal Small\n  Language Models in Reasoning", "abstract": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave made significant advancements in reasoning capabilities. However, they\nstill face challenges such as high computational demands and privacy concerns.\nThis paper focuses on developing efficient Small Language Models (SLMs) and\nMultimodal Small Language Models (MSLMs) that retain competitive reasoning\nabilities. We introduce a novel training pipeline that enhances reasoning\ncapabilities and facilitates deployment on edge devices, achieving\nstate-of-the-art performance while minimizing development costs. \\InfR~ aims to\nadvance AI systems by improving reasoning, reducing adoption barriers, and\naddressing privacy concerns through smaller model sizes. Resources are\navailable at https://github. com/Reallm-Labs/InfiR.", "published": "2025-02-17 09:07:32", "link": "http://arxiv.org/abs/2502.11573v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for\n  Evaluating LLM Performance", "abstract": "Large Language Models (LLMs) have made significant strides in natural\nlanguage generation but often face challenges in tasks requiring precise\ncalculations and structural analysis. This paper investigates the performance\nof state-of-the-art LLMs on language complexity measurement tasks, through the\ncomputation of the LIX readability metric and Average Dependency Distance\n(ADD). Using Swedish high school and university-level essays, we evaluate the\nmodels' abilities to compute LIX scores and perform dependency parsing,\ncomparing their results to established ground truths. Our findings reveal that\nwhile all models demonstrate some capacity for these tasks, ChatGPT-o1-mini\nperforms most consistently, achieving the highest accuracy in both LIX\ncomputation and dependency parsing. Additionally, we observe a strong\nsignificant correlation -0.875 p 0.026 (N=6) between the models' accuracy in\ncomputing LIX and their overall performance on the Massive Multitask Language\nUnderstanding (MMLU) benchmark. These results suggest that language complexity\nmeasurement abilities can serve as a noisy zero-shot proxies for assessing the\ngeneral capabilities of LLMs, providing a practical method for model evaluation\nwithout the need for extensive benchmarking datasets.", "published": "2025-02-17 09:09:58", "link": "http://arxiv.org/abs/2502.11578v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DR.GAP: Mitigating Bias in Large Language Models using Gender-Aware\n  Prompting with Demonstration and Reasoning", "abstract": "Large Language Models (LLMs) exhibit strong natural language processing\ncapabilities but also inherit and amplify societal biases, including gender\nbias, raising fairness concerns. Existing debiasing methods face significant\nlimitations: parameter tuning requires access to model weights, prompt-based\napproaches often degrade model utility, and optimization-based techniques lack\ngeneralizability. To address these challenges, we propose DR.GAP (Demonstration\nand Reasoning for Gender-Aware Prompting), an automated and model-agnostic\napproach that mitigates gender bias while preserving model performance. DR.GAP\nselects bias-revealing examples and generates structured reasoning to guide\nmodels toward more impartial responses. Extensive experiments on coreference\nresolution and QA tasks across multiple LLMs (GPT-3.5, Llama3, and\nLlama2-Alpaca) demonstrate its effectiveness, generalization ability, and\nrobustness. DR.GAP can generalize to vision-language models (VLMs), achieving\nsignificant bias reduction.", "published": "2025-02-17 09:43:36", "link": "http://arxiv.org/abs/2502.11603v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Gender Stereotypes and Biases in Automated Translation from\n  English to Italian using Similarity Networks", "abstract": "This paper is a collaborative effort between Linguistics, Law, and Computer\nScience to evaluate stereotypes and biases in automated translation systems. We\nadvocate gender-neutral translation as a means to promote gender inclusion and\nimprove the objectivity of machine translation. Our approach focuses on\nidentifying gender bias in English-to-Italian translations. First, we define\ngender bias following human rights law and linguistics literature. Then we\nproceed by identifying gender-specific terms such as she/lei and he/lui as key\nelements. We then evaluate the cosine similarity between these target terms and\nothers in the dataset to reveal the model's perception of semantic relations.\nUsing numerical features, we effectively evaluate the intensity and direction\nof the bias. Our findings provide tangible insights for developing and training\ngender-neutral translation algorithms.", "published": "2025-02-17 09:55:32", "link": "http://arxiv.org/abs/2502.11611v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Is Human-Like Text Liked by Humans? Multilingual Human Detection and\n  Preference Against AI", "abstract": "Prior studies have shown that distinguishing text generated by large language\nmodels (LLMs) from human-written one is highly challenging, and often no better\nthan random guessing. To verify the generalizability of this finding across\nlanguages and domains, we perform an extensive case study to identify the upper\nbound of human detection accuracy. Across 16 datasets covering 9 languages and\n9 domains, 19 annotators achieved an average detection accuracy of 87.6%, thus\nchallenging previous conclusions. We find that major gaps between human and\nmachine text lie in concreteness, cultural nuances, and diversity. Prompting by\nexplicitly explaining the distinctions in the prompts can partially bridge the\ngaps in over 50% of the cases. However, we also find that humans do not always\nprefer human-written text, particularly when they cannot clearly identify its\nsource.", "published": "2025-02-17 09:56:46", "link": "http://arxiv.org/abs/2502.11614v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Uncovering the Impact of Chain-of-Thought Reasoning for Direct\n  Preference Optimization: Lessons from Text-to-SQL", "abstract": "Direct Preference Optimization (DPO) has proven effective in complex\nreasoning tasks like math word problems and code generation. However, when\napplied to Text-to-SQL datasets, it often fails to improve performance and can\neven degrade it. Our investigation reveals the root cause: unlike math and code\ntasks, which naturally integrate Chain-of-Thought (CoT) reasoning with DPO,\nText-to-SQL datasets typically include only final answers (gold SQL queries)\nwithout detailed CoT solutions. By augmenting Text-to-SQL datasets with\nsynthetic CoT solutions, we achieve, for the first time, consistent and\nsignificant performance improvements using DPO. Our analysis shows that CoT\nreasoning is crucial for unlocking DPO's potential, as it mitigates reward\nhacking, strengthens discriminative capabilities, and improves scalability.\nThese findings offer valuable insights for building more robust Text-to-SQL\nmodels. To support further research, we publicly release the code and\nCoT-enhanced datasets.", "published": "2025-02-17 10:47:17", "link": "http://arxiv.org/abs/2502.11656v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Exploring LLM-based Student Simulation for Metacognitive Cultivation", "abstract": "Metacognitive education plays a crucial role in cultivating students'\nself-regulation and reflective thinking, providing essential support for those\nwith learning difficulties through academic advising. Simulating students with\ninsufficient learning capabilities using large language models offers a\npromising approach to refining pedagogical methods without ethical concerns.\nHowever, existing simulations often fail to authentically represent students'\nlearning struggles and face challenges in evaluation due to the lack of\nreliable metrics and ethical constraints in data collection. To address these\nissues, we propose a pipeline for automatically generating and filtering\nhigh-quality simulated student agents. Our approach leverages a two-round\nautomated scoring system validated by human experts and employs a score\npropagation module to obtain more consistent scores across the student graph.\nExperimental results demonstrate that our pipeline efficiently identifies\nhigh-quality student agents, and we discuss the traits that influence the\nsimulation's effectiveness. By simulating students with varying degrees of\nlearning difficulties, our work paves the way for broader applications in\npersonalized learning and educational assessment.", "published": "2025-02-17 11:12:47", "link": "http://arxiv.org/abs/2502.11678v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars", "abstract": "Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE.", "published": "2025-02-17 11:16:19", "link": "http://arxiv.org/abs/2502.11681v4", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps\n  through Fill-in-the-Middle Task", "abstract": "Mathematical reasoning represents a critical frontier in advancing large\nlanguage models (LLMs). While step-by-step approaches have emerged as the\ndominant paradigm for mathematical problem-solving in LLMs, the quality of\nreasoning steps in training data fundamentally constrains the performance of\nthe models. Recent studies has demonstrated that more detailed intermediate\nsteps can enhance model performance, yet existing methods for step expansion\neither require more powerful external models or incur substantial computational\ncosts. In this paper, we introduce MathFimer, a novel framework for\nmathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task\nfrom code completion. By decomposing solution chains into prefix-suffix pairs\nand training models to reconstruct missing intermediate steps, we develop a\nspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM\ndataset. We then apply these models to enhance existing mathematical reasoning\ndatasets by inserting detailed intermediate steps into their solution chains,\ncreating MathFimer-expanded versions. Through comprehensive experiments on\nmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQA\nand etc., we demonstrate that models trained on MathFimer-expanded data\nconsistently outperform their counterparts trained on original data across\nvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,\nscalable solution for enhancing mathematical reasoning capabilities in LLMs\nwithout relying on powerful external models or expensive inference procedures.", "published": "2025-02-17 11:22:24", "link": "http://arxiv.org/abs/2502.11684v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChineseSimpleVQA -- \"See the World, Discover Knowledge\": A Chinese\n  Factuality Evaluation for Large Vision Language Models", "abstract": "The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field.", "published": "2025-02-17 12:02:23", "link": "http://arxiv.org/abs/2502.11718v3", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ReviewEval: An Evaluation Framework for AI-Generated Reviews", "abstract": "The escalating volume of academic research, coupled with a shortage of\nqualified reviewers, necessitates innovative approaches to peer review. While\nlarge language model (LLMs) offer potential for automating this process, their\ncurrent limitations include superficial critiques, hallucinations, and a lack\nof actionable insights. This research addresses these challenges by introducing\na comprehensive evaluation framework for AI-generated reviews, that measures\nalignment with human evaluations, verifies factual accuracy, assesses\nanalytical depth, and identifies actionable insights. We also propose a novel\nalignment mechanism that tailors LLM-generated reviews to the unique evaluation\npriorities of individual conferences and journals. To enhance the quality of\nthese reviews, we introduce a self-refinement loop that iteratively optimizes\nthe LLM's review prompts. Our framework establishes standardized metrics for\nevaluating AI-based review systems, thereby bolstering the reliability of\nAI-generated reviews in academic research.", "published": "2025-02-17 12:22:11", "link": "http://arxiv.org/abs/2502.11736v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Selection to Generation: A Survey of LLM-based Active Learning", "abstract": "Active Learning (AL) has been a powerful paradigm for improving model\nefficiency and performance by selecting the most informative data points for\nlabeling and training. In recent active learning frameworks, Large Language\nModels (LLMs) have been employed not only for selection but also for generating\nentirely new data instances and providing more cost-effective annotations.\nMotivated by the increasing importance of high-quality data and efficient model\ntraining in the era of LLMs, we present a comprehensive survey on LLM-based\nActive Learning. We introduce an intuitive taxonomy that categorizes these\ntechniques and discuss the transformative roles LLMs can play in the active\nlearning loop. We further examine the impact of AL on LLM learning paradigms\nand its applications across various domains. Finally, we identify open\nchallenges and propose future research directions. This survey aims to serve as\nan up-to-date resource for researchers and practitioners seeking to gain an\nintuitive understanding of LLM-based AL techniques and deploy them to new\napplications.", "published": "2025-02-17 12:58:17", "link": "http://arxiv.org/abs/2502.11767v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It", "abstract": "The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why current LLMs struggle to detect even simple arithmetic errors.", "published": "2025-02-17 13:00:44", "link": "http://arxiv.org/abs/2502.11771v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and\n  Refinement in Table Reasoning", "abstract": "Despite the remarkable capabilities of large language models (LLMs) in\nvarious reasoning tasks, they still struggle with table reasoning tasks,\nparticularly in maintaining consistency throughout multi-step reasoning\nprocesses. While existing approaches have explored various decomposition\nstrategies, they often lack effective mechanisms to identify and correct errors\nin intermediate reasoning steps, leading to cascading error propagation. To\naddress these issues, we propose Table-Critic, a novel multi-agent framework\nthat facilitates collaborative criticism and iterative refinement of the\nreasoning process until convergence to correct solutions. Our framework\nconsists of four specialized agents: a Judge for error identification, a Critic\nfor comprehensive critiques, a Refiner for process improvement, and a Curator\nfor pattern distillation. To effectively deal with diverse and unpredictable\nerror types, we introduce a self-evolving template tree that systematically\naccumulates critique knowledge through experience-driven learning and guides\nfuture reflections. Extensive experiments have demonstrated that Table-Critic\nachieves substantial improvements over existing methods, achieving superior\naccuracy and error correction rates while maintaining computational efficiency\nand lower solution degradation rate.", "published": "2025-02-17 13:42:12", "link": "http://arxiv.org/abs/2502.11799v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Defining and Evaluating Visual Language Models' Basic Spatial Abilities:\n  A Perspective from Psychometrics", "abstract": "The Theory of Multiple Intelligences underscores the hierarchical nature of\ncognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer\na psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual\nLanguage Models (VLMs): Spatial Perception, Spatial Relation, Spatial\nOrientation, Mental Rotation, and Spatial Visualization. Benchmarking 13\nmainstream VLMs through nine validated psychometric experiments reveals\nsignificant gaps versus humans (average score 24.95 vs. 68.38), with three key\nfindings: 1) VLMs mirror human hierarchies (strongest in 2D orientation,\nweakest in 3D rotation) with independent BSAs (Pearson's r<0.4); 2) Smaller\nmodels such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading\n(30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought\n(0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from\narchitectural constraints. Identified barriers include weak geometry encoding\nand missing dynamic simulation. By linking psychometric BSAs to VLM\ncapabilities, we provide a diagnostic toolkit for spatial intelligence\nevaluation, methodological foundations for embodied AI development, and a\ncognitive science-informed roadmap for achieving human-like spatial\nintelligence.", "published": "2025-02-17 14:50:53", "link": "http://arxiv.org/abs/2502.11859v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models", "abstract": "Existing LLM reasoning methods have shown impressive capabilities across\nvarious tasks, such as solving math and coding problems. However, applying\nthese methods to scenarios without ground-truth answers or rule-based\nverification methods - such as tracking the mental states of an agent - remains\nchallenging. Inspired by the sequential Monte Carlo algorithm, we introduce\nthought-tracing, an inference-time reasoning algorithm designed to trace the\nmental states of specific agents by generating hypotheses and weighting them\nbased on observations without relying on ground-truth solutions to questions in\ndatasets. Our algorithm is modeled after the Bayesian theory-of-mind framework,\nusing LLMs to approximate probabilistic inference over agents' evolving mental\nstates based on their perceptions and actions. We evaluate thought-tracing on\ndiverse theory-of-mind benchmarks, demonstrating significant performance\nimprovements compared to baseline LLMs. Our experiments also reveal interesting\nbehaviors of the recent reasoning models - e.g., o1 and R1 - on theory-of-mind,\nhighlighting the difference of social reasoning compared to other domains.", "published": "2025-02-17 15:08:50", "link": "http://arxiv.org/abs/2502.11881v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay\n  Scoring Capabilities of Multimodal Large Language Models", "abstract": "Automated Essay Scoring (AES) plays a crucial role in educational assessment\nby providing scalable and consistent evaluations of writing tasks. However,\ntraditional AES systems face three major challenges: (1) reliance on\nhandcrafted features that limit generalizability, (2) difficulty in capturing\nfine-grained traits like coherence and argumentation, and (3) inability to\nhandle multimodal contexts. In the era of Multimodal Large Language Models\n(MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES\ncapabilities across lexical-, sentence-, and discourse-level traits. By\nleveraging MLLMs' strengths in trait-specific scoring and multimodal context\nunderstanding, EssayJudge aims to offer precise, context-rich evaluations\nwithout manual feature engineering, addressing longstanding AES limitations.\nOur experiments with 18 representative MLLMs reveal gaps in AES performance\ncompared to human evaluation, particularly in discourse-level traits,\nhighlighting the need for further advancements in MLLM-based AES research. Our\ndataset and code will be available upon acceptance.", "published": "2025-02-17 15:31:59", "link": "http://arxiv.org/abs/2502.11916v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Text to Trust: Empowering AI-assisted Decision Making with Adaptive\n  LLM-powered Analysis", "abstract": "AI-assisted decision making becomes increasingly prevalent, yet individuals\noften fail to utilize AI-based decision aids appropriately especially when the\nAI explanations are absent, potentially as they do not %understand reflect on\nAI's decision recommendations critically. Large language models (LLMs), with\ntheir exceptional conversational and analytical capabilities, present great\nopportunities to enhance AI-assisted decision making in the absence of AI\nexplanations by providing natural-language-based analysis of AI's decision\nrecommendation, e.g., how each feature of a decision making task might\ncontribute to the AI recommendation. In this paper, via a randomized\nexperiment, we first show that presenting LLM-powered analysis of each task\nfeature, either sequentially or concurrently, does not significantly improve\npeople's AI-assisted decision performance. To enable decision makers to better\nleverage LLM-powered analysis, we then propose an algorithmic framework to\ncharacterize the effects of LLM-powered analysis on human decisions and\ndynamically decide which analysis to present. Our evaluation with human\nsubjects shows that this approach effectively improves decision makers'\nappropriate reliance on AI in AI-assisted decision making.", "published": "2025-02-17 15:32:54", "link": "http://arxiv.org/abs/2502.11919v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "Navigating the Helpfulness-Truthfulness Trade-Off with Uncertainty-Aware\n  Instruction Fine-Tuning", "abstract": "Instruction Fine-tuning (IFT) can enhance the helpfulness of Large Language\nModels (LLMs), but it may lower their truthfulness. This trade-off arises\nbecause IFT steers LLMs to generate responses with long-tail knowledge that is\nnot well covered during pre-training, leading to more informative but less\ntruthful answers when generalizing to unseen tasks. In this paper, we\nempirically demonstrate this helpfulness-truthfulness trade-off in IFT and\npropose $\\textbf{UNIT}$, a novel IFT paradigm to address it. UNIT teaches LLMs\nto recognize their uncertainty and explicitly reflect it at the end of their\nresponses. Experimental results show that UNIT-tuned models maintain their\nhelpfulness while distinguishing between certain and uncertain claims, thereby\nreducing hallucinations.", "published": "2025-02-17 16:10:30", "link": "http://arxiv.org/abs/2502.11962v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Presumed Cultural Identity: How Names Shape LLM Responses", "abstract": "Names are deeply tied to human identity. They can serve as markers of\nindividuality, cultural heritage, and personal history. However, using names as\na core indicator of identity can lead to over-simplification of complex\nidentities. When interacting with LLMs, user names are an important point of\ninformation for personalisation. Names can enter chatbot conversations through\ndirect user input (requested by chatbots), as part of task contexts such as CV\nreviews, or as built-in memory features that store user information for\npersonalisation. We study biases associated with names by measuring cultural\npresumptions in the responses generated by LLMs when presented with common\nsuggestion-seeking queries, which might involve making assumptions about the\nuser. Our analyses demonstrate strong assumptions about cultural identity\nassociated with names present in LLM generations across multiple cultures. Our\nwork has implications for designing more nuanced personalisation systems that\navoid reinforcing stereotypes while maintaining meaningful customisation.", "published": "2025-02-17 16:35:15", "link": "http://arxiv.org/abs/2502.11995v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Merging Language and Domain Specific Models: The Impact on Technical\n  Vocabulary Acquisition", "abstract": "Advancements in Natural Language Processing have enabled specialized language\nmodels, but integrating domain-specific knowledge into general-purpose models\nin multilingual settings remains challenging, particularly for technical\nvocabulary. This paper investigates the integration of technical vocabulary in\nmerged language models and explores the knowledge transfer mechanisms involved\nwhen combining a general-purpose language-specific model with a domain-specific\nmodel, focusing on the resulting model's comprehension of technical jargon. Our\nexperiments analyze the impact of this merging process on the target model's\nproficiency in handling specialized terminology. We present a quantitative\nevaluation of the performance of the merged model, comparing it with that of\nthe individual constituent models. The findings offer insights into the\neffectiveness of different model merging methods for enhancing domain-specific\nknowledge and highlight potential challenges and future directions in\nleveraging these methods for cross-lingual knowledge transfer in Natural\nLanguage Processing.", "published": "2025-02-17 16:39:28", "link": "http://arxiv.org/abs/2502.12001v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Demographic Attributes Prediction from Speech Using WavLM Embeddings", "abstract": "This paper introduces a general classifier based on WavLM features, to infer\ndemographic characteristics, such as age, gender, native language, education,\nand country, from speech. Demographic feature prediction plays a crucial role\nin applications like language learning, accessibility, and digital forensics,\nenabling more personalized and inclusive technologies. Leveraging pretrained\nmodels for embedding extraction, the proposed framework identifies key acoustic\nand linguistic fea-tures associated with demographic attributes, achieving a\nMean Absolute Error (MAE) of 4.94 for age prediction and over 99.81% accuracy\nfor gender classification across various datasets. Our system improves upon\nexisting models by up to relative 30% in MAE and up to relative 10% in accuracy\nand F1 scores across tasks, leveraging a diverse range of datasets and large\npretrained models to ensure robustness and generalizability. This study offers\nnew insights into speaker diversity and provides a strong foundation for future\nresearch in speech-based demographic profiling.", "published": "2025-02-17 16:43:47", "link": "http://arxiv.org/abs/2502.12007v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Teaching LLMs According to Their Aptitude: Adaptive Reasoning for\n  Mathematical Problem Solving", "abstract": "Existing approaches to mathematical reasoning with large language models\n(LLMs) rely on Chain-of-Thought (CoT) for generalizability or Tool-Integrated\nReasoning (TIR) for precise computation. While efforts have been made to\ncombine these methods, they primarily rely on post-selection or predefined\nstrategies, leaving an open question: whether LLMs can autonomously adapt their\nreasoning strategy based on their inherent capabilities. In this work, we\npropose TATA (Teaching LLMs According to Their Aptitude), an adaptive framework\nthat enables LLMs to personalize their reasoning strategy spontaneously,\naligning it with their intrinsic aptitude. TATA incorporates base-LLM-aware\ndata selection during supervised fine-tuning (SFT) to tailor training data to\nthe model's unique abilities. This approach equips LLMs to autonomously\ndetermine and apply the appropriate reasoning strategy at test time. We\nevaluate TATA through extensive experiments on six mathematical reasoning\nbenchmarks, using both general-purpose and math-specialized LLMs. Empirical\nresults demonstrate that TATA effectively combines the complementary strengths\nof CoT and TIR, achieving superior or comparable performance with improved\ninference efficiency compared to TIR alone. Further analysis underscores the\ncritical role of aptitude-aware data selection in enabling LLMs to make\neffective and adaptive reasoning decisions and align reasoning strategies with\nmodel capabilities.", "published": "2025-02-17 16:56:23", "link": "http://arxiv.org/abs/2502.12022v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SafeChain: Safety of Language Models with Long Chain-of-Thought\n  Reasoning Capabilities", "abstract": "Emerging large reasoning models (LRMs), such as DeepSeek-R1 models, leverage\nlong chain-of-thought (CoT) reasoning to generate structured intermediate\nsteps, enhancing their reasoning capabilities. However, long CoT does not\ninherently guarantee safe outputs, potentially leading to harmful consequences\nsuch as the introduction of security vulnerabilities in code or the spread of\nmisinformation. Current research on large language model (LLM) safety usually\nfocuses on short-answer responses, overlooking the long CoT style outputs of\nLRMs. To bridge this gap, we conduct a systematic study of LRM safety. First,\nwe investigate safety evaluators calibrated against human annotations. Using\nour newly developed metrics, we thoroughly assess the safety of 12\nstate-of-the-art LRMs on StrongReject and WildJailbreak datasets. Our results\nshow that LRMs are not safe compared to their reasoning advance. Further, we\nperform a fine-grained analysis of the reasoning trace and final answer. We\nfind that three decoding strategies-ZeroThink, LessThink, and MoreThink-can\nimprove model safety without additional training. However, these strategies\neither use constrained reasoning traces or incur high inference costs. To\nbetter strengthen LRM safety, we introduce SafeChain, the first-of-its-kind\nsafety training dataset in CoT style. We fine-tune two LRMs with SafeChain,\nshowing that it not only enhances model safety but also preserves performance\nacross 6 reasoning benchmarks.", "published": "2025-02-17 16:57:56", "link": "http://arxiv.org/abs/2502.12025v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SpeechT: Findings of the First Mentorship in Speech Translation", "abstract": "This work presents the details and findings of the first mentorship in speech\ntranslation (SpeechT), which took place in December 2024 and January 2025. To\nfulfil the requirements of the mentorship, the participants engaged in key\nactivities, including data preparation, modelling, and advanced research.", "published": "2025-02-17 17:18:39", "link": "http://arxiv.org/abs/2502.12050v1", "categories": ["cs.CL", "cs.SD"], "primary_category": "cs.CL"}
{"title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical\n  Guidelines", "abstract": "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.", "published": "2025-02-17 17:20:41", "link": "http://arxiv.org/abs/2502.12051v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Culture is Not Trivia: Sociocultural Theory for Cultural NLP", "abstract": "The field of cultural NLP has recently experienced rapid growth, driven by a\npressing need to ensure that language technologies are effective and safe\nacross a pluralistic user base. This work has largely progressed without a\nshared conception of culture, instead choosing to rely on a wide array of\ncultural proxies. However, this leads to a number of recurring limitations:\ncoarse national boundaries fail to capture nuanced differences that lay within\nthem, limited coverage restricts datasets to only a subset of usually\nhighly-represented cultures, and a lack of dynamicity results in static\ncultural benchmarks that do not change as culture evolves. In this position\npaper, we argue that these methodological limitations are symptomatic of a\ntheoretical gap. We draw on a well-developed theory of culture from\nsociocultural linguistics to fill this gap by 1) demonstrating in a case study\nhow it can clarify methodological constraints and affordances, 2) offering\ntheoretically-motivated paths forward to achieving cultural competence, and 3)\narguing that localization is a more useful framing for the goals of much\ncurrent work in cultural NLP.", "published": "2025-02-17 17:25:11", "link": "http://arxiv.org/abs/2502.12057v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "AI-generated Text Detection with a GLTR-based Approach", "abstract": "The rise of LLMs (Large Language Models) has contributed to the improved\nperformance and development of cutting-edge NLP applications. However, these\ncan also pose risks when used maliciously, such as spreading fake news, harmful\ncontent, impersonating individuals, or facilitating school plagiarism, among\nothers. This is because LLMs can generate high-quality texts, which are\nchallenging to differentiate from those written by humans. GLTR, which stands\nfor Giant Language Model Test Room and was developed jointly by the MIT-IBM\nWatson AI Lab and HarvardNLP, is a visual tool designed to help detect\nmachine-generated texts based on GPT-2, that highlights the words in text\ndepending on the probability that they were machine-generated. One limitation\nof GLTR is that the results it returns can sometimes be ambiguous and lead to\nconfusion. This study aims to explore various ways to improve GLTR's\neffectiveness for detecting AI-generated texts within the context of the\nIberLef-AuTexTification 2023 shared task, in both English and Spanish\nlanguages. Experiment results show that our GLTR-based GPT-2 model overcomes\nthe state-of-the-art models on the English dataset with a macro F1-score of\n80.19%, except for the first ranking model (80.91%). However, for the Spanish\ndataset, we obtained a macro F1-score of 66.20%, which differs by 4.57%\ncompared to the top-performing model.", "published": "2025-02-17 17:32:55", "link": "http://arxiv.org/abs/2502.12064v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Formalizing Complex Mathematical Statements with LLMs: A Study on\n  Mathematical Definitions", "abstract": "Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge\nthe gap between informal mathematics and formal languages through\nautoformalization. However, it is still unclear how well LLMs generalize to\nsophisticated and naturally occurring mathematical statements. To address this\ngap, we investigate the task of autoformalizing real-world mathematical\ndefinitions -- a critical component of mathematical discourse. Specifically, we\nintroduce two novel resources for autoformalisation, collecting definitions\nfrom Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically\nevaluate a range of LLMs, analyzing their ability to formalize definitions into\nIsabelle/HOL. Furthermore, we investigate strategies to enhance LLMs'\nperformance including refinement through external feedback from Proof\nAssistants, and formal definition grounding, where we guide LLMs through\nrelevant contextual elements from formal mathematical libraries. Our findings\nreveal that definitions present a greater challenge compared to existing\nbenchmarks, such as miniF2F. In particular, we found that LLMs still struggle\nwith self-correction, and aligning with relevant mathematical libraries. At the\nsame time, structured refinement methods and definition grounding strategies\nyield notable improvements of up to 16% on self-correction capabilities and 43%\non the reduction of undefined errors, highlighting promising directions for\nenhancing LLM-based autoformalization in real-world scenarios.", "published": "2025-02-17 17:34:48", "link": "http://arxiv.org/abs/2502.12065v2", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "abstract": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.", "published": "2025-02-17 17:37:26", "link": "http://arxiv.org/abs/2502.12067v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unhackable Temporal Rewarding for Scalable Video MLLMs", "abstract": "In the pursuit of superior video-processing MLLMs, we have encountered a\nperplexing paradox: the \"anti-scaling law\", where more data and larger models\nlead to worse performance. This study unmasks the culprit: \"temporal hacking\",\na phenomenon where models shortcut by fixating on select frames, missing the\nfull video narrative. In this work, we systematically establish a comprehensive\ntheory of temporal hacking, defining it from a reinforcement learning\nperspective, introducing the Temporal Perplexity (TPL) score to assess this\nmisalignment, and proposing the Unhackable Temporal Rewarding (UTR) framework\nto mitigate the temporal hacking. Both theoretically and empirically, TPL\nproves to be a reliable indicator of temporal modeling quality, correlating\nstrongly with frame activation patterns. Extensive experiments reveal that UTR\nnot only counters temporal hacking but significantly elevates video\ncomprehension capabilities. This work not only advances video-AI systems but\nalso illuminates the critical importance of aligning proxy rewards with true\nobjectives in MLLM development.", "published": "2025-02-17 17:55:55", "link": "http://arxiv.org/abs/2502.12081v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "AdaSplash: Adaptive Sparse Flash Attention", "abstract": "The computational cost of softmax-based attention in transformers limits\ntheir applicability to long-context tasks. Adaptive sparsity, of which\n$\\alpha$-entmax attention is an example, offers a flexible data-dependent\nalternative, but existing implementations are inefficient and do not leverage\nthe sparsity to obtain runtime and memory gains. In this work, we propose\nAdaSplash, which combines the efficiency of GPU-optimized algorithms with the\nsparsity benefits of $\\alpha$-entmax. We first introduce a hybrid\nHalley-bisection algorithm, resulting in a 7-fold reduction in the number of\niterations needed to compute the $\\alpha$-entmax transformation. Then, we\nimplement custom Triton kernels to efficiently handle adaptive sparsity.\nExperiments with RoBERTa and ModernBERT for text classification and\nsingle-vector retrieval, along with GPT-2 for language modeling, show that our\nmethod achieves substantial improvements in runtime and memory efficiency\ncompared to existing $\\alpha$-entmax implementations. It approaches -- and in\nsome cases surpasses -- the efficiency of highly optimized softmax\nimplementations like FlashAttention-2, enabling long-context training while\nmaintaining strong task performance.", "published": "2025-02-17 17:56:23", "link": "http://arxiv.org/abs/2502.12082v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "APB: Accelerating Distributed Long-Context Inference by Passing\n  Compressed Context Blocks across GPUs", "abstract": "While long-context inference is crucial for advancing large language model\n(LLM) applications, its prefill speed remains a significant bottleneck. Current\napproaches, including sequence parallelism strategies and compute reduction\nthrough approximate attention mechanisms, still fall short of delivering\noptimal inference efficiency. This hinders scaling the inputs to longer\nsequences and processing long-context queries in a timely manner. To address\nthis, we introduce APB, an efficient long-context inference framework that\nleverages multi-host approximate attention to enhance prefill speed by reducing\ncompute and enhancing parallelism simultaneously. APB introduces a\ncommunication mechanism for essential key-value pairs within a sequence\nparallelism framework, enabling a faster inference speed while maintaining task\nperformance. We implement APB by incorporating a tailored FlashAttn kernel\nalongside optimized distribution strategies, supporting diverse models and\nparallelism configurations. APB achieves speedups of up to 9.2x, 4.2x, and 1.6x\ncompared with FlashAttn, RingAttn, and StarAttn, respectively, without any\nobservable task performance degradation. We provide the implementation and\nexperiment code of APB in https://github.com/thunlp/APB.", "published": "2025-02-17 17:59:56", "link": "http://arxiv.org/abs/2502.12085v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Study on Leveraging Search and Self-Feedback for Agent Reasoning", "abstract": "Recent works have demonstrated that incorporating search during inference can\nsignificantly improve reasoning capabilities of language agents. Some\napproaches may make use of the ground truth or rely on model's own generated\nfeedback. The search algorithm uses this feedback to then produce values that\nwill update its criterion for exploring and exploiting various reasoning paths.\nIn this study, we investigate how search and model's self-feedback can be\nleveraged for reasoning tasks. First, we explore differences in ground-truth\nfeedback and self-feedback during search for math reasoning. Second, we observe\nlimitations in applying search techniques to more complex tasks like\ntool-calling and design domain-specific approaches to address these gaps. Our\nexperiments reveal challenges related to generalization when solely relying on\nself-feedback during search. For search to work effectively, either access to\nthe ground-truth is needed or feedback mechanisms need to be carefully designed\nfor the specific task.", "published": "2025-02-17 18:12:36", "link": "http://arxiv.org/abs/2502.12094v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Personality Structured Interview for Large Language Model Simulation in\n  Personality Research", "abstract": "Although psychometrics researchers have recently explored the use of large\nlanguage models (LLMs) as proxies for human participants, LLMs often fail to\ngenerate heterogeneous data with human-like diversity, which diminishes their\nvalue in advancing social science research. To address these challenges, we\nexplored the potential of the theory-informed Personality Structured Interview\n(PSI) as a tool for simulating human responses in personality research. In this\napproach, the simulation is grounded in nuanced real-human interview\ntranscripts that target the personality construct of interest. We have provided\na growing set of 357 structured interview transcripts from a representative\nsample, each containing an individual's response to 32 open-ended questions\ncarefully designed to gather theory-based personality evidence. Additionally,\ngrounded in psychometric research, we have summarized an evaluation framework\nto systematically validate LLM-generated psychometric data. Results from three\nexperiments demonstrate that well-designed structured interviews could improve\nhuman-like heterogeneity in LLM-simulated personality data and predict\npersonality-related behavioral outcomes (i.e., organizational citizenship\nbehaviors and counterproductive work behavior). We further discuss the role of\ntheory-informed structured interviews in LLM-based simulation and outline a\ngeneral framework for designing structured interviews to simulate human-like\ndata for psychometric research.", "published": "2025-02-17 18:31:57", "link": "http://arxiv.org/abs/2502.12109v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A-MEM: Agentic Memory for LLM Agents", "abstract": "While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code for evaluating performance is available at\nhttps://github.com/WujiangXu/AgenticMemory, while the source code of agentic\nmemory system is available at https://github.com/agiresearch/A-mem.", "published": "2025-02-17 18:36:14", "link": "http://arxiv.org/abs/2502.12110v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Scaling Test-Time Compute Without Verification or RL is Suboptimal", "abstract": "Despite substantial advances in scaling test-time compute, an ongoing debate\nin the community is how it should be scaled up to enable continued and\nefficient improvements with scaling. There are largely two approaches: first,\ndistilling successful search or thinking traces; and second, using verification\n(e.g., 0/1 outcome rewards, reward models, or verifiers) to guide reinforcement\nlearning (RL) and search algorithms. In this paper, we prove that finetuning\nLLMs with verifier-based (VB) methods based on RL or search is far superior to\nverifier-free (VF) approaches based on distilling or cloning search traces,\ngiven a fixed amount of compute/data budget. Further, we show that as we scale\ntest-time compute (measured as the output token length) and training data,\nsuboptimality of VF methods scales poorly compared to VB when the base\npre-trained LLM presents a heterogeneous distribution over correct solution\ntraces (e.g., different lengths, styles, etc.) and admits a non-sharp\ndistribution over rewards on traces sampled from it. We formalize this\ncondition using anti-concentration [Erd\\H{o}s, 1945]. This implies a stronger\nresult that VB methods scale better asymptotically, with the performance gap\nbetween VB and VF methods widening as test-time budget grows. We corroborate\nour theory empirically on both didactic and math reasoning problems with\n3/8/32B-sized pre-trained LLMs, where we find verification is crucial for\nscaling test-time compute.", "published": "2025-02-17 18:43:24", "link": "http://arxiv.org/abs/2502.12118v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Query Complexity of Verifier-Assisted Language Generation", "abstract": "Recently, a plethora of works have proposed inference-time algorithms (e.g.\nbest-of-n), which incorporate verifiers to assist the generation process. Their\nquality-efficiency trade-offs have been empirically benchmarked on a variety of\nconstrained generation tasks, but the algorithmic design landscape is still\nlargely poorly understood. In this paper, we develop a mathematical framework\nfor reasoning about constrained generation using a pre-trained language model\ngenerator oracle and a process verifier--which can decide whether a prefix can\nbe extended to a string which satisfies the constraints of choice. We show that\neven in very simple settings, access to a verifier can render an intractable\nproblem (information-theoretically or computationally) to a tractable one. In\nfact, we show even simple algorithms, like tokenwise rejection sampling, can\nenjoy significant benefits from access to a verifier. Empirically, we show that\na natural modification of tokenwise rejection sampling, in which the sampler is\nallowed to \"backtrack\" (i.e., erase the final few generated tokens) has robust\nand substantive benefits over natural baselines (e.g. (blockwise) rejection\nsampling, nucleus sampling)--both in terms of computational efficiency,\naccuracy and diversity.", "published": "2025-02-17 18:46:32", "link": "http://arxiv.org/abs/2502.12123v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "REVERSUM: A Multi-staged Retrieval-Augmented Generation Method to\n  Enhance Wikipedia Tail Biographies through Personal Narratives", "abstract": "Wikipedia is an invaluable resource for factual information about a wide\nrange of entities. However, the quality of articles on less-known entities\noften lags behind that of the well-known ones. This study proposes a novel\napproach to enhancing Wikipedia's B and C category biography articles by\nleveraging personal narratives such as autobiographies and biographies. By\nutilizing a multi-staged retrieval-augmented generation technique -- REVerSum\n-- we aim to enrich the informational content of these lesser-known articles.\nOur study reveals that personal narratives can significantly improve the\nquality of Wikipedia articles, providing a rich source of reliable information\nthat has been underutilized in previous studies. Based on crowd-based\nevaluation, REVerSum generated content outperforms the best performing baseline\nby 17% in terms of integrability to the original Wikipedia article and 28.5\\%\nin terms of informativeness. Code and Data are available at:\nhttps://github.com/sayantan11995/wikipedia_enrichment", "published": "2025-02-17 18:53:42", "link": "http://arxiv.org/abs/2502.12137v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Zero Token-Driven Deep Thinking in LLMs: Unlocking the Full Potential of\n  Existing Parameters via Cyclic Refinement", "abstract": "Resource limitations often constrain the parameter counts of Large Language\nModels (LLMs), hindering their performance. While existing methods employ\nparameter sharing to reuse the same parameter set under fixed budgets, such\napproaches typically force each layer to assume multiple roles with a\npredetermined number of iterations, restricting efficiency and adaptability. In\nthis work, we propose the Zero Token Transformer (ZTT), which features a\nhead-tail decoupled parameter cycling method. We disentangle the first (head)\nand last (tail) layers from parameter cycling and iteratively refine only the\nintermediate layers. Furthermore, we introduce a Zero-Token Mechanism, an\ninternal architectural component rather than an input token, to guide\nlayer-specific computation. At each cycle, the model retrieves a zero token\n(with trainable key values) from a Zero-Token Pool, integrating it alongside\nregular tokens in the attention mechanism. The corresponding attention scores\nnot only reflect each layer's computational importance but also enable dynamic\nearly exits without sacrificing overall model accuracy. Our approach achieves\nsuperior performance under tight parameter budgets, effectively reduces\ncomputational overhead via early exits, and can be readily applied to fine-tune\nexisting pre-trained models for enhanced efficiency and adaptability.", "published": "2025-02-17 04:37:22", "link": "http://arxiv.org/abs/2502.12214v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GLoT: A Novel Gated-Logarithmic Transformer for Efficient Sign Language\n  Translation", "abstract": "Machine Translation has played a critical role in reducing language barriers,\nbut its adaptation for Sign Language Machine Translation (SLMT) has been less\nexplored. Existing works on SLMT mostly use the Transformer neural network\nwhich exhibits low performance due to the dynamic nature of the sign language.\nIn this paper, we propose a novel Gated-Logarithmic Transformer (GLoT) that\ncaptures the long-term temporal dependencies of the sign language as a\ntime-series data. We perform a comprehensive evaluation of GloT with the\ntransformer and transformer-fusion models as a baseline, for\nSign-to-Gloss-to-Text translation. Our results demonstrate that GLoT\nconsistently outperforms the other models across all metrics. These findings\nunderscore its potential to address the communication challenges faced by the\nDeaf and Hard of Hearing community.", "published": "2025-02-17 14:31:00", "link": "http://arxiv.org/abs/2502.12223v1", "categories": ["cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"], "primary_category": "cs.CL"}
{"title": "InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended\n  Conversations with Hidden Context", "abstract": "While large language models excel at following explicit instructions, they\noften struggle with ambiguous or incomplete user requests, defaulting to\nverbose, generic responses rather than seeking clarification. We introduce\nInfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents\nhandle hidden context in open-ended user requests. The benchmark presents\nintentionally ambiguous scenarios that require models to engage in\ninformation-seeking dialogue through clarifying questions before providing\nappropriate responses. Our evaluation of both open and closed-source models\nreveals that while proprietary models generally perform better, all current\nassistants struggle with effectively gathering critical information, often\nrequiring multiple turns to infer user intent and frequently defaulting to\ngeneric responses without proper clarification. We provide a systematic\nmethodology for generating diverse scenarios and evaluating models'\ninformation-seeking capabilities, offering insights into the current\nlimitations of language models in handling ambiguous requests through\nmulti-turn interactions.", "published": "2025-02-17 19:01:10", "link": "http://arxiv.org/abs/2502.12257v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Independence Tests for Language Models", "abstract": "We consider the following problem: given the weights of two models, can we\ntest whether they were trained independently -- i.e., from independent random\ninitializations? We consider two settings: constrained and unconstrained. In\nthe constrained setting, we make assumptions about model architecture and\ntraining and propose a family of statistical tests that yield exact p-values\nwith respect to the null hypothesis that the models are trained from\nindependent random initializations. These p-values are valid regardless of the\ncomposition of either model's training data; we compute them by simulating\nexchangeable copies of each model under our assumptions and comparing various\nsimilarity measures of weights and activations between the original two models\nversus these copies. We report the p-values from these tests on pairs of 21\nopen-weight models (210 total pairs) and correctly identify all pairs of\nnon-independent models. Our tests remain effective even if one model was\nfine-tuned for many tokens. In the unconstrained setting, where we make no\nassumptions about training procedures, can change model architecture, and allow\nfor adversarial evasion attacks, the previous tests no longer work. Instead, we\npropose a new test which matches hidden activations between two models, and\nwhich is robust to adversarial transformations and to changes in model\narchitecture. The test can also do localized testing: identifying specific\nnon-independent components of models. Though we no longer obtain exact p-values\nfrom this, empirically we find it behaves as one and reliably identifies\nnon-independent models. Notably, we can use the test to identify specific parts\nof one model that are derived from another (e.g., how Llama 3.1-8B was pruned\nto initialize Llama 3.2-3B, or shared layers between Mistral-7B and\nStripedHyena-7B), and it is even robust to retraining individual layers of\neither model from scratch.", "published": "2025-02-17 20:01:08", "link": "http://arxiv.org/abs/2502.12292v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Warmup Generations: A Task-Agnostic Approach for Guiding\n  Sequence-to-Sequence Learning with Unsupervised Initial State Generation", "abstract": "Traditional supervised fine-tuning (SFT) strategies for sequence-to-sequence\ntasks often train models to directly generate the target output. Recent work\nhas shown that guiding models with intermediate steps, such as keywords,\noutlines, or reasoning chains, can significantly improve performance,\ncoherence, and interpretability. However, these methods often depend on\npredefined intermediate formats and annotated data, limiting their scalability\nand generalizability. In this work, we introduce a task-agnostic framework that\nenables models to generate intermediate \"warmup\" sequences. These warmup\nsequences, serving as an initial state for subsequent generation, are optimized\nto enhance the probability of generating the target sequence without relying on\nexternal supervision or human-designed structures. Drawing inspiration from\nreinforcement learning principles, our method iteratively refines these\nintermediate steps to maximize their contribution to the final output, similar\nto reward-driven optimization in reinforcement learning with human feedback.\nExperimental results across tasks such as translation, summarization, and\nmulti-choice question answering for logical reasoning show that our approach\noutperforms traditional SFT methods, and offers a scalable and flexible\nsolution for sequence-to-sequence tasks.", "published": "2025-02-17 20:23:42", "link": "http://arxiv.org/abs/2502.12304v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Language Models Learn Typologically Implausible Languages?", "abstract": "Grammatical features across human languages show intriguing correlations\noften attributed to learning biases in humans. However, empirical evidence has\nbeen limited to experiments with highly simplified artificial languages, and\nwhether these correlations arise from domain-general or language-specific\nbiases remains a matter of debate. Language models (LMs) provide an opportunity\nto study artificial language learning at a large scale and with a high degree\nof naturalism. In this paper, we begin with an in-depth discussion of how LMs\nallow us to better determine the role of domain-general learning biases in\nlanguage universals. We then assess learnability differences for LMs resulting\nfrom typologically plausible and implausible languages closely following the\nword-order universals identified by linguistic typologists. We conduct a\nsymmetrical cross-lingual study training and testing LMs on an array of highly\nnaturalistic but counterfactual versions of the English (head-initial) and\nJapanese (head-final) languages. Compared to similar work, our datasets are\nmore naturalistic and fall closer to the boundary of plausibility. Our\nexperiments show that these LMs are often slower to learn these subtly\nimplausible languages, while ultimately achieving similar performance on some\nmetrics regardless of typological plausibility. These findings lend credence to\nthe conclusion that LMs do show some typologically-aligned learning\npreferences, and that the typological patterns may result from, at least to\nsome degree, domain-general learning biases.", "published": "2025-02-17 20:40:01", "link": "http://arxiv.org/abs/2502.12317v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LM Agents for Coordinating Multi-User Information Gathering", "abstract": "This paper introduces PeopleJoin, a benchmark for evaluating LM-mediated\ncollaborative problem solving. Given a user request, PeopleJoin agents must\nidentify teammates who might be able to assist, converse with these teammates\nto gather information, and finally compile a useful answer or summary for the\noriginal user. PeopleJoin comprises two evaluation domains: PeopleJoin-QA,\nfocused on questions about tabular data, and PeopleJoin-DocCreation, focused on\ndocument creation tasks. The two domains are adapted from existing NLP\nbenchmarks for database question answering and multi-document summarization;\nhere, however, the information needed to complete these tasks is distributed\nacross synthetic ``organizations'' of 2--20 users, simulating natural\nmulti-user collaboration scenarios. We implemented several popular LM agent\narchitectures, evaluating their accuracy and efficiency at completing tasks,\nand highlight new research questions that can be studied using PeopleJoin.", "published": "2025-02-17 21:19:45", "link": "http://arxiv.org/abs/2502.12328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Classifiers of Data Sharing Statements in Clinical Trial Records", "abstract": "Digital individual participant data (IPD) from clinical trials are\nincreasingly distributed for potential scientific reuse. The identification of\navailable IPD, however, requires interpretations of textual data-sharing\nstatements (DSS) in large databases. Recent advancements in computational\nlinguistics include pre-trained language models that promise to simplify the\nimplementation of effective classifiers based on textual inputs. In a subset of\n5,000 textual DSS from ClinicalTrials.gov, we evaluate how well classifiers\nbased on domain-specific pre-trained language models reproduce original\navailability categories as well as manually annotated labels. Typical metrics\nindicate that classifiers that predicted manual annotations outperformed those\nthat learned to output the original availability categories. This suggests that\nthe textual DSS descriptions contain applicable information that the\navailability categories do not, and that such classifiers could thus aid the\nautomatic identification of available IPD in large trial databases.", "published": "2025-02-17 22:56:56", "link": "http://arxiv.org/abs/2502.12362v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "That is Unacceptable: the Moral Foundations of Canceling", "abstract": "Canceling is a morally-driven phenomenon that hinders the development of safe\nsocial media platforms and contributes to ideological polarization. To address\nthis issue we present the Canceling Attitudes Detection (CADE) dataset, an\nannotated corpus of canceling incidents aimed at exploring the factors of\ndisagreements in evaluating people canceling attitudes on social media.\nSpecifically, we study the impact of annotators' morality in their perception\nof canceling, showing that morality is an independent axis for the explanation\nof disagreement on this phenomenon. Annotator's judgments heavily depend on the\ntype of controversial events and involved celebrities. This shows the need to\ndevelop more event-centric datasets to better understand how harms are\nperpetrated in social media and to develop more aware technologies for their\ndetection.", "published": "2025-02-17 13:01:06", "link": "http://arxiv.org/abs/2503.05720v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of\n  Autonomous LLM Agents", "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers,\nraising concerns about catastrophic risks in high-stakes scenarios,\nparticularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains.\nBased on the insight that such risks can originate from trade-offs between the\nagent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel\nthree-stage evaluation framework, which is carefully constructed to effectively\nand naturally expose such risks. We conduct 14,400 agentic simulations across\n12 advanced LLMs, with extensive experiments and analysis. Results reveal that\nLLM agents can autonomously engage in catastrophic behaviors and deception,\nwithout being deliberately induced. Furthermore, stronger reasoning abilities\noften increase, rather than mitigate, these risks. We also show that these\nagents can violate instructions and superior commands. On the whole, we\nempirically prove the existence of catastrophic risks in autonomous LLM agents.\nWe release our code to foster further research.", "published": "2025-02-17 02:11:17", "link": "http://arxiv.org/abs/2502.11355v3", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CY"], "primary_category": "cs.CL"}
{"title": "SAIF: A Sparse Autoencoder Framework for Interpreting and Steering\n  Instruction Following of Language Models", "abstract": "The ability of large language models (LLMs) to follow instructions is crucial\nfor their practical applications, yet the underlying mechanisms remain poorly\nunderstood. This paper presents a novel framework that leverages sparse\nautoencoders (SAE) to interpret how instruction following works in these\nmodels. We demonstrate how the features we identify can effectively steer model\noutputs to align with given instructions. Through analysis of SAE latent\nactivations, we identify specific latents responsible for instruction following\nbehavior. Our findings reveal that instruction following capabilities are\nencoded by a distinct set of instruction-relevant SAE latents. These latents\nboth show semantic proximity to relevant instructions and demonstrate causal\neffects on model behavior. Our research highlights several crucial factors for\nachieving effective steering performance: precise feature identification, the\nrole of final layer, and optimal instruction positioning. Additionally, we\ndemonstrate that our methodology scales effectively across SAEs and LLMs of\nvarying sizes.", "published": "2025-02-17 02:11:17", "link": "http://arxiv.org/abs/2502.11356v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sparse Autoencoder Features for Classifications and Transferability", "abstract": "Sparse Autoencoders (SAEs) provide potentials for uncovering structured,\nhuman-interpretable representations in Large Language Models (LLMs), making\nthem a crucial tool for transparent and controllable AI systems. We\nsystematically analyze SAE for interpretable feature extraction from LLMs in\nsafety-critical classification tasks. Our framework evaluates (1) model-layer\nselection and scaling properties, (2) SAE architectural configurations,\nincluding width and pooling strategies, and (3) the effect of binarizing\ncontinuous SAE activations. SAE-derived features achieve macro F1 > 0.8,\noutperforming hidden-state and BoW baselines while demonstrating cross-model\ntransfer from Gemma 2 2B to 9B-IT models. These features generalize in a\nzero-shot manner to cross-lingual toxicity detection and visual classification\ntasks. Our analysis highlights the significant impact of pooling strategies and\nbinarization thresholds, showing that binarization offers an efficient\nalternative to traditional feature selection while maintaining or improving\nperformance. These findings establish new best practices for SAE-based\ninterpretability and enable scalable, transparent deployment of LLMs in\nreal-world applications. Full repo: https://github.com/shan23chen/MOSAIC.", "published": "2025-02-17 02:30:45", "link": "http://arxiv.org/abs/2502.11367v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CCJA: Context-Coherent Jailbreak Attack for Aligned Large Language\n  Models", "abstract": "Despite explicit alignment efforts for large language models (LLMs), they can\nstill be exploited to trigger unintended behaviors, a phenomenon known as\n\"jailbreaking.\" Current jailbreak attack methods mainly focus on discrete\nprompt manipulations targeting closed-source LLMs, relying on manually crafted\nprompt templates and persuasion rules. However, as the capabilities of\nopen-source LLMs improve, ensuring their safety becomes increasingly crucial.\nIn such an environment, the accessibility of model parameters and gradient\ninformation by potential attackers exacerbates the severity of jailbreak\nthreats. To address this research gap, we propose a novel\n\\underline{C}ontext-\\underline{C}oherent \\underline{J}ailbreak\n\\underline{A}ttack (CCJA). We define jailbreak attacks as an optimization\nproblem within the embedding space of masked language models. Through\ncombinatorial optimization, we effectively balance the jailbreak attack success\nrate with semantic coherence. Extensive evaluations show that our method not\nonly maintains semantic consistency but also surpasses state-of-the-art\nbaselines in attack effectiveness. Additionally, by integrating semantically\ncoherent jailbreak prompts generated by our method into widely used black-box\nmethodologies, we observe a notable enhancement in their success rates when\ntargeting closed-source commercial LLMs. This highlights the security threat\nposed by open-source LLMs to commercial counterparts. We will open-source our\ncode if the paper is accepted.", "published": "2025-02-17 02:49:26", "link": "http://arxiv.org/abs/2502.11379v1", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "SMART: Self-Aware Agent for Tool Overuse Mitigation", "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and\ntool use capabilities, but often lack self-awareness, failing to balance these\napproaches effectively. This imbalance leads to Tool Overuse, where models\nunnecessarily rely on external tools for tasks solvable with parametric\nknowledge, increasing computational overhead. Inspired by human metacognition,\nwe introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm\nthat enhances an agent's self-awareness to optimize task handling and reduce\ntool overuse. To support this paradigm, we introduce SMART-ER, a dataset\nspanning three domains, where reasoning alternates between parametric knowledge\nand tool-dependent steps, with each step enriched by rationales explaining when\ntools are necessary. Through supervised training, we develop SMARTAgent, a\nfamily of models that dynamically balance parametric knowledge and tool use.\nEvaluations show that SMARTAgent reduces tool use by 24% while improving\nperformance by over 37%, enabling 7B-scale models to match its 70B counterpart\nand GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test\ndata like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool\ncalls. These highlight the potential of strategic tool use to enhance\nreasoning, mitigate overuse, and bridge the gap between model size and\nperformance, advancing intelligent and resource-efficient agent designs.", "published": "2025-02-17 04:50:37", "link": "http://arxiv.org/abs/2502.11435v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "An Efficient Row-Based Sparse Fine-Tuning", "abstract": "Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning (SFT)\nand Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SFT framework, based on ideas from neural network pruning. At\na high level, we first identify \"important\" neurons/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Using experiments on common language tasks, we\ndemonstrate that our method significantly improves the memory efficiency of SFT\nwithout increasing training time complexity and implementation complexity,\nwhile achieving accuracy comparable to state-of-the-art methods such as LoRA\nand its variants.", "published": "2025-02-17 04:54:42", "link": "http://arxiv.org/abs/2502.11439v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Turn Multi-Modal Question Clarification for Enhanced\n  Conversational Understanding", "abstract": "Conversational query clarification enables users to refine their search\nqueries through interactive dialogue, improving search effectiveness.\nTraditional approaches rely on text-based clarifying questions, which often\nfail to capture complex user preferences, particularly those involving visual\nattributes. While recent work has explored single-turn multi-modal\nclarification with images alongside text, such methods do not fully support the\nprogressive nature of user intent refinement over multiple turns. Motivated by\nthis, we introduce the Multi-turn Multi-modal Clarifying Questions (MMCQ) task,\nwhich combines text and visual modalities to refine user queries in a\nmulti-turn conversation. To facilitate this task, we create a large-scale\ndataset named ClariMM comprising over 13k multi-turn interactions and 33k\nquestion-answer pairs containing multi-modal clarifying questions. We propose\nMario, a retrieval framework that employs a two-phase ranking strategy: initial\nretrieval with BM25, followed by a multi-modal generative re-ranking model that\nintegrates textual and visual information from conversational history. Our\nexperiments show that multi-turn multi-modal clarification outperforms\nuni-modal and single-turn approaches, improving MRR by 12.88%. The gains are\nmost significant in longer interactions, demonstrating the value of progressive\nrefinement for complex queries.", "published": "2025-02-17 04:58:14", "link": "http://arxiv.org/abs/2502.11442v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "GiFT: Gibbs Fine-Tuning for Code Generation", "abstract": "Training Large Language Models (LLMs) with synthetic data is a prevalent\npractice in code generation. A key approach is self-training, where LLMs are\niteratively trained on self-generated correct code snippets. In this case, the\nself-generated codes are drawn from a conditional distribution, conditioned on\na specific seed description. However, the seed description is not the only\nvalid representation that aligns with its intended meaning. With all valid\ndescriptions and codes forming a joint space, codes drawn from the conditional\ndistribution would lead to an underrepresentation of the full description-code\nspace. As such, we propose Gibbs Fine-Tuning (GiFT), a novel self-training\nmethod inspired by Gibbs sampling. GiFT allows self-generated data to be drawn\nfrom the marginal distribution of the joint space, thereby mitigating the\nbiases inherent in conditional sampling. We provide a theoretical analysis\ndemonstrating the potential benefits of fine-tuning LLMs with code derived from\nthe marginal distribution. Furthermore, we propose a perplexity-based code\nselection method to mitigate the imbalanced long-tail distribution of the\nself-generated codes. Empirical evaluation of two LLMs across four datasets\ndemonstrates that GiFT achieves superior performance, particularly on more\nchallenging benchmarks.", "published": "2025-02-17 05:52:44", "link": "http://arxiv.org/abs/2502.11466v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free\n  Continual Learning", "abstract": "Continual learning (CL) is essential for Large Language Models (LLMs) to\nadapt to evolving real-world demands, yet they are susceptible to catastrophic\nforgetting (CF). While traditional CF solutions rely on expensive data\nrehearsal, recent rehearsal-free methods employ model-based and\nregularization-based strategies to address this issue. However, these\napproaches often neglect the model's plasticity, which is crucial to achieving\noptimal performance on newly learned tasks. Consequently, a key challenge in CL\nis striking a balance between preserving plasticity and mitigating CF. To\ntackle this challenge, we propose the $\\textbf{D}$ecomposed\n$\\textbf{A}$ttention-based $\\textbf{T}$ask $\\textbf{A}$daptation (DATA), which\nexplicitly decouples and learns both task-specific and task-shared knowledge\nusing high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA\ndynamically adjusts the weights of adapters of different ranks based on their\nrelevance and distinction from previous tasks, allowing the model to acquire\nnew task-specific skills while effectively retaining previously learned\nknowledge. Specifically, we implement a decomposed component weighting strategy\ncomprising learnable components that collectively generate attention-based\nweights, allowing the model to integrate and utilize diverse knowledge from\neach DATA. Extensive experiments on three widely used benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance. Notably, our\napproach significantly enhances model plasticity and mitigates CF by extending\nlearnable components and employing stochastic restoration during training\niterations.", "published": "2025-02-17 06:35:42", "link": "http://arxiv.org/abs/2502.11482v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Why Vision Language Models Struggle with Visual Arithmetic? Towards\n  Enhanced Chart and Geometry Understanding", "abstract": "Vision Language Models (VLMs) have achieved remarkable progress in multimodal\ntasks, yet they often struggle with visual arithmetic, seemingly simple\ncapabilities like object counting or length comparison, which are essential for\nrelevant complex tasks like chart understanding and geometric reasoning. In\nthis work, we first investigate the root causes of this deficiency through a\nsuite of probing tasks focusing on basic visual arithmetic. Our analysis\nreveals that while pre-trained vision encoders typically capture sufficient\ninformation, the text decoder often fails to decode it correctly for arithmetic\nreasoning. To address this, we propose CogAlign, a novel post-training strategy\ninspired by Piaget's theory of cognitive development. CogAlign trains VLMs to\nrecognize invariant properties under visual transformations. We demonstrate\nthat this approach significantly improves the performance of three diverse VLMs\non our proposed probing tasks. Furthermore, CogAlign enhances performance by an\naverage of 4.6% on CHOCOLATE and 2.9% on MATH-VISION, outperforming or matching\nsupervised fine-tuning methods while requiring only 60% less training data.\nThese results highlight the effectiveness and generalizability of CogAlign in\nimproving fundamental visual arithmetic capabilities and their transfer to\ndownstream tasks.", "published": "2025-02-17 06:54:49", "link": "http://arxiv.org/abs/2502.11492v2", "categories": ["cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Learning to Keep a Promise: Scaling Language Model Decoding Parallelism\n  with Learned Asynchronous Decoding", "abstract": "Decoding with autoregressive large language models (LLMs) traditionally\noccurs sequentially, generating one token after another. An emerging line of\nwork explored parallel decoding by identifying and simultaneously generating\nsemantically independent chunks of LLM responses. However, these techniques\nrely on hand-crafted heuristics tied to syntactic structures like lists and\nparagraphs, making them rigid and imprecise. We present PASTA, a learning-based\nsystem that teaches LLMs to identify semantic independence and express parallel\ndecoding opportunities in their own responses. At its core are PASTA-LANG and\nits interpreter: PASTA-LANG is an annotation language that enables LLMs to\nexpress semantic independence in their own responses; the language interpreter\nacts on these annotations to orchestrate parallel decoding on-the-fly at\ninference time. Through a two-stage finetuning process, we train LLMs to\ngenerate PASTA-LANG annotations that optimize both response quality and\ndecoding speed. Evaluation on AlpacaEval, an instruction following benchmark,\nshows that our approach Pareto-dominates existing methods in terms of decoding\nspeed and response quality; our results demonstrate geometric mean speedups\nranging from 1.21x to 1.93x with corresponding quality changes of +2.2% to\n-7.1%, measured by length-controlled win rates against sequential decoding\nbaseline.", "published": "2025-02-17 07:39:16", "link": "http://arxiv.org/abs/2502.11517v2", "categories": ["cs.CL", "cs.DC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Reasoning Ability of Small Language Models", "abstract": "Reasoning has long been viewed as an emergent property of large language\nmodels (LLMs), appearing at or above a certain scale ($\\sim$100B parameters).\nHowever, recent studies challenge this assumption, showing that small language\nmodels (SLMs) can also achieve competitive reasoning performance. SLMs are\nincreasingly favored for their efficiency and deployability. However, there is\na lack of systematic study on the reasoning abilities of diverse SLMs,\nincluding those trained from scratch or derived from LLMs through quantization,\npruning, and distillation. This raises a critical question: Can SLMs achieve\nreasoning abilities comparable to LLMs? In this work, we systematically survey,\nbenchmark, and analyze 72 SLMs from six model families across 14 reasoning\nbenchmarks. For reliable evaluation, we examine four evaluation methods and\ncompare four LLM judges against human evaluations on 800 data points. We repeat\nall experiments three times to ensure a robust performance assessment.\nAdditionally, we analyze the impact of different prompting strategies in small\nmodels. Beyond accuracy, we also evaluate model robustness under adversarial\nconditions and intermediate reasoning steps. Our findings challenge the\nassumption that scaling is the only way to achieve strong reasoning. Instead,\nwe foresee a future where SLMs with strong reasoning capabilities can be\ndeveloped through structured training or post-training compression. They can\nserve as efficient alternatives to LLMs for reasoning-intensive tasks.", "published": "2025-02-17 08:59:16", "link": "http://arxiv.org/abs/2502.11569v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FaMTEB: Massive Text Embedding Benchmark in Persian Language", "abstract": "In this paper, we introduce a comprehensive benchmark for Persian (Farsi)\ntext embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our\nbenchmark includes 63 datasets spanning seven different tasks: classification,\nclustering, pair classification, reranking, retrieval, summary retrieval, and\nsemantic textual similarity. The datasets are formed as a combination of\nexisting, translated, and newly generated data, offering a diverse evaluation\nframework for Persian language models. Given the increasing use of text\nembedding models in chatbots, evaluation datasets are becoming inseparable\ningredients in chatbot challenges and Retrieval-Augmented Generation systems.\nAs a contribution, we include chatbot evaluation datasets in the MTEB benchmark\nfor the first time. In addition, in this paper, we introduce the new task of\nsummary retrieval which is not part of the tasks included in standard MTEB.\nAnother contribution of this paper is the introduction of a substantial number\nof new Persian language NLP datasets suitable for training and evaluation, some\nof which have no previous counterparts in Persian. We evaluate the performance\nof several Persian and multilingual embedding models in a range of tasks. This\nwork introduces an open-source benchmark with datasets, code and a public\nleaderboard.", "published": "2025-02-17 09:05:21", "link": "http://arxiv.org/abs/2502.11571v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deviation Ratings: A General, Clone-Invariant Rating Method", "abstract": "Many real-world multi-agent or multi-task evaluation scenarios can be\nnaturally modelled as normal-form games due to inherent strategic (adversarial,\ncooperative, and mixed motive) interactions. These strategic interactions may\nbe agentic (e.g. players trying to win), fundamental (e.g. cost vs quality), or\ncomplementary (e.g. niche finding and specialization). In such a formulation,\nit is the strategies (actions, policies, agents, models, tasks, prompts, etc.)\nthat are rated. However, the rating problem is complicated by redundancy and\ncomplexity of N-player strategic interactions. Repeated or similar strategies\ncan distort ratings for those that counter or complement them. Previous work\nproposed ``clone invariant'' ratings to handle such redundancies, but this was\nlimited to two-player zero-sum (i.e. strictly competitive) interactions. This\nwork introduces the first N-player general-sum clone invariant rating, called\ndeviation ratings, based on coarse correlated equilibria. The rating is\nexplored on several domains including LLMs evaluation.", "published": "2025-02-17 10:39:04", "link": "http://arxiv.org/abs/2502.11645v1", "categories": ["cs.GT", "cs.CL", "cs.MA", "stat.OT"], "primary_category": "cs.GT"}
{"title": "Diversity-Oriented Data Augmentation with Large Language Models", "abstract": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points.", "published": "2025-02-17 11:00:40", "link": "http://arxiv.org/abs/2502.11671v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LLM Agents Making Agent Tools", "abstract": "Tool use has turned large language models (LLMs) into powerful agents that\ncan perform complex multi-step tasks by dynamically utilising external software\ncomponents. However, these tools must be implemented in advance by human\ndevelopers, hindering the applicability of LLM agents in domains which demand\nlarge numbers of highly specialised tools, like in life sciences and medicine.\nMotivated by the growing trend of scientific studies accompanied by public code\nrepositories, we propose ToolMaker, a novel agentic framework that autonomously\ntransforms papers with code into LLM-compatible tools. Given a short task\ndescription and a repository URL, ToolMaker autonomously installs required\ndependencies and generates code to perform the task, using a closed-loop\nself-correction mechanism to iteratively diagnose and rectify errors. To\nevaluate our approach, we introduce a benchmark comprising 15 diverse and\ncomplex computational tasks spanning both medical and non-medical domains with\nover 100 unit tests to objectively assess tool correctness and robustness.\nToolMaker correctly implements 80% of the tasks, substantially outperforming\ncurrent state-of-the-art software engineering agents. ToolMaker therefore is a\nstep towards fully autonomous agent-based scientific workflows.", "published": "2025-02-17 11:44:11", "link": "http://arxiv.org/abs/2502.11705v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis", "abstract": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat focus on tasks where pre-trained models already perform well, we develop a\nset of mathematical tasks where fine-tuning yields substantial performance\ngains, which are closer to the practical setting. In our experiments, we\nidentify circuits at various checkpoints during fine-tuning and examine the\ninterplay between circuit analysis, fine-tuning methods, and task complexities.\nFirst, we find that while circuits maintain high node similarity before and\nafter fine-tuning, their edges undergo significant changes, which is in\ncontrast to the previous work\n\\cite{prakash2024finetuningenhancesexistingmechanisms,chhabra2024neuroplasticity}\nthat show circuits only add some additional components after fine-tuning. Based\non these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA)\nmethod, which assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46\\% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.", "published": "2025-02-17 13:59:41", "link": "http://arxiv.org/abs/2502.11812v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code\n  Generation Capabilities", "abstract": "This paper introduces Code-Vision, a benchmark designed to evaluate the\nlogical understanding and code generation capabilities of Multimodal Large\nLanguage Models (MLLMs). It challenges MLLMs to generate a correct program that\nfulfills specific functionality requirements based on a given flowchart, which\nvisually represents the desired algorithm or process. Code-Vision comprises\nthree subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding\nabilities across basic programming, algorithmic, and mathematical\nproblem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision.\nExperimental results demonstrate that there is a large performance difference\nbetween proprietary and open-source models. On Hard problems, GPT-4o can\nachieve 79.3% pass@1, but the best open-source model only achieves 15%. Further\nexperiments reveal that Code-Vision can pose unique challenges compared to\nother multimodal reasoning benchmarks MMCode and MathVista. We also explore the\nreason for the poor performance of the open-source models. All data and codes\nare available at https://github.com/wanghanbinpanda/CodeVision.", "published": "2025-02-17 14:25:45", "link": "http://arxiv.org/abs/2502.11829v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Can LLM Agents Maintain a Persona in Discourse?", "abstract": "Large Language Models (LLMs) are widely used as conversational agents,\nexploiting their capabilities in various sectors such as education, law,\nmedicine, and more. However, LLMs are often subjected to context-shifting\nbehaviour, resulting in a lack of consistent and interpretable\npersonality-aligned interactions. Adherence to psychological traits lacks\ncomprehensive analysis, especially in the case of dyadic (pairwise)\nconversations. We examine this challenge from two viewpoints, initially using\ntwo conversation agents to generate a discourse on a certain topic with an\nassigned personality from the OCEAN framework (Openness, Conscientiousness,\nExtraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This\nis followed by using multiple judge agents to infer the original traits\nassigned to explore prediction consistency, inter-model agreement, and\nalignment with the assigned personality. Our findings indicate that while LLMs\ncan be guided toward personality-driven dialogue, their ability to maintain\npersonality traits varies significantly depending on the combination of models\nand discourse settings. These inconsistencies emphasise the challenges in\nachieving stable and interpretable personality-aligned interactions in LLMs.", "published": "2025-02-17 14:36:39", "link": "http://arxiv.org/abs/2502.11843v1", "categories": ["cs.CL", "cs.AI", "cs.SI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Bitnet.cpp: Efficient Edge Inference for Ternary LLMs", "abstract": "The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has\nspurred interest in ternary LLMs. Despite this, research and practical\napplications focusing on efficient edge inference for ternary LLMs remain\nscarce. To bridge this gap, we introduce Bitnet.cpp, an inference system\noptimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix\nmultiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs,\nBitnet.cpp incorporates a novel mpGEMM library to facilitate\nsub-2-bits-per-weight, efficient and lossless inference. The library features\ntwo core solutions: Ternary Lookup Table (TL), which addresses spatial\ninefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S),\nwhich ensures lossless edge inference, both enabling high-speed inference. Our\nexperiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over\nfull-precision baselines and up to 2.32x over low-bit baselines, setting new\nbenchmarks in the field. Additionally, we expand TL to element-wise lookup\ntable (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and\nempirical evidence of its considerable potential. Bitnet.cpp is publicly\navailable at https://github.com/microsoft/BitNet/tree/paper , offering a\nsophisticated solution for the efficient and practical deployment of edge LLMs.", "published": "2025-02-17 15:06:28", "link": "http://arxiv.org/abs/2502.11880v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "LIMR: Less is More for RL Scaling", "abstract": "In this paper, we ask: what truly determines the effectiveness of RL training\ndata for enhancing language models' reasoning capabilities? While recent\nadvances like o1, Deepseek R1, and Kimi1.5 demonstrate RL's potential, the lack\nof transparency about training data requirements has hindered systematic\nprogress. Starting directly from base models without distillation, we challenge\nthe assumption that scaling up RL training data inherently improves\nperformance. we demonstrate that a strategically selected subset of just 1,389\nsamples can outperform the full 8,523-sample dataset. We introduce Learning\nImpact Measurement (LIM), an automated method to evaluate and prioritize\ntraining samples based on their alignment with model learning trajectories,\nenabling efficient resource utilization and scalable implementation. Our method\nachieves comparable or even superior performance using only 1,389 samples\nversus the full 8,523 samples dataset. Notably, while recent data-efficient\napproaches (e.g., LIMO and s1) show promise with 32B-scale models, we find it\nsignificantly underperforms at 7B-scale through supervised fine-tuning (SFT).\nIn contrast, our RL-based LIMR achieves 16.7% higher accuracy on AIME24 and\noutperforms LIMO and s1 by 13.0% and 22.2% on MATH500. These results\nfundamentally reshape our understanding of RL scaling in LLMs, demonstrating\nthat precise sample selection, rather than data scale, may be the key to\nunlocking enhanced reasoning capabilities. For reproducible research and future\ninnovation, we are open-sourcing LIMR, including implementation of LIM,\ntraining and evaluation code, curated datasets, and trained models at\nhttps://github.com/GAIR-NLP/LIMR.", "published": "2025-02-17 15:13:29", "link": "http://arxiv.org/abs/2502.11886v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o\n  Under Data Scarsity", "abstract": "Existing LMs struggle with proof-oriented programming due to data scarcity,\nwhich manifest in two key ways: (1) a lack of sufficient corpora for\nproof-oriented programming languages such as F*, and (2) the absence of\nlarge-scale, project-level proof-oriented implementations that can teach the\nmodel the intricate reasoning process when performing proof-oriented\nprogramming. We present the first on synthetic data augmentation for project\nlevel proof oriented programming for both generation and repair. Our method\naddresses data scarcity by synthesizing basic proof-oriented programming\nproblems for proficiency in that language; incorporating diverse coding data\nfor reasoning capability elicitation and creating new proofs and repair data\nwithin existing repositories. This approach enables language models to both\nsynthesize and repair proofs for function- and repository-level code. We show\nthat our fine-tuned 14B parameter model, PoPilot, can exceed the performance of\nthe models that outperforms GPT-4o in project-level proof-oriented programming\nby 64% relative margin, and can improve GPT-4o's performance by 54% by\nrepairing its outputs over GPT-4o's self-repair.", "published": "2025-02-17 15:24:11", "link": "http://arxiv.org/abs/2502.11901v1", "categories": ["cs.CL", "cs.PL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Atom of Thoughts for Markov LLM Test-Time Scaling", "abstract": "Large Language Models (LLMs) achieve superior performance through\ntraining-time scaling, and test-time scaling further enhances their\ncapabilities by conducting effective reasoning during inference. However, as\nthe scale of reasoning increases, existing test-time scaling methods suffer\nfrom accumulated historical information, which not only wastes computational\nresources but also interferes with effective reasoning. To address this issue,\nwe observe that complex reasoning can be achieved by solving a series of\nindependent and self-contained subquestions. These subquestions are essentially\n\\textit{atomic questions}, exhibiting the memoryless property similar to Markov\nprocesses. Based on this observation, we propose Atom of Thoughts (\\our), where\neach state transition consists of decomposing the current question into a\ndependency-based directed acyclic graph and contracting its subquestions,\nforming a simplified question that maintains answer equivalence with the\noriginal problem. This answer preservation enables the iterative\n\\textit{decomposition-contraction} process to naturally form a meaningful\nMarkov reasoning process. Furthermore, these atomic states can be seamlessly\nintegrated into existing test-time scaling methods, enabling \\our to serve as a\nplug-in enhancement for improving reasoning capabilities. Experiments across\nsix benchmarks demonstrate the effectiveness of \\our both as a standalone\nframework and a plug-in enhancement. Notably, on HotpotQA, when applied to\ngpt-4o-mini, \\our achieves an \\textbf{80.6\\%} F1 score, surpassing o3-mini by\n\\textbf{3.4\\%} and DeepSeek-R1 by \\textbf{10.6\\%}. The code is available at\n\\href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.", "published": "2025-02-17 16:52:42", "link": "http://arxiv.org/abs/2502.12018v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection", "abstract": "Visual instruction tuning refines pre-trained Multimodal Large Language\nModels (MLLMs) to enhance their real-world task performance. However, the rapid\nexpansion of visual instruction datasets introduces significant data\nredundancy, leading to excessive computational costs. Existing data selection\nmethods predominantly rely on proxy models or loss-based metrics, both of which\nimpose substantial computational overheads due to the necessity of model\ninference and backpropagation. To address this challenge, we propose PRISM, a\nnovel training-free approach for efficient multimodal data selection. Unlike\nexisting methods, PRISM eliminates the reliance on proxy models, warm-up\npretraining, and gradient-based optimization. Instead, it leverages Pearson\ncorrelation analysis to quantify the intrinsic visual encoding properties of\nMLLMs, computing a task-specific correlation score to identify high-value\ninstances. This not only enbles data-efficient selection,but maintains the\noriginal performance. Empirical evaluations across multiple MLLMs demonstrate\nthat PRISM reduces the overall time required for visual instruction tuning and\ndata selection to just 30% of conventional methods, while surpassing fully\nfine-tuned models across eight multimodal and three language understanding\nbenchmarks, achieving a 101.7% relative improvement in final performance.", "published": "2025-02-17 18:43:41", "link": "http://arxiv.org/abs/2502.12119v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws", "abstract": "Scaling laws guide the development of large language models (LLMs) by\noffering estimates for the optimal balance of model size, tokens, and compute.\nMore recently, loss-to-loss scaling laws that relate losses across pretraining\ndatasets and downstream tasks have emerged as a powerful tool for understanding\nand improving LLM performance. In this work, we investigate which factors most\nstrongly influence loss-to-loss scaling. Our experiments reveal that the\npretraining data and tokenizer determine the scaling trend. In contrast, model\nsize, optimization hyperparameters, and even significant architectural\ndifferences, such as between transformer-based models like Llama and\nstate-space models like Mamba, have limited impact. Consequently, practitioners\nshould carefully curate suitable pretraining datasets for optimal downstream\nperformance, while architectures and other settings can be freely optimized for\ntraining efficiency.", "published": "2025-02-17 18:45:25", "link": "http://arxiv.org/abs/2502.12120v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "HARBOR: Exploring Persona Dynamics in Multi-Agent Competition", "abstract": "We investigate factors contributing to LLM agents' success in competitive\nmulti-agent environments, using auctions as a testbed where agents bid to\nmaximize profit. The agents are equipped with bidding domain knowledge,\ndistinct personas that reflect item preferences, and a memory of auction\nhistory. Our work extends the classic auction scenario by creating a realistic\nenvironment where multiple agents bid on houses, weighing aspects such as size,\nlocation, and budget to secure the most desirable homes at the lowest prices.\nParticularly, we investigate three key questions: (a) How does a persona\ninfluence an agent's behavior in a competitive setting? (b) Can an agent\neffectively profile its competitors' behavior during auctions? (c) How can\npersona profiling be leveraged to create an advantage using strategies such as\ntheory of mind? Through a series of experiments, we analyze the behaviors of\nLLM agents and shed light on new findings. Our testbed, called HARBOR, offers a\nvaluable platform for deepening our understanding of multi-agent workflows in\ncompetitive environments.", "published": "2025-02-17 18:58:36", "link": "http://arxiv.org/abs/2502.12149v1", "categories": ["cs.MA", "cs.AI", "cs.CL"], "primary_category": "cs.MA"}
{"title": "Enhancing Frame Detection with Retrieval Augmented Generation", "abstract": "Recent advancements in Natural Language Processing have significantly\nimproved the extraction of structured semantic representations from\nunstructured text, especially through Frame Semantic Role Labeling (FSRL).\nDespite this progress, the potential of Retrieval-Augmented Generation (RAG)\nmodels for frame detection remains under-explored. In this paper, we present\nthe first RAG-based approach for frame detection called RCIF (Retrieve\nCandidates and Identify Frames). RCIF is also the first approach to operate\nwithout the need for explicit target span and comprises three main stages: (1)\ngeneration of frame embeddings from various representations ; (2) retrieval of\ncandidate frames given an input text; and (3) identification of the most\nsuitable frames. We conducted extensive experiments across multiple\nconfigurations, including zero-shot, few-shot, and fine-tuning settings. Our\nresults show that our retrieval component significantly reduces the complexity\nof the task by narrowing the search space thus allowing the frame identifier to\nrefine and complete the set of candidates. Our approach achieves\nstate-of-the-art performance on FrameNet 1.5 and 1.7, demonstrating its\nrobustness in scenarios where only raw text is provided. Furthermore, we\nleverage the structured representation obtained through this method as a proxy\nto enhance generalization across lexical variations in the task of translating\nnatural language questions into SPARQL queries.", "published": "2025-02-17 02:34:02", "link": "http://arxiv.org/abs/2502.12210v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly\n  Possess Test-Time Scaling Capabilities?", "abstract": "The advent of test-time scaling in large language models (LLMs), exemplified\nby OpenAI's o1 series, has advanced reasoning capabilities by scaling\ncomputational resource allocation during inference. While successors like QwQ,\nDeepseek-R1 (R1) and LIMO replicate these advancements, whether these models\ntruly possess test-time scaling capabilities remains underexplored. This study\nfound that longer CoTs of these o1-like models do not consistently enhance\naccuracy; in fact, correct solutions are often shorter than incorrect ones for\nthe same questions. Further investigation shows this phenomenon is closely\nrelated to models' self-revision capabilities - longer CoTs contain more\nself-revisions, which often lead to performance degradation. We then compare\nsequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that\nparallel scaling achieves better coverage and scalability. Based on these\ninsights, we propose Shortest Majority Vote, a method that combines parallel\nscaling strategies with CoT length characteristics, significantly improving\nmodels' test-time scalability compared to conventional majority voting\napproaches.", "published": "2025-02-17 07:21:11", "link": "http://arxiv.org/abs/2502.12215v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs", "abstract": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.", "published": "2025-02-17 08:39:43", "link": "http://arxiv.org/abs/2502.12216v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Optimal Brain Iterative Merging: Mitigating Interference in LLM Merging", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities, but\ntheir high computational costs pose challenges for customization. Model merging\noffers a cost-effective alternative, yet existing methods suffer from\ninterference among parameters, leading to performance degradation. In this\nwork, we propose Optimal Brain Iterative Merging (OBIM), a novel method\ndesigned to mitigate both intra-model and inter-model interference. OBIM\nconsists of two key components: (1) A saliency measurement mechanism that\nevaluates parameter importance based on loss changes induced by individual\nweight alterations, reducing intra-model interference by preserving only\nhigh-saliency parameters. (2) A mutually exclusive iterative merging framework,\nwhich incrementally integrates models using a binary mask to avoid direct\nparameter averaging, thereby mitigating inter-model interference. We validate\nOBIM through experiments on both Supervised Fine-Tuned (SFT) models and\npost-pretrained checkpoints. The results show that OBIM significantly\noutperforms existing merging techniques. Overall, OBIM provides an effective\nand practical solution for enhancing LLM merging.", "published": "2025-02-17 09:07:49", "link": "http://arxiv.org/abs/2502.12217v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Learning to Reason at the Frontier of Learnability", "abstract": "Reinforcement learning is now widely adopted as the final stage of large\nlanguage model training, especially for reasoning-style tasks such as maths\nproblems. Typically, models attempt each question many times during a single\ntraining step and attempt to learn from their successes and failures. However,\nwe demonstrate that throughout training with two popular algorithms (PPO and\nVinePPO) on two widely used datasets, many questions are either solved by all\nattempts - meaning they are already learned - or by none - providing no\nmeaningful training signal. To address this, we adapt a method from the\nreinforcement learning literature - sampling for learnability - and apply it to\nthe reinforcement learning stage of LLM training. Our curriculum prioritises\nquestions with high variance of success, i.e. those where the agent sometimes\nsucceeds, but not always. Our findings demonstrate that this curriculum\nconsistently boosts training performance across multiple algorithms and\ndatasets, paving the way for more efficient and effective reinforcement\nlearning with LLMs.", "published": "2025-02-17 19:16:37", "link": "http://arxiv.org/abs/2502.12272v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Integrating Expert Knowledge into Logical Programs via LLMs", "abstract": "This paper introduces ExKLoP, a novel framework designed to evaluate how\neffectively Large Language Models (LLMs) integrate expert knowledge into\nlogical reasoning systems. This capability is especially valuable in\nengineering, where expert knowledge-such as manufacturer-recommended\noperational ranges-can be directly embedded into automated monitoring systems.\nBy mirroring expert verification steps, tasks like range checking and\nconstraint validation help ensure system safety and reliability. Our approach\nsystematically evaluates LLM-generated logical rules, assessing both syntactic\nfluency and logical correctness in these critical validation tasks. We also\nexplore the models capacity for self-correction via an iterative feedback loop\nbased on code execution outcomes. ExKLoP presents an extensible dataset\ncomprising 130 engineering premises, 950 prompts, and corresponding validation\npoints. It enables comprehensive benchmarking while allowing control over task\ncomplexity and scalability of experiments. We leverage the synthetic data\ncreation methodology to conduct extensive empirical evaluation on a diverse set\nof LLMs including Llama3, Gemma, Mixtral, Mistral, and Qwen. Results reveal\nthat while models generate nearly perfect syntactically correct code, they\nfrequently exhibit logical errors in translating expert knowledge. Furthermore,\niterative self-correction yields only marginal improvements (up to 3%).\nOverall, ExKLoP serves as a robust evaluation platform that streamlines the\nselection of effective models for self-correcting systems while clearly\ndelineating the types of errors encountered. The complete implementation, along\nwith all relevant data, is available at GitHub.", "published": "2025-02-17 19:18:23", "link": "http://arxiv.org/abs/2502.12275v1", "categories": ["cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Factual Inconsistency in Data-to-Text Generation Scales Exponentially\n  with LLM Size: A Statistical Validation", "abstract": "Monitoring factual inconsistency is essential for ensuring trustworthiness in\ndata-to-text generation (D2T). While large language models (LLMs) have\ndemonstrated exceptional performance across various D2T tasks, previous studies\non scaling laws have primarily focused on generalization error through power\nlaw scaling to LLM size (i.e., the number of model parameters). However, no\nresearch has examined the impact of LLM size on factual inconsistency in D2T.\nIn this paper, we investigate how factual inconsistency in D2T scales with LLM\nsize by exploring two scaling laws: power law and exponential scaling. To\nrigorously evaluate and compare these scaling laws, we employ a statistical\nvalidation framework consisting of three key stages: predictive performance\nestimation, goodness-of-fit assessment, and comparative analysis. For a\ncomprehensive empirical study, we analyze three popular LLM families across\nfive D2T datasets, measuring factual inconsistency inversely using four\nstate-of-the-art consistency metrics. Our findings, based on exhaustive\nempirical results and validated through our framework, reveal that, contrary to\nthe widely assumed power law scaling, factual inconsistency in D2T follows an\nexponential scaling with LLM size.", "published": "2025-02-17 23:24:00", "link": "http://arxiv.org/abs/2502.12372v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Can Help Mitigate Barren Plateaus", "abstract": "In the era of noisy intermediate-scale quantum (NISQ) computing, Quantum\nNeural Networks (QNNs) have emerged as a promising approach for various\napplications, yet their training is often hindered by barren plateaus (BPs),\nwhere gradient variance vanishes exponentially as the model size increases. To\naddress this challenge, we propose a new Large Language Model (LLM)-driven\nsearch framework, AdaInit, that iteratively searches for optimal initial\nparameters of QNNs to maximize gradient variance and therefore mitigate BPs.\nUnlike conventional one-time initialization methods, AdaInit dynamically\nrefines QNN's initialization using LLMs with adaptive prompting. Theoretical\nanalysis of the Expected Improvement (EI) proves a supremum for the search,\nensuring this process can eventually identify the optimal initial parameter of\nthe QNN. Extensive experiments across four public datasets demonstrate that\nAdaInit significantly enhances QNN's trainability compared to classic\ninitialization methods, validating its effectiveness in mitigating BPs.", "published": "2025-02-17 05:57:15", "link": "http://arxiv.org/abs/2502.13166v1", "categories": ["quant-ph", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "quant-ph"}
{"title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer", "abstract": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code will be made available at blind_review.", "published": "2025-02-17 08:12:34", "link": "http://arxiv.org/abs/2502.15779v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Invisible Walls in Cities: Leveraging Large Language Models to Predict\n  Urban Segregation Experience with Social Media Content", "abstract": "Understanding experienced segregation in urban daily life is crucial for\naddressing societal inequalities and fostering inclusivity. The abundance of\nuser-generated reviews on social media encapsulates nuanced perceptions and\nfeelings associated with different places, offering rich insights into\nsegregation. However, leveraging this data poses significant challenges due to\nits vast volume, ambiguity, and confluence of diverse perspectives. To tackle\nthese challenges, we propose using Large Language Models (LLMs) to automate\nonline review mining for segregation prediction. We design a Reflective LLM\nCoder to digest social media content into insights consistent with real-world\nfeedback, and eventually produce a codebook capturing key dimensions that\nsignal segregation experience, such as cultural resonance and appeal,\naccessibility and convenience, and community engagement and local involvement.\nGuided by the codebook, LLMs can generate both informative review summaries and\nratings for segregation prediction. Moreover, we design a\nREasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and\nembedding capabilities of language models to integrate multi-channel features\nfor segregation prediction. Experiments on real-world data demonstrate that our\nframework greatly improves prediction accuracy, with a 22.79% elevation in R2\nand a 9.33% reduction in MSE. The derived codebook is generalizable across\nthree different cities, consistently improving prediction accuracy. Moreover,\nour user study confirms that the codebook-guided summaries provide cognitive\ngains for human participants in perceiving POIs' social inclusiveness. Our\nstudy marks an important step toward understanding implicit social barriers and\ninequalities, demonstrating the great potential of promoting social\ninclusiveness with AI.", "published": "2025-02-17 09:52:17", "link": "http://arxiv.org/abs/2503.04773v2", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Toward Metaphor-Fluid Conversation Design for Voice User Interfaces", "abstract": "Metaphors play a critical role in shaping user experiences with Voice User\nInterfaces (VUIs), yet existing designs often rely on static, human-centric\nmetaphors that fail to adapt to diverse contexts and user needs. This paper\nintroduces Metaphor-Fluid Design, a novel approach that dynamically adjusts\nmetaphorical representations based on conversational use-contexts. We compare\nthis approach to a Default VUI, which characterizes the present implementation\nof commercial VUIs commonly designed around the persona of an assistant,\noffering a uniform interaction style across contexts. In Study 1 (N=130),\nmetaphors were mapped to four key use-contexts-commands, information seeking,\nsociality, and error recovery-along the dimensions of formality and hierarchy,\nrevealing distinct preferences for task-specific metaphorical designs. Study 2\n(N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the\nMetaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and\nlikability by aligning better with user expectations for different contexts.\nHowever, individual differences in metaphor preferences highlight the need for\npersonalization. These findings challenge the one-size-fits-all paradigm of VUI\ndesign and demonstrate the potential of Metaphor-Fluid Design to create more\nadaptive and engaging human-AI interactions.", "published": "2025-02-17 08:36:12", "link": "http://arxiv.org/abs/2502.11554v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.ET"], "primary_category": "cs.HC"}
{"title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration", "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent.", "published": "2025-02-17 15:09:45", "link": "http://arxiv.org/abs/2502.11882v4", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Step-Audio: Unified Understanding and Generation in Intelligent Speech\n  Interaction", "abstract": "Real-time speech interaction, serving as a fundamental interface for\nhuman-machine collaboration, holds immense potential. However, current\nopen-source models face limitations such as high costs in voice data\ncollection, weakness in dynamic control, and limited intelligence. To address\nthese challenges, this paper introduces Step-Audio, the first production-ready\nopen-source solution. Key contributions include: 1) a 130B-parameter unified\nspeech-text multi-modal model that achieves unified understanding and\ngeneration, with the Step-Audio-Chat version open-sourced; 2) a generative\nspeech data engine that establishes an affordable voice cloning framework and\nproduces the open-sourced lightweight Step-Audio-TTS-3B model through\ndistillation; 3) an instruction-driven fine control system enabling dynamic\nadjustments across dialects, emotions, singing, and RAP; 4) an enhanced\ncognitive architecture augmented with tool calling and role-playing abilities\nto manage complex tasks effectively. Based on our new StepEval-Audio-360\nevaluation benchmark, Step-Audio achieves state-of-the-art performance in human\nevaluations, especially in terms of instruction following. On open-source\nbenchmarks like LLaMA Question, shows 9.3% average performance improvement,\ndemonstrating our commitment to advancing the development of open-source\nmulti-modal language technologies. Our code and models are available at\nhttps://github.com/stepfun-ai/Step-Audio.", "published": "2025-02-17 15:58:56", "link": "http://arxiv.org/abs/2502.11946v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Improving Rare-Word Recognition of Whisper in Zero-Shot Settings", "abstract": "Whisper, despite being trained on 680K hours of web-scaled audio data, faces\ndifficulty in recognising rare words like domain-specific terms, with a\nsolution being contextual biasing through prompting. To improve upon this\nmethod, in this paper, we propose a supervised learning strategy to fine-tune\nWhisper for contextual biasing instruction. We demonstrate that by using only\n670 hours of Common Voice English set for fine-tuning, our model generalises to\n11 diverse open-source English datasets, achieving a 45.6% improvement in\nrecognition of rare words and 60.8% improvement in recognition of words unseen\nduring fine-tuning over the baseline method. Surprisingly, our model's\ncontextual biasing ability generalises even to languages unseen during\nfine-tuning.", "published": "2025-02-17 09:06:34", "link": "http://arxiv.org/abs/2502.11572v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LMFCA-Net: A Lightweight Model for Multi-Channel Speech Enhancement with\n  Efficient Narrow-Band and Cross-Band Attention", "abstract": "Deep learning based end-to-end multi-channel speech enhancement methods have\nachieved impressive performance by leveraging sub-band, cross-band, and spatial\ninformation. However, these methods often demand substantial computational\nresources, limiting their practicality on terminal devices. This paper presents\na lightweight multi-channel speech enhancement network with decoupled fully\nconnected attention (LMFCA-Net). The proposed LMFCA-Net introduces time-axis\ndecoupled fully-connected attention (T-FCA) and frequency-axis decoupled\nfully-connected attention (F-FCA) mechanisms to effectively capture long-range\nnarrow-band and cross-band information without recurrent units. Experimental\nresults show that LMFCA-Net performs comparably to state-of-the-art methods\nwhile significantly reducing computational complexity and latency, making it a\npromising solution for practical applications.", "published": "2025-02-17 05:42:03", "link": "http://arxiv.org/abs/2502.11462v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "TAPS: Throat and Acoustic Paired Speech Dataset for Deep Learning-Based\n  Speech Enhancement", "abstract": "In high-noise environments such as factories, subways, and busy streets,\ncapturing clear speech is challenging due to background noise. Throat\nmicrophones provide a solution with their noise-suppressing properties,\nreducing the noise while recording speech. However, a significant limitation\nremains: high-frequency information is attenuated as sound waves pass through\nskin and tissue, reducing speech clarity. Recent deep learning approaches have\nshown promise in enhancing throat microphone recordings, but further progress\nis constrained by the absence of standardized dataset. We introduce a throat\nand acoustic paired speech dataset (TAPS), a collection of paired utterances\nrecorded from 60 native Korean speakers using throat and acoustic microphones.\nTo demonstrate the TAPS's utility, we tested three baseline deep learning\nmodels and identified the mapping-based approach as superior in improving\nspeech quality and restoring content. Additionally, we propose an optimal\nmethod to mitigate the signal mismatch between throat and acoustic microphones,\nensuring model performance. These results highlight the potential of TAPS to\nserve as a standardized dataset and advance research in throat microphone-based\nspeech enhancement.", "published": "2025-02-17 06:29:11", "link": "http://arxiv.org/abs/2502.11478v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NaturalL2S: End-to-End High-quality Multispeaker Lip-to-Speech Synthesis\n  with Differential Digital Signal Processing", "abstract": "Recent advancements in visual speech recognition (VSR) have promoted progress\nin lip-to-speech synthesis, where pre-trained VSR models enhance the\nintelligibility of synthesized speech by providing valuable semantic\ninformation. The success achieved by cascade frameworks, which combine\npseudo-VSR with pseudo-text-to-speech (TTS) or implicitly utilize the\ntranscribed text, highlights the benefits of leveraging VSR models. However,\nthese methods typically rely on mel-spectrograms as an intermediate\nrepresentation, which may introduce a key bottleneck: the domain gap between\nsynthetic mel-spectrograms, generated from inherently error-prone lip-to-speech\nmappings, and real mel-spectrograms used to train vocoders. This mismatch\ninevitably degrades synthesis quality. To bridge this gap, we propose Natural\nLip-to-Speech (NaturalL2S), an end-to-end framework integrating acoustic\ninductive biases with differentiable speech generation components.\nSpecifically, we introduce a fundamental frequency (F0) predictor to capture\nprosodic variations in synthesized speech. The predicted F0 then drives a\nDifferentiable Digital Signal Processing (DDSP) synthesizer to generate a\ncoarse signal which serves as prior information for subsequent speech\nsynthesis. Additionally, instead of relying on a reference speaker embedding as\nan auxiliary input, our approach achieves satisfactory performance on speaker\nsimilarity without explicitly modelling speaker characteristics. Both objective\nand subjective evaluation results demonstrate that NaturalL2S can effectively\nenhance the quality of the synthesized speech when compared to state-of-the-art\nmethods. Our demonstration page is accessible at\nhttps://yifan-liang.github.io/NaturalL2S/.", "published": "2025-02-17 16:40:23", "link": "http://arxiv.org/abs/2502.12002v1", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "VANPY: Voice Analysis Framework", "abstract": "Voice data is increasingly being used in modern digital communications, yet\nthere is still a lack of comprehensive tools for automated voice analysis and\ncharacterization. To this end, we developed the VANPY (Voice Analysis in\nPython) framework for automated pre-processing, feature extraction, and\nclassification of voice data. The VANPY is an open-source end-to-end\ncomprehensive framework that was developed for the purpose of speaker\ncharacterization from voice data. The framework is designed with extensibility\nin mind, allowing for easy integration of new components and adaptation to\nvarious voice analysis applications. It currently incorporates over fifteen\nvoice analysis components - including music/speech separation, voice activity\ndetection, speaker embedding, vocal feature extraction, and various\nclassification models.\n  Four of the VANPY's components were developed in-house and integrated into\nthe framework to extend its speaker characterization capabilities: gender\nclassification, emotion classification, age regression, and height regression.\nThe models demonstrate robust performance across various datasets, although not\nsurpassing state-of-the-art performance.\n  As a proof of concept, we demonstrate the framework's ability to extract\nspeaker characteristics on a use-case challenge of analyzing character voices\nfrom the movie \"Pulp Fiction.\" The results illustrate the framework's\ncapability to extract multiple speaker characteristics, including gender, age,\nheight, emotion type, and emotion intensity measured across three dimensions:\narousal, dominance, and valence.", "published": "2025-02-17 21:12:57", "link": "http://arxiv.org/abs/2502.17579v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient Finetuning for Dimensional Speech Emotion Recognition in the\n  Age of Transformers", "abstract": "Accurate speech emotion recognition is essential for developing human-facing\nsystems. Recent advancements have included finetuning large, pretrained\ntransformer models like Wav2Vec 2.0. However, the finetuning process requires\nsubstantial computational resources, including high-memory GPUs and significant\nprocessing time. As the demand for accurate emotion recognition continues to\ngrow, efficient finetuning approaches are needed to reduce the computational\nburden. Our study focuses on dimensional emotion recognition, predicting\nattributes such as activation (calm to excited) and valence (negative to\npositive). We present various finetuning techniques, including full finetuning,\npartial finetuning of transformer layers, finetuning with mixed precision,\npartial finetuning with caching, and low-rank adaptation (LoRA) on the Wav2Vec\n2.0 base model. We find that partial finetuning with mixed precision achieves\nperformance comparable to full finetuning while increasing training speed by\n67%. Caching intermediate representations further boosts efficiency, yielding\nan 88% speedup and a 71% reduction in learnable parameters. We recommend\nfinetuning the final three transformer layers in mixed precision to balance\nperformance and training efficiency, and adding intermediate representation\ncaching for optimal speed with minimal performance trade-offs. These findings\nlower the barriers to finetuning speech emotion recognition systems, making\naccurate emotion recognition more accessible to a broader range of researchers\nand practitioners.", "published": "2025-02-17 22:34:08", "link": "http://arxiv.org/abs/2503.03756v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NOTA: Multimodal Music Notation Understanding for Visual Large Language\n  Model", "abstract": "Symbolic music is represented in two distinct forms: two-dimensional,\nvisually intuitive score images, and one-dimensional, standardized text\nannotation sequences. While large language models have shown extraordinary\npotential in music, current research has primarily focused on unimodal symbol\nsequence text. Existing general-domain visual language models still lack the\nability of music notation understanding. Recognizing this gap, we propose NOTA,\nthe first large-scale comprehensive multimodal music notation dataset. It\nconsists of 1,019,237 records, from 3 regions of the world, and contains 3\ntasks. Based on the dataset, we trained NotaGPT, a music notation visual large\nlanguage model. Specifically, we involve a pre-alignment training phase for\ncross-modal alignment between the musical notes depicted in music score images\nand their textual representation in ABC notation. Subsequent training phases\nfocus on foundational music information extraction, followed by training on\nmusic notation analysis. Experimental results demonstrate that our NotaGPT-7B\nachieves significant improvement on music understanding, showcasing the\neffectiveness of NOTA and the training pipeline. Our datasets are open-sourced\nat https://huggingface.co/datasets/MYTH-Lab/NOTA-dataset.", "published": "2025-02-17 16:39:19", "link": "http://arxiv.org/abs/2502.14893v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
