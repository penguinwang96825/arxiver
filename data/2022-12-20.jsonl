{"title": "AnyTOD: A Programmable Task-Oriented Dialog System", "abstract": "We propose AnyTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system\ncapable of handling unseen tasks without task-specific training. We view TOD as\na program executed by a language model (LM), where program logic and ontology\nis provided by a designer as a schema. To enable generalization to unseen\nschemas and programs without prior training, AnyTOD adopts a neuro-symbolic\napproach. A neural LM keeps track of events occurring during a conversation and\na symbolic program implementing the dialog policy is executed to recommend next\nactions AnyTOD should take. This approach drastically reduces data annotation\nand model training requirements, addressing the enduring challenge of rapidly\nadapting a TOD system to unseen tasks and domains. We demonstrate\nstate-of-the-art results on STAR, ABCD and SGD benchmarks. We also demonstrate\nstrong zero-shot transfer ability in low-resource settings, such as zero-shot\non MultiWOZ. In addition, we release STARv2, an updated version of the STAR\ndataset with richer annotations, for benchmarking zero-shot end-to-end TOD\nmodels.", "published": "2022-12-20 01:23:01", "link": "http://arxiv.org/abs/2212.09939v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dialog2API: Task-Oriented Dialogue with API Description and Example\n  Programs", "abstract": "Functionality and dialogue experience are two important factors of\ntask-oriented dialogue systems. Conventional approaches with closed schema\n(e.g., conversational semantic parsing) often fail as both the functionality\nand dialogue experience are strongly constrained by the underlying schema. We\nintroduce a new paradigm for task-oriented dialogue - Dialog2API - to greatly\nexpand the functionality and provide seamless dialogue experience. The\nconversational model interacts with the environment by generating and executing\nprograms triggering a set of pre-defined APIs. The model also manages the\ndialogue policy and interact with the user through generating appropriate\nnatural language responses. By allowing generating free-form programs,\nDialog2API supports composite goals by combining different APIs, whereas\nunrestricted program revision provides natural and robust dialogue experience.\nTo facilitate Dialog2API, the core model is provided with API documents, an\nexecution environment and optionally some example dialogues annotated with\nprograms. We propose an approach tailored for the Dialog2API, where the\ndialogue states are represented by a stack of programs, with most recently\nmentioned program on the top of the stack. Dialog2API can work with many\napplication scenarios such as software automation and customer service. In this\npaper, we construct a dataset for AWS S3 APIs and present evaluation results of\nin-context learning baselines.", "published": "2022-12-20 01:52:46", "link": "http://arxiv.org/abs/2212.09946v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of\n  Faithfulness Metrics", "abstract": "The proliferation of automatic faithfulness metrics for summarization has\nproduced a need for benchmarks to evaluate them. While existing benchmarks\nmeasure the correlation with human judgements of faithfulness on\nmodel-generated summaries, they are insufficient for diagnosing whether metrics\nare: 1) consistent, i.e., indicate lower faithfulness as errors are introduced\ninto a summary, 2) effective on human-written texts, and 3) sensitive to\ndifferent error types (as summaries can contain multiple errors). To address\nthese needs, we present a benchmark of unfaithful minimal pairs (BUMP), a\ndataset of 889 human-written, minimally different summary pairs, where a single\nerror is introduced to a summary from the CNN/DailyMail dataset to produce an\nunfaithful summary. We find BUMP complements existing benchmarks in a number of\nways: 1) the summaries in BUMP are harder to discriminate and less probable\nunder SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be\nused to measure the consistency of metrics, and reveals that the most\ndiscriminative metrics tend not to be the most consistent, and 3) unlike\ndatasets containing generated summaries with multiple errors, BUMP enables the\nmeasurement of metrics' performance on individual error types.", "published": "2022-12-20 02:17:30", "link": "http://arxiv.org/abs/2212.09955v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Improving Summarization Factual Consistency from Natural Language\n  Feedback", "abstract": "Despite the recent progress in language generation models, their outputs may\nnot always meet user expectations. In this work, we study whether informational\nfeedback in natural language can be leveraged to improve generation quality and\nuser preference alignment. To this end, we consider factual consistency in\nsummarization, the quality that the summary should only contain information\nsupported by the input documents, as the user-expected preference. We collect a\nhigh-quality dataset, DeFacto, containing human demonstrations and\ninformational natural language feedback consisting of corrective instructions,\nedited summaries, and explanations with respect to the factual consistency of\nthe summary. Using our dataset, we study three natural language generation\ntasks: (1) editing a summary by following the human feedback, (2) generating\nhuman feedback for editing the original summary, and (3) revising the initial\nsummary to correct factual errors by generating both the human feedback and\nedited summary. We show that DeFacto can provide factually consistent\nhuman-edited summaries and further insights into summarization factual\nconsistency thanks to its informational natural language feedback. We further\ndemonstrate that fine-tuned language models can leverage our dataset to improve\nthe summary factual consistency, while large language models lack the zero-shot\nlearning ability in our proposed tasks that require controllable text\ngeneration.", "published": "2022-12-20 02:47:37", "link": "http://arxiv.org/abs/2212.09968v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Robustness of Text-to-SQL Models Against Natural and Realistic\n  Adversarial Table Perturbation", "abstract": "The robustness of Text-to-SQL parsers against adversarial perturbations plays\na crucial role in delivering highly reliable applications. Previous studies\nalong this line primarily focused on perturbations in the natural language\nquestion side, neglecting the variability of tables. Motivated by this, we\npropose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to\nmeasure the robustness of Text-to-SQL models. Following this proposition, we\ncurate ADVETA, the first robustness evaluation benchmark featuring natural and\nrealistic ATPs. All tested state-of-the-art models experience dramatic\nperformance drops on ADVETA, revealing models' vulnerability in real-world\npractices. To defend against ATP, we build a systematic adversarial training\nexample generation framework tailored for better contextualization of tabular\ndata. Experiments show that our approach not only brings the best robustness\nimprovement against table-side perturbations but also substantially empowers\nmodels against NL-side perturbations. We release our benchmark and code at:\nhttps://github.com/microsoft/ContextualSP.", "published": "2022-12-20 04:38:23", "link": "http://arxiv.org/abs/2212.09994v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of\n  What Matters", "abstract": "Chain-of-Thought (CoT) prompting can dramatically improve the multi-step\nreasoning abilities of large language models (LLMs). CoT explicitly encourages\nthe LLM to generate intermediate rationales for solving a problem, by providing\na series of reasoning steps in the demonstrations. Despite its success, there\nis still little understanding of what makes CoT prompting effective and which\naspects of the demonstrated reasoning steps contribute to its performance. In\nthis paper, we show that CoT reasoning is possible even with invalid\ndemonstrations - prompting with invalid reasoning steps can achieve over 80-90%\nof the performance obtained using CoT under various metrics, while still\ngenerating coherent lines of reasoning during inference. Further experiments\nshow that other aspects of the rationales, such as being relevant to the query\nand correctly ordering the reasoning steps, are much more important for\neffective CoT reasoning. Overall, these findings both deepen our understanding\nof CoT prompting, and open up new questions regarding LLMs' capability to learn\nto reason in context.", "published": "2022-12-20 05:20:54", "link": "http://arxiv.org/abs/2212.10001v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "(QA)$^2$: Question Answering with Questionable Assumptions", "abstract": "Naturally occurring information-seeking questions often contain questionable\nassumptions -- assumptions that are false or unverifiable. Questions containing\nquestionable assumptions are challenging because they require a distinct answer\nstrategy that deviates from typical answers for information-seeking questions.\nFor instance, the question \"When did Marie Curie discover Uranium?\" cannot be\nanswered as a typical \"when\" question without addressing the false assumption\n\"Marie Curie discovered Uranium\". In this work, we propose (QA)$^2$ (Question\nAnswering with Questionable Assumptions), an open-domain evaluation dataset\nconsisting of naturally occurring search engine queries that may or may not\ncontain questionable assumptions. To be successful on (QA)$^2$, systems must be\nable to detect questionable assumptions and also be able to produce adequate\nresponses for both typical information-seeking questions and ones with\nquestionable assumptions. Through human rater acceptability on end-to-end QA\nwith (QA)$^2$, we find that current models do struggle with handling\nquestionable assumptions, leaving substantial headroom for progress.", "published": "2022-12-20 05:25:12", "link": "http://arxiv.org/abs/2212.10003v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PLUE: Language Understanding Evaluation Benchmark for Privacy Policies\n  in English", "abstract": "Privacy policies provide individuals with information about their rights and\nhow their personal information is handled. Natural language understanding (NLU)\ntechnologies can support individuals and practitioners to understand better\nprivacy practices described in lengthy and complex documents. However, existing\nefforts that use NLU technologies are limited by processing the language in a\nway exclusive to a single task focusing on certain privacy practices. To this\nend, we introduce the Privacy Policy Language Understanding Evaluation (PLUE)\nbenchmark, a multi-task benchmark for evaluating the privacy policy language\nunderstanding across various tasks. We also collect a large corpus of privacy\npolicies to enable privacy policy domain-specific language model pre-training.\nWe evaluate several generic pre-trained language models and continue\npre-training them on the collected corpus. We demonstrate that domain-specific\ncontinual pre-training offers performance improvements across all tasks.", "published": "2022-12-20 05:58:32", "link": "http://arxiv.org/abs/2212.10011v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Modeling with Latent Situations", "abstract": "Language models (LMs) often generate incoherent outputs: they refer to events\nand entity states that are incompatible with the state of the world described\nin their inputs. We introduce SituationSupervision, a family of approaches for\nimproving coherence in LMs by training them to construct and condition on\nexplicit representations of entities and their states. SituationSupervision has\ntwo components: an auxiliary situation modeling task that trains models to\npredict state representations in context, and a latent state inference\nprocedure that imputes these states from partially annotated training data.\nSituationSupervision can be applied to both fine-tuning (by supervising LMs to\nencode state variables in their hidden representations) and prompting (by\ninducing LMs to interleave textual descriptions of entity states with output\ntext). In both cases, SituationSupervision requires only a small number of\nstate annotations to produce major coherence improvements (between 4-11%),\nshowing that standard LMs can be sample-efficiently trained to model not just\nlanguage but the situations it describes.", "published": "2022-12-20 05:59:42", "link": "http://arxiv.org/abs/2212.10012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization", "abstract": "Dialogue summarization has recently garnered significant attention due to its\nwide range of applications. However, existing methods for summarizing dialogues\nhave limitations because they do not take into account the inherent structure\nof dialogue and rely heavily on labeled data, which can lead to poor\nperformance in new domains. In this work, we propose DIONYSUS (dynamic input\noptimization in pre-training for dialogue summarization), a pre-trained\nencoder-decoder model for summarizing dialogues in any new domain. To pre-train\nDIONYSUS, we create two pseudo summaries for each dialogue example: one is\nproduced by a fine-tuned summarization model, and the other is a collection of\ndialogue turns that convey important information. We then choose one of these\npseudo summaries based on the difference in information distribution across\ndifferent types of dialogues. This selected pseudo summary serves as the\nobjective for pre-training DIONYSUS using a self-supervised approach on a large\ndialogue corpus. Our experiments show that DIONYSUS outperforms existing\nmethods on six datasets, as demonstrated by its ROUGE scores in zero-shot and\nfew-shot settings.", "published": "2022-12-20 06:21:21", "link": "http://arxiv.org/abs/2212.10018v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When Do Decompositions Help for Machine Reading?", "abstract": "Answering complex questions often requires multi-step reasoning in order to\nobtain the final answer. Most research into decompositions of complex questions\ninvolves open-domain systems, which have shown success in using these\ndecompositions for improved retrieval. In the machine reading setting, however,\nwork to understand when decompositions are helpful is understudied. We conduct\nexperiments on decompositions in machine reading to unify recent work in this\nspace, using a range of models and datasets. We find that decompositions can be\nhelpful in the few-shot case, giving several points of improvement in exact\nmatch scores. However, we also show that when models are given access to\ndatasets with around a few hundred or more examples, decompositions are not\nhelpful (and can actually be detrimental). Thus, our analysis implies that\nmodels can learn decompositions implicitly even with limited data.", "published": "2022-12-20 06:23:02", "link": "http://arxiv.org/abs/2212.10019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation", "abstract": "In this work, we explore a useful but often neglected methodology for\nrobustness analysis of text generation evaluation metrics: stress tests with\nsynthetic data. Basically, we design and synthesize a wide range of potential\nerrors and check whether they result in a commensurate drop in the metric\nscores. We examine a range of recently proposed evaluation metrics based on\npretrained language models, for the tasks of open-ended generation,\ntranslation, and summarization. Our experiments reveal interesting\ninsensitivities, biases, or even loopholes in existing metrics. For example, we\nfind that BERTScore is confused by truncation errors in summarization, and\nMAUVE (built on top of GPT-2) is insensitive to errors at the beginning or\nmiddle of generations. Further, we investigate the reasons behind these blind\nspots and suggest practical workarounds for a more reliable evaluation of text\ngeneration. We have released our code and data at\nhttps://github.com/cloudygoose/blindspot_nlg.", "published": "2022-12-20 06:24:25", "link": "http://arxiv.org/abs/2212.10020v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Twitter BERT Approach for Offensive Language Detection in Marathi", "abstract": "Automated offensive language detection is essential in combating the spread\nof hate speech, particularly in social media. This paper describes our work on\nOffensive Language Identification in low resource Indic language Marathi. The\nproblem is formulated as a text classification task to identify a tweet as\noffensive or non-offensive. We evaluate different mono-lingual and\nmulti-lingual BERT models on this classification task, focusing on BERT models\npre-trained with social media datasets. We compare the performance of MuRIL,\nMahaTweetBERT, MahaTweetBERT-Hateful, and MahaBERT on the HASOC 2022 test set.\nWe also explore external data augmentation from other existing Marathi hate\nspeech corpus HASOC 2021 and L3Cube-MahaHate. The MahaTweetBERT, a BERT model,\npre-trained on Marathi tweets when fine-tuned on the combined dataset (HASOC\n2021 + HASOC 2022 + MahaHate), outperforms all models with an F1 score of 98.43\non the HASOC 2022 test set. With this, we also provide a new state-of-the-art\nresult on HASOC 2022 / MOLD v2 test set.", "published": "2022-12-20 07:22:45", "link": "http://arxiv.org/abs/2212.10039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Augmentation Strategy for Visually Rich Documents", "abstract": "Many business workflows require extracting important fields from form-like\ndocuments (e.g. bank statements, bills of lading, purchase orders, etc.).\nRecent techniques for automating this task work well only when trained with\nlarge datasets. In this work we propose a novel data augmentation technique to\nimprove performance when training data is scarce, e.g. 10-250 documents. Our\ntechnique, which we call FieldSwap, works by swapping out the key phrases of a\nsource field with the key phrases of a target field to generate new synthetic\nexamples of the target field for use in training. We demonstrate that this\napproach can yield 1-7 F1 point improvements in extraction performance.", "published": "2022-12-20 07:44:25", "link": "http://arxiv.org/abs/2212.10047v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WeCheck: Strong Factual Consistency Checker via Weakly Supervised\n  Learning", "abstract": "A crucial issue of current text generation models is that they often\nuncontrollably generate factually inconsistent text with respective of their\ninputs. Limited by the lack of annotated data, existing works in evaluating\nfactual consistency directly transfer the reasoning ability of models trained\non other data-rich upstream tasks like question answering (QA) and natural\nlanguage inference (NLI) without any further adaptation. As a result, they\nperform poorly on the real generated text and are biased heavily by their\nsingle-source upstream tasks. To alleviate this problem, we propose a weakly\nsupervised framework that aggregates multiple resources to train a precise and\nefficient factual metric, namely WeCheck. WeCheck first utilizes a generative\nmodel to accurately label a real generated sample by aggregating its weak\nlabels, which are inferred from multiple resources. Then, we train the target\nmetric model with the weak supervision while taking noises into consideration.\nComprehensive experiments on a variety of tasks demonstrate the strong\nperformance of WeCheck, which achieves a 3.4\\% absolute improvement over\nprevious state-of-the-art methods on TRUE benchmark on average.", "published": "2022-12-20 08:04:36", "link": "http://arxiv.org/abs/2212.10057v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Rule-Neural Coreference Resolution System based on Actor-Critic\n  Learning", "abstract": "A coreference resolution system is to cluster all mentions that refer to the\nsame entity in a given context. All coreference resolution systems need to\ntackle two main tasks: one task is to detect all of the potential mentions, and\nthe other is to learn the linking of an antecedent for each possible mention.\nIn this paper, we propose a hybrid rule-neural coreference resolution system\nbased on actor-critic learning, such that it can achieve better coreference\nperformance by leveraging the advantages from both the heuristic rules and a\nneural conference model. This end-to-end system can also perform both mention\ndetection and resolution by leveraging a joint training algorithm. We\nexperiment on the BERT model to generate input span representations. Our model\nwith the BERT span representation achieves the state-of-the-art performance\namong the models on the CoNLL-2012 Shared Task English Test Set.", "published": "2022-12-20 08:55:47", "link": "http://arxiv.org/abs/2212.10087v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3\n  and Challenging for GPT-4", "abstract": "Large language models (LLMs) have demonstrated solid zero-shot reasoning\ncapabilities, which is reflected in their performance on the current test\ntasks. This calls for a more challenging benchmark requiring highly advanced\nreasoning ability to be solved. In this paper, we introduce such a benchmark,\nconsisting of 191 long-form (1200 words on average) mystery narratives\nconstructed as detective puzzles. Puzzles are sourced from the \"5 Minute\nMystery\" platform and include a multiple-choice question for evaluation. Only\n47% of humans solve a puzzle successfully on average, while the best human\nsolvers achieve over 80% success rate. We show that GPT-3 models barely\noutperform random on this benchmark (with 28% accuracy) while state-of-the-art\nGPT-4 solves only 38% of puzzles. This indicates that there is still a\nsignificant gap in the deep reasoning abilities of LLMs and humans and\nhighlights the need for further research in this area. Our work introduces a\nchallenging benchmark for future studies on reasoning in language models and\ncontributes to a better understanding of the limits of LLMs' abilities.", "published": "2022-12-20 09:34:43", "link": "http://arxiv.org/abs/2212.10114v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quirk or Palmer: A Comparative Study of Modal Verb Frameworks with\n  Annotated Datasets", "abstract": "Modal verbs, such as \"can\", \"may\", and \"must\", are commonly used in daily\ncommunication to convey the speaker's perspective related to the likelihood\nand/or mode of the proposition. They can differ greatly in meaning depending on\nhow they're used and the context of a sentence (e.g. \"They 'must' help each\nother out.\" vs. \"They 'must' have helped each other out.\") Despite their\npractical importance in natural language understanding, linguists have yet to\nagree on a single, prominent framework for the categorization of modal verb\nsenses. This lack of agreement stems from high degrees of flexibility and\npolysemy from the modal verbs, making it more difficult for researchers to\nincorporate insights from this family of words into their work. This work\npresents Moverb dataset, which consists of 27,240 annotations of modal verb\nsenses over 4,540 utterances containing one or more sentences from social\nconversations. Each utterance is annotated by three annotators using two\ndifferent theoretical frameworks (i.e., Quirk and Palmer) of modal verb senses.\nWe observe that both frameworks have similar inter-annotator agreements,\ndespite having different numbers of sense types (8 for Quirk and 3 for Palmer).\nWith the RoBERTa-based classifiers fine-tuned on \\dataset, we achieve F1 scores\nof 82.2 and 78.3 on Quirk and Palmer, respectively, showing that modal verb\nsense disambiguation is not a trivial task. Our dataset will be publicly\navailable with our final version.", "published": "2022-12-20 10:44:18", "link": "http://arxiv.org/abs/2212.10152v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Naamapadam: A Large-Scale Named Entity Annotated Data for Indic\n  Languages", "abstract": "We present, Naamapadam, the largest publicly available Named Entity\nRecognition (NER) dataset for the 11 major Indian languages from two language\nfamilies. The dataset contains more than 400k sentences annotated with a total\nof at least 100k entities from three standard entity categories (Person,\nLocation, and, Organization) for 9 out of the 11 languages. The training\ndataset has been automatically created from the Samanantar parallel corpus by\nprojecting automatically tagged entities from an English sentence to the\ncorresponding Indian language translation. We also create manually annotated\ntestsets for 9 languages. We demonstrate the utility of the obtained dataset on\nthe Naamapadam-test dataset. We also release IndicNER, a multilingual IndicBERT\nmodel fine-tuned on Naamapadam training set. IndicNER achieves an F1 score of\nmore than $80$ for $7$ out of $9$ test languages. The dataset and models are\navailable under open-source licences at\nhttps://ai4bharat.iitm.ac.in/naamapadam.", "published": "2022-12-20 11:15:24", "link": "http://arxiv.org/abs/2212.10168v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Document-level Relation Extraction with Relation Correlations", "abstract": "Document-level relation extraction faces two overlooked challenges: long-tail\nproblem and multi-label problem. Previous work focuses mainly on obtaining\nbetter contextual representations for entity pairs, hardly address the above\nchallenges. In this paper, we analyze the co-occurrence correlation of\nrelations, and introduce it into DocRE task for the first time. We argue that\nthe correlations can not only transfer knowledge between data-rich relations\nand data-scarce ones to assist in the training of tailed relations, but also\nreflect semantic distance guiding the classifier to identify semantically close\nrelations for multi-label entity pairs. Specifically, we use relation embedding\nas a medium, and propose two co-occurrence prediction sub-tasks from both\ncoarse- and fine-grained perspectives to capture relation correlations.\nFinally, the learned correlation-aware embeddings are used to guide the\nextraction of relational facts. Substantial experiments on two popular DocRE\ndatasets are conducted, and our method achieves superior results compared to\nbaselines. Insightful analysis also demonstrates the potential of relation\ncorrelations to address the above challenges.", "published": "2022-12-20 11:17:52", "link": "http://arxiv.org/abs/2212.10171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Role of Parallel Data in Cross-lingual Transfer Learning", "abstract": "While prior work has established that the use of parallel data is conducive\nfor cross-lingual learning, it is unclear if the improvements come from the\ndata itself, or if it is the modeling of parallel interactions that matters.\nExploring this, we examine the usage of unsupervised machine translation to\ngenerate synthetic parallel data, and compare it to supervised machine\ntranslation and gold parallel data. We find that even model generated parallel\ndata can be useful for downstream tasks, in both a general setting (continued\npretraining) as well as the task-specific setting (translate-train), although\nour best results are still obtained using real parallel data. Our findings\nsuggest that existing multilingual models do not exploit the full potential of\nmonolingual data, and prompt the community to reconsider the traditional\ncategorization of cross-lingual learning approaches.", "published": "2022-12-20 11:23:04", "link": "http://arxiv.org/abs/2212.10173v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Human-Like Evaluation for Natural Language Generation with Error\n  Analysis", "abstract": "The state-of-the-art language model-based automatic metrics, e.g. BARTScore,\nbenefiting from large-scale contextualized pre-training, have been successfully\nused in a wide range of natural language generation (NLG) tasks, including\nmachine translation, text summarization, and data-to-text. Recent studies show\nthat considering both major errors (e.g. mistranslated tokens) and minor errors\n(e.g. imperfections in fluency) can produce high-quality human judgments. This\ninspires us to approach the final goal of the evaluation metrics (human-like\nevaluations) by automatic error analysis. To this end, we augment BARTScore by\nincorporating the human-like error analysis strategies, namely BARTScore++,\nwhere the final score consists of both the evaluations of major errors and\nminor errors. Experimental results show that BARTScore++ can consistently\nimprove the performance of vanilla BARTScore and outperform existing\ntop-scoring metrics in 20 out of 25 test settings. We hope our technique can\nalso be extended to other pre-trained model-based metrics. We will release our\ncode and scripts to facilitate the community.", "published": "2022-12-20 11:36:22", "link": "http://arxiv.org/abs/2212.10179v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for\n  Indian Languages", "abstract": "The rapid growth of machine translation (MT) systems has necessitated\ncomprehensive studies to meta-evaluate evaluation metrics being used, which\nenables a better selection of metrics that best reflect MT quality.\nUnfortunately, most of the research focuses on high-resource languages, mainly\nEnglish, the observations for which may not always apply to other languages.\nIndian languages, having over a billion speakers, are linguistically different\nfrom English, and to date, there has not been a systematic study of evaluating\nMT systems from English into Indian languages. In this paper, we fill this gap\nby creating an MQM dataset consisting of 7000 fine-grained annotations,\nspanning 5 Indian languages and 7 MT systems, and use it to establish\ncorrelations between annotator scores and scores obtained using existing\nautomatic metrics. Our results show that pre-trained metrics, such as COMET,\nhave the highest correlations with annotator scores. Additionally, we find that\nthe metrics do not adequately capture fluency-based errors in Indian languages,\nand there is a need to develop metrics focused on Indian languages. We hope\nthat our dataset and analysis will help promote further research in this area.", "published": "2022-12-20 11:37:22", "link": "http://arxiv.org/abs/2212.10180v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pay Attention to Your Tone: Introducing a New Dataset for Polite\n  Language Rewrite", "abstract": "We introduce \\textsc{PoliteRewrite} -- a dataset for polite language rewrite\nwhich is a novel sentence rewrite task. Compared with previous text style\ntransfer tasks that can be mostly addressed by slight token- or phrase-level\nedits, polite language rewrite requires deep understanding and extensive\nsentence-level edits over an offensive and impolite sentence to deliver the\nsame message euphemistically and politely, which is more challenging -- not\nonly for NLP models but also for human annotators to rewrite with effort. To\nalleviate the human effort for efficient annotation, we first propose a novel\nannotation paradigm by a collaboration of human annotators and GPT-3.5 to\nannotate \\textsc{PoliteRewrite}. The released dataset has 10K polite sentence\nrewrites annotated collaboratively by GPT-3.5 and human, which can be used as\ngold standard for training, validation and test; and 100K high-quality polite\nsentence rewrites by GPT-3.5 without human review. We wish this work (The\ndataset (10K+100K) will be released soon) could contribute to the research on\nmore challenging sentence rewrite, and provoke more thought in future on\nresource annotation paradigm with the help of the large-scaled pretrained\nmodels.", "published": "2022-12-20 12:02:34", "link": "http://arxiv.org/abs/2212.10190v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adam: Dense Retrieval Distillation with Adaptive Dark Examples", "abstract": "To improve the performance of the dual-encoder retriever, one effective\napproach is knowledge distillation from the cross-encoder ranker. Existing\nworks construct the candidate passages following the supervised learning\nsetting where a query is paired with a positive passage and a batch of\nnegatives. However, through empirical observation, we find that even the hard\nnegatives from advanced methods are still too trivial for the teacher to\ndistinguish, preventing the teacher from transferring abundant dark knowledge\nto the student through its soft label. To alleviate this issue, we propose\nADAM, a knowledge distillation framework that can better transfer the dark\nknowledge held in the teacher with Adaptive Dark exAMples. Different from\nprevious works that only rely on one positive and hard negatives as candidate\npassages, we create dark examples that all have moderate relevance to the query\nthrough mixing-up and masking in discrete space. Furthermore, as the quality of\nknowledge held in different training instances varies as measured by the\nteacher's confidence score, we propose a self-paced distillation strategy that\nadaptively concentrates on a subset of high-quality instances to conduct our\ndark-example-based knowledge distillation to help the student learn better. We\nconduct experiments on two widely-used benchmarks and verify the effectiveness\nof our method.", "published": "2022-12-20 12:03:19", "link": "http://arxiv.org/abs/2212.10192v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EIT: Enhanced Interactive Transformer", "abstract": "Two principles: the complementary principle and the consensus principle are\nwidely acknowledged in the literature of multi-view learning. However, the\ncurrent design of multi-head self-attention, an instance of multi-view\nlearning, prioritizes the complementarity while ignoring the consensus. To\naddress this problem, we propose an enhanced multi-head self-attention (EMHA).\nFirst, to satisfy the complementary principle, EMHA removes the one-to-one\nmapping constraint among queries and keys in multiple subspaces and allows each\nquery to attend to multiple keys. On top of that, we develop a method to fully\nencourage consensus among heads by introducing two interaction models, namely\ninner-subspace interaction and cross-subspace interaction. Extensive\nexperiments on a wide range of language tasks (e.g., machine translation,\nabstractive summarization and grammar correction, language modeling), show its\nsuperiority, with a very modest increase in model size. Our code would be\navailable at: https://github.com/zhengkid/EIT-Enhanced-Interactive-Transformer.", "published": "2022-12-20 12:16:46", "link": "http://arxiv.org/abs/2212.10197v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator", "abstract": "Pre-trained models have achieved remarkable success in natural language\nprocessing (NLP). However, existing pre-training methods underutilize the\nbenefits of language understanding for generation. Inspired by the idea of\nGenerative Adversarial Networks (GANs), we propose a GAN-style model for\nencoder-decoder pre-training by introducing an auxiliary discriminator,\nunifying the ability of language understanding and generation in a single\nmodel. Our model, named as GanLM, is trained with two pre-training objectives:\nreplaced token detection and replaced token denoising. Specifically, given\nmasked source sentences, the generator outputs the target distribution and the\ndiscriminator predicts whether the target sampled tokens from distribution are\nincorrect. The target sentence is replaced with misclassified tokens to\nconstruct noisy previous context, which is used to generate the gold sentence.\nIn general, both tasks improve the ability of language understanding and\ngeneration by selectively using the denoising data. Extensive experiments in\nlanguage generation benchmarks show that GanLM with the powerful language\nunderstanding capability outperforms various strong pre-trained language models\n(PLMs) and achieves state-of-the-art performance.", "published": "2022-12-20 12:51:11", "link": "http://arxiv.org/abs/2212.10218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-trained Language Models for Keyphrase Generation: A Thorough\n  Empirical Study", "abstract": "Neural models that do not rely on pre-training have excelled in the keyphrase\ngeneration task with large annotated datasets. Meanwhile, new approaches have\nincorporated pre-trained language models (PLMs) for their data efficiency.\nHowever, there lacks a systematic study of how the two types of approaches\ncompare and how different design choices can affect the performance of\nPLM-based models. To fill in this knowledge gap and facilitate a more informed\nuse of PLMs for keyphrase extraction and keyphrase generation, we present an\nin-depth empirical study. Formulating keyphrase extraction as sequence labeling\nand keyphrase generation as sequence-to-sequence generation, we perform\nextensive experiments in three domains. After showing that PLMs have\ncompetitive high-resource performance and state-of-the-art low-resource\nperformance, we investigate important design choices including in-domain PLMs,\nPLMs with different pre-training objectives, using PLMs with a parameter\nbudget, and different formulations for present keyphrases. Further results show\nthat (1) in-domain BERT-like PLMs can be used to build strong and\ndata-efficient keyphrase generation models; (2) with a fixed parameter budget,\nprioritizing model depth over width and allocating more layers in the encoder\nleads to better encoder-decoder models; and (3) introducing four in-domain\nPLMs, we achieve a competitive performance in the news domain and the\nstate-of-the-art performance in the scientific domain.", "published": "2022-12-20 13:20:21", "link": "http://arxiv.org/abs/2212.10233v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diffusion Glancing Transformer for Parallel Sequence to Sequence\n  Learning", "abstract": "Previously, non-autoregressive models were widely perceived as being superior\nin generation efficiency but inferior in generation quality due to the\ndifficulties of modeling multiple target modalities. To enhance the\nmulti-modality modeling ability, we propose the diffusion glancing transformer,\nwhich employs a modality diffusion process and residual glancing sampling. The\nmodality diffusion process is a discrete process that interpolates the\nmulti-modal distribution along the decoding steps, and the residual glancing\nsampling approach guides the model to continuously learn the remaining\nmodalities across the layers. Experimental results on various machine\ntranslation and text generation benchmarks demonstrate that DIFFGLAT achieves\nbetter generation accuracy while maintaining fast decoding speed compared with\nboth autoregressive and non-autoregressive models.", "published": "2022-12-20 13:36:25", "link": "http://arxiv.org/abs/2212.10240v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Original or Translated? On the Use of Parallel Data for Translation\n  Quality Estimation", "abstract": "Machine Translation Quality Estimation (QE) is the task of evaluating\ntranslation output in the absence of human-written references. Due to the\nscarcity of human-labeled QE data, previous works attempted to utilize the\nabundant unlabeled parallel corpora to produce additional training data with\npseudo labels. In this paper, we demonstrate a significant gap between parallel\ndata and real QE data: for QE data, it is strictly guaranteed that the source\nside is original texts and the target side is translated (namely\ntranslationese). However, for parallel data, it is indiscriminate and the\ntranslationese may occur on either source or target side. We compare the impact\nof parallel data with different translation directions in QE data augmentation,\nand find that using the source-original part of parallel corpus consistently\noutperforms its target-original counterpart. Moreover, since the WMT corpus\nlacks direction information for each parallel sentence, we train a classifier\nto distinguish source- and target-original bitext, and carry out an analysis of\ntheir difference in both style and domain. Together, these findings suggest\nusing source-original parallel data for QE data augmentation, which brings a\nrelative improvement of up to 4.0% and 6.4% compared to undifferentiated data\non sentence- and word-level QE tasks respectively.", "published": "2022-12-20 14:06:45", "link": "http://arxiv.org/abs/2212.10257v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Triplet: Leveraging the Most Data for Multimodal Machine\n  Translation", "abstract": "Multimodal machine translation (MMT) aims to improve translation quality by\nincorporating information from other modalities, such as vision. Previous MMT\nsystems mainly focus on better access and use of visual information and tend to\nvalidate their methods on image-related datasets. These studies face two\nchallenges. First, they can only utilize triple data (bilingual texts with\nimages), which is scarce; second, current benchmarks are relatively restricted\nand do not correspond to realistic scenarios. Therefore, this paper\ncorrespondingly establishes new methods and new datasets for MMT. First, we\npropose a framework 2/3-Triplet with two new approaches to enhance MMT by\nutilizing large-scale non-triple data: monolingual image-text data and parallel\ntext-only data. Second, we construct an English-Chinese {e}-commercial\n{m}ulti{m}odal {t}ranslation dataset (including training and testing), named\nEMMT, where its test set is carefully selected as some words are ambiguous and\nshall be translated mistakenly without the help of images. Experiments show\nthat our method is more suitable for real-world scenarios and can significantly\nimprove translation performance by using more non-triple data. In addition, our\nmodel also rivals various SOTA models in conventional multimodal translation\nbenchmarks.", "published": "2022-12-20 15:02:38", "link": "http://arxiv.org/abs/2212.10313v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HINT: Hypernetwork Instruction Tuning for Efficient Zero- & Few-Shot\n  Generalisation", "abstract": "Recent NLP models have shown the remarkable ability to effectively generalise\n`zero-shot' to new tasks using only natural language instructions as guidance.\nHowever, many of these approaches suffer from high computational costs due to\ntheir reliance on concatenating lengthy instructions with every input example,\nresulting in costly reprocessing of the instruction. To avoid this, we\nintroduce Hypernetworks for INstruction Tuning (HINT), which convert task\ninstructions and examples into parameter-efficient modules inserted into an\nunderlying model using a pretrained text encoder, eliminating the need to\ninclude instructions in the model input. The hypernetwork in HINT also produces\nan encoded instruction, which we concatenate with encoded inputs during\ndecoding to further improve performance. HINT models outperform strong\nstate-of-the-art baselines by over 10% when controlling for compute (measured\nin FLOPs). By converting instructions into modules, HINT models can effectively\ndisregard the length of instructions and few-shot example inputs in terms of\ncompute usage. As a result, HINT can enhance its performance by up to 25% by\nincorporating additional few-shot data, while utilizing only up to 5% more\ncompute. This combines the strengths of parameter-efficient fine-tuning and\nin-context learning.", "published": "2022-12-20 15:07:37", "link": "http://arxiv.org/abs/2212.10315v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers", "abstract": "Diffusion model, a new generative modelling paradigm, has achieved great\nsuccess in image, audio, and video generation. However, considering the\ndiscrete categorical nature of text, it is not trivial to extend continuous\ndiffusion models to natural language, and text diffusion models are less\nstudied. Sequence-to-sequence text generation is one of the essential natural\nlanguage processing topics. In this work, we apply diffusion models to approach\nsequence-to-sequence text generation, and explore whether the superiority\ngeneration performance of diffusion model can transfer to natural language\ndomain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence\ngeneration. SeqDiffuSeq uses an encoder-decoder Transformers architecture to\nmodel denoising function. In order to improve generation quality, SeqDiffuSeq\ncombines the self-conditioning technique and a newly proposed adaptive noise\nschedule technique. The adaptive noise schedule has the difficulty of denoising\nevenly distributed across time steps, and considers exclusive noise schedules\nfor tokens at different positional order. Experiment results illustrate the\ngood performance on sequence-to-sequence generation in terms of text quality\nand inference time.", "published": "2022-12-20 15:16:24", "link": "http://arxiv.org/abs/2212.10325v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CoCo: Coherence-Enhanced Machine-Generated Text Detection Under Data\n  Limitation With Contrastive Learning", "abstract": "Machine-Generated Text (MGT) detection, a task that discriminates MGT from\nHuman-Written Text (HWT), plays a crucial role in preventing misuse of text\ngenerative models, which excel in mimicking human writing style recently.\nLatest proposed detectors usually take coarse text sequences as input and\nfine-tune pretrained models with standard cross-entropy loss. However, these\nmethods fail to consider the linguistic structure of texts. Moreover, they lack\nthe ability to handle the low-resource problem which could often happen in\npractice considering the enormous amount of textual data online. In this paper,\nwe present a coherence-based contrastive learning model named CoCo to detect\nthe possible MGT under low-resource scenario. To exploit the linguistic\nfeature, we encode coherence information in form of graph into text\nrepresentation. To tackle the challenges of low data resource, we employ a\ncontrastive learning framework and propose an improved contrastive loss for\npreventing performance degradation brought by simple samples. The experiment\nresults on two public datasets and two self-constructed datasets prove our\napproach outperforms the state-of-art methods significantly. Also, we\nsurprisingly find that MGTs originated from up-to-date language models could be\neasier to detect than these from previous models, in our experiments. And we\npropose some preliminary explanations for this counter-intuitive phenomena. All\nthe codes and datasets are open-sourced.", "published": "2022-12-20 15:26:19", "link": "http://arxiv.org/abs/2212.10341v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive\n  Field Analysis", "abstract": "Length extrapolation permits training a transformer language model on short\nsequences that preserves perplexities when tested on substantially longer\nsequences. A relative positional embedding design, ALiBi, has had the widest\nusage to date. We dissect ALiBi via the lens of receptive field analysis\nempowered by a novel cumulative normalized gradient tool. The concept of\nreceptive field further allows us to modify the vanilla Sinusoidal positional\nembedding to create ~\\textbf{Sandwich}, the first parameter-free relative\npositional embedding design that truly length information uses longer than the\ntraining sequence. Sandwich shares with KERPLE and T5 the same logarithmic\ndecaying temporal bias pattern with learnable relative positional embeddings;\nthese elucidate future extrapolatable positional embedding design.", "published": "2022-12-20 15:40:17", "link": "http://arxiv.org/abs/2212.10356v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Curation Alone Can Stabilize In-context Learning", "abstract": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks by prompting them with a sequence of training examples. However, it is\nknown that ICL is very sensitive to the choice of training examples: randomly\nsampling examples from a training set leads to high variance in performance. In\nthis paper, we show that carefully curating a subset of training data greatly\nstabilizes ICL performance without any other changes to the ICL algorithm\n(e.g., prompt retrieval or calibration). We introduce two methods to choose\ntraining subsets -- both score training examples individually, then select the\nhighest-scoring ones. CondAcc scores a training example by its average dev-set\nICL accuracy when combined with random training examples, while Datamodels\nlearns linear regressors that estimate how the presence of each training\nexample influences LLM outputs. Across five tasks and two LLMs, sampling from\nstable subsets selected by CondAcc and Datamodels improves average accuracy\nover sampling from the entire training set by 7.7% and 6.3%, respectively.\nSurprisingly, the stable subset examples are not especially diverse in content\nor low in perplexity, in contrast with other work suggesting that diversity and\nperplexity are important when prompting LLMs.", "published": "2022-12-20 15:58:54", "link": "http://arxiv.org/abs/2212.10378v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To Adapt or to Annotate: Challenges and Interventions for Domain\n  Adaptation in Open-Domain Question Answering", "abstract": "Recent advances in open-domain question answering (ODQA) have demonstrated\nimpressive accuracy on standard Wikipedia style benchmarks. However, it is less\nclear how robust these models are and how well they perform when applied to\nreal-world applications in drastically different domains. While there has been\nsome work investigating how well ODQA models perform when tested for\nout-of-domain (OOD) generalization, these studies have been conducted only\nunder conservative shifts in data distribution and typically focus on a single\ncomponent (ie. retrieval) rather than an end-to-end system. In response, we\npropose a more realistic and challenging domain shift evaluation setting and,\nthrough extensive experiments, study end-to-end model performance. We find that\nnot only do models fail to generalize, but high retrieval scores often still\nyield poor answer prediction accuracy. We then categorize different types of\nshifts and propose techniques that, when presented with a new dataset, predict\nif intervention methods are likely to be successful. Finally, using insights\nfrom this analysis, we propose and evaluate several intervention methods which\nimprove end-to-end answer F1 score by up to 24 points.", "published": "2022-12-20 16:06:09", "link": "http://arxiv.org/abs/2212.10381v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empowering Sentence Encoders with Prompting and Label Retrieval for\n  Zero-shot Text Classification", "abstract": "With contrastive pre-training, sentence encoders are generally optimized to\nlocate semantically similar samples closer to each other in their embedding\nspaces. In this work, we focus on the potential of their embedding spaces to be\nreadily adapted to zero-shot text classification, as semantically distinct\nsamples are already well-separated. Our framework, RaLP (Retrieval augmented\nLabel Prompts for sentence encoder), encodes prompted label candidates with a\nsentence encoder, then assigns the label whose prompt embedding has the highest\nsimilarity with the input text embedding. In order to compensate for the\npotentially poorly descriptive labels in their original format, RaLP retrieves\nsentences that are semantically similar to the original label prompt from\nexternal corpora and use them as additional pseudo-label prompts. RaLP achieves\ncompetitive or stronger performance than much larger baselines on various\nclosed-set classification and multiple-choice QA datasets under zero-shot\nsettings. We show that the retrieval component plays a pivotal role in RaLP's\nsuccess, and its results are robustly attained regardless of verbalizer\nvariations.", "published": "2022-12-20 16:18:03", "link": "http://arxiv.org/abs/2212.10391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Debiasing Stance Detection Models with Counterfactual Reasoning and\n  Adversarial Bias Learning", "abstract": "Stance detection models may tend to rely on dataset bias in the text part as\na shortcut and thus fail to sufficiently learn the interaction between the\ntargets and texts. Recent debiasing methods usually treated features learned by\nsmall models or big models at earlier steps as bias features and proposed to\nexclude the branch learning those bias features during inference. However, most\nof these methods fail to disentangle the ``good'' stance features and ``bad''\nbias features in the text part. In this paper, we investigate how to mitigate\ndataset bias in stance detection. Motivated by causal effects, we leverage a\nnovel counterfactual inference framework, which enables us to capture the\ndataset bias in the text part as the direct causal effect of the text on\nstances and reduce the dataset bias in the text part by subtracting the direct\ntext effect from the total causal effect. We novelly model bias features as\nfeatures that correlate with the stance labels but fail on intermediate stance\nreasoning subtasks and propose an adversarial bias learning module to model the\nbias more accurately. To verify whether our model could better model the\ninteraction between texts and targets, we test our model on recently proposed\ntest sets to evaluate the understanding of the task from various aspects.\nExperiments demonstrate that our proposed method (1) could better model the\nbias features, and (2) outperforms existing debiasing baselines on both the\noriginal dataset and most of the newly constructed test sets.", "published": "2022-12-20 16:20:56", "link": "http://arxiv.org/abs/2212.10392v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for\n  Summarization", "abstract": "To prevent the costly and inefficient use of resources on low-quality\nannotations, we want a method for creating a pool of dependable annotators who\ncan effectively complete difficult tasks, such as evaluating automatic\nsummarization. Thus, we investigate the recruitment of high-quality Amazon\nMechanical Turk workers via a two-step pipeline. We show that we can\nsuccessfully filter out subpar workers before they carry out the evaluations\nand obtain high-agreement annotations with similar constraints on resources.\nAlthough our workers demonstrate a strong consensus among themselves and\nCloudResearch workers, their alignment with expert judgments on a subset of the\ndata is not as expected and needs further training in correctness. This paper\nstill serves as a best practice for the recruitment of qualified annotators in\nother challenging annotation tasks.", "published": "2022-12-20 16:25:42", "link": "http://arxiv.org/abs/2212.10397v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geographic and Geopolitical Biases of Language Models", "abstract": "Pretrained language models (PLMs) often fail to fairly represent target users\nfrom certain world regions because of the under-representation of those regions\nin training datasets. With recent PLMs trained on enormous data sources,\nquantifying their potential biases is difficult, due to their black-box nature\nand the sheer scale of the data sources. In this work, we devise an approach to\nstudy the geographic bias (and knowledge) present in PLMs, proposing a\nGeographic-Representation Probing Framework adopting a self-conditioning method\ncoupled with entity-country mappings. Our findings suggest PLMs'\nrepresentations map surprisingly well to the physical world in terms of\ncountry-to-country associations, but this knowledge is unequally shared across\nlanguages. Last, we explain how large PLMs despite exhibiting notions of\ngeographical proximity, over-amplify geopolitical favouritism at inference\ntime.", "published": "2022-12-20 16:32:54", "link": "http://arxiv.org/abs/2212.10408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClarifyDelphi: Reinforced Clarification Questions with Defeasibility\n  Rewards for Social and Moral Situations", "abstract": "Context is everything, even in commonsense moral reasoning. Changing contexts\ncan flip the moral judgment of an action; \"Lying to a friend\" is wrong in\ngeneral, but may be morally acceptable if it is intended to protect their life.\n  We present ClarifyDelphi, an interactive system that learns to ask\nclarification questions (e.g., why did you lie to your friend?) in order to\nelicit additional salient contexts of a social or moral situation. We posit\nthat questions whose potential answers lead to diverging moral judgments are\nthe most informative. Thus, we propose a reinforcement learning framework with\na defeasibility reward that aims to maximize the divergence between moral\njudgments of hypothetical answers to a question. Human evaluation demonstrates\nthat our system generates more relevant, informative and defeasible questions\ncompared to competitive baselines. Our work is ultimately inspired by studies\nin cognitive science that have investigated the flexibility in moral cognition\n(i.e., the diverse contexts in which moral rules can be bent), and we hope that\nresearch in this direction can assist both cognitive and computational\ninvestigations of moral judgments.", "published": "2022-12-20 16:33:09", "link": "http://arxiv.org/abs/2212.10409v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Perplexed by Quality: A Perplexity-based Method for Adult and Harmful\n  Content Detection in Multilingual Heterogeneous Web Data", "abstract": "As demand for large corpora increases with the size of current\nstate-of-the-art language models, using web data as the main part of the\npre-training corpus for these models has become a ubiquitous practice. This, in\nturn, has introduced an important challenge for NLP practitioners, as they are\nnow confronted with the task of developing highly optimized models and\npipelines for pre-processing large quantities of textual data, which implies,\neffectively classifying and filtering multilingual, heterogeneous and noisy\ndata, at web scale. One of the main components of this pre-processing step for\nthe pre-training corpora of large language models, is the removal of adult and\nharmful content. In this paper we explore different methods for detecting adult\nand harmful of content in multilingual heterogeneous web data. We first show\nhow traditional methods in harmful content detection, that seemingly perform\nquite well in small and specialized datasets quickly break down when confronted\nwith heterogeneous noisy web data. We then resort to using a perplexity based\napproach but with a twist: Instead of using a so-called \"clean\" corpus to train\na small language model and then use perplexity so select the documents with low\nperplexity, i.e., the documents that resemble this so-called \"clean\" corpus the\nmost. We train solely with adult and harmful textual data, and then select the\ndocuments having a perplexity value above a given threshold. This approach will\nvirtually cluster our documents into two distinct groups, which will greatly\nfacilitate the choice of the threshold for the perplexity and will also allow\nus to obtain higher precision than with the traditional classification methods\nfor detecting adult and harmful content.", "published": "2022-12-20 17:14:45", "link": "http://arxiv.org/abs/2212.10440v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Socratic Pretraining: Question-Driven Pretraining for Controllable\n  Summarization", "abstract": "In long document controllable summarization, where labeled data is scarce,\npretrained models struggle to adapt to the task and effectively respond to user\nqueries. In this paper, we introduce Socratic pretraining, a question-driven,\nunsupervised pretraining objective specifically designed to improve\ncontrollability in summarization tasks. By training a model to generate and\nanswer relevant questions in a given context, Socratic pretraining enables the\nmodel to more effectively adhere to user-provided queries and identify relevant\ncontent to be summarized. We demonstrate the effectiveness of this approach\nthrough extensive experimentation on two summarization domains, short stories\nand dialogue, and multiple control strategies: keywords, questions, and factoid\nQA pairs. Our pretraining method relies only on unlabeled documents and a\nquestion generation system and outperforms pre-finetuning approaches that use\nadditional supervised data. Furthermore, our results show that Socratic\npretraining cuts task-specific labeled data requirements in half, is more\nfaithful to user-provided queries, and achieves state-of-the-art performance on\nQMSum and SQuALITY.", "published": "2022-12-20 17:27:10", "link": "http://arxiv.org/abs/2212.10449v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is GPT-3 a Good Data Annotator?", "abstract": "Data annotation is the process of labeling data that could be used to train\nmachine learning models. Having high-quality annotation is crucial, as it\nallows the model to learn the relationship between the input data and the\ndesired output. GPT-3, a large-scale language model developed by OpenAI, has\ndemonstrated impressive zero- and few-shot performance on a wide range of NLP\ntasks. It is therefore natural to wonder whether it can be used to effectively\nannotate data for NLP tasks. In this paper, we evaluate the performance of\nGPT-3 as a data annotator by comparing it with traditional data annotation\nmethods and analyzing its output on a range of tasks. Through this analysis, we\naim to provide insight into the potential of GPT-3 as a general-purpose data\nannotator in NLP.", "published": "2022-12-20 17:28:41", "link": "http://arxiv.org/abs/2212.10450v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MULTI3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for\n  Natural Language Understanding in Task-Oriented Dialogue", "abstract": "Task-oriented dialogue (TOD) systems have been widely deployed in many\nindustries as they deliver more efficient customer support. These systems are\ntypically constructed for a single domain or language and do not generalise\nwell beyond this. To support work on Natural Language Understanding (NLU) in\nTOD across multiple languages and domains simultaneously, we constructed\nMULTI3NLU++, a multilingual, multi-intent, multi-domain dataset. MULTI3NLU++\nextends the English only NLU++ dataset to include manual translations into a\nrange of high, medium, and low resource languages (Spanish, Marathi, Turkish\nand Amharic), in two domains (BANKING and HOTELS). Because of its multi-intent\nproperty, MULTI3NLU++ represents complex and natural user goals, and therefore\nallows us to measure the realistic performance of TOD systems in a varied set\nof the world's languages. We use MULTI3NLU++ to benchmark state-of-the-art\nmultilingual models for the NLU tasks of intent detection and slot labelling\nfor TOD systems in the multilingual setting. The results demonstrate the\nchallenging nature of the dataset, particularly in the low-resource language\nsetting, offering ample room for future experimentation in multi-domain\nmultilingual TOD setups.", "published": "2022-12-20 17:34:25", "link": "http://arxiv.org/abs/2212.10455v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Go-tuning: Improving Zero-shot Learning Abilities of Smaller Language\n  Models", "abstract": "With increasing scale, large language models demonstrate both quantitative\nimprovement and new qualitative capabilities, especially as zero-shot learners,\nlike GPT-3. However, these results rely heavily on delicate prompt design and\nlarge computation. In this work, we explore whether the strong zero-shot\nability could be achieved at a smaller model scale without any external\nsupervised data. To achieve this goal, we revisit masked language modeling and\npresent a geometry-guided self-supervised learning method (Go-tuningfor short)\nby taking a small number of task-aware self-supervised data to update language\nmodels further. Experiments show that Go-tuning can enable T5-small (80M)\ncompetitive zero-shot results compared with large language models, such as\nT5-XL (3B). We also apply Go-tuning on multi-task settings and develop a\nmulti-task model, mgo-T5 (250M). It can reach the average performance of OPT\n(175B) on 9 datasets.", "published": "2022-12-20 17:36:49", "link": "http://arxiv.org/abs/2212.10461v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SODA: Million-scale Dialogue Distillation with Social Commonsense\n  Contextualization", "abstract": "Data scarcity has been a long standing issue in the field of open-domain\nsocial dialogue. To quench this thirst, we present SODA: the first publicly\navailable, million-scale high-quality social dialogue dataset. By\ncontextualizing social commonsense knowledge from a knowledge graph, we are\nable to distill an exceptionally broad spectrum of social interactions from a\nlarge language model. Human evaluation shows that conversations in SODA are\nmore consistent, specific, and (surprisingly) natural than those in prior\nhuman-authored datasets.\n  Using SODA, we train COSMO: a generalizable conversation model that is\nsignificantly more natural and consistent on unseen datasets than\nbest-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna).\nExperiments reveal COSMO is sometimes even preferred to the original\nhuman-written gold responses. Additionally, our results shed light on the\ndistinction between knowledge-enriched conversations and natural social\nchitchats. We plan to make our data, model, and code public.", "published": "2022-12-20 17:38:47", "link": "http://arxiv.org/abs/2212.10465v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controllable Text Generation with Language Constraints", "abstract": "We consider the task of text generation in language models with constraints\nspecified in natural language. To this end, we first create a challenging\nbenchmark Cognac that provides as input to the model a topic with example text,\nalong with a constraint on text to be avoided. Unlike prior work, our benchmark\ncontains knowledge-intensive constraints sourced from databases like Wordnet\nand Wikidata, which allows for straightforward evaluation while striking a\nbalance between broad attribute-level and narrow lexical-level controls. We\nfind that even state-of-the-art language models like GPT-3 fail often on this\ntask, and propose a solution to leverage a language model's own internal\nknowledge to guide generation. Our method, called CognacGen, first queries the\nlanguage model to generate guidance terms for a specified topic or constraint,\nand uses the guidance to modify the model's token generation probabilities. We\npropose three forms of guidance (binary verifier, top-k tokens, textual\nexample), and employ prefix-tuning approaches to distill the guidance to tackle\ndiverse natural language constraints. Through extensive empirical evaluations,\nwe demonstrate that CognacGen can successfully generalize to unseen\ninstructions and outperform competitive baselines in generating constraint\nconforming text.", "published": "2022-12-20 17:39:21", "link": "http://arxiv.org/abs/2212.10466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generic Temporal Reasoning with Differential Analysis and Explanation", "abstract": "Temporal reasoning is the task of predicting temporal relations of event\npairs. While temporal reasoning models can perform reasonably well on in-domain\nbenchmarks, we have little idea of these systems' generalizability due to\nexisting datasets' limitations. In this work, we introduce a novel task named\nTODAY that bridges this gap with temporal differential analysis, which as the\nname suggests, evaluates whether systems can correctly understand the effect of\nincremental changes. Specifically, TODAY introduces slight contextual changes\nfor given event pairs, and systems are asked to tell how this subtle contextual\nchange would affect relevant temporal relation distributions. To facilitate\nlearning, TODAY also annotates human explanations. We show that existing\nmodels, including GPT-3.5, drop to random guessing on TODAY, suggesting that\nthey heavily rely on spurious information rather than proper reasoning for\ntemporal predictions. On the other hand, we show that TODAY's supervision style\nand explanation annotations can be used in joint learning, encouraging models\nto use more appropriate signals during training and thus outperform across\nseveral benchmarks. TODAY can also be used to train models to solicit\nincidental supervision from noisy sources such as GPT-3.5, thus moving us more\ntoward the goal of generic temporal reasoning systems.", "published": "2022-12-20 17:40:03", "link": "http://arxiv.org/abs/2212.10467v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BMX: Boosting Natural Language Generation Metrics with Explainability", "abstract": "State-of-the-art natural language generation evaluation metrics are based on\nblack-box language models. Hence, recent works consider their explainability\nwith the goals of better understandability for humans and better metric\nanalysis, including failure cases. In contrast, our proposed method BMX:\nBoosting Natural Language Generation Metrics with explainability explicitly\nleverages explanations to boost the metrics' performance. In particular, we\nperceive feature importance explanations as word-level scores, which we\nconvert, via power means, into a segment-level score. We then combine this\nsegment-level score with the original metric to obtain a better metric. Our\ntests show improvements for multiple metrics across MT and summarization\ndatasets. While improvements in machine translation are small, they are strong\nfor summarization. Notably, BMX with the LIME explainer and preselected\nparameters achieves an average improvement of 0.087 points in Spearman\ncorrelation on the system-level evaluation of SummEval.", "published": "2022-12-20 17:41:18", "link": "http://arxiv.org/abs/2212.10469v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning\n  and Generation with Large Language Models", "abstract": "Previous work has demonstrated the effectiveness of planning for story\ngeneration exclusively in a monolingual setting focusing primarily on English.\nWe consider whether planning brings advantages to automatic story generation\nacross languages. We propose a new task of cross-lingual story generation with\nplanning and present a new dataset for this task. We conduct a comprehensive\nstudy of different plans and generate stories in several languages, by\nleveraging the creative and reasoning capabilities of large pre-trained\nlanguage models. Our results demonstrate that plans which structure stories\ninto three acts lead to more coherent and interesting narratives, while\nallowing to explicitly control their content and structure.", "published": "2022-12-20 17:42:16", "link": "http://arxiv.org/abs/2212.10471v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free\n  Language Models", "abstract": "State-of-the-art poetry generation systems are often complex. They either\nconsist of task-specific model pipelines, incorporate prior knowledge in the\nform of manually created constraints, or both. In contrast, end-to-end models\nwould not suffer from the overhead of having to model prior knowledge and could\nlearn the nuances of poetry from data alone, reducing the degree of human\nsupervision required. In this work, we investigate end-to-end poetry generation\nconditioned on styles such as rhyme, meter, and alliteration. We identify and\naddress lack of training data and mismatching tokenization algorithms as\npossible limitations of past attempts. In particular, we successfully pre-train\nByGPT5, a new token-free decoder-only language model, and fine-tune it on a\nlarge custom corpus of English and German quatrains annotated with our styles.\nWe show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and\nChatGPT, while also being more parameter efficient and performing favorably\ncompared to humans. In addition, we analyze its runtime performance and\ndemonstrate that it is not prone to memorization. We make our code, models, and\ndatasets publicly available.", "published": "2022-12-20 17:49:49", "link": "http://arxiv.org/abs/2212.10474v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimpleStyle: An Adaptable Style Transfer Approach", "abstract": "Attribute-controlled text rewriting, also known as text style-transfer, has a\ncrucial role in regulating attributes and biases of textual training data and a\nmachine generated text. In this work we present SimpleStyle, a minimalist yet\neffective approach for style-transfer composed of two simple ingredients:\ncontrolled denoising and output filtering. Despite the simplicity of our\napproach, which can be succinctly described with a few lines of code, it is\ncompetitive with previous state-of-the-art methods both in automatic and in\nhuman evaluation. To demonstrate the adaptability and practical value of our\nsystem beyond academic data, we apply SimpleStyle to transfer a wide range of\ntext attributes appearing in real-world textual data from social networks.\nAdditionally, we introduce a novel \"soft noising\" technique that further\nimproves the performance of our system. We also show that teaching a student\nmodel to generate the output of SimpleStyle can result in a system that\nperforms style transfer of equivalent quality with only a single greedy-decoded\nsample. Finally, we suggest our method as a remedy for the fundamental\nincompatible baseline issue that holds progress in the field. We offer our\nprotocol as a simple yet strong baseline for works that wish to make\nincremental advancements in the field of attribute controlled text rewriting.", "published": "2022-12-20 18:12:49", "link": "http://arxiv.org/abs/2212.10498v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Current Task-oriented Dialogue Models Automate Real-world Scenarios\n  in the Wild?", "abstract": "Task-oriented dialogue (TOD) systems are mainly based on the\nslot-filling-based TOD (SF-TOD) framework, in which dialogues are broken down\ninto smaller, controllable units (i.e., slots) to fulfill a specific task. A\nseries of approaches based on this framework achieved remarkable success on\nvarious TOD benchmarks. However, we argue that the current TOD benchmarks are\nlimited to surrogate real-world scenarios and that the current TOD models are\nstill a long way to cover the scenarios. In this position paper, we first\nidentify current status and limitations of SF-TOD systems. After that, we\nexplore the WebTOD framework, the alternative direction for building a scalable\nTOD system when a web/mobile interface is available. In WebTOD, the dialogue\nsystem learns how to understand the web/mobile interface that the human agent\ninteracts with, powered by a large-scale language model.", "published": "2022-12-20 18:18:41", "link": "http://arxiv.org/abs/2212.10504v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interleaving Retrieval with Chain-of-Thought Reasoning for\n  Knowledge-Intensive Multi-Step Questions", "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at\ngenerating natural language reasoning steps or Chains-of-Thoughts (CoT) for\nmulti-step question answering (QA). They struggle, however, when the necessary\nknowledge is either unavailable to the LLM or not up-to-date within its\nparameters. While using the question to retrieve relevant text from an external\nknowledge source helps LLMs, we observe that this one-step retrieve-and-read\napproach is insufficient for multi-step QA. Here, \\textit{what to retrieve}\ndepends on \\textit{what has already been derived}, which in turn may depend on\n\\textit{what was previously retrieved}. To address this, we propose IRCoT, a\nnew approach for multi-step QA that interleaves retrieval with steps\n(sentences) in a CoT, guiding the retrieval with CoT and in turn using\nretrieved results to improve CoT. Using IRCoT with GPT3 substantially improves\nretrieval (up to 21 points) as well as downstream QA (up to 15 points) on four\ndatasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar\nsubstantial gains in out-of-distribution (OOD) settings as well as with much\nsmaller models such as Flan-T5-large without additional training. IRCoT reduces\nmodel hallucination, resulting in factually more accurate CoT reasoning. Code,\ndata, and prompts are available at \\url{https://github.com/stonybrooknlp/ircot}", "published": "2022-12-20 18:26:34", "link": "http://arxiv.org/abs/2212.10509v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CausalDialogue: Modeling Utterance-level Causality in Conversations", "abstract": "Despite their widespread adoption, neural conversation models have yet to\nexhibit natural chat capabilities with humans. In this research, we examine\nuser utterances as causes and generated responses as effects, recognizing that\nchanges in a cause should produce a different effect. To further explore this\nconcept, we have compiled and expanded upon a new dataset called CausalDialogue\nthrough crowd-sourcing. This dataset includes multiple cause-effect pairs\nwithin a directed acyclic graph (DAG) structure. Our analysis reveals that\ntraditional loss functions struggle to effectively incorporate the DAG\nstructure, leading us to propose a causality-enhanced method called Exponential\nMaximum Average Treatment Effect (ExMATE) to enhance the impact of causality at\nthe utterance level in training neural conversation models. To evaluate the\nneeds of considering causality in dialogue generation, we built a comprehensive\nbenchmark on CausalDialogue dataset using different models, inference, and\ntraining methods. Through experiments, we find that a causality-inspired loss\nlike ExMATE can improve the diversity and agility of conventional loss function\nand there is still room for improvement to reach human-level quality on this\nnew dataset.", "published": "2022-12-20 18:31:50", "link": "http://arxiv.org/abs/2212.10515v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Privacy-Preserving Domain Adaptation of Semantic Parsers", "abstract": "Task-oriented dialogue systems often assist users with personal or\nconfidential matters. For this reason, the developers of such a system are\ngenerally prohibited from observing actual usage. So how can they know where\nthe system is failing and needs more training data or new functionality? In\nthis work, we study ways in which realistic user utterances can be generated\nsynthetically, to help increase the linguistic and functional coverage of the\nsystem, without compromising the privacy of actual users. To this end, we\npropose a two-stage Differentially Private (DP) generation method which first\ngenerates latent semantic parses, and then generates utterances based on the\nparses. Our proposed approach improves MAUVE by 2.5$\\times$ and parse tree\nfunction type overlap by 1.3$\\times$ relative to current approaches for private\nsynthetic data generation, improving both on fluency and semantic coverage. We\nfurther validate our approach on a realistic domain adaptation task of adding\nnew functionality from private user data to a semantic parser, and show overall\ngains of 8.5% points in accuracy with the new feature.", "published": "2022-12-20 18:35:21", "link": "http://arxiv.org/abs/2212.10520v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transformers Go for the LOLs: Generating (Humourous) Titles from\n  Scientific Abstracts End-to-End", "abstract": "We consider the end-to-end abstract-to-title generation problem, exploring\nseven recent transformer based models (including ChatGPT) fine-tuned on more\nthan 30k abstract-title pairs from NLP and machine learning (ML) venues. As an\nextension, we also consider the harder problem of generating humorous paper\ntitles. For the latter, we compile the first large-scale humor annotated\ndataset for scientific papers in the NLP/ML domains, comprising almost ~2.6k\ntitles. We evaluate all models using human and automatic metrics. Our human\nevaluation suggests that our best end-to-end system performs similarly to human\nauthors (but arguably slightly worse). Generating funny titles is more\ndifficult, however, and our automatic systems clearly underperform relative to\nhumans and often learn dataset artefacts of humor. Finally, ChatGPT, without\nany fine-tuning, performs on the level of our best fine-tuned system.", "published": "2022-12-20 18:37:11", "link": "http://arxiv.org/abs/2212.10522v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DISCO: Distilling Counterfactuals with Large Language Models", "abstract": "Models trained with counterfactually augmented data learn representations of\nthe causal structure of tasks, enabling robust generalization. However,\nhigh-quality counterfactual data is scarce for most tasks and not easily\ngenerated at scale. When crowdsourced, such data is typically limited in scale\nand diversity; when generated using supervised methods, it is computationally\nexpensive to extend to new counterfactual dimensions. In this work, we\nintroduce DISCO (DIStilled COunterfactual Data), a new method for automatically\ngenerating high quality counterfactual data at scale. DISCO engineers prompts\nto generate phrasal perturbations with a large general language model. Then, a\ntask-specific teacher model filters these generations to distill high-quality\ncounterfactual data. While task-agnostic, we apply our pipeline to the task of\nnatural language inference (NLI) and find that on challenging evaluations such\nas the NLI stress test, comparatively smaller student models trained with DISCO\ngenerated counterfactuals are more robust (6% absolute) and generalize better\nacross distributions (2%) compared to models trained without data augmentation.\nFurthermore, DISCO augmented models are 10% more consistent between\ncounterfactual pairs on three evaluation sets, demonstrating that DISCO\naugmentation enables models to more reliably learn causal representations. Our\nrepository is available at: https://github.com/eric11eca/disco", "published": "2022-12-20 18:46:08", "link": "http://arxiv.org/abs/2212.10534v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measure More, Question More: Experimental Studies on Transformer-based\n  Language Models and Complement Coercion", "abstract": "Transformer-based language models have shown strong performance on an array\nof natural language understanding tasks. However, the question of how these\nmodels react to implicit meaning has been largely unexplored. We investigate\nthis using the complement coercion phenomenon, which involves sentences like\n\"The student finished the book about sailing\" where the action \"reading\" is\nimplicit. We compare LMs' surprisal estimates at various critical sentence\nregions in sentences with and without implicit meaning. Effects associated with\nrecovering implicit meaning were found at a critical region other than where\nsentences minimally differ. We then use follow-up experiments to factor out\npotential confounds, revealing different perspectives that offer a richer and\nmore accurate picture.", "published": "2022-12-20 18:46:20", "link": "http://arxiv.org/abs/2212.10536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good\n  movie, and a good prompt too?", "abstract": "Large language models can perform new tasks in a zero-shot fashion, given\nnatural language prompts that specify the desired behavior. Such prompts are\ntypically hand engineered, but can also be learned with gradient-based methods\nfrom labeled data. However, it is underexplored what factors make the prompts\neffective, especially when the prompts are natural language. In this paper, we\ninvestigate common attributes shared by effective prompts. We first propose a\nhuman readable prompt tuning method (F LUENT P ROMPT) based on Langevin\ndynamics that incorporates a fluency constraint to find a diverse distribution\nof effective and fluent prompts. Our analysis reveals that effective prompts\nare topically related to the task domain and calibrate the prior probability of\nlabel words. Based on these findings, we also propose a method for generating\nprompts using only unlabeled data, outperforming strong baselines by an average\nof 7.0% accuracy across three tasks.", "published": "2022-12-20 18:47:13", "link": "http://arxiv.org/abs/2212.10539v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantically-informed Hierarchical Event Modeling", "abstract": "Prior work has shown that coupling sequential latent variable models with\nsemantic ontological knowledge can improve the representational capabilities of\nevent modeling approaches. In this work, we present a novel, doubly\nhierarchical, semi-supervised event modeling framework that provides structural\nhierarchy while also accounting for ontological hierarchy. Our approach\nconsists of multiple layers of structured latent variables, where each\nsuccessive layer compresses and abstracts the previous layers. We guide this\ncompression through the injection of structured ontological knowledge that is\ndefined at the type level of events: importantly, our model allows for partial\ninjection of semantic knowledge and it does not depend on observing instances\nat any particular level of the semantic ontology. Across two different datasets\nand four different evaluation metrics, we demonstrate that our approach is able\nto out-perform the previous state-of-the-art approaches by up to 8.5%,\ndemonstrating the benefits of structured and semantic hierarchical knowledge\nfor event modeling.", "published": "2022-12-20 18:51:23", "link": "http://arxiv.org/abs/2212.10547v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T-Projection: High Quality Annotation Projection for Sequence Labeling\n  Tasks", "abstract": "In the absence of readily available labeled data for a given sequence\nlabeling task and language, annotation projection has been proposed as one of\nthe possible strategies to automatically generate annotated data. Annotation\nprojection has often been formulated as the task of transporting, on parallel\ncorpora, the labels pertaining to a given span in the source language into its\ncorresponding span in the target language. In this paper we present\nT-Projection, a novel approach for annotation projection that leverages large\npretrained text-to-text language models and state-of-the-art machine\ntranslation technology. T-Projection decomposes the label projection task into\ntwo subtasks: (i) A candidate generation step, in which a set of projection\ncandidates using a multilingual T5 model is generated and, (ii) a candidate\nselection step, in which the generated candidates are ranked based on\ntranslation probabilities. We conducted experiments on intrinsic and extrinsic\ntasks in 5 Indo-European and 8 low-resource African languages. We demostrate\nthat T-projection outperforms previous annotation projection methods by a wide\nmargin. We believe that T-Projection can help to automatically alleviate the\nlack of high-quality training data for sequence labeling tasks. Code and data\nare publicly available.", "published": "2022-12-20 18:51:48", "link": "http://arxiv.org/abs/2212.10548v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Length-Extrapolatable Transformer", "abstract": "Position modeling plays a critical role in Transformers. In this paper, we\nfocus on length extrapolation, i.e., training on short texts while evaluating\nlonger sequences. We define attention resolution as an indicator of\nextrapolation. Then we propose two designs to improve the above metric of\nTransformers. Specifically, we introduce a relative position embedding to\nexplicitly maximize attention resolution. Moreover, we use blockwise causal\nattention during inference for better resolution. We evaluate different\nTransformer variants with language modeling. Experimental results show that our\nmodel achieves strong performance in both interpolation and extrapolation\nsettings. The code will be available at https://aka.ms/LeX-Transformer.", "published": "2022-12-20 18:56:20", "link": "http://arxiv.org/abs/2212.10554v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines", "abstract": "Dialogue models are able to generate coherent and fluent responses, but they\ncan still be challenging to control and may produce non-engaging, unsafe\nresults. This unpredictability diminishes user trust and can hinder the use of\nthe models in the real world. To address this, we introduce DialGuide, a novel\nframework for controlling dialogue model behavior using natural language rules,\nor guidelines. These guidelines provide information about the context they are\napplicable to and what should be included in the response, allowing the models\nto generate responses that are more closely aligned with the developer's\nexpectations and intent. We evaluate DialGuide on three tasks in open-domain\ndialogue response generation: guideline selection, response generation, and\nresponse entailment verification. Our dataset contains 10,737 positive and\n15,467 negative dialogue context-response-guideline triplets across two domains\n- chit-chat and safety. We provide baseline models for the tasks and benchmark\ntheir performance. We also demonstrate that DialGuide is effective in the\ndialogue safety domain, producing safe and engaging responses that follow\ndeveloper guidelines.", "published": "2022-12-20 18:57:18", "link": "http://arxiv.org/abs/2212.10557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform\n  Gradient Descent as Meta-Optimizers", "abstract": "Large pretrained language models have shown surprising in-context learning\n(ICL) ability. With a few demonstration input-label pairs, they can predict the\nlabel for an unseen input without parameter updates. Despite the great success\nin performance, its working mechanism still remains an open question. In this\npaper, we explain language models as meta-optimizers and understand in-context\nlearning as implicit finetuning. Theoretically, we figure out that Transformer\nattention has a dual form of gradient descent. On top of it, we understand ICL\nas follows: GPT first produces meta-gradients according to the demonstration\nexamples, and then these meta-gradients are applied to the original GPT to\nbuild an ICL model. We comprehensively compare the behaviors of in-context\nlearning and explicit finetuning on real tasks to provide empirical evidence\nthat supports our understanding. Experimental results show that in-context\nlearning behaves similarly to explicit finetuning from multiple perspectives.\nInspired by the dual form between Transformer attention and gradient descent,\nwe design a momentum-based attention by analogy with gradient descent with\nmomentum. The improved performance over vanilla attention further supports our\nunderstanding from another perspective, and more importantly, shows the\npotential to utilize our understanding for future model design. The code is\navailable at \\url{https://aka.ms/icl}.", "published": "2022-12-20 18:58:48", "link": "http://arxiv.org/abs/2212.10559v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BLIND: Bias Removal With No Demographics", "abstract": "Models trained on real-world data tend to imitate and amplify social biases.\nCommon methods to mitigate biases require prior information on the types of\nbiases that should be mitigated (e.g., gender or racial bias) and the social\ngroups associated with each data sample. In this work, we introduce BLIND, a\nmethod for bias removal with no prior knowledge of the demographics in the\ndataset. While training a model on a downstream task, BLIND detects biased\nsamples using an auxiliary model that predicts the main model's success, and\ndown-weights those samples during the training process. Experiments with racial\nand gender biases in sentiment classification and occupation classification\ntasks demonstrate that BLIND mitigates social biases without relying on a\ncostly demographic annotation process. Our method is competitive with other\nmethods that require demographic information and sometimes even surpasses them.", "published": "2022-12-20 18:59:42", "link": "http://arxiv.org/abs/2212.10563v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Ontologically Faithful Generation of Non-Player Character Dialogues", "abstract": "We introduce a language generation task grounded in a popular video game\nenvironment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration)\nrequires models to produce trees of dialogue between video game characters that\naccurately reflect quest and entity specifications stated in natural language.\nKNUDGE is constructed from side quest dialogues drawn directly from game data\nof Obsidian Entertainment's The Outer Worlds, leading to real-world\ncomplexities in generation: (1) dialogues are branching trees as opposed to\nlinear chains of utterances; (2) utterances must remain faithful to the game\nlore -- character personas, backstories, and entity relationships; and (3) a\ndialogue must accurately reveal new quest details to the human player. We\nreport results for a set of neural generation models using supervised and\nin-context learning techniques; we find competent performance but room for\nfuture work addressing the challenges of creating realistic, game-quality\ndialogues.", "published": "2022-12-20 19:48:10", "link": "http://arxiv.org/abs/2212.10618v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "mFACE: Multilingual Summarization with Factual Consistency Evaluation", "abstract": "Abstractive summarization has enjoyed renewed interest in recent years,\nthanks to pre-trained language models and the availability of large-scale\ndatasets. Despite promising results, current models still suffer from\ngenerating factually inconsistent summaries, reducing their utility for\nreal-world application. Several recent efforts attempt to address this by\ndevising models that automatically detect factual inconsistencies in machine\ngenerated summaries. However, they focus exclusively on English, a language\nwith abundant resources. In this work, we leverage factual consistency\nevaluation models to improve multilingual summarization. We explore two\nintuitive approaches to mitigate hallucinations based on the signal provided by\na multilingual NLI model, namely data filtering and controlled generation.\nExperimental results in the 45 languages from the XLSum dataset show gains over\nstrong baselines in both automatic and human evaluation.", "published": "2022-12-20 19:52:41", "link": "http://arxiv.org/abs/2212.10622v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KronA: Parameter Efficient Tuning with Kronecker Adapter", "abstract": "Fine-tuning a Pre-trained Language Model (PLM) on a specific downstream task\nhas been a well-known paradigm in Natural Language Processing. However, with\nthe ever-growing size of PLMs, training the entire model on several downstream\ntasks becomes very expensive and resource-hungry. Recently, different Parameter\nEfficient Tuning (PET) techniques are proposed to improve the efficiency of\nfine-tuning PLMs. One popular category of PET methods is the low-rank\nadaptation methods which insert learnable truncated SVD modules into the\noriginal model either sequentially or in parallel. However, low-rank\ndecomposition suffers from limited representation power. In this work, we\naddress this problem using the Kronecker product instead of the low-rank\nrepresentation. We introduce KronA, a Kronecker product-based adapter module\nfor efficient fine-tuning of Transformer-based PLMs. We apply the proposed\nmethods for fine-tuning T5 on the GLUE benchmark to show that incorporating the\nKronecker-based modules can outperform state-of-the-art PET methods.", "published": "2022-12-20 20:56:52", "link": "http://arxiv.org/abs/2212.10650v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Trustworthy Social Bias Measurement", "abstract": "How do we design measures of social bias that we trust? While prior work has\nintroduced several measures, no measure has gained widespread trust: instead,\nmounting evidence argues we should distrust these measures. In this work, we\ndesign bias measures that warrant trust based on the cross-disciplinary theory\nof measurement modeling. To combat the frequently fuzzy treatment of social\nbias in NLP, we explicitly define social bias, grounded in principles drawn\nfrom social science research. We operationalize our definition by proposing a\ngeneral bias measurement framework DivDist, which we use to instantiate 5\nconcrete bias measures. To validate our measures, we propose a rigorous testing\nprotocol with 8 testing criteria (e.g. predictive validity: do measures predict\nbiases in US employment?). Through our testing, we demonstrate considerable\nevidence to trust our measures, showing they overcome conceptual, technical,\nand empirical deficiencies present in prior measures.", "published": "2022-12-20 18:45:12", "link": "http://arxiv.org/abs/2212.11672v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Question Duplicate and Related Questions Detection in\n  e-learning platforms", "abstract": "Online learning platforms provide diverse questions to gauge the learners'\nunderstanding of different concepts. The repository of questions has to be\nconstantly updated to ensure a diverse pool of questions to conduct assessments\nfor learners. However, it is impossible for the academician to manually skim\nthrough the large repository of questions to check for duplicates when\nonboarding new questions from external sources. Hence, we propose a tool QDup\nin this paper that can surface near-duplicate and semantically related\nquestions without any supervised data. The proposed tool follows an\nunsupervised hybrid pipeline of statistical and neural approaches for\nincorporating different nuances in similarity for the task of question\nduplicate detection. We demonstrate that QDup can detect near-duplicate\nquestions and also suggest related questions for practice with remarkable\naccuracy and speed from a large repository of questions. The demo video of the\ntool can be found at https://www.youtube.com/watch?v=loh0_-7XLW4.", "published": "2022-12-20 11:52:52", "link": "http://arxiv.org/abs/2301.05150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving the Robustness of Summarization Models by Detecting and\n  Removing Input Noise", "abstract": "The evaluation of abstractive summarization models typically uses test data\nthat is identically distributed as training data. In real-world practice,\ndocuments to be summarized may contain input noise caused by text extraction\nartifacts or data pipeline bugs. The robustness of model performance under\ndistribution shift caused by such noise is relatively under-studied. We present\na large empirical study quantifying the sometimes severe loss in performance\n(up to 12 ROUGE-1 points) from different types of input noise for a range of\ndatasets and model sizes. We then propose a light-weight method for detecting\nand removing such noise in the input during model inference without requiring\nany extra training, auxiliary models, or even prior knowledge of the type of\nnoise. Our proposed approach effectively mitigates the loss in performance,\nrecovering a large fraction of the performance drop, sometimes as large as 11\nROUGE-1 points.", "published": "2022-12-20 00:33:11", "link": "http://arxiv.org/abs/2212.09928v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Defending Against Disinformation Attacks in Open-Domain Question\n  Answering", "abstract": "Recent work in open-domain question answering (ODQA) has shown that\nadversarial poisoning of the search collection can cause large drops in\naccuracy for production systems. However, little to no work has proposed\nmethods to defend against these attacks. To do so, we rely on the intuition\nthat redundant information often exists in large corpora. To find it, we\nintroduce a method that uses query augmentation to search for a diverse set of\npassages that could answer the original question but are less likely to have\nbeen poisoned. We integrate these new passages into the model through the\ndesign of a novel confidence method, comparing the predicted answer to its\nappearance in the retrieved contexts (what we call Confidence from Answer\nRedundancy, i.e. CAR). Together these methods allow for a simple but effective\nway to defend against poisoning attacks that provides gains of nearly 20% exact\nmatch across varying levels of data poisoning/knowledge conflicts.", "published": "2022-12-20 05:25:01", "link": "http://arxiv.org/abs/2212.10002v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file\n  Context", "abstract": "While pre-trained language models (LM) for code have achieved great success\nin code completion, they generate code conditioned only on the contents within\nthe file, i.e., in-file context, but ignore the rich semantics in other files\nwithin the same project, i.e., cross-file context, a critical source of\ninformation that is especially useful in modern modular software development.\nSuch overlooking constrains code language models' capacity in code completion,\nleading to unexpected behaviors such as generating hallucinated class member\nfunctions or function calls with unexpected arguments. In this work, we develop\na cross-file context finder tool, CCFINDER, that effectively locates and\nretrieves the most relevant cross-file context. We propose CoCoMIC, a framework\nthat incorporates cross-file context to learn the in-file and cross-file\ncontext jointly on top of pretrained code LMs. CoCoMIC successfully improves\nthe existing code LM with a 33.94% relative increase in exact match and a\n28.69% relative increase in identifier matching for code completion when the\ncross-file context is provided.", "published": "2022-12-20 05:48:09", "link": "http://arxiv.org/abs/2212.10007v2", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Enhancing Task Bot Engagement with Synthesized Open-Domain Dialog", "abstract": "Many efforts have been made to construct dialog systems for different types\nof conversations, such as task-oriented dialog (TOD) and open-domain dialog\n(ODD). To better mimic human-level conversations that usually fuse various\ndialog modes, it is essential to build a system that can effectively handle\nboth TOD and ODD and access different knowledge sources. To address the lack of\navailable data for the fused task, we propose a framework for automatically\ngenerating dialogues that combine knowledge-grounded ODDs and TODs in various\nsettings. Additionally, we introduce a unified model PivotBot that is capable\nof appropriately adopting TOD and ODD modes and accessing different knowledge\nsources in order to effectively tackle the fused task. Evaluation results\ndemonstrate the superior ability of the proposed model to switch seamlessly\nbetween TOD and ODD tasks.", "published": "2022-12-20 05:51:47", "link": "http://arxiv.org/abs/2212.10008v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DocAsRef: An Empirical Study on Repurposing Reference-Based Summary\n  Quality Metrics Reference-Freely", "abstract": "Automated summary quality assessment falls into two categories:\nreference-based and reference-free. Reference-based metrics, historically\ndeemed more accurate due to the additional information provided by\nhuman-written references, are limited by their reliance on human input. In this\npaper, we hypothesize that the comparison methodologies used by some\nreference-based metrics to evaluate a system summary against its corresponding\nreference can be effectively adapted to assess it against its source document,\nthereby transforming these metrics into reference-free ones. Experimental\nresults support this hypothesis. After being repurposed reference-freely, the\nzero-shot BERTScore using the pretrained DeBERTa-large-MNLI model of <0.5B\nparameters consistently outperforms its original reference-based version across\nvarious aspects on the SummEval and Newsroom datasets. It also excels in\ncomparison to most existing reference-free metrics and closely competes with\nzero-shot summary evaluators based on GPT-3.5.", "published": "2022-12-20 06:01:13", "link": "http://arxiv.org/abs/2212.10013v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "When Federated Learning Meets Pre-trained Language Models'\n  Parameter-Efficient Tuning Methods", "abstract": "With increasing privacy concerns on data, recent studies have made\nsignificant progress using federated learning (FL) on privacy-sensitive natural\nlanguage processing (NLP) tasks. Much literature suggests fully fine-tuning\npre-trained language models (PLMs) in the FL paradigm can mitigate the data\nheterogeneity problem and close the performance gap with centralized training.\nHowever, large PLMs bring the curse of prohibitive communication overhead and\nlocal model adaptation costs for the FL system. To this end, we introduce\nvarious parameter-efficient tuning (PETuning) methods into federated learning.\nSpecifically, we provide a holistic empirical study of representative PLMs\ntuning methods in FL. The experimental results cover the analysis of data\nheterogeneity levels, data scales, and different FL scenarios. Overall\ncommunication overhead can be significantly reduced by locally tuning and\nglobally aggregating lightweight model parameters while maintaining acceptable\nperformance in various FL settings. To facilitate the research of PETuning in\nFL, we also develop a federated tuning framework FedPETuning, which allows\npractitioners to exploit different PETuning methods under the FL training\nparadigm conveniently. The source code is available at\n\\url{https://github.com/iezhuozhuo/FedETuning/tree/deltaTuning}.", "published": "2022-12-20 06:44:32", "link": "http://arxiv.org/abs/2212.10025v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Do language models have coherent mental models of everyday things?", "abstract": "When people think of everyday things like an egg, they typically have a\nmental image associated with it. This allows them to correctly judge, for\nexample, that \"the yolk surrounds the shell\" is a false statement. Do language\nmodels similarly have a coherent picture of such everyday things? To\ninvestigate this, we propose a benchmark dataset consisting of 100 everyday\nthings, their parts, and the relationships between these parts, expressed as\n11,720 \"X relation Y?\" true/false questions. Using these questions as probes,\nwe observe that state-of-the-art pre-trained language models (LMs) like GPT-3\nand Macaw have fragments of knowledge about these everyday things, but do not\nhave fully coherent \"parts mental models\" (54-59% accurate, 19-43% conditional\nconstraint violation). We propose an extension where we add a constraint\nsatisfaction layer on top of the LM's raw predictions to apply commonsense\nconstraints. As well as removing inconsistencies, we find that this also\nsignificantly improves accuracy (by 16-20%), suggesting how the incoherence of\nthe LM's pictures of everyday things can be significantly reduced.", "published": "2022-12-20 06:54:04", "link": "http://arxiv.org/abs/2212.10029v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Framework of Customer Review Analysis Using the Aspect-Based Opinion\n  Mining Approach", "abstract": "Opinion mining is the branch of computation that deals with opinions,\nappraisals, attitudes, and emotions of people and their different aspects. This\nfield has attracted substantial research interest in recent years. Aspect-level\n(called aspect-based opinion mining) is often desired in practical applications\nas it provides detailed opinions or sentiments about different aspects of\nentities and entities themselves, which are usually required for action. Aspect\nextraction and entity extraction are thus two core tasks of aspect-based\nopinion mining. his paper has presented a framework of aspect-based opinion\nmining based on the concept of transfer learning. on real-world customer\nreviews available on the Amazon website. The model has yielded quite\nsatisfactory results in its task of aspect-based opinion mining.", "published": "2022-12-20 07:54:58", "link": "http://arxiv.org/abs/2212.10051v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "I Cast Detect Thoughts: Learning to Converse and Guide with Intents and\n  Theory-of-Mind in Dungeons and Dragons", "abstract": "We propose a novel task, G4C, to study teacher-student natural language\ninteractions in a goal-driven and grounded environment. Dungeons and Dragons\n(D&D), a role-playing game, provides an ideal setting to investigate such\ninteractions. Here, the Dungeon Master (DM), i.e., the teacher, guides the\nactions of several players -- students, each with their own personas and\nabilities -- to achieve shared goals grounded in a fantasy world. Our approach\nis to decompose and model these interactions into (1) the DM's intent to guide\nplayers toward a given goal; (2) the DM's guidance utterance to the players\nexpressing this intent; and (3) a theory-of-mind (ToM) model that anticipates\nthe players' reaction to the guidance one turn into the future. We develop a\nnovel reinforcement learning (RL) method for training a DM that generates\nguidance for players by rewarding utterances where the intent matches the\nToM-anticipated player actions. Human and automated evaluations show that a DM\ntrained to explicitly model intents and incorporate ToM of the players using RL\ngenerates better-quality guidance that is 3x more likely to fulfill the DM's\nintent than a vanilla natural language generation (NLG) approach.", "published": "2022-12-20 08:06:55", "link": "http://arxiv.org/abs/2212.10060v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DOC: Improving Long Story Coherence With Detailed Outline Control", "abstract": "We propose the Detailed Outline Control (DOC) framework for improving\nlong-range plot coherence when automatically generating\nseveral-thousand-word-long stories. DOC consists of two complementary\ncomponents: a detailed outliner and a detailed controller. The detailed\noutliner creates a more detailed, hierarchically structured outline, shifting\ncreative burden from the main drafting procedure to the planning stage. The\ndetailed controller ensures the more detailed outline is still respected during\ngeneration by controlling story passages to align with outline details. In\nhuman evaluations of automatically generated stories, DOC substantially\noutperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5%\nabsolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans\nalso judged DOC to be much more controllable in an interactive generation\nsetting.", "published": "2022-12-20 08:30:58", "link": "http://arxiv.org/abs/2212.10077v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Rumour detection using graph neural network and oversampling in\n  benchmark Twitter dataset", "abstract": "Recently, online social media has become a primary source for new information\nand misinformation or rumours. In the absence of an automatic rumour detection\nsystem the propagation of rumours has increased manifold leading to serious\nsocietal damages. In this work, we propose a novel method for building\nautomatic rumour detection system by focusing on oversampling to alleviating\nthe fundamental challenges of class imbalance in rumour detection task. Our\noversampling method relies on contextualised data augmentation to generate\nsynthetic samples for underrepresented classes in the dataset. The key idea\nexploits selection of tweets in a thread for augmentation which can be achieved\nby introducing a non-random selection criteria to focus the augmentation\nprocess on relevant tweets. Furthermore, we propose two graph neural\nnetworks(GNN) to model non-linear conversations on a thread. To enhance the\ntweet representations in our method we employed a custom feature selection\ntechnique based on state-of-the-art BERTweet model. Experiments of three\npublicly available datasets confirm that 1) our GNN models outperform the the\ncurrent state-of-the-art classifiers by more than 20%(F1-score); 2) our\noversampling technique increases the model performance by more than\n9%;(F1-score) 3) focusing on relevant tweets for data augmentation via\nnon-random selection criteria can further improve the results; and 4) our\nmethod has superior capabilities to detect rumours at very early stage.", "published": "2022-12-20 08:43:10", "link": "http://arxiv.org/abs/2212.10080v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tackling Ambiguity with Images: Improved Multimodal Machine Translation\n  and Contrastive Evaluation", "abstract": "One of the major challenges of machine translation (MT) is ambiguity, which\ncan in some cases be resolved by accompanying context such as images. However,\nrecent work in multimodal MT (MMT) has shown that obtaining improvements from\nimages is challenging, limited not only by the difficulty of building effective\ncross-modal representations, but also by the lack of specific evaluation and\ntraining data. We present a new MMT approach based on a strong text-only MT\nmodel, which uses neural adapters, a novel guided self-attention mechanism and\nwhich is jointly trained on both visually-conditioned masking and MMT. We also\nintroduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation\nset of ambiguous sentences and their possible translations, accompanied by\ndisambiguating images corresponding to each translation. Our approach obtains\ncompetitive results compared to strong text-only models on standard\nEnglish-to-French, English-to-German and English-to-Czech benchmarks and\noutperforms baselines and state-of-the-art MMT systems by a large margin on our\ncontrastive test set. Our code and CoMMuTE are freely available.", "published": "2022-12-20 10:18:18", "link": "http://arxiv.org/abs/2212.10140v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Do I have the Knowledge to Answer? Investigating Answerability of\n  Knowledge Base Questions", "abstract": "When answering natural language questions over knowledge bases, missing\nfacts, incomplete schema and limited scope naturally lead to many questions\nbeing unanswerable. While answerability has been explored in other QA settings,\nit has not been studied for QA over knowledge bases (KBQA). We create\nGrailQAbility, a new benchmark KBQA dataset with unanswerability, by first\nidentifying various forms of KB incompleteness that make questions\nunanswerable, and then systematically adapting GrailQA (a popular KBQA dataset\nwith only answerable questions). Experimenting with three state-of-the-art KBQA\nmodels, we find that all three models suffer a drop in performance even after\nsuitable adaptation for unanswerable questions. In addition, these often detect\nunanswerability for wrong reasons and find specific forms of unanswerability\nparticularly difficult to handle. This underscores the need for further\nresearch in making KBQA systems robust to unanswerability", "published": "2022-12-20 12:00:26", "link": "http://arxiv.org/abs/2212.10189v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Extrinsic Evaluation of Machine Translation Metrics", "abstract": "Automatic machine translation (MT) metrics are widely used to distinguish the\ntranslation qualities of machine translation systems across relatively large\ntest sets (system-level evaluation). However, it is unclear if automatic\nmetrics are reliable at distinguishing good translations from bad translations\nat the sentence level (segment-level evaluation). In this paper, we investigate\nhow useful MT metrics are at detecting the success of a machine translation\ncomponent when placed in a larger platform with a downstream task. We evaluate\nthe segment-level performance of the most widely used MT metrics (chrF, COMET,\nBERTScore, etc.) on three downstream cross-lingual tasks (dialogue state\ntracking, question answering, and semantic parsing). For each task, we only\nhave access to a monolingual task-specific model. We calculate the correlation\nbetween the metric's ability to predict a good/bad translation with the\nsuccess/failure on the final task for the Translate-Test setup. Our experiments\ndemonstrate that all metrics exhibit negligible correlation with the extrinsic\nevaluation of the downstream outcomes. We also find that the scores provided by\nneural metrics are not interpretable mostly because of undefined ranges. We\nsynthesise our analysis into recommendations for future MT metrics to produce\nlabels rather than scores for more informative interaction between machine\ntranslation and multilingual language understanding.", "published": "2022-12-20 14:39:58", "link": "http://arxiv.org/abs/2212.10297v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Does It Affect You? Social and Learning Implications of Using\n  Cognitive-Affective State Recognition for Proactive Human-Robot Tutoring", "abstract": "Using robots in educational contexts has already shown to be beneficial for a\nstudent's learning and social behaviour. For levitating them to the next level\nof providing more effective and human-like tutoring, the ability to adapt to\nthe user and to express proactivity is fundamental. By acting proactively,\nintelligent robotic tutors anticipate possible situations where problems for\nthe student may arise and act in advance for preventing negative outcomes.\nStill, the decisions of when and how to behave proactively are open questions.\nTherefore, this paper deals with the investigation of how the student's\ncognitive-affective states can be used by a robotic tutor for triggering\nproactive tutoring dialogue. In doing so, it is aimed to improve the learning\nexperience. For this reason, a concept learning task scenario was observed\nwhere a robotic assistant proactively helped when negative user states were\ndetected. In a learning task, the user's states of frustration and confusion\nwere deemed to have negative effects on the outcome of the task and were used\nto trigger proactive behaviour. In an empirical user study with 40\nundergraduate and doctoral students, we studied whether the initiation of\nproactive behaviour after the detection of signs of confusion and frustration\nimproves the student's concentration and trust in the agent. Additionally, we\ninvestigated which level of proactive dialogue is useful for promoting the\nstudent's concentration and trust. The results show that high proactive\nbehaviour harms trust, especially when triggered during negative\ncognitive-affective states but contributes to keeping the student focused on\nthe task when triggered in these states. Based on our study results, we further\ndiscuss future steps for improving the proactive assistance of robotic tutoring\nsystems.", "published": "2022-12-20 15:31:58", "link": "http://arxiv.org/abs/2212.10346v1", "categories": ["cs.RO", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Self-Adaptive In-Context Learning: An Information Compression\n  Perspective for In-Context Example Selection and Ordering", "abstract": "Despite the surprising few-shot performance of in-context learning (ICL), it\nis still a common practice to randomly sample examples to serve as context.\nThis paper advocates a new principle for ICL: self-adaptive in-context\nlearning. The self-adaption mechanism is introduced to help each sample find an\nin-context example permutation (i.e., selection and ordering) that can derive\nthe correct prediction, thus maximizing performance. To validate the\neffectiveness of self-adaptive ICL, we propose a general select-then-rank\nframework and instantiate it with new selection and ranking algorithms. Upon\nextensive evaluation on eight different NLP datasets, our self-adaptive ICL\nmethod achieves a 40% relative improvement over the common practice setting.\nFurther analysis reveals the enormous potential of self-adaptive ICL that it\nmight be able to close the gap between ICL and finetuning given more advanced\nalgorithms. Our code is released to facilitate future research in this area:\nhttps://github.com/Shark-NLP/self-adaptive-ICL", "published": "2022-12-20 15:55:21", "link": "http://arxiv.org/abs/2212.10375v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What Are You Token About? Dense Retrieval as Distributions Over the\n  Vocabulary", "abstract": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we\nhave little understanding of how they represent text, and why this leads to\ngood performance. In this work, we shed light on this question via\ndistributions over the vocabulary. We propose to interpret the vector\nrepresentations produced by dual encoders by projecting them into the model's\nvocabulary space. We show that the resulting projections contain rich semantic\ninformation, and draw connection between them and sparse retrieval. We find\nthat this view can offer an explanation for some of the failure cases of dense\nretrievers. For example, we observe that the inability of models to handle tail\nentities is correlated with a tendency of the token distributions to forget\nsome of the tokens of those entities. We leverage this insight and propose a\nsimple way to enrich query and passage representations with lexical information\nat inference time, and show that this significantly improves performance\ncompared to the original model in zero-shot settings, and specifically on the\nBEIR benchmark.", "published": "2022-12-20 16:03:25", "link": "http://arxiv.org/abs/2212.10380v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Contrastive Learning Reduces Hallucination in Conversations", "abstract": "Pre-trained language models (LMs) store knowledge in their parameters and can\ngenerate informative responses when used in conversational systems. However,\nLMs suffer from the problem of \"hallucination:\" they may generate\nplausible-looking statements that are irrelevant or factually incorrect. To\naddress this problem, we propose a contrastive learning scheme, named MixCL. A\nnovel mixed contrastive objective is proposed to explicitly optimize the\nimplicit knowledge elicitation process of LMs, and thus reduce their\nhallucination in conversations. We also examine negative sampling strategies of\nretrieved hard negatives and model-generated negatives. We conduct experiments\non Wizard-of-Wikipedia, a public, open-domain knowledge-grounded dialogue\nbenchmark, and assess the effectiveness of MixCL. MixCL effectively reduces the\nhallucination of LMs in conversations and achieves the highest performance\namong LM-based dialogue agents in terms of relevancy and factuality. We show\nthat MixCL achieves comparable performance to state-of-the-art KB-based\napproaches while enjoying notable advantages in terms of efficiency and\nscalability.", "published": "2022-12-20 16:26:18", "link": "http://arxiv.org/abs/2212.10400v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Reasoning in Large Language Models: A Survey", "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial\nrole in activities such as problem solving, decision making, and critical\nthinking. In recent years, large language models (LLMs) have made significant\nprogress in natural language processing, and there is observation that these\nmodels may exhibit reasoning abilities when they are sufficiently large.\nHowever, it is not yet clear to what extent LLMs are capable of reasoning. This\npaper provides a comprehensive overview of the current state of knowledge on\nreasoning in LLMs, including techniques for improving and eliciting reasoning\nin these models, methods and benchmarks for evaluating reasoning abilities,\nfindings and implications of previous research in this field, and suggestions\non future directions. Our aim is to provide a detailed and up-to-date review of\nthis topic and stimulate meaningful discussion and future work.", "published": "2022-12-20 16:29:03", "link": "http://arxiv.org/abs/2212.10403v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AnnoBERT: Effectively Representing Multiple Annotators' Label Choices to\n  Improve Hate Speech Detection", "abstract": "Supervised approaches generally rely on majority-based labels. However, it is\nhard to achieve high agreement among annotators in subjective tasks such as\nhate speech detection. Existing neural network models principally regard labels\nas categorical variables, while ignoring the semantic information in diverse\nlabel texts. In this paper, we propose AnnoBERT, a first-of-its-kind\narchitecture integrating annotator characteristics and label text with a\ntransformer-based model to detect hate speech, with unique representations\nbased on each annotator's characteristics via Collaborative Topic Regression\n(CTR) and integrate label text to enrich textual representations. During\ntraining, the model associates annotators with their label choices given a\npiece of text; during evaluation, when label information is not available, the\nmodel predicts the aggregated label given by the participating annotators by\nutilising the learnt association. The proposed approach displayed an advantage\nin detecting hate speech, especially in the minority class and edge cases with\nannotator disagreement. Improvement in the overall performance is the largest\nwhen the dataset is more label-imbalanced, suggesting its practical value in\nidentifying real-world hate speech, as the volume of hate speech in-the-wild is\nextremely small on social media, when compared with normal (non-hate) speech.\nThrough ablation studies, we show the relative contributions of annotator\nembeddings and label text to the model performance, and tested a range of\nalternative annotator embeddings and label text combinations.", "published": "2022-12-20 16:30:11", "link": "http://arxiv.org/abs/2212.10405v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Distillation for Long Document Retrieval", "abstract": "Long document retrieval aims to fetch query-relevant documents from a\nlarge-scale collection, where knowledge distillation has become de facto to\nimprove a retriever by mimicking a heterogeneous yet powerful cross-encoder.\nHowever, in contrast to passages or sentences, retrieval on long documents\nsuffers from the scope hypothesis that a long document may cover multiple\ntopics. This maximizes their structure heterogeneity and poses a\ngranular-mismatch issue, leading to an inferior distillation efficacy. In this\nwork, we propose a new learning framework, fine-grained distillation (FGD), for\nlong-document retrievers. While preserving the conventional dense retrieval\nparadigm, it first produces global-consistent representations crossing\ndifferent fine granularity and then applies multi-granular aligned distillation\nmerely during training. In experiments, we evaluate our framework on two\nlong-document retrieval benchmarks, which show state-of-the-art performance.", "published": "2022-12-20 17:00:36", "link": "http://arxiv.org/abs/2212.10423v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Parameter-efficient Zero-shot Transfer for Cross-Language Dense\n  Retrieval with Adapters", "abstract": "A popular approach to creating a zero-shot cross-language retrieval model is\nto substitute a monolingual pretrained language model in the retrieval model\nwith a multilingual pretrained language model such as Multilingual BERT. This\nmultilingual model is fined-tuned to the retrieval task with monolingual data\nsuch as English MS MARCO using the same training recipe as the monolingual\nretrieval model used. However, such transferred models suffer from mismatches\nin the languages of the input text during training and inference. In this work,\nwe propose transferring monolingual retrieval models using adapters, a\nparameter-efficient component for a transformer network. By adding adapters\npretrained on language tasks for a specific language with task-specific\nadapters, prior work has shown that the adapter-enhanced models perform better\nthan fine-tuning the entire model when transferring across languages in various\nNLP tasks. By constructing dense retrieval models with adapters, we show that\nmodels trained with monolingual data are more effective than fine-tuning the\nentire model when transferring to a Cross Language Information Retrieval (CLIR)\nsetting. However, we found that the prior suggestion of replacing the language\nadapters to match the target language at inference time is suboptimal for dense\nretrieval models. We provide an in-depth analysis of this discrepancy between\nother cross-language NLP tasks and CLIR.", "published": "2022-12-20 17:25:04", "link": "http://arxiv.org/abs/2212.10448v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "abstract": "While dense retrieval has been shown effective and efficient across tasks and\nlanguages, it remains difficult to create effective fully zero-shot dense\nretrieval systems when no relevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning and encoding relevance. Instead,\nwe propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a\nquery, HyDE first zero-shot instructs an instruction-following language model\n(e.g. InstructGPT) to generate a hypothetical document. The document captures\nrelevance patterns but is unreal and may contain false details. Then, an\nunsupervised contrastively learned encoder~(e.g. Contriever) encodes the\ndocument into an embedding vector. This vector identifies a neighborhood in the\ncorpus embedding space, where similar real documents are retrieved based on\nvector similarity. This second step ground the generated document to the actual\ncorpus, with the encoder's dense bottleneck filtering out the incorrect\ndetails. Our experiments show that HyDE significantly outperforms the\nstate-of-the-art unsupervised dense retriever Contriever and shows strong\nperformance comparable to fine-tuned retrievers, across various tasks (e.g. web\nsearch, QA, fact verification) and languages~(e.g. sw, ko, ja).", "published": "2022-12-20 18:09:52", "link": "http://arxiv.org/abs/2212.10496v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Measure-Theoretic Characterization of Tight Language Models", "abstract": "Language modeling, a central task in natural language processing, involves\nestimating a probability distribution over strings. In most cases, the\nestimated distribution sums to 1 over all finite strings. However, in some\npathological cases, probability mass can ``leak'' onto the set of infinite\nsequences. In order to characterize the notion of leakage more precisely, this\npaper offers a measure-theoretic treatment of language modeling. We prove that\nmany popular language model families are in fact tight, meaning that they will\nnot leak in this sense. We also generalize characterizations of tightness\nproposed in previous works.", "published": "2022-12-20 18:17:11", "link": "http://arxiv.org/abs/2212.10502v2", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Mini-Model Adaptation: Efficiently Extending Pretrained Models to New\n  Languages via Aligned Shallow Training", "abstract": "Prior work shows that it is possible to expand pretrained Masked Language\nModels (MLMs) to new languages by learning a new set of embeddings, while\nkeeping the transformer body frozen. Despite learning a small subset of\nparameters, this approach is not compute-efficient, as training the new\nembeddings requires a full forward and backward pass over the entire model. We\npropose mini-model adaptation, a compute-efficient alternative that builds a\nshallow mini-model from a fraction of a large model's parameters. New\nlanguage-specific embeddings can then be efficiently trained over the\nmini-model and plugged into the aligned large model for rapid cross-lingual\ntransfer. We explore two approaches to learn mini-models: MiniJoint, which\njointly pretrains the primary model and the mini-model using a single\ntransformer with a secondary MLM head at a middle layer; and MiniPost, where we\nstart from a regular pretrained model, build a mini-model by extracting and\nfreezing a few layers, and learn a small number of parameters on top.\nExperiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches\nthe performance of the standard approach using 2.3x less compute on average.", "published": "2022-12-20 18:17:28", "link": "http://arxiv.org/abs/2212.10503v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding\n  Tasks", "abstract": "Spoken language understanding (SLU) tasks have been studied for many decades\nin the speech research community, but have not received as much attention as\nlower-level tasks like speech and speaker recognition. In particular, there are\nnot nearly as many SLU task benchmarks, and many of the existing ones use data\nthat is not freely available to all researchers. Recent work has begun to\nintroduce such benchmark datasets for several tasks. In this work, we introduce\nseveral new annotated SLU benchmark tasks based on freely available speech\ndata, which complement existing benchmarks and address gaps in the SLU\nevaluation landscape. We contribute four tasks: question answering and\nsummarization involve inference over longer speech sequences; named entity\nlocalization addresses the speech-specific task of locating the targeted\ncontent in the signal; dialog act classification identifies the function of a\ngiven speech utterance. We follow the blueprint of the Spoken Language\nUnderstanding Evaluation (SLUE) benchmark suite. In order to facilitate the\ndevelopment of SLU models that leverage the success of pre-trained speech\nrepresentations, we will be publishing for each task (i) annotations for a\nrelatively small fine-tuning set, (ii) annotated development and test sets, and\n(iii) baseline models for easy reproducibility and comparisons. In this work,\nwe present the details of data collection and annotation and the performance of\nthe baseline models. We also perform sensitivity analysis of pipeline models'\nperformance (speech recognizer + text model) to the speech recognition\naccuracy, using more than 20 state-of-the-art speech recognition models.", "published": "2022-12-20 18:39:59", "link": "http://arxiv.org/abs/2212.10525v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Open Domain Multi-document Summarization: A Comprehensive Study of Model\n  Brittleness under Retrieval", "abstract": "Multi-document summarization (MDS) assumes a set of topic-related documents\nare provided as input. In practice, this document set is not always available;\nit would need to be retrieved given an information need, i.e. a question or\ntopic statement, a setting we dub \"open-domain\" MDS. We study this more\nchallenging setting by formalizing the task and bootstrapping it using existing\ndatasets, retrievers and summarizers. Via extensive automatic and human\nevaluation, we determine: (1) state-of-the-art summarizers suffer large\nreductions in performance when applied to open-domain MDS, (2) additional\ntraining in the open-domain setting can reduce this sensitivity to imperfect\nretrieval, and (3) summarizers are insensitive to the retrieval of duplicate\ndocuments and the order of retrieved documents, but highly sensitive to other\nerrors, like the retrieval of irrelevant documents. Based on our results, we\nprovide practical guidelines to enable future work on open-domain MDS, e.g. how\nto choose the number of retrieved documents to summarize. Our results suggest\nthat new retrieval and summarization methods and annotated resources for\ntraining and evaluation are necessary for further progress in the open-domain\nsetting.", "published": "2022-12-20 18:41:38", "link": "http://arxiv.org/abs/2212.10526v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "HYRR: Hybrid Infused Reranking for Passage Retrieval", "abstract": "We present Hybrid Infused Reranking for Passages Retrieval (HYRR), a\nframework for training rerankers based on a hybrid of BM25 and neural retrieval\nmodels. Retrievers based on hybrid models have been shown to outperform both\nBM25 and neural models alone. Our approach exploits this improved performance\nwhen training a reranker, leading to a robust reranking model. The reranker, a\ncross-attention neural model, is shown to be robust to different first-stage\nretrieval systems, achieving better performance than rerankers simply trained\nupon the first-stage retrievers in the multi-stage systems. We present\nevaluations on a supervised passage retrieval task using MS MARCO and zero-shot\nretrieval tasks using BEIR. The empirical results show strong performance on\nboth evaluations.", "published": "2022-12-20 18:44:21", "link": "http://arxiv.org/abs/2212.10528v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Detoxifying Text with MaRCo: Controllable Revision with Experts and\n  Anti-Experts", "abstract": "Text detoxification has the potential to mitigate the harms of toxicity by\nrephrasing text to remove offensive meaning, but subtle toxicity remains\nchallenging to tackle. We introduce MaRCo, a detoxification algorithm that\ncombines controllable generation and text rewriting methods using a Product of\nExperts with autoencoder language models (LMs). MaRCo uses likelihoods under a\nnon-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to\nmask and potentially replace. We evaluate our method on several subtle toxicity\nand microaggressions datasets, and show that it not only outperforms baselines\non automatic metrics, but MaRCo's rewrites are preferred 2.1 $\\times$ more in\nhuman evaluation. Its applicability to instances of subtle toxicity is\nespecially promising, demonstrating a path forward for addressing increasingly\nelusive online hate.", "published": "2022-12-20 18:50:00", "link": "http://arxiv.org/abs/2212.10543v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Pretraining Without Attention", "abstract": "Transformers have been essential to pretraining success in NLP. While other\narchitectures have been used, downstream accuracy is either significantly\nworse, or requires attention layers to match standard benchmarks such as GLUE.\nThis work explores pretraining without attention by using recent advances in\nsequence routing based on state-space models (SSMs). Our proposed model,\nBidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative\ngating architecture that has been effective in simplified sequence modeling\narchitectures. The model learns static layers that do not consider pair-wise\ninteractions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE\nand can be extended to long-form pretraining of 4096 tokens without\napproximation. Analysis shows that while the models have similar average\naccuracy, the approach has different inductive biases than BERT in terms of\ninteractions and syntactic representations. All models from this work are\navailable at https://github.com/jxiw/BiGS.", "published": "2022-12-20 18:50:08", "link": "http://arxiv.org/abs/2212.10544v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining\n  Concept Relationships", "abstract": "In this paper, we propose DimonGen, which aims to generate diverse sentences\ndescribing concept relationships in various everyday scenarios. To support\nthis, we first create a benchmark dataset for this task by adapting the\nexisting CommonGen dataset. We then propose a two-stage model called MoREE to\ngenerate the target sentences. MoREE consists of a mixture of retrievers model\nthat retrieves diverse context sentences related to the given concepts, and a\nmixture of generators model that generates diverse sentences based on the\nretrieved contexts. We conduct experiments on the DimonGen task and show that\nMoREE outperforms strong baselines in terms of both the quality and diversity\nof the generated sentences. Our results demonstrate that MoREE is able to\ngenerate diverse sentences that reflect different relationships between\nconcepts, leading to a comprehensive understanding of concept relationships.", "published": "2022-12-20 18:50:29", "link": "http://arxiv.org/abs/2212.10545v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lego-MT: Learning Detachable Models for Massively Multilingual Machine\n  Translation", "abstract": "Multilingual neural machine translation (MNMT) aims to build a unified model\nfor many language directions. Existing monolithic models for MNMT encounter two\nchallenges: parameter interference among languages and inefficient inference\nfor large models. In this paper, we revisit the classic multi-way structures\nand develop a detachable model by assigning each language (or group of\nlanguages) to an individual branch that supports plug-and-play training and\ninference. To address the needs of learning representations for all languages\nin a unified space, we propose a novel efficient training recipe, upon which we\nbuild an effective detachable model, Lego-MT. For a fair comparison, we collect\ndata from OPUS and build a translation benchmark covering 433 languages and\n1.3B parallel data. Experiments show that Lego-MT with 1.2B parameters brings\nan average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters.\nThe proposed training recipe brings a 28.2$\\times$ speedup over the\nconventional multi-way training method.\\footnote{\n\\url{https://github.com/CONE-MT/Lego-MT}.}", "published": "2022-12-20 18:54:08", "link": "http://arxiv.org/abs/2212.10551v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On-the-fly Denoising for Data Augmentation in Natural Language\n  Understanding", "abstract": "Data Augmentation (DA) is frequently used to provide additional training data\nwithout extra human annotation automatically. However, data augmentation may\nintroduce noisy data that impairs training. To guarantee the quality of\naugmented data, existing methods either assume no noise exists in the augmented\ndata and adopt consistency training or use simple heuristics such as training\nloss and diversity constraints to filter out \"noisy\" data. However, those\nfiltered examples may still contain useful information, and dropping them\ncompletely causes a loss of supervision signals. In this paper, based on the\nassumption that the original dataset is cleaner than the augmented data, we\npropose an on-the-fly denoising technique for data augmentation that learns\nfrom soft augmented labels provided by an organic teacher model trained on the\ncleaner original data. To further prevent overfitting on noisy labels, a simple\nself-regularization module is applied to force the model prediction to be\nconsistent across two distinct dropouts. Our method can be applied to general\naugmentation techniques and consistently improve the performance on both text\nclassification and question-answering tasks.", "published": "2022-12-20 18:58:33", "link": "http://arxiv.org/abs/2212.10558v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "abstract": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.", "published": "2022-12-20 18:59:19", "link": "http://arxiv.org/abs/2212.10560v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Character-Aware Models Improve Visual Text Rendering", "abstract": "Current image generation models struggle to reliably produce well-formed\nvisual text. In this paper, we investigate a key contributing factor: popular\ntext-to-image models lack character-level input features, making it much harder\nto predict a word's visual makeup as a series of glyphs. To quantify this\neffect, we conduct a series of experiments comparing character-aware vs.\ncharacter-blind text encoders. In the text-only domain, we find that\ncharacter-aware models provide large gains on a novel spelling task\n(WikiSpell). Applying our learnings to the visual domain, we train a suite of\nimage generation models, and show that character-aware variants outperform\ntheir character-blind counterparts across a range of novel text rendering tasks\n(our DrawText benchmark). Our models set a much higher state-of-the-art on\nvisual spelling, with 30+ point accuracy gains over competitors on rare words,\ndespite training on far fewer examples.", "published": "2022-12-20 18:59:23", "link": "http://arxiv.org/abs/2212.10562v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "In-context Learning Distillation: Transferring Few-shot Learning Ability\n  of Pre-trained Language Models", "abstract": "Given the success with in-context learning of large pre-trained language\nmodels, we introduce in-context learning distillation to transfer in-context\nfew-shot learning ability from large models to smaller models. We propose to\ncombine in-context learning objectives with language modeling objectives to\ndistill both the ability to read in-context examples and task knowledge to the\nsmaller models. We perform in-context learning distillation under two different\nfew-shot learning paradigms: Meta In-context Tuning (Meta-ICT) and Multitask\nIn-context Tuning (Multitask-ICT). Multitask-ICT performs better on multitask\nfew-shot learning but also requires more computation than Meta-ICT. Our method\nshows consistent improvements for both Meta-ICT and Multitask-ICT on two\nbenchmarks: LAMA and CrossFit. Our extensive experiments and analysis reveal\nthat in-context learning objectives and language modeling objectives are\ncomplementary under the Multitask-ICT paradigm. In-context learning objectives\nachieve the best performance when combined with language modeling objectives.", "published": "2022-12-20 22:11:35", "link": "http://arxiv.org/abs/2212.10670v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Causally Testing Gender Bias in LLMs: A Case Study on Occupational Bias", "abstract": "Generated texts from large language models (LLMs) have been shown to exhibit\na variety of harmful, human-like biases against various demographics. These\nfindings motivate research efforts aiming to understand and measure such\neffects. This paper introduces a causal formulation for bias measurement in\ngenerative language models. Based on this theoretical foundation, we outline a\nlist of desiderata for designing robust bias benchmarks. We then propose a\nbenchmark called OccuGender, with a bias-measuring procedure to investigate\noccupational gender bias. We test several state-of-the-art open-source LLMs on\nOccuGender, including Llama, Mistral, and their instruction-tuned versions. The\nresults show that these models exhibit substantial occupational gender bias.\nLastly, we discuss prompting strategies for bias mitigation and an extension of\nour causal formulation to illustrate the generalizability of our framework. Our\ncode and data https://github.com/chenyuen0103/gender-bias.", "published": "2022-12-20 22:41:24", "link": "http://arxiv.org/abs/2212.10678v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Task Ambiguity in Humans and Language Models", "abstract": "Language models have recently achieved strong performance across a wide range\nof NLP benchmarks. However, unlike benchmarks, real world tasks are often\npoorly specified, and agents must deduce the user's intended behavior from a\ncombination of context, instructions, and examples. We investigate how both\nhumans and models behave in the face of such task ambiguity by proposing\nAmbiBench, a new benchmark of six ambiguously-specified classification tasks.\nWe evaluate humans and models on AmbiBench by seeing how well they identify the\nintended task using 1) instructions with varying degrees of ambiguity, and 2)\ndifferent numbers of labeled examples. We find that the combination of model\nscaling (to 175B parameters) and training with human feedback data enables\nmodels to approach or exceed the accuracy of human participants across tasks,\nbut that either one alone is not sufficient. In addition, we show how to\ndramatically improve the accuracy of language models trained without\nlarge-scale human feedback training by finetuning on a small number of\nambiguous in-context examples, providing a promising direction for teaching\nmodels to generalize well in the face of ambiguity.", "published": "2022-12-20 18:35:33", "link": "http://arxiv.org/abs/2212.10711v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Smooth Sailing: Improving Active Learning for Pre-trained Language\n  Models with Representation Smoothness Analysis", "abstract": "Developed to alleviate prohibitive labeling costs, active learning (AL)\nmethods aim to reduce label complexity in supervised learning. While recent\nwork has demonstrated the benefit of using AL in combination with large\npre-trained language models (PLMs), it has often overlooked the practical\nchallenges that hinder the effectiveness of AL. We address these challenges by\nleveraging representation smoothness analysis to ensure AL is feasible, that\nis, both effective and practicable. Firstly, we propose an early stopping\ntechnique that does not require a validation set -- often unavailable in\nrealistic AL conditions -- and observe significant improvements over random\nsampling across multiple datasets and AL methods. Further, we find that task\nadaptation improves AL, whereas standard short fine-tuning in AL does not\nprovide improvements over random sampling. Our work demonstrates the usefulness\nof representation smoothness analysis for AL and introduces an AL stopping\ncriterion that reduces label complexity.", "published": "2022-12-20 19:37:20", "link": "http://arxiv.org/abs/2212.11680v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Future Sight: Dynamic Story Generation with Large Pretrained Language\n  Models", "abstract": "Recent advances in deep learning research, such as transformers, have\nbolstered the ability for automated agents to generate creative texts similar\nto those that a human would write. By default, transformer decoders can only\ngenerate new text with respect to previously generated text. The output\ndistribution of candidate tokens at any position is conditioned on previously\nselected tokens using a self-attention mechanism to emulate the property of\nautoregression. This is inherently limiting for tasks such as controllable\nstory generation where it may be necessary to condition on future plot events\nwhen writing a story. In this work, we propose Future Sight, a method for\nfinetuning a pretrained generative transformer on the task of future\nconditioning. Transformer decoders are typically pretrained on the task of\ncompleting a context, one token at a time, by means of self-attention. Future\nSight additionally enables a decoder to attend to an encoded future plot event.\nThis motivates the decoder to expand on the context in a way that logically\nconcludes with the provided future. During inference, the future plot event can\nbe written by a human author to steer the narrative being generated in a\ncertain direction. We evaluate the efficacy of our approach on a story\ngeneration task with human evaluators.", "published": "2022-12-20 01:53:26", "link": "http://arxiv.org/abs/2212.09947v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Joint Speech Transcription and Translation: Pseudo-Labeling with\n  Out-of-Distribution Data", "abstract": "Self-training has been shown to be helpful in addressing data scarcity for\nmany domains, including vision, speech, and language. Specifically,\nself-training, or pseudo-labeling, labels unsupervised data and adds that to\nthe training pool. In this work, we investigate and use pseudo-labeling for a\nrecently proposed novel setup: joint transcription and translation of speech,\nwhich suffers from an absence of sufficient data resources. We show that under\nsuch data-deficient circumstances, the unlabeled data can significantly vary in\ndomain from the supervised data, which results in pseudo-label quality\ndegradation. We investigate two categories of remedies that require no\nadditional supervision and target the domain mismatch: pseudo-label filtering\nand data augmentation. We show that pseudo-label analysis and processing as\nsuch results in additional gains on top of the vanilla pseudo-labeling setup\nresulting in total improvements of up to 0.6% absolute WER and 2.2 BLEU points.", "published": "2022-12-20 03:54:44", "link": "http://arxiv.org/abs/2212.09982v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Benchmarking Spatial Relationships in Text-to-Image Generation", "abstract": "Spatial understanding is a fundamental aspect of computer vision and integral\nfor human-level reasoning about images, making it an important component for\ngrounded language understanding. While recent text-to-image synthesis (T2I)\nmodels have shown unprecedented improvements in photorealism, it is unclear\nwhether they have reliable spatial understanding capabilities. We investigate\nthe ability of T2I models to generate correct spatial relationships among\nobjects and present VISOR, an evaluation metric that captures how accurately\nthe spatial relationship described in text is generated in the image. To\nbenchmark existing models, we introduce a dataset, $\\mathrm{SR}_{2D}$, that\ncontains sentences describing two or more objects and the spatial relationships\nbetween them. We construct an automated evaluation pipeline to recognize\nobjects and their spatial relationships, and employ it in a large-scale\nevaluation of T2I models. Our experiments reveal a surprising finding that,\nalthough state-of-the-art T2I models exhibit high image quality, they are\nseverely limited in their ability to generate multiple objects or the specified\nspatial relations between them. Our analyses demonstrate several biases and\nartifacts of T2I models such as the difficulty with generating multiple\nobjects, a bias towards generating the first object mentioned, spatially\ninconsistent outputs for equivalent relationships, and a correlation between\nobject co-occurrence and spatial understanding capabilities. We conduct a human\nstudy that shows the alignment between VISOR and human judgement about spatial\nunderstanding. We offer the $\\mathrm{SR}_{2D}$ dataset and the VISOR metric to\nthe community in support of T2I reasoning research.", "published": "2022-12-20 06:03:51", "link": "http://arxiv.org/abs/2212.10015v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Large Language Models Are Reasoning Teachers", "abstract": "Recent works have shown that chain-of-thought (CoT) prompting can elicit\nlanguage models to solve complex reasoning tasks, step-by-step. However,\nprompt-based CoT methods are dependent on very large models such as GPT-3 175B\nwhich are prohibitive to deploy at scale. In this paper, we use these large\nmodels as reasoning teachers to enable complex reasoning in smaller models and\nreduce model size requirements by several orders of magnitude. We propose\nFine-tune-CoT, a method that generates reasoning samples from very large\nteacher models to fine-tune smaller models. We evaluate our method on a wide\nrange of public models and complex tasks. We find that Fine-tune-CoT enables\nsubstantial reasoning capability in small models, far outperforming\nprompt-based baselines and even the teacher model in many tasks. Additionally,\nwe extend our method by leveraging the teacher model's ability to generate\nmultiple distinct rationales for each original sample. Enriching the\nfine-tuning data with such diverse reasoning results in a substantial\nperformance boost across datasets, even for very small models. We conduct\nablations and sample studies to understand the emergence of reasoning\ncapabilities of student models. Our code implementation and data are available\nat https://github.com/itsnamgyu/reasoning-teacher.", "published": "2022-12-20 08:24:45", "link": "http://arxiv.org/abs/2212.10071v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Pretrained Language Models for Neural Code Intelligence", "abstract": "As the complexity of modern software continues to escalate, software\nengineering has become an increasingly daunting and error-prone endeavor. In\nrecent years, the field of Neural Code Intelligence (NCI) has emerged as a\npromising solution, leveraging the power of deep learning techniques to tackle\nanalytical tasks on source code with the goal of improving programming\nefficiency and minimizing human errors within the software industry. Pretrained\nlanguage models have become a dominant force in NCI research, consistently\ndelivering state-of-the-art results across a wide range of tasks, including\ncode summarization, generation, and translation. In this paper, we present a\ncomprehensive survey of the NCI domain, including a thorough review of\npretraining techniques, tasks, datasets, and model architectures. We hope this\npaper will serve as a bridge between the natural language and programming\nlanguage communities, offering insights for future research in this rapidly\nevolving field.", "published": "2022-12-20 08:34:56", "link": "http://arxiv.org/abs/2212.10079v1", "categories": ["cs.SE", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Optimization Techniques for Unsupervised Complex Table Reasoning via\n  Self-Training Framework", "abstract": "Structured tabular data is a fundamental data type in numerous fields, and\nthe capacity to reason over tables is crucial for answering questions and\nvalidating hypotheses. However, constructing labeled data for complex reasoning\ntasks is labor intensive, and the quantity of annotated data remains\ninsufficient to support the intricate demands of real-world applications. To\naddress the insufficient annotation challenge, we present a self-training\nframework for unsupervised complex tabular reasoning (UCTR-ST) by generating\ndiverse synthetic data with complex logic. Specifically, UCTR-ST incorporates\nseveral essential techniques: we aggregate diverse programs and execute them on\ntables based on a \"Program-Management\" component, and we bridge the gap between\nprograms and text with a powerful \"Program-Transformation\" module that\ngenerates natural language sentences with complex logic. Furthermore, we\noptimize the procedure using a \"Table-Text Manipulator\" to handle joint\ntable-text reasoning scenarios. The entire framework utilizes self-training\ntechniques to leverage the unlabeled training data, which results in\nsignificant performance improvements when tested on real-world data.\nExperimental results demonstrate that UCTRST achieves above 90% of the\nsupervised model performance on different tasks and domains, reducing the\ndependence on manual annotation. Additionally, our approach can serve as a data\naugmentation technique, significantly boosting the performance of supervised\nmodels in low-resourced domains.", "published": "2022-12-20 09:15:03", "link": "http://arxiv.org/abs/2212.10097v2", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Human-Guided Fair Classification for Natural Language Processing", "abstract": "Text classifiers have promising applications in high-stake tasks such as\nresume screening and content moderation. These classifiers must be fair and\navoid discriminatory decisions by being invariant to perturbations of sensitive\nattributes such as gender or ethnicity. However, there is a gap between human\nintuition about these perturbations and the formal similarity specifications\ncapturing them. While existing research has started to address this gap,\ncurrent methods are based on hardcoded word replacements, resulting in\nspecifications with limited expressivity or ones that fail to fully align with\nhuman intuition (e.g., in cases of asymmetric counterfactuals). This work\nproposes novel methods for bridging this gap by discovering expressive and\nintuitive individual fairness specifications. We show how to leverage\nunsupervised style transfer and GPT-3's zero-shot capabilities to automatically\ngenerate expressive candidate pairs of semantically similar sentences that\ndiffer along sensitive attributes. We then validate the generated pairs via an\nextensive crowdsourcing study, which confirms that a lot of these pairs align\nwith human intuition about fairness in the context of toxicity classification.\nFinally, we show how limited amounts of human feedback can be leveraged to\nlearn a similarity specification that can be used to train downstream\nfairness-aware models.", "published": "2022-12-20 10:46:40", "link": "http://arxiv.org/abs/2212.10154v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Emotion Selectable End-to-End Text-based Speech Editing", "abstract": "Text-based speech editing allows users to edit speech by intuitively cutting,\ncopying, and pasting text to speed up the process of editing speech. In the\nprevious work, CampNet (context-aware mask prediction network) is proposed to\nrealize text-based speech editing, significantly improving the quality of\nedited speech. This paper aims at a new task: adding emotional effect to the\nediting speech during the text-based speech editing to make the generated\nspeech more expressive. To achieve this task, we propose Emo-CampNet (emotion\nCampNet), which can provide the option of emotional attributes for the\ngenerated speech in text-based speech editing and has the one-shot ability to\nedit unseen speakers' speech. Firstly, we propose an end-to-end\nemotion-selectable text-based speech editing model. The key idea of the model\nis to control the emotion of generated speech by introducing additional emotion\nattributes based on the context-aware mask prediction network. Secondly, to\nprevent the emotion of the generated speech from being interfered by the\nemotional components in the original speech, a neutral content generator is\nproposed to remove the emotion from the original speech, which is optimized by\nthe generative adversarial framework. Thirdly, two data augmentation methods\nare proposed to enrich the emotional and pronunciation information in the\ntraining set, which can enable the model to edit the unseen speaker's speech.\nThe experimental results that 1) Emo-CampNet can effectively control the\nemotion of the generated speech in the process of text-based speech editing;\nAnd can edit unseen speakers' speech. 2) Detailed ablation experiments further\nprove the effectiveness of emotional selectivity and data augmentation methods.\nThe demo page is available at https://hairuo55.github.io/Emo-CampNet/", "published": "2022-12-20 12:02:40", "link": "http://arxiv.org/abs/2212.10191v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing", "abstract": "Recently it has been shown that state-of-the-art NLP models are vulnerable to\nadversarial attacks, where the predictions of a model can be drastically\naltered by slight modifications to the input (such as synonym substitutions).\nWhile several defense techniques have been proposed, and adapted, to the\ndiscrete nature of text adversarial attacks, the benefits of general-purpose\nregularization methods such as label smoothing for language models, have not\nbeen studied. In this paper, we study the adversarial robustness provided by\nvarious label smoothing strategies in foundational models for diverse NLP tasks\nin both in-domain and out-of-domain settings. Our experiments show that label\nsmoothing significantly improves adversarial robustness in pre-trained models\nlike BERT, against various popular attacks. We also analyze the relationship\nbetween prediction confidence and robustness, showing that label smoothing\nreduces over-confident errors on adversarial examples.", "published": "2022-12-20 14:06:50", "link": "http://arxiv.org/abs/2212.10258v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ReCode: Robustness Evaluation of Code Generation Models", "abstract": "Code generation models have achieved impressive performance. However, they\ntend to be brittle as slight edits to a prompt could lead to very different\ngenerations; these robustness properties, critical for user experience when\ndeployed in real-life applications, are not well understood. Most existing\nworks on robustness in text or code tasks have focused on classification, while\nrobustness in generation tasks is an uncharted area and to date there is no\ncomprehensive benchmark for robustness in code generation. In this paper, we\npropose ReCode, a comprehensive robustness evaluation benchmark for code\ngeneration models. We customize over 30 transformations specifically for code\non docstrings, function and variable names, code syntax, and code format. They\nare carefully designed to be natural in real-life coding practice, preserve the\noriginal semantic meaning, and thus provide multifaceted assessments of a\nmodel's robustness performance. With human annotators, we verified that over\n90% of the perturbed prompts do not alter the semantic meaning of the original\nprompt. In addition, we define robustness metrics for code generation models\nconsidering the worst-case behavior under each type of perturbation, taking\nadvantage of the fact that executing the generated code can serve as objective\nevaluation. We demonstrate ReCode on SOTA models using HumanEval, MBPP, as well\nas function completion tasks derived from them. Interesting observations\ninclude: better robustness for CodeGen over InCoder and GPT-J; models are most\nsensitive to syntax perturbations; more challenging robustness evaluation on\nMBPP over HumanEval.", "published": "2022-12-20 14:11:31", "link": "http://arxiv.org/abs/2212.10264v1", "categories": ["cs.LG", "cs.CL", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Localising In-Domain Adaptation of Transformer-Based Biomedical Language\n  Models", "abstract": "In the era of digital healthcare, the huge volumes of textual information\ngenerated every day in hospitals constitute an essential but underused asset\nthat could be exploited with task-specific, fine-tuned biomedical language\nrepresentation models, improving patient care and management. For such\nspecialized domains, previous research has shown that fine-tuning models\nstemming from broad-coverage checkpoints can largely benefit additional\ntraining rounds over large-scale in-domain resources. However, these resources\nare often unreachable for less-resourced languages like Italian, preventing\nlocal medical institutions to employ in-domain adaptation. In order to reduce\nthis gap, our work investigates two accessible approaches to derive biomedical\nlanguage models in languages other than English, taking Italian as a concrete\nuse-case: one based on neural machine translation of English resources,\nfavoring quantity over quality; the other based on a high-grade, narrow-scoped\ncorpus natively written in Italian, thus preferring quality over quantity. Our\nstudy shows that data quantity is a harder constraint than data quality for\nbiomedical adaptation, but the concatenation of high-quality data can improve\nmodel performance even when dealing with relatively size-limited corpora. The\nmodels published from our investigations have the potential to unlock important\nresearch opportunities for Italian hospitals and academia. Finally, the set of\nlessons learned from the study constitutes valuable insights towards a solution\nto build biomedical language models that are generalizable to other\nless-resourced languages and different domain settings.", "published": "2022-12-20 16:59:56", "link": "http://arxiv.org/abs/2212.10422v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Execution-Based Evaluation for Open-Domain Code Generation", "abstract": "To extend the scope of coding queries to more realistic settings, we propose\nODEX, the first Open-Domain EXecution-based natural language (NL) to Python\ncode generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse\nlibraries, along with 1,707 human-written test cases for execution. Our NL-Code\npairs are harvested from StackOverflow forums to encourage natural and\npractical coding queries. Moreover, ODEX supports four natural languages as\nintents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing\nbehavioral differences among top-performing code language models (LM). While\nCODEX achieves better overall results, CODEGEN improves effectively via scaling\n-- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show\nsubstantial gaps between open and closed domains, but CODEGEN gaps tend to\ndecrease with model size while CODEX gaps increase. We release ODEX to\nfacilitate research into open-domain problems for the code generation\ncommunity.", "published": "2022-12-20 17:54:37", "link": "http://arxiv.org/abs/2212.10481v2", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "DePlot: One-shot visual language reasoning by plot-to-table translation", "abstract": "Visual language such as charts and plots is ubiquitous in the human world.\nComprehending plots and charts requires strong reasoning skills. Prior\nstate-of-the-art (SOTA) models require at least tens of thousands of training\nexamples and their reasoning capabilities are still much limited, especially on\ncomplex human-written queries. This paper presents the first one-shot solution\nto visual language reasoning. We decompose the challenge of visual language\nreasoning into two steps: (1) plot-to-text translation, and (2) reasoning over\nthe translated text. The key in this method is a modality conversion module,\nnamed as DePlot, which translates the image of a plot or chart to a linearized\ntable. The output of DePlot can then be directly used to prompt a pretrained\nlarge language model (LLM), exploiting the few-shot reasoning capabilities of\nLLMs. To obtain DePlot, we standardize the plot-to-table task by establishing\nunified task formats and metrics, and train DePlot end-to-end on this task.\nDePlot can then be used off-the-shelf together with LLMs in a plug-and-play\nfashion. Compared with a SOTA model finetuned on more than >28k data points,\nDePlot+LLM with just one-shot prompting achieves a 24.0% improvement over\nfinetuned SOTA on human-written queries from the task of chart QA.", "published": "2022-12-20 18:20:50", "link": "http://arxiv.org/abs/2212.10505v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "When Not to Trust Language Models: Investigating Effectiveness of\n  Parametric and Non-Parametric Memories", "abstract": "Despite their impressive performance on diverse tasks, large language models\n(LMs) still struggle with tasks requiring rich world knowledge, implying the\nlimitations of relying solely on their parameters to encode a wealth of world\nknowledge. This paper aims to understand LMs' strengths and limitations in\nmemorizing factual knowledge, by conducting large-scale knowledge probing\nexperiments of 10 models and 4 augmentation methods on PopQA, our new\nopen-domain QA dataset with 14k questions. We find that LMs struggle with less\npopular factual knowledge, and that scaling fails to appreciably improve\nmemorization of factual knowledge in the long tail. We then show that\nretrieval-augmented LMs largely outperform orders of magnitude larger LMs,\nwhile unassisted LMs remain competitive in questions about high-popularity\nentities. Based on those findings, we devise a simple, yet effective, method\nfor powerful and efficient retrieval-augmented LMs, which retrieves\nnon-parametric memories only when necessary. Experimental results show that\nthis significantly improves models' performance while reducing the inference\ncosts.", "published": "2022-12-20 18:30:15", "link": "http://arxiv.org/abs/2212.10511v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Psychological Safety of Large Language Models", "abstract": "In this work, we designed unbiased prompts to systematically evaluate the\npsychological safety of large language models (LLMs). First, we tested five\ndifferent LLMs by using two personality tests: Short Dark Triad (SD-3) and Big\nFive Inventory (BFI). All models scored higher than the human average on SD-3,\nsuggesting a relatively darker personality pattern. Despite being instruction\nfine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and\nGPT-4 still showed dark personality patterns; these models scored higher than\nself-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3.\nThen, we evaluated the LLMs in the GPT series by using well-being tests to\nstudy the impact of fine-tuning with more training data. We observed a\ncontinuous increase in the well-being scores of GPT models. Following these\nobservations, we showed that fine-tuning Llama-2-chat-7B with responses from\nBFI using direct preference optimization could effectively reduce the\npsychological toxicity of the model. Based on the findings, we recommended the\napplication of systematic and comprehensive psychological metrics to further\nevaluate and improve the safety of LLMs.", "published": "2022-12-20 18:45:07", "link": "http://arxiv.org/abs/2212.10529v3", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "A Survey of Deep Learning for Mathematical Reasoning", "abstract": "Mathematical reasoning is a fundamental aspect of human intelligence and is\napplicable in various fields, including science, engineering, finance, and\neveryday life. The development of artificial intelligence (AI) systems capable\nof solving math problems and proving theorems has garnered significant interest\nin the fields of machine learning and natural language processing. For example,\nmathematics serves as a testbed for aspects of reasoning that are challenging\nfor powerful deep learning models, driving new algorithmic and modeling\nadvances. On the other hand, recent advances in large-scale neural language\nmodels have opened up new benchmarks and opportunities to use deep learning for\nmathematical reasoning. In this survey paper, we review the key tasks,\ndatasets, and methods at the intersection of mathematical reasoning and deep\nlearning over the past decade. We also evaluate existing benchmarks and\nmethods, and discuss future research directions in this domain.", "published": "2022-12-20 18:46:16", "link": "http://arxiv.org/abs/2212.10535v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models", "abstract": "Large-scale neural network models combining text and images have made\nincredible progress in recent years. However, it remains an open question to\nwhat extent such models encode compositional representations of the concepts\nover which they operate, such as correctly identifying \"red cube\" by reasoning\nover the constituents \"red\" and \"cube\". In this work, we focus on the ability\nof a large pretrained vision and language model (CLIP) to encode compositional\nconcepts and to bind variables in a structure-sensitive way (e.g.,\ndifferentiating \"cube behind sphere\" from \"sphere behind cube\"). To inspect the\nperformance of CLIP, we compare several architectures from research on\ncompositional distributional semantics models (CDSMs), a line of research that\nattempts to implement traditional compositional linguistic structures within\nembedding spaces. We benchmark them on three synthetic datasets -\nsingle-object, two-object, and relational - designed to test concept binding.\nWe find that CLIP can compose concepts in a single-object setting, but in\nsituations where concept binding is needed, performance drops dramatically. At\nthe same time, CDSMs also perform poorly, with best performance at chance\nlevel.", "published": "2022-12-20 18:46:28", "link": "http://arxiv.org/abs/2212.10537v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Cross-modal Attention Congruence Regularization for Vision-Language\n  Relation Alignment", "abstract": "Despite recent progress towards scaling up multimodal vision-language models,\nthese models are still known to struggle on compositional generalization\nbenchmarks such as Winoground. We find that a critical component lacking from\ncurrent vision-language models is relation-level alignment: the ability to\nmatch directional semantic relations in text (e.g., \"mug in grass\") with\nspatial relationships in the image (e.g., the position of the mug relative to\nthe grass). To tackle this problem, we show that relation alignment can be\nenforced by encouraging the directed language attention from 'mug' to 'grass'\n(capturing the semantic relation 'in') to match the directed visual attention\nfrom the mug to the grass. Tokens and their corresponding objects are softly\nidentified using the cross-modal attention. We prove that this notion of soft\nrelation alignment is equivalent to enforcing congruence between vision and\nlanguage attention matrices under a 'change of basis' provided by the\ncross-modal attention matrix. Intuitively, our approach projects visual\nattention into the language attention space to calculate its divergence from\nthe actual language attention, and vice versa. We apply our Cross-modal\nAttention Congruence Regularization (CACR) loss to UNITER and improve on the\nstate-of-the-art approach to Winoground.", "published": "2022-12-20 18:53:14", "link": "http://arxiv.org/abs/2212.10549v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PairReranker: Pairwise Reranking for Natural Language Generation", "abstract": "Pre-trained language models have been successful in natural language\ngeneration (NLG) tasks. While various decoding methods have been employed, they\noften produce suboptimal results. We first present an empirical analysis of\nthree NLG tasks: summarization, machine translation, and constrained text\ngeneration. We found that selecting the best output from the results of\nmultiple decoding methods can significantly improve performance. To further\nimprove reranking for NLG tasks, we proposed a novel method,\n\\textsc{PairReranker}, which uses a single encoder and a pairwise loss function\nto jointly encode a source input and a pair of candidates and compare them.\nExperiments on three NLG tasks demonstrated the effectiveness and flexibility\nof \\textsc{PairReranker}, showing strong results, compared with previous\nbaselines. In addition, our \\textsc{PairReranker} can generalize to\nsignificantly improve GPT-3 (text-davinci-003) results (e.g., 24.55\\% on\nCommonGen and 11.35\\% on WMT18 zh-en), even though our rerankers are not\ntrained with any GPT-3 candidates.", "published": "2022-12-20 18:56:57", "link": "http://arxiv.org/abs/2212.10555v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parsel: Algorithmic Reasoning with Language Models by Composing\n  Decompositions", "abstract": "Despite recent success in large language model (LLM) reasoning, LLMs struggle\nwith hierarchical multi-step reasoning tasks like generating complex programs.\nFor these tasks, humans often start with a high-level algorithmic design and\nimplement each part gradually. We introduce Parsel, a framework enabling\nautomatic implementation and validation of complex algorithms with code LLMs.\nWith Parsel, we automatically decompose algorithmic tasks into hierarchical\nnatural language function descriptions and then search over combinations of\npossible function implementations using tests. We show that Parsel can be used\nacross domains requiring hierarchical reasoning, including program synthesis\nand robotic planning. We find that, using Parsel, LLMs solve more\ncompetition-level problems in the APPS dataset, resulting in pass rates over\n75\\% higher than prior results from directly sampling AlphaCode and Codex,\nwhile often using a smaller sample budget. Moreover, with automatically\ngenerated tests, we find that Parsel can improve the state-of-the-art pass@1\nperformance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated\nrobotic plans using Parsel are more than twice as likely to be considered\naccurate than directly generated plans. Lastly, we explore how Parsel addresses\nLLM limitations and discuss how Parsel may be useful for human programmers. We\nrelease our code at https://github.com/ezelikman/parsel", "published": "2022-12-20 18:59:23", "link": "http://arxiv.org/abs/2212.10561v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re-evaluating the Need for Multimodal Signals in Unsupervised Grammar\n  Induction", "abstract": "Are multimodal inputs necessary for grammar induction? Recent work has shown\nthat multimodal training inputs can improve grammar induction. However, these\nimprovements are based on comparisons to weak text-only baselines that were\ntrained on relatively little textual data. To determine whether multimodal\ninputs are needed in regimes with large amounts of textual training data, we\ndesign a stronger text-only baseline, which we refer to as LC-PCFG. LC-PCFG is\na C-PFCG that incorporates em-beddings from text-only large language models\n(LLMs). We use a fixed grammar family to directly compare LC-PCFG to various\nmulti-modal grammar induction methods. We compare performance on four benchmark\ndatasets. LC-PCFG provides an up to 17% relative improvement in Corpus-F1\ncompared to state-of-the-art multimodal grammar induction methods. LC-PCFG is\nalso more computationally efficient, providing an up to 85% reduction in\nparameter count and 8.8x reduction in training time compared to multimodal\napproaches. These results suggest that multimodal inputs may not be necessary\nfor grammar induction, and emphasize the importance of strong vision-free\nbaselines for evaluating the benefit of multimodal approaches.", "published": "2022-12-20 18:59:50", "link": "http://arxiv.org/abs/2212.10564v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "METEOR Guided Divergence for Video Captioning", "abstract": "Automatic video captioning aims for a holistic visual scene understanding. It\nrequires a mechanism for capturing temporal context in video frames and the\nability to comprehend the actions and associations of objects in a given\ntimeframe. Such a system should additionally learn to abstract video sequences\ninto sensible representations as well as to generate natural written language.\nWhile the majority of captioning models focus solely on the visual inputs,\nlittle attention has been paid to the audiovisual modality. To tackle this\nissue, we propose a novel two-fold approach. First, we implement a\nreward-guided KL Divergence to train a video captioning model which is\nresilient towards token permutations. Second, we utilise a Bi-Modal\nHierarchical Reinforcement Learning (BMHRL) Transformer architecture to capture\nlong-term temporal dependencies of the input data as a foundation for our\nhierarchical captioning module. Using our BMHRL, we show the suitability of the\nHRL agent in the generation of content-complete and grammatically sound\nsentences by achieving $4.91$, $2.23$, and $10.80$ in BLEU3, BLEU4, and METEOR\nscores, respectively on the ActivityNet Captions dataset. Finally, we make our\nBMHRL framework and trained models publicly available for users and developers\nat https://github.com/d-rothen/bmhrl.", "published": "2022-12-20 23:30:47", "link": "http://arxiv.org/abs/2212.10690v1", "categories": ["cs.CV", "cs.CL", "cs.LG", "I.2.10"], "primary_category": "cs.CV"}
{"title": "Generation-Augmented Query Expansion For Code Retrieval", "abstract": "Pre-trained language models have achieved promising success in code retrieval\ntasks, where a natural language documentation query is given to find the most\nrelevant existing code snippet. However, existing models focus only on\noptimizing the documentation code pairs by embedding them into latent space,\nwithout the association of external knowledge. In this paper, we propose a\ngeneration-augmented query expansion framework. Inspired by the human retrieval\nprocess - sketching an answer before searching, in this work, we utilize the\npowerful code generation model to benefit the code retrieval task.\nSpecifically, we demonstrate that rather than merely retrieving the target code\nsnippet according to the documentation query, it would be helpful to augment\nthe documentation query with its generation counterpart - generated code\nsnippets from the code generation model. To the best of our knowledge, this is\nthe first attempt that leverages the code generation model to enhance the code\nretrieval task. We achieve new state-of-the-art results on the CodeSearchNet\nbenchmark and surpass the baselines significantly.", "published": "2022-12-20 23:49:37", "link": "http://arxiv.org/abs/2212.10692v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "A survey on text generation using generative adversarial networks", "abstract": "This work presents a thorough review concerning recent studies and text\ngeneration advancements using Generative Adversarial Networks. The usage of\nadversarial learning for text generation is promising as it provides\nalternatives to generate the so-called \"natural\" language. Nevertheless,\nadversarial text generation is not a simple task as its foremost architecture,\nthe Generative Adversarial Networks, were designed to cope with continuous\ninformation (image) instead of discrete data (text). Thus, most works are based\non three possible options, i.e., Gumbel-Softmax differentiation, Reinforcement\nLearning, and modified training objectives. All alternatives are reviewed in\nthis survey as they present the most recent approaches for generating text\nusing adversarial-based techniques. The selected works were taken from renowned\ndatabases, such as Science Direct, IEEEXplore, Springer, Association for\nComputing Machinery, and arXiv, whereas each selected work has been critically\nanalyzed and assessed to present its objective, methodology, and experimental\nresults.", "published": "2022-12-20 17:54:08", "link": "http://arxiv.org/abs/2212.11119v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation for Change", "abstract": "Evaluation is the central means for assessing, understanding, and\ncommunicating about NLP models. In this position paper, we argue evaluation\nshould be more than that: it is a force for driving change, carrying a\nsociological and political character beyond its technical dimensions. As a\nforce, evaluation's power arises from its adoption: under our view, evaluation\nsucceeds when it achieves the desired change in the field. Further, by framing\nevaluation as a force, we consider how it competes with other forces. Under our\nanalysis, we conjecture that the current trajectory of NLP suggests\nevaluation's power is waning, in spite of its potential for realizing more\npluralistic ambitions in the field. We conclude by discussing the legitimacy of\nthis power, who acquires this power and how it distributes. Ultimately, we hope\nthe research community will more aggressively harness evaluation for change.", "published": "2022-12-20 17:49:27", "link": "http://arxiv.org/abs/2212.11670v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving the quality of neural TTS using long-form content and\n  multi-speaker multi-style modeling", "abstract": "Neural text-to-speech (TTS) can provide quality close to natural speech if an\nadequate amount of high-quality speech material is available for training.\nHowever, acquiring speech data for TTS training is costly and time-consuming,\nespecially if the goal is to generate different speaking styles. In this work,\nwe show that we can transfer speaking style across speakers and improve the\nquality of synthetic speech by training a multi-speaker multi-style (MSMS)\nmodel with long-form recordings, in addition to regular TTS recordings. In\nparticular, we show that 1) multi-speaker modeling improves the overall TTS\nquality, 2) the proposed MSMS approach outperforms pre-training and fine-tuning\napproach when utilizing additional multi-speaker data, and 3) long-form\nspeaking style is highly rated regardless of the target text domain.", "published": "2022-12-20 08:28:34", "link": "http://arxiv.org/abs/2212.10075v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Simple Feature Method for Prosody Rhythm Comparison", "abstract": "Of all components of Prosody, Rhythm has been regarded as the hardest to\naddress, as it is utterly linked to Pitch and Intensity. Nevertheless, Rhythm\nis a very good indicator of a speaker's fluency in a foreign language or even\nof some diseases. Canonical ways to measure Rhythm, such as $\\Delta C$ or\n$\\%V$, involve a cumbersome process of segment alignment, often leading to\nmodest and questionable results. Perceptively, however, rhythm does not sound\nas difficult, as humans can grasp it even when the text is not fully\nintelligible. In this work, we develop an empirical and unsupervised method of\nrhythm assessment, which does not rely on the content. We have created a\nfixed-length representation of each utterance, Peak Embedding (PE), which\ncodifies the proportional distance between peaks of the chosen Low-Level\nDescriptors. Clustering pairs of small sentence-like units, we have attained\naverages of 0.444 for Silhouette Coefficient using PE with Loudness, and 0.979\nfor Global Separability Index with a combination of PE with Pitch and Loudness.\nClustering same-structure words, we have attained averages of 0.196 for\nSilhouette Coefficient and 0.864 for Global Separability Index for PE with\nLoudness.", "published": "2022-12-20 12:26:44", "link": "http://arxiv.org/abs/2212.10201v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "TTS-Guided Training for Accent Conversion Without Parallel Data", "abstract": "Accent Conversion (AC) seeks to change the accent of speech from one (source)\nto another (target) while preserving the speech content and speaker identity.\nHowever, many AC approaches rely on source-target parallel speech data. We\npropose a novel accent conversion framework without the need of parallel data.\nSpecifically, a text-to-speech (TTS) system is first pretrained with\ntarget-accented speech data. This TTS model and its hidden representations are\nexpected to be associated only with the target accent. Then, a speech encoder\nis trained to convert the accent of the speech under the supervision of the\npretrained TTS model. In doing so, the source-accented speech and its\ncorresponding transcription are forwarded to the speech encoder and the\npretrained TTS, respectively. The output of the speech encoder is optimized to\nbe the same as the text embedding in the TTS system. At run-time, the speech\nencoder is combined with the pretrained TTS decoder to convert the\nsource-accented speech toward the target. In the experiments, we converted\nEnglish with two source accents (Chinese and Indian) to the target accent\n(American/British/Canadian). Both objective metrics and subjective listening\ntests successfully validate that, without any parallel data, the proposed\napproach generates speech samples that are close to the target accent with high\nspeech quality.", "published": "2022-12-20 12:33:45", "link": "http://arxiv.org/abs/2212.10204v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploring Effective Fusion Algorithms for Speech Based Self-Supervised\n  Learning Models", "abstract": "Self-supervised learning (SSL) has achieved great success in various areas\nincluding speech processing. Recently, it is proven that speech based SSL\nmodels are able to extract superior universal representations on a range of\ndownstream tasks compared to traditional hand-craft feature (e.g. FBank, MFCC)\nin the SUPERB benchmark. However, different types of SSL models might exhibit\ndistinct strengths on different downstream tasks. In order to better utilize\nthe potential power of SSL models, in this work, we explore the effective\nfusion on multiple SSL models. A series of model fusion algorithms are\ninvestigated and compared by combining two types of SSL models, Hubert and\nData2vec, on two representative tasks from SUPERB benchmark, which are speaker\nidentification (SID) and automatic speech recognition (ASR) tasks. The\nexperimental results demonstrate that our proposed fusion algorithms can\nfurther boost the individual model significantly.", "published": "2022-12-20 09:09:02", "link": "http://arxiv.org/abs/2212.10092v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hopf Physical Reservoir Computer for Reconfigurable Sound Recognition", "abstract": "The Hopf oscillator is a nonlinear oscillator that exhibits limit cycle\nmotion. This reservoir computer utilizes the vibratory nature of the\noscillator, which makes it an ideal candidate for reconfigurable sound\nrecognition tasks. In this paper, the capabilities of the Hopf reservoir\ncomputer performing sound recognition are systematically demonstrated. This\nwork shows that the Hopf reservoir computer can offer superior sound\nrecognition accuracy compared to legacy approaches (e.g., a Mel spectrum +\nmachine learning approach). More importantly, the Hopf reservoir computer\noperating as a sound recognition system does not require audio preprocessing\nand has a very simple setup while still offering a high degree of\nreconfigurability. These features pave the way of applying physical reservoir\ncomputing for sound recognition in low power edge devices.", "published": "2022-12-20 15:51:20", "link": "http://arxiv.org/abs/2212.10370v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual Transformers for Primates Classification and Covid Detection", "abstract": "We apply the vision transformer, a deep machine learning model build around\nthe attention mechanism, on mel-spectrogram representations of raw audio\nrecordings. When adding mel-based data augmentation techniques and\nsample-weighting, we achieve comparable performance on both (PRS and CCS\nchallenge) tasks of ComParE21, outperforming most single model baselines. We\nfurther introduce overlapping vertical patching and evaluate the influence of\nparameter configurations. Index Terms: audio classification, attention,\nmel-spectrogram, unbalanced data-sets, computational paralinguistics", "published": "2022-12-20 09:10:25", "link": "http://arxiv.org/abs/2212.10093v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Addressing the Selection Bias in Voice Assistance: Training Voice\n  Assistance Model in Python with Equal Data Selection", "abstract": "In recent times, voice assistants have become a part of our day-to-day lives,\nallowing information retrieval by voice synthesis, voice recognition, and\nnatural language processing. These voice assistants can be found in many\nmodern-day devices such as Apple, Amazon, Google, and Samsung. This project is\nprimarily focused on Virtual Assistance in Natural Language Processing. Natural\nLanguage Processing is a form of AI that helps machines understand people and\ncreate feedback loops. This project will use deep learning to create a Voice\nRecognizer and use Commonvoice and data collected from the local community for\nmodel training using Google Colaboratory. After recognizing a command, the AI\nassistant will be able to perform the most suitable actions and then give a\nresponse.\n  The motivation for this project comes from the race and gender bias that\nexists in many virtual assistants. The computer industry is primarily dominated\nby the male gender, and because of this, many of the products produced do not\nregard women. This bias has an impact on natural language processing. This\nproject will be utilizing various open-source projects to implement machine\nlearning algorithms and train the assistant algorithm to recognize different\ntypes of voices, accents, and dialects. Through this project, the goal to use\nvoice data from underrepresented groups to build a voice assistant that can\nrecognize voices regardless of gender, race, or accent. Increasing the\nrepresentation of women in the computer industry is important for the future of\nthe industry. By representing women in the initial study of voice assistants,\nit can be shown that females play a vital role in the development of this\ntechnology. In line with related work, this project will use first-hand data\nfrom the college population and middle-aged adults to train voice assistant to\ncombat gender bias.", "published": "2022-12-20 21:26:05", "link": "http://arxiv.org/abs/2301.00646v1", "categories": ["eess.AS", "cs.MA", "cs.RO", "cs.SD"], "primary_category": "eess.AS"}
{"title": "VSVC: Backdoor attack against Keyword Spotting based on Voiceprint\n  Selection and Voice Conversion", "abstract": "Keyword spotting (KWS) based on deep neural networks (DNNs) has achieved\nmassive success in voice control scenarios. However, training of such DNN-based\nKWS systems often requires significant data and hardware resources.\nManufacturers often entrust this process to a third-party platform. This makes\nthe training process uncontrollable, where attackers can implant backdoors in\nthe model by manipulating third-party training data. An effective backdoor\nattack can force the model to make specified judgments under certain\nconditions, i.e., triggers. In this paper, we design a backdoor attack scheme\nbased on Voiceprint Selection and Voice Conversion, abbreviated as VSVC.\nExperimental results demonstrated that VSVC is feasible to achieve an average\nattack success rate close to 97% in four victim models when poisoning less than\n1% of the training data.", "published": "2022-12-20 09:24:25", "link": "http://arxiv.org/abs/2212.10103v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
