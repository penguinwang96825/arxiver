{"title": "Attention-based Memory Selection Recurrent Network for Language Modeling", "abstract": "Recurrent neural networks (RNNs) have achieved great success in language\nmodeling. However, since the RNNs have fixed size of memory, their memory\ncannot store all the information about the words it have seen before in the\nsentence, and thus the useful long-term information may be ignored when\npredicting the next words. In this paper, we propose Attention-based Memory\nSelection Recurrent Network (AMSRN), in which the model can review the\ninformation stored in the memory at each previous time step and select the\nrelevant information to help generate the outputs. In AMSRN, the attention\nmechanism finds the time steps storing the relevant information in the memory,\nand memory selection determines which dimensions of the memory are involved in\ncomputing the attention weights and from which the information is extracted.In\nthe experiments, AMSRN outperformed long short-term memory (LSTM) based\nlanguage models on both English and Chinese corpora. Moreover, we investigate\nusing entropy as a regularizer for attention weights and visualize how the\nattention mechanism helps language modeling.", "published": "2016-11-26 04:25:00", "link": "http://arxiv.org/abs/1611.08656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Graph Representation with Jointly Structural and Textual\n  Encoding", "abstract": "The objective of knowledge graph embedding is to encode both entities and\nrelations of knowledge graphs into continuous low-dimensional vector spaces.\nPreviously, most works focused on symbolic representation of knowledge graph\nwith structure information, which can not handle new entities or entities with\nfew facts well. In this paper, we propose a novel deep architecture to utilize\nboth structural and textual information of entities. Specifically, we introduce\nthree neural models to encode the valuable information from text description of\nentity, among which an attentive model can select related information as\nneeded. Then, a gating mechanism is applied to integrate representations of\nstructure and text into a unified architecture. Experiments show that our\nmodels outperform baseline by margin on link prediction and triplet\nclassification tasks. Source codes of this paper will be available on Github.", "published": "2016-11-26 05:42:20", "link": "http://arxiv.org/abs/1611.08661v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fill it up: Exploiting partial dependency annotations in a minimum\n  spanning tree parser", "abstract": "Unsupervised models of dependency parsing typically require large amounts of\nclean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect\nsupervision (e.g. language universals and rules) can help, but we show that\nobtaining small amounts of direct supervision - here, partial dependency\nannotations - provides a strong balance between zero and full supervision. We\nadapt the unsupervised ConvexMST dependency parser to learn from partial\ndependencies expressed in the Graph Fragment Language. With less than 24 hours\nof total annotation, we obtain 7% and 17% absolute improvement in unlabeled\ndependency scores for English and Spanish, respectively, compared to the same\nparser using only universal grammar constraints.", "published": "2016-11-26 23:39:41", "link": "http://arxiv.org/abs/1611.08765v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Visual Dialog", "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org", "published": "2016-11-26 06:39:28", "link": "http://arxiv.org/abs/1611.08669v5", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems", "abstract": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN)\nfor multiple tasks (domains) face scalability problems. We propose a method for\nmulti-domain dialogue policy learning---termed NDQN, and apply it to an\ninformation-seeking spoken dialogue system in the domains of restaurants and\nhotels. Experimental results comparing DQN (baseline) versus NDQN (proposed)\nusing simulations report that our proposed method exhibits better scalability\nand is promising for optimising the behaviour of multi-domain dialogue systems.", "published": "2016-11-26 07:53:22", "link": "http://arxiv.org/abs/1611.08675v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Structural Correspondence Learning for Cross-lingual Sentiment\n  Classification with One-to-many Mappings", "abstract": "Structural correspondence learning (SCL) is an effective method for\ncross-lingual sentiment classification. This approach uses unlabeled documents\nalong with a word translation oracle to automatically induce task specific,\ncross-lingual correspondences. It transfers knowledge through identifying\nimportant features, i.e., pivot features. For simplicity, however, it assumes\nthat the word translation oracle maps each pivot feature in source language to\nexactly only one word in target language. This one-to-one mapping between words\nin different languages is too strict. Also the context is not considered at\nall. In this paper, we propose a cross-lingual SCL based on distributed\nrepresentation of words; it can learn meaningful one-to-many mappings for pivot\nwords using large amounts of monolingual data and a small dictionary. We\nconduct experiments on NLP\\&CC 2013 cross-lingual sentiment analysis dataset,\nemploying English as source language, and Chinese as target language. Our\nmethod does not rely on the parallel corpora and the experimental results show\nthat our approach is more competitive than the state-of-the-art methods in\ncross-lingual sentiment classification.", "published": "2016-11-26 20:11:00", "link": "http://arxiv.org/abs/1611.08737v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
