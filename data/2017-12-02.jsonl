{"title": "Improving Visually Grounded Sentence Representations with Self-Attention", "abstract": "Sentence representation models trained only on language could potentially\nsuffer from the grounding problem. Recent work has shown promising results in\nimproving the qualities of sentence representations by jointly training them\nwith associated image features. However, the grounding capability is limited\ndue to distant connection between input sentences and image features by the\ndesign of the architecture. In order to further close the gap, we propose\napplying self-attention mechanism to the sentence encoder to deepen the\ngrounding effect. Our results on transfer tasks show that self-attentive\nencoders are better for visual grounding, as they exploit specific words with\nstrong visual associations.", "published": "2017-12-02 14:14:50", "link": "http://arxiv.org/abs/1712.00609v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
