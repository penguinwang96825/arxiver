{"title": "TransModality: An End2End Fusion Method with Transformer for Multimodal\n  Sentiment Analysis", "abstract": "Multimodal sentiment analysis is an important research area that predicts\nspeaker's sentiment tendency through features extracted from textual, visual\nand acoustic modalities. The central challenge is the fusion method of the\nmultimodal information. A variety of fusion methods have been proposed, but few\nof them adopt end-to-end translation models to mine the subtle correlation\nbetween modalities. Enlightened by recent success of Transformer in the area of\nmachine translation, we propose a new fusion method, TransModality, to address\nthe task of multimodal sentiment analysis. We assume that translation between\nmodalities contributes to a better joint representation of speaker's utterance.\nWith Transformer, the learned features embody the information both from the\nsource modality and the target modality. We validate our model on multiple\nmultimodal datasets: CMU-MOSI, MELD, IEMOCAP. The experiments show that our\nproposed method achieves the state-of-the-art performance.", "published": "2020-09-07 06:11:56", "link": "http://arxiv.org/abs/2009.02902v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UIT-HSE at WNUT-2020 Task 2: Exploiting CT-BERT for Identifying COVID-19\n  Information on the Twitter Social Network", "abstract": "Recently, COVID-19 has affected a variety of real-life aspects of the world\nand led to dreadful consequences. More and more tweets about COVID-19 has been\nshared publicly on Twitter. However, the plurality of those Tweets are\nuninformative, which is challenging to build automatic systems to detect the\ninformative ones for useful AI applications. In this paper, we present our\nresults at the W-NUT 2020 Shared Task 2: Identification of Informative COVID-19\nEnglish Tweets. In particular, we propose our simple but effective approach\nusing the transformer-based models based on COVID-Twitter-BERT (CT-BERT) with\ndifferent fine-tuning techniques. As a result, we achieve the F1-Score of\n90.94\\% with the third place on the leaderboard of this task which attracted 56\nsubmitted teams in total.", "published": "2020-09-07 08:20:31", "link": "http://arxiv.org/abs/2009.02935v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why Not Simply Translate? A First Swedish Evaluation Benchmark for\n  Semantic Similarity", "abstract": "This paper presents the first Swedish evaluation benchmark for textual\nsemantic similarity. The benchmark is compiled by simply running the English\nSTS-B dataset through the Google machine translation API. This paper discusses\npotential problems with using such a simple approach to compile a Swedish\nevaluation benchmark, including translation errors, vocabulary variation, and\nproductive compounding. Despite some obvious problems with the resulting\ndataset, we use the benchmark to compare the majority of the currently existing\nSwedish text representations, demonstrating that native models outperform\nmultilingual ones, and that simple bag of words performs remarkably well.", "published": "2020-09-07 14:07:12", "link": "http://arxiv.org/abs/2009.03116v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "COVCOR20 at WNUT-2020 Task 2: An Attempt to Combine Deep Learning and\n  Expert rules", "abstract": "In the scope of WNUT-2020 Task 2, we developed various text classification\nsystems, using deep learning models and one using linguistically informed\nrules. While both of the deep learning systems outperformed the system using\nthe linguistically informed rules, we found that through the integration of\n(the output of) the three systems a better performance could be achieved than\nthe standalone performance of each approach in a cross-validation setting.\nHowever, on the test data the performance of the integration was slightly lower\nthan our best performing deep learning model. These results hardly indicate any\nprogress in line of integrating machine learning and expert rules driven\nsystems. We expect that the release of the annotation manuals and gold labels\nof the test data after this workshop will shed light on these perplexing\nresults.", "published": "2020-09-07 15:54:23", "link": "http://arxiv.org/abs/2009.03191v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TorchKGE: Knowledge Graph Embedding in Python and PyTorch", "abstract": "TorchKGE is a Python module for knowledge graph (KG) embedding relying solely\non PyTorch. This package provides researchers and engineers with a clean and\nefficient API to design and test new models. It features a KG data structure,\nsimple model interfaces and modules for negative sampling and model evaluation.\nIts main strength is a very fast evaluation module for the link prediction\ntask, a central application of KG embedding. Various KG embedding models are\nalso already implemented. Special attention has been paid to code efficiency\nand simplicity, documentation and API consistency. It is distributed using PyPI\nunder BSD license. Source code and pointers to documentation and deployment can\nbe found at https://github.com/torchkge-team/torchkge.", "published": "2020-09-07 09:21:34", "link": "http://arxiv.org/abs/2009.02963v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ambiguity Hierarchy of Regular Infinite Tree Languages", "abstract": "An automaton is unambiguous if for every input it has at most one accepting\ncomputation. An automaton is k-ambiguous (for k > 0) if for every input it has\nat most k accepting computations. An automaton is boundedly ambiguous if it is\nk-ambiguous for some $k \\in \\mathbb{N}$. An automaton is finitely\n(respectively, countably) ambiguous if for every input it has at most finitely\n(respectively, countably) many accepting computations.\n  The degree of ambiguity of a regular language is defined in a natural way. A\nlanguage is k-ambiguous (respectively, boundedly, finitely, countably\nambiguous) if it is accepted by a k-ambiguous (respectively, boundedly,\nfinitely, countably ambiguous) automaton. Over finite words every regular\nlanguage is accepted by a deterministic automaton. Over finite trees every\nregular language is accepted by an unambiguous automaton. Over $\\omega$-words\nevery regular language is accepted by an unambiguous B\\\"uchi automaton and by a\ndeterministic parity automaton. Over infinite trees Carayol et al. showed that\nthere are ambiguous languages.\n  We show that over infinite trees there is a hierarchy of degrees of\nambiguity: For every k > 1 there are k-ambiguous languages that are not k - 1\nambiguous; and there are finitely (respectively countably, uncountably)\nambiguous languages that are not boundedly (respectively finitely, countably)\nambiguous.", "published": "2020-09-07 10:08:24", "link": "http://arxiv.org/abs/2009.02985v3", "categories": ["cs.LO", "cs.CL", "03B70, 68Q45", "F.4.3"], "primary_category": "cs.LO"}
{"title": "Uncovering the Corona Virus Map Using Deep Entities and Relationship\n  Models", "abstract": "We extract entities and relationships related to COVID-19 from a corpus of\narticles related to Corona virus by employing a novel entities and relationship\nmodel. The entity recognition and relationship discovery models are trained\nwith a multi-task learning objective on a large annotated corpus. We employ a\nconcept masking paradigm to prevent the evolution of neural networks\nfunctioning as an associative memory and induce right inductive bias guiding\nthe network to make inference using only the context. We uncover several import\nsubnetworks, highlight important terms and concepts and elucidate several\ntreatment modalities employed in related ailments in the past.", "published": "2020-09-07 12:48:36", "link": "http://arxiv.org/abs/2009.03068v1", "categories": ["cs.CL", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Robust Spoken Language Understanding with RL-based Value Error Recovery", "abstract": "Spoken Language Understanding (SLU) aims to extract structured semantic\nrepresentations (e.g., slot-value pairs) from speech recognized texts, which\nsuffers from errors of Automatic Speech Recognition (ASR). To alleviate the\nproblem caused by ASR-errors, previous works may apply input adaptations to the\nspeech recognized texts, or correct ASR errors in predicted values by searching\nthe most similar candidates in pronunciation. However, these two methods are\napplied separately and independently. In this work, we propose a new robust SLU\nframework to guide the SLU input adaptation with a rule-based value error\nrecovery module. The framework consists of a slot tagging model and a\nrule-based value error recovery module. We pursue on an adapted slot tagging\nmodel which can extract potential slot-value pairs mentioned in ASR hypotheses\nand is suitable for the existing value error recovery module. After the value\nerror recovery, we can achieve a supervision signal (reward) by comparing\nrefined slot-value pairs with annotations. Since operations of the value error\nrecovery are non-differentiable, we exploit policy gradient based Reinforcement\nLearning (RL) to optimize the SLU model. Extensive experiments on the public\nCATSLU dataset show the effectiveness of our proposed approach, which can\nimprove the robustness of SLU and outperform the baselines by significant\nmargins.", "published": "2020-09-07 13:32:07", "link": "http://arxiv.org/abs/2009.03095v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NLP-CIC at SemEval-2020 Task 9: Analysing sentiment in code-switching\n  language using a simple deep-learning classifier", "abstract": "Code-switching is a phenomenon in which two or more languages are used in the\nsame message. Nowadays, it is quite common to find messages with languages\nmixed in social media. This phenomenon presents a challenge for sentiment\nanalysis. In this paper, we use a standard convolutional neural network model\nto predict the sentiment of tweets in a blend of Spanish and English languages.\nOur simple approach achieved a F1-score of 0.71 on test set on the competition.\nWe analyze our best model capabilities and perform error analysis to expose\nimportant difficulties for classifying sentiment in a code-switching setting.", "published": "2020-09-07 19:57:09", "link": "http://arxiv.org/abs/2009.03397v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Robust Conversational AI with Grounded Text Generation", "abstract": "This article presents a hybrid approach based on a Grounded Text Generation\n(GTG) model to building robust task bots at scale. GTG is a hybrid model which\nuses a large-scale Transformer neural network as its backbone, combined with\nsymbol-manipulation modules for knowledge base inference and prior knowledge\nencoding, to generate responses grounded in dialog belief state and real-world\nknowledge for task completion. GTG is pre-trained on large amounts of raw text\nand human conversational data, and can be fine-tuned to complete a wide range\nof tasks.\n  The hybrid approach and its variants are being developed simultaneously by\nmultiple research teams. The primary results reported on task-oriented dialog\nbenchmarks are very promising, demonstrating the big potential of this\napproach. This article provides an overview of this progress and discusses\nrelated methods and technologies that can be incorporated for building robust\nconversational AI systems.", "published": "2020-09-07 23:49:28", "link": "http://arxiv.org/abs/2009.03457v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "E-BERT: A Phrase and Product Knowledge Enhanced Language Model for\n  E-commerce", "abstract": "Pre-trained language models such as BERT have achieved great success in a\nbroad range of natural language processing tasks. However, BERT cannot well\nsupport E-commerce related tasks due to the lack of two levels of domain\nknowledge, i.e., phrase-level and product-level. On one hand, many E-commerce\ntasks require an accurate understanding of domain phrases, whereas such\nfine-grained phrase-level knowledge is not explicitly modeled by BERT's\ntraining objective. On the other hand, product-level knowledge like product\nassociations can enhance the language modeling of E-commerce, but they are not\nfactual knowledge thus using them indiscriminately may introduce noise. To\ntackle the problem, we propose a unified pre-training framework, namely,\nE-BERT. Specifically, to preserve phrase-level knowledge, we introduce Adaptive\nHybrid Masking, which allows the model to adaptively switch from learning\npreliminary word knowledge to learning complex phrases, based on the fitting\nprogress of two modes. To utilize product-level knowledge, we introduce\nNeighbor Product Reconstruction, which trains E-BERT to predict a product's\nassociated neighbors with a denoising cross attention layer. Our investigation\nreveals promising results in four downstream tasks, i.e., review-based question\nanswering, aspect extraction, aspect sentiment classification, and product\nclassification.", "published": "2020-09-07 00:15:36", "link": "http://arxiv.org/abs/2009.02835v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Team Alex at CLEF CheckThat! 2020: Identifying Check-Worthy Tweets With\n  Transformer Models", "abstract": "While misinformation and disinformation have been thriving in social media\nfor years, with the emergence of the COVID-19 pandemic, the political and the\nhealth misinformation merged, thus elevating the problem to a whole new level\nand giving rise to the first global infodemic. The fight against this infodemic\nhas many aspects, with fact-checking and debunking false and misleading claims\nbeing among the most important ones. Unfortunately, manual fact-checking is\ntime-consuming and automatic fact-checking is resource-intense, which means\nthat we need to pre-filter the input social media posts and to throw out those\nthat do not appear to be check-worthy. With this in mind, here we propose a\nmodel for detecting check-worthy tweets about COVID-19, which combines deep\ncontextualized text representations with modeling the social context of the\ntweet. We further describe a number of additional experiments and comparisons,\nwhich we believe should be useful for future research as they provide some\nindication about what techniques are effective for the task. Our official\nsubmission to the English version of CLEF-2020 CheckThat! Task 1, system\nTeam_Alex, was ranked second with a MAP score of 0.8034, which is almost tied\nwith the wining system, lagging behind by just 0.003 MAP points absolute.", "published": "2020-09-07 08:03:21", "link": "http://arxiv.org/abs/2009.02931v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "cs.SI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance\n  with Data Hiding", "abstract": "Recent advances in natural language generation have introduced powerful\nlanguage models with high-quality output text. However, this raises concerns\nabout the potential misuse of such models for malicious purposes. In this\npaper, we study natural language watermarking as a defense to help better mark\nand trace the provenance of text. We introduce the Adversarial Watermarking\nTransformer (AWT) with a jointly trained encoder-decoder and adversarial\ntraining that, given an input text and a binary message, generates an output\ntext that is unobtrusively encoded with the given message. We further study\ndifferent training and inference strategies to achieve minimal changes to the\nsemantics and correctness of the input text.\n  AWT is the first end-to-end model to hide data in text by automatically\nlearning -- without ground truth -- word substitutions along with their\nlocations in order to encode the message. We empirically show that our model is\neffective in largely preserving text utility and decoding the watermark while\nhiding its presence against adversaries. Additionally, we demonstrate that our\nmethod is robust against a range of attacks.", "published": "2020-09-07 11:01:24", "link": "http://arxiv.org/abs/2009.03015v2", "categories": ["cs.CR", "cs.CL", "cs.CY", "cs.LG", "I.2.7"], "primary_category": "cs.CR"}
{"title": "KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition", "abstract": "We present KoSpeech, an open-source software, which is modular and extensible\nend-to-end Korean automatic speech recognition (ASR) toolkit based on the deep\nlearning library PyTorch. Several automatic speech recognition open-source\ntoolkits have been released, but all of them deal with non-Korean languages,\nsuch as English (e.g. ESPnet, Espresso). Although AI Hub opened 1,000 hours of\nKorean speech corpus known as KsponSpeech, there is no established\npreprocessing method and baseline model to compare model performances.\nTherefore, we propose preprocessing methods for KsponSpeech corpus and a\nbaseline model for benchmarks. Our baseline model is based on Listen, Attend\nand Spell (LAS) architecture and ables to customize various training\nhyperparameters conveniently. By KoSpeech, we hope this could be a guideline\nfor those who research Korean speech recognition. Our baseline model achieved\n10.31% character error rate (CER) at KsponSpeech corpus only with the acoustic\nmodel. Our source code is available here.", "published": "2020-09-07 13:25:36", "link": "http://arxiv.org/abs/2009.03092v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Measuring Massive Multitask Language Understanding", "abstract": "We propose a new test to measure a text model's multitask accuracy. The test\ncovers 57 tasks including elementary mathematics, US history, computer science,\nlaw, and more. To attain high accuracy on this test, models must possess\nextensive world knowledge and problem solving ability. We find that while most\nrecent models have near random-chance accuracy, the very largest GPT-3 model\nimproves over random chance by almost 20 percentage points on average. However,\non every one of the 57 tasks, the best models still need substantial\nimprovements before they can reach expert-level accuracy. Models also have\nlopsided performance and frequently do not know when they are wrong. Worse,\nthey still have near-random accuracy on some socially important subjects such\nas morality and law. By comprehensively evaluating the breadth and depth of a\nmodel's academic and professional understanding, our test can be used to\nanalyze models across many tasks and to identify important shortcomings.", "published": "2020-09-07 17:59:25", "link": "http://arxiv.org/abs/2009.03300v3", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Generative Language Modeling for Automated Theorem Proving", "abstract": "We explore the application of transformer-based language models to automated\ntheorem proving. This work is motivated by the possibility that a major\nlimitation of automated theorem provers compared to humans -- the generation of\noriginal mathematical terms -- might be addressable via generation from\nlanguage models. We present an automated prover and proof assistant, GPT-f, for\nthe Metamath formalization language, and analyze its performance. GPT-f found\nnew short proofs that were accepted into the main Metamath library, which is to\nour knowledge, the first time a deep-learning based system has contributed\nproofs that were adopted by a formal mathematics community.", "published": "2020-09-07 19:50:10", "link": "http://arxiv.org/abs/2009.03393v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Is Everything Fine, Grandma? Acoustic and Linguistic Modeling for Robust\n  Elderly Speech Emotion Recognition", "abstract": "Acoustic and linguistic analysis for elderly emotion recognition is an\nunder-studied and challenging research direction, but essential for the\ncreation of digital assistants for the elderly, as well as unobtrusive\ntelemonitoring of elderly in their residences for mental healthcare purposes.\nThis paper presents our contribution to the INTERSPEECH 2020 Computational\nParalinguistics Challenge (ComParE) - Elderly Emotion Sub-Challenge, which is\ncomprised of two ternary classification tasks for arousal and valence\nrecognition. We propose a bi-modal framework, where these tasks are modeled\nusing state-of-the-art acoustic and linguistic features, respectively. In this\nstudy, we demonstrate that exploiting task-specific dictionaries and resources\ncan boost the performance of linguistic models, when the amount of labeled data\nis small. Observing a high mismatch between development and test set\nperformances of various models, we also propose alternative training and\ndecision fusion strategies to better estimate and improve the generalization\nperformance.", "published": "2020-09-07 21:19:16", "link": "http://arxiv.org/abs/2009.03432v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Language Generation with Sentence Coherence Objective", "abstract": "Conditional story generation and contextual text continuation have become\nincreasingly popular topics in NLP community. Existing models are often prone\nto output paragraphs of texts that gradually diverge from the given prompt.\nAlthough the generated text may have a reasonable perplexity and diversity, it\ncould easily be identified by human as gibberish. The goal of our project is to\nimprove the coherence and consistency across sentences in a language-generation\nmodel. We aim to solve this issue by first training a sentence pair coherence\nclassifier with GPT-2 pretrained model, and then co-train the GPT-2 language\nmodel with this new coherence objective using a method analogous to the\nREINFORCE algorithm. This fine-tuned language model is able to generate lengthy\nparagraph conditioned on a given topic without diverging too much. The\nsimplicity of this model allows it to be applicable to a variety of underlying\nlanguage model architecture since it only modifies the final layer of the\npre-trained model.", "published": "2020-09-07 06:10:03", "link": "http://arxiv.org/abs/2009.06358v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "An End-to-end Architecture of Online Multi-channel Speech Separation", "abstract": "Multi-speaker speech recognition has been one of the keychallenges in\nconversation transcription as it breaks the singleactive speaker assumption\nemployed by most state-of-the-artspeech recognition systems. Speech separation\nis consideredas a remedy to this problem. Previously, we introduced a sys-tem,\ncalledunmixing,fixed-beamformerandextraction(UFE),that was shown to be\neffective in addressing the speech over-lap problem in conversation\ntranscription. With UFE, an inputmixed signal is processed by fixed\nbeamformers, followed by aneural network post filtering. Although promising\nresults wereobtained, the system contains multiple individually\ndevelopedmodules, leading potentially sub-optimum performance. In thiswork, we\nintroduce an end-to-end modeling version of UFE. Toenable gradient propagation\nall the way, an attentional selectionmodule is proposed, where an attentional\nweight is learnt foreach beamformer and spatial feature sampled over space.\nEx-perimental results show that the proposed system achieves com-parable\nperformance in an offline evaluation with the originalseparate processing-based\npipeline, while producing remark-able improvements in an online evaluation.", "published": "2020-09-07 14:53:27", "link": "http://arxiv.org/abs/2009.03141v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Toward Speech Separation in The Pre-Cocktail Party Problem with TasTas", "abstract": "In this note, we propose to use TasTas \\cite{shi2020speech} for the\nend-to-end approach to monaural speech separation in the pre-cocktail party\nproblem. Our experiments on the public WSJ0-5mix data corpus results in 10.41dB\nSDR improvement. If online voice data remixing augmentation\n\\cite{zeghidour2020wavesplit} is adopted in training, an 11.14dB SDR\nimprovement can be achieved. We have open-sourced our re-implementation of the\nDPRNN-TasNet in\nhttps://github.com/ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separation,\nand our TasTas is realized based on this implementation of DPRNN-TasNet, it is\nbelieved that the results in this paper can be reproduced with ease.", "published": "2020-09-07 09:18:18", "link": "http://arxiv.org/abs/2009.03692v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Digital Envelope Estimation Via Geometric Properties of an Arbitrary\n  Real Signal", "abstract": "Envelope detection techniques have applications in areas like medicine, sound\nclassification and synthesis, seismology and speech recognition. Nevertheless,\na general approach to digital envelope detection of signals with rich spectral\ncontent doesn't exist, as most methods involve manual intervention, in the form\nof filter design, smoothing, and other specific design choices, based on prior\nknowledge of the signals under investigation. To address this problem, we\npropose an algorithm that uses intrinsic characteristics of a signal to\nestimate its envelope, eliminating the necessity of parameter tuning. The\napproach here described draws inspiration from geometric concepts to estimate\nthe temporal envelope of an arbitrary signal; specifically, a new measure of\ndiscrete curvature is used to obtain the average radius of curvature of a\ndiscrete wave, that will serve as a threshold to identify the waves samples\nthat are part of the envelope. The algorithm compares favourably with classic\nenvelope detection techniques based on smoothing, filtering and the Hilbert\nTransform, besides being physically plausible. We provide visualizations of the\nenvelope extracted via the algorithm for various real-world signals, with very\ndiverse characteristics, such as voice, spoken and sang, and pitched and\nnon-pitched musical instruments, and discuss some approaches to assess the\nquality of the obtained envelopes. A Python module implementing the algorithm\nwas made available via the Python Package Index; interactive visualizations of\nenvelopes for a diverse range of digital waves, as well as the source code for\nthe Python implementation, are available online.", "published": "2020-09-07 02:25:22", "link": "http://arxiv.org/abs/2009.02860v2", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Deep Learning-Based Single-Ended Objective Quality Measures for\n  Time-Scale Modified Audio", "abstract": "Objective evaluation of audio processed with Time-Scale Modification (TSM) is\nseeing a resurgence of interest. Recently, a labelled time-scaled audio dataset\nwas used to train an objective measure for TSM evaluation. This DE measure was\nan extension of Perceptual Evaluation of Audio Quality, and required reference\nand test signals. In this paper, two single-ended objective quality measures\nfor time-scaled audio are proposed that do not require a reference signal. Data\ndriven features are created by either a convolutional neural network (CNN) or a\nbidirectional gated recurrent unit (BGRU) network and fed to a fully-connected\nnetwork to predict subjective mean opinion scores. The proposed CNN and BGRU\nmeasures achieve an average Root Mean Squared Error of 0.608 and 0.576, and a\nmean Pearson correlation of 0.771 and 0.794, respectively. The proposed\nmeasures are used to evaluate TSM algorithms, and comparisons are provided for\n16 TSM implementations. The objective measure is available at\nhttps://www.github.com/zygurt/TSM.", "published": "2020-09-07 08:28:00", "link": "http://arxiv.org/abs/2009.02940v1", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
