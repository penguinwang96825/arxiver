{"title": "Exploring Question Understanding and Adaptation in Neural-Network-Based\n  Question Answering", "abstract": "The last several years have seen intensive interest in exploring\nneural-network-based models for machine comprehension (MC) and question\nanswering (QA). In this paper, we approach the problems by closely modelling\nquestions in a neural network framework. We first introduce syntactic\ninformation to help encode questions. We then view and model different types of\nquestions and the information shared among them as an adaptation task and\nproposed adaptation models for them. On the Stanford Question Answering Dataset\n(SQuAD), we show that these approaches can help attain better results over a\ncompetitive baseline.", "published": "2017-03-14 17:43:25", "link": "http://arxiv.org/abs/1703.04617v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Learning of Correlated Sequence Labelling Tasks Using\n  Bidirectional Recurrent Neural Networks", "abstract": "The stream of words produced by Automatic Speech Recognition (ASR) systems is\ntypically devoid of punctuations and formatting. Most natural language\nprocessing applications expect segmented and well-formatted texts as input,\nwhich is not available in ASR output. This paper proposes a novel technique of\njointly modeling multiple correlated tasks such as punctuation and\ncapitalization using bidirectional recurrent neural networks, which leads to\nimproved performance for each of these tasks. This method could be extended for\njoint modeling of any other correlated sequence labeling tasks.", "published": "2017-03-14 18:26:12", "link": "http://arxiv.org/abs/1703.04650v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A computational investigation of sources of variability in sentence\n  comprehension difficulty in aphasia", "abstract": "We present a computational evaluation of three hypotheses about sources of\ndeficit in sentence comprehension in aphasia: slowed processing, intermittent\ndeficiency, and resource reduction. The ACT-R based Lewis and Vasishth (2005)\nmodel is used to implement these three proposals. Slowed processing is\nimplemented as slowed default production-rule firing time; intermittent\ndeficiency as increased random noise in activation of chunks in memory; and\nresource reduction as reduced goal activation. As data, we considered subject\nvs. object rela- tives whose matrix clause contained either an NP or a\nreflexive, presented in a self-paced listening modality to 56 individuals with\naphasia (IWA) and 46 matched controls. The participants heard the sentences and\ncarried out a picture verification task to decide on an interpretation of the\nsentence. These response accuracies are used to identify the best parameters\n(for each participant) that correspond to the three hypotheses mentioned above.\nWe show that controls have more tightly clustered (less variable) parameter\nvalues than IWA; specifically, compared to controls, among IWA there are more\nindividuals with low goal activations, high noise, and slow default action\ntimes. This suggests that (i) individual patients show differential amounts of\ndeficit along the three dimensions of slowed processing, intermittent\ndeficient, and resource reduction, (ii) overall, there is evidence for all\nthree sources of deficit playing a role, and (iii) IWA have a more variable\nrange of parameter values than controls. In sum, this study contributes a proof\nof concept of a quantitative implementation of, and evidence for, these three\naccounts of comprehension deficits in aphasia.", "published": "2017-03-14 19:14:32", "link": "http://arxiv.org/abs/1703.04677v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multichannel End-to-end Speech Recognition", "abstract": "The field of speech recognition is in the midst of a paradigm shift:\nend-to-end neural networks are challenging the dominance of hidden Markov\nmodels as a core technology. Using an attention mechanism in a recurrent\nencoder-decoder architecture solves the dynamic time alignment problem,\nallowing joint end-to-end training of the acoustic and language modeling\ncomponents. In this paper we extend the end-to-end framework to encompass\nmicrophone array signal processing for noise suppression and speech enhancement\nwithin the acoustic encoding network. This allows the beamforming components to\nbe optimized jointly within the recognition architecture to improve the\nend-to-end speech recognition objective. Experiments on the noisy speech\nbenchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system\noutperformed the attention-based baseline with input from a conventional\nadaptive beamformer.", "published": "2017-03-14 22:28:51", "link": "http://arxiv.org/abs/1703.04783v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role\n  Labeling", "abstract": "Semantic role labeling (SRL) is the task of identifying the\npredicate-argument structure of a sentence. It is typically regarded as an\nimportant step in the standard NLP pipeline. As the semantic representations\nare closely related to syntactic ones, we exploit syntactic information in our\nmodel. We propose a version of graph convolutional networks (GCNs), a recent\nclass of neural networks operating on graphs, suited to model syntactic\ndependency graphs. GCNs over syntactic dependency trees are used as sentence\nencoders, producing latent feature representations of words in a sentence. We\nobserve that GCN layers are complementary to LSTM ones: when we stack both GCN\nand LSTM layers, we obtain a substantial improvement over an already\nstate-of-the-art LSTM SRL model, resulting in the best reported scores on the\nstandard benchmark (CoNLL-2009) both for Chinese and English.", "published": "2017-03-14 23:25:34", "link": "http://arxiv.org/abs/1703.04826v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Making Neural QA as Simple as Possible but not Simpler", "abstract": "Recent development of large-scale question answering (QA) datasets triggered\na substantial amount of research into end-to-end neural architectures for QA.\nIncreasingly complex systems have been conceived without comparison to simpler\nneural baseline systems that would justify their complexity. In this work, we\npropose a simple heuristic that guides the development of neural baseline\nsystems for the extractive QA task. We find that there are two ingredients\nnecessary for building a high-performing neural QA system: first, the awareness\nof question words while processing the context and second, a composition\nfunction that goes beyond simple bag-of-words modeling, such as recurrent\nneural networks. Our results show that FastQA, a system that meets these two\nrequirements, can achieve very competitive performance compared with existing\nmodels. We argue that this surprising finding puts results of previous systems\nand the complexity of recent QA datasets into perspective.", "published": "2017-03-14 23:09:45", "link": "http://arxiv.org/abs/1703.04816v3", "categories": ["cs.CL", "cs.AI", "cs.NE"], "primary_category": "cs.CL"}
