{"title": "Data Expansion using Back Translation and Paraphrasing for Hate Speech\n  Detection", "abstract": "With proliferation of user generated contents in social media platforms,\nestablishing mechanisms to automatically identify toxic and abusive content\nbecomes a prime concern for regulators, researchers, and society. Keeping the\nbalance between freedom of speech and respecting each other dignity is a major\nconcern of social media platform regulators. Although, automatic detection of\noffensive content using deep learning approaches seems to provide encouraging\nresults, training deep learning-based models requires large amounts of\nhigh-quality labeled data, which is often missing. In this regard, we present\nin this paper a new deep learning-based method that fuses a Back Translation\nmethod, and a Paraphrasing technique for data augmentation. Our pipeline\ninvestigates different word-embedding-based architectures for classification of\nhate speech. The back translation technique relies on an encoder-decoder\narchitecture pre-trained on a large corpus and mostly used for machine\ntranslation. In addition, paraphrasing exploits the transformer model and the\nmixture of experts to generate diverse paraphrases. Finally, LSTM, and CNN are\ncompared to seek enhanced classification results. We evaluate our proposal on\nfive publicly available datasets; namely, AskFm corpus, Formspring dataset,\nWarner and Waseem dataset, Olid, and Wikipedia toxic comments dataset. The\nperformance of the proposal together with comparison to some related\nstate-of-art results demonstrate the effectiveness and soundness of our\nproposal.", "published": "2021-05-25 09:52:42", "link": "http://arxiv.org/abs/2106.04681v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Modeling and Progression of American Digital News Media During the\n  Onset of the COVID-19 Pandemic", "abstract": "Currently, the world is in the midst of a severe global pandemic, which has\naffected all aspects of people's lives. As a result, there is a deluge of\nCOVID-related digital media articles published in the United States, due to the\ndisparate effects of the pandemic. This large volume of information is\ndifficult to consume by the audience in a reasonable amount of time. In this\npaper, we develop a Natural Language Processing (NLP) pipeline that is capable\nof automatically distilling various digital articles into manageable pieces of\ninformation, while also modelling the progression topics discussed over time in\norder to aid readers in rapidly gaining holistic perspectives on pressing\nissues (i.e., the COVID-19 pandemic) from a diverse array of sources. We\nachieve these goals by first collecting a large corpus of COVID-related\narticles during the onset of the pandemic. After, we apply unsupervised and\nsemi-supervised learning procedures to summarize articles, then cluster them\nbased on their similarities using the community detection methods. Next, we\nidentify the topic of each cluster of articles using the BART algorithm.\nFinally, we provide a detailed digital media analysis based on the NLP-pipeline\noutputs and show how the conversation surrounding COVID-19 evolved over time.", "published": "2021-05-25 14:27:47", "link": "http://arxiv.org/abs/2106.09572v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference", "abstract": "Existing pre-trained language models (PLMs) are often computationally\nexpensive in inference, making them impractical in various resource-limited\nreal-world applications. To address this issue, we propose a dynamic token\nreduction approach to accelerate PLMs' inference, named TR-BERT, which could\nflexibly adapt the layer number of each token in inference to avoid redundant\ncalculation. Specially, TR-BERT formulates the token reduction process as a\nmulti-step token selection problem and automatically learns the selection\nstrategy via reinforcement learning. The experimental results on several\ndownstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to\nsatisfy various performance demands. Moreover, TR-BERT can also achieve better\nperformance with less computation in a suite of long-text tasks since its\ntoken-level layer number adaption greatly accelerates the self-attention\noperation in PLMs. The source code and experiment details of this paper can be\nobtained from https://github.com/thunlp/TR-BERT.", "published": "2021-05-25 02:28:51", "link": "http://arxiv.org/abs/2105.11618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Complex Knowledge Base Question Answering: Methods,\n  Challenges and Solutions", "abstract": "Knowledge base question answering (KBQA) aims to answer a question over a\nknowledge base (KB). Recently, a large number of studies focus on semantically\nor syntactically complicated questions. In this paper, we elaborately summarize\nthe typical challenges and solutions for complex KBQA. We begin with\nintroducing the background about the KBQA task. Next, we present the two\nmainstream categories of methods for complex KBQA, namely semantic\nparsing-based (SP-based) methods and information retrieval-based (IR-based)\nmethods. We then review the advanced methods comprehensively from the\nperspective of the two categories. Specifically, we explicate their solutions\nto the typical challenges. Finally, we conclude and discuss some promising\ndirections for future research.", "published": "2021-05-25 03:45:30", "link": "http://arxiv.org/abs/2105.11644v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-Task Learning of Generation and Classification for Emotion-Aware\n  Dialogue Response Generation", "abstract": "For a computer to naturally interact with a human, it needs to be human-like.\nIn this paper, we propose a neural response generation model with multi-task\nlearning of generation and classification, focusing on emotion. Our model based\non BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model,\nis trained to generate responses and recognize emotions simultaneously.\nFurthermore, we weight the losses for the tasks to control the update of\nparameters. Automatic evaluations and crowdsourced manual evaluations show that\nthe proposed model makes generated responses more emotionally aware.", "published": "2021-05-25 06:41:20", "link": "http://arxiv.org/abs/2105.11696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Argument Undermining: Counter-Argument Generation by Attacking Weak\n  Premises", "abstract": "Text generation has received a lot of attention in computational\nargumentation research as of recent. A particularly challenging task is the\ngeneration of counter-arguments. So far, approaches primarily focus on\nrebutting a given conclusion, yet other ways to counter an argument exist. In\nthis work, we go beyond previous research by exploring argument undermining,\nthat is, countering an argument by attacking one of its premises. We\nhypothesize that identifying the argument's weak premises is key to effective\ncountering. Accordingly, we propose a pipeline approach that first assesses the\npremises' strength and then generates a counter-argument targeting the weak\nones. On the one hand, both manual and automatic evaluation proves the\nimportance of identifying weak premises in counter-argument generation. On the\nother hand, when considering correctness and content richness, human annotators\nfavored our approach over state-of-the-art counter-argument generation.", "published": "2021-05-25 08:39:14", "link": "http://arxiv.org/abs/2105.11752v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dynamic Semantic Graph Construction and Reasoning for Explainable\n  Multi-hop Science Question Answering", "abstract": "Knowledge retrieval and reasoning are two key stages in multi-hop question\nanswering (QA) at web scale. Existing approaches suffer from low confidence\nwhen retrieving evidence facts to fill the knowledge gap and lack transparent\nreasoning process. In this paper, we propose a new framework to exploit more\nvalid facts while obtaining explainability for multi-hop QA by dynamically\nconstructing a semantic graph and reasoning over it. We employ Abstract Meaning\nRepresentation (AMR) as semantic graph representation. Our framework contains\nthree new ideas: (a) {\\tt AMR-SG}, an AMR-based Semantic Graph, constructed by\ncandidate fact AMRs to uncover any hop relations among question, answer and\nmultiple facts. (b) A novel path-based fact analytics approach exploiting {\\tt\nAMR-SG} to extract active facts from a large fact pool to answer questions. (c)\nA fact-level relation modeling leveraging graph convolution network (GCN) to\nguide the reasoning process. Results on two scientific multi-hop QA datasets\nshow that we can surpass recent approaches including those using additional\nknowledge graphs while maintaining high explainability on OpenBookQA and\nachieve a new state-of-the-art result on ARC-Challenge in a computationally\npracticable setting.", "published": "2021-05-25 09:14:55", "link": "http://arxiv.org/abs/2105.11776v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Redundancy in Clinical Text", "abstract": "The current mode of use of Electronic Health Record (EHR) elicits text\nredundancy. Clinicians often populate new documents by duplicating existing\nnotes, then updating accordingly. Data duplication can lead to a propagation of\nerrors, inconsistencies and misreporting of care. Therefore, quantifying\ninformation redundancy can play an essential role in evaluating innovations\nthat operate on clinical narratives.\n  This work is a quantitative examination of information redundancy in EHR\nnotes. We present and evaluate two strategies to measure redundancy: an\ninformation-theoretic approach and a lexicosyntactic and semantic model. We\nevaluate the measures by training large Transformer-based language models using\nclinical text from a large openly available US-based ICU dataset and a large\nmulti-site UK based Trust. By comparing the information-theoretic content of\nthe trained models with open-domain language models, the language models\ntrained using clinical text have shown ~1.5x to ~3x less efficient than\nopen-domain corpora. Manual evaluation shows a high correlation with\nlexicosyntactic and semantic redundancy, with averages ~43 to ~65%.", "published": "2021-05-25 11:01:45", "link": "http://arxiv.org/abs/2105.11832v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empirical Error Modeling Improves Robustness of Noisy Neural Sequence\n  Labeling", "abstract": "Despite recent advances, standard sequence labeling systems often fail when\nprocessing noisy user-generated text or consuming the output of an Optical\nCharacter Recognition (OCR) process. In this paper, we improve the noise-aware\ntraining method by proposing an empirical error generation approach that\nemploys a sequence-to-sequence model trained to perform translation from\nerror-free to erroneous text. Using an OCR engine, we generated a large\nparallel text corpus for training and produced several real-world noisy\nsequence labeling benchmarks for evaluation. Moreover, to overcome the data\nsparsity problem that exacerbates in the case of imperfect textual input, we\nlearned noisy language model-based embeddings. Our approach outperformed the\nbaseline noise generation and error correction techniques on the erroneous\nsequence labeling data sets. To facilitate future research on robustness, we\nmake our code, embeddings, and data conversion scripts publicly available.", "published": "2021-05-25 12:15:45", "link": "http://arxiv.org/abs/2105.11872v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Focus Attention: Promoting Faithfulness and Diversity in Summarization", "abstract": "Professional summaries are written with document-level information, such as\nthe theme of the document, in mind. This is in contrast with most seq2seq\ndecoders which simultaneously learn to focus on salient content, while deciding\nwhat to generate, at each decoding step. With the motivation to narrow this\ngap, we introduce Focus Attention Mechanism, a simple yet effective method to\nencourage decoders to proactively generate tokens that are similar or topical\nto the input document. Further, we propose a Focus Sampling method to enable\ngeneration of diverse summaries, an area currently understudied in\nsummarization. When evaluated on the BBC extreme summarization task, two\nstate-of-the-art models augmented with Focus Attention generate summaries that\nare closer to the target and more faithful to their input documents,\noutperforming their vanilla counterparts on \\rouge and multiple faithfulness\nmeasures. We also empirically demonstrate that Focus Sampling is more effective\nin generating diverse and faithful summaries than top-$k$ or nucleus\nsampling-based decoding methods.", "published": "2021-05-25 13:25:18", "link": "http://arxiv.org/abs/2105.11921v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extending rational models of communication from beliefs to actions", "abstract": "Speakers communicate to influence their partner's beliefs and shape their\nactions. Belief- and action-based objectives have been explored independently\nin recent computational models, but it has been challenging to explicitly\ncompare or integrate them. Indeed, we find that they are conflated in standard\nreferential communication tasks. To distinguish these accounts, we introduce a\nnew paradigm called signaling bandits, generalizing classic Lewis signaling\ngames to a multi-armed bandit setting where all targets in the context have\nsome relative value. We develop three speaker models: a belief-oriented speaker\nwith a purely informative objective; an action-oriented speaker with an\ninstrumental objective; and a combined speaker which integrates the two by\ninducing listener beliefs that generally lead to desirable actions. We then\npresent a series of simulations demonstrating that grounding production choices\nin future listener actions results in relevance effects and flexible uses of\nnonliteral language. More broadly, our findings suggest that language games\nbased on richer decision problems are a promising avenue for insight into\nrational communication.", "published": "2021-05-25 13:58:01", "link": "http://arxiv.org/abs/2105.11950v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph", "abstract": "Abstractive summarization for long-document or multi-document remains\nchallenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing\nlong-distance relations in text. In this paper, we present BASS, a novel\nframework for Boosting Abstractive Summarization based on a unified Semantic\ngraph, which aggregates co-referent phrases distributing across a long range of\ncontext and conveys rich relations between phrases. Further, a graph-based\nencoder-decoder model is proposed to improve both the document representation\nand summary generation process by leveraging the graph structure. Specifically,\nseveral graph augmentation methods are designed to encode both the explicit and\nimplicit relations in the text while the graph-propagation attention mechanism\nis developed in the decoder to select salient content into the summary.\nEmpirical results show that the proposed architecture brings substantial\nimprovements for both long-document and multi-document summarization tasks.", "published": "2021-05-25 16:20:48", "link": "http://arxiv.org/abs/2105.12041v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IntelliCAT: Intelligent Machine Translation Post-Editing with Quality\n  Estimation and Translation Suggestion", "abstract": "We present IntelliCAT, an interactive translation interface with neural\nmodels that streamline the post-editing process on machine translation output.\nWe leverage two quality estimation (QE) models at different granularities:\nsentence-level QE, to predict the quality of each machine-translated sentence,\nand word-level QE, to locate the parts of the machine-translated sentence that\nneed correction. Additionally, we introduce a novel translation suggestion\nmodel conditioned on both the left and right contexts, providing alternatives\nfor specific words or phrases for correction. Finally, with word alignments,\nIntelliCAT automatically preserves the original document's styles in the\ntranslated document. The experimental results show that post-editing based on\nthe proposed QE and translation suggestions can significantly improve\ntranslation quality. Furthermore, a user study reveals that three features\nprovided in IntelliCAT significantly accelerate the post-editing task,\nachieving a 52.9\\% speedup in translation time compared to translating from\nscratch. The interface is publicly available at\nhttps://intellicat.beringlab.com/.", "published": "2021-05-25 19:00:22", "link": "http://arxiv.org/abs/2105.12172v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "NukeLM: Pre-Trained and Fine-Tuned Language Models for the Nuclear and\n  Energy Domains", "abstract": "Natural language processing (NLP) tasks (text classification, named entity\nrecognition, etc.) have seen revolutionary improvements over the last few\nyears. This is due to language models such as BERT that achieve deep knowledge\ntransfer by using a large pre-trained model, then fine-tuning the model on\nspecific tasks. The BERT architecture has shown even better performance on\ndomain-specific tasks when the model is pre-trained using domain-relevant\ntexts. Inspired by these recent advancements, we have developed NukeLM, a\nnuclear-domain language model pre-trained on 1.5 million abstracts from the\nU.S. Department of Energy Office of Scientific and Technical Information (OSTI)\ndatabase. This NukeLM model is then fine-tuned for the classification of\nresearch articles into either binary classes (related to the nuclear fuel cycle\n[NFC] or not) or multiple categories related to the subject of the article. We\nshow that continued pre-training of a BERT-style architecture prior to\nfine-tuning yields greater performance on both article classification tasks.\nThis information is critical for properly triaging manuscripts, a necessary\ntask for better understanding citation networks that publish in the nuclear\nspace, and for uncovering new areas of research in the nuclear (or\nnuclear-relevant) domains.", "published": "2021-05-25 20:00:59", "link": "http://arxiv.org/abs/2105.12192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent\n  Detection and Slot Filling", "abstract": "In this paper, we investigate few-shot joint learning for dialogue language\nunderstanding. Most existing few-shot models learn a single task each time with\nonly a few examples. However, dialogue language understanding contains two\nclosely related tasks, i.e., intent detection and slot filling, and often\nbenefits from jointly learning the two tasks. This calls for new few-shot\nlearning techniques that are able to capture task relations from only a few\nexamples and jointly learn multiple tasks. To achieve this, we propose a\nsimilarity-based few-shot learning scheme, named Contrastive Prototype Merging\nnetwork (ConProm), that learns to bridge metric spaces of intent and slot on\ndata-rich domains, and then adapt the bridged metric space to the specific\nfew-shot domain. Experiments on two public datasets, Snips and FewJoint, show\nthat our model significantly outperforms the strong baselines in one and five\nshots settings.", "published": "2021-05-25 15:07:11", "link": "http://arxiv.org/abs/2106.07343v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Guiding the Growth: Difficulty-Controllable Question Generation through\n  Step-by-Step Rewriting", "abstract": "This paper explores the task of Difficulty-Controllable Question Generation\n(DCQG), which aims at generating questions with required difficulty levels.\nPrevious research on this task mainly defines the difficulty of a question as\nwhether it can be correctly answered by a Question Answering (QA) system,\nlacking interpretability and controllability. In our work, we redefine question\ndifficulty as the number of inference steps required to answer it and argue\nthat Question Generation (QG) systems should have stronger control over the\nlogic of generated questions. To this end, we propose a novel framework that\nprogressively increases question difficulty through step-by-step rewriting\nunder the guidance of an extracted reasoning chain. A dataset is automatically\nconstructed to facilitate the research, on which extensive experiments are\nconducted to test the performance of our method.", "published": "2021-05-25 06:43:13", "link": "http://arxiv.org/abs/2105.11698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence\n  Representation Transfer", "abstract": "Learning high-quality sentence representations benefits a wide range of\nnatural language processing tasks. Though BERT-based pre-trained language\nmodels achieve high performance on many downstream tasks, the native derived\nsentence representations are proved to be collapsed and thus produce a poor\nperformance on the semantic textual similarity (STS) tasks. In this paper, we\npresent ConSERT, a Contrastive Framework for Self-Supervised Sentence\nRepresentation Transfer, that adopts contrastive learning to fine-tune BERT in\nan unsupervised and effective way. By making use of unlabeled texts, ConSERT\nsolves the collapse issue of BERT-derived sentence representations and make\nthem more applicable for downstream tasks. Experiments on STS datasets\ndemonstrate that ConSERT achieves an 8\\% relative improvement over the previous\nstate-of-the-art, even comparable to the supervised SBERT-NLI. And when further\nincorporating NLI supervision, we achieve new state-of-the-art performance on\nSTS tasks. Moreover, ConSERT obtains comparable results with only 1000 samples\navailable, showing its robustness in data scarcity scenarios.", "published": "2021-05-25 08:15:01", "link": "http://arxiv.org/abs/2105.11741v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Super Tickets in Pre-Trained Language Models: From Model Compression to\n  Improving Generalization", "abstract": "The Lottery Ticket Hypothesis suggests that an over-parametrized network\nconsists of ``lottery tickets'', and training a certain collection of them\n(i.e., a subnetwork) can match the performance of the full model. In this\npaper, we study such a collection of tickets, which is referred to as ``winning\ntickets'', in extremely over-parametrized models, e.g., pre-trained language\nmodels. We observe that at certain compression ratios, the generalization\nperformance of the winning tickets can not only match but also exceed that of\nthe full model. In particular, we observe a phase transition phenomenon: As the\ncompression ratio increases, generalization performance of the winning tickets\nfirst improves then deteriorates after a certain threshold. We refer to the\ntickets on the threshold as ``super tickets''. We further show that the phase\ntransition is task and model dependent -- as the model size becomes larger and\nthe training data set becomes smaller, the transition becomes more pronounced.\nOur experiments on the GLUE benchmark show that the super tickets improve\nsingle task fine-tuning by $0.9$ points on BERT-base and $1.0$ points on\nBERT-large, in terms of task-average score. We also demonstrate that adaptively\nsharing the super tickets across tasks benefits multi-task learning.", "published": "2021-05-25 15:10:05", "link": "http://arxiv.org/abs/2105.12002v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "The incel lexicon: Deciphering the emergent cryptolect of a global\n  misogynistic community", "abstract": "Evolving out of a gender-neutral framing of an involuntary celibate identity,\nthe concept of `incels' has come to refer to an online community of men who\nbear antipathy towards themselves, women, and society-at-large for their\nperceived inability to find and maintain sexual relationships. By exploring\nincel language use on Reddit, a global online message board, we contextualize\nthe incel community's online expressions of misogyny and real-world acts of\nviolence perpetrated against women. After assembling around three million\ncomments from incel-themed Reddit channels, we analyze the temporal dynamics of\na data driven rank ordering of the glossary of phrases belonging to an emergent\nincel lexicon. Our study reveals the generation and normalization of an\nextensive coded misogynist vocabulary in service of the group's identity.", "published": "2021-05-25 15:20:13", "link": "http://arxiv.org/abs/2105.12006v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "NEUer at SemEval-2021 Task 4: Complete Summary Representation by Filling\n  Answers into Question for Matching Reading Comprehension", "abstract": "SemEval task 4 aims to find a proper option from multiple candidates to\nresolve the task of machine reading comprehension. Most existing approaches\npropose to concat question and option together to form a context-aware model.\nHowever, we argue that straightforward concatenation can only provide a\ncoarse-grained context for the MRC task, ignoring the specific positions of the\noption relative to the question. In this paper, we propose a novel MRC model by\nfilling options into the question to produce a fine-grained context (defined as\nsummary) which can better reveal the relationship between option and question.\nWe conduct a series of experiments on the given dataset, and the results show\nthat our approach outperforms other counterparts to a large extent.", "published": "2021-05-25 16:31:26", "link": "http://arxiv.org/abs/2105.12051v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Taxonomy of academic plagiarism methods", "abstract": "The article gives an overview of the plagiarism domain, with focus on\nacademic plagiarism. The article defines plagiarism, explains the origin of the\nterm, as well as plagiarism related terms. It identifies the extent of the\nplagiarism domain and then focuses on the plagiarism subdomain of text\ndocuments, for which it gives an overview of current classifications and\ntaxonomies and then proposes a more comprehensive classification according to\nseveral criteria: their origin and purpose, technical implementation,\nconsequence, complexity of detection and according to the number of linguistic\nsources. The article suggests the new classification of academic plagiarism,\ndescribes sorts and methods of plagiarism, types and categories, approaches and\nphases of plagiarism detection, the classification of methods and algorithms\nfor plagiarism detection. The title of the article explicitly targets the\nacademic community, but it is sufficiently general and interdisciplinary, so it\ncan be useful for many other professionals like software developers, linguists\nand librarians.", "published": "2021-05-25 16:49:08", "link": "http://arxiv.org/abs/2105.12068v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Context-Sensitive Visualization of Deep Learning Natural Language\n  Processing Models", "abstract": "The introduction of Transformer neural networks has changed the landscape of\nNatural Language Processing (NLP) during the last years. So far, none of the\nvisualization systems has yet managed to examine all the facets of the\nTransformers. This gave us the motivation of the current work. We propose a new\nNLP Transformer context-sensitive visualization method that leverages existing\nNLP tools to find the most significant groups of tokens (words) that have the\ngreatest effect on the output, thus preserving some context from the original\ntext. First, we use a sentence-level dependency parser to highlight promising\nword groups. The dependency parser creates a tree of relationships between the\nwords in the sentence. Next, we systematically remove adjacent and non-adjacent\ntuples of \\emph{n} tokens from the input text, producing several new texts with\nthose tokens missing. The resulting texts are then passed to a pre-trained BERT\nmodel. The classification output is compared with that of the full text, and\nthe difference in the activation strength is recorded. The modified texts that\nproduce the largest difference in the target classification output neuron are\nselected, and the combination of removed words are then considered to be the\nmost influential on the model's output. Finally, the most influential word\ncombinations are visualized in a heatmap.", "published": "2021-05-25 20:26:38", "link": "http://arxiv.org/abs/2105.12202v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Impact of detecting clinical trial elements in exploration of COVID-19\n  literature", "abstract": "The COVID-19 pandemic has driven ever-greater demand for tools which enable\nefficient exploration of biomedical literature. Although semi-structured\ninformation resulting from concept recognition and detection of the defining\nelements of clinical trials (e.g. PICO criteria) has been commonly used to\nsupport literature search, the contributions of this abstraction remain poorly\nunderstood, especially in relation to text-based retrieval. In this study, we\ncompare the results retrieved by a standard search engine with those filtered\nusing clinically-relevant concepts and their relations. With analysis based on\nthe annotations from the TREC-COVID shared task, we obtain quantitative as well\nas qualitative insights into characteristics of relational and concept-based\nliterature exploration. Most importantly, we find that the relational concept\nselection filters the original retrieved collection in a way that decreases the\nproportion of unjudged documents and increases the precision, which means that\nthe user is likely to be exposed to a larger number of relevant documents.", "published": "2021-05-25 23:41:24", "link": "http://arxiv.org/abs/2105.12261v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report\n  Generation", "abstract": "Medical report generation is one of the most challenging tasks in medical\nimage analysis. Although existing approaches have achieved promising results,\nthey either require a predefined template database in order to retrieve\nsentences or ignore the hierarchical nature of medical report generation. To\naddress these issues, we propose MedWriter that incorporates a novel\nhierarchical retrieval mechanism to automatically extract both report and\nsentence-level templates for clinically accurate report generation. MedWriter\nfirst employs the Visual-Language Retrieval~(VLR) module to retrieve the most\nrelevant reports for the given images. To guarantee the logical coherence\nbetween sentences, the Language-Language Retrieval~(LLR) module is introduced\nto retrieve relevant sentences based on the previous generated description. At\nlast, a language decoder fuses image features and features from retrieved\nreports and sentences to generate meaningful medical reports. We verified the\neffectiveness of our model by automatic evaluation and human evaluation on two\ndatasets, i.e., Open-I and MIMIC-CXR.", "published": "2021-05-25 07:47:23", "link": "http://arxiv.org/abs/2106.06471v1", "categories": ["cs.CL", "cs.CV", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Personalized Transformer for Explainable Recommendation", "abstract": "Personalization of natural language generation plays a vital role in a large\nspectrum of tasks, such as explainable recommendation, review summarization and\ndialog systems. In these tasks, user and item IDs are important identifiers for\npersonalization. Transformer, which is demonstrated with strong language\nmodeling capability, however, is not personalized and fails to make use of the\nuser and item IDs since the ID tokens are not even in the same semantic space\nas the words. To address this problem, we present a PErsonalized Transformer\nfor Explainable Recommendation (PETER), on which we design a simple and\neffective learning objective that utilizes the IDs to predict the words in the\ntarget explanation, so as to endow the IDs with linguistic meanings and to\nachieve personalized Transformer. Besides generating explanations, PETER can\nalso make recommendations, which makes it a unified model for the whole\nrecommendation-explanation pipeline. Extensive experiments show that our small\nunpretrained model outperforms fine-tuned BERT on the generation task, in terms\nof both effectiveness and efficiency, which highlights the importance and the\nnice utility of our design.", "published": "2021-05-25 01:42:47", "link": "http://arxiv.org/abs/2105.11601v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "ViBERTgrid: A Jointly Trained Multi-Modal 2D Document Representation for\n  Key Information Extraction from Documents", "abstract": "Recent grid-based document representations like BERTgrid allow the\nsimultaneous encoding of the textual and layout information of a document in a\n2D feature map so that state-of-the-art image segmentation and/or object\ndetection models can be straightforwardly leveraged to extract key information\nfrom documents. However, such methods have not achieved comparable performance\nto state-of-the-art sequence- and graph-based methods such as LayoutLM and PICK\nyet. In this paper, we propose a new multi-modal backbone network by\nconcatenating a BERTgrid to an intermediate layer of a CNN model, where the\ninput of CNN is a document image and the BERTgrid is a grid of word embeddings,\nto generate a more powerful grid-based document representation, named\nViBERTgrid. Unlike BERTgrid, the parameters of BERT and CNN in our multimodal\nbackbone network are trained jointly. Our experimental results demonstrate that\nthis joint training strategy improves significantly the representation ability\nof ViBERTgrid. Consequently, our ViBERTgrid-based key information extraction\napproach has achieved state-of-the-art performance on real-world datasets.", "published": "2021-05-25 05:12:08", "link": "http://arxiv.org/abs/2105.11672v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Look inside. Predicting stock prices by analysing an enterprise intranet\n  social network and using word co-occurrence networks", "abstract": "This study looks into employees' communication, offering novel metrics which\ncan help to predict a company's stock price. We studied the intranet forum of a\nlarge Italian company, exploring the interactions and the use of language of\nabout 8,000 employees. We built a network linking words included in the general\ndiscourse. In this network, we focused on the position of the node representing\nthe company brand. We found that a lower sentiment, a higher betweenness\ncentrality of the company brand, a denser word co-occurrence network and more\nequally distributed centrality scores of employees (lower group betweenness\ncentrality) are all significant predictors of higher stock prices. Our findings\noffers new metrics that can be helpful for scholars, company managers and\nprofessional investors and could be integrated into existing forecasting models\nto improve their accuracy. Lastly, we contribute to the research on word\nco-occurrence networks by extending their field of application.", "published": "2021-05-25 09:17:22", "link": "http://arxiv.org/abs/2105.11780v1", "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7; J.4; H.4.0"], "primary_category": "cs.CL"}
{"title": "Extending the Abstraction of Personality Types based on MBTI with\n  Machine Learning and Natural Language Processing", "abstract": "A data-centric approach with Natural Language Processing (NLP) to predict\npersonality types based on the MBTI (an introspective self-assessment\nquestionnaire that indicates different psychological preferences about how\npeople perceive the world and make decisions) through systematic enrichment of\ntext representation, based on the domain of the area, under the generation of\nfeatures based on three types of analysis: sentimental, grammatical and\naspects. The experimentation had a robust baseline of stacked models, with\npremature optimization of hyperparameters through grid search, with gradual\nfeedback, for each of the four classifiers (dichotomies) of MBTI. The results\nshowed that attention to the data iteration loop focused on quality,\nexplanatory power and representativeness for the abstraction of more\nrelevant/important resources for the studied phenomenon made it possible to\nimprove the evaluation metrics results more quickly and less costly than\ncomplex models such as the LSTM or state of the art ones as BERT, as well as\nthe importance of these results by comparisons made from various perspectives.\nIn addition, the study demonstrated a broad spectrum for the evolution and\ndeepening of the task and possible approaches for a greater extension of the\nabstraction of personality types.", "published": "2021-05-25 10:00:16", "link": "http://arxiv.org/abs/2105.11798v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "F.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "Big data and big values: When companies need to rethink themselves", "abstract": "In order to face the complexity of business environments and detect\npriorities while triggering contingency strategies, we propose a new\nmethodological approach that combines text mining, social network and big data\nanalytics, with the assessment of stakeholders' attitudes towards company core\nvalues. This approach was applied in a case study where we considered the\nTwitter discourse about core values in Italy. We collected more than 94,000\ntweets related to the core values of the firms listed in Fortune's ranking of\nthe World's Most Admired Companies (2013-2017). For the Italian scenario, we\nfound three predominant core values orientations (Customers, Employees and\nExcellence) - which should be at the basis of any business strategy - and three\nlatent ones (Economic-Financial Growth, Citizenship and Social Responsibility),\nwhich need periodic attention. Our contribution is mostly methodological and\nextends the research on text mining and on online big data analytics applied in\ncomplex business contexts.", "published": "2021-05-25 16:26:38", "link": "http://arxiv.org/abs/2105.12048v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph", "J.4; I.2.7; H.4.0"], "primary_category": "cs.SI"}
{"title": "VISITRON: Visual Semantics-Aligned Interactively Trained\n  Object-Navigator", "abstract": "Interactive robots navigating photo-realistic environments need to be trained\nto effectively leverage and handle the dynamic nature of dialogue in addition\nto the challenges underlying vision-and-language navigation (VLN). In this\npaper, we present VISITRON, a multi-modal Transformer-based navigator better\nsuited to the interactive regime inherent to Cooperative Vision-and-Dialog\nNavigation (CVDN). VISITRON is trained to: i) identify and associate\nobject-level concepts and semantics between the environment and dialogue\nhistory, ii) identify when to interact vs. navigate via imitation learning of a\nbinary classification head. We perform extensive pre-training and fine-tuning\nablations with VISITRON to gain empirical insights and improve performance on\nCVDN. VISITRON's ability to identify when to interact leads to a natural\ngeneralization of the game-play mode introduced by Roman et al.\n(arXiv:2005.00728) for enabling the use of such models in different\nenvironments. VISITRON is competitive with models on the static CVDN\nleaderboard and attains state-of-the-art performance on the Success weighted by\nPath Length (SPL) metric.", "published": "2021-05-25 00:21:54", "link": "http://arxiv.org/abs/2105.11589v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO", "I.2.9"], "primary_category": "cs.CV"}
{"title": "Deep Neural Networks and End-to-End Learning for Audio Compression", "abstract": "Recent achievements in end-to-end deep learning have encouraged the\nexploration of tasks dealing with highly structured data with unified deep\nnetwork models. Having such models for compressing audio signals has been\nchallenging since it requires discrete representations that are not easy to\ntrain with end-to-end backpropagation. In this paper, we present an end-to-end\ndeep learning approach that combines recurrent neural networks (RNNs) within\nthe training strategy of variational autoencoders (VAEs) with a binary\nrepresentation of the latent space. We apply a reparametrization trick for the\nBernoulli distribution for the discrete representations, which allows smooth\nbackpropagation. In addition, our approach allows the separation of the encoder\nand decoder, which is necessary for compression tasks. To our best knowledge,\nthis is the first end-to-end learning for a single audio compression model with\nRNNs, and our model achieves a Signal to Distortion Ratio (SDR) of 20.54.", "published": "2021-05-25 05:36:30", "link": "http://arxiv.org/abs/2105.11681v2", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "RNNoise-Ex: Hybrid Speech Enhancement System based on RNN and Spectral\n  Features", "abstract": "Recent interest in exploiting Deep Learning techniques for Noise Suppression,\nhas led to the creation of Hybrid Denoising Systems that combine classic Signal\nProcessing with Deep Learning. In this paper, we concentrated our efforts on\nextending the RNNoise denoising system (arXiv:1709.08243) with the inclusion of\ncomplementary features during the training phase. We present a comprehensive\nexplanation of the set-up process of a modified system and present the\ncomparative results derived from a performance evaluation analysis, using a\nreference version of RNNoise as control.", "published": "2021-05-25 10:32:08", "link": "http://arxiv.org/abs/2105.11813v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Modulation Front-End for Music Audio Tagging", "abstract": "Convolutional Neural Networks have been extensively explored in the task of\nautomatic music tagging. The problem can be approached by using either\nengineered time-frequency features or raw audio as input. Modulation filter\nbank representations that have been actively researched as a basis for timbre\nperception have the potential to facilitate the extraction of perceptually\nsalient features. We explore end-to-end learned front-ends for audio\nrepresentation learning, ModNet and SincModNet, that incorporate a temporal\nmodulation processing block. The structure is effectively analogous to a\nmodulation filter bank, where the FIR filter center frequencies are learned in\na data-driven manner. The expectation is that a perceptually motivated filter\nbank can provide a useful representation for identifying music features. Our\nexperimental results provide a fully visualisable and interpretable front-end\ntemporal modulation decomposition of raw audio. We evaluate the performance of\nour model against the state-of-the-art of music tagging on the MagnaTagATune\ndataset. We analyse the impact on performance for particular tags when\ntime-frequency bands are subsampled by the modulation filters at a\nprogressively reduced rate. We demonstrate that modulation filtering provides\npromising results for music tagging and feature representation, without using\nextensive musical domain knowledge in the design of this front-end.", "published": "2021-05-25 11:05:24", "link": "http://arxiv.org/abs/2105.11836v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spectrum Correction: Acoustic Scene Classification with Mismatched\n  Recording Devices", "abstract": "Machine learning algorithms, when trained on audio recordings from a limited\nset of devices, may not generalize well to samples recorded using other devices\nwith different frequency responses. In this work, a relatively straightforward\nmethod is introduced to address this problem. Two variants of the approach are\npresented. First requires aligned examples from multiple devices, the second\napproach alleviates this requirement. This method works for both time and\nfrequency domain representations of audio recordings. Further, a relation to\nstandardization and Cepstral Mean Subtraction is analysed. The proposed\napproach becomes effective even when very few examples are provided. This\nmethod was developed during the Detection and Classification of Acoustic Scenes\nand Events (DCASE) 2019 challenge and won the 1st place in the scenario with\nmis-matched recording devices with the accuracy of 75%. Source code for the\nexperiments can be found online.", "published": "2021-05-25 11:53:17", "link": "http://arxiv.org/abs/2105.11856v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "I.5.2"], "primary_category": "cs.SD"}
