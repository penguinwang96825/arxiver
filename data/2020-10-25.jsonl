{"title": "Towards Medical Knowmetrics: Representing and Computing Medical\n  Knowledge using Semantic Predications as the Knowledge Unit and the\n  Uncertainty as the Knowledge Context", "abstract": "In China, Prof. Hongzhou Zhao and Zeyuan Liu are the pioneers of the concept\n\"knowledge unit\" and \"knowmetrics\" for measuring knowledge. However, the\ndefinition of \"computable knowledge object\" remains controversial so far in\ndifferent fields. For example, it is defined as 1) quantitative scientific\nconcept in natural science and engineering, 2) knowledge point in the field of\neducation research, and 3) semantic predications, i.e.,\nSubject-Predicate-Object (SPO) triples in biomedical fields. The Semantic\nMEDLINE Database (SemMedDB), a high-quality public repository of SPO triples\nextracted from medical literature, provides a basic data infrastructure for\nmeasuring medical knowledge. In general, the study of extracting SPO triples as\ncomputable knowledge unit from unstructured scientific text has been\noverwhelmingly focusing on scientific knowledge per se. Since the SPO triples\nwould be possibly extracted from hypothetical, speculative statements or even\nconflicting and contradictory assertions, the knowledge status (i.e., the\nuncertainty), which serves as an integral and critical part of scientific\nknowledge has been largely overlooked. This article aims to put forward a\nframework for Medical Knowmetrics using the SPO triples as the knowledge unit\nand the uncertainty as the knowledge context. The lung cancer publications\ndataset is used to validate the proposed framework. The uncertainty of medical\nknowledge and how its status evolves over time indirectly reflect the strength\nof competing knowledge claims, and the probability of certainty for a given SPO\ntriple. We try to discuss the new insights using the uncertainty-centric\napproaches to detect research fronts, and identify knowledge claims with high\ncertainty level, in order to improve the efficacy of knowledge-driven decision\nsupport.", "published": "2020-10-25 04:27:43", "link": "http://arxiv.org/abs/2010.13031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuning ERNIE for chest abnormal imaging signs extraction", "abstract": "Chest imaging reports describe the results of chest radiography procedures.\nAutomatic extraction of abnormal imaging signs from chest imaging reports has a\npivotal role in clinical research and a wide range of downstream medical tasks.\nHowever, there are few studies on information extraction from Chinese chest\nimaging reports. In this paper, we formulate chest abnormal imaging sign\nextraction as a sequence tagging and matching problem. On this basis, we\npropose a transferred abnormal imaging signs extractor with pretrained ERNIE as\nthe backbone, named EASON (fine-tuning ERNIE with CRF for Abnormal Signs\nExtractiON), which can address the problem of data insufficiency. In addition,\nto assign the attributes (the body part and degree) to corresponding abnormal\nimaging signs from the results of the sequence tagging model, we design a\nsimple but effective tag2relation algorithm based on the nature of chest\nimaging report text. We evaluate our method on the corpus provided by a medical\nbig data company, and the experimental results demonstrate that our method\nachieves significant and consistent improvement compared to other baselines.", "published": "2020-10-25 05:18:14", "link": "http://arxiv.org/abs/2010.13040v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Commonsense knowledge adversarial dataset that challenges ELECTRA", "abstract": "Commonsense knowledge is critical in human reading comprehension. While\nmachine comprehension has made significant progress in recent years, the\nability in handling commonsense knowledge remains limited. Synonyms are one of\nthe most widely used commonsense knowledge. Constructing adversarial dataset is\nan important approach to find weak points of machine comprehension models and\nsupport the design of solutions. To investigate machine comprehension models'\nability in handling the commonsense knowledge, we created a Question and Answer\nDataset with common knowledge of Synonyms (QADS). QADS are questions generated\nbased on SQuAD 2.0 by applying commonsense knowledge of synonyms. The synonyms\nare extracted from WordNet. Words often have multiple meanings and synonyms. We\nused an enhanced Lesk algorithm to perform word sense disambiguation to\nidentify synonyms for the context. ELECTRA achieves the state-of-art result on\nthe SQuAD 2.0 dataset in 2019. With scale, ELECTRA can achieve similar\nperformance as BERT does. However, QADS shows that ELECTRA has little ability\nto handle commonsense knowledge of synonyms. In our experiment, ELECTRA-small\ncan achieve 70% accuracy on SQuAD 2.0, but only 20% on QADS. ELECTRA-large did\nnot perform much better. Its accuracy on SQuAD 2.0 is 88% but dropped\nsignificantly to 26% on QADS. In our earlier experiments, BERT, although also\nfailed badly on QADS, was not as bad as ELECTRA. The result shows that even\ntop-performing NLP models have little ability to handle commonsense knowledge\nwhich is essential in reading comprehension.", "published": "2020-10-25 07:17:45", "link": "http://arxiv.org/abs/2010.13049v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense\n  Knowledge", "abstract": "Understanding context-dependent variation in word meanings is a key aspect of\nhuman language comprehension supported by the lexicon. Lexicographic resources\n(e.g., WordNet) capture only some of this context-dependent variation; for\nexample, they often do not encode how closely senses, or discretized word\nmeanings, are related to one another. Our work investigates whether recent\nadvances in NLP, specifically contextualized word embeddings, capture\nhuman-like distinctions between English word senses, such as polysemy and\nhomonymy. We collect data from a behavioral, web-based experiment, in which\nparticipants provide judgments of the relatedness of multiple WordNet senses of\na word in a two-dimensional spatial arrangement task. We find that\nparticipants' judgments of the relatedness between senses are correlated with\ndistances between senses in the BERT embedding space. Homonymous senses (e.g.,\nbat as mammal vs. bat as sports equipment) are reliably more distant from one\nanother in the embedding space than polysemous ones (e.g., chicken as animal\nvs. chicken as meat). Our findings point towards the potential utility of\ncontinuous-space representations of sense meanings.", "published": "2020-10-25 07:56:52", "link": "http://arxiv.org/abs/2010.13057v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transgender Community Sentiment Analysis from Social Media Data: A\n  Natural Language Processing Approach", "abstract": "Transgender community is experiencing a huge disparity in mental health\nconditions compared with the general population. Interpreting the social medial\ndata posted by transgender people may help us understand the sentiments of\nthese sexual minority groups better and apply early interventions. In this\nstudy, we manually categorize 300 social media comments posted by transgender\npeople to the sentiment of negative, positive, and neutral. 5 machine learning\nalgorithms and 2 deep neural networks are adopted to build sentiment analysis\nclassifiers based on the annotated data. Results show that our annotations are\nreliable with a high Cohen's Kappa score over 0.8 across all three classes.\nLSTM model yields an optimal performance of accuracy over 0.85 and AUC of\n0.876. Our next step will focus on using advanced natural language processing\nalgorithms on a larger annotated dataset.", "published": "2020-10-25 08:13:34", "link": "http://arxiv.org/abs/2010.13062v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Autoencoding Improves Pre-trained Word Embeddings", "abstract": "Prior work investigating the geometry of pre-trained word embeddings have\nshown that word embeddings to be distributed in a narrow cone and by centering\nand projecting using principal component vectors one can increase the accuracy\nof a given set of pre-trained word embeddings. However, theoretically, this\npost-processing step is equivalent to applying a linear autoencoder to minimise\nthe squared l2 reconstruction error. This result contradicts prior work (Mu and\nViswanath, 2018) that proposed to remove the top principal components from\npre-trained embeddings. We experimentally verify our theoretical claims and\nshow that retaining the top principal components is indeed useful for improving\npre-trained word embeddings, without requiring access to additional linguistic\nresources or labelled data.", "published": "2020-10-25 11:30:05", "link": "http://arxiv.org/abs/2010.13094v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The LMU Munich System for the WMT 2020 Unsupervised Machine Translation\n  Shared Task", "abstract": "This paper describes the submission of LMU Munich to the WMT 2020\nunsupervised shared task, in two language directions, German<->Upper Sorbian.\nOur core unsupervised neural machine translation (UNMT) system follows the\nstrategy of Chronopoulou et al. (2020), using a monolingual pretrained language\ngeneration model (on German) and fine-tuning it on both German and Upper\nSorbian, before initializing a UNMT model, which is trained with online\nbacktranslation. Pseudo-parallel data obtained from an unsupervised statistical\nmachine translation (USMT) system is used to fine-tune the UNMT model. We also\napply BPE-Dropout to the low resource (Upper Sorbian) data to obtain a more\nrobust system. We additionally experiment with residual adapters and find them\nuseful in the Upper Sorbian->German direction. We explore sampling during\nbacktranslation and curriculum learning to use SMT translations in a more\nprincipled way. Finally, we ensemble our best-performing systems and reach a\nBLEU score of 32.4 on German->Upper Sorbian and 35.2 on Upper Sorbian->German.", "published": "2020-10-25 19:04:03", "link": "http://arxiv.org/abs/2010.13192v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Turn-level Dialog Evaluation with Dialog-level Weak Signals for\n  Bot-Human Hybrid Customer Service Systems", "abstract": "We developed a machine learning approach that quantifies multiple aspects of\nthe success or values in Customer Service contacts, at anytime during the\ninteraction. Specifically, the value/reward function regarding to the\nturn-level behaviors across human agents, chatbots and other hybrid dialog\nsystems is characterized by the incremental information and confidence gain\nbetween sentences, based on the token-level predictions from a multi-task\nneural network trained with only weak signals in dialog-level\nattributes/states. The resulting model, named Value Profiler, serves as a\ngoal-oriented dialog manager that enhances conversations by regulating\nautomated decisions with its reward and state predictions. It supports both\nreal-time monitoring and scalable offline customer experience evaluation, for\nboth bot- and human-handled contacts. We show how it improves Amazon customer\nservice quality in several applications.", "published": "2020-10-25 19:36:23", "link": "http://arxiv.org/abs/2011.06395v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discriminative Nearest Neighbor Few-Shot Intent Detection by\n  Transferring Natural Language Inference", "abstract": "Intent detection is one of the core components of goal-oriented dialog\nsystems, and detecting out-of-scope (OOS) intents is also a practically\nimportant skill. Few-shot learning is attracting much attention to mitigate\ndata scarcity, but OOS detection becomes even more challenging. In this paper,\nwe present a simple yet effective approach, discriminative nearest neighbor\nclassification with deep self-attention. Unlike softmax classifiers, we\nleverage BERT-style pairwise encoding to train a binary classifier that\nestimates the best matched training example for a user input. We propose to\nboost the discriminative ability by transferring a natural language inference\n(NLI) model. Our extensive experiments on a large-scale multi-domain intent\ndetection task show that our method achieves more stable and accurate in-domain\nand OOS detection accuracy than RoBERTa-based classifiers and embedding-based\nnearest neighbor approaches. More notably, the NLI transfer enables our 10-shot\nmodel to perform competitively with 50-shot or even full-shot classifiers,\nwhile we can keep the inference time constant by leveraging a faster embedding\nretrieval model.", "published": "2020-10-25 00:39:32", "link": "http://arxiv.org/abs/2010.13009v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fair Embedding Engine: A Library for Analyzing and Mitigating Gender\n  Bias in Word Embeddings", "abstract": "Non-contextual word embedding models have been shown to inherit human-like\nstereotypical biases of gender, race and religion from the training corpora. To\ncounter this issue, a large body of research has emerged which aims to mitigate\nthese biases while keeping the syntactic and semantic utility of embeddings\nintact. This paper describes Fair Embedding Engine (FEE), a library for\nanalysing and mitigating gender bias in word embeddings. FEE combines various\nstate of the art techniques for quantifying, visualising and mitigating gender\nbias in word embeddings under a standard abstraction. FEE will aid\npractitioners in fast track analysis of existing debiasing methods on their\nembedding models. Further, it will allow rapid prototyping of new methods by\nevaluating their performance on a suite of standard metrics.", "published": "2020-10-25 17:31:12", "link": "http://arxiv.org/abs/2010.13168v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "CRAB: Class Representation Attentive BERT for Hate Speech Identification\n  in Social Media", "abstract": "In recent years, social media platforms have hosted an explosion of hate\nspeech and objectionable content. The urgent need for effective automatic hate\nspeech detection models have drawn remarkable investment from companies and\nresearchers. Social media posts are generally short and their semantics could\ndrastically be altered by even a single token. Thus, it is crucial for this\ntask to learn context-aware input representations, and consider relevancy\nscores between input embeddings and class representations as an additional\nsignal. To accommodate these needs, this paper introduces CRAB (Class\nRepresentation Attentive BERT), a neural model for detecting hate speech in\nsocial media. The model benefits from two semantic representations: (i)\ntrainable token-wise and sentence-wise class representations, and (ii)\ncontextualized input embeddings from state-of-the-art BERT encoder. To\ninvestigate effectiveness of CRAB, we train our model on Twitter data and\ncompare it against strong baselines. Our results show that CRAB achieves 1.89%\nrelative improved Macro-averaged F1 over state-of-the-art baseline. The results\nof this research open an opportunity for the future research on automated\nabusive behavior detection in social media", "published": "2020-10-25 04:11:30", "link": "http://arxiv.org/abs/2010.13028v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Orthros: Non-autoregressive End-to-end Speech Translation with\n  Dual-decoder", "abstract": "Fast inference speed is an important goal towards real-world deployment of\nspeech translation (ST) systems. End-to-end (E2E) models based on the\nencoder-decoder architecture are more suitable for this goal than traditional\ncascaded systems, but their effectiveness regarding decoding speed has not been\nexplored so far. Inspired by recent progress in non-autoregressive (NAR)\nmethods in text-based translation, which generates target tokens in parallel by\neliminating conditional dependencies, we study the problem of NAR decoding for\nE2E-ST. We propose a novel NAR E2E-ST framework, Orthros, in which both NAR and\nautoregressive (AR) decoders are jointly trained on the shared speech encoder.\nThe latter is used for selecting better translation among various length\ncandidates generated from the former, which dramatically improves the\neffectiveness of a large length beam with negligible overhead. We further\ninvestigate effective length prediction methods from speech inputs and the\nimpact of vocabulary sizes. Experiments on four benchmarks show the\neffectiveness of the proposed method in improving inference speed while\nmaintaining competitive translation quality compared to state-of-the-art AR\nE2E-ST systems.", "published": "2020-10-25 06:35:30", "link": "http://arxiv.org/abs/2010.13047v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Two-stage Textual Knowledge Distillation for End-to-End Spoken Language\n  Understanding", "abstract": "End-to-end approaches open a new way for more accurate and efficient spoken\nlanguage understanding (SLU) systems by alleviating the drawbacks of\ntraditional pipeline systems. Previous works exploit textual information for an\nSLU model via pre-training with automatic speech recognition or fine-tuning\nwith knowledge distillation. To utilize textual information more effectively,\nthis work proposes a two-stage textual knowledge distillation method that\nmatches utterance-level representations and predicted logits of two modalities\nduring pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a\nspeech encoder because it captures general and rich features. Furthermore, we\nimprove the performance, especially in a low-resource scenario, with data\naugmentation methods by randomly masking spans of discrete audio tokens and\ncontextualized hidden representations. Consequently, we push the\nstate-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy\nin the full dataset setting and 99.5% in the 10% subset setting. Throughout the\nablation studies, we empirically verify that all used methods are crucial to\nthe final performance, providing the best practice for spoken language\nunderstanding. Code is available at https://github.com/clovaai/textual-kd-slu.", "published": "2020-10-25 12:36:05", "link": "http://arxiv.org/abs/2010.13105v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ExplanationLP: Abductive Reasoning for Explainable Science Question\n  Answering", "abstract": "We propose a novel approach for answering and explaining multiple-choice\nscience questions by reasoning on grounding and abstract inference chains. This\npaper frames question answering as an abductive reasoning problem, constructing\nplausible explanations for each choice and then selecting the candidate with\nthe best explanation as the final answer. Our system, ExplanationLP, elicits\nexplanations by constructing a weighted graph of relevant facts for each\ncandidate answer and extracting the facts that satisfy certain structural and\nsemantic constraints. To extract the explanations, we employ a linear\nprogramming formalism designed to select the optimal subgraph. The graphs'\nweighting function is composed of a set of parameters, which we fine-tune to\noptimize answer selection performance. We carry out our experiments on the\nWorldTree and ARC-Challenge corpus to empirically demonstrate the following\nconclusions: (1) Grounding-Abstract inference chains provides the semantic\ncontrol to perform explainable abductive reasoning (2) Efficiency and\nrobustness in learning with a fewer number of parameters by outperforming\ncontemporary explainable and transformer-based approaches in a similar setting\n(3) Generalisability by outperforming SOTA explainable approaches on general\nscience question sets.", "published": "2020-10-25 14:49:24", "link": "http://arxiv.org/abs/2010.13128v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Probing Acoustic Representations for Phonetic Properties", "abstract": "Pre-trained acoustic representations such as wav2vec and DeCoAR have attained\nimpressive word error rates (WER) for speech recognition benchmarks,\nparticularly when labeled data is limited. But little is known about what\nphonetic properties these various representations acquire, and how well they\nencode transferable features of speech. We compare features from two\nconventional and four pre-trained systems in some simple frame-level phonetic\nclassification tasks, with classifiers trained on features from one version of\nthe TIMIT dataset and tested on features from another. All contextualized\nrepresentations offered some level of transferability across domains, and\nmodels pre-trained on more audio data give better results; but overall, DeCoAR,\nthe system with the simplest architecture, performs best. This type of\nbenchmarking analysis can thus uncover relative strengths of various proposed\nacoustic representations.", "published": "2020-10-25 00:12:32", "link": "http://arxiv.org/abs/2010.13007v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Crowdsourcing approach for subjective evaluation of echo impairment", "abstract": "The quality of acoustic echo cancellers (AECs) in real-time communication\nsystems is typically evaluated using objective metrics like ERLE and PESQ, and\nless commonly with lab-based subjective tests like ITU-T Rec. P.831. We will\nshow that these objective measures are not well correlated to subjective\nmeasures. We then introduce an open-source crowdsourcing approach for\nsubjective evaluation of echo impairment which can be used to evaluate the\nperformance of AECs. We provide a study that shows this tool is accurate and\nhighly reproducible. This new tool has been recently used in the ICASSP 2021\nAEC Challenge which made the challenge possible to do quickly and cost\neffectively.", "published": "2020-10-25 08:17:18", "link": "http://arxiv.org/abs/2010.13063v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "An Improved Event-Independent Network for Polyphonic Sound Event\n  Localization and Detection", "abstract": "Polyphonic sound event localization and detection (SELD), which jointly\nperforms sound event detection (SED) and direction-of-arrival (DoA) estimation,\ndetects the type and occurrence time of sound events as well as their\ncorresponding DoA angles simultaneously. We study the SELD task from a\nmulti-task learning perspective. Two open problems are addressed in this paper.\nFirstly, to detect overlapping sound events of the same type but with different\nDoAs, we propose to use a trackwise output format and solve the accompanying\ntrack permutation problem with permutation-invariant training. Multi-head\nself-attention is further used to separate tracks. Secondly, a previous finding\nis that, by using hard parameter-sharing, SELD suffers from a performance loss\ncompared with learning the subtasks separately. This is solved by a soft\nparameter-sharing scheme. We term the proposed method as Event Independent\nNetwork V2 (EINV2), which is an improved version of our previously-proposed\nmethod and an end-to-end network for SELD. We show that our proposed EINV2 for\njoint SED and DoA estimation outperforms previous methods by a large margin,\nand has comparable performance to state-of-the-art ensemble models.", "published": "2020-10-25 11:27:22", "link": "http://arxiv.org/abs/2010.13092v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cascaded all-pass filters with randomized center frequencies and phase\n  polarity for acoustic and speech measurement and data augmentation", "abstract": "We introduce a new member of TSP (Time Stretched Pulse) for acoustic and\nspeech measurement infrastructure, based on a simple all-pass filter and\nsystematic randomization. This new infrastructure fundamentally upgrades our\nprevious measurement procedure, which enables simultaneous measurement of\nmultiple attributes, including non-linear ones without requiring extra\nfiltering nor post-processing. Our new proposal establishes a theoretically\nsolid, flexible, and extensible foundation in acoustic measurement. Moreover,\nit is general enough to provide versatile research tools for other fields, such\nas biological signal analysis. We illustrate using acoustic measurements and\ndata augmentation as representative examples among various prospective\napplications. We open-sourced MATLAB implementation. It consists of an\ninteractive and real-time acoustic tool, MATLAB functions, and supporting\nmaterials.", "published": "2020-10-25 18:35:14", "link": "http://arxiv.org/abs/2010.13185v2", "categories": ["cs.SD", "eess.AS", "68U06(Primary), 68T06, 68W06(Secondary)"], "primary_category": "cs.SD"}
{"title": "Subjective Evaluation of Noise Suppression Algorithms in Crowdsourcing", "abstract": "The quality of the speech communication systems, which include noise\nsuppression algorithms, are typically evaluated in laboratory experiments\naccording to the ITU-T Rec. P.835, in which participants rate background noise,\nspeech signal, and overall quality separately. This paper introduces an\nopen-source toolkit for conducting subjective quality evaluation of noise\nsuppressed speech in crowdsourcing. We followed the ITU-T Rec. P.835, and P.808\nand highly automate the process to prevent moderator's error. To assess the\nvalidity of our evaluation method, we compared the Mean Opinion Scores (MOS),\ncalculate using ratings collected with our implementation, and the MOS values\nfrom a standard laboratory experiment conducted according to the ITU-T Rec\nP.835. Results show a high validity in all three scales namely background\nnoise, speech signal and overall quality (average PCC = 0.961). Results of a\nround-robin test (N=5) showed that our implementation is also a highly\nreproducible evaluation method (PCC=0.99). Finally, we used our implementation\nin the INTERSPEECH 2021 Deep Noise Suppression Challenge as the primary\nevaluation metric, which demonstrates it is practical to use at scale. The\nresults are analyzed to determine why the overall performance was the best in\nterms of background noise and speech quality.", "published": "2020-10-25 19:24:27", "link": "http://arxiv.org/abs/2010.13200v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "IR-GAN: Room Impulse Response Generator for Far-field Speech Recognition", "abstract": "We present a Generative Adversarial Network (GAN) based room impulse response\ngenerator (IR-GAN) for generating realistic synthetic room impulse responses\n(RIRs). IR-GAN extracts acoustic parameters from captured real-world RIRs and\nuses these parameters to generate new synthetic RIRs. We use these generated\nsynthetic RIRs to improve far-field automatic speech recognition in new\nenvironments that are different from the ones used in training datasets. In\nparticular, we augment the far-field speech training set by convolving our\nsynthesized RIRs with a clean LibriSpeech dataset. We evaluate the quality of\nour synthetic RIRs on the real-world LibriSpeech test set created using\nreal-world RIRs from the BUT ReverbDB and AIR datasets. Our IR-GAN reports up\nto an 8.95% lower error rate than Geometric Acoustic Simulator (GAS) in\nfar-field speech recognition benchmarks. We further improve the performance\nwhen we combine our synthetic RIRs with synthetic impulse responses generated\nusing GAS. This combination can reduce the word error rate by up to 14.3% in\nfar-field speech recognition benchmarks.", "published": "2020-10-25 21:06:06", "link": "http://arxiv.org/abs/2010.13219v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An iterative framework for self-supervised deep speaker representation\n  learning", "abstract": "In this paper, we propose an iterative framework for self-supervised speaker\nrepresentation learning based on a deep neural network (DNN). The framework\nstarts with training a self-supervision speaker embedding network by maximizing\nagreement between different segments within an utterance via a contrastive\nloss. Taking advantage of DNN's ability to learn from data with label noise, we\npropose to cluster the speaker embedding obtained from the previous speaker\nnetwork and use the subsequent class assignments as pseudo labels to train a\nnew DNN. Moreover, we iteratively train the speaker network with pseudo labels\ngenerated from the previous step to bootstrap the discriminative power of a\nDNN. Speaker verification experiments are conducted on the VoxCeleb dataset.\nThe results show that our proposed iterative self-supervised learning framework\noutperformed previous works using self-supervision. The speaker network after 5\niterations obtains a 61% performance gain over the speaker embedding model\ntrained with contrastive loss.", "published": "2020-10-25 16:16:22", "link": "http://arxiv.org/abs/2010.14751v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enactive Mandala: Audio-visualizing Brain Waves", "abstract": "We are exploring the design and implementation of artificial expressions,\nkinetic audio-visual representations of real-time physiological data that\nreflect emotional and cognitive state. In this work, we demonstrate a\nprototype, the Enactive Mandala, which maps real-time EEG signals to modulate\nambient music and animated visual music. Transparent real-time audio-visual\nfeedback of brainwave qualities supports intuitive insight into the connection\nbetween thoughts and physiological states.", "published": "2020-10-25 04:48:47", "link": "http://arxiv.org/abs/2010.13035v1", "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS", "H.5.5; J.5"], "primary_category": "cs.HC"}
{"title": "Speakerfilter-Pro: an improved target speaker extractor combines the\n  time domain and frequency domain", "abstract": "This paper introduces an improved target speaker extractor, referred to as\nSpeakerfilter-Pro, based on our previous Speakerfilter model. The Speakerfilter\nuses a bi-direction gated recurrent unit (BGRU) module to characterize the\ntarget speaker from anchor speech and use a convolutional recurrent network\n(CRN) module to separate the target speech from a noisy signal.Different from\nthe Speakerfilter, the Speakerfilter-Pro sticks a WaveUNet module in the\nbeginning and the ending, respectively. The WaveUNet has been proven to have a\nbetter ability to perform speech separation in the time domain. In order to\nextract the target speaker information better, the complex spectrum instead of\nthe magnitude spectrum is utilized as the input feature for the CRN module.\nExperiments are conducted on the two-speaker dataset (WSJ0-mix2) which is\nwidely used for speaker extraction. The systematic evaluation shows that the\nSpeakerfilter-Pro outperforms the Speakerfilter and other baselines, and\nachieves a signal-to-distortion ratio (SDR) of 14.95 dB.", "published": "2020-10-25 07:30:30", "link": "http://arxiv.org/abs/2010.13053v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Attention is All You Need in Speech Separation", "abstract": "Recurrent Neural Networks (RNNs) have long been the dominant architecture in\nsequence-to-sequence learning. RNNs, however, are inherently sequential models\nthat do not allow parallelization of their computations. Transformers are\nemerging as a natural alternative to standard RNNs, replacing recurrent\ncomputations with a multi-head attention mechanism. In this paper, we propose\nthe SepFormer, a novel RNN-free Transformer-based neural network for speech\nseparation. The SepFormer learns short and long-term dependencies with a\nmulti-scale approach that employs transformers. The proposed model achieves\nstate-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It\nreaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on\nWSJ0-3mix. The SepFormer inherits the parallelization advantages of\nTransformers and achieves a competitive performance even when downsampling the\nencoded representation by a factor of 8. It is thus significantly faster and it\nis less memory-demanding than the latest speech separation systems with\ncomparable performance.", "published": "2020-10-25 16:28:54", "link": "http://arxiv.org/abs/2010.13154v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A \"DIY\" data acquisition system for acoustic field measurements under\n  harsh conditions", "abstract": "Monitoring active volcanos is an ongoing and important task helping to\nunderstand and predict volcanic eruptions. In recent years, analysing the\nacoustic properties of eruptions became more relevant. We present an\ninexpensive, lightweight, portable, easy to use and modular acoustic data\nacquisition system for field measurements that can record data with up to\n100~kHz. The system is based on a Raspberry Pi 3 B running a custom build bare\nmetal operating system. It connects to an external analog - digital converter\nwith the microphone sensor. A GPS receiver allows the logging of the position\nand in addition the recording of a very accurate time signal synchronously to\nthe acoustic data. With that, it is possible for multiple modules to\neffectively work as a single microphone array. The whole system can be build\nwith low cost and demands only minimal technical infrastructure. We demonstrate\na possible use of such a microphone array by deploying 20 modules on the active\nvolcano \\textit{Stromboli} in the Aeolian Islands by Sicily, Italy. We use the\ncollected acoustic data to indentify the sound source position for all recorded\neruptions.", "published": "2020-10-25 16:41:51", "link": "http://arxiv.org/abs/2010.13158v1", "categories": ["physics.geo-ph", "cs.SD", "eess.AS"], "primary_category": "physics.geo-ph"}
{"title": "Unified Gradient Reweighting for Model Biasing with Applications to\n  Source Separation", "abstract": "Recent deep learning approaches have shown great improvement in audio source\nseparation tasks. However, the vast majority of such work is focused on\nimproving average separation performance, often neglecting to examine or\ncontrol the distribution of the results. In this paper, we propose a simple,\nunified gradient reweighting scheme, with a lightweight modification to bias\nthe learning process of a model and steer it towards a certain distribution of\nresults. More specifically, we reweight the gradient updates of each batch,\nusing a user-specified probability distribution. We apply this method to\nvarious source separation tasks, in order to shift the operating point of the\nmodels towards different objectives. We demonstrate different parameterizations\nof our unified reweighting scheme can be used towards addressing several\nreal-world problems, such as unreliable separation estimates. Our framework\nenables the user to control a robustness trade-off between worst and average\nperformance. Moreover, we experimentally show that our unified reweighting\nscheme can also be used in order to shift the focus of the model towards being\nmore accurate for user-specified sound classes or even towards easier examples\nin order to enable faster convergence.", "published": "2020-10-25 21:41:45", "link": "http://arxiv.org/abs/2010.13228v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
