{"title": "Exclusive Hierarchical Decoding for Deep Keyphrase Generation", "abstract": "Keyphrase generation (KG) aims to summarize the main ideas of a document into\na set of keyphrases. A new setting is recently introduced into this problem, in\nwhich, given a document, the model needs to predict a set of keyphrases and\nsimultaneously determine the appropriate number of keyphrases to produce.\nPrevious work in this setting employs a sequential decoding process to generate\nkeyphrases. However, such a decoding method ignores the intrinsic hierarchical\ncompositionality existing in the keyphrase set of a document. Moreover,\nprevious work tends to generate duplicated keyphrases, which wastes time and\ncomputing resources. To overcome these limitations, we propose an exclusive\nhierarchical decoding framework that includes a hierarchical decoding process\nand either a soft or a hard exclusion mechanism. The hierarchical decoding\nprocess is to explicitly model the hierarchical compositionality of a keyphrase\nset. Both the soft and the hard exclusion mechanisms keep track of\npreviously-predicted keyphrases within a window size to enhance the diversity\nof the generated keyphrases. Extensive experiments on multiple KG benchmark\ndatasets demonstrate the effectiveness of our method to generate less\nduplicated and more accurate keyphrases.", "published": "2020-04-18 02:58:00", "link": "http://arxiv.org/abs/2004.08511v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep\n  Contextual Word Embeddings and Hierarchical Attention", "abstract": "The Web has become the main platform where people express their opinions\nabout entities of interest and their associated aspects. Aspect-Based Sentiment\nAnalysis (ABSA) aims to automatically compute the sentiment towards these\naspects from opinionated text. In this paper we extend the state-of-the-art\nHybrid Approach for Aspect-Based Sentiment Analysis (HAABSA) method in two\ndirections. First we replace the non-contextual word embeddings with deep\ncontextual word embeddings in order to better cope with the word semantics in a\ngiven text. Second, we use hierarchical attention by adding an extra attention\nlayer to the HAABSA high-level representations in order to increase the method\nflexibility in modeling the input data. Using two standard datasets (SemEval\n2015 and SemEval 2016) we show that the proposed extensions improve the\naccuracy of the built model for ABSA.", "published": "2020-04-18 17:54:55", "link": "http://arxiv.org/abs/2004.08673v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimAlign: High Quality Word Alignments without Parallel Training Data\n  using Static and Contextualized Embeddings", "abstract": "Word alignments are useful for tasks like statistical and neural machine\ntranslation (NMT) and cross-lingual annotation projection. Statistical word\naligners perform well, as do methods that extract alignments jointly with\ntranslations in NMT. However, most approaches require parallel training data,\nand quality decreases as less training data is available. We propose word\nalignment methods that require no parallel data. The key idea is to leverage\nmultilingual word embeddings, both static and contextualized, for word\nalignment. Our multilingual embeddings are created from monolingual data only\nwithout relying on any parallel data or dictionaries. We find that alignments\ncreated from embeddings are superior for four and comparable for two language\npairs compared to those produced by traditional statistical aligners, even with\nabundant parallel data; e.g., contextualized embeddings achieve a word\nalignment F1 for English-German that is 5 percentage points higher than\neflomal, a high-quality statistical aligner, trained on 100k parallel\nsentences.", "published": "2020-04-18 23:10:36", "link": "http://arxiv.org/abs/2004.08728v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Formal Hierarchy of RNN Architectures", "abstract": "We develop a formal hierarchy of the expressive capacity of RNN\narchitectures. The hierarchy is based on two formal properties: space\ncomplexity, which measures the RNN's memory, and rational recurrence, defined\nas whether the recurrent update can be described by a weighted finite-state\nmachine. We place several RNN variants within this hierarchy. For example, we\nprove the LSTM is not rational, which formally separates it from the related\nQRNN (Bradbury et al., 2016). We also show how these models' expressive\ncapacity is expanded by stacking multiple layers or composing them with\ndifferent pooling functions. Our results build on the theory of \"saturated\"\nRNNs (Merrill, 2019). While formally extending these findings to unsaturated\nRNNs is left to future work, we hypothesize that the practical learnable\ncapacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings\nfrom training unsaturated networks on formal languages support this conjecture.", "published": "2020-04-18 00:57:54", "link": "http://arxiv.org/abs/2004.08500v4", "categories": ["cs.CL", "cs.FL"], "primary_category": "cs.CL"}
{"title": "Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation", "abstract": "Question Generation (QG) is fundamentally a simple syntactic transformation;\nhowever, many aspects of semantics influence what questions are good to form.\nWe implement this observation by developing SynQG, a set of transparent\nsyntactic rules leveraging universal dependencies, shallow semantic parsing,\nlexical resources, and custom rules which transform declarative sentences into\nquestion-answer pairs. We utilize PropBank argument descriptions and VerbNet\nstate predicates to incorporate shallow semantic content, which helps generate\nquestions of a descriptive nature and produce inferential and semantically\nricher questions than existing systems. In order to improve syntactic fluency\nand eliminate grammatically incorrect questions, we employ back-translation\nover the output of these syntactic rules. A set of crowd-sourced evaluations\nshows that our system can generate a larger number of highly grammatical and\nrelevant questions than previous QG systems and that back-translation\ndrastically improves grammaticality at a slight cost of generating irrelevant\nquestions.", "published": "2020-04-18 19:57:39", "link": "http://arxiv.org/abs/2004.08694v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Effect of Text Color on Word Embeddings", "abstract": "In natural scenes and documents, we can find the correlation between a text\nand its color. For instance, the word, \"hot\", is often printed in red, while\n\"cold\" is often in blue. This correlation can be thought of as a feature that\nrepresents the semantic difference between the words. Based on this\nobservation, we propose the idea of using text color for word embeddings. While\ntext-only word embeddings (e.g. word2vec) have been extremely successful, they\noften represent antonyms as similar since they are often interchangeable in\nsentences. In this paper, we try two tasks to verify the usefulness of text\ncolor in understanding the meanings of words, especially in identifying\nsynonyms and antonyms. First, we quantify the color distribution of words from\nthe book cover images and analyze the correlation between the color and meaning\nof the word. Second, we try to retrain word embeddings with the color\ndistribution of words as a constraint. By observing the changes in the word\nembeddings of synonyms and antonyms before and after re-training, we aim to\nunderstand the kind of words that have positive or negative effects in their\nword embeddings when incorporating text color information.", "published": "2020-04-18 05:14:18", "link": "http://arxiv.org/abs/2004.08526v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Automatically Characterizing Targeted Information Operations Through\n  Biases Present in Discourse on Twitter", "abstract": "This paper considers the problem of automatically characterizing overall\nattitudes and biases that may be associated with emerging information\noperations via artificial intelligence. Accurate analysis of these emerging\ntopics usually requires laborious, manual analysis by experts to annotate\nmillions of tweets to identify biases in new topics. We introduce extensions of\nthe Word Embedding Association Test from Caliskan et al. to a new domain\n(Caliskan, 2017). Our practical and unsupervised method is used to quantify\nbiases promoted in information operations. We validate our method using known\ninformation operation-related tweets from Twitter's Transparency Report. We\nperform a case study on the COVID-19 pandemic to evaluate our method's\nperformance on non-labeled Twitter data, demonstrating its usability in\nemerging domains.", "published": "2020-04-18 23:03:14", "link": "http://arxiv.org/abs/2004.08726v3", "categories": ["cs.CY", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Enhancing Pharmacovigilance with Drug Reviews and Social Media", "abstract": "This paper explores whether the use of drug reviews and social media could be\nleveraged as potential alternative sources for pharmacovigilance of adverse\ndrug reactions (ADRs). We examined the performance of BERT alongside two\nvariants that are trained on biomedical papers, BioBERT7, and clinical notes,\nClinical BERT8. A variety of 8 different BERT models were fine-tuned and\ncompared across three different tasks in order to evaluate their relative\nperformance to one another in the ADR tasks. The tasks include sentiment\nclassification of drug reviews, presence of ADR in twitter postings, and named\nentity recognition of ADRs in twitter postings. BERT demonstrates its\nflexibility with high performance across all three different pharmacovigilance\nrelated tasks.", "published": "2020-04-18 23:35:24", "link": "http://arxiv.org/abs/2004.08731v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MatchboxNet: 1D Time-Channel Separable Convolutional Neural Network\n  Architecture for Speech Commands Recognition", "abstract": "We present an MatchboxNet - an end-to-end neural network for speech command\nrecognition. MatchboxNet is a deep residual network composed from blocks of 1D\ntime-channel separable convolution, batch-normalization, ReLU and dropout\nlayers. MatchboxNet reaches state-of-the-art accuracy on the Google Speech\nCommands dataset while having significantly fewer parameters than similar\nmodels. The small footprint of MatchboxNet makes it an attractive candidate for\ndevices with limited computational resources. The model is highly scalable, so\nmodel accuracy can be improved with modest additional memory and compute.\nFinally, we show how intensive data augmentation using an auxiliary noise\ndataset improves robustness in the presence of background noise.", "published": "2020-04-18 05:49:27", "link": "http://arxiv.org/abs/2004.08531v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
