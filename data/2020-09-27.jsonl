{"title": "Stylized Dialogue Response Generation Using Stylized Unpaired Texts", "abstract": "Generating stylized responses is essential to build intelligent and engaging\ndialogue systems. However, this task is far from well-explored due to the\ndifficulties of rendering a particular style in coherent responses, especially\nwhen the target style is embedded only in unpaired texts that cannot be\ndirectly used to train the dialogue model. This paper proposes a stylized\ndialogue generation method that can capture stylistic features embedded in\nunpaired texts. Specifically, our method can produce dialogue responses that\nare both coherent to the given context and conform to the target style. In this\nstudy, an inverse dialogue model is first introduced to predict possible posts\nfor the input responses, and then this inverse model is used to generate\nstylized pseudo dialogue pairs based on these stylized unpaired texts. Further,\nthese pseudo pairs are employed to train the stylized dialogue model with a\njoint training process, and a style routing approach is proposed to intensify\nstylistic features in the decoder. Automatic and manual evaluations on two\ndatasets demonstrate that our method outperforms competitive baselines in\nproducing coherent and style-intensive dialogue responses.", "published": "2020-09-27 01:04:06", "link": "http://arxiv.org/abs/2009.12719v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Brief Survey and Comparative Study of Recent Development of Pronoun\n  Coreference Resolution", "abstract": "Pronoun Coreference Resolution (PCR) is the task of resolving pronominal\nexpressions to all mentions they refer to. Compared with the general\ncoreference resolution task, the main challenge of PCR is the coreference\nrelation prediction rather than the mention detection. As one important natural\nlanguage understanding (NLU) component, pronoun resolution is crucial for many\ndownstream tasks and still challenging for existing models, which motivates us\nto survey existing approaches and think about how to do better. In this survey,\nwe first introduce representative datasets and models for the ordinary pronoun\ncoreference resolution task. Then we focus on recent progress on hard pronoun\ncoreference resolution problems (e.g., Winograd Schema Challenge) to analyze\nhow well current models can understand commonsense. We conduct extensive\nexperiments to show that even though current models are achieving good\nperformance on the standard evaluation set, they are still not ready to be used\nin real applications (e.g., all SOTA models struggle on correctly resolving\npronouns to infrequent objects). All experiment codes are available at\nhttps://github.com/HKUST-KnowComp/PCR.", "published": "2020-09-27 01:40:01", "link": "http://arxiv.org/abs/2009.12721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "abstract": "We propose a simple and efficient multi-hop dense retrieval approach for\nanswering complex open-domain questions, which achieves state-of-the-art\nperformance on two multi-hop datasets, HotpotQA and multi-evidence FEVER.\nContrary to previous work, our method does not require access to any\ncorpus-specific information, such as inter-document hyperlinks or\nhuman-annotated entity markers, and can be applied to any unstructured text\ncorpus. Our system also yields a much better efficiency-accuracy trade-off,\nmatching the best published accuracy on HotpotQA while being 10 times faster at\ninference time.", "published": "2020-09-27 06:12:29", "link": "http://arxiv.org/abs/2009.12756v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What does it mean to be language-agnostic? Probing multilingual sentence\n  encoders for typological properties", "abstract": "Multilingual sentence encoders have seen much success in cross-lingual model\ntransfer for downstream NLP tasks. Yet, we know relatively little about the\nproperties of individual languages or the general patterns of linguistic\nvariation that they encode. We propose methods for probing sentence\nrepresentations from state-of-the-art multilingual encoders (LASER, M-BERT, XLM\nand XLM-R) with respect to a range of typological properties pertaining to\nlexical, morphological and syntactic structure. In addition, we investigate how\nthis information is distributed across all layers of the models. Our results\nshow interesting differences in encoding linguistic variation associated with\ndifferent pretraining strategies.", "published": "2020-09-27 15:00:52", "link": "http://arxiv.org/abs/2009.12862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Pre-training for Biomedical Question Answering", "abstract": "We explore the suitability of unsupervised representation learning methods on\nbiomedical text -- BioBERT, SciBERT, and BioSentVec -- for biomedical question\nanswering. To further improve unsupervised representations for biomedical QA,\nwe introduce a new pre-training task from unlabeled data designed to reason\nabout biomedical entities in the context. Our pre-training method consists of\ncorrupting a given context by randomly replacing some mention of a biomedical\nentity with a random entity mention and then querying the model with the\ncorrect entity mention in order to locate the corrupted part of the context.\nThis de-noising task enables the model to learn good representations from\nabundant, unlabeled biomedical text that helps QA tasks and minimizes the\ntrain-test mismatch between the pre-training task and the downstream QA tasks\nby requiring the model to predict spans. Our experiments show that pre-training\nBioBERT on the proposed pre-training task significantly boosts performance and\noutperforms the previous best model from the 7th BioASQ Task 7b-Phase B\nchallenge.", "published": "2020-09-27 21:07:51", "link": "http://arxiv.org/abs/2009.12952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Local and non-local dependency learning and emergence of rule-like\n  representations in speech data by Deep Convolutional Generative Adversarial\n  Networks", "abstract": "This paper argues that training GANs on local and non-local dependencies in\nspeech data offers insights into how deep neural networks discretize continuous\ndata and how symbolic-like rule-based morphophonological processes emerge in a\ndeep convolutional architecture. Acquisition of speech has recently been\nmodeled as a dependency between latent space and data generated by GANs in\nBegu\\v{s} (2020b; arXiv:2006.03965), who models learning of a simple local\nallophonic distribution. We extend this approach to test learning of local and\nnon-local phonological processes that include approximations of morphological\nprocesses. We further parallel outputs of the model to results of a behavioral\nexperiment where human subjects are trained on the data used for training the\nGAN network. Four main conclusions emerge: (i) the networks provide useful\ninformation for computational models of speech acquisition even if trained on a\ncomparatively small dataset of an artificial grammar learning experiment; (ii)\nlocal processes are easier to learn than non-local processes, which matches\nboth behavioral data in human subjects and typology in the world's languages.\nThis paper also proposes (iii) how we can actively observe the network's\nprogress in learning and explore the effect of training steps on learning\nrepresentations by keeping latent space constant across different training\nsteps. Finally, this paper shows that (iv) the network learns to encode the\npresence of a prefix with a single latent variable; by interpolating this\nvariable, we can actively observe the operation of a non-local phonological\nprocess. The proposed technique for retrieving learning representations has\ngeneral implications for our understanding of how GANs discretize continuous\nspeech data and suggests that rule-like generalizations in the training data\nare represented as an interaction between variables in the network's latent\nspace.", "published": "2020-09-27 00:02:34", "link": "http://arxiv.org/abs/2009.12711v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-timescale Representation Learning in LSTM Language Models", "abstract": "Language models must capture statistical dependencies between words at\ntimescales ranging from very short to very long. Earlier work has demonstrated\nthat dependencies in natural language tend to decay with distance between words\naccording to a power law. However, it is unclear how this knowledge can be used\nfor analyzing or designing neural network language models. In this work, we\nderived a theory for how the memory gating mechanism in long short-term memory\n(LSTM) language models can capture power law decay. We found that unit\ntimescales within an LSTM, which are determined by the forget gate bias, should\nfollow an Inverse Gamma distribution. Experiments then showed that LSTM\nlanguage models trained on natural English text learn to approximate this\ntheoretical distribution. Further, we found that explicitly imposing the\ntheoretical distribution upon the model during training yielded better language\nmodel perplexity overall, with particular improvements for predicting\nlow-frequency (rare) words. Moreover, the explicit multi-timescale model\nselectively routes information about different types of words through units\nwith different timescales, potentially improving model interpretability. These\nresults demonstrate the importance of careful, theoretically-motivated analysis\nof memory and timescale in language models.", "published": "2020-09-27 02:13:38", "link": "http://arxiv.org/abs/2009.12727v2", "categories": ["cs.CL", "cs.LG", "91F20", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Modeling Topical Relevance for Multi-Turn Dialogue Generation", "abstract": "Topic drift is a common phenomenon in multi-turn dialogue. Therefore, an\nideal dialogue generation models should be able to capture the topic\ninformation of each context, detect the relevant context, and produce\nappropriate responses accordingly. However, existing models usually use word or\nsentence level similarities to detect the relevant contexts, which fail to well\ncapture the topical level relevance. In this paper, we propose a new model,\nnamed STAR-BTM, to tackle this problem. Firstly, the Biterm Topic Model is\npre-trained on the whole training dataset. Then, the topic level attention\nweights are computed based on the topic representation of each context.\nFinally, the attention weights and the topic distribution are utilized in the\ndecoding process to generate the corresponding responses. Experimental results\non both Chinese customer services data and English Ubuntu dialogue data show\nthat STAR-BTM significantly outperforms several state-of-the-art methods, in\nterms of both metric-based and human evaluations.", "published": "2020-09-27 03:33:22", "link": "http://arxiv.org/abs/2009.12735v1", "categories": ["cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inductively Representing Out-of-Knowledge-Graph Entities by Optimal\n  Estimation Under Translational Assumptions", "abstract": "Conventional Knowledge Graph Completion (KGC) assumes that all test entities\nappear during training. However, in real-world scenarios, Knowledge Graphs (KG)\nevolve fast with out-of-knowledge-graph (OOKG) entities added frequently, and\nwe need to represent these entities efficiently. Most existing Knowledge Graph\nEmbedding (KGE) methods cannot represent OOKG entities without costly\nretraining on the whole KG. To enhance efficiency, we propose a simple and\neffective method that inductively represents OOKG entities by their optimal\nestimation under translational assumptions. Given pretrained embeddings of the\nin-knowledge-graph (IKG) entities, our method needs no additional learning.\nExperimental results show that our method outperforms the state-of-the-art\nmethods with higher efficiency on two KGC tasks with OOKG entities.", "published": "2020-09-27 07:12:18", "link": "http://arxiv.org/abs/2009.12765v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hierarchical Deep Multi-modal Network for Medical Visual Question\n  Answering", "abstract": "Visual Question Answering in Medical domain (VQA-Med) plays an important role\nin providing medical assistance to the end-users. These users are expected to\nraise either a straightforward question with a Yes/No answer or a challenging\nquestion that requires a detailed and descriptive answer. The existing\ntechniques in VQA-Med fail to distinguish between the different question types\nsometimes complicates the simpler problems, or over-simplifies the complicated\nones. It is certainly true that for different question types, several distinct\nsystems can lead to confusion and discomfort for the end-users. To address this\nissue, we propose a hierarchical deep multi-modal network that analyzes and\nclassifies end-user questions/queries and then incorporates a query-specific\napproach for answer prediction. We refer our proposed approach as Hierarchical\nQuestion Segregation based Visual Question Answering, in short HQS-VQA. Our\ncontributions are three-fold, viz. firstly, we propose a question segregation\n(QS) technique for VQAMed; secondly, we integrate the QS model to the\nhierarchical deep multi-modal neural network to generate proper answers to the\nqueries related to medical images; and thirdly, we study the impact of QS in\nMedical-VQA by comparing the performance of the proposed model with QS and a\nmodel without QS. We evaluate the performance of our proposed model on two\nbenchmark datasets, viz. RAD and CLEF18. Experimental results show that our\nproposed HQS-VQA technique outperforms the baseline models with significant\nmargins. We also conduct a detailed quantitative and qualitative analysis of\nthe obtained results and discover potential causes of errors and their\nsolutions.", "published": "2020-09-27 07:24:41", "link": "http://arxiv.org/abs/2009.12770v1", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT", "abstract": "Transformer-based pre-training models like BERT have achieved remarkable\nperformance in many natural language processing tasks.However, these models are\nboth computation and memory expensive, hindering their deployment to\nresource-constrained devices. In this work, we propose TernaryBERT, which\nternarizes the weights in a fine-tuned BERT model. Specifically, we use both\napproximation-based and loss-aware ternarization methods and empirically\ninvestigate the ternarization granularity of different parts of BERT. Moreover,\nto reduce the accuracy degradation caused by the lower capacity of low bits, we\nleverage the knowledge distillation technique in the training process.\nExperiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT\noutperforms the other BERT quantization methods, and even achieves comparable\nperformance as the full-precision model while being 14.9x smaller.", "published": "2020-09-27 10:17:28", "link": "http://arxiv.org/abs/2009.12812v3", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
