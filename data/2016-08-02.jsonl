{"title": "New word analogy corpus for exploring embeddings of Czech words", "abstract": "The word embedding methods have been proven to be very useful in many tasks\nof NLP (Natural Language Processing). Much has been investigated about word\nembeddings of English words and phrases, but only little attention has been\ndedicated to other languages.\n  Our goal in this paper is to explore the behavior of state-of-the-art word\nembedding methods on Czech, the language that is characterized by very rich\nmorphology. We introduce new corpus for word analogy task that inspects\nsyntactic, morphosyntactic and semantic properties of Czech words and phrases.\nWe experiment with Word2Vec and GloVe algorithms and discuss the results on\nthis corpus. The corpus is available for the research community.", "published": "2016-08-02 12:31:06", "link": "http://arxiv.org/abs/1608.00789v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Representations of Word Senses and Concepts", "abstract": "Representing the semantics of linguistic items in a machine-interpretable\nform has been a major goal of Natural Language Processing since its earliest\ndays. Among the range of different linguistic items, words have attracted the\nmost research attention. However, word representations have an important\nlimitation: they conflate different meanings of a word into a single vector.\nRepresentations of word senses have the potential to overcome this inherent\nlimitation. Indeed, the representation of individual word senses and concepts\nhas recently gained in popularity with several experimental results showing\nthat a considerable performance improvement can be achieved across different\nNLP applications upon moving from word level to the deeper sense and concept\nlevels. Another interesting point regarding the representation of concepts and\nword senses is that these models can be seamlessly applied to other linguistic\nitems, such as words, phrases and sentences.", "published": "2016-08-02 14:35:31", "link": "http://arxiv.org/abs/1608.00841v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity", "abstract": "Verbs play a critical role in the meaning of sentences, but these ubiquitous\nwords have received little attention in recent distributional semantics\nresearch. We introduce SimVerb-3500, an evaluation resource that provides human\nratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed\nverb types from the USF free-association database, providing at least three\nexamples for every VerbNet class. This broad coverage facilitates detailed\nanalyses of how syntactic and semantic phenomena together influence human\nunderstanding of verb meaning. Further, with significantly larger development\nand test sets than existing benchmarks, SimVerb-3500 enables more robust\nevaluation of representation learning architectures and promotes the\ndevelopment of methods tailored to verbs. We hope that SimVerb-3500 will enable\na richer understanding of the diversity and complexity of verb semantics and\nguide the development of systems that can effectively represent and interpret\nthis meaning.", "published": "2016-08-02 15:35:12", "link": "http://arxiv.org/abs/1608.00869v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation for Small-footprint Highway Networks", "abstract": "Deep learning has significantly advanced state-of-the-art of speech\nrecognition in the past few years. However, compared to conventional Gaussian\nmixture acoustic models, neural network models are usually much larger, and are\ntherefore not very deployable in embedded devices. Previously, we investigated\na compact highway deep neural network (HDNN) for acoustic modelling, which is a\ntype of depth-gated feedforward neural network. We have shown that HDNN-based\nacoustic models can achieve comparable recognition accuracy with much smaller\nnumber of model parameters compared to plain deep neural network (DNN) acoustic\nmodels. In this paper, we push the boundary further by leveraging on the\nknowledge distillation technique that is also known as {\\it teacher-student}\ntraining, i.e., we train the compact HDNN model with the supervision of a high\naccuracy cumbersome model. Furthermore, we also investigate sequence training\nand adaptation in the context of teacher-student training. Our experiments were\nperformed on the AMI meeting speech recognition corpus. With this technique, we\nsignificantly improved the recognition accuracy of the HDNN acoustic model with\nless than 0.8 million parameters, and narrowed the gap between this model and\nthe plain DNN with 30 million parameters.", "published": "2016-08-02 16:39:10", "link": "http://arxiv.org/abs/1608.00892v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Segmental Cascades for Speech Recognition", "abstract": "Discriminative segmental models offer a way to incorporate flexible feature\nfunctions into speech recognition. However, their appeal has been limited by\ntheir computational requirements, due to the large number of possible segments\nto consider. Multi-pass cascades of segmental models introduce features of\nincreasing complexity in different passes, where in each pass a segmental model\nrescores lattices produced by a previous (simpler) segmental model. In this\npaper, we explore several ways of making segmental cascades efficient and\npractical: reducing the feature set in the first pass, frame subsampling, and\nvarious pruning approaches. In experiments on phonetic recognition, we find\nthat with a combination of such techniques, it is possible to maintain\ncompetitive performance while greatly reducing decoding, pruning, and training\ntime.", "published": "2016-08-02 18:45:53", "link": "http://arxiv.org/abs/1608.00929v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolutionary forces in language change", "abstract": "Languages and genes are both transmitted from generation to generation, with\nopportunity for differential reproduction and survivorship of forms. Here we\napply a rigorous inference framework, drawn from population genetics, to\ndistinguish between two broad mechanisms of language change: drift and\nselection. Drift is change that results from stochasticity in transmission and\nit may occur in the absence of any intrinsic difference between linguistic\nforms; whereas selection is truly an evolutionary force arising from intrinsic\ndifferences -- for example, when one form is preferred by members of the\npopulation. Using large corpora of parsed texts spanning the 12th century to\nthe 21st century, we analyze three examples of grammatical changes in English:\nthe regularization of past-tense verbs, the rise of the periphrastic `do', and\nsyntactic variation in verbal negation. We show that we can reject stochastic\ndrift in favor of a selective force driving some of these language changes, but\nnot others. The strength of drift depends on a word's frequency, and so drift\nprovides an alternative explanation for why some words are more prone to change\nthan others. Our results suggest an important role for stochasticity in\nlanguage change, and they provide a null model against which selective theories\nof language evolution must be compared.", "published": "2016-08-02 19:04:58", "link": "http://arxiv.org/abs/1608.00938v1", "categories": ["q-bio.PE", "cs.CL"], "primary_category": "q-bio.PE"}
{"title": "RETURNN: The RWTH Extensible Training framework for Universal Recurrent\n  Neural Networks", "abstract": "In this work we release our extensible and easily configurable neural network\ntraining software. It provides a rich set of functional layers with a\nparticular focus on efficient training of recurrent neural network topologies\non multiple GPUs. The source of the software package is public and freely\navailable for academic research purposes and can be used as a framework or as a\nstandalone tool which supports a flexible configuration. The software allows to\ntrain state-of-the-art deep bidirectional long short-term memory (LSTM) models\non both one dimensional data like speech or two dimensional data like\nhandwritten text and was used to develop successful submission systems in\nseveral evaluation campaigns.", "published": "2016-08-02 16:43:27", "link": "http://arxiv.org/abs/1608.00895v2", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Proceedings of the 2016 Workshop on Semantic Spaces at the Intersection\n  of NLP, Physics and Cognitive Science", "abstract": "This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces\nat the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which\nwas held on the 11th of June at the University of Strathclyde, Glasgow, and was\nco-located with Quantum Physics and Logic (QPL 2016). Exploiting the common\nground provided by the concept of a vector space, the workshop brought together\nresearchers working at the intersection of Natural Language Processing (NLP),\ncognitive science, and physics, offering them an appropriate forum for\npresenting their uniquely motivated work and ideas. The interplay between these\nthree disciplines inspired theoretically motivated approaches to the\nunderstanding of how word meanings interact with each other in sentences and\ndiscourse, how diagrammatic reasoning depicts and simplifies this interaction,\nhow language models are determined by input from the world, and how word and\nsentence meanings interact logically. This first edition of the workshop\nconsisted of three invited talks from distinguished speakers (Hans Briegel,\nPeter G\\\"ardenfors, Dominic Widdows) and eight presentations of selected\ncontributed papers. Each submission was refereed by at least three members of\nthe Programme Committee, who delivered detailed and insightful comments and\nsuggestions.", "published": "2016-08-02 22:28:45", "link": "http://arxiv.org/abs/1608.01018v1", "categories": ["cs.CL", "cs.AI", "math.CT", "quant-ph"], "primary_category": "cs.CL"}
