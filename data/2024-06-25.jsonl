{"title": "Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence\n  Coverage", "abstract": "News media structure their reporting of events or issues using certain\nperspectives.\n  When describing an incident involving gun violence, for example, some\njournalists may focus on mental health or gun regulation, while others may\nemphasize the discussion of gun rights. Such perspectives are called\n\\say{frames} in communication research. We study, for the first time, the value\nof combining lead images and their contextual information with text to identify\nthe frame of a given news article. We observe that using multiple modes of\ninformation(article- and image-derived features) improves prediction of news\nframes over any single mode of information when the images are relevant to the\nframes of the headlines. We also observe that frame image relevance is related\nto the ease of conveying frames via images, which we call frame concreteness.\nAdditionally, we release the first multimodal news framing dataset related to\ngun violence in the U.S., curated and annotated by communication researchers.\nThe dataset will allow researchers to further examine the use of multiple\ninformation modalities for studying media framing.", "published": "2024-06-25 01:56:47", "link": "http://arxiv.org/abs/2406.17213v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CogMG: Collaborative Augmentation Between Large Language Model and\n  Knowledge Graph", "abstract": "Large language models have become integral to question-answering applications\ndespite their propensity for generating hallucinations and factually inaccurate\ncontent. Querying knowledge graphs to reduce hallucinations in LLM meets the\nchallenge of incomplete knowledge coverage in knowledge graphs. On the other\nhand, updating knowledge graphs by information extraction and knowledge graph\ncompletion faces the knowledge update misalignment issue. In this work, we\nintroduce a collaborative augmentation framework, CogMG, leveraging knowledge\ngraphs to address the limitations of LLMs in QA scenarios, explicitly targeting\nthe problems of incomplete knowledge coverage and knowledge update\nmisalignment. The LLMs identify and decompose required knowledge triples that\nare not present in the KG, enriching them and aligning updates with real-world\ndemands. We demonstrate the efficacy of this approach through a supervised\nfine-tuned LLM within an agent framework, showing significant improvements in\nreducing hallucinations and enhancing factual accuracy in QA responses. Our\ncode and video are publicly available.", "published": "2024-06-25 02:37:12", "link": "http://arxiv.org/abs/2406.17231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human\n  Belief Networks", "abstract": "Creating human-like large language model (LLM) agents is crucial for faithful\nsocial simulation. Having LLMs role-play based on demographic information\nsometimes improves human likeness but often does not. This study assessed\nwhether LLM alignment with human behavior can be improved by integrating\ninformation from empirically-derived human belief networks. Using data from a\nhuman survey, we estimated a belief network encompassing 64 topics loading on\nnine non-overlapping latent factors. We then seeded LLM-based agents with an\nopinion on one topic, and assessed the alignment of its expressed opinions on\nremaining test topics with corresponding human data. Role-playing based on\ndemographic information alone did not align LLM and human opinions, but seeding\nthe agent with a single belief greatly improved alignment for topics related in\nthe belief network, and not for topics outside the network. These results\nsuggest a novel path for human-LLM belief alignment in work seeking to simulate\nand understand patterns of belief distributions in society.", "published": "2024-06-25 02:37:29", "link": "http://arxiv.org/abs/2406.17232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding Language Model Circuits through Knowledge Editing", "abstract": "Recent advances in language model interpretability have identified circuits,\ncritical subnetworks that replicate model behaviors, yet how knowledge is\nstructured within these crucial subnetworks remains opaque. To gain an\nunderstanding toward the knowledge in the circuits, we conduct systematic\nknowledge editing experiments on the circuits of the GPT-2 language model. Our\nanalysis reveals intriguing patterns in how circuits respond to editing\nattempts, the extent of knowledge distribution across network components, and\nthe architectural composition of knowledge-bearing circuits. These findings\noffer insights into the complex relationship between model circuits and\nknowledge representation, deepening the understanding of how information is\norganized within language models. Our findings offer novel insights into the\n``meanings'' of the circuits, and introduce directions for further\ninterpretability and safety research of language models.", "published": "2024-06-25 03:09:53", "link": "http://arxiv.org/abs/2406.17241v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nupdating their knowledge post-training remains a critical challenge. While\nrecent model editing techniques like Rank-One Model Editing (ROME) show\npromise, their effectiveness may vary based on the nature of the knowledge\nbeing edited. We introduce the concept of ``perplexingness'': the degree to\nwhich new knowledge conflicts with an LLM's learned conceptual hierarchies and\ncategorical relationships. For instance, editing ``British Shorthair is a kind\nof cat'' to ``British Shorthair is a kind of dog'' represents a\nlow-perplexingness edit within the same taxonomic level, while editing ``A cat\nis a kind of animal'' to ``A cat is a kind of plant'' represents a\nhigh-perplexingness edit that violates fundamental categorical boundaries. To\nsystematically investigate this phenomenon, we introduce HierarchyData, a\ncarefully curated dataset of 99 hyponym-hypernym pairs across diverse\ncategories. Through controlled experiments across three models and four editing\nmethods, we demonstrate a strong negative correlation between the\nperplexingness of new knowledge and the effectiveness of knowledge editing. Our\nanalysis reveals that edits involving more abstract concepts (hypernyms)\ngenerally exhibit higher perplexingness and are more resistant to modification\nthan their specific counterparts (hyponyms). These findings highlight a\nfundamental challenge in LLM knowledge editing: the more a new fact contradicts\nan LLM's learned conceptual hierarchies, the harder it becomes to reliably\nencode that knowledge.", "published": "2024-06-25 03:41:02", "link": "http://arxiv.org/abs/2406.17253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MPCODER: Multi-user Personalized Code Generator with Explicit and\n  Implicit Style Representation Learning", "abstract": "Large Language Models (LLMs) have demonstrated great potential for assisting\ndevelopers in their daily development. However, most research focuses on\ngenerating correct code, how to use LLMs to generate personalized code has\nseldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user\nPersonalized Code Generator) to generate personalized code for multiple users.\nTo better learn coding style features, we utilize explicit coding style\nresidual learning to capture the syntax code style standards and implicit style\nlearning to capture the semantic code style conventions. We train a multi-user\nstyle adapter to better differentiate the implicit feature representations of\ndifferent users through contrastive learning, ultimately enabling personalized\ncode generation for multiple users. We further propose a novel evaluation\nmetric for estimating similarities between codes of different coding styles.\nThe experimental results show the effectiveness of our approach for this novel\ntask.", "published": "2024-06-25 03:45:28", "link": "http://arxiv.org/abs/2406.17255v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Hallucination in Fictional Character Role-Play", "abstract": "Role-playing has wide-ranging applications in customer support, embodied\nagents, and computational social science. The influence of parametric world\nknowledge of large language models (LLMs) often causes role-playing characters\nto act out of character and to hallucinate about things outside the scope of\ntheir knowledge. In this work, we focus on the evaluation and mitigation of\nhallucination in fictional character role-play. We introduce a dataset with\nover 2,000 characters and 72,000 interviews, including 18,000 adversarial\nquestions. We propose RoleFact, a role-playing method that mitigates\nhallucination by modulating the influence of parametric knowledge using a\npre-calibrated confidence threshold. Experiments show that the proposed method\nimproves the factual precision of generated responses by 18% for adversarial\nquestions with a 44% reduction in temporal hallucination for time-sensitive\ninterviews. The code and the dataset are available at\nhttps://github.com/NafisSadeq/rolefact.git.", "published": "2024-06-25 03:56:33", "link": "http://arxiv.org/abs/2406.17260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRAWL: Tensor Reduced and Approximated Weights for Large Language Models", "abstract": "Recent research has shown that pruning large-scale language models for\ninference is an effective approach to improving model efficiency, significantly\nreducing model weights with minimal impact on performance. Interestingly,\npruning can sometimes even enhance accuracy by removing noise that accumulates\nduring training, particularly through matrix decompositions. However, recent\nwork has primarily focused on single matrix decompositions or lower precision\ntechniques, which may fail to fully capture structural patterns. To address\nthese limitations, we introduce TRAWL (Tensor Reduced and Approximated Weights\nfor Large Language Models), a technique that applies tensor decomposition\nacross multiple weight matrices to effectively denoise LLMs by capturing global\nstructural patterns. Our experiments show that TRAWL improves model performance\nby up to 16% over baseline models on benchmark datasets, without requiring\nadditional data, training, or fine-tuning.", "published": "2024-06-25 04:01:32", "link": "http://arxiv.org/abs/2406.17261v3", "categories": ["cs.CL", "68T50 (Primary), 65F55 (Secondary)", "I.2.7"], "primary_category": "cs.CL"}
{"title": "D2LLM: Decomposed and Distilled Large Language Models for Semantic\n  Search", "abstract": "The key challenge in semantic search is to create models that are both\naccurate and efficient in pinpointing relevant sentences for queries. While\nBERT-style bi-encoders excel in efficiency with pre-computed embeddings, they\noften miss subtle nuances in search tasks. Conversely, GPT-style LLMs with\ncross-encoder designs capture these nuances but are computationally intensive,\nhindering real-time applications. In this paper, we present D2LLMs-Decomposed\nand Distilled LLMs for semantic search-that combines the best of both worlds.\nWe decompose a cross-encoder into an efficient bi-encoder integrated with\nPooling by Multihead Attention and an Interaction Emulation Module, achieving\nnuanced understanding and pre-computability. Knowledge from the LLM is\ndistilled into this model using contrastive, rank, and feature imitation\ntechniques. Our experiments show that D2LLM surpasses five leading baselines in\nterms of all metrics across three tasks, particularly improving NLI task\nperformance by at least 6.45%. The source code is available at\nhttps://github.com/codefuse-ai/D2LLM.", "published": "2024-06-25 04:03:04", "link": "http://arxiv.org/abs/2406.17262v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning\n  Graph", "abstract": "The current paradigm of evaluating Large Language Models (LLMs) through\nstatic benchmarks comes with significant limitations, such as vulnerability to\ndata contamination and a lack of adaptability to the evolving capabilities of\nLLMs. Therefore, evaluation methods that can adapt and generate evaluation data\nwith controlled complexity are urgently needed. In this work, we introduce\nDynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to\ndynamically extend current benchmarks with controlled complexity and diversity.\nSpecifically, we first extract the reasoning graphs of data points in current\nbenchmarks and then perturb the reasoning graphs to generate novel testing\ndata. Such newly generated test samples can have different levels of complexity\nwhile maintaining linguistic diversity similar to the original benchmarks. We\nfurther use a code-augmented LLM to ensure the label correctness of newly\ngenerated data. We apply our DARG framework to diverse reasoning tasks in four\ndomains with 15 state-of-the-art LLMs. Experimental results show that almost\nall LLMs experience a performance decrease with increased complexity and\ncertain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit\nmore biases when being evaluated via the data generated by DARG with higher\ncomplexity levels. These observations provide useful insights into how to\ndynamically and adaptively evaluate LLMs. The code is available at\nhttps://github.com/SALT-NLP/DARG.", "published": "2024-06-25 04:27:53", "link": "http://arxiv.org/abs/2406.17271v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure", "abstract": "Autoregressive language models demonstrate excellent performance in various\nscenarios. However, the inference efficiency is limited by its\none-step-one-word generation mode, which has become a pressing problem recently\nas the models become increasingly larger. Speculative decoding employs a \"draft\nand then verify\" mechanism to allow multiple tokens to be generated in one\nstep, realizing lossless acceleration. Existing methods mainly adopt fixed\nheuristic draft structures, which fail to adapt to different situations to\nmaximize the acceptance length during verification. To alleviate this dilemma,\nwe proposed OPT-Tree, an algorithm to construct adaptive and scalable draft\ntrees. It searches the optimal tree structure that maximizes the mathematical\nexpectation of the acceptance length in each decoding step. Experimental\nresults reveal that OPT-Tree outperforms the existing draft structures and\nachieves a speed-up ratio of up to 3.2 compared with autoregressive decoding.\nIf the draft model is powerful enough and the node budget is sufficient, it can\ngenerate more than ten tokens in a single step. Our code is available at\nhttps://github.com/Jikai0Wang/OPT-Tree.", "published": "2024-06-25 04:45:53", "link": "http://arxiv.org/abs/2406.17276v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SetBERT: Enhancing Retrieval Performance for Boolean Logic and Set\n  Operation Queries", "abstract": "We introduce SetBERT, a fine-tuned BERT-based model designed to enhance query\nembeddings for set operations and Boolean logic queries, such as Intersection\n(AND), Difference (NOT), and Union (OR). SetBERT significantly improves\nretrieval performance for logic-structured queries, an area where both\ntraditional and neural retrieval methods typically underperform. We propose an\ninnovative use of inversed-contrastive loss, focusing on identifying the\nnegative sentence, and fine-tuning BERT with a dataset generated via prompt\nGPT. Furthermore, we demonstrate that, unlike other BERT-based models,\nfine-tuning with triplet loss actually degrades performance for this specific\ntask. Our experiments reveal that SetBERT-base not only significantly\noutperforms BERT-base (up to a 63% improvement in Recall) but also achieves\nperformance comparable to the much larger BERT-large model, despite being only\none-third the size.", "published": "2024-06-25 05:14:54", "link": "http://arxiv.org/abs/2406.17282v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Recursive Encoding for Cuneiform Signs", "abstract": "One of the most significant problems in cuneiform pedagogy is the process of\nlooking up unknown signs, which often involves a tedious page-by-page search\nthrough a sign list. This paper proposes a new \"recursive encoding\" for signs,\nwhich represents the arrangement of strokes in a way a computer can process. A\nseries of new algorithms then offers students a new way to look up signs by any\ndistinctive component, as well as providing new ways to render signs and\ntablets electronically.", "published": "2024-06-25 05:18:12", "link": "http://arxiv.org/abs/2406.17283v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large\n  Language Models", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning\ncapabilities, particularly in textual mathematical problem-solving. However,\nexisting open-source image instruction fine-tuning datasets, containing limited\nquestion-answer pairs per image, do not fully exploit visual information to\nenhance the multimodal mathematical reasoning capabilities of Multimodal LLMs\n(MLLMs). To bridge this gap, we address the lack of high-quality, diverse\nmultimodal mathematical datasets by collecting 40K high-quality images with\nquestion-answer pairs from 24 existing datasets and synthesizing 320K new\npairs, creating the MathV360K dataset, which enhances both the breadth and\ndepth of multimodal mathematical questions. We introduce Math-LLaVA, a\nLLaVA-1.5-based model fine-tuned with MathV360K. This novel approach\nsignificantly improves the multimodal mathematical reasoning capabilities of\nLLaVA-1.5, achieving a 19-point increase and comparable performance to GPT-4V\non MathVista's minitest split, and yielding leading performance on Math-V and\nMathVerse. Furthermore, Math-LLaVA demonstrates enhanced generalizability,\nshowing substantial improvements on the MMMU benchmark. Our research highlights\nthe importance of dataset diversity and synthesis in advancing MLLMs'\nmathematical reasoning abilities. The code and data are available at:\n\\url{https://github.com/HZQ950419/Math-LLaVA}.", "published": "2024-06-25 05:43:21", "link": "http://arxiv.org/abs/2406.17294v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CausalScore: An Automatic Reference-Free Metric for Assessing Response\n  Relevance in Open-Domain Dialogue Systems", "abstract": "Automatically evaluating the quality of responses in open-domain dialogue\nsystems is a challenging but crucial task. Current evaluation metrics often\nfail to align with human judgments, especially when assessing responses that\nare grammatically correct. To address this issue, we propose a novel metric,\ncalled CausalScore, which assesses the relevance of responses by measuring the\ncausal strength between dialogue histories and responses. The causal strength\nis estimated by utilizing both unconditional dependence and conditional\ndependencies from the dialogue history to responses. We compare our metric with\nthe existing competitive metrics in terms of their alignment with human\njudgements. Our experimental results demonstrate that CausalScore significantly\nsurpasses existing state-of-the-art metrics by aligning better with human\njudgements. Additionally, we collect a new dialogue dataset CGDIALOG+ with\nhuman-annotated causal relations and a set of pairwise human judgements to\nfacilitate the development of future automatic metrics.", "published": "2024-06-25 06:08:16", "link": "http://arxiv.org/abs/2406.17300v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging LLMs for Dialogue Quality Measurement", "abstract": "In task-oriented conversational AI evaluation, unsupervised methods poorly\ncorrelate with human judgments, and supervised approaches lack generalization.\nRecent advances in large language models (LLMs) show robust zeroshot and\nfew-shot capabilities across NLP tasks. This paper explores using LLMs for\nautomated dialogue quality evaluation, experimenting with various\nconfigurations on public and proprietary datasets. Manipulating factors such as\nmodel size, in-context examples, and selection techniques, we examine\n\"chain-of-thought\" (CoT) reasoning and label extraction procedures. Our results\nshow that (1) larger models yield more accurate dialogue labels; (2)\nalgorithmic selection of in-context examples outperforms random selection; (3)\nCoT reasoning where an LLM is asked to provide justifications before outputting\nfinal labels improves performance; and (4) fine-tuned LLMs outperform\nout-of-the-box ones. Our results indicate that LLMs that are suitably\nfine-tuned and have sufficient reasoning capabilities can be leveraged for\nautomated dialogue evaluation.", "published": "2024-06-25 06:19:47", "link": "http://arxiv.org/abs/2406.17304v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval Augmented Instruction Tuning for Open NER with Large Language\n  Models", "abstract": "The strong capability of large language models (LLMs) has been applied to\ninformation extraction (IE) through either retrieval augmented prompting or\ninstruction tuning (IT). However, the best way to incorporate information with\nLLMs for IE remains an open question. In this paper, we explore Retrieval\nAugmented Instruction Tuning (RA-IT) for IE, focusing on the task of open named\nentity recognition (NER). Specifically, for each training sample, we retrieve\nsemantically similar examples from the training dataset as the context and\nprepend them to the input of the original instruction. To evaluate our RA-IT\napproach more thoroughly, we construct a Chinese IT dataset for open NER and\nevaluate RA-IT in both English and Chinese scenarios. Experimental results\nverify the effectiveness of RA-IT across various data sizes and in both English\nand Chinese scenarios. We also conduct thorough studies to explore the impacts\nof various retrieval strategies in the proposed RA-IT framework. Code and data\nare available at: https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER", "published": "2024-06-25 06:24:50", "link": "http://arxiv.org/abs/2406.17305v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Not All Preference Pairs Are Created Equal: A Recipe for\n  Annotation-Efficient Iterative Preference Learning", "abstract": "Iterative preference learning, though yielding superior performances,\nrequires online annotated preference labels. In this work, we study strategies\nto select worth-annotating response pairs for cost-efficient annotation while\nachieving competitive or even better performances compared with the random\nselection baseline for iterative preference learning. Built on assumptions\nregarding uncertainty and distribution shifts, we propose a comparative view to\nrank the implicit reward margins as predicted by DPO to select the response\npairs that yield more benefits. Through extensive experiments, we show that\nannotating those response pairs with small margins is generally better than\nlarge or random, under both single- and multi-iteration scenarios. Besides, our\nempirical results suggest allocating more annotation budgets in the earlier\niterations rather than later across multiple iterations.", "published": "2024-06-25 06:49:16", "link": "http://arxiv.org/abs/2406.17312v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Empirical Study on the Characteristics of Bias upon Context Length\n  Variation for Bangla", "abstract": "Pretrained language models inherently exhibit various social biases,\nprompting a crucial examination of their social impact across various\nlinguistic contexts due to their widespread usage. Previous studies have\nprovided numerous methods for intrinsic bias measurements, predominantly\nfocused on high-resource languages. In this work, we aim to extend these\ninvestigations to Bangla, a low-resource language. Specifically, in this study,\nwe (1) create a dataset for intrinsic gender bias measurement in Bangla, (2)\ndiscuss necessary adaptations to apply existing bias measurement methods for\nBangla, and (3) examine the impact of context length variation on bias\nmeasurement, a factor that has been overlooked in previous studies. Through our\nexperiments, we demonstrate a clear dependency of bias metrics on context\nlength, highlighting the need for nuanced considerations in Bangla bias\nanalysis. We consider our work as a stepping stone for bias measurement in the\nBangla Language and make all of our resources publicly available to support\nfuture research.", "published": "2024-06-25 08:49:11", "link": "http://arxiv.org/abs/2406.17375v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Three-Pronged Approach to Cross-Lingual Adaptation with Multilingual\n  LLMs", "abstract": "Low-resource languages, by its very definition, tend to be under represented\nin the pre-training corpora of Large Language Models. In this work, we\ninvestigate three low-resource cross-lingual approaches that enable an LLM\nadapt to tasks in previously unseen languages. Llama-2 is an LLM where Indic\nlanguages, among many other language families, contribute to less than\n$0.005\\%$ of the total $2$ trillion token pre-training corpora. In this work,\nwe experiment with the English-dominated Llama-2 for cross-lingual transfer to\nthree Indic languages, Bengali, Hindi, and Tamil as target languages. We study\nthree approaches for cross-lingual transfer, under ICL and fine-tuning. One, we\nfind that adding additional supervisory signals via a dominant language in the\nLLM, leads to improvements, both under in-context learning and fine-tuning.\nTwo, adapting the target languages to word reordering may be beneficial under\nICL, but its impact diminishes with fine tuning. Finally, continued\npre-training in one low-resource language can improve model performance for\nother related low-resource languages.", "published": "2024-06-25 08:53:46", "link": "http://arxiv.org/abs/2406.17377v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Native Design Bias: Studying the Impact of English Nativeness on\n  Language Model Performance", "abstract": "Large Language Models (LLMs) excel at providing information acquired during\npretraining on large-scale corpora and following instructions through user\nprompts. This study investigates whether the quality of LLM responses varies\ndepending on the demographic profile of users. Considering English as the\nglobal lingua franca, along with the diversity of its dialects among speakers\nof different native languages, we explore whether non-native English speakers\nreceive lower-quality or even factually incorrect responses from LLMs more\nfrequently. Our results show that performance discrepancies occur when LLMs are\nprompted by native versus non-native English speakers and persist when\ncomparing native speakers from Western countries with others. Additionally, we\nfind a strong anchoring effect when the model recognizes or is made aware of\nthe user's nativeness, which further degrades the response quality when\ninteracting with non-native speakers. Our analysis is based on a newly\ncollected dataset with over 12,000 unique annotations from 124 annotators,\nincluding information on their native language and English proficiency.", "published": "2024-06-25 09:04:21", "link": "http://arxiv.org/abs/2406.17385v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Ask Informative Questions: Enhancing LLMs with Preference\n  Optimization and Expected Information Gain", "abstract": "Questions are essential tools for acquiring the necessary information to\ncomplete information-seeking tasks. However, large language models (LLMs),\nespecially open-source models, often perform poorly in generating informative\nquestions, as measured by expected information gain (EIG). In this paper, we\npropose a method to enhance the informativeness of LLM-generated questions in\n20-question game dialogues. We sample multiple questions from the same model\n(LLAMA 2-CHAT 7B) for each game and create pairs of low-EIG and high-EIG\nquestions to apply a Direct Preference Optimization (DPO) algorithm. Our\nresults show that this method produces more effective questions (in terms of\nEIG), even in domains different from those used to train the DPO model.", "published": "2024-06-25 10:44:01", "link": "http://arxiv.org/abs/2406.17453v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MedCare: Advancing Medical LLMs through Decoupling Clinical Alignment\n  and Knowledge Aggregation", "abstract": "Large language models (LLMs) have shown substantial progress in natural\nlanguage understanding and generation, proving valuable especially in the\nmedical field. Despite advancements, challenges persist due to the complexity\nand diversity inherent in medical tasks, which can be categorized as\nknowledge-intensive tasks and alignment-required tasks. Previous approaches\neither ignore the latter task or focus on a minority of tasks and hence lose\ngeneralization. To address these drawbacks, we propose a progressive\nfine-tuning pipeline. This pipeline employs a Knowledge Aggregator and a Noise\naggregator to encode diverse knowledge in the first stage and filter out\ndetrimental information. In the second stage, we drop the Noise Aggregator to\navoid the interference of suboptimal representation and leverage an additional\nalignment module optimized towards an orthogonal direction to the knowledge\nspace to mitigate knowledge forgetting. Based on this two-stage paradigm, we\nproposed a Medical LLM through decoupling Clinical Alignment and Knowledge\nAggregation (MedCare), which is designed to achieve state-of-the-art (SOTA)\nperformance on over 20 medical tasks, as well as SOTA results on specific\nmedical alignment tasks. Various model sizes of MedCare (1.8B, 7B, 14B) all\ndemonstrate significant improvements over existing models with similar model\nsizes.", "published": "2024-06-25 12:05:56", "link": "http://arxiv.org/abs/2406.17484v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Entropy-Based Decoding for Retrieval-Augmented Large Language Models", "abstract": "Augmenting Large Language Models (LLMs) with retrieved external knowledge has\nproven effective for improving the factual accuracy of generated responses.\nDespite their success, retrieval-augmented LLMs still face the distractibility\nissue, where the generated responses are negatively influenced by noise from\nboth external and internal knowledge sources. In this paper, we introduce a\nnovel, training-free decoding method guided by entropy considerations to\nmitigate this issue. Our approach utilizes entropy-based document-parallel\nensemble decoding to prioritize low-entropy distributions from retrieved\ndocuments, thereby enhancing the extraction of relevant information of context.\nAdditionally, it incorporates a contrastive decoding mechanism that contrasts\nthe obtained low-entropy ensemble distribution with the high-entropy\ndistribution derived from the model's internal knowledge across layers, which\nensures a greater emphasis on reliable external information. Extensive\nexperiments on open-domain question answering datasets demonstrate the\nsuperiority of our method.", "published": "2024-06-25 12:59:38", "link": "http://arxiv.org/abs/2406.17519v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-style In-Context Learning for Few-shot Hierarchical Text\n  Classification", "abstract": "Hierarchical text classification (HTC) is an important task with broad\napplications, while few-shot HTC has gained increasing interest recently. While\nin-context learning (ICL) with large language models (LLMs) has achieved\nsignificant success in few-shot learning, it is not as effective for HTC\nbecause of the expansive hierarchical label sets and extremely-ambiguous\nlabels. In this work, we introduce the first ICL-based framework with LLM for\nfew-shot HTC. We exploit a retrieval database to identify relevant\ndemonstrations, and an iterative policy to manage multi-layer hierarchical\nlabels. Particularly, we equip the retrieval database with HTC label-aware\nrepresentations for the input texts, which is achieved by continual training on\na pretrained language model with masked language modeling (MLM), layer-wise\nclassification (CLS, specifically for HTC), and a novel divergent contrastive\nlearning (DCL, mainly for adjacent semantically-similar labels) objective.\nExperimental results on three benchmark datasets demonstrate superior\nperformance of our method, and we can achieve state-of-the-art results in\nfew-shot HTC.", "published": "2024-06-25 13:19:41", "link": "http://arxiv.org/abs/2406.17534v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieval-Augmented Code Generation for Situated Action Generation: A\n  Case Study on Minecraft", "abstract": "In the Minecraft Collaborative Building Task, two players collaborate: an\nArchitect (A) provides instructions to a Builder (B) to assemble a specified\nstructure using 3D blocks. In this work, we investigate the use of large\nlanguage models (LLMs) to predict the sequence of actions taken by the Builder.\nLeveraging LLMs' in-context learning abilities, we use few-shot prompting\ntechniques, that significantly improve performance over baseline methods.\nAdditionally, we present a detailed analysis of the gaps in performance for\nfuture work", "published": "2024-06-25 13:43:24", "link": "http://arxiv.org/abs/2406.17553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data at\n  Scale", "abstract": "The performance of a large language model (LLM) depends heavily on the\nquality and size of its pretraining dataset. However, the pretraining datasets\nfor state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly\navailable and very little is known about how they were created. In this work,\nwe introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl\nsnapshots that produces better-performing LLMs than other open pretraining\ndatasets. To advance the understanding of how best to curate high-quality\npretraining datasets, we carefully document and ablate all of the design\nchoices used in FineWeb, including in-depth investigations of deduplication and\nfiltering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion\ntoken collection of educational text filtered from FineWeb. LLMs pretrained on\nFineWeb-Edu exhibit dramatically better performance on knowledge- and\nreasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we\npublicly release our data curation codebase and all of the models trained\nduring our ablation experiments.", "published": "2024-06-25 13:50:56", "link": "http://arxiv.org/abs/2406.17557v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FrenchToxicityPrompts: a Large Benchmark for Evaluating and Mitigating\n  Toxicity in French Texts", "abstract": "Large language models (LLMs) are increasingly popular but are also prone to\ngenerating bias, toxic or harmful language, which can have detrimental effects\non individuals and communities. Although most efforts is put to assess and\nmitigate toxicity in generated content, it is primarily concentrated on\nEnglish, while it's essential to consider other languages as well. For\naddressing this issue, we create and release FrenchToxicityPrompts, a dataset\nof 50K naturally occurring French prompts and their continuations, annotated\nwith toxicity scores from a widely used toxicity classifier. We evaluate 14\ndifferent models from four prevalent open-sourced families of LLMs against our\ndataset to assess their potential toxicity across various dimensions. We hope\nthat our contribution will foster future research on toxicity detection and\nmitigation beyond Englis", "published": "2024-06-25 14:02:11", "link": "http://arxiv.org/abs/2406.17566v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Text-to-SQL for IoT Defense: A Comprehensive Framework for\n  Querying and Classifying IoT Threats", "abstract": "Recognizing the promise of natural language interfaces to databases, prior\nstudies have emphasized the development of text-to-SQL systems. While\nsubstantial progress has been made in this field, existing research has\nconcentrated on generating SQL statements from text queries. The broader\nchallenge, however, lies in inferring new information about the returned data.\nOur research makes two major contributions to address this gap. First, we\nintroduce a novel Internet-of-Things (IoT) text-to-SQL dataset comprising\n10,985 text-SQL pairs and 239,398 rows of network traffic activity. The dataset\ncontains additional query types limited in prior text-to-SQL datasets, notably\ntemporal-related queries. Our dataset is sourced from a smart building's IoT\necosystem exploring sensor read and network traffic data. Second, our dataset\nallows two-stage processing, where the returned data (network traffic) from a\ngenerated SQL can be categorized as malicious or not. Our results show that\njoint training to query and infer information about the data can improve\noverall text-to-SQL performance, nearly matching substantially larger models.\nWe also show that current large language models (e.g., GPT3.5) struggle to\ninfer new information about returned data, thus our dataset provides a novel\ntest bed for integrating complex domain-specific reasoning into LLMs.", "published": "2024-06-25 14:14:35", "link": "http://arxiv.org/abs/2406.17574v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongIns: A Challenging Long-context Instruction-based Exam for LLMs", "abstract": "The long-context capabilities of large language models (LLMs) have been a hot\ntopic in recent years. To evaluate the performance of LLMs in different\nscenarios, various assessment benchmarks have emerged. However, as most of\nthese benchmarks focus on identifying key information to answer questions,\nwhich mainly requires the retrieval ability of LLMs, these benchmarks can\npartially represent the reasoning performance of LLMs from large amounts of\ninformation. Meanwhile, although LLMs often claim to have context windows of\n32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual\nsupported length of these LLMs. To address these issues, we propose the LongIns\nbenchmark dataset, a challenging long-context instruction-based exam for LLMs,\nwhich is built based on the existing instruction datasets. Specifically, in our\nLongIns, we introduce three evaluation settings: Global Instruction & Single\nTask (GIST), Local Instruction & Single Task (LIST), and Local Instruction &\nMultiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations\non existing LLMs and have the following important findings: (1). The\ntop-performing GPT-4 with 128k context length performs poorly on the evaluation\ncontext window of 16k in our LongIns. (2). For the multi-hop reasoning ability\nof many existing LLMs, significant efforts are still needed under short context\nwindows (less than 4k).", "published": "2024-06-25 14:31:26", "link": "http://arxiv.org/abs/2406.17588v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"Seeing the Big through the Small\": Can LLMs Approximate Human Judgment\n  Distributions on NLI from a Few Explanations?", "abstract": "Human label variation (HLV) is a valuable source of information that arises\nwhen multiple human annotators provide different labels for valid reasons. In\nNatural Language Inference (NLI) earlier approaches to capturing HLV involve\neither collecting annotations from many crowd workers to represent human\njudgment distribution (HJD) or use expert linguists to provide detailed\nexplanations for their chosen labels. While the former method provides denser\nHJD information, obtaining it is resource-intensive. In contrast, the latter\noffers richer textual information but it is challenging to scale up to many\nhuman judges. Besides, large language models (LLMs) are increasingly used as\nevaluators (\"LLM judges\") but with mixed results, and few works aim to study\nHJDs. This study proposes to exploit LLMs to approximate HJDs using a small\nnumber of expert labels and explanations. Our experiments show that a few\nexplanations significantly improve LLMs' ability to approximate HJDs with and\nwithout explicit labels, thereby providing a solution to scale up annotations\nfor HJD. However, fine-tuning smaller soft-label aware models with the\nLLM-generated model judgment distributions (MJDs) presents partially\ninconsistent results: while similar in distance, their resulting fine-tuned\nmodels and visualized distributions differ substantially. We show the\nimportance of complementing instance-level distance measures with a\nglobal-level shape metric and visualization to more effectively evaluate MJDs\nagainst human judgment distributions.", "published": "2024-06-25 14:42:17", "link": "http://arxiv.org/abs/2406.17600v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variationist: Exploring Multifaceted Variation and Bias in Written\n  Language Data", "abstract": "Exploring and understanding language data is a fundamental stage in all areas\ndealing with human language. It allows NLP practitioners to uncover quality\nconcerns and harmful biases in data before training, and helps linguists and\nsocial scientists to gain insight into language use and human behavior. Yet,\nthere is currently a lack of a unified, customizable tool to seamlessly inspect\nand visualize language variation and bias across multiple variables, language\nunits, and diverse metrics that go beyond descriptive statistics. In this\npaper, we introduce Variationist, a highly-modular, extensible, and\ntask-agnostic tool that fills this gap. Variationist handles at once a\npotentially unlimited combination of variable types and semantics across\ndiversity and association metrics with regards to the language unit of choice,\nand orchestrates the creation of up to five-dimensional interactive charts for\nover 30 variable type-semantics combinations. Through our case studies on\ncomputational dialectology, human label variation, and text generation, we show\nhow Variationist enables researchers from different disciplines to effortlessly\nanswer specific research questions or unveil undesired associations in language\ndata. A Python library, code, documentation, and tutorials are made publicly\navailable to the research community.", "published": "2024-06-25 15:41:07", "link": "http://arxiv.org/abs/2406.17647v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying AI Psychology: A Psychometrics Benchmark for Large Language\n  Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional task-solving\ncapabilities, increasingly adopting roles akin to human-like assistants. The\nbroader integration of LLMs into society has sparked interest in whether they\nmanifest psychological attributes, and whether these attributes are\nstable-inquiries that could deepen the understanding of their behaviors.\nInspired by psychometrics, this paper presents a framework for investigating\npsychology in LLMs, including psychological dimension identification,\nassessment dataset curation, and assessment with results validation. Following\nthis framework, we introduce a comprehensive psychometrics benchmark for LLMs\nthat covers six psychological dimensions: personality, values, emotion, theory\nof mind, motivation, and intelligence. This benchmark includes thirteen\ndatasets featuring diverse scenarios and item types. Our findings indicate that\nLLMs manifest a broad spectrum of psychological attributes. We also uncover\ndiscrepancies between LLMs' self-reported traits and their behaviors in\nreal-world scenarios. This paper demonstrates a thorough psychometric\nassessment of LLMs, providing insights into reliable evaluation and potential\napplications in AI and social sciences.", "published": "2024-06-25 16:09:08", "link": "http://arxiv.org/abs/2406.17675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VarBench: Robust Language Model Benchmarking Through Dynamic Variable\n  Perturbation", "abstract": "As large language models achieve impressive scores on traditional benchmarks,\nan increasing number of researchers are becoming concerned about benchmark data\nleakage during pre-training, commonly known as the data contamination problem.\nTo ensure fair evaluation, recent benchmarks release only the training and\nvalidation sets, keeping the test set labels closed-source. They require anyone\nwishing to evaluate his language model to submit the model's predictions for\ncentralized processing and then publish the model's result on their\nleaderboard. However, this submission process is inefficient and prevents\neffective error analysis. To address this issue, we propose to variabilize\nbenchmarks and evaluate language models dynamically. Specifically, we extract\nvariables from each test case and define a value range for each variable. For\neach evaluation, we sample new values from these value ranges to create unique\ntest cases, thus ensuring a fresh evaluation each time. We applied this\nvariable perturbation method to four datasets: GSM8K, ARC, CommonsenseQA, and\nTruthfulQA, which cover mathematical generation and multiple-choice tasks. Our\nexperimental results demonstrate that this approach provides a more accurate\nassessment of the true capabilities of language models, effectively mitigating\nthe contamination problem.", "published": "2024-06-25 16:13:53", "link": "http://arxiv.org/abs/2406.17681v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ViANLI: Adversarial Natural Language Inference for Vietnamese", "abstract": "The development of Natural Language Processing (NLI) datasets and models has\nbeen inspired by innovations in annotation design. With the rapid development\nof machine learning models today, the performance of existing machine learning\nmodels has quickly reached state-of-the-art results on a variety of tasks\nrelated to natural language processing, including natural language inference\ntasks. By using a pre-trained model during the annotation process, it is\npossible to challenge current NLI models by having humans produce\npremise-hypothesis combinations that the machine model cannot correctly\npredict. To remain attractive and challenging in the research of natural\nlanguage inference for Vietnamese, in this paper, we introduce the adversarial\nNLI dataset to the NLP research community with the name ViANLI. This data set\ncontains more than 10K premise-hypothesis pairs and is built by a continuously\nadjusting process to obtain the most out of the patterns generated by the\nannotators. ViANLI dataset has brought many difficulties to many current SOTA\nmodels when the accuracy of the most powerful model on the test set only\nreached 48.4%. Additionally, the experimental results show that the models\ntrained on our dataset have significantly improved the results on other\nVietnamese NLI datasets.", "published": "2024-06-25 16:58:19", "link": "http://arxiv.org/abs/2406.17716v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Following Length Constraints in Instructions", "abstract": "Aligned instruction following models can better fulfill user requests than\ntheir unaligned counterparts. However, it has been shown that there is a length\nbias in evaluation of such models, and that training algorithms tend to exploit\nthis bias by learning longer responses. In this work we show how to train\nmodels that can be controlled at inference time with instructions containing\ndesired length constraints. Such models are superior in length instructed\nevaluations, outperforming standard instruction following models such as GPT4,\nLlama 3 and Mixtral.", "published": "2024-06-25 17:29:52", "link": "http://arxiv.org/abs/2406.17744v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Clinical Evidence Synthesis with Large Language Models", "abstract": "Synthesizing clinical evidence largely relies on systematic reviews of\nclinical trials and retrospective analyses from medical literature. However,\nthe rapid expansion of publications presents challenges in efficiently\nidentifying, summarizing, and updating clinical evidence. Here, we introduce\nTrialMind, a generative artificial intelligence (AI) pipeline for facilitating\nhuman-AI collaboration in three crucial tasks for evidence synthesis: study\nsearch, screening, and data extraction. To assess its performance, we chose\npublished systematic reviews to build the benchmark dataset, named\nTrialReviewBench, which contains 100 systematic reviews and the associated\n2,220 clinical studies. Our results show that TrialMind excels across all three\ntasks. In study search, it generates diverse and comprehensive search queries\nto achieve high recall rates (Ours 0.711-0.834 v.s. Human baseline\n0.138-0.232). For study screening, TrialMind surpasses traditional\nembedding-based methods by 30% to 160%. In data extraction, it outperforms a\nGPT-4 baseline by 29.6% to 61.5%. We further conducted user studies to confirm\nits practical utility. Compared to manual efforts, human-AI collaboration using\nTrialMind yielded a 71.4% recall lift and 44.2% time savings in study screening\nand a 23.5% accuracy lift and 63.4% time savings in data extraction.\nAdditionally, when comparing synthesized clinical evidence presented in forest\nplots, medical experts favored TrialMind's outputs over GPT-4's outputs in\n62.5% to 100% of cases. These findings show the promise of LLM-based approaches\nlike TrialMind to accelerate clinical evidence synthesis via streamlining study\nsearch, screening, and data extraction from medical literature, with\nexceptional performance improvement when working with human experts.", "published": "2024-06-25 17:41:52", "link": "http://arxiv.org/abs/2406.17755v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cloaked Classifiers: Pseudonymization Strategies on Sensitive\n  Classification Tasks", "abstract": "Protecting privacy is essential when sharing data, particularly in the case\nof an online radicalization dataset that may contain personal information. In\nthis paper, we explore the balance between preserving data usefulness and\nensuring robust privacy safeguards, since regulations like the European GDPR\nshape how personal information must be handled. We share our method for\nmanually pseudonymizing a multilingual radicalization dataset, ensuring\nperformance comparable to the original data. Furthermore, we highlight the\nimportance of establishing comprehensive guidelines for processing sensitive\nNLP data by sharing our complete pseudonymization process, our guidelines, the\nchallenges we encountered as well as the resulting dataset.", "published": "2024-06-25 18:30:25", "link": "http://arxiv.org/abs/2406.17875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Script-Agnostic Language Identification", "abstract": "Language identification is used as the first step in many data collection and\ncrawling efforts because it allows us to sort online text into\nlanguage-specific buckets. However, many modern languages, such as Konkani,\nKashmiri, Punjabi etc., are synchronically written in several scripts.\nMoreover, languages with different writing systems do not share significant\nlexical, semantic, and syntactic properties in neural representation spaces,\nwhich is a disadvantage for closely related languages and low-resource\nlanguages, especially those from the Indian Subcontinent. To counter this, we\npropose learning script-agnostic representations using several different\nexperimental strategies (upscaling, flattening, and script mixing) focusing on\nfour major Dravidian languages (Tamil, Telugu, Kannada, and Malayalam). We find\nthat word-level script randomization and exposure to a language written in\nmultiple scripts is extremely valuable for downstream script-agnostic language\nidentification, while also maintaining competitive performance on naturally\noccurring text.", "published": "2024-06-25 19:23:42", "link": "http://arxiv.org/abs/2406.17901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "X-ray Made Simple: Lay Radiology Report Generation and Robust Evaluation", "abstract": "Radiology Report Generation (RRG) has advanced considerably with the\ndevelopment of multimodal generative models. Despite the progress, the field\nstill faces significant challenges in evaluation, as existing metrics lack\nrobustness and fairness. We reveal that, RRG with high performance on existing\nlexical-based metrics (e.g. BLEU) might be more of a mirage - a model can get a\nhigh BLEU only by learning the template of reports. This has become a pressing\nissue for RRG due to the highly patternized nature of these reports. In\naddition, standard radiology reports are often highly technical. Helping\npatients understand these reports is crucial from a patient's perspective, yet\nthis has been largely overlooked in previous work. In this work, we\nun-intuitively approach these problems by proposing the Layman's RRG framework\nthat can systematically improve RRG with day-to-day language. Specifically, our\nframework first contributes a translated Layman's terms dataset. Building upon\nthe dataset, we then propose a semantics-based evaluation method, which is\neffective in mitigating the inflated numbers of BLEU and provides more robust\nevaluation. We show that training on the layman's terms dataset encourages\nmodels to focus on the semantics of the reports, as opposed to overfitting to\nlearning the report templates. Last, we reveal a promising scaling law between\nthe number of training examples and semantics gain provided by our dataset,\ncompared to the inverse pattern brought by the original formats.", "published": "2024-06-25 19:52:01", "link": "http://arxiv.org/abs/2406.17911v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning", "abstract": "Large language models (LLMs) have shown remarkable abilities in diverse\nnatural language processing (NLP) tasks. The LLMs generally undergo supervised\nfine-tuning (SFT) followed by preference alignment to be usable in downstream\napplications. However, this sequential training pipeline leads to alignment tax\nthat degrades the LLM performance.\n  This paper introduces PAFT, a new PArallel training paradigm for effective\nLLM Fine-Tuning, which independently performs SFT and preference alignment\n(e.g., DPO and ORPO, etc.) with the same pre-trained model on respective\ndatasets. The model produced by SFT and the model from preference alignment are\nthen merged into a final model by parameter fusing for use in downstream\napplications. This work reveals important findings that preference alignment\nlike DPO naturally results in a sparse model while SFT leads to a natural dense\nmodel which needs to be sparsified for effective model merging. This paper\nintroduces an effective interference resolution which reduces the redundancy by\nsparsifying the delta parameters. The LLM resulted from the new training\nparadigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard.\nComprehensive evaluation shows the effectiveness of the parallel training\nparadigm.", "published": "2024-06-25 20:11:37", "link": "http://arxiv.org/abs/2406.17923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do they mean 'us'? Interpreting Referring Expressions in Intergroup Bias", "abstract": "The variations between in-group and out-group speech (intergroup bias) are\nsubtle and could underlie many social phenomena like stereotype perpetuation\nand implicit bias. In this paper, we model the intergroup bias as a tagging\ntask on English sports comments from forums dedicated to fandom for NFL teams.\nWe curate a unique dataset of over 6 million game-time comments from opposing\nperspectives (the teams in the game), each comment grounded in a non-linguistic\ndescription of the events that precipitated these comments (live win\nprobabilities for each team). Expert and crowd annotations justify modeling the\nbias through tagging of implicit and explicit referring expressions and reveal\nthe rich, contextual understanding of language and the world required for this\ntask. For large-scale analysis of intergroup variation, we use LLMs for\nautomated tagging, and discover that some LLMs perform best when prompted with\nlinguistic descriptions of the win probability at the time of the comment,\nrather than numerical probability. Further, large-scale tagging of comments\nusing LLMs uncovers linear variations in the form of referent across win\nprobabilities that distinguish in-group and out-group utterances. Code and data\nare available at https://github.com/venkatasg/intergroup-nfl .", "published": "2024-06-25 21:47:53", "link": "http://arxiv.org/abs/2406.17947v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crafting Customisable Characters with LLMs: Introducing SimsChat, a\n  Persona-Driven Role-Playing Agent Framework", "abstract": "Large Language Models (LLMs) demonstrate remarkable ability to comprehend\ninstructions and generate human-like text, enabling sophisticated agent\nsimulation beyond basic behavior replication. However, the potential for\ncreating freely customisable characters remains underexplored. We introduce the\nCustomisable Conversation Agent Framework, which employs LLMs to simulate\nreal-world characters through personalised characteristic feature injection,\nenabling diverse character creation according to user preferences. We propose\nthe SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn\nrole-playing dialogues across 1,360 real-world scenes. Characters are initially\ncustomised using pre-defined elements (career, aspiration, traits, skills),\nthen expanded through personal and social profiles. Building on this, we\npresent SimsChat, a freely customisable role-playing agent incorporating\nvarious realistic settings and topic-specified character interactions.\nExperimental results on both SimsConv and WikiRoleEval datasets demonstrate\nSimsChat's superior performance in maintaining character consistency, knowledge\naccuracy, and appropriate question rejection compared to existing models. Our\nframework provides valuable insights for developing more accurate and\ncustomisable human simulacra. Our data and code are publicly available at\nhttps://github.com/Bernard-Yang/SimsChat.", "published": "2024-06-25 22:44:17", "link": "http://arxiv.org/abs/2406.17962v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unmasking the Imposters: How Censorship and Domain Adaptation Affect the\n  Detection of Machine-Generated Tweets", "abstract": "The rapid development of large language models (LLMs) has significantly\nimproved the generation of fluent and convincing text, raising concerns about\ntheir potential misuse on social media platforms. We present a comprehensive\nmethodology for creating nine Twitter datasets to examine the generative\ncapabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These\ndatasets encompass four censored and five uncensored model configurations,\nincluding 7B and 8B parameter base-instruction models of the three open-source\nLLMs. Additionally, we perform a data quality analysis to assess the\ncharacteristics of textual outputs from human, \"censored,\" and \"uncensored\"\nmodels, employing semantic meaning, lexical richness, structural patterns,\ncontent characteristics, and detector performance metrics to identify\ndifferences and similarities. Our evaluation demonstrates that \"uncensored\"\nmodels significantly undermine the effectiveness of automated detection\nmethods. This study addresses a critical gap by exploring smaller open-source\nmodels and the ramifications of \"uncensoring,\" providing valuable insights into\nhow domain adaptation and content moderation strategies influence both the\ndetectability and structural characteristics of machine-generated text.", "published": "2024-06-25 22:49:17", "link": "http://arxiv.org/abs/2406.17967v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EDEN: Empathetic Dialogues for English learning", "abstract": "Dialogue systems have been used as conversation partners in English learning,\nbut few have studied whether these systems improve learning outcomes. Student\npassion and perseverance, or grit, has been associated with language learning\nsuccess. Recent work establishes that as students perceive their English\nteachers to be more supportive, their grit improves. Hypothesizing that the\nsame pattern applies to English-teaching chatbots, we create EDEN, a robust\nopen-domain chatbot for spoken conversation practice that provides empathetic\nfeedback. To construct EDEN, we first train a specialized spoken utterance\ngrammar correction model and a high-quality social chit-chat conversation\nmodel. We then conduct a preliminary user study with a variety of strategies\nfor empathetic feedback. Our experiment suggests that using adaptive empathetic\nfeedback leads to higher perceived affective support. Furthermore, elements of\nperceived affective support positively correlate with student grit.", "published": "2024-06-25 23:36:16", "link": "http://arxiv.org/abs/2406.17982v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Constructed Context Decompilation with Fined-grained Alignment\n  Enhancement", "abstract": "Decompilation transforms compiled code back into a high-level programming\nlanguage for analysis when source code is unavailable. Previous work has\nprimarily focused on enhancing decompilation performance by increasing the\nscale of model parameters or training data for pre-training. Based on the\ncharacteristics of the decompilation task, we propose two methods: (1) Without\nfine-tuning, the Self-Constructed Context Decompilation (sc$^2$dec) method\nrecompiles the LLM's decompilation results to construct pairs for in-context\nlearning, helping the model improve decompilation performance. (2) Fine-grained\nAlignment Enhancement (FAE), which meticulously aligns assembly code with\nsource code at the statement level by leveraging debugging information, is\nemployed during the fine-tuning phase to achieve further improvements in\ndecompilation. By integrating these two methods, we achieved a Re-Executability\nperformance improvement of approximately 3.90% on the Decompile-Eval benchmark,\nestablishing a new state-of-the-art performance of 52.41%. The code, data, and\nmodels are available at https://github.com/AlongWY/sccdec.", "published": "2024-06-25 02:37:53", "link": "http://arxiv.org/abs/2406.17233v2", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Can We Trust the Performance Evaluation of Uncertainty Estimation\n  Methods in Text Summarization?", "abstract": "Text summarization, a key natural language generation (NLG) task, is vital in\nvarious domains. However, the high cost of inaccurate summaries in\nrisk-critical applications, particularly those involving human-in-the-loop\ndecision-making, raises concerns about the reliability of uncertainty\nestimation on text summarization (UE-TS) evaluation methods. This concern stems\nfrom the dependency of uncertainty model metrics on diverse and potentially\nconflicting NLG metrics. To address this issue, we introduce a comprehensive\nUE-TS benchmark incorporating 31 NLG metrics across four dimensions. The\nbenchmark evaluates the uncertainty estimation capabilities of two large\nlanguage models and one pre-trained language model on three datasets, with\nhuman-annotation analysis incorporated where applicable. We also assess the\nperformance of 14 common uncertainty estimation methods within this benchmark.\nOur findings emphasize the importance of considering multiple uncorrelated NLG\nmetrics and diverse uncertainty estimation methods to ensure reliable and\nefficient evaluation of UE-TS techniques. Our code and data are available\nhttps://github.com/he159ok/Benchmark-of-Uncertainty-Estimation-Methods-in-Text-Summarization.", "published": "2024-06-25 04:41:17", "link": "http://arxiv.org/abs/2406.17274v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Predicting the Big Five Personality Traits in Chinese Counselling\n  Dialogues Using Large Language Models", "abstract": "Accurate assessment of personality traits is crucial for effective\npsycho-counseling, yet traditional methods like self-report questionnaires are\ntime-consuming and biased. This study exams whether Large Language Models\n(LLMs) can predict the Big Five personality traits directly from counseling\ndialogues and introduces an innovative framework to perform the task. Our\nframework applies role-play and questionnaire-based prompting to condition LLMs\non counseling sessions, simulating client responses to the Big Five Inventory.\nWe evaluated our framework on 853 real-world counseling sessions, finding a\nsignificant correlation between LLM-predicted and actual Big Five traits,\nproving the validity of framework. Moreover, ablation studies highlight the\nimportance of role-play simulations and task simplification via questionnaires\nin enhancing prediction accuracy. Meanwhile, our fine-tuned Llama3-8B model,\nutilizing Direct Preference Optimization with Supervised Fine-Tuning, achieves\na 130.95\\% improvement, surpassing the state-of-the-art Qwen1.5-110B by 36.94\\%\nin personality prediction validity. In conclusion, LLMs can predict personality\nbased on counseling dialogues. Our code and model are publicly available at\n\\url{https://github.com/kuri-leo/BigFive-LLM-Predictor}, providing a valuable\ntool for future research in computational psychometrics.", "published": "2024-06-25 05:30:55", "link": "http://arxiv.org/abs/2406.17287v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dual-Space Knowledge Distillation for Large Language Models", "abstract": "Knowledge distillation (KD) is known as a promising solution to compress\nlarge language models (LLMs) via transferring their knowledge to smaller\nmodels. During this process, white-box KD methods usually minimize the distance\nbetween the output distributions of the two models so that more knowledge can\nbe transferred. However, in the current white-box KD framework, the output\ndistributions are from the respective output spaces of the two models, using\ntheir own prediction heads. We argue that the space discrepancy will lead to\nlow similarity between the teacher model and the student model on both\nrepresentation and distribution levels. Furthermore, this discrepancy also\nhinders the KD process between models with different vocabularies, which is\ncommon for current LLMs. To address these issues, we propose a dual-space\nknowledge distillation (DSKD) framework that unifies the output spaces of the\ntwo models for KD. On the basis of DSKD, we further develop a cross-model\nattention mechanism, which can automatically align the representations of the\ntwo models with different vocabularies. Thus, our framework is not only\ncompatible with various distance functions for KD (e.g., KL divergence) like\nthe current framework, but also supports KD between any two LLMs regardless of\ntheir vocabularies. Experiments on task-agnostic instruction-following\nbenchmarks show that DSKD significantly outperforms the current white-box KD\nframework with various distance functions, and also surpasses existing KD\nmethods for LLMs with different vocabularies.", "published": "2024-06-25 07:25:15", "link": "http://arxiv.org/abs/2406.17328v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens", "abstract": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nLLM-based embedder, the obtained text embedding will be able to be aligned with\nthe key tokens in the input text. We first fully analyze this phenomenon on\neight LLM-based embedders and show that this phenomenon is universal and is not\naffected by model architecture, training strategy, and embedding method. With a\ndeeper analysis, we find that the main change in embedding space between these\nembedders and their LLM backbones is in the first principal component. By\nadjusting the first principal component, we can align text embedding with the\nkey tokens. Finally, we give several examples to demonstrate the vast\napplication potential of this finding: (1) we propose a simple and practical\nsparse retrieval method based on the aligned tokens, which can achieve 80% of\nthe dense retrieval effect of the same model while reducing the computation\nsignificantly; (2) we show that our findings provide a novel perspective to\nhelp understand novel technologies (e.g., instruction-following embedding) and\nfuzzy concepts (e.g., semantic relatedness vs. similarity) in this field.", "published": "2024-06-25 08:55:12", "link": "http://arxiv.org/abs/2406.17378v3", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Make Some Noise: Unlocking Language Model Parallel Inference Capability\n  through Noisy Training", "abstract": "Existing speculative decoding methods typically require additional model\nstructure and training processes to assist the model for draft token\ngeneration. This makes the migration of acceleration methods to the new model\nmore costly and more demanding on device memory. To address this problem, we\npropose the Make Some Noise (MSN) training framework as a replacement for the\nsupervised fine-tuning stage of the large language model. The training method\nsimply introduces some noise at the input for the model to learn the denoising\ntask. It significantly enhances the parallel decoding capability of the model\nwithout affecting the original task capability. In addition, we propose a\ntree-based retrieval-augmented Jacobi (TR-Jacobi) decoding strategy to further\nimprove the inference speed of MSN models. Experiments in both the general and\ncode domains have shown that MSN can improve inference speed by 2.3-2.7x times\nwithout compromising model performance. The MSN model also achieves comparable\nacceleration ratios to the SOTA model with additional model structure on\nSpec-Bench.", "published": "2024-06-25 09:25:39", "link": "http://arxiv.org/abs/2406.17404v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leave No Document Behind: Benchmarking Long-Context LLMs with Extended\n  Multi-Doc QA", "abstract": "Long-context modeling capabilities have garnered widespread attention,\nleading to the emergence of Large Language Models (LLMs) with ultra-context\nwindows. Meanwhile, benchmarks for evaluating long-context LLMs are gradually\ncatching up. However, existing benchmarks employ irrelevant noise texts to\nartificially extend the length of test cases, diverging from the real-world\nscenarios of long-context applications. To bridge this gap, we propose a novel\nlong-context benchmark, Loong, aligning with realistic scenarios through\nextended multi-document question answering (QA). Unlike typical document QA, in\nLoong's test cases, each document is relevant to the final answer, ignoring any\ndocument will lead to the failure of the answer. Furthermore, Loong introduces\nfour types of tasks with a range of context lengths: Spotlight Locating,\nComparison, Clustering, and Chain of Reasoning, to facilitate a more realistic\nand comprehensive evaluation of long-context understanding. Extensive\nexperiments indicate that existing long-context language models still exhibit\nconsiderable potential for enhancement. Retrieval augmented generation (RAG)\nachieves poor performance, demonstrating that Loong can reliably assess the\nmodel's long-context modeling capabilities.", "published": "2024-06-25 09:42:56", "link": "http://arxiv.org/abs/2406.17419v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Grammatical Error Correction via Contextual Data Augmentation", "abstract": "Nowadays, data augmentation through synthetic data has been widely used in\nthe field of Grammatical Error Correction (GEC) to alleviate the problem of\ndata scarcity. However, these synthetic data are mainly used in the\npre-training phase rather than the data-limited fine-tuning phase due to\ninconsistent error distribution and noisy labels. In this paper, we propose a\nsynthetic data construction method based on contextual augmentation, which can\nensure an efficient augmentation of the original data with a more consistent\nerror distribution. Specifically, we combine rule-based substitution with\nmodel-based generation, using the generative model to generate a richer context\nfor the extracted error patterns. Besides, we also propose a relabeling-based\ndata cleaning method to mitigate the effects of noisy labels in synthetic data.\nExperiments on CoNLL14 and BEA19-Test show that our proposed augmentation\nmethod consistently and substantially outperforms strong baselines and achieves\nthe state-of-the-art level with only a few synthetic data.", "published": "2024-06-25 10:49:56", "link": "http://arxiv.org/abs/2406.17456v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Tool Retrieval with Iterative Feedback from Large Language\n  Models", "abstract": "Tool learning aims to enhance and expand large language models' (LLMs)\ncapabilities with external tools, which has gained significant attention\nrecently. Current methods have shown that LLMs can effectively handle a certain\namount of tools through in-context learning or fine-tuning. However, in\nreal-world scenarios, the number of tools is typically extensive and\nirregularly updated, emphasizing the necessity for a dedicated tool retrieval\ncomponent. Tool retrieval is nontrivial due to the following challenges: 1)\ncomplex user instructions and tool descriptions; 2) misalignment between tool\nretrieval and tool usage models. To address the above issues, we propose to\nenhance tool retrieval with iterative feedback from the large language model.\nSpecifically, we prompt the tool usage model, i.e., the LLM, to provide\nfeedback for the tool retriever model in multi-round, which could progressively\nimprove the tool retriever's understanding of instructions and tools and reduce\nthe gap between the two standalone components. We build a unified and\ncomprehensive benchmark to evaluate tool retrieval models. The extensive\nexperiments indicate that our proposed approach achieves advanced performance\nin both in-domain evaluation and out-of-domain evaluation.", "published": "2024-06-25 11:12:01", "link": "http://arxiv.org/abs/2406.17465v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer-based Named Entity Recognition with Combined Data\n  Representation", "abstract": "This study examines transformer-based models and their effectiveness in named\nentity recognition tasks. The study investigates data representation\nstrategies, including single, merged, and context, which respectively use one\nsentence, multiple sentences, and sentences joined with attention to context\nper vector. Analysis shows that training models with a single strategy may lead\nto poor performance on different data representations. To address this\nlimitation, the study proposes a combined training procedure that utilizes all\nthree strategies to improve model stability and adaptability. The results of\nthis approach are presented and discussed for four languages (English, Polish,\nCzech, and German) across various datasets, demonstrating the effectiveness of\nthe combined strategy.", "published": "2024-06-25 11:41:16", "link": "http://arxiv.org/abs/2406.17474v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking Mental State Representations in Language Models", "abstract": "While numerous works have assessed the generative performance of language\nmodels (LMs) on tasks requiring Theory of Mind reasoning, research into the\nmodels' internal representation of mental states remains limited. Recent work\nhas used probing to demonstrate that LMs can represent beliefs of themselves\nand others. However, these claims are accompanied by limited evaluation, making\nit difficult to assess how mental state representations are affected by model\ndesign and training choices. We report an extensive benchmark with various LM\ntypes with different model sizes, fine-tuning approaches, and prompt designs to\nstudy the robustness of mental state representations and memorisation issues\nwithin the probes. Our results show that the quality of models' internal\nrepresentations of the beliefs of others increases with model size and, more\ncrucially, with fine-tuning. We are the first to study how prompt variations\nimpact probing performance on theory of mind tasks. We demonstrate that models'\nrepresentations are sensitive to prompt variations, even when such variations\nshould be beneficial. Finally, we complement previous activation editing\nexperiments on Theory of Mind tasks and show that it is possible to improve\nmodels' reasoning performance by steering their activations without the need to\ntrain any probe.", "published": "2024-06-25 12:51:06", "link": "http://arxiv.org/abs/2406.17513v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LumberChunker: Long-Form Narrative Document Segmentation", "abstract": "Modern NLP tasks increasingly rely on dense retrieval methods to access\nup-to-date and relevant contextual information. We are motivated by the premise\nthat retrieval benefits from segments that can vary in size such that a\ncontent's semantic independence is better captured. We propose LumberChunker, a\nmethod leveraging an LLM to dynamically segment documents, which iteratively\nprompts the LLM to identify the point within a group of sequential passages\nwhere the content begins to shift. To evaluate our method, we introduce\nGutenQA, a benchmark with 3000 \"needle in a haystack\" type of question-answer\npairs derived from 100 public domain narrative books available on Project\nGutenberg. Our experiments show that LumberChunker not only outperforms the\nmost competitive baseline by 7.37% in retrieval performance (DCG@20) but also\nthat, when integrated into a RAG pipeline, LumberChunker proves to be more\neffective than other chunking methods and competitive baselines, such as the\nGemini 1.5M Pro. Our Code and Data are available at\nhttps://github.com/joaodsmarques/LumberChunker", "published": "2024-06-25 13:08:35", "link": "http://arxiv.org/abs/2406.17526v1", "categories": ["cs.CL", "cs.IR", "I.2"], "primary_category": "cs.CL"}
{"title": "Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian\n  Benchmark", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to generate and manipulate human language, highlighting\ntheir potential across various applications. Evaluating LLMs in languages other\nthan English is crucial for ensuring their linguistic versatility, cultural\nrelevance, and applicability in diverse global contexts, thus broadening their\nusability and effectiveness. We tackle this challenge by introducing a\nstructured benchmark using the INVALSI tests, a set of well-established\nassessments designed to measure educational competencies across Italy. Our\nstudy makes three primary contributions: Firstly, we adapt the INVALSI\nbenchmark for automated LLM evaluation, which involves rigorous adaptation of\nthe test format to suit automated processing while retaining the essence of the\noriginal tests. Secondly, we provide a detailed assessment of current LLMs,\noffering a crucial reference point for the academic community. Finally, we\nvisually compare the performance of these models against human results.\nAdditionally, researchers are invited to submit their models for ongoing\nevaluation, ensuring the benchmark remains a current and valuable resource.", "published": "2024-06-25 13:20:08", "link": "http://arxiv.org/abs/2406.17535v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-assessment, Exhibition, and Recognition: a Review of Personality in\n  Large Language Models", "abstract": "As large language models (LLMs) appear to behave increasingly human-like in\ntext-based interactions, more and more researchers become interested in\ninvestigating personality in LLMs. However, the diversity of psychological\npersonality research and the rapid development of LLMs have led to a broad yet\nfragmented landscape of studies in this interdisciplinary field. Extensive\nstudies across different research focuses, different personality psychometrics,\nand different LLMs make it challenging to have a holistic overview and further\npose difficulties in applying findings to real-world applications. In this\npaper, we present a comprehensive review by categorizing current studies into\nthree research problems: self-assessment, exhibition, and recognition, based on\nthe intrinsic characteristics and external manifestations of personality in\nLLMs. For each problem, we provide a thorough analysis and conduct in-depth\ncomparisons of their corresponding solutions. Besides, we summarize research\nfindings and open challenges from current studies and further discuss their\nunderlying causes. We also collect extensive publicly available resources to\nfacilitate interested researchers and developers. Lastly, we discuss the\npotential future research directions and application scenarios. Our paper is\nthe first comprehensive survey of up-to-date literature on personality in LLMs.\nBy presenting a clear taxonomy, in-depth analysis, promising future directions,\nand extensive resource collections, we aim to provide a better understanding\nand facilitate further advancements in this emerging field.", "published": "2024-06-25 15:08:44", "link": "http://arxiv.org/abs/2406.17624v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue\n  Coreference", "abstract": "As large language models (LLMs) constantly evolve, ensuring their safety\nremains a critical research problem. Previous red-teaming approaches for LLM\nsafety have primarily focused on single prompt attacks or goal hijacking. To\nthe best of our knowledge, we are the first to study LLM safety in multi-turn\ndialogue coreference. We created a dataset of 1,400 questions across 14\ncategories, each featuring multi-turn coreference safety attacks. We then\nconducted detailed evaluations on five widely used open-source LLMs. The\nresults indicated that under multi-turn coreference safety attacks, the highest\nattack success rate was 56% with the LLaMA2-Chat-7b model, while the lowest was\n13.9% with the Mistral-7B-Instruct model. These findings highlight the safety\nvulnerabilities in LLMs during dialogue coreference interactions.", "published": "2024-06-25 15:13:02", "link": "http://arxiv.org/abs/2406.17626v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Knowledge Distillation in Automated Annotation: Supervised Text\n  Classification with LLM-Generated Training Labels", "abstract": "Computational social science (CSS) practitioners often rely on human-labeled\ndata to fine-tune supervised text classifiers. We assess the potential for\nresearchers to augment or replace human-generated training data with surrogate\ntraining labels from generative large language models (LLMs). We introduce a\nrecommended workflow and test this LLM application by replicating 14\nclassification tasks and measuring performance. We employ a novel corpus of\nEnglish-language text classification data sets from recent CSS articles in\nhigh-impact journals. Because these data sets are stored in password-protected\narchives, our analyses are less prone to issues of contamination. For each\ntask, we compare supervised classifiers fine-tuned using GPT-4 labels against\nclassifiers fine-tuned with human annotations and against labels from GPT-4 and\nMistral-7B with few-shot in-context learning. Our findings indicate that\nsupervised classification models fine-tuned on LLM-generated labels perform\ncomparably to models fine-tuned with labels from human annotators. Fine-tuning\nmodels using LLM-generated labels can be a fast, efficient and cost-effective\nmethod of building supervised text classifiers.", "published": "2024-06-25 15:20:25", "link": "http://arxiv.org/abs/2406.17633v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Banishing LLM Hallucinations Requires Rethinking Generalization", "abstract": "Despite their powerful chat, coding, and reasoning abilities, Large Language\nModels (LLMs) frequently hallucinate. Conventional wisdom suggests that\nhallucinations are a consequence of a balance between creativity and\nfactuality, which can be mitigated, but not eliminated, by grounding the LLM in\nexternal knowledge sources. Through extensive systematic experiments, we show\nthat these traditional approaches fail to explain why LLMs hallucinate in\npractice. Specifically, we show that LLMs augmented with a massive Mixture of\nMemory Experts (MoME) can easily memorize large datasets of random numbers. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple neural networks trained to predict the next token hallucinate when\nthe training loss is above a threshold as it usually does in practice when\ntraining on internet scale data. We interpret our findings by comparing against\ntraditional retrieval methods for mitigating hallucinations. We use our\nfindings to design a first generation model for removing hallucinations --\nLamini-1 -- that stores facts in a massive mixture of millions of memory\nexperts that are retrieved dynamically.", "published": "2024-06-25 15:31:01", "link": "http://arxiv.org/abs/2406.17642v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "From Distributional to Overton Pluralism: Investigating Large Language\n  Model Alignment", "abstract": "The alignment process changes several properties of a large language model's\n(LLM's) output distribution. We analyze two aspects of post-alignment\ndistributional shift of LLM responses. First, we re-examine previously reported\nreductions in response diversity post-alignment. Our analysis suggests that an\napparent drop in the diversity of responses is largely explained by quality\ncontrol and information aggregation. Alignment suppresses irrelevant and\nunhelpful content while shifting the output distribution toward longer\nresponses that cover information spanning several responses from the base LLM,\nessentially presenting diverse information in a single response. Finding little\nevidence that alignment suppresses useful information, it is natural to ask the\nopposite question: do aligned models surface information that cannot be\nrecovered from base models? Our second investigation shows this is not the case\nand the behavior of aligned models is recoverable from base models without\nfine-tuning. A combination of in-context examples and lower-resolution semantic\nhints about response content can elicit responses from base LLMs that are as\nsimilar to alignment-tuned LLM responses as alignment-tuned LLM responses are\nto each other. Taken together, these results indicate that current alignment\ntechniques capture but do not extend the useful subset of assistant-like base\nLLM behavior, providing further evidence for the Superficial Alignment\nHypothesis. They also show that in-context alignment can go surprisingly far as\na strategy for imitating aligned LLMs without fine-tuning. Our code and data is\navailable at https://github.com/thomlake/investigating-alignment.", "published": "2024-06-25 16:32:33", "link": "http://arxiv.org/abs/2406.17692v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Find Parent then Label Children: A Two-stage Taxonomy Completion Method\n  with Pre-trained Language Model", "abstract": "Taxonomies, which organize domain concepts into hierarchical structures, are\ncrucial for building knowledge systems and downstream applications. As domain\nknowledge evolves, taxonomies need to be continuously updated to include new\nconcepts. Previous approaches have mainly focused on adding concepts to the\nleaf nodes of the existing hierarchical tree, which does not fully utilize the\ntaxonomy's knowledge and is unable to update the original taxonomy structure\n(usually involving non-leaf nodes). In this paper, we propose a two-stage\nmethod called ATTEMPT for taxonomy completion. Our method inserts new concepts\ninto the correct position by finding a parent node and labeling child nodes.\nSpecifically, by combining local nodes with prompts to generate natural\nsentences, we take advantage of pre-trained language models for\nhypernym/hyponymy recognition. Experimental results on two public datasets\n(including six domains) show that ATTEMPT performs best on both taxonomy\ncompletion and extension tasks, surpassing existing methods.", "published": "2024-06-25 17:25:02", "link": "http://arxiv.org/abs/2406.17739v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted\n  Phenomenon", "abstract": "Memorization in language models is typically treated as a homogenous\nphenomenon, neglecting the specifics of the memorized data. We instead model\nmemorization as the effect of a set of complex factors that describe each\nsample and relate it to the model and corpus. To build intuition around these\nfactors, we break memorization down into a taxonomy: recitation of highly\nduplicated sequences, reconstruction of inherently predictable sequences, and\nrecollection of sequences that are neither. We demonstrate the usefulness of\nour taxonomy by using it to construct a predictive model for memorization. By\nanalyzing dependencies and inspecting the weights of the predictive model, we\nfind that different factors influence the likelihood of memorization\ndifferently depending on the taxonomic category.", "published": "2024-06-25 17:32:16", "link": "http://arxiv.org/abs/2406.17746v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Measuring and Benchmarking Large Language Models' Capabilities to\n  Generate Persuasive Language", "abstract": "We are exposed to much information trying to influence us, such as teaser\nmessages, debates, politically framed news, and propaganda - all of which use\npersuasive language. With the recent interest in Large Language Models (LLMs),\nwe study the ability of LLMs to produce persuasive text. As opposed to prior\nwork which focuses on particular domains or types of persuasion, we conduct a\ngeneral study across various domains to measure and benchmark to what degree\nLLMs produce persuasive language - both when explicitly instructed to rewrite\ntext to be more or less persuasive and when only instructed to paraphrase. We\nconstruct the new dataset Persuasive-Pairs of pairs of a short text and its\nrewrite by an LLM to amplify or diminish persuasive language. We multi-annotate\nthe pairs on a relative scale for persuasive language: a valuable resource in\nitself, and for training a regression model to score and benchmark persuasive\nlanguage, including for new LLMs across domains. In our analysis, we find that\ndifferent 'personas' in LLaMA3's system prompt change persuasive language\nsubstantially, even when only instructed to paraphrase.", "published": "2024-06-25 17:40:47", "link": "http://arxiv.org/abs/2406.17753v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context\n  Learning", "abstract": "This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual\nin-context knowledge editing (IKE) across 53 languages, unifying three\nknowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff.\nCross-lingual KE, which requires knowledge edited in one language to generalize\nacross others while preserving unrelated knowledge, remains underexplored. To\naddress this gap, we systematically evaluate IKE under zero-shot, one-shot, and\nfew-shot setups, incorporating tailored metric-specific demonstrations. Our\nfindings reveal that model scale and demonstration alignment critically govern\ncross-lingual IKE efficacy, with larger models and tailored demonstrations\nsignificantly improving performance. Linguistic properties, particularly script\ntype, strongly influence performance variation across languages, with non-Latin\nlanguages underperforming due to issues like language confusion.", "published": "2024-06-25 17:48:56", "link": "http://arxiv.org/abs/2406.17764v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Improving Arithmetic Reasoning Ability of Large Language Models through\n  Relation Tuples, Verification and Dynamic Feedback", "abstract": "Current representations used in reasoning steps of large language models can\nmostly be categorized into two main types: (1) natural language, which is\ndifficult to verify; and (2) non-natural language, usually programming code,\nwhich is difficult for people who are unfamiliar with coding to read. In this\npaper, we propose to use a semi-structured form to represent reasoning steps of\nlarge language models. Specifically, we use relation tuples, which are not only\nhuman-readable but also machine-friendly and easier to verify than natural\nlanguage. We implement a framework that includes three main components: (1)\nintroducing relation tuples into the reasoning steps of large language models;\n(2) implementing an automatic verification process of reasoning steps with a\nlocal code interpreter based on relation tuples; and (3) integrating a simple\nand effective dynamic feedback mechanism, which we found helpful for\nself-improvement of large language models. The experimental results on various\narithmetic datasets demonstrate the effectiveness of our method in improving\nthe arithmetic reasoning ability of large language models. The source code is\navailable at https://github.com/gpgg/art.", "published": "2024-06-25 18:21:00", "link": "http://arxiv.org/abs/2406.17873v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mapping the Past: Geographically Linking an Early 20th Century Swedish\n  Encyclopedia with Wikidata", "abstract": "In this paper, we describe the extraction of all the location entries from a\nprominent Swedish encyclopedia from the early 20th century, the \\textit{Nordisk\nFamiljebok} `Nordic Family Book.' We focused on the second edition called\n\\textit{Uggleupplagan}, which comprises 38 volumes and over 182,000 articles.\nThis makes it one of the most extensive Swedish encyclopedias. Using a\nclassifier, we first determined the category of the entries. We found that\napproximately 22 percent of them were locations. We applied a named entity\nrecognition to these entries and we linked them to Wikidata. Wikidata enabled\nus to extract their precise geographic locations resulting in almost 18,000\nvalid coordinates. We then analyzed the distribution of these locations and the\nentry selection process. It showed a higher density within Sweden, Germany, and\nthe United Kingdom. The paper sheds light on the selection and representation\nof geographic information in the \\textit{Nordisk Familjebok}, providing\ninsights into historical and societal perspectives. It also paves the way for\nfuture investigations into entry selection in different time periods and\ncomparative analyses among various encyclopedias.", "published": "2024-06-25 19:34:00", "link": "http://arxiv.org/abs/2406.17903v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Encourage or Inhibit Monosemanticity? Revisit Monosemanticity from a\n  Feature Decorrelation Perspective", "abstract": "To better interpret the intrinsic mechanism of large language models (LLMs),\nrecent studies focus on monosemanticity on its basic units. A monosemantic\nneuron is dedicated to a single and specific concept, which forms a one-to-one\ncorrelation between neurons and concepts. Despite extensive research in\nmonosemanticity probing, it remains unclear whether monosemanticity is\nbeneficial or harmful to model capacity. To explore this question, we revisit\nmonosemanticity from the feature decorrelation perspective and advocate for its\nencouragement. We experimentally observe that the current conclusion by\nwang2024learning, which suggests that decreasing monosemanticity enhances model\nperformance, does not hold when the model changes. Instead, we demonstrate that\nmonosemanticity consistently exhibits a positive correlation with model\ncapacity, in the preference alignment process. Consequently, we apply feature\ncorrelation as a proxy for monosemanticity and incorporate a feature\ndecorrelation regularizer into the dynamic preference optimization process. The\nexperiments show that our method not only enhances representation diversity and\nactivation sparsity but also improves preference alignment performance.", "published": "2024-06-25 22:51:08", "link": "http://arxiv.org/abs/2406.17969v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Evaluating Fairness in Large Vision-Language Models Across Diverse\n  Demographic Attributes and Prompts", "abstract": "Large vision-language models (LVLMs) have recently achieved significant\nprogress, demonstrating strong capabilities in open-world visual understanding.\nHowever, it is not yet clear how LVLMs address demographic biases in real life,\nespecially the disparities across attributes such as gender, skin tone, age and\nrace. In this paper, We empirically investigate visual fairness in several\nmainstream LVLMs by auditing their performance disparities across demographic\nattributes using public fairness benchmark datasets (e.g., FACET, UTKFace). Our\nfairness evaluation framework employs direct and single-choice question prompt\non visual question-answering/classification tasks. Despite advancements in\nvisual understanding, our zero-shot prompting results show that both\nopen-source and closed-source LVLMs continue to exhibit fairness issues across\ndifferent prompts and demographic groups. Furthermore, we propose a potential\nmulti-modal Chain-of-thought (CoT) based strategy for bias mitigation,\napplicable to both open-source and closed-source LVLMs. This approach enhances\ntransparency and offers a scalable solution for addressing fairness, providing\na solid foundation for future bias reduction efforts.", "published": "2024-06-25 23:11:39", "link": "http://arxiv.org/abs/2406.17974v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "MoE-CT: A Novel Approach For Large Language Models Training With\n  Resistance To Catastrophic Forgetting", "abstract": "The advent of large language models (LLMs) has predominantly catered to\nhigh-resource languages, leaving a disparity in performance for low-resource\nlanguages. Conventional Continual Training (CT) approaches to bridge this gap\noften undermine a model's original linguistic proficiency when expanding to\nmultilingual contexts. Addressing this issue, we introduce a novel MoE-CT\narchitecture, a paradigm that innovatively separates the base model's learning\nfrom the multilingual expansion process. Our design freezes the original LLM\nparameters, thus safeguarding its performance in high-resource languages, while\nan appended MoE module, trained on diverse language datasets, augments\nlow-resource language proficiency. Our approach significantly outperforms\nconventional CT methods, as evidenced by our experiments, which show marked\nimprovements in multilingual benchmarks without sacrificing the model's\noriginal language performance. Moreover, our MoE-CT framework demonstrates\nenhanced resistance to forgetting and superior transfer learning capabilities.\nBy preserving the base model's integrity and focusing on strategic parameter\nexpansion, our methodology advances multilingual language modeling and\nrepresents a significant step forward for low-resource language inclusion in\nLLMs, indicating a fruitful direction for future research in language\ntechnologies.", "published": "2024-06-25 11:03:45", "link": "http://arxiv.org/abs/2407.00875v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discrete Diffusion Language Model for Efficient Text Summarization", "abstract": "While diffusion models excel at conditional generating high-quality images,\nprior works in discrete diffusion models were not evaluated on conditional\nlong-text generation. In this work, we address the limitations of prior\ndiscrete diffusion models for conditional long-text generation, particularly in\nlong sequence-to-sequence tasks such as abstractive summarization. Despite fast\ndecoding speeds compared to autoregressive methods, previous diffusion models\nfailed on the abstractive summarization task due to the incompatibility between\nthe backbone architectures and the random noising process. To overcome these\nchallenges, we introduce a novel semantic-aware noising process that enables\nTransformer backbones to handle long sequences effectively. Additionally, we\npropose CrossMamba, an adaptation of the Mamba model to the encoder-decoder\nparadigm, which integrates seamlessly with the random absorbing noising\nprocess. Our approaches achieve state-of-the-art performance on three benchmark\nsummarization datasets: Gigaword, CNN/DailyMail, and Arxiv, outperforming\nexisting discrete diffusion models on ROUGE metrics as well as possessing much\nfaster speed in inference compared to autoregressive models.", "published": "2024-06-25 09:55:22", "link": "http://arxiv.org/abs/2407.10998v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house\n  Criteria by Criteria Division and Zero-shot Plus Few-shot", "abstract": "With the rapid development of large language models (LLM), the evaluation of\nLLM becomes increasingly important. Measuring text generation tasks such as\nsummarization and article creation is very difficult. Especially in specific\napplication domains (e.g., to-business or to-customer service), in-house\nevaluation criteria have to meet not only general standards (correctness,\nhelpfulness and creativity, etc.) but also specific needs of customers and\nbusiness security requirements at the same time, making the evaluation more\ndifficult. So far, the evaluation of LLM in business scenarios has mainly\nrelied on manual, which is expensive and time-consuming. In this paper, we\npropose a model-based evaluation method: TALEC, which allows users to flexibly\nset their own evaluation criteria, and uses in-context learning (ICL) to teach\njudge model these in-house criteria. In addition, we try combining zero-shot\nand few-shot to make the judge model focus on more information. We also propose\na prompt paradigm and an engineering approach to adjust and iterate the shots\n,helping judge model to better understand the complex criteria. We then compare\nfine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC\ndemonstrates a strong capability to accurately reflect human preferences and\nachieves a correlation of over 80% with human judgments, outperforming even the\ninter-human correlation in some tasks. The code is released in\nhttps://github.com/zlkqz/auto_eval", "published": "2024-06-25 10:02:42", "link": "http://arxiv.org/abs/2407.10999v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative AI Systems: A Systems-based Perspective on Generative AI", "abstract": "Large Language Models (LLMs) have revolutionized AI systems by enabling\ncommunication with machines using natural language. Recent developments in\nGenerative AI (GenAI) like Vision-Language Models (GPT-4V) and Gemini have\nshown great promise in using LLMs as multimodal systems. This new research line\nresults in building Generative AI systems, GenAISys for short, that are capable\nof multimodal processing and content creation, as well as decision-making.\nGenAISys use natural language as a communication means and modality encoders as\nI/O interfaces for processing various data sources. They are also equipped with\ndatabases and external specialized tools, communicating with the system through\na module for information retrieval and storage. This paper aims to explore and\nstate new research directions in Generative AI Systems, including how to design\nGenAISys (compositionality, reliability, verifiability), build and train them,\nand what can be learned from the system-based perspective. Cross-disciplinary\napproaches are needed to answer open questions about the inner workings of\nGenAI systems.", "published": "2024-06-25 12:51:47", "link": "http://arxiv.org/abs/2407.11001v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation\n  Systems", "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural\npattern for incorporating domain-specific knowledge into user-facing chat\napplications powered by Large Language Models (LLMs). RAG systems are\ncharacterized by (1) a document retriever that queries a domain-specific corpus\nfor context information relevant to an input query, and (2) an LLM that\ngenerates a response based on the provided query and context. However,\ncomprehensive evaluation of RAG systems remains a challenge due to the lack of\nunified evaluation criteria and annotated datasets. In response, we introduce\nRAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k\nexamples. It covers five unique industry-specific domains and various RAG task\ntypes. RAGBench examples are sourced from industry corpora such as user\nmanuals, making it particularly relevant for industry applications. Further, we\nformalize the TRACe evaluation framework: a set of explainable and actionable\nRAG evaluation metrics applicable across all RAG domains. We release the\nlabeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.\nRAGBench explainable labels facilitate holistic evaluation of RAG systems,\nenabling actionable feedback for continuous improvement of production\napplications. Thorough extensive benchmarking, we find that LLM-based RAG\nevaluation methods struggle to compete with a finetuned RoBERTa model on the\nRAG evaluation task. We identify areas where existing approaches fall short and\npropose the adoption of RAGBench with TRACe towards advancing the state of RAG\nevaluation systems.", "published": "2024-06-25 20:23:15", "link": "http://arxiv.org/abs/2407.11005v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Panacea: A foundation model for clinical trial search, summarization,\n  design, and recruitment", "abstract": "Clinical trials are fundamental in developing new drugs, medical devices, and\ntreatments. However, they are often time-consuming and have low success rates.\nAlthough there have been initial attempts to create large language models\n(LLMs) for clinical trial design and patient-trial matching, these models\nremain task-specific and not adaptable to diverse clinical trial tasks. To\naddress this challenge, we propose a clinical trial foundation model named\nPanacea, designed to handle multiple tasks, including trial search, trial\nsummarization, trial design, and patient-trial matching. We also assemble a\nlarge-scale dataset, named TrialAlign, of 793,279 trial documents and 1,113,207\ntrial-related scientific papers, to infuse clinical knowledge into the model by\npre-training. We further curate TrialInstruct, which has 200,866 of instruction\ndata for fine-tuning. These resources enable Panacea to be widely applicable\nfor a range of clinical trial tasks based on user requirements.\n  We evaluated Panacea on a new benchmark, named TrialPanorama, which covers\neight clinical trial tasks. Our method performed the best on seven of the eight\ntasks compared to six cutting-edge generic or medicine-specific LLMs.\nSpecifically, Panacea showed great potential to collaborate with human experts\nin crafting the design of eligibility criteria, study arms, and outcome\nmeasures, in multi-round conversations. In addition, Panacea achieved 14.42%\nimprovement in patient-trial matching, 41.78% to 52.02% improvement in trial\nsearch, and consistently ranked at the top for five aspects of trial\nsummarization. Our approach demonstrates the effectiveness of Panacea in\nclinical trials and establishes a comprehensive resource, including training\ndata, model, and benchmark, for developing clinical trial foundation models,\npaving the path for AI-based clinical trial development.", "published": "2024-06-25 21:29:25", "link": "http://arxiv.org/abs/2407.11007v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CharED: Character-wise Ensemble Decoding for Large Language Models", "abstract": "Large language models (LLMs) have shown remarkable potential for problem\nsolving, with open source models achieving increasingly impressive performance\non benchmarks measuring areas from logical reasoning to mathematical ability.\nEnsembling models can further improve capabilities across a variety of domains.\nHowever, conventional methods of combining models at inference time such as\nshallow fusion necessitate a shared vocabulary and tokenization, and\nalternatives like fine-tuning for domain-specific performance are both time\nconsuming and computationally expensive. We therefore present an inference-time\nensembling algorithm aimed at \"averaging\" outputs from multiple LLMs and\nillustrate its improved performance across multiple domains compared to its\nconstituent models alone. Character-wise ensemble decoding, CharED, finds the\nmarginal distribution of each character for an individual model and performs a\nweighted average to generate an output, character by character. In coding,\nmath, and toxicity benchmarks, we find our proposed model able to combine\ncomplimentary strengths of multiple LLMs, regardless of vocabulary,\ntokenization, or model size.", "published": "2024-06-25 22:35:07", "link": "http://arxiv.org/abs/2407.11009v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unlocking Continual Learning Abilities in Language Models", "abstract": "Language models (LMs) exhibit impressive performance and generalization\ncapabilities. However, LMs struggle with the persistent challenge of\ncatastrophic forgetting, which undermines their long-term sustainability in\ncontinual learning (CL). Existing approaches usually address the issue by\nincorporating old task data or task-wise inductive bias into LMs. However, old\ndata and accurate task information are often unavailable or costly to collect,\nhindering the availability of current CL approaches for LMs. To address this\nlimitation, we introduce $\\textbf{MIGU}$ ($\\textbf{M}$agn$\\textbf{I}$tude-based\n$\\textbf{G}$radient $\\textbf{U}$pdating for continual learning), a\nrehearsal-free and task-label-free method that only updates the model\nparameters with large magnitudes of output in LMs' linear layers. MIGU is based\non our observation that the L1-normalized magnitude distribution of the output\nin LMs' linear layers is different when the LM models deal with different task\ndata. By imposing this simple constraint on the gradient update process, we can\nleverage the inherent behaviors of LMs, thereby unlocking their innate CL\nabilities. Our experiments demonstrate that MIGU is universally applicable to\nall three LM architectures (T5, RoBERTa, and Llama2), delivering\nstate-of-the-art or on-par performance across continual finetuning and\ncontinual pre-training settings on four CL benchmarks. For example, MIGU brings\na 15.2% average accuracy improvement over conventional parameter-efficient\nfinetuning baselines in a 15-task CL benchmark. MIGU can also seamlessly\nintegrate with all three existing CL types to further enhance performance. Code\nis available at https://github.com/wenyudu/MIGU.", "published": "2024-06-25 03:24:06", "link": "http://arxiv.org/abs/2406.17245v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Leveraging Parameter-Efficient Transfer Learning for Multi-Lingual\n  Text-to-Speech Adaptation", "abstract": "Different languages have distinct phonetic systems and vary in their prosodic\nfeatures making it challenging to develop a Text-to-Speech (TTS) model that can\neffectively synthesise speech in multilingual settings. Furthermore, TTS\narchitecture needs to be both efficient enough to capture nuances in multiple\nlanguages and efficient enough to be practical for deployment. The standard\napproach is to build transformer based model such as SpeechT5 and train it on\nlarge multilingual dataset. As the size of these models grow the conventional\nfine-tuning for adapting these model becomes impractical due to heavy\ncomputational cost. In this paper, we proposes to integrate parameter-efficient\ntransfer learning (PETL) methods such as adapters and hypernetwork with TTS\narchitecture for multilingual speech synthesis. Notably, in our experiments\nPETL methods able to achieve comparable or even better performance compared to\nfull fine-tuning with only $\\sim$2.5\\% tunable parameters.The code and samples\nare available at: https://anonymous.4open.science/r/multilingualTTS-BA4C.", "published": "2024-06-25 03:50:54", "link": "http://arxiv.org/abs/2406.17257v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "AG-LSEC: Audio Grounded Lexical Speaker Error Correction", "abstract": "Speaker Diarization (SD) systems are typically audio-based and operate\nindependently of the ASR system in traditional speech transcription pipelines\nand can have speaker errors due to SD and/or ASR reconciliation, especially\naround speaker turns and regions of speech overlap. To reduce these errors, a\nLexical Speaker Error Correction (LSEC), in which an external language model\nprovides lexical information to correct the speaker errors, was recently\nproposed. Though the approach achieves good Word Diarization error rate (WDER)\nimprovements, it does not use any additional acoustic information and is prone\nto miscorrections. In this paper, we propose to enhance and acoustically ground\nthe LSEC system with speaker scores directly derived from the existing SD\npipeline. This approach achieves significant relative WDER reductions in the\nrange of 25-40% over the audio-based SD, ASR system and beats the LSEC system\nby 15-25% relative on RT03-CTS, Callhome American English and Fisher datasets.", "published": "2024-06-25 04:20:49", "link": "http://arxiv.org/abs/2406.17266v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Delving into the Utilisation of ChatGPT in Scientific Publications in\n  Astronomy", "abstract": "Rapid progress in the capabilities of machine learning approaches in natural\nlanguage processing has culminated in the rise of large language models over\nthe last two years. Recent works have shown unprecedented adoption of these for\nacademic writing, especially in some fields, but their pervasiveness in\nastronomy has not been studied sufficiently. To remedy this, we extract words\nthat ChatGPT uses more often than humans when generating academic text and\nsearch a total of 1 million articles for them. This way, we assess the\nfrequency of word occurrence in published works in astronomy tracked by the\nNASA Astrophysics Data System since 2000. We then perform a statistical\nanalysis of the occurrences. We identify a list of words favoured by ChatGPT\nand find a statistically significant increase for these words against a control\ngroup in 2024, which matches the trend in other disciplines. These results\nsuggest a widespread adoption of these models in the writing of astronomy\npapers. We encourage organisations, publishers, and researchers to work\ntogether to identify ethical and pragmatic guidelines to maximise the benefits\nof these systems while maintaining scientific rigour.", "published": "2024-06-25 07:15:10", "link": "http://arxiv.org/abs/2406.17324v2", "categories": ["cs.CL", "astro-ph.IM", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Leveraging Synthetic Audio Data for End-to-End Low-Resource Speech\n  Translation", "abstract": "This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2024) for Irish-to-English speech\ntranslation. We built end-to-end systems based on Whisper, and employed a\nnumber of data augmentation techniques, such as speech back-translation and\nnoise augmentation. We investigate the effect of using synthetic audio data and\ndiscuss several methods for enriching signal diversity.", "published": "2024-06-25 08:26:41", "link": "http://arxiv.org/abs/2406.17363v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing\n  LLMs Beyond Integer Bit-Levels", "abstract": "We present a simple meta quantization approach that quantizes different\nlayers of a large language model (LLM) at different bit levels, and is\nindependent of the underlying quantization technique. Specifically, we quantize\nthe most important layers to higher bit precision and less important layers to\nlower bits. We propose two effective strategies to measure the importance of\nlayers within LLMs: the first measures the importance of a layer based on how\ndifferent its output embeddings are from the input embeddings (higher is\nbetter); the second estimates the importance of a layer using the number of\nlayer weights that are much larger than average (smaller is better). We show\nthat quantizing different layers at varying bits according to our importance\nscores results in minimal performance drop with a far more compressed model\nsize. Finally, we present several practical key takeaways from our variable\nlayer-wise quantization experiments: (a) LLM performance under variable\nquantization remains close to the original model until 25-50% of layers are\nmoved in lower quantization using our proposed ordering but only until 5-10% if\nmoved using no specific ordering; (b) Adding layer importance to inherently\ndynamic quantization techniques can further improve their performance, showing\nthat our approach is complementary to other dynamic quantization methods; (c)\nQuantizing LLMs to lower bits performs substantially better than pruning unless\nextreme quantization (2-bit) is used; and (d) Layer-wise quantization to lower\nbits works better in the case of larger LLMs with more layers compared to\nsmaller LLMs with fewer layers. Our code is publicly available at\nhttps://github.com/RazvanDu/LayerwiseQuant/.", "published": "2024-06-25 09:37:15", "link": "http://arxiv.org/abs/2406.17415v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.0"], "primary_category": "cs.CL"}
{"title": "Towards Probing Speech-Specific Risks in Large Multimodal Models: A\n  Taxonomy, Benchmark, and Insights", "abstract": "Large Multimodal Models (LMMs) have achieved great success recently,\ndemonstrating a strong capability to understand multimodal information and to\ninteract with human users. Despite the progress made, the challenge of\ndetecting high-risk interactions in multimodal settings, and in particular in\nspeech modality, remains largely unexplored. Conventional research on risk for\nspeech modality primarily emphasises the content (e.g., what is captured as\ntranscription). However, in speech-based interactions, paralinguistic cues in\naudio can significantly alter the intended meaning behind utterances. In this\nwork, we propose a speech-specific risk taxonomy, covering 8 risk categories\nunder hostility (malicious sarcasm and threats), malicious imitation (age,\ngender, ethnicity), and stereotypical biases (age, gender, ethnicity). Based on\nthe taxonomy, we create a small-scale dataset for evaluating current LMMs\ncapability in detecting these categories of risk. We observe even the latest\nmodels remain ineffective to detect various paralinguistic-specific risks in\nspeech (e.g., Gemini 1.5 Pro is performing only slightly above random\nbaseline). Warning: this paper contains biased and offensive examples.", "published": "2024-06-25 10:08:45", "link": "http://arxiv.org/abs/2406.17430v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Understand DL-Lite Ontologies? An Empirical\n  Study", "abstract": "Large language models (LLMs) have shown significant achievements in solving a\nwide range of tasks. Recently, LLMs' capability to store, retrieve and infer\nwith symbolic knowledge has drawn a great deal of attention, showing their\npotential to understand structured information. However, it is not yet known\nwhether LLMs can understand Description Logic (DL) ontologies. In this work, we\nempirically analyze the LLMs' capability of understanding DL-Lite ontologies\ncovering 6 representative tasks from syntactic and semantic aspects. With\nextensive experiments, we demonstrate both the effectiveness and limitations of\nLLMs in understanding DL-Lite ontologies. We find that LLMs can understand\nformal syntax and model-theoretic semantics of concepts and roles. However,\nLLMs struggle with understanding TBox NI transitivity and handling ontologies\nwith large ABoxes. We hope that our experiments and analyses provide more\ninsights into LLMs and inspire to build more faithful knowledge engineering\nsolutions.", "published": "2024-06-25 13:16:34", "link": "http://arxiv.org/abs/2406.17532v2", "categories": ["cs.AI", "cs.CL", "cs.LO"], "primary_category": "cs.AI"}
{"title": "CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization", "abstract": "Large language models (LLMs) have recently demonstrated remarkable\nperformance across diverse language tasks. But their deployment is often\nconstrained by their substantial computational and storage requirements.\nQuantization has emerged as a key technique for addressing this challenge,\nenabling the compression of large models with minimal impact on performance.\nThe recent GPTQ algorithm, a post-training quantization (PTQ) method, has\nproven highly effective for compressing LLMs, sparking a wave of research that\nleverages GPTQ as a core component. Recognizing the pivotal role of GPTQ in the\nPTQ landscape, we introduce CDQuant, a simple and scalable alternative to GPTQ\nwith improved performance. CDQuant uses greedy coordinate descent to minimize\nthe layer-wise reconstruction loss to achieve high-quality quantized weights.\nOur algorithm is easy to implement and scales efficiently to models with\nhundreds of billions of parameters. We perform extensive evaluation on Gemma,\nand PaLM2 model families, and demonstrate that CDQuant consistently outperforms\nGPTQ in 2-4 bit weight quantization. Moreover, CDQuant improves the performance\nof state-of-the-art PTQ techniques such as QuIP and FrameQuant when used as a\nreplacement for their GPTQ component, resulting in further gains in quality.", "published": "2024-06-25 13:29:14", "link": "http://arxiv.org/abs/2406.17542v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-property Steering of Large Language Models with Dynamic Activation\n  Composition", "abstract": "Activation steering methods were shown to be effective in conditioning\nlanguage model generation by additively intervening over models' intermediate\nrepresentations. However, the evaluation of these techniques has so far been\nlimited to single conditioning properties and synthetic settings. In this work,\nwe conduct a comprehensive evaluation of various activation steering\nstrategies, highlighting the property-dependent nature of optimal parameters to\nensure a robust effect throughout generation. To address this issue, we propose\nDynamic Activation Composition, an information-theoretic approach to modulate\nthe steering intensity of one or more properties throughout generation. Our\nexperiments on multi-property steering show that our method successfully\nmaintains high conditioning while minimizing the impact of conditioning on\ngeneration fluency.", "published": "2024-06-25 14:00:42", "link": "http://arxiv.org/abs/2406.17563v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Building an End-to-End Multilingual Automatic Lyrics\n  Transcription Model", "abstract": "Multilingual automatic lyrics transcription (ALT) is a challenging task due\nto the limited availability of labelled data and the challenges introduced by\nsinging, compared to multilingual automatic speech recognition. Although some\nmultilingual singing datasets have been released recently, English continues to\ndominate these collections. Multilingual ALT remains underexplored due to the\nscale of data and annotation quality. In this paper, we aim to create a\nmultilingual ALT system with available datasets. Inspired by architectures that\nhave been proven effective for English ALT, we adapt these techniques to the\nmultilingual scenario by expanding the target vocabulary set. We then evaluate\nthe performance of the multilingual model in comparison to its monolingual\ncounterparts. Additionally, we explore various conditioning methods to\nincorporate language information into the model. We apply analysis by language\nand combine it with the language classification performance. Our findings\nreveal that the multilingual model performs consistently better than the\nmonolingual models trained on the language subsets. Furthermore, we demonstrate\nthat incorporating language information significantly enhances performance.", "published": "2024-06-25 15:02:32", "link": "http://arxiv.org/abs/2406.17618v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP", "abstract": "Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering three\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? 3. How do these gap\nreduction approaches affect the downstream performance? We design AlignCLIP, in\norder to answer these questions and through extensive experiments, we show that\nAlignCLIP achieves noticeable enhancements in the cross-modal alignment of the\nembeddings, and thereby, reduces the modality gap, while improving the\nperformance across several zero-shot and fine-tuning downstream evaluations.", "published": "2024-06-25 15:24:02", "link": "http://arxiv.org/abs/2406.17639v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "ELIZA Reinterpreted: The world's first chatbot was not intended as a\n  chatbot at all", "abstract": "ELIZA, often considered the world's first chatbot, was written by Joseph\nWeizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot,\nbut rather to build a platform for research into human-machine conversation and\nthe important cognitive processes of interpretation and misinterpretation. His\npurpose was obscured by ELIZA's fame, resulting in large part from the\nfortuitous timing of it's creation, and it's escape into the wild. In this\npaper I provide a rich historical context for ELIZA's creation, demonstrating\nthat ELIZA arose from the intersection of some of the central threads in the\ntechnical history of AI. I also briefly discuss how ELIZA escaped into the\nworld, and how its accidental escape, along with several coincidental turns of\nthe programming language screws, led both to the misapprehension that ELIZA was\nintended as a chatbot, and to the loss of the original ELIZA to history for\nover 50 years.", "published": "2024-06-25 15:41:40", "link": "http://arxiv.org/abs/2406.17650v2", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.AI"}
{"title": "LLM-ARC: Enhancing LLMs with an Automated Reasoning Critic", "abstract": "We introduce LLM-ARC, a neuro-symbolic framework designed to enhance the\nlogical reasoning capabilities of Large Language Models (LLMs), by combining\nthem with an Automated Reasoning Critic (ARC). LLM-ARC employs an Actor-Critic\nmethod where the LLM Actor generates declarative logic programs along with\ntests for semantic correctness, while the Automated Reasoning Critic evaluates\nthe code, runs the tests and provides feedback on test failures for iterative\nrefinement. Implemented using Answer Set Programming (ASP), LLM-ARC achieves a\nnew state-of-the-art accuracy of 88.32% on the FOLIO benchmark which tests\ncomplex logical reasoning capabilities. Our experiments demonstrate significant\nimprovements over LLM-only baselines, highlighting the importance of logic test\ngeneration and iterative self-refinement. We achieve our best result using a\nfully automated self-supervised training loop where the Actor is trained on\nend-to-end dialog traces with Critic feedback. We discuss potential\nenhancements and provide a detailed error analysis, showcasing the robustness\nand efficacy of LLM-ARC for complex natural language reasoning tasks.", "published": "2024-06-25 15:52:15", "link": "http://arxiv.org/abs/2406.17663v2", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "This Paper Had the Smartest Reviewers -- Flattery Detection Utilising an\n  Audio-Textual Transformer-Based Approach", "abstract": "Flattery is an important aspect of human communication that facilitates\nsocial bonding, shapes perceptions, and influences behavior through strategic\ncompliments and praise, leveraging the power of speech to build rapport\neffectively. Its automatic detection can thus enhance the naturalness of\nhuman-AI interactions. To meet this need, we present a novel audio textual\ndataset comprising 20 hours of speech and train machine learning models for\nautomatic flattery detection. In particular, we employ pretrained AST,\nWav2Vec2, and Whisper models for the speech modality, and Whisper TTS models\ncombined with a RoBERTa text classifier for the textual modality. Subsequently,\nwe build a multimodal classifier by combining text and audio representations.\nEvaluation on unseen test data demonstrates promising results, with Unweighted\nAverage Recall scores reaching 82.46% in audio-only experiments, 85.97% in\ntext-only experiments, and 87.16% using a multimodal approach.", "published": "2024-06-25 15:57:02", "link": "http://arxiv.org/abs/2406.17667v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model", "abstract": "Large language models (LLMs) show amazing performance on many domain-specific\ntasks after fine-tuning with some appropriate data. However, many\ndomain-specific data are privately distributed across multiple owners. Thus,\nthis dilemma raises the interest in how to perform LLM fine-tuning in federated\nlearning (FL). However, confronted with limited computation and communication\ncapacities, FL clients struggle to fine-tune an LLM effectively. To this end,\nwe introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL.\nSpecifically, our method involves the server generating a compressed LLM and\naligning its performance with the full model. Subsequently, the clients\nfine-tune a lightweight yet important part of the compressed model, referred to\nas an adapter. Notice that as the server has no access to the private data\nowned by the clients, the data used for alignment by the server has a different\ndistribution from the one used for fine-tuning by clients. We formulate the\nproblem into a bi-level optimization problem to minimize the negative effect of\ndata discrepancy and derive the updating rules for the server and clients. We\nconduct extensive experiments on LLaMA-2, empirically showing that the adapter\nhas exceptional performance when reintegrated into the global LLM. The results\nalso indicate that the proposed FedBiOT significantly reduces resource\nconsumption compared to existing benchmarks, all while achieving comparable\nperformance levels.", "published": "2024-06-25 16:45:47", "link": "http://arxiv.org/abs/2406.17706v1", "categories": ["cs.LG", "cs.CL", "cs.DC"], "primary_category": "cs.LG"}
{"title": "LLM Targeted Underperformance Disproportionately Impacts Vulnerable\n  Users", "abstract": "While state-of-the-art Large Language Models (LLMs) have shown impressive\nperformance on many tasks, there has been extensive research on undesirable\nmodel behavior such as hallucinations and bias. In this work, we investigate\nhow the quality of LLM responses changes in terms of information accuracy,\ntruthfulness, and refusals depending on three user traits: English proficiency,\neducation level, and country of origin. We present extensive experimentation on\nthree state-of-the-art LLMs and two different datasets targeting truthfulness\nand factuality. Our findings suggest that undesirable behaviors in\nstate-of-the-art LLMs occur disproportionately more for users with lower\nEnglish proficiency, of lower education status, and originating from outside\nthe US, rendering these models unreliable sources of information towards their\nmost vulnerable users.", "published": "2024-06-25 17:24:07", "link": "http://arxiv.org/abs/2406.17737v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CaLMQA: Exploring culturally specific long-form question answering\n  across 23 languages", "abstract": "Large language models (LLMs) are used for long-form question answering\n(LFQA), which requires them to generate paragraph-length answers to complex\nquestions. While LFQA has been well-studied in English, this research has not\nbeen extended to other languages. To bridge this gap, we introduce CaLMQA, a\ncollection of 1.5K complex culturally specific questions spanning 23 languages\nand 51 culturally agnostic questions translated from English into 22 other\nlanguages. We define culturally specific questions as those uniquely or more\nlikely to be asked by people from cultures associated with the question's\nlanguage. We collect naturally-occurring questions from community web forums\nand hire native speakers to write questions to cover under-resourced,\nrarely-studied languages such as Fijian and Kirundi. Our dataset contains\ndiverse, complex questions that reflect cultural topics (e.g. traditions, laws,\nnews) and the language usage of native speakers. We automatically evaluate a\nsuite of open- and closed-source models on CaLMQA by detecting incorrect\nlanguage and token repetitions in answers, and observe that the quality of\nLLM-generated answers degrades significantly for some low-resource languages.\nLastly, we perform human evaluation on a subset of models and languages. Manual\nevaluation reveals that model performance is significantly worse for culturally\nspecific questions than for culturally agnostic questions. Our findings\nhighlight the need for further research in non-English LFQA and provide an\nevaluation framework.", "published": "2024-06-25 17:45:26", "link": "http://arxiv.org/abs/2406.17761v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic speech recognition for the Nepali language using CNN,\n  bidirectional LSTM and ResNet", "abstract": "This paper presents an end-to-end deep learning model for Automatic Speech\nRecognition (ASR) that transcribes Nepali speech to text. The model was trained\nand tested on the OpenSLR (audio, text) dataset. The majority of the audio\ndataset have silent gaps at both ends which are clipped during dataset\npreprocessing for a more uniform mapping of audio frames and their\ncorresponding texts. Mel Frequency Cepstral Coefficients (MFCCs) are used as\naudio features to feed into the model. The model having Bidirectional LSTM\npaired with ResNet and one-dimensional CNN produces the best results for this\ndataset out of all the models (neural networks with variations of LSTM, GRU,\nCNN, and ResNet) that have been trained so far. This novel model uses\nConnectionist Temporal Classification (CTC) function for loss calculation\nduring training and CTC beam search decoding for predicting characters as the\nmost likely sequence of Nepali text. On the test dataset, the character error\nrate (CER) of 17.06 percent has been achieved. The source code is available at:\nhttps://github.com/manishdhakal/ASR-Nepali-using-CNN-BiLSTM-ResNet.", "published": "2024-06-25 12:14:01", "link": "http://arxiv.org/abs/2406.17825v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CTBench: A Comprehensive Benchmark for Evaluating Language Model\n  Capabilities in Clinical Trial Design", "abstract": "CTBench is introduced as a benchmark to assess language models (LMs) in\naiding clinical study design. Given study-specific metadata, CTBench evaluates\nAI models' ability to determine the baseline features of a clinical trial (CT),\nwhich include demographic and relevant features collected at the trial's start\nfrom all participants. These baseline features, typically presented in CT\npublications (often as Table 1), are crucial for characterizing study cohorts\nand validating results. Baseline features, including confounders and\ncovariates, are also necessary for accurate treatment effect estimation in\nstudies involving observational data. CTBench consists of two datasets:\n\"CT-Repo,\" containing baseline features from 1,690 clinical trials sourced from\nclinicaltrials.gov, and \"CT-Pub,\" a subset of 100 trials with more\ncomprehensive baseline features gathered from relevant publications. Two\nLM-based evaluation methods are developed to compare the actual baseline\nfeature lists against LM-generated responses. \"ListMatch-LM\" and\n\"ListMatch-BERT\" use GPT-4o and BERT scores (at various thresholds),\nrespectively, for evaluation. To establish baseline results, advanced prompt\nengineering techniques using LLaMa3-70B-Instruct and GPT-4o in zero-shot and\nthree-shot learning settings are applied to generate potential baseline\nfeatures. The performance of GPT-4o as an evaluator is validated through\nhuman-in-the-loop evaluations on the CT-Pub dataset, where clinical experts\nconfirm matches between actual and LM-generated features. The results highlight\na promising direction with significant potential for improvement, positioning\nCTBench as a useful tool for advancing research on AI in CT design and\npotentially enhancing the efficacy and robustness of CTs.", "published": "2024-06-25 18:52:48", "link": "http://arxiv.org/abs/2406.17888v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FASA: a Flexible and Automatic Speech Aligner for Extracting\n  High-quality Aligned Children Speech Data", "abstract": "Automatic Speech Recognition (ASR) for adults' speeches has made significant\nprogress by employing deep neural network (DNN) models recently, but\nimprovement in children's speech is still unsatisfactory due to children's\nspeech's distinct characteristics. DNN models pre-trained on adult data often\nstruggle in generalizing children's speeches with fine tuning because of the\nlack of high-quality aligned children's speeches. When generating datasets,\nhuman annotations are not scalable, and existing forced-alignment tools are not\nusable as they make impractical assumptions about the quality of the input\ntranscriptions. To address these challenges, we propose a new forced-alignment\ntool, FASA, as a flexible and automatic speech aligner to extract high-quality\naligned children's speech data from many of the existing noisy children's\nspeech data. We demonstrate its usage on the CHILDES dataset and show that FASA\ncan improve data quality by 13.6$\\times$ over human annotations.", "published": "2024-06-25 20:37:16", "link": "http://arxiv.org/abs/2406.17926v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Sequential Editing for Lifelong Training of Speech Recognition Models", "abstract": "Automatic Speech Recognition (ASR) traditionally assumes known domains, but\nadding data from a new domain raises concerns about computational\ninefficiencies linked to retraining models on both existing and new domains.\nFine-tuning solely on new domain risks Catastrophic Forgetting (CF). To address\nthis, Lifelong Learning (LLL) algorithms have been proposed for ASR. Prior\nresearch has explored techniques such as Elastic Weight Consolidation,\nKnowledge Distillation, and Replay, all of which necessitate either additional\nparameters or access to prior domain data. We propose Sequential Model Editing\nas a novel method to continually learn new domains in ASR systems. Different\nthan previous methods, our approach does not necessitate access to prior\ndatasets or the introduction of extra parameters. Our study demonstrates up to\n15% Word Error Rate Reduction (WERR) over fine-tuning baseline, and superior\nefficiency over other LLL techniques on CommonVoice English multi-accent\ndataset.", "published": "2024-06-25 20:52:09", "link": "http://arxiv.org/abs/2406.17935v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "NormTab: Improving Symbolic Reasoning in LLMs Through Tabular Data\n  Normalization", "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities in parsing textual data and generating code. However, their\nperformance in tasks involving tabular data, especially those requiring\nsymbolic reasoning, faces challenges due to the structural variance and\ninconsistency in table cell values often found in web tables. In this paper, we\nintroduce NormTab, a novel framework aimed at enhancing the symbolic reasoning\nperformance of LLMs by normalizing web tables. We study table normalization as\na stand-alone, one-time preprocessing step using LLMs to support symbolic\nreasoning on tabular data. Our experimental evaluation, conducted on\nchallenging web table datasets such as WikiTableQuestion and TabFact,\ndemonstrates that leveraging NormTab significantly improves symbolic reasoning\nperformance, showcasing the importance and effectiveness of web table\nnormalization for enhancing LLM-based symbolic reasoning tasks.", "published": "2024-06-25 22:40:03", "link": "http://arxiv.org/abs/2406.17961v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LABOR-LLM: Language-Based Occupational Representations with Large\n  Language Models", "abstract": "Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,\nthat predicts a worker's next job as a function of career history (an\n\"occupation model\"). CAREER was initially estimated (\"pre-trained\") using a\nlarge, unrepresentative resume dataset, which served as a \"foundation model,\"\nand parameter estimation was continued (\"fine-tuned\") using data from a\nrepresentative survey. CAREER had better predictive performance than\nbenchmarks. This paper considers an alternative where the resume-based\nfoundation model is replaced by a large language model (LLM). We convert\ntabular data from the survey into text files that resemble resumes and\nfine-tune the LLMs using these text files with the objective to predict the\nnext token (word). The resulting fine-tuned LLM is used as an input to an\noccupation model. Its predictive performance surpasses all prior models. We\ndemonstrate the value of fine-tuning and further show that by adding more\ncareer data from a different population, fine-tuning smaller LLMs surpasses the\nperformance of fine-tuning larger models.", "published": "2024-06-25 23:07:18", "link": "http://arxiv.org/abs/2406.17972v3", "categories": ["cs.LG", "cs.CL", "econ.EM"], "primary_category": "cs.LG"}
{"title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How\n  to Fix It)", "abstract": "Whether LLMs memorize their training data and what this means, from measuring\nprivacy leakage to detecting copyright violations, has become a rapidly growing\narea of research. In the last few months, more than 10 new methods have been\nproposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary\nto traditional MIAs which rely on fixed-but randomized-records or models, these\nmethods are mostly trained and tested on datasets collected post-hoc. Sets of\nmembers and non-members, used to evaluate the MIA, are constructed using\ninformed guesses after the release of a model. This lack of randomization\nraises concerns of a distribution shift between members and non-members. In\nthis work, we first extensively review the literature on MIAs against LLMs and\nshow that, while most work focuses on sequence-level MIAs evaluated in post-hoc\nsetups, a range of target models, motivations and units of interest are\nconsidered. We then quantify distribution shifts present in 6 datasets used in\nthe literature using a model-less bag of word classifier and show that all\ndatasets constructed post-hoc suffer from strong distribution shifts. These\nshifts invalidate the claims of LLMs memorizing strongly in real-world\nscenarios and, potentially, also the methodological contributions of the recent\npapers based on these datasets. Yet, all hope might not be lost. We introduce\nimportant considerations to properly evaluate MIAs against LLMs and discuss, in\nturn, potential ways forwards: randomized test splits, injections of randomized\n(unique) sequences, randomized fine-tuning, and several post-hoc control\nmethods. While each option comes with its advantages and limitations, we\nbelieve they collectively provide solid grounds to guide MIA development and\nstudy LLM memorization. We conclude with an overview of recommended approaches\nto benchmark sequence-level and document-level MIAs against LLMs.", "published": "2024-06-25 23:12:07", "link": "http://arxiv.org/abs/2406.17975v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ARES: Alternating Reinforcement Learning and Supervised Fine-Tuning for\n  Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback", "abstract": "Large Multimodal Models (LMMs) excel at comprehending human instructions and\ndemonstrate remarkable results across a broad spectrum of tasks. Reinforcement\nLearning from Human Feedback (RLHF) and AI Feedback (RLAIF) further refine LLMs\nby aligning them with specific preferences. These methods primarily use\nranking-based feedback for entire generations. With advanced AI models\n(Teacher), such as GPT-4 and Claude 3 Opus, we can request various types of\ndetailed feedback that are expensive for humans to provide. We propose a\ntwo-stage algorithm ARES that Alternates REinforcement Learning (RL) and\nSupervised Fine-Tuning (SFT). First, we request the Teacher to score how much\neach sentence contributes to solving the problem in a Chain-of-Thought (CoT).\nThis sentence-level feedback allows us to consider individual valuable\nsegments, providing more granular rewards for the RL procedure. Second, we ask\nthe Teacher to correct the wrong reasoning after the RL stage. The RL procedure\nrequires massive efforts for hyperparameter tuning and often generates errors\nlike repetitive words and incomplete sentences. With the correction feedback,\nwe stabilize the RL fine-tuned model through SFT. We conduct experiments on\nmulti-model dataset ScienceQA and A-OKVQA to demonstrate the effectiveness of\nour proposal. ARES rationale reasoning achieves around 70% win rate against\nbaseline models judged by GPT-4o. Additionally, we observe that the improved\nrationale reasoning leads to a 2.5% increase in inference answer accuracy on\naverage for the multi-modal datasets.", "published": "2024-06-25 07:20:11", "link": "http://arxiv.org/abs/2407.00087v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Autonomous Prompt Engineering in Large Language Models", "abstract": "Prompt engineering is a crucial yet challenging task for optimizing the\nperformance of large language models (LLMs) on customized tasks. This\npioneering research introduces the Automatic Prompt Engineering Toolbox (APET),\nwhich enables GPT-4 to autonomously apply prompt engineering techniques. By\nleveraging sophisticated strategies such as Expert Prompting, Chain of Thought,\nand Tree of Thoughts, APET empowers GPT-4 to dynamically optimize prompts,\nresulting in substantial improvements in tasks like Word Sorting (4.4%\nincrease) and Geometric Shapes (6.8% increase). Despite encountering challenges\nin complex tasks such as Checkmate in One (-14.8%), these findings demonstrate\nthe transformative potential of APET in automating complex prompt optimization\nprocesses without the use of external data. Overall, this research represents a\nsignificant leap in AI development, presenting a robust framework for future\ninnovations in autonomous AI systems and highlighting the ability of GPT-4 to\nbring prompt engineering theory to practice. It establishes a foundation for\nenhancing performance in complex task performance and broadening the practical\napplications of these techniques in real-world scenarios.", "published": "2024-06-25 10:14:44", "link": "http://arxiv.org/abs/2407.11000v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "MoESD: Mixture of Experts Stable Diffusion to Mitigate Gender Bias", "abstract": "Text-to-image models are known to propagate social biases. For example, when\nprompted to generate images of people in certain professions, these models tend\nto systematically generate specific genders or ethnicities. In this paper, we\nshow that this bias is already present in the text encoder of the model and\nintroduce a Mixture-of-Experts approach by identifying text-encoded bias in the\nlatent space and then creating a Bias-Identification Gate mechanism. More\nspecifically, we propose MoESD (Mixture of Experts Stable Diffusion) with BiAs\n(Bias Adapters) to mitigate gender bias in text-to-image models. We also\ndemonstrate that introducing an arbitrary special token to the prompt is\nessential during the mitigation process. With experiments focusing on gender\nbias, we show that our approach successfully mitigates gender bias while\nmaintaining image quality.", "published": "2024-06-25 14:59:31", "link": "http://arxiv.org/abs/2407.11002v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using Large Language Models in Public Transit Systems, San Antonio as a\n  case study", "abstract": "The integration of large language models into public transit systems\nrepresents a significant advancement in urban transportation management and\npassenger experience. This study examines the impact of LLMs within San\nAntonio's public transit system, leveraging their capabilities in natural\nlanguage processing, data analysis, and real time communication. By utilizing\nGTFS and other public transportation information, the research highlights the\ntransformative potential of LLMs in enhancing route planning, reducing wait\ntimes, and providing personalized travel assistance. Our case study is the city\nof San Antonio as part of a project aiming to demonstrate how LLMs can optimize\nresource allocation, improve passenger satisfaction, and support decision\nmaking processes in transit management. We evaluated LLM responses to questions\nrelated to both information retrieval and also understanding. Ultimately, we\nbelieve that the adoption of LLMs in public transit systems can lead to more\nefficient, responsive, and user-friendly transportation networks, providing a\nmodel for other cities to follow.", "published": "2024-06-25 16:32:56", "link": "http://arxiv.org/abs/2407.11003v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators", "abstract": "Large pretrained models can be used as annotators, helping replace or augment\ncrowdworkers and enabling distilling generalist models into smaller specialist\nmodels. Unfortunately, this comes at a cost: employing top-of-the-line models\noften requires paying thousands of dollars for API calls, while the resulting\ndatasets are static and challenging to audit. To address these challenges, we\npropose a simple alternative: rather than directly querying labels from\npretrained models, we task models to generate programs that can produce labels.\nThese programs can be stored and applied locally, re-used and extended, and\ncost orders of magnitude less. Our system, Alchemist, obtains comparable to or\nbetter performance than large language model-based annotation in a range of\ntasks for a fraction of the cost: on average, improvements amount to a 12.9%\nenhancement while the total labeling costs across all datasets are reduced by a\nfactor of approximately 500x.", "published": "2024-06-25 17:58:26", "link": "http://arxiv.org/abs/2407.11004v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating the Efficacy of Foundational Models: Advancing Benchmarking\n  Practices to Enhance Fine-Tuning Decision-Making", "abstract": "Recently, large language models (LLMs) have expanded into various domains.\nHowever, there remains a need to evaluate how these models perform when\nprompted with commonplace queries compared to domain-specific queries, which\nmay be useful for benchmarking prior to fine-tuning for domain-specific\ndownstream tasks. This study evaluates LLMs, specifically Gemma-2B and\nGemma-7B, across diverse domains, including cybersecurity, medicine, and\nfinance, compared to common knowledge queries. This study utilizes a\ncomprehensive methodology to assess foundational models, which includes problem\nformulation, data analysis, and the development of ThroughCut, a novel outlier\ndetection technique that automatically identifies response throughput outliers\nbased on their conciseness. This methodological rigor enhances the credibility\nof the presented evaluation frameworks. This study focused on assessing\ninference time, response length, throughput, quality, and resource utilization\nand investigated the correlations between these factors. The results indicate\nthat model size and types of prompts used for inference significantly\ninfluenced response length and quality. In addition, common prompts, which\ninclude various types of queries, generate diverse and inconsistent responses\nat irregular intervals. In contrast, domain-specific prompts consistently\ngenerate concise responses within a reasonable time. Overall, this study\nunderscores the need for comprehensive evaluation frameworks to enhance the\nreliability of benchmarking procedures in multidomain AI research.", "published": "2024-06-25 20:52:31", "link": "http://arxiv.org/abs/2407.11006v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Figuring out Figures: Using Textual References to Caption Scientific\n  Figures", "abstract": "Figures are essential channels for densely communicating complex ideas in\nscientific papers. Previous work in automatically generating figure captions\nhas been largely unsuccessful and has defaulted to using single-layer LSTMs,\nwhich no longer achieve state-of-the-art performance. In our work, we use the\nSciCap datasets curated by Hsu et al. and use a variant of a CLIP+GPT-2\nencoder-decoder model with cross-attention to generate captions conditioned on\nthe image. Furthermore, we augment our training pipeline by creating a new\ndataset MetaSciCap that incorporates textual metadata from the original paper\nrelevant to the figure, such as the title, abstract, and in-text references. We\nuse SciBERT to encode the textual metadata and use this encoding alongside the\nfigure embedding. In our experimentation with different models, we found that\nthe CLIP+GPT-2 model performs better when it receives all textual metadata from\nthe SciBERT encoder in addition to the figure, but employing a SciBERT+GPT2\nmodel that uses only the textual metadata achieved optimal performance.", "published": "2024-06-25 21:49:21", "link": "http://arxiv.org/abs/2407.11008v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Specific language impairment (SLI) detection pipeline from\n  transcriptions of spontaneous narratives", "abstract": "Specific Language Impairment (SLI) is a disorder that affects communication\nand can affect both comprehension and expression. This study focuses on\neffectively detecting SLI in children using transcripts of spontaneous\nnarratives from 1063 interviews. A three-stage cascading pipeline was proposed\nf. In the first stage, feature extraction and dimensionality reduction of the\ndata are performed using the Random Forest (RF) and Spearman correlation\nmethods. In the second stage, the most predictive variables from the first\nstage are estimated using logistic regression, which is used in the last stage\nto detect SLI in children from transcripts of spontaneous narratives using a\nnearest neighbor classifier. The results revealed an accuracy of 97.13% in\nidentifying SLI, highlighting aspects such as the length of the responses, the\nquality of their utterances, and the complexity of the language. This new\napproach, framed in natural language processing, offers significant benefits to\nthe field of SLI detection by avoiding complex subjective variables and\nfocusing on quantitative metrics directly related to the child's performance.", "published": "2024-06-25 19:22:57", "link": "http://arxiv.org/abs/2407.12012v1", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Interpretable Learners", "abstract": "The trade-off between expressiveness and interpretability remains a core\nchallenge when building human-centric predictive models for classification and\ndecision-making. While symbolic rules offer interpretability, they often lack\nexpressiveness, whereas neural networks excel in performance but are known for\nbeing black boxes. In this paper, we show a combination of Large Language\nModels (LLMs) and symbolic programs can bridge this gap. In the proposed\nLLM-based Symbolic Programs (LSPs), the pretrained LLM with natural language\nprompts provides a massive set of interpretable modules that can transform raw\ninput into natural language concepts. Symbolic programs then integrate these\nmodules into an interpretable decision rule. To train LSPs, we develop a\ndivide-and-conquer approach to incrementally build the program from scratch,\nwhere the learning process of each step is guided by LLMs. To evaluate the\neffectiveness of LSPs in extracting interpretable and accurate knowledge from\ndata, we introduce IL-Bench, a collection of diverse tasks, including both\nsynthetic and real-world scenarios across different modalities. Empirical\nresults demonstrate LSP's superior performance compared to traditional\nneurosymbolic programs and vanilla automatic prompt tuning methods. Moreover,\nas the knowledge learned by LSP is a combination of natural language\ndescriptions and symbolic rules, it is easily transferable to humans\n(interpretable), and other LLMs, and generalizes well to out-of-distribution\nsamples.", "published": "2024-06-25 02:18:15", "link": "http://arxiv.org/abs/2406.17224v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SC", "68T05"], "primary_category": "cs.AI"}
{"title": "ET tu, CLIP? Addressing Common Object Errors for Unseen Environments", "abstract": "We introduce a simple method that employs pre-trained CLIP encoders to\nenhance model generalization in the ALFRED task. In contrast to previous\nliterature where CLIP replaces the visual encoder, we suggest using CLIP as an\nadditional module through an auxiliary object detection objective. We validate\nour method on the recently proposed Episodic Transformer architecture and\ndemonstrate that incorporating CLIP improves task performance on the unseen\nvalidation set. Additionally, our analysis results support that CLIP especially\nhelps with leveraging object descriptions, detecting small objects, and\ninterpreting rare words.", "published": "2024-06-25 18:35:13", "link": "http://arxiv.org/abs/2406.17876v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "primary_category": "cs.CV"}
{"title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer\n  and Group Masked Language Model", "abstract": "We propose a novel two-stage text-to-speech (TTS) framework with two types of\ndiscrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech\nsynthesis. It features two core components: the Interpreting module, which\nprocesses text and a speech prompt into semantic tokens focusing on linguistic\ncontents and alignment, and the Speaking module, which captures the timbre of\nthe target voice to generate acoustic tokens from semantic tokens, enriching\nspeech reconstruction. The Interpreting stage employs a transducer for its\nrobustness in aligning text to speech. In contrast, the Speaking stage utilizes\na Conformer-based architecture integrated with a Grouped Masked Language Model\n(G-MLM) to boost computational efficiency. Our experiments verify that this\ninnovative structure surpasses the conventional models in the zero-shot\nscenario in terms of speech quality and speaker similarity.", "published": "2024-06-25 06:46:47", "link": "http://arxiv.org/abs/2406.17310v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "SpecMaskGIT: Masked Generative Modeling of Audio Spectrograms for\n  Efficient Audio Synthesis and Beyond", "abstract": "Recent advances in generative models that iteratively synthesize audio clips\nsparked great success to text-to-audio synthesis (TTA), but with the cost of\nslow synthesis speed and heavy computation. Although there have been attempts\nto accelerate the iterative procedure, high-quality TTA systems remain\ninefficient due to hundreds of iterations required in the inference phase and\nlarge amount of model parameters. To address the challenges, we propose\nSpecMaskGIT, a light-weighted, efficient yet effective TTA model based on the\nmasked generative modeling of spectrograms. First, SpecMaskGIT synthesizes a\nrealistic 10s audio clip by less than 16 iterations, an order-of-magnitude less\nthan previous iterative TTA methods. As a discrete model, SpecMaskGIT\noutperforms larger VQ-Diffusion and auto-regressive models in the TTA\nbenchmark, while being real-time with only 4 CPU cores or even 30x faster with\na GPU. Next, built upon a latent space of Mel-spectrogram, SpecMaskGIT has a\nwider range of applications (e.g., the zero-shot bandwidth extension) than\nsimilar methods built on the latent wave domain. Moreover, we interpret\nSpecMaskGIT as a generative extension to previous discriminative audio masked\nTransformers, and shed light on its audio representation learning potential. We\nhope our work inspires the exploration of masked audio modeling toward further\ndiverse scenarios.", "published": "2024-06-25 16:02:59", "link": "http://arxiv.org/abs/2406.17672v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial\n  Information and Non-target Signals", "abstract": "This paper proposes a new task called spatial voice conversion, which aims to\nconvert a target voice while preserving spatial information and non-target\nsignals. Traditional voice conversion methods focus on single-channel\nwaveforms, ignoring the stereo listening experience inherent in human hearing.\nOur baseline approach addresses this gap by integrating blind source separation\n(BSS), voice conversion (VC), and spatial mixing to handle multi-channel\nwaveforms. Through experimental evaluations, we organize and identify the key\nchallenges inherent in this task, such as maintaining audio quality and\naccurately preserving spatial information. Our results highlight the\nfundamental difficulties in balancing these aspects, providing a benchmark for\nfuture research in spatial voice conversion. The proposed method's code is\npublicly available to encourage further exploration in this domain.", "published": "2024-06-25 17:10:39", "link": "http://arxiv.org/abs/2406.17722v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Tagging in Infant-centric Home Soundscapes", "abstract": "Certain environmental noises have been associated with negative developmental\noutcomes for infants and young children. Though classifying or tagging sound\nevents in a domestic environment is an active research area, previous studies\nfocused on data collected from a non-stationary microphone placed in the\nenvironment or from the perspective of adults. Further, many of these works\nignore infants or young children in the environment or have data collected from\nonly a single family where noise from the fixed sound source can be moderate at\nthe infant's position or vice versa. Thus, despite the recent success of large\npre-trained models for noise event detection, the performance of these models\non infant-centric noise soundscapes in the home is yet to be explored. To\nbridge this gap, we have collected and labeled noises in home soundscapes from\n22 families in an unobtrusive manner, where the data are collected through an\ninfant-worn recording device. In this paper, we explore the performance of a\nlarge pre-trained model (Audio Spectrogram Transformer [AST]) on our\nnoise-conditioned infant-centric environmental data as well as publicly\navailable home environmental datasets. Utilizing different training strategies\nsuch as resampling, utilizing public datasets, mixing public and infant-centric\ntraining sets, and data augmentation using noise and masking, we evaluate the\nperformance of a large pre-trained model on sparse and imbalanced\ninfant-centric data. Our results show that fine-tuning the large pre-trained\nmodel by combining our collected dataset with public datasets increases the\nF1-score from 0.11 (public datasets) and 0.76 (collected datasets) to 0.84\n(combined datasets) and Cohen's Kappa from 0.013 (public datasets) and 0.77\n(collected datasets) to 0.83 (combined datasets) compared to only training with\npublic or collected datasets, respectively.", "published": "2024-06-25 00:15:54", "link": "http://arxiv.org/abs/2406.17190v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Embeddings for Detecting Individual Symptoms of\n  Depression", "abstract": "Depression, a prevalent mental health disorder impacting millions globally,\ndemands reliable assessment systems. Unlike previous studies that focus solely\non either detecting depression or predicting its severity, our work identifies\nindividual symptoms of depression while also predicting its severity using\nspeech input. We leverage self-supervised learning (SSL)-based speech models to\nbetter utilize the small-sized datasets that are frequently encountered in this\ntask. Our study demonstrates notable performance improvements by utilizing SSL\nembeddings compared to conventional speech features. We compare various types\nof SSL pretrained models to elucidate the type of speech information (semantic,\nspeaker, or prosodic) that contributes the most in identifying different\nsymptoms. Additionally, we evaluate the impact of combining multiple SSL\nembeddings on performance. Furthermore, we show the significance of multi-task\nlearning for identifying depressive symptoms effectively.", "published": "2024-06-25 02:35:37", "link": "http://arxiv.org/abs/2406.17229v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Beyond Silence: Bias Analysis through Loss and Asymmetric Approach in\n  Audio Anti-Spoofing", "abstract": "Current trends in audio anti-spoofing detection research strive to improve\nmodels' ability to generalize across unseen attacks by learning to identify a\nvariety of spoofing artifacts. This emphasis has primarily focused on the spoof\nclass. Recently, several studies have noted that the distribution of silence\ndiffers between the two classes, which can serve as a shortcut. In this paper,\nwe extend class-wise interpretations beyond silence. We employ loss analysis\nand asymmetric methodologies to move away from traditional attack-focused and\nresult-oriented evaluations towards a deeper examination of model behaviors.\nOur investigations highlight the significant differences in training dynamics\nbetween the two classes, emphasizing the need for future research to focus on\nrobust modeling of the bonafide class.", "published": "2024-06-25 03:24:12", "link": "http://arxiv.org/abs/2406.17246v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker-Independent Acoustic-to-Articulatory Inversion through\n  Multi-Channel Attention Discriminator", "abstract": "We present a novel speaker-independent acoustic-to-articulatory inversion\n(AAI) model, overcoming the limitations observed in conventional AAI models\nthat rely on acoustic features derived from restricted datasets. To address\nthese challenges, we leverage representations from a pre-trained\nself-supervised learning (SSL) model to more effectively estimate the global,\nlocal, and kinematic pattern information in Electromagnetic Articulography\n(EMA) signals during the AAI process. We train our model using an adversarial\napproach and introduce an attention-based Multi-duration phoneme discriminator\n(MDPD) designed to fully capture the intricate relationship among multi-channel\narticulatory signals. Our method achieves a Pearson correlation coefficient of\n0.847, marking state-of-the-art performance in speaker-independent AAI models.\nThe implementation details and code can be found online.", "published": "2024-06-25 07:29:10", "link": "http://arxiv.org/abs/2406.17329v1", "categories": ["eess.SP", "cs.SD", "eess.AS", "physics.bio-ph"], "primary_category": "eess.SP"}
{"title": "Temporal-Channel Modeling in Multi-head Self-Attention for Synthetic\n  Speech Detection", "abstract": "Recent synthetic speech detectors leveraging the Transformer model have\nsuperior performance compared to the convolutional neural network counterparts.\nThis improvement could be due to the powerful modeling ability of the\nmulti-head self-attention (MHSA) in the Transformer model, which learns the\ntemporal relationship of each input token. However, artifacts of synthetic\nspeech can be located in specific regions of both frequency channels and\ntemporal segments, while MHSA neglects this temporal-channel dependency of the\ninput sequence. In this work, we proposed a Temporal-Channel Modeling (TCM)\nmodule to enhance MHSA's capability for capturing temporal-channel\ndependencies. Experimental results on the ASVspoof 2021 show that with only\n0.03M additional parameters, the TCM module can outperform the state-of-the-art\nsystem by 9.25% in EER. Further ablation study reveals that utilizing both\ntemporal and channel information yields the most improvement for detecting\nsynthetic speech.", "published": "2024-06-25 08:50:43", "link": "http://arxiv.org/abs/2406.17376v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SonicSense: Object Perception from In-Hand Acoustic Vibration", "abstract": "We introduce SonicSense, a holistic design of hardware and software to enable\nrich robot object perception through in-hand acoustic vibration sensing. While\nprevious studies have shown promising results with acoustic sensing for object\nperception, current solutions are constrained to a handful of objects with\nsimple geometries and homogeneous materials, single-finger sensing, and mixing\ntraining and testing on the same objects. SonicSense enables container\ninventory status differentiation, heterogeneous material prediction, 3D shape\nreconstruction, and object re-identification from a diverse set of 83\nreal-world objects. Our system employs a simple but effective heuristic\nexploration policy to interact with the objects as well as end-to-end\nlearning-based algorithms to fuse vibration signals to infer object properties.\nOur framework underscores the significance of in-hand acoustic vibration\nsensing in advancing robot tactile perception.", "published": "2024-06-25 20:47:10", "link": "http://arxiv.org/abs/2406.17932v2", "categories": ["cs.RO", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic\n  Alignment", "abstract": "Large Language Model (LLM) based text-to-speech (TTS) systems have\ndemonstrated remarkable capabilities in handling large speech datasets and\ngenerating natural speech for new speakers. However, LLM-based TTS models are\nnot robust as the generated output can contain repeating words, missing words\nand mis-aligned speech (referred to as hallucinations or attention errors),\nespecially when the text contains multiple occurrences of the same token. We\nexamine these challenges in an encoder-decoder transformer model and find that\ncertain cross-attention heads in such models implicitly learn the text and\nspeech alignment when trained for predicting speech tokens for a given text. To\nmake the alignment more robust, we propose techniques utilizing CTC loss and\nattention priors that encourage monotonic cross-attention over the text tokens.\nOur guided attention training technique does not introduce any new learnable\nparameters and significantly improves robustness of LLM-based TTS models.", "published": "2024-06-25 22:18:52", "link": "http://arxiv.org/abs/2406.17957v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
