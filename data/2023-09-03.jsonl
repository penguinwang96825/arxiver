{"title": "Business Process Text Sketch Automation Generation Using Large Language\n  Model", "abstract": "Business Process Management (BPM) is gaining increasing attention as it has\nthe potential to cut costs while boosting output and quality. Business process\ndocument generation is a crucial stage in BPM. However, due to a shortage of\ndatasets, data-driven deep learning techniques struggle to deliver the expected\nresults. We propose an approach to transform Conditional Process Trees (CPTs)\ninto Business Process Text Sketches (BPTSs) using Large Language Models (LLMs).\nThe traditional prompting approach (Few-shot In-Context Learning) tries to get\nthe correct answer in one go, and it can find the pattern of transforming\nsimple CPTs into BPTSs, but for close-domain and CPTs with complex hierarchy,\nthe traditional prompts perform weakly and with low correctness. We suggest\nusing this technique to break down a difficult CPT into a number of basic CPTs\nand then solve each one in turn, drawing inspiration from the\ndivide-and-conquer strategy. We chose 100 process trees with depths ranging\nfrom 2 to 5 at random, as well as CPTs with many nodes, many degrees of\nselection, and cyclic nesting. Experiments show that our method can achieve a\ncorrect rate of 93.42%, which is 45.17% better than traditional prompting\nmethods. Our proposed method provides a solution for business process document\ngeneration in the absence of datasets, and secondly, it becomes potentially\npossible to provide a large number of datasets for the process model extraction\n(PME) domain.", "published": "2023-09-03 04:19:02", "link": "http://arxiv.org/abs/2309.01071v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Study on the Implementation of Generative AI Services Using an\n  Enterprise Data-Based LLM Application Architecture", "abstract": "This study presents a method for implementing generative AI services by\nutilizing the Large Language Models (LLM) application architecture. With recent\nadvancements in generative AI technology, LLMs have gained prominence across\nvarious domains. In this context, the research addresses the challenge of\ninformation scarcity and proposes specific remedies by harnessing LLM\ncapabilities. The investigation delves into strategies for mitigating the issue\nof inadequate data, offering tailored solutions. The study delves into the\nefficacy of employing fine-tuning techniques and direct document integration to\nalleviate data insufficiency. A significant contribution of this work is the\ndevelopment of a Retrieval-Augmented Generation (RAG) model, which tackles the\naforementioned challenges. The RAG model is carefully designed to enhance\ninformation storage and retrieval processes, ensuring improved content\ngeneration. The research elucidates the key phases of the information storage\nand retrieval methodology underpinned by the RAG model. A comprehensive\nanalysis of these steps is undertaken, emphasizing their significance in\naddressing the scarcity of data. The study highlights the efficacy of the\nproposed method, showcasing its applicability through illustrative instances.\nBy implementing the RAG model for information storage and retrieval, the\nresearch not only contributes to a deeper comprehension of generative AI\ntechnology but also facilitates its practical usability within enterprises\nutilizing LLMs. This work holds substantial value in advancing the field of\ngenerative AI, offering insights into enhancing data-driven content generation\nand fostering active utilization of LLM-based services within corporate\nsettings.", "published": "2023-09-03 07:03:17", "link": "http://arxiv.org/abs/2309.01105v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MedChatZH: a Better Medical Adviser Learns from Better Instructions", "abstract": "Generative large language models (LLMs) have shown great success in various\napplications, including question-answering (QA) and dialogue systems. However,\nin specialized domains like traditional Chinese medical QA, these models may\nperform unsatisfactorily without fine-tuning on domain-specific datasets. To\naddress this, we introduce MedChatZH, a dialogue model designed specifically\nfor traditional Chinese medical QA. Our model is pre-trained on Chinese\ntraditional medical books and fine-tuned with a carefully curated medical\ninstruction dataset. It outperforms several solid baselines on a real-world\nmedical dialogue dataset. We release our model, code, and dataset on\nhttps://github.com/tyang816/MedChatZH to facilitate further research in the\ndomain of traditional Chinese medicine and LLMs.", "published": "2023-09-03 08:08:15", "link": "http://arxiv.org/abs/2309.01114v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Attention Where It Matters: Rethinking Visual Document Understanding\n  with Selective Region Concentration", "abstract": "We propose a novel end-to-end document understanding model called SeRum\n(SElective Region Understanding Model) for extracting meaningful information\nfrom document images, including document analysis, retrieval, and office\nautomation.\n  Unlike state-of-the-art approaches that rely on multi-stage technical schemes\nand are computationally expensive,\n  SeRum converts document image understanding and recognition tasks into a\nlocal decoding process of the visual tokens of interest, using a content-aware\ntoken merge module.\n  This mechanism enables the model to pay more attention to regions of interest\ngenerated by the query decoder, improving the model's effectiveness and\nspeeding up the decoding speed of the generative scheme.\n  We also designed several pre-training tasks to enhance the understanding and\nlocal awareness of the model.\n  Experimental results demonstrate that SeRum achieves state-of-the-art\nperformance on document understanding tasks and competitive results on text\nspotting tasks.\n  SeRum represents a substantial advancement towards enabling efficient and\neffective end-to-end document understanding.", "published": "2023-09-03 10:14:34", "link": "http://arxiv.org/abs/2309.01131v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Visual Interpretation-Based Self-Improved Classification System Using\n  Virtual Adversarial Training", "abstract": "The successful application of large pre-trained models such as BERT in\nnatural language processing has attracted more attention from researchers.\nSince the BERT typically acts as an end-to-end black box, classification\nsystems based on it usually have difficulty in interpretation and low\nrobustness. This paper proposes a visual interpretation-based self-improving\nclassification model with a combination of virtual adversarial training (VAT)\nand BERT models to address the above problems. Specifically, a fine-tuned BERT\nmodel is used as a classifier to classify the sentiment of the text. Then, the\npredicted sentiment classification labels are used as part of the input of\nanother BERT for spam classification via a semi-supervised training manner\nusing VAT. Additionally, visualization techniques, including visualizing the\nimportance of words and normalizing the attention head matrix, are employed to\nanalyze the relevance of each component to classification accuracy. Moreover,\nbrand-new features will be found in the visual analysis, and classification\nperformance will be improved. Experimental results on Twitter's tweet dataset\ndemonstrate the effectiveness of the proposed model on the classification task.\nFurthermore, the ablation study results illustrate the effect of different\ncomponents of the proposed model on the classification results.", "published": "2023-09-03 15:07:24", "link": "http://arxiv.org/abs/2309.01196v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Representations Matter: Embedding Modes of Large Language Models using\n  Dynamic Mode Decomposition", "abstract": "Existing large language models (LLMs) are known for generating \"hallucinated\"\ncontent, namely a fabricated text of plausibly looking, yet unfounded, facts.\nTo identify when these hallucination scenarios occur, we examine the properties\nof the generated text in the embedding space. Specifically, we draw inspiration\nfrom the dynamic mode decomposition (DMD) tool in analyzing the pattern\nevolution of text embeddings across sentences. We empirically demonstrate how\nthe spectrum of sentence embeddings over paragraphs is constantly low-rank for\nthe generated text, unlike that of the ground-truth text. Importantly, we find\nthat evaluation cases having LLM hallucinations correspond to ground-truth\nembedding patterns with a higher number of modes being poorly approximated by\nthe few modes associated with LLM embedding patterns. In analogy to near-field\nelectromagnetic evanescent waves, the embedding DMD eigenmodes of the generated\ntext with hallucinations vanishes quickly across sentences as opposed to those\nof the ground-truth text. This suggests that the hallucinations result from\nboth the generation techniques and the underlying representation.", "published": "2023-09-03 19:10:18", "link": "http://arxiv.org/abs/2309.01245v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BDC-Adapter: Brownian Distance Covariance for Better Vision-Language\n  Reasoning", "abstract": "Large-scale pre-trained Vision-Language Models (VLMs), such as CLIP and\nALIGN, have introduced a new paradigm for learning transferable visual\nrepresentations. Recently, there has been a surge of interest among researchers\nin developing lightweight fine-tuning techniques to adapt these models to\ndownstream visual tasks. We recognize that current state-of-the-art fine-tuning\nmethods, such as Tip-Adapter, simply consider the covariance between the query\nimage feature and features of support few-shot training samples, which only\ncaptures linear relations and potentially instigates a deceptive perception of\nindependence. To address this issue, in this work, we innovatively introduce\nBrownian Distance Covariance (BDC) to the field of vision-language reasoning.\nThe BDC metric can model all possible relations, providing a robust metric for\nmeasuring feature dependence. Based on this, we present a novel method called\nBDC-Adapter, which integrates BDC prototype similarity reasoning and\nmulti-modal reasoning network prediction to perform classification tasks. Our\nextensive experimental results show that the proposed BDC-Adapter can freely\nhandle non-linear relations and fully characterize independence, outperforming\nthe current state-of-the-art methods by large margins.", "published": "2023-09-03 19:45:02", "link": "http://arxiv.org/abs/2309.01256v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Generative Data Augmentation using LLMs improves Distributional\n  Robustness in Question Answering", "abstract": "Robustness in Natural Language Processing continues to be a pertinent issue,\nwhere state of the art models under-perform under naturally shifted\ndistributions. In the context of Question Answering, work on domain adaptation\nmethods continues to be a growing body of research. However, very little\nattention has been given to the notion of domain generalization under natural\ndistribution shifts, where the target domain is unknown. With drastic\nimprovements in the quality and access to generative models, we answer the\nquestion: How do generated datasets influence the performance of QA models\nunder natural distribution shifts? We perform experiments on 4 different\ndatasets under varying amounts of distribution shift, and analyze how\n\"in-the-wild\" generation can help achieve domain generalization. We take a\ntwo-step generation approach, generating both contexts and QA pairs to augment\nexisting datasets. Through our experiments, we demonstrate how augmenting\nreading comprehension datasets with generated data leads to better robustness\ntowards natural distribution shifts.", "published": "2023-09-03 03:27:06", "link": "http://arxiv.org/abs/2309.06358v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Generative Recommendation: A Survey and\n  Visionary Discussions", "abstract": "Large language models (LLM) not only have revolutionized the field of natural\nlanguage processing (NLP) but also have the potential to reshape many other\nfields, e.g., recommender systems (RS). However, most of the related work\ntreats an LLM as a component of the conventional recommendation pipeline (e.g.,\nas a feature extractor), which may not be able to fully leverage the generative\npower of LLM. Instead of separating the recommendation process into multiple\nstages, such as score computation and re-ranking, this process can be\nsimplified to one stage with LLM: directly generating recommendations from the\ncomplete pool of items. This survey reviews the progress, methods, and future\ndirections of LLM-based generative recommendation by examining three questions:\n1) What generative recommendation is, 2) Why RS should advance to generative\nrecommendation, and 3) How to implement LLM-based generative recommendation for\nvarious RS tasks. We hope that this survey can provide the context and guidance\nneeded to explore this interesting and emerging topic.", "published": "2023-09-03 12:33:47", "link": "http://arxiv.org/abs/2309.01157v2", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large\n  Language Models", "abstract": "While large language models (LLMs) have demonstrated remarkable capabilities\nacross a range of downstream tasks, a significant concern revolves around their\npropensity to exhibit hallucinations: LLMs occasionally generate content that\ndiverges from the user input, contradicts previously generated context, or\nmisaligns with established world knowledge. This phenomenon poses a substantial\nchallenge to the reliability of LLMs in real-world scenarios. In this paper, we\nsurvey recent efforts on the detection, explanation, and mitigation of\nhallucination, with an emphasis on the unique challenges posed by LLMs. We\npresent taxonomies of the LLM hallucination phenomena and evaluation\nbenchmarks, analyze existing approaches aiming at mitigating LLM hallucination,\nand discuss potential directions for future research.", "published": "2023-09-03 16:56:48", "link": "http://arxiv.org/abs/2309.01219v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large AI Model Empowered Multimodal Semantic Communications", "abstract": "Multimodal signals, including text, audio, image, and video, can be\nintegrated into Semantic Communication (SC) systems to provide an immersive\nexperience with low latency and high quality at the semantic level. However,\nthe multimodal SC has several challenges, including data heterogeneity,\nsemantic ambiguity, and signal distortion during transmission. Recent\nadvancements in large AI models, particularly in the Multimodal Language Model\n(MLM) and Large Language Model (LLM), offer potential solutions for addressing\nthese issues. To this end, we propose a Large AI Model-based Multimodal SC\n(LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment\n(MMA) that utilizes the MLM to enable the transformation between multimodal and\nunimodal data while preserving semantic consistency. Then, a personalized\nLLM-based Knowledge Base (LKB) is proposed, which allows users to perform\npersonalized semantic extraction or recovery through the LLM. This effectively\naddresses the semantic ambiguity. Finally, we apply the Conditional Generative\nadversarial network-based channel Estimation (CGE) for estimating the wireless\nchannel state information. This approach effectively mitigates the impact of\nfading channels in SC. Finally, we conduct simulations that demonstrate the\nsuperior performance of the LAM-MSC framework.", "published": "2023-09-03 19:24:34", "link": "http://arxiv.org/abs/2309.01249v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "MSM-VC: High-fidelity Source Style Transfer for Non-Parallel Voice\n  Conversion by Multi-scale Style Modeling", "abstract": "In addition to conveying the linguistic content from source speech to\nconverted speech, maintaining the speaking style of source speech also plays an\nimportant role in the voice conversion (VC) task, which is essential in many\nscenarios with highly expressive source speech, such as dubbing and data\naugmentation. Previous work generally took explicit prosodic features or\nfixed-length style embedding extracted from source speech to model the speaking\nstyle of source speech, which is insufficient to achieve comprehensive style\nmodeling and target speaker timbre preservation. Inspired by the style's\nmulti-scale nature of human speech, a multi-scale style modeling method for the\nVC task, referred to as MSM-VC, is proposed in this paper. MSM-VC models the\nspeaking style of source speech from different levels. To effectively convey\nthe speaking style and meanwhile prevent timbre leakage from source speech to\nconverted speech, each level's style is modeled by specific representation.\nSpecifically, prosodic features, pre-trained ASR model's bottleneck features,\nand features extracted by a model trained with a self-supervised strategy are\nadopted to model the frame, local, and global-level styles, respectively.\nBesides, to balance the performance of source style modeling and target speaker\ntimbre preservation, an explicit constraint module consisting of a pre-trained\nspeech emotion recognition model and a speaker classifier is introduced to\nMSM-VC. This explicit constraint module also makes it possible to simulate the\nstyle transfer inference process during the training to improve the\ndisentanglement ability and alleviate the mismatch between training and\ninference. Experiments performed on the highly expressive speech corpus\ndemonstrate that MSM-VC is superior to the state-of-the-art VC methods for\nmodeling source speech style while maintaining good speech quality and speaker\nsimilarity.", "published": "2023-09-03 11:33:27", "link": "http://arxiv.org/abs/2309.01142v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "NADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement", "abstract": "The goal of speech enhancement (SE) is to eliminate the background\ninterference from the noisy speech signal. Generative models such as diffusion\nmodels (DM) have been applied to the task of SE because of better\ngeneralization in unseen noisy scenes. Technical routes for the DM-based SE\nmethods can be summarized into three types: task-adapted diffusion process\nformulation, generator-plus-conditioner (GPC) structures and the multi-stage\nframeworks. We focus on the first two approaches, which are constructed under\nthe GPC architecture and use the task-adapted diffusion process to better deal\nwith the real noise. However, the performance of these SE models is limited by\nthe following issues: (a) Non-Gaussian noise estimation in the task-adapted\ndiffusion process. (b) Conditional domain bias caused by the weak conditioner\ndesign in the GPC structure. (c) Large amount of residual noise caused by\nunreasonable interpolation operations during inference. To solve the above\nproblems, we propose a noise-aware diffusion-based SE model (NADiffuSE) to\nboost the SE performance, where the noise representation is extracted from the\nnoisy speech signal and introduced as a global conditional information for\nestimating the non-Gaussian components. Furthermore, the anchor-based inference\nalgorithm is employed to achieve a compromise between the speech distortion and\nnoise residual. In order to mitigate the performance degradation caused by the\nconditional domain bias in the GPC framework, we investigate three model\nvariants, all of which can be viewed as multi-stage SE based on the\npreprocessing networks for Mel spectrograms. Experimental results show that\nNADiffuSE outperforms other DM-based SE models under the GPC infrastructure.\nAudio samples are available at: https://square-of-w.github.io/NADiffuSE-demo/.", "published": "2023-09-03 16:33:49", "link": "http://arxiv.org/abs/2309.01212v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Federated Few-shot Learning for Cough Classification with Edge Devices", "abstract": "Automatically classifying cough sounds is one of the most critical tasks for\nthe diagnosis and treatment of respiratory diseases. However, collecting a huge\namount of labeled cough dataset is challenging mainly due to high laborious\nexpenses, data scarcity, and privacy concerns. In this work, our aim is to\ndevelop a framework that can effectively perform cough classification even in\nsituations when enormous cough data is not available, while also addressing\nprivacy concerns. Specifically, we formulate a new problem to tackle these\nchallenges and adopt few-shot learning and federated learning to design a novel\nframework, termed F2LCough, for solving the newly formulated problem. We\nillustrate the superiority of our method compared with other approaches on\nCOVID-19 Thermal Face & Cough dataset, in which F2LCough achieves an average\nF1-Score of 86%. Our results show the feasibility of few-shot learning combined\nwith federated learning to build a classification model of cough sounds. This\nnew methodology is able to classify cough sounds in data-scarce situations and\nmaintain privacy properties. The outcomes of this work can be a fundamental\nframework for building support systems for the detection and diagnosis of\ncough-related diseases.", "published": "2023-09-03 04:48:41", "link": "http://arxiv.org/abs/2309.01076v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Acoustic-to-articulatory inversion for dysarthric speech: Are\n  pre-trained self-supervised representations favorable?", "abstract": "Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic\nto the articulatory space. Signal-processing features like the MFCCs, have been\nwidely used for the AAI task. For subjects with dysarthric speech, AAI is\nchallenging because of an imprecise and indistinct pronunciation. In this work,\nwe perform AAI for dysarthric speech using representations from pre-trained\nself-supervised learning (SSL) models. We demonstrate the impact of different\npre-trained features on this challenging AAI task, at low-resource conditions.\nIn addition, we also condition x-vectors to the extracted SSL features to train\na BLSTM network. In the seen case, we experiment with three AAI training\nschemes (subject-specific, pooled, and fine-tuned). The results, consistent\nacross training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves\na relative improvement of the Pearson Correlation Coefficient (CC) by ~1.81%\nand ~4.56% for healthy controls and patients, respectively, over MFCCs. We\nobserve similar average trends for different SSL features in the unseen case.\nOverall, SSL networks like wav2vec, APC, and DeCoAR, trained with feature\nreconstruction or future timestep prediction tasks, perform well in predicting\ndysarthric articulatory trajectories.", "published": "2023-09-03 07:44:38", "link": "http://arxiv.org/abs/2309.01108v4", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Noise robust speech emotion recognition with signal-to-noise ratio\n  adapting speech enhancement", "abstract": "Speech emotion recognition (SER) often experiences reduced performance due to\nbackground noise. In addition, making a prediction on signals with only\nbackground noise could undermine user trust in the system. In this study, we\npropose a Noise Robust Speech Emotion Recognition system, NRSER. NRSER employs\nspeech enhancement (SE) to effectively reduce the noise in input signals. Then,\nthe signal-to-noise-ratio (SNR)-level detection structure and waveform\nreconstitution strategy are introduced to reduce the negative impact of SE on\nspeech signals with no or little background noise. Our experimental results\nshow that NRSER can effectively improve the noise robustness of the SER system,\nincluding preventing the system from making emotion recognition on signals\nconsisting solely of background noise. Moreover, the proposed SNR-level\ndetection structure can be used individually for tasks such as data selection.", "published": "2023-09-03 13:00:04", "link": "http://arxiv.org/abs/2309.01164v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MAGMA: Music Aligned Generative Motion Autodecoder", "abstract": "Mapping music to dance is a challenging problem that requires spatial and\ntemporal coherence along with a continual synchronization with the music's\nprogression. Taking inspiration from large language models, we introduce a\n2-step approach for generating dance using a Vector Quantized-Variational\nAutoencoder (VQ-VAE) to distill motion into primitives and train a Transformer\ndecoder to learn the correct sequencing of these primitives. We also evaluate\nthe importance of music representations by comparing naive music feature\nextraction using Librosa to deep audio representations generated by\nstate-of-the-art audio compression algorithms. Additionally, we train\nvariations of the motion generator using relative and absolute positional\nencodings to determine the effect on generated motion quality when generating\narbitrarily long sequence lengths. Our proposed approach achieve\nstate-of-the-art results in music-to-motion generation benchmarks and enables\nthe real-time generation of considerably longer motion sequences, the ability\nto chain multiple motion sequences seamlessly, and easy customization of motion\nsequences to meet style requirements.", "published": "2023-09-03 15:21:47", "link": "http://arxiv.org/abs/2309.01202v1", "categories": ["cs.GR", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
