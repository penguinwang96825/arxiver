{"title": "MISMATCH: Fine-grained Evaluation of Machine-generated Text with\n  Mismatch Error Types", "abstract": "With the growing interest in large language models, the need for evaluating\nthe quality of machine text compared to reference (typically human-generated)\ntext has become focal attention. Most recent works focus either on\ntask-specific evaluation metrics or study the properties of machine-generated\ntext captured by the existing metrics. In this work, we propose a new\nevaluation scheme to model human judgments in 7 NLP tasks, based on the\nfine-grained mismatches between a pair of texts. Inspired by the recent efforts\nin several NLP tasks for fine-grained evaluation, we introduce a set of 13\nmismatch error types such as spatial/geographic errors, entity errors, etc, to\nguide the model for better prediction of human judgments. We propose a neural\nframework for evaluating machine texts that uses these mismatch error types as\nauxiliary tasks and re-purposes the existing single-number evaluation metrics\nas additional scalar features, in addition to textual features extracted from\nthe machine and reference texts. Our experiments reveal key insights about the\nexisting metrics via the mismatch errors. We show that the mismatch errors\nbetween the sentence pairs on the held-out datasets from 7 NLP tasks align well\nwith the human evaluation.", "published": "2023-06-18 01:38:53", "link": "http://arxiv.org/abs/2306.10452v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transferring Neural Potentials For High Order Dependency Parsing", "abstract": "High order dependency parsing leverages high order features such as siblings\nor grandchildren to improve state of the art accuracy of current first order\ndependency parsers. The present paper uses biaffine scores to provide an\nestimate of the arc scores and is then propagated into a graphical model. The\ninference inside the graphical model is solved using dual decomposition. The\npresent algorithm propagates biaffine neural scores to the graphical model and\nby leveraging dual decomposition inference, the overall circuit is trained\nend-to-end to transfer first order informations to the high order informations.", "published": "2023-06-18 03:58:41", "link": "http://arxiv.org/abs/2306.10469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Static Benchmarks to Adaptive Testing: Psychometrics in AI\n  Evaluation", "abstract": "As AI systems continue to grow, particularly generative models like Large\nLanguage Models (LLMs), their rigorous evaluation is crucial for development\nand deployment. To determine their adequacy, researchers have developed various\nlarge-scale benchmarks against a so-called gold-standard test set and report\nmetrics averaged across all items. However, this static evaluation paradigm\nincreasingly shows its limitations, including high computational costs, data\ncontamination, and the impact of low-quality or erroneous items on evaluation\nreliability and efficiency. In this Perspective, drawing from human\npsychometrics, we discuss a paradigm shift from static evaluation methods to\nadaptive testing. This involves estimating the characteristics and value of\neach test item in the benchmark and dynamically adjusting items in real-time,\ntailoring the evaluation based on the model's ongoing performance instead of\nrelying on a fixed test set. This paradigm not only provides a more robust\nability estimation but also significantly reduces the number of test items\nrequired. We analyze the current approaches, advantages, and underlying reasons\nfor adopting psychometrics in AI evaluation. We propose that adaptive testing\nwill become the new norm in AI model evaluation, enhancing both the efficiency\nand effectiveness of assessing advanced intelligence systems.", "published": "2023-06-18 09:54:33", "link": "http://arxiv.org/abs/2306.10512v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolutionary Verbalizer Search for Prompt-based Few Shot Text\n  Classification", "abstract": "Recent advances for few-shot text classification aim to wrap textual inputs\nwith task-specific prompts to cloze questions. By processing them with a masked\nlanguage model to predict the masked tokens and using a verbalizer that\nconstructs the mapping between predicted words and target labels. This approach\nof using pre-trained language models is called prompt-based tuning, which could\nremarkably outperform conventional fine-tuning approach in the low-data\nscenario. As the core of prompt-based tuning, the verbalizer is usually\nhandcrafted with human efforts or suboptimally searched by gradient descent. In\nthis paper, we focus on automatically constructing the optimal verbalizer and\npropose a novel evolutionary verbalizer search (EVS) algorithm, to improve\nprompt-based tuning with the high-performance verbalizer. Specifically,\ninspired by evolutionary algorithm (EA), we utilize it to automatically evolve\nvarious verbalizers during the evolutionary procedure and select the best one\nafter several iterations. Extensive few-shot experiments on five text\nclassification datasets show the effectiveness of our method.", "published": "2023-06-18 10:03:11", "link": "http://arxiv.org/abs/2306.10514v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniMC: A Unified Framework for Long-Term Memory Conversation via\n  Relevance Representation Learning", "abstract": "Open-domain long-term memory conversation can establish long-term intimacy\nwith humans, and the key is the ability to understand and memorize long-term\ndialogue history information. Existing works integrate multiple models for\nmodelling through a pipeline, which ignores the coupling between different\nstages. In this paper, we propose a Unified framework for Long-term Memory\nConversations (UniMC), which increases the connection between different stages\nby learning relevance representation. Specifically, we decompose the main task\ninto three subtasks based on probability graphs: 1) conversation summarization,\n2) memory retrieval, 3) memory-augmented generation. Each subtask involves\nlearning a representation for calculating the relevance between the query and\nmemory, which is modelled by inserting a special token at the beginning of the\ndecoder input. The relevance representation learning strengthens the connection\nacross subtasks through parameter sharing and joint training. Extensive\nexperimental results show that the proposed method consistently improves over\nstrong baselines and yields better dialogue consistency and engagingness.", "published": "2023-06-18 12:30:50", "link": "http://arxiv.org/abs/2306.10543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Summarization from Leaderboards to Practice: Choosing A Representation\n  Backbone and Ensuring Robustness", "abstract": "Academic literature does not give much guidance on how to build the best\npossible customer-facing summarization system from existing research\ncomponents. Here we present analyses to inform the selection of a system\nbackbone from popular models; we find that in both automatic and human\nevaluation, BART performs better than PEGASUS and T5. We also find that when\napplied cross-domain, summarizers exhibit considerably worse performance. At\nthe same time, a system fine-tuned on heterogeneous domains performs well on\nall domains and will be most suitable for a broad-domain summarizer. Our work\nhighlights the need for heterogeneous domain summarization benchmarks. We find\nconsiderable variation in system output that can be captured only with human\nevaluation and are thus unlikely to be reflected in standard leaderboards with\nonly automatic evaluation.", "published": "2023-06-18 13:35:41", "link": "http://arxiv.org/abs/2306.10555v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis", "abstract": "Sentiment analysis is a well-known natural language processing task that\ninvolves identifying the emotional tone or polarity of a given piece of text.\nWith the growth of social media and other online platforms, sentiment analysis\nhas become increasingly crucial for businesses and organizations seeking to\nmonitor and comprehend customer feedback as well as opinions. Supervised\nlearning algorithms have been popularly employed for this task, but they\nrequire human-annotated text to create the classifier. To overcome this\nchallenge, lexicon-based tools have been used. A drawback of lexicon-based\nalgorithms is their reliance on pre-defined sentiment lexicons, which may not\ncapture the full range of sentiments in natural language. ChatGPT is a new\nproduct of OpenAI and has emerged as the most popular AI product. It can answer\nquestions on various topics and tasks. This study explores the use of ChatGPT\nas a tool for data labeling for different sentiment analysis tasks. It is\nevaluated on two distinct sentiment analysis datasets with varying purposes.\nThe results demonstrate that ChatGPT outperforms other lexicon-based\nunsupervised methods with significant improvements in overall accuracy.\nSpecifically, compared to the best-performing lexical-based algorithms, ChatGPT\nachieves a remarkable increase in accuracy of 20% for the tweets dataset and\napproximately 25% for the Amazon reviews dataset. These findings highlight the\nexceptional performance of ChatGPT in sentiment analysis tasks, surpassing\nexisting lexicon-based approaches by a significant margin. The evidence\nsuggests it can be used for annotation on different sentiment analysis events\nand taskss.", "published": "2023-06-18 12:20:42", "link": "http://arxiv.org/abs/2306.17177v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Information Extraction with Meta-Pretrained Self-Retrieval", "abstract": "Universal Information Extraction~(Universal IE) aims to solve different\nextraction tasks in a uniform text-to-structure generation manner. Such a\ngeneration procedure tends to struggle when there exist complex information\nstructures to be extracted. Retrieving knowledge from external knowledge bases\nmay help models to overcome this problem but it is impossible to construct a\nknowledge base suitable for various IE tasks. Inspired by the fact that large\namount of knowledge are stored in the pretrained language models~(PLM) and can\nbe retrieved explicitly, in this paper, we propose MetaRetriever to retrieve\ntask-specific knowledge from PLMs to enhance universal IE. As different IE\ntasks need different knowledge, we further propose a Meta-Pretraining Algorithm\nwhich allows MetaRetriever to quicktly achieve maximum task-specific retrieval\nperformance when fine-tuning on downstream IE tasks. Experimental results show\nthat MetaRetriever achieves the new state-of-the-art on 4 IE tasks, 12 datasets\nunder fully-supervised, low-resource and few-shot scenarios.", "published": "2023-06-18 00:16:00", "link": "http://arxiv.org/abs/2306.10444v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generation of Radiology Findings in Chest X-Ray by Leveraging\n  Collaborative Knowledge", "abstract": "Among all the sub-sections in a typical radiology report, the Clinical\nIndications, Findings, and Impression often reflect important details about the\nhealth status of a patient. The information included in Impression is also\noften covered in Findings. While Findings and Impression can be deduced by\ninspecting the image, Clinical Indications often require additional context.\nThe cognitive task of interpreting medical images remains the most critical and\noften time-consuming step in the radiology workflow. Instead of generating an\nend-to-end radiology report, in this paper, we focus on generating the Findings\nfrom automated interpretation of medical images, specifically chest X-rays\n(CXRs). Thus, this work focuses on reducing the workload of radiologists who\nspend most of their time either writing or narrating the Findings. Unlike past\nresearch, which addresses radiology report generation as a single-step image\ncaptioning task, we have further taken into consideration the complexity of\ninterpreting CXR images and propose a two-step approach: (a) detecting the\nregions with abnormalities in the image, and (b) generating relevant text for\nregions with abnormalities by employing a generative large language model\n(LLM). This two-step approach introduces a layer of interpretability and aligns\nthe framework with the systematic reasoning that radiologists use when\nreviewing a CXR.", "published": "2023-06-18 00:51:28", "link": "http://arxiv.org/abs/2306.10448v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Gender Bias in Transformer Models: A comprehensive survey", "abstract": "Gender bias in artificial intelligence (AI) has emerged as a pressing concern\nwith profound implications for individuals' lives. This paper presents a\ncomprehensive survey that explores gender bias in Transformer models from a\nlinguistic perspective. While the existence of gender bias in language models\nhas been acknowledged in previous studies, there remains a lack of consensus on\nhow to effectively measure and evaluate this bias. Our survey critically\nexamines the existing literature on gender bias in Transformers, shedding light\non the diverse methodologies and metrics employed to assess bias. Several\nlimitations in current approaches to measuring gender bias in Transformers are\nidentified, encompassing the utilization of incomplete or flawed metrics,\ninadequate dataset sizes, and a dearth of standardization in evaluation\nmethods. Furthermore, our survey delves into the potential ramifications of\ngender bias in Transformers for downstream applications, including dialogue\nsystems and machine translation. We underscore the importance of fostering\nequity and fairness in these systems by emphasizing the need for heightened\nawareness and accountability in developing and deploying language technologies.\nThis paper serves as a comprehensive overview of gender bias in Transformer\nmodels, providing novel insights and offering valuable directions for future\nresearch in this critical domain.", "published": "2023-06-18 11:40:47", "link": "http://arxiv.org/abs/2306.10530v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "\"You might think about slightly revising the title\": identifying hedges\n  in peer-tutoring interactions", "abstract": "Hedges play an important role in the management of conversational\ninteraction. In peer tutoring, they are notably used by tutors in dyads (pairs\nof interlocutors) experiencing low rapport to tone down the impact of\ninstructions and negative feedback. Pursuing the objective of building a\ntutoring agent that manages rapport with students in order to improve learning,\nwe used a multimodal peer-tutoring dataset to construct a computational\nframework for identifying hedges. We compared approaches relying on pre-trained\nresources with others that integrate insights from the social science\nliterature. Our best performance involved a hybrid approach that outperforms\nthe existing baseline while being easier to interpret. We employ a model\nexplainability tool to explore the features that characterize hedges in\npeer-tutoring conversations, and we identify some novel features, and the\nbenefits of such a hybrid model approach.", "published": "2023-06-18 12:47:54", "link": "http://arxiv.org/abs/2306.14911v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MOSPC: MOS Prediction Based on Pairwise Comparison", "abstract": "As a subjective metric to evaluate the quality of synthesized speech, Mean\nopinion score~(MOS) usually requires multiple annotators to score the same\nspeech. Such an annotation approach requires a lot of manpower and is also\ntime-consuming. MOS prediction model for automatic evaluation can significantly\nreduce labor cost. In previous works, it is difficult to accurately rank the\nquality of speech when the MOS scores are close. However, in practical\napplications, it is more important to correctly rank the quality of synthesis\nsystems or sentences than simply predicting MOS scores. Meanwhile, as each\nannotator scores multiple audios during annotation, the score is probably a\nrelative value based on the first or the first few speech scores given by the\nannotator. Motivated by the above two points, we propose a general framework\nfor MOS prediction based on pair comparison (MOSPC), and we utilize C-Mixup\nalgorithm to enhance the generalization performance of MOSPC. The experiments\non BVCC and VCC2018 show that our framework outperforms the baselines on most\nof the correlation coefficient metrics, especially on the metric KTAU related\nto quality ranking. And our framework also surpasses the strong baseline in\nranking accuracy on each fine-grained segment. These results indicate that our\nframework contributes to improving the ranking accuracy of speech quality.", "published": "2023-06-18 07:38:17", "link": "http://arxiv.org/abs/2306.10493v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Importance of Human-Labeled Data in the Era of LLMs", "abstract": "The advent of large language models (LLMs) has brought about a revolution in\nthe development of tailored machine learning models and sparked debates on\nredefining data requirements. The automation facilitated by the training and\nimplementation of LLMs has led to discussions and aspirations that human-level\nlabeling interventions may no longer hold the same level of importance as in\nthe era of supervised learning. This paper presents compelling arguments\nsupporting the ongoing relevance of human-labeled data in the era of LLMs.", "published": "2023-06-18 12:12:03", "link": "http://arxiv.org/abs/2306.14910v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT\n  3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking", "abstract": "This study aimed to evaluate the proficiency of prominent Large Language\nModels (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and\nMicrosoft's Bing AI in discerning the truthfulness of news items using black\nbox testing. A total of 100 fact-checked news items, all sourced from\nindependent fact-checking agencies, were presented to each of these LLMs under\ncontrolled conditions. Their responses were classified into one of three\ncategories: True, False, and Partially True/False. The effectiveness of the\nLLMs was gauged based on the accuracy of their classifications against the\nverified facts provided by the independent agencies. The results showed a\nmoderate proficiency across all models, with an average score of 65.25 out of\n100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71,\nsuggesting an edge in newer LLMs' abilities to differentiate fact from\ndeception. However, when juxtaposed against the performance of human\nfact-checkers, the AI models, despite showing promise, lag in comprehending the\nsubtleties and contexts inherent in news information. The findings highlight\nthe potential of AI in the domain of fact-checking while underscoring the\ncontinued importance of human cognitive skills and the necessity for persistent\nadvancements in AI capabilities. Finally, the experimental data produced from\nthe simulation of this work is openly available on Kaggle.", "published": "2023-06-18 04:30:29", "link": "http://arxiv.org/abs/2306.17176v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Comparison of Machine Learning Methods for Assigning Software Issues to\n  Team Members", "abstract": "Software issues contain units of work to fix, improve, or create new threads\nduring the development and facilitate communication among the team members.\nAssigning an issue to the most relevant team member and determining a category\nof an issue is a tedious and challenging task. Wrong classifications cause\ndelays and rework in the project and trouble among the team members. This paper\nproposes a set of carefully curated linguistic features for shallow machine\nlearning methods and compares the performance of shallow and ensemble methods\nwith deep language models. Unlike the state-of-the-art, we assign issues to\nfour roles (designer, developer, tester, and leader) rather than to specific\nindividuals or teams to contribute to the generality of our solution. We also\nconsider the level of experience of the developers to reflect the industrial\npractices in our solution formulation. We collect and annotate five industrial\ndata sets from one of the top three global television producers to evaluate our\nproposal and compare it with deep language models. Our data sets contain 5324\nissues in total. We show that an ensemble classifier of shallow techniques\nachieves 0.92 for issue assignment in accuracy which is statistically\ncomparable to the state-of-the-art deep language models. The contributions\ninclude the public sharing of five annotated industrial issue data sets, the\ndevelopment of a clear and comprehensive feature set, the introduction of a\nnovel label set, and the validation of the efficacy of an ensemble classifier\nof shallow machine learning techniques.", "published": "2023-06-18 20:06:58", "link": "http://arxiv.org/abs/2307.00009v2", "categories": ["cs.CL", "cs.LG", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Channel-Spatial-Based Few-Shot Bird Sound Event Detection", "abstract": "In this paper, we propose a model for bird sound event detection that focuses\non a small number of training samples within the everyday long-tail\ndistribution. As a result, we investigate bird sound detection using the\nfew-shot learning paradigm. By integrating channel and spatial attention\nmechanisms, improved feature representations can be learned from few-shot\ntraining datasets. We develop a Metric Channel-Spatial Network model by\nincorporating a Channel Spatial Squeeze-Excitation block into the prototype\nnetwork, combining it with these attention mechanisms. We evaluate the Metric\nChannel Spatial Network model on the DCASE 2022 Take5 dataset benchmark,\nachieving an F-measure of 66.84% and a PSDS of 58.98%. Our experiment\ndemonstrates that the combination of channel and spatial attention mechanisms\neffectively enhances the performance of bird sound classification and\ndetection.", "published": "2023-06-18 08:39:57", "link": "http://arxiv.org/abs/2306.10499v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on\n  Language Models", "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have\nrecently achieved new state-of-the-art performance in zero-shot audio\ngeneration. In this paper, we explore the feasibility of LMs for zero-shot\nvoice conversion. An intuitive approach is to follow AudioLM - Tokenizing\nspeech into semantic and acoustic tokens respectively by HuBERT and\nSoundStream, and converting source semantic tokens to target acoustic tokens\nconditioned on acoustic tokens of the target speaker. However, such an approach\nencounters several issues: 1) the linguistic content contained in semantic\ntokens may get dispersed during multi-layer modeling while the lengthy speech\ninput in the voice conversion task makes contextual learning even harder; 2)\nthe semantic tokens still contain speaker-related information, which may be\nleaked to the target speech, lowering the target speaker similarity; 3) the\ngeneration diversity in the sampling of the LM can lead to unexpected outcomes\nduring inference, leading to unnatural pronunciation and speech quality\ndegradation. To mitigate these problems, we propose LM-VC, a two-stage language\nmodeling approach that generates coarse acoustic tokens for recovering the\nsource linguistic content and target speaker's timbre, and then reconstructs\nthe fine for acoustic details as converted speech. Specifically, to enhance\ncontent preservation and facilitates better disentanglement, a masked prefix LM\nwith a mask prediction strategy is used for coarse acoustic modeling. This\nmodel is encouraged to recover the masked content from the surrounding context\nand generate target speech based on the target speaker's utterance and\ncorrupted semantic tokens. Besides, to further alleviate the sampling error in\nthe generation, an external LM, which employs window attention to capture the\nlocal acoustic relations, is introduced to participate in the coarse acoustic\nmodeling.", "published": "2023-06-18 10:59:06", "link": "http://arxiv.org/abs/2306.10521v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SURT 2.0: Advances in Transducer-based Multi-talker Speech Recognition", "abstract": "The Streaming Unmixing and Recognition Transducer (SURT) model was proposed\nrecently as an end-to-end approach for continuous, streaming, multi-talker\nspeech recognition (ASR). Despite impressive results on multi-turn meetings,\nSURT has notable limitations: (i) it suffers from leakage and omission related\nerrors; (ii) it is computationally expensive, due to which it has not seen\nadoption in academia; and (iii) it has only been evaluated on synthetic\nmixtures. In this work, we propose several modifications to the original SURT\nwhich are carefully designed to fix the above limitations. In particular, we\n(i) change the unmixing module to a mask estimator that uses dual-path\nmodeling, (ii) use a streaming zipformer encoder and a stateless decoder for\nthe transducer, (iii) perform mixture simulation using force-aligned\nsubsegments, (iv) pre-train the transducer on single-speaker data, (v) use\nauxiliary objectives in the form of masking loss and encoder CTC loss, and (vi)\nperform domain adaptation for far-field recognition. We show that our\nmodifications allow SURT 2.0 to outperform its predecessor in terms of\nmulti-talker ASR results, while being efficient enough to train with academic\nresources. We conduct our evaluations on 3 publicly available meeting\nbenchmarks -- LibriCSS, AMI, and ICSI, where our best model achieves WERs of\n16.9%, 44.6% and 32.2%, respectively, on far-field unsegmented recordings. We\nrelease training recipes and pre-trained models:\nhttps://sites.google.com/view/surt2.", "published": "2023-06-18 13:49:53", "link": "http://arxiv.org/abs/2306.10559v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "DuTa-VC: A Duration-aware Typical-to-atypical Voice Conversion Approach\n  with Diffusion Probabilistic Model", "abstract": "We present a novel typical-to-atypical voice conversion approach (DuTa-VC),\nwhich (i) can be trained with nonparallel data (ii) first introduces diffusion\nprobabilistic model (iii) preserves the target speaker identity (iv) is aware\nof the phoneme duration of the target speaker. DuTa-VC consists of three parts:\nan encoder transforms the source mel-spectrogram into a duration-modified\nspeaker-independent mel-spectrogram, a decoder performs the reverse diffusion\nto generate the target mel-spectrogram, and a vocoder is applied to reconstruct\nthe waveform. Objective evaluations conducted on the UASpeech show that DuTa-VC\nis able to capture severity characteristics of dysarthric speech, reserves\nspeaker identity, and significantly improves dysarthric speech recognition as a\ndata augmentation. Subjective evaluations by two expert speech pathologists\nvalidate that DuTa-VC can preserve the severity and type of dysarthria of the\ntarget speakers in the synthesized speech.", "published": "2023-06-18 15:55:16", "link": "http://arxiv.org/abs/2306.10588v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "MARBLE: Music Audio Representation Benchmark for Universal Evaluation", "abstract": "In the era of extensive intersection between art and Artificial Intelligence\n(AI), such as image generation and fiction co-creation, AI for music remains\nrelatively nascent, particularly in music understanding. This is evident in the\nlimited work on deep music representations, the scarcity of large-scale\ndatasets, and the absence of a universal and community-driven benchmark. To\naddress this issue, we introduce the Music Audio Representation Benchmark for\nuniversaL Evaluation, termed MARBLE. It aims to provide a benchmark for various\nMusic Information Retrieval (MIR) tasks by defining a comprehensive taxonomy\nwith four hierarchy levels, including acoustic, performance, score, and\nhigh-level description. We then establish a unified protocol based on 14 tasks\non 8 public-available datasets, providing a fair and standard assessment of\nrepresentations of all open-sourced pre-trained models developed on music\nrecordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and\nreproducible suite for the community, with a clear statement on copyright\nissues on datasets. Results suggest recently proposed large-scale pre-trained\nmusical language models perform the best in most tasks, with room for further\nimprovement. The leaderboard and toolkit repository are published at\nhttps://marble-bm.shef.ac.uk to promote future music AI research.", "published": "2023-06-18 12:56:46", "link": "http://arxiv.org/abs/2306.10548v4", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Hearing Lips in Noise: Universal Viseme-Phoneme Mapping and Transfer for\n  Robust Audio-Visual Speech Recognition", "abstract": "Audio-visual speech recognition (AVSR) provides a promising solution to\nameliorate the noise-robustness of audio-only speech recognition with visual\ninformation. However, most existing efforts still focus on audio modality to\nimprove robustness considering its dominance in AVSR task, with noise\nadaptation techniques such as front-end denoise processing. Though effective,\nthese methods are usually faced with two practical challenges: 1) lack of\nsufficient labeled noisy audio-visual training data in some real-world\nscenarios and 2) less optimal model generality to unseen testing noises. In\nthis work, we investigate the noise-invariant visual modality to strengthen\nrobustness of AVSR, which can adapt to any testing noises while without\ndependence on noisy training data, a.k.a., unsupervised noise adaptation.\nInspired by human perception mechanism, we propose a universal viseme-phoneme\nmapping (UniVPM) approach to implement modality transfer, which can restore\nclean audio from visual signals to enable speech recognition under any noisy\nconditions. Extensive experiments on public benchmarks LRS3 and LRS2 show that\nour approach achieves the state-of-the-art under various noisy as well as clean\nconditions. In addition, we also outperform previous state-of-the-arts on\nvisual speech recognition task.", "published": "2023-06-18 13:53:34", "link": "http://arxiv.org/abs/2306.10563v1", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "MIR-GAN: Refining Frame-Level Modality-Invariant Representations with\n  Adversarial Network for Audio-Visual Speech Recognition", "abstract": "Audio-visual speech recognition (AVSR) attracts a surge of research interest\nrecently by leveraging multimodal signals to understand human speech.\nMainstream approaches addressing this task have developed sophisticated\narchitectures and techniques for multi-modality fusion and representation\nlearning. However, the natural heterogeneity of different modalities causes\ndistribution gap between their representations, making it challenging to fuse\nthem. In this paper, we aim to learn the shared representations across\nmodalities to bridge their gap. Different from existing similar methods on\nother multimodal tasks like sentiment analysis, we focus on the temporal\ncontextual dependencies considering the sequence-to-sequence task setting of\nAVSR. In particular, we propose an adversarial network to refine frame-level\nmodality-invariant representations (MIR-GAN), which captures the commonality\nacross modalities to ease the subsequent multimodal fusion process. Extensive\nexperiments on public benchmarks LRS3 and LRS2 show that our approach\noutperforms the state-of-the-arts.", "published": "2023-06-18 14:02:20", "link": "http://arxiv.org/abs/2306.10567v1", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "STHG: Spatial-Temporal Heterogeneous Graph Learning for Advanced\n  Audio-Visual Diarization", "abstract": "This report introduces our novel method named STHG for the Audio-Visual\nDiarization task of the Ego4D Challenge 2023. Our key innovation is that we\nmodel all the speakers in a video using a single, unified heterogeneous graph\nlearning framework. Unlike previous approaches that require a separate\ncomponent solely for the camera wearer, STHG can jointly detect the speech\nactivities of all people including the camera wearer. Our final method obtains\n61.1% DER on the test set of Ego4D, which significantly outperforms all the\nbaselines as well as last year's winner. Our submission achieved 1st place in\nthe Ego4D Challenge 2023. We additionally demonstrate that applying the\noff-the-shelf speech recognition system to the diarized speech segments by STHG\nproduces a competitive performance on the Speech Transcription task of this\nchallenge.", "published": "2023-06-18 17:55:02", "link": "http://arxiv.org/abs/2306.10608v4", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
