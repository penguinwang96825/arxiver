{"title": "Deciphering Diagnoses: How Large Language Models Explanations Influence\n  Clinical Decision Making", "abstract": "Clinical Decision Support Systems (CDSS) utilize evidence-based knowledge and\npatient data to offer real-time recommendations, with Large Language Models\n(LLMs) emerging as a promising tool to generate plain-text explanations for\nmedical decisions. This study explores the effectiveness and reliability of\nLLMs in generating explanations for diagnoses based on patient complaints.\nThree experienced doctors evaluated LLM-generated explanations of the\nconnection between patient complaints and doctor and model-assigned diagnoses\nacross several stages. Experimental results demonstrated that LLM explanations\nsignificantly increased doctors' agreement rates with given diagnoses and\nhighlighted potential errors in LLM outputs, ranging from 5% to 30%. The study\nunderscores the potential and challenges of LLMs in healthcare and emphasizes\nthe need for careful integration and evaluation to ensure patient safety and\noptimal clinical utility.", "published": "2023-10-03 00:08:23", "link": "http://arxiv.org/abs/2310.01708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stack Attention: Improving the Ability of Transformers to Model\n  Hierarchical Patterns", "abstract": "Attention, specifically scaled dot-product attention, has proven effective\nfor natural language, but it does not have a mechanism for handling\nhierarchical patterns of arbitrary nesting depth, which limits its ability to\nrecognize certain syntactic structures. To address this shortcoming, we propose\nstack attention: an attention operator that incorporates stacks, inspired by\ntheir theoretical connections to context-free languages (CFLs). We show that\nstack attention is analogous to standard attention, but with a latent model of\nsyntax that requires no syntactic supervision. We propose two variants: one\nrelated to deterministic pushdown automata (PDAs) and one based on\nnondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.\nWe show that transformers with stack attention are very effective at learning\nCFLs that standard transformers struggle on, achieving strong results on a CFL\nwith theoretically maximal parsing difficulty. We also show that stack\nattention is more effective at natural language modeling under a constrained\nparameter budget, and we include results on machine translation.", "published": "2023-10-03 02:18:06", "link": "http://arxiv.org/abs/2310.01749v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs", "abstract": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.", "published": "2023-10-03 05:17:08", "link": "http://arxiv.org/abs/2310.01801v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ring Attention with Blockwise Transformers for Near-Infinite Context", "abstract": "Transformers have emerged as the architecture of choice for many\nstate-of-the-art AI models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands imposed by Transformers\nlimit their ability to handle long sequences, thereby posing challenges in\nutilizing videos, actions, and other long-form sequences and modalities in\ncomplex environments. We present a novel approach, Ring Attention with\nBlockwise Transformers (Ring Attention), which leverages blockwise computation\nof self-attention and feedforward to distribute long sequences across multiple\ndevices while fully overlapping the communication of key-value blocks with the\ncomputation of blockwise attention. Our approach enables training and inference\nof sequences that are up to device count times longer than those achievable by\nprior memory-efficient Transformers, without resorting to approximations or\nincurring additional communication and computation overheads. Extensive\nexperiments on language modeling and reinforcement learning tasks demonstrate\nthe effectiveness of our approach in allowing millions of tokens context size\nand improving performance.", "published": "2023-10-03 08:44:50", "link": "http://arxiv.org/abs/2310.01889v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Controlling Topic-Focus Articulation in Meaning-to-Text Generation using\n  Graph Neural Networks", "abstract": "A bare meaning representation can be expressed in various ways using natural\nlanguage, depending on how the information is structured on the surface level.\nWe are interested in finding ways to control topic-focus articulation when\ngenerating text from meaning. We focus on distinguishing active and passive\nvoice for sentences with transitive verbs. The idea is to add pragmatic\ninformation such as topic to the meaning representation, thereby forcing either\nactive or passive voice when given to a natural language generation system. We\nuse graph neural models because there is no explicit information about word\norder in a meaning represented by a graph. We try three different methods for\ntopic-focus articulation (TFA) employing graph neural models for a\nmeaning-to-text generation task. We propose a novel encoding strategy about\nnode aggregation in graph neural models, which instead of traditional encoding\nby aggregating adjacent node information, learns node representations by using\ndepth-first search. The results show our approach can get competitive\nperformance with state-of-art graph models on general text generation, and lead\nto significant improvements on the task of active-passive conversion compared\nto traditional adjacency-based aggregation strategies. Different types of TFA\ncan have a huge impact on the performance of the graph models.", "published": "2023-10-03 13:51:01", "link": "http://arxiv.org/abs/2310.02053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Instances Need More Care: Rewriting Prompts for Instances with LLMs in\n  the Loop Yields Better Zero-Shot Performance", "abstract": "Large language models (LLMs) have revolutionized zero-shot task performance,\nmitigating the need for task-specific annotations while enhancing task\ngeneralizability. Despite its advancements, current methods using trigger\nphrases such as \"Let's think step by step\" remain limited. This study\nintroduces PRomPTed, an approach that optimizes the zero-shot prompts for\nindividual task instances following an innovative manner of \"LLMs in the loop\".\nOur comprehensive evaluation across 13 datasets and 10 task types based on\nGPT-4 reveals that PRomPTed significantly outperforms both the naive zero-shot\napproaches and a strong baseline (i.e., \"Output Refinement\") which refines the\ntask output instead of the input prompt. Our experimental results also\nconfirmed the generalization of this advantage to the relatively weaker\nGPT-3.5. Even more intriguingly, we found that leveraging GPT-3.5 to rewrite\nprompts for the stronger GPT-4 not only matches but occasionally exceeds the\nefficacy of using GPT-4 as the prompt rewriter. Our research thus presents a\nhuge value in not only enhancing zero-shot LLM performance but also potentially\nenabling supervising LLMs with their weaker counterparts, a capability\nattracting much interest recently. Finally, our additional experiments confirm\nthe generalization of the advantages to open-source LLMs such as Mistral 7B and\nMixtral 8x7B.", "published": "2023-10-03 14:51:34", "link": "http://arxiv.org/abs/2310.02107v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models Meet Knowledge Graphs to Answer Factoid Questions", "abstract": "Recently, it has been shown that the incorporation of structured knowledge\ninto Large Language Models significantly improves the results for a variety of\nNLP tasks. In this paper, we propose a method for exploring pre-trained\nText-to-Text Language Models enriched with additional information from\nKnowledge Graphs for answering factoid questions. More specifically, we propose\nan algorithm for subgraphs extraction from a Knowledge Graph based on question\nentities and answer candidates. Then, we procure easily interpreted information\nwith Transformer-based models through the linearization of the extracted\nsubgraphs. Final re-ranking of the answer candidates with the extracted\ninformation boosts Hits@1 scores of the pre-trained text-to-text language\nmodels by 4-6%.", "published": "2023-10-03 15:57:00", "link": "http://arxiv.org/abs/2310.02166v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Language Models be Instructed to Protect Personal Information?", "abstract": "Large multimodal language models have proven transformative in numerous\napplications. However, these models have been shown to memorize and leak\npre-training data, raising serious user privacy and information security\nconcerns. While data leaks should be prevented, it is also crucial to examine\nthe trade-off between the privacy protection and model utility of proposed\napproaches. In this paper, we introduce PrivQA -- a multimodal benchmark to\nassess this privacy/utility trade-off when a model is instructed to protect\nspecific categories of personal information in a simulated scenario. We also\npropose a technique to iteratively self-moderate responses, which significantly\nimproves privacy. However, through a series of red-teaming experiments, we find\nthat adversaries can also easily circumvent these protections with simple\njailbreaking methods through textual and/or image inputs. We believe PrivQA has\nthe potential to support the development of new models with improved privacy\nprotections, as well as the adversarial robustness of these protections. We\nrelease the entire PrivQA dataset at https://llm-access-control.github.io/.", "published": "2023-10-03 17:30:33", "link": "http://arxiv.org/abs/2310.02224v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework", "abstract": "Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance and diagnosis. Current CHAs, especially\nthose utilizing Large Language Models (LLMs), primarily focus on conversation\naspects. However, they offer limited agent capabilities, specifically lacking\nmulti-step problem-solving, personalized conversations, and multimodal data\nanalysis. Our aim is to overcome these limitations. We propose openCHA, an\nopen-source LLM-powered framework, to empower conversational agents to generate\na personalized response for users' healthcare queries. This framework enables\ndevelopers to integrate external sources including data sources, knowledge\nbases, and analysis models, into their LLM-based solutions. openCHA includes an\norchestrator to plan and execute actions for gathering information from\nexternal sources, essential for formulating responses to user inquiries. It\nfacilitates knowledge acquisition, problem-solving capabilities, multilingual\nand multimodal conversations, and fosters interaction with various AI\nplatforms. We illustrate the framework's proficiency in handling complex\nhealthcare tasks via two demonstrations and four use cases. Moreover, we\nrelease openCHA as open source available to the community via GitHub.", "published": "2023-10-03 18:54:10", "link": "http://arxiv.org/abs/2310.02374v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of\n  Large Language Models with Misconceptions", "abstract": "We propose novel evaluations for mathematical reasoning capabilities of Large\nLanguage Models (LLMs) based on mathematical misconceptions. Our primary\napproach is to simulate LLMs as a novice learner and an expert tutor, aiming to\nidentify the incorrect answer to math question resulted from a specific\nmisconception and to recognize the misconception(s) behind an incorrect answer,\nrespectively. Contrary to traditional LLMs-based mathematical evaluations that\nfocus on answering math questions correctly, our approach takes inspirations\nfrom principles in educational learning sciences. We explicitly ask LLMs to\nmimic a novice learner by answering questions in a specific incorrect manner\nbased on incomplete knowledge; and to mimic an expert tutor by identifying\nmisconception(s) corresponding to an incorrect answer to a question. Using\nsimple grade-school math problems, our experiments reveal that, while LLMs can\neasily answer these questions correctly, they struggle to identify 1) the\nincorrect answer corresponding to specific incomplete knowledge\n(misconceptions); 2) the misconceptions that explain particular incorrect\nanswers. Our study indicates new opportunities for enhancing LLMs' math\nreasoning capabilities, especially on developing robust student simulation and\nexpert tutoring models in the educational applications such as intelligent\ntutoring systems.", "published": "2023-10-03 21:19:50", "link": "http://arxiv.org/abs/2310.02439v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Backdoor Adjustment of Confounding by Provenance for Robust Text\n  Classification of Multi-institutional Clinical Notes", "abstract": "Natural Language Processing (NLP) methods have been broadly applied to\nclinical tasks. Machine learning and deep learning approaches have been used to\nimprove the performance of clinical NLP. However, these approaches require\nsufficiently large datasets for training, and trained models have been shown to\ntransfer poorly across sites. These issues have led to the promotion of data\ncollection and integration across different institutions for accurate and\nportable models. However, this can introduce a form of bias called confounding\nby provenance. When source-specific data distributions differ at deployment,\nthis may harm model performance. To address this issue, we evaluate the utility\nof backdoor adjustment for text classification in a multi-site dataset of\nclinical notes annotated for mentions of substance abuse. Using an evaluation\nframework devised to measure robustness to distributional shifts, we assess the\nutility of backdoor adjustment. Our results indicate that backdoor adjustment\ncan effectively mitigate for confounding shift.", "published": "2023-10-03 21:40:44", "link": "http://arxiv.org/abs/2310.02451v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEA: Sparse Linear Attention with Estimated Attention Mask", "abstract": "The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.", "published": "2023-10-03 03:56:26", "link": "http://arxiv.org/abs/2310.01777v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large Language Models Cannot Self-Correct Reasoning Yet", "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with\ntheir unparalleled text generation capabilities across various applications.\nNevertheless, concerns persist regarding the accuracy and appropriateness of\ntheir generated content. A contemporary methodology, self-correction, has been\nproposed as a remedy to these issues. Building upon this premise, this paper\ncritically examines the role and efficacy of self-correction within LLMs,\nshedding light on its true potential and limitations. Central to our\ninvestigation is the notion of intrinsic self-correction, whereby an LLM\nattempts to correct its initial responses based solely on its inherent\ncapabilities, without the crutch of external feedback. In the context of\nreasoning, our research indicates that LLMs struggle to self-correct their\nresponses without external feedback, and at times, their performance even\ndegrades after self-correction. Drawing from these insights, we offer\nsuggestions for future research and practical applications in this field.", "published": "2023-10-03 04:56:12", "link": "http://arxiv.org/abs/2310.01798v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Benchmarking and Improving Generator-Validator Consistency of Language\n  Models", "abstract": "As of September 2023, ChatGPT correctly answers \"what is 7+8\" with 15, but\nwhen asked \"7+8=15, True or False\" it responds with \"False\". This inconsistency\nbetween generating and validating an answer is prevalent in language models\n(LMs) and erodes trust. In this paper, we propose a framework for measuring the\nconsistency between generation and validation (which we call\ngenerator-validator consistency, or GV-consistency), finding that even GPT-4, a\nstate-of-the-art LM, is GV-consistent only 76% of the time. To improve the\nconsistency of LMs, we propose to finetune on the filtered generator and\nvalidator responses that are GV-consistent, and call this approach consistency\nfine-tuning. We find that this approach improves GV-consistency of Alpaca-30B\nfrom 60% to 93%, and the improvement extrapolates to unseen tasks and domains\n(e.g., GV-consistency for positive style transfers extrapolates to unseen\nstyles like humor). In addition to improving consistency, consistency\nfine-tuning improves both generator quality and validator accuracy without\nusing any labeled data. Evaluated across 6 tasks, including math questions,\nknowledge-intensive QA, and instruction following, our method improves the\ngenerator quality by 16% and the validator accuracy by 6.3% across all tasks.", "published": "2023-10-03 07:23:22", "link": "http://arxiv.org/abs/2310.01846v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better\n  Account for Brain Language Representations?", "abstract": "To decipher the algorithm underlying the human brain's language\nrepresentation, previous work probed brain responses to language input with\npre-trained artificial neural network (ANN) models fine-tuned on NLU tasks.\nHowever, full fine-tuning generally updates the entire parametric space and\ndistorts pre-trained features, cognitively inconsistent with the brain's robust\nmulti-task learning ability. Prompt-tuning, in contrast, protects pre-trained\nweights and learns task-specific embeddings to fit a task. Could prompt-tuning\ngenerate representations that better account for the brain's language\nrepresentations than fine-tuning? If so, what kind of NLU task leads a\npre-trained model to better decode the information represented in the human\nbrain? We investigate these questions by comparing prompt-tuned and fine-tuned\nrepresentations in neural decoding, that is predicting the linguistic stimulus\nfrom the brain activities evoked by the stimulus. We find that on none of the\n10 NLU tasks, full fine-tuning significantly outperforms prompt-tuning in\nneural decoding, implicating that a more brain-consistent tuning method yields\nrepresentations that better correlate with brain data. Moreover, we identify\nthat tasks dealing with fine-grained concept meaning yield representations that\nbetter decode brain activation patterns than other tasks, especially the\nsyntactic chunking task. This indicates that our brain encodes more\nfine-grained concept information than shallow syntactic information when\nrepresenting languages.", "published": "2023-10-03 07:34:30", "link": "http://arxiv.org/abs/2310.01854v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Hierarchical Evaluation Framework: Best Practices for Human Evaluation", "abstract": "Human evaluation plays a crucial role in Natural Language Processing (NLP) as\nit assesses the quality and relevance of developed systems, thereby\nfacilitating their enhancement. However, the absence of widely accepted human\nevaluation metrics in NLP hampers fair comparisons among different systems and\nthe establishment of universal assessment standards. Through an extensive\nanalysis of existing literature on human evaluation metrics, we identified\nseveral gaps in NLP evaluation methodologies. These gaps served as motivation\nfor developing our own hierarchical evaluation framework. The proposed\nframework offers notable advantages, particularly in providing a more\ncomprehensive representation of the NLP system's performance. We applied this\nframework to evaluate the developed Machine Reading Comprehension system, which\nwas utilized within a human-AI symbiosis model. The results highlighted the\nassociations between the quality of inputs and outputs, underscoring the\nnecessity to evaluate both components rather than solely focusing on outputs.\nIn future work, we will investigate the potential time-saving benefits of our\nproposed framework for evaluators assessing NLP systems.", "published": "2023-10-03 09:46:02", "link": "http://arxiv.org/abs/2310.01917v2", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Language Models as Knowledge Bases for Visual Word Sense Disambiguation", "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies\nbetween linguistic sense disambiguation and fine-grained multimodal retrieval.\nThe recent advancements in the development of visiolinguistic (VL) transformers\nsuggest some off-the-self implementations with encouraging results, which\nhowever we argue that can be further improved. To this end, we propose some\nknowledge-enhancement techniques towards improving the retrieval performance of\nVL transformers via the usage of Large Language Models (LLMs) as Knowledge\nBases. More specifically, knowledge stored in LLMs is retrieved with the help\nof appropriate prompts in a zero-shot manner, achieving performance\nadvancements. Moreover, we convert VWSD to a purely textual question-answering\n(QA) problem by considering generated image captions as multiple-choice\ncandidate answers. Zero-shot and few-shot prompting strategies are leveraged to\nexplore the potential of such a transformation, while Chain-of-Thought (CoT)\nprompting in the zero-shot setting is able to reveal the internal reasoning\nsteps an LLM follows to select the appropriate candidate. In total, our\npresented approach is the first one to analyze the merits of exploiting\nknowledge stored in LLMs in different ways to solve WVSD.", "published": "2023-10-03 11:11:55", "link": "http://arxiv.org/abs/2310.01960v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Jury: A Comprehensive Evaluation Toolkit", "abstract": "Evaluation plays a critical role in deep learning as a fundamental block of\nany prediction-based system. However, the vast number of Natural Language\nProcessing (NLP) tasks and the development of various metrics have led to\nchallenges in evaluating different systems with different metrics. To address\nthese challenges, we introduce jury, a toolkit that provides a unified\nevaluation framework with standardized structures for performing evaluation\nacross different tasks and metrics. The objective of jury is to standardize and\nimprove metric evaluation for all systems and aid the community in overcoming\nthe challenges in evaluation. Since its open-source release, jury has reached a\nwide audience and is available at https://github.com/obss/jury.", "published": "2023-10-03 13:31:28", "link": "http://arxiv.org/abs/2310.02040v2", "categories": ["cs.CL", "cs.AI", "I.2.7; D.1.3"], "primary_category": "cs.CL"}
{"title": "Tuning Large language model for End-to-end Speech Translation", "abstract": "With the emergence of large language models (LLMs), multimodal models based\non LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,\nand SpeechGPT exhibit an impressive ability to comprehend and generate human\ninstructions. However, their performance often falters when faced with complex\ntasks like end-to-end speech translation (E2E-ST), a cross-language and\ncross-modal translation task. In comparison to single-modal models, multimodal\nmodels lag behind in these scenarios. This paper introduces LST, a Large\nmultimodal model designed to excel at the E2E-ST task. LST consists of a speech\nfrontend, an adapter, and a LLM backend. The training of LST consists of two\nstages: (1) Modality adjustment, where the adapter is tuned to align speech\nrepresentation with text embedding space, and (2) Downstream task fine-tuning,\nwhere both the adapter and LLM model are trained to optimize performance on the\nE2EST task. Experimental results on the MuST-C speech translation benchmark\ndemonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on\nEn-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a\nnew state-of-the-art. Additionally, we conduct an in-depth analysis of\nsingle-modal model selection and the impact of training strategies, which lays\nthe foundation for future research. We will open up our code and models after\nreview.", "published": "2023-10-03 13:43:50", "link": "http://arxiv.org/abs/2310.02050v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus", "abstract": "In this report, we describe the vision, challenges, and scientific\ncontributions of the Task Wizard team, TWIZ, in the Alexa Prize TaskBot\nChallenge 2022. Our vision, is to build TWIZ bot as an helpful, multimodal,\nknowledgeable, and engaging assistant that can guide users towards the\nsuccessful completion of complex manual tasks. To achieve this, we focus our\nefforts on three main research questions: (1) Humanly-Shaped Conversations, by\nproviding information in a knowledgeable way; (2) Multimodal Stimulus, making\nuse of various modalities including voice, images, and videos; and (3)\nZero-shot Conversational Flows, to improve the robustness of the interaction to\nunseen scenarios. TWIZ is an assistant capable of supporting a wide range of\ntasks, with several innovative features such as creative cooking, video\nnavigation through voice, and the robust TWIZ-LLM, a Large Language Model\ntrained for dialoguing about complex manual tasks. Given ratings and feedback\nprovided by users, we observed that TWIZ bot is an effective and robust system,\ncapable of guiding users through tasks while providing several multimodal\nstimuli.", "published": "2023-10-03 14:59:35", "link": "http://arxiv.org/abs/2310.02118v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Extraction of Medication and Temporal Relation from Clinical Text using\n  Neural Language Models", "abstract": "Clinical texts, represented in electronic medical records (EMRs), contain\nrich medical information and are essential for disease prediction, personalised\ninformation recommendation, clinical decision support, and medication pattern\nmining and measurement. Relation extractions between medication mentions and\ntemporal information can further help clinicians better understand the\npatients' treatment history. To evaluate the performances of deep learning (DL)\nand large language models (LLMs) in medication extraction and temporal\nrelations classification, we carry out an empirical investigation of\n\\textbf{MedTem} project using several advanced learning structures including\nBiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER),\nand BERT-CNN for temporal relation extraction (RE), in addition to the\nexploration of different word embedding techniques. Furthermore, we also\ndesigned a set of post-processing roles to generate structured output on\nmedications and the temporal relation. Our experiments show that CNN-BiLSTM\nslightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding\n75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro\nAverage. BERT-CNN model also produced reasonable evaluation scores 64.48,\n67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction\ntest set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted\nat \\url{https://github.com/HECTA-UoM/MedTem}", "published": "2023-10-03 17:37:22", "link": "http://arxiv.org/abs/2310.02229v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Who's Harry Potter? Approximate Unlearning in LLMs", "abstract": "Large language models (LLMs) are trained on massive internet corpora that\noften contain copyrighted content. This poses legal and ethical challenges for\nthe developers and users of these models, as well as the original authors and\npublishers. In this paper, we propose a novel technique for unlearning a subset\nof the training data from a LLM, without having to retrain it from scratch.\n  We evaluate our technique on the task of unlearning the Harry Potter books\nfrom the Llama2-7b model (a generative language model recently open-sourced by\nMeta). While the model took over 184K GPU-hours to pretrain, we show that in\nabout 1 GPU hour of finetuning, we effectively erase the model's ability to\ngenerate or recall Harry Potter-related content, while its performance on\ncommon benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains\nalmost unaffected. We make our fine-tuned model publicly available on\nHuggingFace for community evaluation. To the best of our knowledge, this is the\nfirst paper to present an effective technique for unlearning in generative\nlanguage models.\n  Our technique consists of three main components: First, we use a reinforced\nmodel that is further trained on the target data to identify the tokens that\nare most related to the unlearning target, by comparing its logits with those\nof a baseline model. Second, we replace idiosyncratic expressions in the target\ndata with generic counterparts, and leverage the model's own predictions to\ngenerate alternative labels for every token. These labels aim to approximate\nthe next-token predictions of a model that has not been trained on the target\ndata. Third, we finetune the model on these alternative labels, which\neffectively erases the original text from the model's memory whenever it is\nprompted with its context.", "published": "2023-10-03 17:48:14", "link": "http://arxiv.org/abs/2310.02238v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Harnessing Pre-Trained Sentence Transformers for Offensive Language\n  Detection in Indian Languages", "abstract": "In our increasingly interconnected digital world, social media platforms have\nemerged as powerful channels for the dissemination of hate speech and offensive\ncontent. This work delves into the domain of hate speech detection, placing\nspecific emphasis on three low-resource Indian languages: Bengali, Assamese,\nand Gujarati. The challenge is framed as a text classification task, aimed at\ndiscerning whether a tweet contains offensive or non-offensive content.\nLeveraging the HASOC 2023 datasets, we fine-tuned pre-trained BERT and SBERT\nmodels to evaluate their effectiveness in identifying hate speech. Our findings\nunderscore the superiority of monolingual sentence-BERT models, particularly in\nthe Bengali language, where we achieved the highest ranking. However, the\nperformance in Assamese and Gujarati languages signifies ongoing opportunities\nfor enhancement. Our goal is to foster inclusive online spaces by countering\nhate speech proliferation.", "published": "2023-10-03 17:53:09", "link": "http://arxiv.org/abs/2310.02249v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ProtoNER: Few shot Incremental Learning for Named Entity Recognition\n  using Prototypical Networks", "abstract": "Key value pair (KVP) extraction or Named Entity Recognition(NER) from\nvisually rich documents has been an active area of research in document\nunderstanding and data extraction domain. Several transformer based models such\nas LayoutLMv2, LayoutLMv3, and LiLT have emerged achieving state of the art\nresults. However, addition of even a single new class to the existing model\nrequires (a) re-annotation of entire training dataset to include this new class\nand (b) retraining the model again. Both of these issues really slow down the\ndeployment of updated model. \\\\ We present \\textbf{ProtoNER}: Prototypical\nNetwork based end-to-end KVP extraction model that allows addition of new\nclasses to an existing model while requiring minimal number of newly annotated\ntraining samples. The key contributions of our model are: (1) No dependency on\ndataset used for initial training of the model, which alleviates the need to\nretain original training dataset for longer duration as well as data\nre-annotation which is very time consuming task, (2) No intermediate synthetic\ndata generation which tends to add noise and results in model's performance\ndegradation, and (3) Hybrid loss function which allows model to retain\nknowledge about older classes as well as learn about newly added classes.\\\\\nExperimental results show that ProtoNER finetuned with just 30 samples is able\nto achieve similar results for the newly added classes as that of regular model\nfinetuned with 2600 samples.", "published": "2023-10-03 18:52:19", "link": "http://arxiv.org/abs/2310.02372v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit\n  Quantization and Robustness", "abstract": "Large Mixture of Experts (MoE) models could achieve state-of-the-art quality\non various language tasks, including machine translation task, thanks to the\nefficient model scaling capability with expert parallelism. However, it has\nbrought a fundamental issue of larger memory consumption and increased memory\nbandwidth bottleneck at deployment time. In this paper, we propose Mixture of\nQuantized Experts (MoQE) which is a simple weight-only quantization method\napplying ultra low-bit down to 2-bit quantizations only to expert weights for\nmitigating the increased memory and latency issues of MoE models. We show that\nlow-bit quantization together with the MoE architecture delivers a reliable\nmodel performance while reducing the memory size significantly even without any\nadditional training in most cases. In particular, expert layers in MoE models\nare much more robust to the quantization than conventional feedforward networks\n(FFN) layers. In our comprehensive analysis, we show that MoE models with 2-bit\nexpert weights can deliver better model performance than the dense model\ntrained on the same dataset. As a result of low-bit quantization, we show the\nmodel size can be reduced by 79.6% of the original half precision floating\npoint (fp16) MoE model. Combined with an optimized GPU runtime implementation,\nit also achieves 1.24X speed-up on A100 GPUs.", "published": "2023-10-03 20:11:23", "link": "http://arxiv.org/abs/2310.02410v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can Large Language Models Provide Security & Privacy Advice? Measuring\n  the Ability of LLMs to Refute Misconceptions", "abstract": "Users seek security & privacy (S&P) advice from online resources, including\ntrusted websites and content-sharing platforms. These resources help users\nunderstand S&P technologies and tools and suggest actionable strategies. Large\nLanguage Models (LLMs) have recently emerged as trusted information sources.\nHowever, their accuracy and correctness have been called into question. Prior\nresearch has outlined the shortcomings of LLMs in answering multiple-choice\nquestions and user ability to inadvertently circumvent model restrictions\n(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable\nS&P advice is not well-explored. In this paper, we measure their ability to\nrefute popular S&P misconceptions that the general public holds. We first study\nrecent academic literature to curate a dataset of over a hundred S&P-related\nmisconceptions across six different topics. We then query two popular LLMs\n(Bard and ChatGPT) and develop a labeling guide to evaluate their responses to\nthese misconceptions. To comprehensively evaluate their responses, we further\napply three strategies: query each misconception multiple times, generate and\nquery their paraphrases, and solicit source URLs of the responses. Both models\ndemonstrate, on average, a 21.3% non-negligible error rate, incorrectly\nsupporting popular S&P misconceptions. The error rate increases to 32.6% when\nwe repeatedly query LLMs with the same or paraphrased misconceptions. We also\nexpose that models may partially support a misconception or remain\nnoncommittal, refusing a firm stance on misconceptions. Our exploration of\ninformation sources for responses revealed that LLMs are susceptible to\nproviding invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to\nunrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).", "published": "2023-10-03 20:54:29", "link": "http://arxiv.org/abs/2310.02431v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "The Empty Signifier Problem: Towards Clearer Paradigms for\n  Operationalising \"Alignment\" in Large Language Models", "abstract": "In this paper, we address the concept of \"alignment\" in large language models\n(LLMs) through the lens of post-structuralist socio-political theory,\nspecifically examining its parallels to empty signifiers. To establish a shared\nvocabulary around how abstract concepts of alignment are operationalised in\nempirical datasets, we propose a framework that demarcates: 1) which dimensions\nof model behaviour are considered important, then 2) how meanings and\ndefinitions are ascribed to these dimensions, and by whom. We situate existing\nempirical literature and provide guidance on deciding which paradigm to follow.\nThrough this framework, we aim to foster a culture of transparency and critical\nevaluation, aiding the community in navigating the complexities of aligning\nLLMs with human populations.", "published": "2023-10-03 22:02:17", "link": "http://arxiv.org/abs/2310.02457v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Investigating Large Language Models' Perception of Emotion Using\n  Appraisal Theory", "abstract": "Large Language Models (LLM) like ChatGPT have significantly advanced in\nrecent years and are now being used by the general public. As more people\ninteract with these systems, improving our understanding of these black box\nmodels is crucial, especially regarding their understanding of human\npsychological aspects. In this work, we investigate their emotion perception\nthrough the lens of appraisal and coping theory using the Stress and Coping\nProcess Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting\nof multiple stories that evolve over time and differ in key appraisal variables\nsuch as controllability and changeability. We applied SCPQ to three recent LLMs\nfrom OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with\npredictions from the appraisal theory and human data. The results show that\nLLMs' responses are similar to humans in terms of dynamics of appraisal and\ncoping, but their responses did not differ along key appraisal dimensions as\npredicted by the theory and data. The magnitude of their responses is also\nquite different from humans in several variables. We also found that GPTs can\nbe quite sensitive to instruction and how questions are asked. This work adds\nto the growing literature evaluating the psychological aspects of LLMs and\nhelps enrich our understanding of the current models.", "published": "2023-10-03 16:34:47", "link": "http://arxiv.org/abs/2310.04450v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language\n  Models", "abstract": "The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.", "published": "2023-10-03 19:44:37", "link": "http://arxiv.org/abs/2310.04451v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "What's Next in Affective Modeling? Large Language Models", "abstract": "Large Language Models (LLM) have recently been shown to perform well at\nvarious tasks from language understanding, reasoning, storytelling, and\ninformation search to theory of mind. In an extension of this work, we explore\nthe ability of GPT-4 to solve tasks related to emotion prediction. GPT-4\nperforms well across multiple emotion tasks; it can distinguish emotion\ntheories and come up with emotional stories. We show that by prompting GPT-4 to\nidentify key factors of an emotional experience, it is able to manipulate the\nemotional intensity of its own stories. Furthermore, we explore GPT-4's ability\non reverse appraisals by asking it to predict either the goal, belief, or\nemotion of a person using the other two. In general, GPT-4 can make the correct\ninferences. We suggest that LLMs could play an important role in affective\nmodeling; however, they will not fully replace works that attempt to model the\nmechanisms underlying emotion-related processes.", "published": "2023-10-03 16:39:20", "link": "http://arxiv.org/abs/2310.18322v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ensemble Distillation for Unsupervised Constituency Parsing", "abstract": "We investigate the unsupervised constituency parsing task, which organizes\nwords and phrases of a sentence into a hierarchical structure without using\nlinguistically annotated data. We observe that existing unsupervised parsers\ncapture differing aspects of parsing structures, which can be leveraged to\nenhance unsupervised parsing performance. To this end, we propose a notion of\n\"tree averaging,\" based on which we further propose a novel ensemble method for\nunsupervised parsing. To improve inference efficiency, we further distill the\nensemble knowledge into a student model; such an ensemble-then-distill process\nis an effective approach to mitigate the over-smoothing problem existing in\ncommon multi-teacher distilling methods. Experiments show that our method\nsurpasses all previous approaches, consistently demonstrating its effectiveness\nand robustness across various runs, with different ensemble components, and\nunder domain-shift conditions.", "published": "2023-10-03 01:02:44", "link": "http://arxiv.org/abs/2310.01717v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Nugget: Neural Agglomerative Embeddings of Text", "abstract": "Embedding text sequences is a widespread requirement in modern language\nunderstanding. Existing approaches focus largely on constant-size\nrepresentations. This is problematic, as the amount of information contained in\ntext often varies with the length of the input. We propose a solution called\nNugget, which encodes language into a representation based on a dynamically\nselected subset of input tokens. These nuggets are learned through tasks like\nautoencoding and machine translation, and intuitively segment language into\nmeaningful units. We demonstrate Nugget outperforms related approaches in tasks\ninvolving semantic comparison. Finally, we illustrate these compact units allow\nfor expanding the contextual window of a language model (LM), suggesting new\nfuture LMs that can condition on significantly larger amounts of content.", "published": "2023-10-03 01:47:49", "link": "http://arxiv.org/abs/2310.01732v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis", "abstract": "Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.", "published": "2023-10-03 04:14:17", "link": "http://arxiv.org/abs/2310.01783v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Empirical Study of PEFT techniques for Winter Wheat Segmentation", "abstract": "Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced\nsignificant growth and have been extensively employed to adapt large vision and\nlanguage models to various domains, enabling satisfactory model performance\nwith minimal computational needs. Despite these advances, more research has yet\nto delve into potential PEFT applications in real-life scenarios, particularly\nin the critical domains of remote sensing and crop monitoring. The diversity of\nclimates across different regions and the need for comprehensive large-scale\ndatasets have posed significant obstacles to accurately identify crop types\nacross varying geographic locations and changing growing seasons. This study\nseeks to bridge this gap by comprehensively exploring the feasibility of\ncross-area and cross-year out-of-distribution generalization using the\nState-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to\nexplore PEFT approaches for crop monitoring. Specifically, we focus on adapting\nthe SOTA TSViT model to address winter wheat field segmentation, a critical\ntask for crop monitoring and food security. This adaptation process involves\nintegrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and\nprompt tuning. Using PEFT techniques, we achieved notable results comparable to\nthose achieved using full fine-tuning methods while training only a mere 0.7%\nparameters of the whole TSViT architecture. The in-house labeled data-set,\nreferred to as the Beqaa-Lebanon dataset, comprises high-quality annotated\npolygons for wheat and non-wheat classes with a total surface of 170 kmsq, over\nfive consecutive years. Using Sentinel-2 images, our model achieved a 84%\nF1-score. We intend to publicly release the Lebanese winter wheat data set,\ncode repository, and model weights.", "published": "2023-10-03 06:42:28", "link": "http://arxiv.org/abs/2310.01825v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Trainable Noise Model as an XAI evaluation method: application on Sobol\n  for remote sensing image segmentation", "abstract": "eXplainable Artificial Intelligence (XAI) has emerged as an essential\nrequirement when dealing with mission-critical applications, ensuring\ntransparency and interpretability of the employed black box AI models. The\nsignificance of XAI spans various domains, from healthcare to finance, where\nunderstanding the decision-making process of deep learning algorithms is\nessential. Most AI-based computer vision models are often black boxes; hence,\nproviding explainability of deep neural networks in image processing is crucial\nfor their wide adoption and deployment in medical image analysis, autonomous\ndriving, and remote sensing applications. Recently, several XAI methods for\nimage classification tasks have been introduced. On the contrary, image\nsegmentation has received comparatively less attention in the context of\nexplainability, although it is a fundamental task in computer vision\napplications, especially in remote sensing. Only some research proposes\ngradient-based XAI algorithms for image segmentation. This paper adapts the\nrecent gradient-free Sobol XAI method for semantic segmentation. To measure the\nperformance of the Sobol method for segmentation, we propose a quantitative XAI\nevaluation method based on a learnable noise model. The main objective of this\nmodel is to induce noise on the explanation maps, where higher induced noise\nsignifies low accuracy and vice versa. A benchmark analysis is conducted to\nevaluate and compare performance of three XAI methods, including Seg-Grad-CAM,\nSeg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation\ntechnique. This constitutes the first attempt to run and evaluate XAI methods\nusing high-resolution satellite images.", "published": "2023-10-03 06:51:48", "link": "http://arxiv.org/abs/2310.01828v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Extending CAM-based XAI methods for Remote Sensing Imagery Segmentation", "abstract": "Current AI-based methods do not provide comprehensible physical\ninterpretations of the utilized data, extracted features, and\npredictions/inference operations. As a result, deep learning models trained\nusing high-resolution satellite imagery lack transparency and explainability\nand can be merely seen as a black box, which limits their wide-level adoption.\nExperts need help understanding the complex behavior of AI models and the\nunderlying decision-making process. The explainable artificial intelligence\n(XAI) field is an emerging field providing means for robust, practical, and\ntrustworthy deployment of AI models. Several XAI techniques have been proposed\nfor image classification tasks, whereas the interpretation of image\nsegmentation remains largely unexplored. This paper offers to bridge this gap\nby adapting the recent XAI classification algorithms and making them usable for\nmuti-class image segmentation, where we mainly focus on buildings' segmentation\nfrom high-resolution satellite images. To benchmark and compare the performance\nof the proposed approaches, we introduce a new XAI evaluation methodology and\nmetric based on \"Entropy\" to measure the model uncertainty. Conventional XAI\nevaluation methods rely mainly on feeding area-of-interest regions from the\nimage back to the pre-trained (utility) model and then calculating the average\nchange in the probability of the target class. Those evaluation metrics lack\nthe needed robustness, and we show that using Entropy to monitor the model\nuncertainty in segmenting the pixels within the target class is more suitable.\nWe hope this work will pave the way for additional XAI research for image\nsegmentation and applications in the remote sensing discipline.", "published": "2023-10-03 07:01:23", "link": "http://arxiv.org/abs/2310.01837v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Preserving Phonemic Distinctions for Ordinal Regression: A Novel Loss\n  Function for Automatic Pronunciation Assessment", "abstract": "Automatic pronunciation assessment (APA) manages to quantify the\npronunciation proficiency of a second language (L2) learner in a language.\nPrevailing approaches to APA normally leverage neural models trained with a\nregression loss function, such as the mean-squared error (MSE) loss, for\nproficiency level prediction. Despite most regression models can effectively\ncapture the ordinality of proficiency levels in the feature space, they are\nconfronted with a primary obstacle that different phoneme categories with the\nsame proficiency level are inevitably forced to be close to each other,\nretaining less phoneme-discriminative information. On account of this, we\ndevise a phonemic contrast ordinal (PCO) loss for training regression-based APA\nmodels, which aims to preserve better phonemic distinctions between phoneme\ncategories meanwhile considering ordinal relationships of the regression target\noutput. Specifically, we introduce a phoneme-distinct regularizer into the MSE\nloss, which encourages feature representations of different phoneme categories\nto be far apart while simultaneously pulling closer the representations\nbelonging to the same phoneme category by means of weighted distances. An\nextensive set of experiments carried out on the speechocean762 benchmark\ndataset suggest the feasibility and effectiveness of our model in relation to\nsome existing state-of-the-art models.", "published": "2023-10-03 07:05:37", "link": "http://arxiv.org/abs/2310.01839v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Zero-Shot Refinement of Buildings' Segmentation Models using SAM", "abstract": "Foundation models have excelled in various tasks but are often evaluated on\ngeneral benchmarks. The adaptation of these models for specific domains, such\nas remote sensing imagery, remains an underexplored area. In remote sensing,\nprecise building instance segmentation is vital for applications like urban\nplanning. While Convolutional Neural Networks (CNNs) perform well, their\ngeneralization can be limited. For this aim, we present a novel approach to\nadapt foundation models to address existing models' generalization dropback.\nAmong several models, our focus centers on the Segment Anything Model (SAM), a\npotent foundation model renowned for its prowess in class-agnostic image\nsegmentation capabilities. We start by identifying the limitations of SAM,\nrevealing its suboptimal performance when applied to remote sensing imagery.\nMoreover, SAM does not offer recognition abilities and thus fails to classify\nand tag localized objects. To address these limitations, we introduce different\nprompting strategies, including integrating a pre-trained CNN as a prompt\ngenerator. This novel approach augments SAM with recognition abilities, a first\nof its kind. We evaluated our method on three remote sensing datasets,\nincluding the WHU Buildings dataset, the Massachusetts Buildings dataset, and\nthe AICrowd Mapping Challenge. For out-of-distribution performance on the WHU\ndataset, we achieve a 5.47\\% increase in IoU and a 4.81\\% improvement in\nF1-score. For in-distribution performance on the WHU dataset, we observe a\n2.72\\% and 1.58\\% increase in True-Positive-IoU and True-Positive-F1 score,\nrespectively. Our code is publicly available at this Repo\n(https://github.com/geoaigroup/GEOAI-ECRS2023), hoping to inspire further\nexploration of foundation models for domain-specific tasks within the remote\nsensing community.", "published": "2023-10-03 07:19:59", "link": "http://arxiv.org/abs/2310.01845v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BYOM: Building Your Own Multi-Task Model For Free", "abstract": "Recently, various merging methods have been proposed to build a multi-task\nmodel from task-specific finetuned models without retraining. However, existing\nmethods suffer from a large performance deterioration compared to using\nmultiple task-specific models. In this paper, we propose to inject\ntask-specific knowledge into the merged model and design two\nparameter-efficient approaches (BYOM-FFT and BYOM-LoRA) to Build Your Own\nMulti-task model. BYOM-FFT is for merging fully finetuned models, while\nBYOM-LoRA is for LoRA-finetuned models. Both methods are data-free and\ncomputation-efficient. Extensive experiments on computer vision and natural\nlanguage processing tasks show that the proposed BYOM methods outperform\nexisting merging methods by a large margin. Moreover, BYOM-FFT is general and\ncan be integrated into existing merging methods to further boost performance.", "published": "2023-10-03 08:39:33", "link": "http://arxiv.org/abs/2310.01886v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of\n  Text-To-Image Models", "abstract": "Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have\ndemonstrated remarkable prompt-based image generation capabilities.\nMultilingual encoders may have a substantial impact on the cultural agency of\nthese models, as language is a conduit of culture. In this study, we explore\nthe cultural perception embedded in TTI models by characterizing culture across\nthree hierarchical tiers: cultural dimensions, cultural domains, and cultural\nconcepts. Based on this ontology, we derive prompt templates to unlock the\ncultural knowledge in TTI models, and propose a comprehensive suite of\nevaluation techniques, including intrinsic evaluations using the CLIP space,\nextrinsic evaluations with a Visual-Question-Answer (VQA) model and human\nassessments, to evaluate the cultural content of TTI-generated images. To\nbolster our research, we introduce the CulText2I dataset, derived from six\ndiverse TTI models and spanning ten languages. Our experiments provide insights\nregarding Do, What, Which and How research questions about the nature of\ncultural encoding in TTI models, paving the way for cross-cultural applications\nof these models.", "published": "2023-10-03 10:13:36", "link": "http://arxiv.org/abs/2310.01929v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable\n  Autonomous Driving", "abstract": "Large Language Models (LLMs) have shown promise in the autonomous driving\nsector, particularly in generalization and interpretability. We introduce a\nunique object-level multimodal LLM architecture that merges vectorized numeric\nmodalities with a pre-trained LLM to improve context understanding in driving\nsituations. We also present a new dataset of 160k QA pairs derived from 10k\ndriving scenarios, paired with high quality control commands collected with RL\nagent and question answer pairs generated by teacher LLM (GPT-3.5). A distinct\npretraining strategy is devised to align numeric vector modalities with static\nLLM representations using vector captioning language data. We also introduce an\nevaluation metric for Driving QA and demonstrate our LLM-driver's proficiency\nin interpreting driving scenarios, answering questions, and decision-making.\nOur findings highlight the potential of LLM-based driving action generation in\ncomparison to traditional behavioral cloning. We make our benchmark, datasets,\nand model available for further exploration.", "published": "2023-10-03 11:05:14", "link": "http://arxiv.org/abs/2310.01957v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Fill in the Blank: Exploring and Enhancing LLM Capabilities for Backward\n  Reasoning in Math Word Problems", "abstract": "While forward reasoning (i.e., find the answer given the question) has been\nexplored extensively in recent literature, backward reasoning is relatively\nunexplored. We examine the backward reasoning capabilities of LLMs on Math Word\nProblems (MWPs): given a mathematical question and its answer, with some\ndetails omitted from the question, can LLMs effectively retrieve the missing\ninformation? On modifying three benchmark datasets for this task, to evaluate\nthis task: GSM8k, SVAMP, and MultiArith, we find a significant drop in the\naccuracy of models on this task compared to forward reasoning across SOTA LLMs\n(GPT4, GPT3.5, PaLM-2, and LLaMa). Motivated by the fact backward reasoning can\nbe seen as the ''inverse'' of forward reasoning, we propose variations of three\ndifferent forward reasoning strategies to improve performance. Rephrase\nreformulates the given problem into a forward reasoning problem, PAL-Tools\ncombines the idea of Program-Aided LLMs to produce a set of equations that can\nbe solved by an external solver, and Check your Work exploits the availability\nof natural verifier of high accuracy in the forward direction, interleaving\nsolving and verification steps. Finally, realizing that each of our base\nmethods correctly solves a different set of problems, we propose a novel\nBayesian formulation for creating an ensemble over the base methods to further\nboost the accuracy. Extensive experimentation demonstrates successive\nimprovement in the performance of LLMs on the backward reasoning task, using\nour strategies, with our ensemble-based method resulting in significant\nperformance gains compared to the SOTA forward reasoning strategies we adapt.", "published": "2023-10-03 12:03:06", "link": "http://arxiv.org/abs/2310.01991v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.3"], "primary_category": "cs.CL"}
{"title": "Towards End-to-End Embodied Decision Making via Multi-modal Large\n  Language Model: Explorations with GPT4-Vision and Beyond", "abstract": "In this study, we explore the potential of Multimodal Large Language Models\n(MLLMs) in improving embodied decision-making processes for agents. While Large\nLanguage Models (LLMs) have been widely used due to their advanced reasoning\nskills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual\nunderstanding and reasoning capabilities. We investigate whether\nstate-of-the-art MLLMs can handle embodied decision-making in an end-to-end\nmanner and whether collaborations between LLMs and MLLMs can enhance\ndecision-making. To address these questions, we introduce a new benchmark\ncalled PCA-EVAL, which evaluates embodied decision-making from the perspectives\nof Perception, Cognition, and Action. Additionally, we propose HOLMES, a\nmulti-agent cooperation framework that allows LLMs to leverage MLLMs and APIs\nto gather multimodal information for informed decision-making. We compare\nend-to-end embodied decision-making and HOLMES on our benchmark and find that\nthe GPT4-Vision model demonstrates strong end-to-end embodied decision-making\nabilities, outperforming GPT4-HOLMES in terms of average decision accuracy\n(+3%). However, this performance is exclusive to the latest GPT4-Vision model,\nsurpassing the open-source state-of-the-art MLLM by 26%. Our results indicate\nthat powerful MLLMs like GPT4-Vision hold promise for decision-making in\nembodied agents, offering new avenues for MLLM research. Code and data are open\nat https://github.com/pkunlp-icler/PCA-EVAL/.", "published": "2023-10-03 14:13:36", "link": "http://arxiv.org/abs/2310.02071v4", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "primary_category": "cs.AI"}
{"title": "A Dynamic LLM-Powered Agent Network for Task-Oriented Agent\n  Collaboration", "abstract": "Recent studies show that collaborating multiple large language model (LLM)\npowered agents is a promising way for task solving. However, current approaches\nare constrained by using a fixed number of agents and static communication\nstructures. In this work, we propose automatically selecting a team of agents\nfrom candidates to collaborate in a dynamic communication structure toward\ndifferent tasks and domains. Specifically, we build a framework named Dynamic\nLLM-Powered Agent Network ($\\textbf{DyLAN}$) for LLM-powered agent\ncollaboration, operating a two-stage paradigm: (1) Team Optimization and (2)\nTask Solving. During the first stage, we utilize an $\\textit{agent selection}$\nalgorithm, based on an unsupervised metric called $\\textit{Agent Importance\nScore}$, enabling the selection of best agents according to their contributions\nin a preliminary trial, oriented to the given task. Then, in the second stage,\nthe selected agents collaborate dynamically according to the query.\nEmpirically, we demonstrate that DyLAN outperforms strong baselines in code\ngeneration, decision-making, general reasoning, and arithmetic reasoning tasks\nwith moderate computational cost. On specific subjects in MMLU, selecting a\nteam of agents in the team optimization stage improves accuracy by up to 25.0%\nin DyLAN.", "published": "2023-10-03 16:05:48", "link": "http://arxiv.org/abs/2310.02170v2", "categories": ["cs.CL", "cs.AI", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Ask Again, Then Fail: Large Language Models' Vacillations in Judgment", "abstract": "We observe that current conversational language models often waver in their\njudgments when faced with follow-up questions, even if the original judgment\nwas correct. This wavering presents a significant challenge for generating\nreliable responses and building user trust. To comprehensively assess this\nissue, we introduce a \\textsc{Follow-up Questioning Mechanism} along with two\nmetrics to quantify this inconsistency, confirming its widespread presence in\ncurrent language models. To mitigate this issue, we explore various prompting\nstrategies for closed-source models; moreover, we develop a training-based\nframework \\textsc{Unwavering-FQ} that teaches language models to maintain their\noriginally correct judgments through synthesized high-quality preference data.\nOur experimental results confirm the effectiveness of our framework and its\nability to enhance the general capabilities of models.", "published": "2023-10-03 16:08:41", "link": "http://arxiv.org/abs/2310.02174v5", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Language Models Represent Space and Time", "abstract": "The capabilities of large language models (LLMs) have sparked debate over\nwhether such systems just learn an enormous collection of superficial\nstatistics or a set of more coherent and grounded representations that reflect\nthe real world. We find evidence for the latter by analyzing the learned\nrepresentations of three spatial datasets (world, US, NYC places) and three\ntemporal datasets (historical figures, artworks, news headlines) in the Llama-2\nfamily of models. We discover that LLMs learn linear representations of space\nand time across multiple scales. These representations are robust to prompting\nvariations and unified across different entity types (e.g. cities and\nlandmarks). In addition, we identify individual \"space neurons\" and \"time\nneurons\" that reliably encode spatial and temporal coordinates. While further\ninvestigation is needed, our results suggest modern LLMs learn rich\nspatiotemporal representations of the real world and possess basic ingredients\nof a world model.", "published": "2023-10-03 17:06:52", "link": "http://arxiv.org/abs/2310.02207v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Think before you speak: Training Language Models With Pause Tokens", "abstract": "Language models generate responses by producing a series of tokens in\nimmediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$\nhidden vectors per layer, one vector per preceding token. What if instead we\nwere to let the model manipulate say, $K+10$ hidden vectors, before it outputs\nthe $(K+1)^{th}$ token? We operationalize this idea by performing training and\ninference on language models with a (learnable) $\\textit{pause}$ token, a\nsequence of which is appended to the input prefix. We then delay extracting the\nmodel's outputs until the last pause token is seen, thereby allowing the model\nto process extra computation before committing to an answer. We empirically\nevaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M\nparameters with causal pretraining on C4, and on downstream tasks covering\nreasoning, question-answering, general understanding and fact recall. Our main\nfinding is that inference-time delays show gains when the model is both\npre-trained and finetuned with delays. For the 1B model, we witness gains on 8\nof 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of\nSQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of\nGSM8k. Our work raises a range of conceptual and practical future research\nquestions on making delayed next-token prediction a widely applicable new\nparadigm.", "published": "2023-10-03 17:32:41", "link": "http://arxiv.org/abs/2310.02226v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in\n  Visual Contexts", "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit\nimpressive problem-solving skills in many tasks and domains, but their ability\nin mathematical reasoning in visual contexts has not been systematically\nstudied. To bridge this gap, we present MathVista, a benchmark designed to\ncombine challenges from diverse mathematical and visual tasks. It consists of\n6,141 examples, derived from 28 existing multimodal datasets involving\nmathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and\nPaperQA). Completing these tasks requires fine-grained, deep visual\nunderstanding and compositional reasoning, which all state-of-the-art\nfoundation models find challenging. With MathVista, we have conducted a\ncomprehensive, quantitative evaluation of 12 prominent foundation models. The\nbest-performing GPT-4V model achieves an overall accuracy of 49.9%,\nsubstantially outperforming Bard, the second-best performer, by 15.1%. Our\nin-depth analysis reveals that the superiority of GPT-4V is mainly attributed\nto its enhanced visual perception and mathematical reasoning. However, GPT-4V\nstill falls short of human performance by 10.4%, as it often struggles to\nunderstand complex figures and perform rigorous reasoning. This significant gap\nunderscores the critical role that MathVista will play in the development of\ngeneral-purpose AI agents capable of tackling mathematically intensive and\nvisually rich real-world tasks. We further explore the new ability of\nself-verification, the application of self-consistency, and the interactive\nchatbot capabilities of GPT-4V, highlighting its promising potential for future\nresearch. The project is available at https://mathvista.github.io/.", "published": "2023-10-03 17:57:24", "link": "http://arxiv.org/abs/2310.02255v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Automatic Pair Construction for Contrastive Post-training", "abstract": "Alignment serves as an important step to steer large language models (LLMs)\ntowards human preferences. In this paper, we propose an automatic way to\nconstruct contrastive data for LLM, using preference pairs from multiple models\nof varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We compare the\ncontrastive techniques of SLiC and DPO to SFT baselines and find that DPO\nprovides a step-function improvement even after continuing SFT saturates. We\nalso explore a data curriculum learning scheme for contrastive post-training,\nwhich starts by learning from \"easier\" pairs and transitioning to \"harder\"\nones, which further improves alignment. Finally, we scale up our experiments to\ntrain with more data and larger models like Orca. Remarkably, our automatic\ncontrastive post-training further improves the performance of Orca, already a\nstate-of-the-art instruction learning model tuned with GPT-4 outputs, to\noutperform ChatGPT.", "published": "2023-10-03 17:59:46", "link": "http://arxiv.org/abs/2310.02263v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalizable Long-Horizon Manipulations with Large Language Models", "abstract": "This work introduces a framework harnessing the capabilities of Large\nLanguage Models (LLMs) to generate primitive task conditions for generalizable\nlong-horizon manipulations with novel objects and unseen tasks. These task\nconditions serve as guides for the generation and adjustment of Dynamic\nMovement Primitives (DMP) trajectories for long-horizon task execution. We\nfurther create a challenging robotic manipulation task suite based on Pybullet\nfor long-horizon task evaluation. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of our framework on both\nfamiliar tasks involving new objects and novel but related tasks, highlighting\nthe potential of LLMs in enhancing robotic system versatility and adaptability.\nProject website: https://object814.github.io/Task-Condition-With-LLM/", "published": "2023-10-03 17:59:46", "link": "http://arxiv.org/abs/2310.02264v1", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation", "abstract": "Several recent advances in AI systems solve problems by providing a\n\"scaffolding\" program that structures multiple calls to language models (LMs)\nto generate better outputs. A scaffolding program is written in a programming\nlanguage such as Python. In this work, we use a language-model-infused\nscaffolding program to improve itself. We start with a seed \"improver\" that\nimproves an input program according to a given utility function by querying an\nLM several times and returning the best solution. We then run this seed\nimprover to improve itself. Across a small set of downstream tasks, the\nresulting improved improver generates programs with significantly better\nperformance than its seed improver. A variety of self-improvement strategies\nare proposed by the language model, including beam search, genetic algorithms,\nand simulated annealing. Since the language models themselves are not altered,\nthis is not full recursive self-improvement. Nonetheless, it demonstrates that\na modern language model, GPT-4 in our experiments, is capable of writing code\nthat can call itself to improve itself. We consider concerns around the\ndevelopment of self-improving technologies and evaluate the frequency with\nwhich the generated code bypasses a sandbox.", "published": "2023-10-03 17:59:32", "link": "http://arxiv.org/abs/2310.02304v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "On the definition of toxicity in NLP", "abstract": "The fundamental problem in toxicity detection task lies in the fact that the\ntoxicity is ill-defined. This causes us to rely on subjective and vague data in\nmodels' training, which results in non-robust and non-accurate results: garbage\nin - garbage out.\n  This work suggests a new, stress-level-based definition of toxicity designed\nto be objective and context-aware. On par with it, we also describe possible\nways of applying this new definition to dataset creation and model training.", "published": "2023-10-03 18:32:34", "link": "http://arxiv.org/abs/2310.02357v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unsupervised Speech Recognition with N-Skipgram and Positional Unigram\n  Matching", "abstract": "Training unsupervised speech recognition systems presents challenges due to\nGAN-associated instability, misalignment between speech and text, and\nsignificant memory demands. To tackle these challenges, we introduce a novel\nASR system, ESPUM. This system harnesses the power of lower-order N-skipgrams\n(up to N=3) combined with positional unigram statistics gathered from a small\nbatch of samples. Evaluated on the TIMIT benchmark, our model showcases\ncompetitive performance in ASR and phoneme segmentation tasks. Access our\npublicly available code at https://github.com/lwang114/GraphUnsupASR.", "published": "2023-10-03 19:05:32", "link": "http://arxiv.org/abs/2310.02382v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "MindTheDApp: A Toolchain for Complex Network-Driven Structural Analysis\n  of Ethereum-based Decentralised Applications", "abstract": "This paper presents MindTheDApp, a toolchain designed specifically for the\nstructural analysis of Ethereum-based Decentralized Applications (DApps), with\na distinct focus on a complex network-driven approach. Unlike existing tools,\nour toolchain combines the power of ANTLR4 and Abstract Syntax Tree (AST)\ntraversal techniques to transform the architecture and interactions within\nsmart contracts into a specialized bipartite graph. This enables advanced\nnetwork analytics to highlight operational efficiencies within the DApp's\narchitecture.\n  The bipartite graph generated by the proposed tool comprises two sets of\nnodes: one representing smart contracts, interfaces, and libraries, and the\nother including functions, events, and modifiers. Edges in the graph connect\nfunctions to smart contracts they interact with, offering a granular view of\ninterdependencies and execution flow within the DApp. This network-centric\napproach allows researchers and practitioners to apply complex network theory\nin understanding the robustness, adaptability, and intricacies of decentralized\nsystems.\n  Our work contributes to the enhancement of security in smart contracts by\nallowing the visualisation of the network, and it provides a deep understanding\nof the architecture and operational logic within DApps. Given the growing\nimportance of smart contracts in the blockchain ecosystem and the emerging\napplication of complex network theory in technology, our toolchain offers a\ntimely contribution to both academic research and practical applications in the\nfield of blockchain technology.", "published": "2023-10-03 20:03:08", "link": "http://arxiv.org/abs/2310.02408v1", "categories": ["cs.IT", "cs.CL", "math.IT"], "primary_category": "cs.IT"}
{"title": "Dodo: Dynamic Contextual Compression for Decoder-only LMs", "abstract": "Transformer-based language models (LMs) are inefficient in long contexts. We\npropose Dodo, a solution for context compression. Instead of one vector per\ntoken in a standard transformer model, Dodo represents text with a dynamic\nnumber of hidden states at each layer, reducing the cost of self-attention to a\nfraction of typical time and space. Moreover, off-the-shelf models such as\nLLaMA can be adapted to Dodo by efficient parameter tuning methods such as\nLoRA. In use, Dodo can act as either an autoregressive LM or a context\ncompressor for downstream tasks. We demonstrate through experiments in language\nmodeling, question answering, and summarization that Dodo retains capabilities\nin these tasks, while drastically reducing the overhead during decoding. For\nexample, in the autoencoding task, Dodo shrinks context at a 20x compression\nratio with a BLEU score of 98% for reconstruction, achieving nearly lossless\nencoding.", "published": "2023-10-03 20:07:06", "link": "http://arxiv.org/abs/2310.02409v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Can a student Large Language Model perform as well as it's teacher?", "abstract": "The burgeoning complexity of contemporary deep learning models, while\nachieving unparalleled accuracy, has inadvertently introduced deployment\nchallenges in resource-constrained environments. Knowledge distillation, a\ntechnique aiming to transfer knowledge from a high-capacity \"teacher\" model to\na streamlined \"student\" model, emerges as a promising solution to this dilemma.\nThis paper provides a comprehensive overview of the knowledge distillation\nparadigm, emphasizing its foundational principles such as the utility of soft\nlabels and the significance of temperature scaling. Through meticulous\nexamination, we elucidate the critical determinants of successful distillation,\nincluding the architecture of the student model, the caliber of the teacher,\nand the delicate balance of hyperparameters. While acknowledging its profound\nadvantages, we also delve into the complexities and challenges inherent in the\nprocess. Our exploration underscores knowledge distillation's potential as a\npivotal technique in optimizing the trade-off between model performance and\ndeployment efficiency.", "published": "2023-10-03 20:34:59", "link": "http://arxiv.org/abs/2310.02421v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Low-Resource Languages Jailbreak GPT-4", "abstract": "AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.", "published": "2023-10-03 21:30:56", "link": "http://arxiv.org/abs/2310.02446v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PrivacyMind: Large Language Models Can Be Contextual Privacy Protection\n  Learners", "abstract": "The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains contextually sensitive personally identifiable information (PII).\nDirect fine-tuning of LLMs on this data without privacy protection poses a risk\nof data leakage of sensitive PII during inference time. To address this\nchallenge, we introduce Contextual Privacy Protection Language Models\n(PrivacyMind), a novel paradigm for fine-tuning LLMs that effectively injects\ndomain-specific knowledge while safeguarding inference-time data privacy. Our\nwork offers a theoretical analysis for model design and benchmarks various\ntechniques such as corpus curation, penalty-based unlikelihood in training\nloss, instruction-based tuning, etc. Extensive experiments across diverse\ndatasets and scenarios demonstrate the effectiveness of our approaches. In\nparticular, instruction tuning with both positive and negative examples stands\nout as a promising method, effectively protecting private data while enhancing\nthe model's knowledge. Our work underscores the potential for Large Language\nModels as robust contextual privacy protection learners. The complete code and\ndata for the work can be found at https://github.com/Yijia-Xiao/PrivacyMind.", "published": "2023-10-03 22:37:01", "link": "http://arxiv.org/abs/2310.02469v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ResidualTransformer: Residual Low-Rank Learning with Weight-Sharing for\n  Transformer Layers", "abstract": "Memory constraint of always-on devices is one of the major concerns when\ndeploying speech processing models on these devices. While larger models\ntrained with sufficiently large amount of data generally perform better, making\nthem fit in the device memory is a demanding challenge. In this paper, we aim\nto reduce model size by reparameterizing model weights across Transformer\nencoder layers and assuming a special weight composition and structure. More\nspecifically, inspired by ResNet and the more recent LoRA work, we propose an\napproach named ResidualTransformer, where each weight matrix in a Transformer\nlayer comprises 1) a shared full-rank component with its adjacent layers, and\n2) a unique low-rank component to itself. The low-rank matrices only account\nfor a small amount of model size increase. In addition, we add diagonal weight\nmatrices to improve modeling capacity of the low-rank matrices. Experiments of\nour 10k-hour speech recognition and speech translation tasks show that the\nTransformer encoder size can be reduced by ~3X with very slight performance\ndegradation.", "published": "2023-10-03 23:31:48", "link": "http://arxiv.org/abs/2310.02489v2", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Short text classification with machine learning in the social sciences:\n  The case of climate change on Twitter", "abstract": "To analyse large numbers of texts, social science researchers are\nincreasingly confronting the challenge of text classification. When manual\nlabeling is not possible and researchers have to find automatized ways to\nclassify texts, computer science provides a useful toolbox of machine-learning\nmethods whose performance remains understudied in the social sciences. In this\narticle, we compare the performance of the most widely used text classifiers by\napplying them to a typical research scenario in social science research: a\nrelatively small labeled dataset with infrequent occurrence of categories of\ninterest, which is a part of a large unlabeled dataset. As an example case, we\nlook at Twitter communication regarding climate change, a topic of increasing\nscholarly interest in interdisciplinary social science research. Using a novel\ndataset including 5,750 tweets from various international organizations\nregarding the highly ambiguous concept of climate change, we evaluate the\nperformance of methods in automatically classifying tweets based on whether\nthey are about climate change or not. In this context, we highlight two main\nfindings. First, supervised machine-learning methods perform better than\nstate-of-the-art lexicons, in particular as class balance increases. Second,\ntraditional machine-learning methods, such as logistic regression and random\nforest, perform similarly to sophisticated deep-learning methods, whilst\nrequiring much less training time and computational resources. The results have\nimportant implications for the analysis of short texts in social science\nresearch.", "published": "2023-10-03 22:09:43", "link": "http://arxiv.org/abs/2310.04452v1", "categories": ["cs.CL", "cs.LG", "stat.ME"], "primary_category": "cs.CL"}
{"title": "HPC-GPT: Integrating Large Language Model for High-Performance Computing", "abstract": "Large Language Models (LLMs), including the LLaMA model, have exhibited their\nefficacy across various general-domain natural language processing (NLP) tasks.\nHowever, their performance in high-performance computing (HPC) domain tasks has\nbeen less than optimal due to the specialized expertise required to interpret\nthe model responses. In response to this challenge, we propose HPC-GPT, a novel\nLLaMA-based model that has been supervised fine-tuning using generated QA\n(Question-Answer) instances for the HPC domain. To evaluate its effectiveness,\nwe concentrate on two HPC tasks: managing AI models and datasets for HPC, and\ndata race detection. By employing HPC-GPT, we demonstrate comparable\nperformance with existing methods on both tasks, exemplifying its excellence in\nHPC-related scenarios. Our experiments on open-source benchmarks yield\nextensive results, underscoring HPC-GPT's potential to bridge the performance\ngap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way\nfor LLMs to excel in HPC domains, simplifying the utilization of language\nmodels in complex computing applications.", "published": "2023-10-03 01:34:55", "link": "http://arxiv.org/abs/2311.12833v1", "categories": ["cs.DC", "cs.AI", "cs.CL"], "primary_category": "cs.DC"}
{"title": "OceanGPT: A Large Language Model for Ocean Science Tasks", "abstract": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reasons are the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever large language\nmodel in the ocean domain, which is expert in various ocean science tasks. We\nalso propose OceanGPT, a novel framework to automatically obtain a large volume\nof ocean domain instruction data, which generates instructions based on\nmulti-agent collaboration. Additionally, we construct the first oceanography\nbenchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean\ndomain. Though comprehensive experiments, OceanGPT not only shows a higher\nlevel of knowledge expertise for oceans science tasks but also gains\npreliminary embodied intelligence capabilities in ocean technology.", "published": "2023-10-03 13:17:35", "link": "http://arxiv.org/abs/2310.02031v8", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "primary_category": "cs.CL"}
{"title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology\n  View", "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in\nintricate social environments, a pressing query emerges: Can these NLP systems\nmirror human-esque collaborative intelligence, in a multi-agent society\nconsisting of multiple large language models (LLMs)? This paper probes the\ncollaboration mechanisms among contemporary NLP systems by melding practical\nexperiments with theoretical insights. We fabricate four unique `societies'\ncomprised of LLM agents, where each agent is characterized by a specific\n`trait' (easy-going or overconfident) and engages in collaboration with a\ndistinct `thinking pattern' (debate or reflection). Through evaluating these\nmulti-agent societies on three benchmark datasets, we discern that certain\ncollaborative strategies not only outshine previous top-tier approaches, but\nalso optimize efficiency (using fewer API tokens). Moreover, our results\nfurther illustrate that LLM agents manifest human-like social behaviors, such\nas conformity and consensus reaching, mirroring foundational social psychology\ntheories. In conclusion, we integrate insights from social psychology to\ncontextualize the collaboration of LLM agents, inspiring further investigations\ninto the collaboration mechanism for LLMs. We commit to sharing our code and\ndatasets\\footnote{\\url{https://github.com/zjunlp/MachineSoM}.}, hoping to\ncatalyze further research in this promising avenue.", "published": "2023-10-03 15:05:52", "link": "http://arxiv.org/abs/2310.02124v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models", "abstract": "As the cost associated with fine-tuning Large Language Models (LLMs)\ncontinues to rise, recent research efforts have pivoted towards developing\nmethodologies to edit implicit knowledge embedded within LLMs. Yet, there's\nstill a dark cloud lingering overhead -- will knowledge editing trigger\nbutterfly effect? since it is still unclear whether knowledge editing might\nintroduce side effects that pose potential risks or not. This paper pioneers\nthe investigation into the potential pitfalls associated with knowledge editing\nfor LLMs. To achieve this, we introduce new benchmark datasets and propose\ninnovative evaluation metrics. Our results underline two pivotal concerns: (1)\nKnowledge Conflict: Editing groups of facts that logically clash can magnify\nthe inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)\nKnowledge Distortion: Altering parameters with the aim of editing factual\nknowledge can irrevocably warp the innate knowledge structure of LLMs.\nExperimental results vividly demonstrate that knowledge editing might\ninadvertently cast a shadow of unintended consequences on LLMs, which warrant\nattention and efforts for future works. Code and data are available at\nhttps://github.com/zjunlp/PitfallsKnowledgeEditing.", "published": "2023-10-03 15:10:46", "link": "http://arxiv.org/abs/2310.02129v5", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Editing Personality for Large Language Models", "abstract": "This paper introduces an innovative task focused on editing the personality\ntraits of Large Language Models (LLMs). This task seeks to adjust the models'\nresponses to opinion-related questions on specified topics since an\nindividual's personality often manifests in the form of their expressed\nopinions, thereby showcasing different personality traits. Specifically, we\nconstruct PersonalityEdit, a new benchmark dataset to address this task.\nDrawing on the theory in Social Psychology, we isolate three representative\ntraits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation\nfor our benchmark. We then gather data using GPT-4, generating responses that\nalign with a specified topic and embody the targeted personality trait. We\nconduct comprehensive experiments involving various baselines and discuss the\nrepresentation of personality behavior in LLMs. Our findings uncover potential\nchallenges of the proposed task, illustrating several remaining issues. We\nanticipate that our work can stimulate further annotation in model editing and\npersonality-related research. Code is available at\nhttps://github.com/zjunlp/EasyEdit.", "published": "2023-10-03 16:02:36", "link": "http://arxiv.org/abs/2310.02168v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Automatic Quality Assessment of Wikipedia Articles -- A Systematic\n  Literature Review", "abstract": "Wikipedia is the world's largest online encyclopedia, but maintaining article\nquality through collaboration is challenging. Wikipedia designed a quality\nscale, but with such a manual assessment process, many articles remain\nunassessed. We review existing methods for automatically measuring the quality\nof Wikipedia articles, identifying and comparing machine learning algorithms,\narticle features, quality metrics, and used datasets, examining 149 distinct\nstudies, and exploring commonalities and gaps in them. The literature is\nextensive, and the approaches follow past technological trends. However,\nmachine learning is still not widely used by Wikipedia, and we hope that our\nanalysis helps future researchers change that reality.", "published": "2023-10-03 17:45:39", "link": "http://arxiv.org/abs/2310.02235v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "An evolutionary model of personality traits related to cooperative\n  behavior using a large language model", "abstract": "This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.", "published": "2023-10-03 14:35:27", "link": "http://arxiv.org/abs/2310.05976v1", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.GT", "cs.MA", "cs.NE", "q-bio.PE", "68W50, 91A22"], "primary_category": "physics.soc-ph"}
{"title": "Mel-Band RoFormer for Music Source Separation", "abstract": "Recently, multi-band spectrogram-based approaches such as Band-Split RNN\n(BSRNN) have demonstrated promising results for music source separation. In our\nrecent work, we introduce the BS-RoFormer model which inherits the idea of\nband-split scheme in BSRNN at the front-end, and then uses the hierarchical\nTransformer with Rotary Position Embedding (RoPE) to model the inner-band and\ninter-band sequences for multi-band mask estimation. This model has achieved\nstate-of-the-art performance, but the band-split scheme is defined empirically,\nwithout analytic supports from the literature. In this paper, we propose\nMel-RoFormer, which adopts the Mel-band scheme that maps the frequency bins\ninto overlapped subbands according to the mel scale. In contract, the\nband-split mapping in BSRNN and BS-RoFormer is non-overlapping and designed\nbased on heuristics. Using the MUSDB18HQ dataset for experiments, we\ndemonstrate that Mel-RoFormer outperforms BS-RoFormer in the separation tasks\nof vocals, drums, and other stems.", "published": "2023-10-03 05:53:23", "link": "http://arxiv.org/abs/2310.01809v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-visual child-adult speaker classification in dyadic interactions", "abstract": "Interactions involving children span a wide range of important domains from\nlearning to clinical diagnostic and therapeutic contexts. Automated analyses of\nsuch interactions are motivated by the need to seek accurate insights and offer\nscale and robustness across diverse and wide-ranging conditions. Identifying\nthe speech segments belonging to the child is a critical step in such modeling.\nConventional child-adult speaker classification typically relies on audio\nmodeling approaches, overlooking visual signals that convey speech articulation\ninformation, such as lip motion. Building on the foundation of an audio-only\nchild-adult speaker classification pipeline, we propose incorporating visual\ncues through active speaker detection and visual processing models. Our\nframework involves video pre-processing, utterance-level child-adult speaker\ndetection, and late fusion of modality-specific predictions. We demonstrate\nfrom extensive experiments that a visually aided classification pipeline\nenhances the accuracy and robustness of the classification. We show relative\nimprovements of 2.38% and 3.97% in F1 macro score when one face and two faces\nare visible, respectively.", "published": "2023-10-03 08:09:32", "link": "http://arxiv.org/abs/2310.01867v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Prompting Audios Using Acoustic Properties For Emotion Representation", "abstract": "Emotions lie on a continuum, but current models treat emotions as a finite\nvalued discrete variable. This representation does not capture the diversity in\nthe expression of emotion. To better represent emotions we propose the use of\nnatural language descriptions (or prompts). In this work, we address the\nchallenge of automatically generating these prompts and training a model to\nbetter learn emotion representations from audio and prompt pairs. We use\nacoustic properties that are correlated to emotion like pitch, intensity,\nspeech rate, and articulation rate to automatically generate prompts i.e.\n'acoustic prompts'. We use a contrastive learning objective to map speech to\ntheir respective acoustic prompts. We evaluate our model on Emotion Audio\nRetrieval and Speech Emotion Recognition. Our results show that the acoustic\nprompts significantly improve the model's performance in EAR, in various\nPrecision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on\nthe Ravdess dataset.", "published": "2023-10-03 13:06:58", "link": "http://arxiv.org/abs/2310.02298v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
