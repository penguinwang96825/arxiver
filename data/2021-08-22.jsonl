{"title": "Improving Distantly Supervised Relation Extraction with Self-Ensemble\n  Noise Filtering", "abstract": "Distantly supervised models are very popular for relation extraction since we\ncan obtain a large amount of training data using the distant supervision method\nwithout human annotation. In distant supervision, a sentence is considered as a\nsource of a tuple if the sentence contains both entities of the tuple. However,\nthis condition is too permissive and does not guarantee the presence of\nrelevant relation-specific information in the sentence. As such, distantly\nsupervised training data contains much noise which adversely affects the\nperformance of the models. In this paper, we propose a self-ensemble filtering\nmechanism to filter out the noisy samples during the training process. We\nevaluate our proposed framework on the New York Times dataset which is obtained\nvia distant supervision. Our experiments with multiple state-of-the-art neural\nrelation extraction models show that our proposed filtering mechanism improves\nthe robustness of the models and increases their F1 scores.", "published": "2021-08-22 11:23:36", "link": "http://arxiv.org/abs/2108.09689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UzBERT: pretraining a BERT model for Uzbek", "abstract": "Pretrained language models based on the Transformer architecture have\nachieved state-of-the-art results in various natural language processing tasks\nsuch as part-of-speech tagging, named entity recognition, and question\nanswering. However, no such monolingual model for the Uzbek language is\npublicly available. In this paper, we introduce UzBERT, a pretrained Uzbek\nlanguage model based on the BERT architecture. Our model greatly outperforms\nmultilingual BERT on masked language model accuracy. We make the model publicly\navailable under the MIT open-source license.", "published": "2021-08-22 18:28:22", "link": "http://arxiv.org/abs/2108.09814v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dual-Decoder Conformer for Multilingual Speech Recognition", "abstract": "Transformer-based models have recently become very popular for\nsequence-to-sequence applications such as machine translation and speech\nrecognition. This work proposes a dual-decoder transformer model for\nlow-resource multilingual speech recognition for Indian languages. Our proposed\nmodel consists of a Conformer [1] encoder, two parallel transformer decoders,\nand a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme\nrecognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence\nalong with language information. We consider phoneme recognition and language\nidentification as auxiliary tasks in the multi-task learning framework. We\njointly optimize the network for phoneme recognition, grapheme recognition, and\nlanguage identification tasks with Joint CTC-Attention [2] training. Our\nexperiments show that we can obtain a significant reduction in WER over the\nbaseline approaches. We also show that our dual-decoder approach obtains\nsignificant improvement over the single decoder approach.", "published": "2021-08-22 09:22:28", "link": "http://arxiv.org/abs/2109.03277v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual Speech Recognition for Low-Resource Indian Languages using\n  Multi-Task conformer", "abstract": "Transformers have recently become very popular for sequence-to-sequence\napplications such as machine translation and speech recognition. In this work,\nwe propose a multi-task learning-based transformer model for low-resource\nmultilingual speech recognition for Indian languages. Our proposed model\nconsists of a conformer [1] encoder and two parallel transformer decoders. We\nuse a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme\ndecoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme\nrecognition task as an auxiliary task for our multi-task learning framework. We\njointly optimize the network for both phoneme and grapheme recognition tasks\nusing Joint CTC-Attention [2] training. We use a conditional decoding scheme to\ninject the language information into the model before predicting the grapheme\nsequence. Our experiments show that our proposed approach can obtain\nsignificant improvement over previous approaches [4]. We also show that our\nconformer-based dual-decoder approach outperforms both the transformer-based\ndual-decoder approach and single decoder approach. Finally, We compare\nmonolingual ASR models with our proposed multilingual ASR approach.", "published": "2021-08-22 09:32:15", "link": "http://arxiv.org/abs/2109.03969v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Using Large Pre-Trained Models with Cross-Modal Attention for\n  Multi-Modal Emotion Recognition", "abstract": "Recently, self-supervised pre-training has shown significant improvements in\nmany areas of machine learning, including speech and NLP. We propose using\nlarge self-supervised pre-trained models for both audio and text modality with\ncross-modality attention for multimodal emotion recognition. We use Wav2Vec2.0\n[1] as an audio encoder base for robust speech features extraction and the BERT\nmodel [2] as a text encoder base for better contextual representation of text.\nThese high capacity models trained on large amounts of unlabeled data contain\nrich feature representations and improve the downstream task's performance. We\nuse the cross-modal attention [3] mechanism to learn alignment between audio\nand text representations from self-supervised models. Cross-modal attention\nalso helps in extracting interactive information between audio and text\nfeatures. We obtain utterance-level feature representation from frame-level\nfeatures using statistics pooling for both audio and text modality and combine\nthem using the early fusion technique. Our experiments show that the proposed\napproach obtains a 1.88% absolute improvement in accuracy compared to the\nprevious state-of-the-art method [3] on the IEMOCAP dataset [35]. We also\nconduct unimodal experiments for both audio and text modalities and compare\nthem with previous best methods.", "published": "2021-08-22 09:01:52", "link": "http://arxiv.org/abs/2108.09669v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Generalizing RNN-Transducer to Out-Domain Audio via Sparse\n  Self-Attention Layers", "abstract": "Recurrent neural network transducer (RNN-T) is an end-to-end speech\nrecognition framework converting input acoustic frames into a character\nsequence. The state-of-the-art encoder network for RNN-T is the Conformer,\nwhich can effectively model the local-global context information via its\nconvolution and self-attention layers. Although Conformer RNN-T has shown\noutstanding performance, most studies have been verified in the setting where\nthe train and test data are drawn from the same domain. The domain mismatch\nproblem for Conformer RNN-T has not been intensively investigated yet, which is\nan important issue for the product-level speech recognition system. In this\nstudy, we identified that fully connected self-attention layers in the\nConformer caused high deletion errors, specifically in the long-form out-domain\nutterances. To address this problem, we introduce sparse self-attention layers\nfor Conformer-based encoder networks, which can exploit local and generalized\nglobal information by pruning most of the in-domain fitted global connections.\nAlso, we propose a state reset method for the generalization of the prediction\nnetwork to cope with long-form utterances. Applying proposed methods to an\nout-domain test, we obtained 27.6% relative character error rate (CER)\nreduction compared to the fully connected self-attention layer-based\nConformers.", "published": "2021-08-22 08:06:15", "link": "http://arxiv.org/abs/2108.10752v2", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
