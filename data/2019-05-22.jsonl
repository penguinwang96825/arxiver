{"title": "Corpus Augmentation by Sentence Segmentation for Low-Resource Neural\n  Machine Translation", "abstract": "Neural Machine Translation (NMT) has been proven to achieve impressive\nresults. The NMT system translation results depend strongly on the size and\nquality of parallel corpora. Nevertheless, for many language pairs, no\nrich-resource parallel corpora exist. As described in this paper, we propose a\ncorpus augmentation method by segmenting long sentences in a corpus using\nback-translation and generating pseudo-parallel sentence pairs. The experiment\nresults of the Japanese-Chinese and Chinese-Japanese translation with\nJapanese-Chinese scientific paper excerpt corpus (ASPEC-JC) show that the\nmethod improves translation performance.", "published": "2019-05-22 04:11:13", "link": "http://arxiv.org/abs/1905.08945v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recent Advances in Neural Question Generation", "abstract": "Emerging research in Neural Question Generation (NQG) has started to\nintegrate a larger variety of inputs, and generating questions requiring higher\nlevels of cognition. These trends point to NQG as a bellwether for NLP, about\nhow human intelligence embodies the skills of curiosity and integration.\n  We present a comprehensive survey of neural question generation, examining\nthe corpora, methodologies, and evaluation methods. From this, we elaborate on\nwhat we see as emerging on NQG's trend: in terms of the learning paradigms,\ninput modalities, and cognitive levels considered by NQG. We end by pointing\nout the potential directions ahead.", "published": "2019-05-22 04:38:06", "link": "http://arxiv.org/abs/1905.08949v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Joint Named-Entity Recognizer for Heterogeneous Tag-sets Using a Tag\n  Hierarchy", "abstract": "We study a variant of domain adaptation for named-entity recognition where\nmultiple, heterogeneously tagged training sets are available. Furthermore, the\ntest tag-set is not identical to any individual training tag-set. Yet, the\nrelations between all tags are provided in a tag hierarchy, covering the test\ntags as a combination of training tags. This setting occurs when various\ndatasets are created using different annotation schemes. This is also the case\nof extending a tag-set with a new tag by annotating only the new tag in a new\ndataset. We propose to use the given tag hierarchy to jointly learn a neural\nnetwork that shares its tagging layer among all tag-sets. We compare this model\nto combining independent models and to a model based on the multitasking\napproach. Our experiments show the benefit of the tag-hierarchy model,\nespecially when facing non-trivial consolidation of tag-sets.", "published": "2019-05-22 13:40:38", "link": "http://arxiv.org/abs/1905.09135v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentence Length", "abstract": "The distribution of sentence length in ordinary language is not well captured\nby the existing models. Here we survey previous models of sentence length and\npresent our random walk model that offers both a better fit with the data and a\nbetter understanding of the distribution. We develop a generalization of KL\ndivergence, discuss measuring the noise inherent in a corpus, and present a\nhyperparameter-free Bayesian model comparison method that has strong conceptual\nties to Minimal Description Length modeling. The models we obtain require only\na few dozen bits, orders of magnitude less than the naive nonparametric MDL\nmodels would.", "published": "2019-05-22 13:45:05", "link": "http://arxiv.org/abs/1905.09139v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Data with Mixup for Sentence Classification: An Empirical\n  Study", "abstract": "Mixup, a recent proposed data augmentation method through linearly\ninterpolating inputs and modeling targets of random samples, has demonstrated\nits capability of significantly improving the predictive accuracy of the\nstate-of-the-art networks for image classification. However, how this technique\ncan be applied to and what is its effectiveness on natural language processing\n(NLP) tasks have not been investigated. In this paper, we propose two\nstrategies for the adaption of Mixup on sentence classification: one performs\ninterpolation on word embeddings and another on sentence embeddings. We conduct\nexperiments to evaluate our methods using several benchmark datasets. Our\nstudies show that such interpolation strategies serve as an effective, domain\nindependent data augmentation approach for sentence classification, and can\nresult in significant accuracy improvement for both CNN and LSTM models.", "published": "2019-05-22 03:55:54", "link": "http://arxiv.org/abs/1905.08941v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ANTIQUE: A Non-Factoid Question Answering Benchmark", "abstract": "Considering the widespread use of mobile and voice search, answer passage\nretrieval for non-factoid questions plays a critical role in modern information\nretrieval systems. Despite the importance of the task, the community still\nfeels the significant lack of large-scale non-factoid question answering\ncollections with real questions and comprehensive relevance judgments. In this\npaper, we develop and release a collection of 2,626 open-domain non-factoid\nquestions from a diverse set of categories. The dataset, called ANTIQUE,\ncontains 34,011 manual relevance annotations. The questions were asked by real\nusers in a community question answering service, i.e., Yahoo! Answers.\nRelevance judgments for all the answers to each question were collected through\ncrowdsourcing. To facilitate further research, we also include a brief analysis\nof the data as well as baseline results on both classical and recently\ndeveloped neural IR models.", "published": "2019-05-22 05:32:03", "link": "http://arxiv.org/abs/1905.08957v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Retrieving Multi-Entity Associations: An Evaluation of Combination Modes\n  for Word Embeddings", "abstract": "Word embeddings have gained significant attention as learnable\nrepresentations of semantic relations between words, and have been shown to\nimprove upon the results of traditional word representations. However, little\neffort has been devoted to using embeddings for the retrieval of entity\nassociations beyond pairwise relations. In this paper, we use popular embedding\nmethods to train vector representations of an entity-annotated news corpus, and\nevaluate their performance for the task of predicting entity participation in\nnews events versus a traditional word cooccurrence network as a baseline. To\nsupport queries for events with multiple participating entities, we test a\nnumber of combination modes for the embedding vectors. While we find that even\nthe best combination modes for word embeddings do not quite reach the\nperformance of the full cooccurrence network, especially for rare entities, we\nobserve that different embedding methods model different types of relations,\nthereby indicating the potential for ensemble methods.", "published": "2019-05-22 10:13:48", "link": "http://arxiv.org/abs/1905.09052v1", "categories": ["cs.IR", "cs.CL", "H.3.3"], "primary_category": "cs.IR"}
{"title": "Deeper Text Understanding for IR with Contextual Neural Language\n  Modeling", "abstract": "Neural networks provide new possibilities to automatically learn complex\nlanguage patterns and query-document relations. Neural IR models have achieved\npromising results in learning query-document relevance patterns, but few\nexplorations have been done on understanding the text content of a query or a\ndocument. This paper studies leveraging a recently-proposed contextual neural\nlanguage model, BERT, to provide deeper text understanding for IR. Experimental\nresults demonstrate that the contextual text representations from BERT are more\neffective than traditional word embeddings. Compared to bag-of-words retrieval\nmodels, the contextual language model can better leverage language structures,\nbringing large improvements on queries written in natural languages. Combining\nthe text understanding ability with search knowledge leads to an enhanced\npre-trained BERT model that can benefit related search tasks where training\ndata are limited.", "published": "2019-05-22 16:11:47", "link": "http://arxiv.org/abs/1905.09217v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Can a Humanoid Robot be part of the Organizational Workforce? A User\n  Study Leveraging Sentiment Analysis", "abstract": "Hiring robots for the workplaces is a challenging task as robots have to\ncater to customer demands, follow organizational protocols and behave with\nsocial etiquette. In this study, we propose to have a humanoid social robot,\nNadine, as a customer service agent in an open social work environment. The\nobjective of this study is to analyze the effects of humanoid robots on\ncustomers at work environment, and see if it can handle social scenarios. We\npropose to evaluate these objectives through two modes, namely, survey\nquestionnaire and customer feedback. We also propose a novel approach to\nanalyze customer feedback data (text) using sentic computing methods.\nSpecifically, we employ aspect extraction and sentiment analysis to analyze the\ndata. From our framework, we detect sentiment associated to the aspects that\nmainly concerned the customers during their interaction. This allows us to\nunderstand customers expectations and current limitations of robots as\nemployees.", "published": "2019-05-22 03:34:59", "link": "http://arxiv.org/abs/1905.08937v2", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "From web crawled text to project descriptions: automatic summarizing of\n  social innovation projects", "abstract": "In the past decade, social innovation projects have gained the attention of\npolicy makers, as they address important social issues in an innovative manner.\nA database of social innovation is an important source of information that can\nexpand collaboration between social innovators, drive policy and serve as an\nimportant resource for research. Such a database needs to have projects\ndescribed and summarized. In this paper, we propose and compare several methods\n(e.g. SVM-based, recurrent neural network based, ensambled) for describing\nprojects based on the text that is available on project websites. We also\naddress and propose a new metric for automated evaluation of summaries based on\ntopic modelling.", "published": "2019-05-22 11:49:37", "link": "http://arxiv.org/abs/1905.09086v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simplified Neural Unsupervised Domain Adaptation", "abstract": "Unsupervised domain adaptation (UDA) is the task of modifying a statistical\nmodel trained on labeled data from a source domain to achieve better\nperformance on data from a target domain, with access to only unlabeled data in\nthe target domain. Existing state-of-the-art UDA approaches use neural networks\nto learn representations that can predict the values of subset of important\nfeatures called \"pivot features.\" In this work, we show that it is possible to\nimprove on these methods by jointly training the representation learner with\nthe task learner, and examine the importance of existing pivot selection\nmethods.", "published": "2019-05-22 14:11:30", "link": "http://arxiv.org/abs/1905.09153v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FastSpeech: Fast, Robust and Controllable Text to Speech", "abstract": "Neural network based end-to-end text to speech (TTS) has significantly\nimproved the quality of synthesized speech. Prominent methods (e.g., Tacotron\n2) usually first generate mel-spectrogram from text, and then synthesize speech\nfrom the mel-spectrogram using vocoder such as WaveNet. Compared with\ntraditional concatenative and statistical parametric approaches, neural network\nbased end-to-end models suffer from slow inference speed, and the synthesized\nspeech is usually not robust (i.e., some words are skipped or repeated) and\nlack of controllability (voice speed or prosody control). In this work, we\npropose a novel feed-forward network based on Transformer to generate\nmel-spectrogram in parallel for TTS. Specifically, we extract attention\nalignments from an encoder-decoder based teacher model for phoneme duration\nprediction, which is used by a length regulator to expand the source phoneme\nsequence to match the length of the target mel-spectrogram sequence for\nparallel mel-spectrogram generation. Experiments on the LJSpeech dataset show\nthat our parallel model matches autoregressive models in terms of speech\nquality, nearly eliminates the problem of word skipping and repeating in\nparticularly hard cases, and can adjust voice speed smoothly. Most importantly,\ncompared with autoregressive Transformer TTS, our model speeds up\nmel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x.\nTherefore, we call our model FastSpeech.", "published": "2019-05-22 17:50:21", "link": "http://arxiv.org/abs/1905.09263v5", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
