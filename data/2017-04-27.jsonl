{"title": "Question Answering on Knowledge Bases and Text using Universal Schema\n  and Memory Networks", "abstract": "Existing question answering methods infer answers either from a knowledge\nbase or from raw text. While knowledge base (KB) methods are good at answering\ncompositional questions, their performance is often affected by the\nincompleteness of the KB. Au contraire, web text contains millions of facts\nthat are absent in the KB, however in an unstructured form. {\\it Universal\nschema} can support reasoning on the union of both structured KBs and\nunstructured text by aligning them in a common embedded space. In this paper we\nextend universal schema to natural language question answering, employing\n\\emph{memory networks} to attend to the large body of facts in the combination\nof text and KB. Our models can be trained in an end-to-end fashion on\nquestion-answer pairs. Evaluation results on \\spades fill-in-the-blank question\nanswering dataset show that exploiting universal schema for question answering\nis better than using either a KB or text alone. This model also outperforms the\ncurrent state-of-the-art by 8.5 $F_1$ points.\\footnote{Code and data available\nin \\url{https://rajarshd.github.io/TextKBQA}}", "published": "2017-04-27 00:03:02", "link": "http://arxiv.org/abs/1704.08384v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Structured Natural Language Representations for Semantic\n  Parsing", "abstract": "We introduce a neural semantic parser that converts natural language\nutterances to intermediate representations in the form of predicate-argument\nstructures, which are induced with a transition system and subsequently mapped\nto target domains. The semantic parser is trained end-to-end using annotated\nlogical forms or their denotations. We obtain competitive results on various\ndatasets. The induced predicate-argument structures shed light on the types of\nrepresentations useful for semantic parsing and how these are different from\nlinguistically motivated ones.", "published": "2017-04-27 00:24:20", "link": "http://arxiv.org/abs/1704.08387v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duluth at Semeval-2017 Task 7 : Puns upon a midnight dreary, Lexical\n  Semantics for the weak and weary", "abstract": "This paper describes the Duluth systems that participated in SemEval-2017\nTask 7 : Detection and Interpretation of English Puns. The Duluth systems\nparticipated in all three subtasks, and relied on methods that included word\nsense disambiguation and measures of semantic relatedness.", "published": "2017-04-27 00:29:17", "link": "http://arxiv.org/abs/1704.08388v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Duluth at SemEval-2017 Task 6: Language Models in Humor Detection", "abstract": "This paper describes the Duluth system that participated in SemEval-2017 Task\n6 #HashtagWars: Learning a Sense of Humor. The system participated in Subtasks\nA and B using N-gram language models, ranking highly in the task evaluation.\nThis paper discusses the results of our system in the development and\nevaluation stages and from two post-evaluation runs.", "published": "2017-04-27 00:40:33", "link": "http://arxiv.org/abs/1704.08390v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A GRU-Gated Attention Model for Neural Machine Translation", "abstract": "Neural machine translation (NMT) heavily relies on an attention network to\nproduce a context vector for each target word prediction. In practice, we find\nthat context vectors for different target words are quite similar to one\nanother and therefore are insufficient in discriminatively predicting target\nwords. The reason for this might be that context vectors produced by the\nvanilla attention network are just a weighted sum of source representations\nthat are invariant to decoder states. In this paper, we propose a novel\nGRU-gated attention model (GAtt) for NMT which enhances the degree of\ndiscrimination of context vectors by enabling source representations to be\nsensitive to the partial translation generated by the decoder. GAtt uses a\ngated recurrent unit (GRU) to combine two types of information: treating a\nsource annotation vector originally produced by the bidirectional encoder as\nthe history state while the corresponding previous decoder state as the input\nto the GRU. The GRU-combined information forms a new source annotation vector.\nIn this way, we can obtain translation-sensitive source representations which\nare then feed into the attention network to generate discriminative context\nvectors. We further propose a variant that regards a source annotation vector\nas the current input while the previous decoder state as the history.\nExperiments on NIST Chinese-English translation tasks show that both GAtt-based\nmodels achieve significant improvements over the vanilla attentionbased NMT.\nFurther analyses on attention weights and context vectors demonstrate the\neffectiveness of GAtt in improving the discrimination power of representations\nand handling the challenging issue of over-translation.", "published": "2017-04-27 04:25:41", "link": "http://arxiv.org/abs/1704.08430v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Neural Network Techniques for Feature Extraction from Text", "abstract": "This paper aims to catalyze the discussions about text feature extraction\ntechniques using neural network architectures. The research questions discussed\nin the paper focus on the state-of-the-art neural network techniques that have\nproven to be useful tools for language processing, language generation, text\nclassification and other computational linguistics tasks.", "published": "2017-04-27 12:27:25", "link": "http://arxiv.org/abs/1704.08531v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Learning a Neural Semantic Parser from User Feedback", "abstract": "We present an approach to rapidly and easily build natural language\ninterfaces to databases for new domains, whose performance improves over time\nbased on user feedback, and requires minimal intervention. To achieve this, we\nadapt neural sequence models to map utterances directly to SQL with its full\nexpressivity, bypassing any intermediate meaning representations. These models\nare immediately deployed online to solicit feedback from real users to flag\nincorrect queries. Finally, the popularity of SQL facilitates gathering\nannotations for incorrect predictions using the crowd, which is directly used\nto improve our models. This complete feedback loop, without intermediate\nrepresentations or database specific engineering, opens up new ways of building\nhigh quality semantic parsers. Experiments suggest that this approach can be\ndeployed quickly for any new target domain, as we show by learning a semantic\nparser for an online academic database from scratch.", "published": "2017-04-27 22:05:06", "link": "http://arxiv.org/abs/1704.08760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Multimodal Emotion Recognition using Deep Neural Networks", "abstract": "Automatic affect recognition is a challenging task due to the various\nmodalities emotions can be expressed with. Applications can be found in many\ndomains including multimedia retrieval and human computer interaction. In\nrecent years, deep neural networks have been used with great success in\ndetermining emotional states. Inspired by this success, we propose an emotion\nrecognition system using auditory and visual modalities. To capture the\nemotional content for various styles of speaking, robust features need to be\nextracted. To this purpose, we utilize a Convolutional Neural Network (CNN) to\nextract features from the speech, while for the visual modality a deep residual\nnetwork (ResNet) of 50 layers. In addition to the importance of feature\nextraction, a machine learning algorithm needs also to be insensitive to\noutliers while being able to model the context. To tackle this problem, Long\nShort-Term Memory (LSTM) networks are utilized. The system is then trained in\nan end-to-end fashion where - by also taking advantage of the correlations of\nthe each of the streams - we manage to significantly outperform the traditional\napproaches based on auditory and visual handcrafted features for the prediction\nof spontaneous and natural emotions on the RECOLA database of the AVEC 2016\nresearch challenge on emotion recognition.", "published": "2017-04-27 15:14:33", "link": "http://arxiv.org/abs/1704.08619v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Multimodal Word Distributions", "abstract": "Word embeddings provide point representations of words containing useful\nsemantic information. We introduce multimodal word distributions formed from\nGaussian mixtures, for multiple word meanings, entailment, and rich uncertainty\ninformation. To learn these distributions, we propose an energy-based\nmax-margin objective. We show that the resulting approach captures uniquely\nexpressive semantic information, and outperforms alternatives, such as word2vec\nskip-grams, and Gaussian embeddings, on benchmark datasets such as word\nsimilarity and entailment.", "published": "2017-04-27 03:59:54", "link": "http://arxiv.org/abs/1704.08424v2", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
