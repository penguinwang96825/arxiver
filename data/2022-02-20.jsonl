{"title": "Contextual Semantic Embeddings for Ontology Subsumption Prediction", "abstract": "Automating ontology construction and curation is an important but challenging\ntask in knowledge engineering and artificial intelligence. Prediction by\nmachine learning techniques such as contextual semantic embedding is a\npromising direction, but the relevant research is still preliminary especially\nfor expressive ontologies in Web Ontology Language (OWL). In this paper, we\npresent a new subsumption prediction method named BERTSubs for classes of OWL\nontology. It exploits the pre-trained language model BERT to compute contextual\nembeddings of a class, where customized templates are proposed to incorporate\nthe class context (e.g., neighbouring classes) and the logical existential\nrestriction. BERTSubs is able to predict multiple kinds of subsumers including\nnamed classes from the same ontology or another ontology, and existential\nrestrictions from the same ontology. Extensive evaluation on five real-world\nontologies for three different subsumption tasks has shown the effectiveness of\nthe templates and that BERTSubs can dramatically outperform the baselines that\nuse (literal-aware) knowledge graph embeddings, non-contextual word embeddings\nand the state-of-the-art OWL ontology embeddings.", "published": "2022-02-20 11:14:04", "link": "http://arxiv.org/abs/2202.09791v4", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "$\\mathcal{Y}$-Tuning: An Efficient Tuning Paradigm for Large-Scale\n  Pre-Trained Models via Label Representation Learning", "abstract": "With the success of large-scale pre-trained models (PTMs), how efficiently\nadapting PTMs to downstream tasks has attracted tremendous attention,\nespecially for PTMs with billions of parameters. Although some\nparameter-efficient tuning paradigms have been proposed to address this\nproblem, they still require large resources to compute the gradients in the\ntraining phase. In this paper, we propose $\\mathcal{Y}$-Tuning, an efficient\nyet effective paradigm to adapt frozen large-scale PTMs to specific downstream\ntasks. $\\mathcal{Y}$-tuning learns dense representations for labels\n$\\mathcal{Y}$ defined in a given task and aligns them to fixed feature\nrepresentation. Without tuning the features of input text and model parameters,\n$\\mathcal{Y}$-tuning is both parameter-efficient and training-efficient. For\n$\\text{DeBERTa}_\\text{XXL}$ with 1.6 billion parameters, $\\mathcal{Y}$-tuning\nachieves performance more than $96\\%$ of full fine-tuning on GLUE Benchmark\nwith only $2\\%$ tunable parameters and much fewer training costs.", "published": "2022-02-20 13:49:34", "link": "http://arxiv.org/abs/2202.09817v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph-based Extractive Explainer for Recommendations", "abstract": "Explanations in a recommender system assist users in making informed\ndecisions among a set of recommended items. Great research attention has been\ndevoted to generating natural language explanations to depict how the\nrecommendations are generated and why the users should pay attention to them.\nHowever, due to different limitations of those solutions, e.g., template-based\nor generation-based, it is hard to make the explanations easily perceivable,\nreliable and personalized at the same time.\n  In this work, we develop a graph attentive neural network model that\nseamlessly integrates user, item, attributes, and sentences for\nextraction-based explanation. The attributes of items are selected as the\nintermediary to facilitate message passing for user-item specific evaluation of\nsentence relevance. And to balance individual sentence relevance, overall\nattribute coverage, and content redundancy, we solve an integer linear\nprogramming problem to make the final selection of sentences. Extensive\nempirical evaluations against a set of state-of-the-art baseline methods on two\nbenchmark review datasets demonstrated the generation quality of the proposed\nsolution.", "published": "2022-02-20 04:56:10", "link": "http://arxiv.org/abs/2202.09730v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Hierarchical Interpretation of Neural Text Classification", "abstract": "Recent years have witnessed increasing interests in developing interpretable\nmodels in Natural Language Processing (NLP). Most existing models aim at\nidentifying input features such as words or phrases important for model\npredictions. Neural models developed in NLP however often compose word\nsemantics in a hierarchical manner and text classification requires\nhierarchical modelling to aggregate local information in order to deal with\ntopic and label shifts more effectively. As such, interpretation by words or\nphrases only cannot faithfully explain model decisions in text classification.\nThis paper proposes a novel Hierarchical INTerpretable neural text classifier,\ncalled Hint, which can automatically generate explanations of model predictions\nin the form of label-associated topics in a hierarchical manner. Model\ninterpretation is no longer at the word level, but built on topics as the basic\nsemantic unit. Experimental results on both review datasets and news datasets\nshow that our proposed approach achieves text classification results on par\nwith existing state-of-the-art text classifiers, and generates interpretations\nmore faithful to model predictions and better understood by humans than other\ninterpretable neural text classifiers.", "published": "2022-02-20 11:15:03", "link": "http://arxiv.org/abs/2202.09792v3", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "towards automatic transcription of polyphonic electric guitar music:a\n  new dataset and a multi-loss transformer model", "abstract": "In this paper, we propose a new dataset named EGDB, that con-tains\ntranscriptions of the electric guitar performance of 240 tab-latures rendered\nwith different tones. Moreover, we benchmark theperformance of two well-known\ntranscription models proposed orig-inally for the piano on this dataset, along\nwith a multi-loss Trans-former model that we newly propose. Our evaluation on\nthis datasetand a separate set of real-world recordings demonstrate the\ninfluenceof timbre on the accuracy of guitar sheet transcription, the\npotentialof using multiple losses for Transformers, as well as the room\nforfurther improvement for this task.", "published": "2022-02-20 20:47:35", "link": "http://arxiv.org/abs/2202.09907v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "It's Raw! Audio Generation with State-Space Models", "abstract": "Developing architectures suitable for modeling raw audio is a challenging\nproblem due to the high sampling rates of audio waveforms. Standard sequence\nmodeling approaches like RNNs and CNNs have previously been tailored to fit the\ndemands of audio, but the resultant architectures make undesirable\ncomputational tradeoffs and struggle to model waveforms effectively. We propose\nSaShiMi, a new multi-scale architecture for waveform modeling built around the\nrecently introduced S4 model for long sequence modeling. We identify that S4\ncan be unstable during autoregressive generation, and provide a simple\nimprovement to its parameterization by drawing connections to Hurwitz matrices.\nSaShiMi yields state-of-the-art performance for unconditional waveform\ngeneration in the autoregressive setting. Additionally, SaShiMi improves\nnon-autoregressive generation performance when used as the backbone\narchitecture for a diffusion model. Compared to prior architectures in the\nautoregressive generation setting, SaShiMi generates piano and speech waveforms\nwhich humans find more musical and coherent respectively, e.g. 2x better mean\nopinion scores than WaveNet on an unconditional speech generation task. On a\nmusic generation task, SaShiMi outperforms WaveNet on density estimation and\nspeed at both training and inference even when using 3x fewer parameters. Code\ncan be found at https://github.com/HazyResearch/state-spaces and samples at\nhttps://hazyresearch.stanford.edu/sashimi-examples.", "published": "2022-02-20 04:45:46", "link": "http://arxiv.org/abs/2202.09729v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Enhancing Affective Representations of Music-Induced EEG through\n  Multimodal Supervision and latent Domain Adaptation", "abstract": "The study of Music Cognition and neural responses to music has been\ninvaluable in understanding human emotions. Brain signals, though, manifest a\nhighly complex structure that makes processing and retrieving meaningful\nfeatures challenging, particularly of abstract constructs like affect.\nMoreover, the performance of learning models is undermined by the limited\namount of available neuronal data and their severe inter-subject variability.\nIn this paper we extract efficient, personalized affective representations from\nEEG signals during music listening. To this end, we employ music signals as a\nsupervisory modality to EEG, aiming to project their semantic correspondence\nonto a common representation space. We utilize a bi-modal framework by\ncombining an LSTM-based attention model to process EEG and a pre-trained model\nfor music tagging, along with a reverse domain discriminator to align the\ndistributions of the two modalities, further constraining the learning process\nwith emotion tags. The resulting framework can be utilized for emotion\nrecognition both directly, by performing supervised predictions from either\nmodality, and indirectly, by providing relevant music samples to EEG input\nqueries. The experimental findings show the potential of enhancing neuronal\ndata through stimulus information for recognition purposes and yield insights\ninto the distribution and temporal variance of music-induced affective\nfeatures.", "published": "2022-02-20 07:32:12", "link": "http://arxiv.org/abs/2202.09750v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
