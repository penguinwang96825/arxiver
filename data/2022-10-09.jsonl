{"title": "HEGEL: Hypergraph Transformer for Long Document Summarization", "abstract": "Extractive summarization for long documents is challenging due to the\nextended structured input context. The long-distance sentence dependency\nhinders cross-sentence relations modeling, the critical step of extractive\nsummarization. This paper proposes HEGEL, a hypergraph neural network for long\ndocument summarization by capturing high-order cross-sentence relations. HEGEL\nupdates and learns effective sentence representations with hypergraph\ntransformer layers and fuses different types of sentence dependencies,\nincluding latent topics, keywords coreference, and section structure. We\nvalidate HEGEL by conducting extensive experiments on two benchmark datasets,\nand experimental results demonstrate the effectiveness and efficiency of HEGEL.", "published": "2022-10-09 00:32:50", "link": "http://arxiv.org/abs/2210.04126v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Align: Modeling Deep Cross-lingual Interactions for Word Alignment", "abstract": "Word alignment which aims to extract lexicon translation equivalents between\nsource and target sentences, serves as a fundamental tool for natural language\nprocessing. Recent studies in this area have yielded substantial improvements\nby generating alignments from contextualized embeddings of the pre-trained\nmultilingual language models. However, we find that the existing approaches\ncapture few interactions between the input sentence pairs, which degrades the\nword alignment quality severely, especially for the ambiguous words in the\nmonolingual context. To remedy this problem, we propose Cross-Align to model\ndeep interactions between the input sentence pairs, in which the source and\ntarget sentences are encoded separately with the shared self-attention modules\nin the shallow layers, while cross-lingual interactions are explicitly\nconstructed by the cross-attention modules in the upper layers. Besides, to\ntrain our model effectively, we propose a two-stage training framework, where\nthe model is trained with a simple Translation Language Modeling (TLM)\nobjective in the first stage and then finetuned with a self-supervised\nalignment objective in the second stage. Experiments show that the proposed\nCross-Align achieves the state-of-the-art (SOTA) performance on four out of\nfive language pairs.", "published": "2022-10-09 02:24:35", "link": "http://arxiv.org/abs/2210.04141v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-strait Variations on Two Near-synonymous Loanwords xie2shang1 and\n  tan2pan4: A Corpus-based Comparative Study", "abstract": "This study attempts to investigate cross-strait variations on two typical\nsynonymous loanwords in Chinese, i.e. xie2shang1 and tan2pan4, drawn on MARVS\ntheory. Through a comparative analysis, the study found some distributional,\neventual, and contextual similarities and differences across Taiwan and\nMainland Mandarin. Compared with the underused tan2pan4, xie2shang1 is\nsignificantly overused in Taiwan Mandarin and vice versa in Mainland Mandarin.\nAdditionally, though both words can refer to an inchoative process in Mainland\nand Taiwan Mandarin, the starting point for xie2shang1 in Mainland Mandarin is\nsomewhat blurring compared with the usage in Taiwan Mandarin. Further on, in\nTaiwan Mandarin, tan2pan4 can be used in economic and diplomatic contexts,\nwhile xie2shang1 is used almost exclusively in political contexts. In Mainland\nMandarin, however, the two words can be used in a hybrid manner within\npolitical contexts; moreover, tan2pan4 is prominently used in diplomatic\ncontexts with less reference to economic activities, while xie2sahng1 can be\nfound in both political and legal contexts, emphasizing a role of mediation.", "published": "2022-10-09 04:10:58", "link": "http://arxiv.org/abs/2210.04161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Label-Driven Denoising Framework for Multi-Label Few-Shot Aspect\n  Category Detection", "abstract": "Multi-Label Few-Shot Aspect Category Detection (FS-ACD) is a new sub-task of\naspect-based sentiment analysis, which aims to detect aspect categories\naccurately with limited training instances. Recently, dominant works use the\nprototypical network to accomplish this task, and employ the attention\nmechanism to extract keywords of aspect category from the sentences to produce\nthe prototype for each aspect. However, they still suffer from serious noise\nproblems: (1) due to lack of sufficient supervised data, the previous methods\neasily catch noisy words irrelevant to the current aspect category, which\nlargely affects the quality of the generated prototype; (2) the\nsemantically-close aspect categories usually generate similar prototypes, which\nare mutually noisy and confuse the classifier seriously. In this paper, we\nresort to the label information of each aspect to tackle the above problems,\nalong with proposing a novel Label-Driven Denoising Framework (LDF). Extensive\nexperimental results show that our framework achieves better performance than\nother state-of-the-art methods.", "published": "2022-10-09 10:37:43", "link": "http://arxiv.org/abs/2210.04220v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revealing Patient-Reported Experiences in Healthcare from Social Media\n  using the DAPMAV Framework", "abstract": "Understanding patient experience in healthcare is increasingly important and\ndesired by medical professionals in a patient-centered care approach.\nHealthcare discourse on social media presents an opportunity to gain a unique\nperspective on patient-reported experiences, complementing traditional survey\ndata. These social media reports often appear as first-hand accounts of\npatients' journeys through the healthcare system, whose details extend beyond\nthe confines of structured surveys and at a far larger scale than focus groups.\nHowever, in contrast with the vast presence of patient-experience data on\nsocial media and the potential benefits the data offers, it attracts\ncomparatively little research attention due to the technical proficiency\nrequired for text analysis. In this paper, we introduce the\nDesign-Acquire-Process-Model-Analyse-Visualise (DAPMAV) framework to provide an\noverview of techniques and an approach to capture patient-reported experiences\nfrom social media data. We apply this framework in a case study on prostate\ncancer data from /r/ProstateCancer, demonstrate the framework's value in\ncapturing specific aspects of patient concern (such as sexual dysfunction),\nprovide an overview of the discourse, and show narrative and emotional\nprogression through these stories. We anticipate this framework to apply to a\nwide variety of areas in healthcare, including capturing and differentiating\nexperiences across minority groups, geographic boundaries, and types of\nillnesses.", "published": "2022-10-09 11:38:41", "link": "http://arxiv.org/abs/2210.04232v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Improving Zero-shot Multi-hop Reasoning in Generative\n  Question Answering", "abstract": "Generative question answering (QA) models generate answers to questions\neither solely based on the parameters of the model (the closed-book setting) or\nadditionally retrieving relevant evidence (the open-book setting). Generative\nQA models can answer some relatively complex questions, but the mechanism\nthrough which they do so is still poorly understood. We perform several studies\naimed at better understanding the multi-hop reasoning capabilities of\ngenerative QA models. First, we decompose multi-hop questions into multiple\ncorresponding single-hop questions, and find marked inconsistency in QA models'\nanswers on these pairs of ostensibly identical question chains. Second, we find\nthat models lack zero-shot multi-hop reasoning ability: when trained only on\nsingle-hop questions, models generalize poorly to multi-hop questions. Finally,\nwe demonstrate that it is possible to improve models' zero-shot multi-hop\nreasoning capacity through two methods that approximate real multi-hop natural\nlanguage (NL) questions by training on either concatenation of single-hop\nquestions or logical forms (SPARQL). In sum, these results demonstrate that\nmulti-hop reasoning does not emerge naturally in generative QA models, but can\nbe encouraged by advances in training or modeling techniques.", "published": "2022-10-09 11:48:07", "link": "http://arxiv.org/abs/2210.04234v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multi-turn Emotional Support Dialogue Generation with\n  Lookahead Strategy Planning", "abstract": "Providing Emotional Support (ES) to soothe people in emotional distress is an\nessential capability in social interactions. Most existing researches on\nbuilding ES conversation systems only considered single-turn interactions with\nusers, which was over-simplified. In comparison, multi-turn ES conversation\nsystems can provide ES more effectively, but face several new technical\nchallenges, including: (1) how to adopt appropriate support strategies to\nachieve the long-term dialogue goal of comforting the user's emotion; (2) how\nto dynamically model the user's state. In this paper, we propose a novel system\nMultiESC to address these issues. For strategy planning, drawing inspiration\nfrom the A* search algorithm, we propose lookahead heuristics to estimate the\nfuture user feedback after using particular strategies, which helps to select\nstrategies that can lead to the best long-term effects. For user state\nmodeling, MultiESC focuses on capturing users' subtle emotional expressions and\nunderstanding their emotion causes. Extensive experiments show that MultiESC\nsignificantly outperforms competitive baselines in both dialogue generation and\nstrategy planning. Our codes are available at\nhttps://github.com/lwgkzl/MultiESC.", "published": "2022-10-09 12:23:47", "link": "http://arxiv.org/abs/2210.04242v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Pre-Training by Reducing Representation Confusion", "abstract": "In this work, we revisit the Transformer-based pre-trained language models\nand identify two different types of information confusion in position encoding\nand model representations, respectively. Firstly, we show that in the relative\nposition encoding, the joint modeling about relative distances and directions\nbrings confusion between two heterogeneous information. It may make the model\nunable to capture the associative semantics of the same distance and the\nopposite directions, which in turn affects the performance of downstream tasks.\nSecondly, we notice the BERT with Mask Language Modeling (MLM) pre-training\nobjective outputs similar token representations (last hidden states of\ndifferent tokens) and head representations (attention weights of different\nheads), which may make the diversity of information expressed by different\ntokens and heads limited. Motivated by the above investigation, we propose two\nnovel techniques to improve pre-trained language models: Decoupled Directional\nRelative Position (DDRP) encoding and MTH pre-training objective. DDRP\ndecouples the relative distance features and the directional features in\nclassical relative position encoding. MTH applies two novel auxiliary\nregularizers besides MLM to enlarge the dissimilarities between (a) last hidden\nstates of different tokens, and (b) attention weights of different heads. These\ndesigns allow the model to capture different categories of information more\nclearly, as a way to alleviate information confusion in representation learning\nfor better optimization. Extensive experiments and ablation studies on GLUE\nbenchmark demonstrate the effectiveness of our proposed methods.", "published": "2022-10-09 12:35:04", "link": "http://arxiv.org/abs/2210.04246v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noise-Robust De-Duplication at Scale", "abstract": "Identifying near duplicates within large, noisy text corpora has a myriad of\napplications that range from de-duplicating training datasets, reducing privacy\nrisk, and evaluating test set leakage, to identifying reproduced news articles\nand literature within large corpora. Across these diverse applications, the\noverwhelming majority of work relies on N-grams. Limited efforts have been made\nto evaluate how well N-gram methods perform, in part because it is unclear how\none could create an unbiased evaluation dataset for a massive corpus. This\nstudy uses the unique timeliness of historical news wires to create a 27,210\ndocument dataset, with 122,876 positive duplicate pairs, for studying\nnoise-robust de-duplication. The time-sensitivity of news makes comprehensive\nhand labelling feasible - despite the massive overall size of the corpus - as\nduplicates occur within a narrow date range. The study then develops and\nevaluates a range of de-duplication methods: hashing and N-gram overlap (which\npredominate in the literature), a contrastively trained bi-encoder, and a\nre-rank style approach combining a bi- and cross-encoder. The neural approaches\nsignificantly outperform hashing and N-gram overlap. We show that the\nbi-encoder scales well, de-duplicating a 10 million article corpus on a single\nGPU card in a matter of hours. We also apply our pre-trained model to the\nRealNews and patent portions of C4 (Colossal Clean Crawled Corpus),\nillustrating that a neural approach can identify many near duplicates missed by\nhashing, in the presence of various types of noise. The public release of our\nNEWS-COPY de-duplication dataset, codebase, and the pre-trained models will\nfacilitate further research and applications.", "published": "2022-10-09 13:30:42", "link": "http://arxiv.org/abs/2210.04261v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency\n  of Adapters", "abstract": "Adapter Tuning, which freezes the pretrained language models (PLMs) and only\nfine-tunes a few extra modules, becomes an appealing efficient alternative to\nthe full model fine-tuning. Although computationally efficient, the recent\nAdapters often increase parameters (e.g. bottleneck dimension) for matching the\nperformance of full model fine-tuning, which we argue goes against their\noriginal intention. In this work, we re-examine the parameter-efficiency of\nAdapters through the lens of network pruning (we name such plug-in concept as\n\\texttt{SparseAdapter}) and find that SparseAdapter can achieve comparable or\nbetter performance than standard Adapters when the sparse ratio reaches up to\n80\\%. Based on our findings, we introduce an easy but effective setting\n``\\textit{Large-Sparse}'' to improve the model capacity of Adapters under the\nsame parameter budget. Experiments on five competitive Adapters upon three\nadvanced PLMs show that with proper sparse method (e.g. SNIP) and ratio (e.g.\n40\\%) SparseAdapter can consistently outperform their corresponding\ncounterpart. Encouragingly, with the \\textit{Large-Sparse} setting, we can\nobtain further appealing gains, even outperforming the full fine-tuning by a\nlarge margin. Our code will be released at:\nhttps://github.com/Shwai-He/SparseAdapter.", "published": "2022-10-09 15:28:48", "link": "http://arxiv.org/abs/2210.04284v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "QAScore -- An Unsupervised Unreferenced Metric for the Question\n  Generation Evaluation", "abstract": "Question Generation (QG) aims to automate the task of composing questions for\na passage with a set of chosen answers found within the passage. In recent\nyears, the introduction of neural generation models has resulted in substantial\nimprovements of automatically generated questions in terms of quality,\nespecially compared to traditional approaches that employ manually crafted\nheuristics. However, the metrics commonly applied in QG evaluations have been\ncriticized for their low agreement with human judgement. We therefore propose a\nnew reference-free evaluation metric that has the potential to provide a better\nmechanism for evaluating QG systems, called QAScore. Instead of fine-tuning a\nlanguage model to maximize its correlation with human judgements, QAScore\nevaluates a question by computing the cross entropy according to the\nprobability that the language model can correctly generate the masked words in\nthe answer to that question. Furthermore, we conduct a new crowd-sourcing human\nevaluation experiment for the QG evaluation to investigate how QAScore and\nother metrics can correlate with human judgements. Experiments show that\nQAScore obtains a stronger correlation with the results of our proposed human\nevaluation method compared to existing traditional word-overlap-based metrics\nsuch as BLEU and ROUGE, as well as the existing pretrained-model-based metric\nBERTScore.", "published": "2022-10-09 19:00:39", "link": "http://arxiv.org/abs/2210.04320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contrastive Representation Learning for Conversational Question\n  Answering over Knowledge Graphs", "abstract": "This paper addresses the task of conversational question answering (ConvQA)\nover knowledge graphs (KGs). The majority of existing ConvQA methods rely on\nfull supervision signals with a strict assumption of the availability of gold\nlogical forms of queries to extract answers from the KG. However, creating such\na gold logical form is not viable for each potential question in a real-world\nscenario. Hence, in the case of missing gold logical forms, the existing\ninformation retrieval-based approaches use weak supervision via heuristics or\nreinforcement learning, formulating ConvQA as a KG path ranking problem.\nDespite missing gold logical forms, an abundance of conversational contexts,\nsuch as entire dialog history with fluent responses and domain information, can\nbe incorporated to effectively reach the correct KG path. This work proposes a\ncontrastive representation learning-based approach to rank KG paths\neffectively. Our approach solves two key challenges. Firstly, it allows weak\nsupervision-based learning that omits the necessity of gold annotations.\nSecond, it incorporates the conversational context (entire dialog history and\ndomain information) to jointly learn its homogeneous representation with KG\npaths to improve contrastive representations for effective path ranking. We\nevaluate our approach on standard datasets for ConvQA, on which it\nsignificantly outperforms existing baselines on all domains and overall.\nSpecifically, in some cases, the Mean Reciprocal Rank (MRR) and Hit@5 ranking\nmetrics improve by absolute 10 and 18 points, respectively, compared to the\nstate-of-the-art performance.", "published": "2022-10-09 23:11:58", "link": "http://arxiv.org/abs/2210.04373v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Span Representations for Named Entity Recognition", "abstract": "Span-based models are one of the most straightforward methods for named\nentity recognition (NER). Existing span-based NER systems shallowly aggregate\nthe token representations to span representations. However, this typically\nresults in significant ineffectiveness for long-span entities, a coupling\nbetween the representations of overlapping spans, and ultimately a performance\ndegradation. In this study, we propose DSpERT (Deep Span Encoder\nRepresentations from Transformers), which comprises a standard Transformer and\na span Transformer. The latter uses low-layered span representations as\nqueries, and aggregates the token representations as keys and values, layer by\nlayer from bottom to top. Thus, DSpERT produces span representations of deep\nsemantics.\n  With weight initialization from pretrained language models, DSpERT achieves\nperformance higher than or competitive with recent state-of-the-art systems on\neight NER benchmarks. Experimental results verify the importance of the depth\nfor span representations, and show that DSpERT performs particularly well on\nlong-span entities and nested structures. Further, the deep span\nrepresentations are well structured and easily separable in the feature space.", "published": "2022-10-09 06:29:04", "link": "http://arxiv.org/abs/2210.04182v2", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Controllable Dialogue Simulation with In-Context Learning", "abstract": "Building dialogue systems requires a large corpus of annotated dialogues.\nSuch datasets are usually created via crowdsourcing, which is expensive and\ntime-consuming. In this paper, we propose \\textsc{Dialogic}, a novel dialogue\nsimulation method based on large language model in-context learning to automate\ndataset creation. Seeded with a few annotated dialogues, \\textsc{Dialogic}\nautomatically selects in-context examples for demonstration and prompts GPT-3\nto generate new dialogues and annotations in a controllable way. Our method can\nrapidly expand a small set of dialogue data with minimum or zero \\textit{human\ninvolvement} and \\textit{parameter update} and is thus much more cost-efficient\nand time-saving than crowdsourcing. Experimental results on the MultiWOZ\ndataset demonstrate that training a model on the simulated dialogues leads to\neven better performance than using the same amount of human-generated dialogues\nunder the challenging low-resource settings, with as few as 85 dialogues as a\nseed. When enough data is available, our method can still serve as an effective\ndata augmentation method. Human evaluation results also show that our simulated\ndialogues have near-human fluency and annotation accuracy. The code and data\nare available at \\textbf{\\url{https://github.com/Leezekun/dialogic}}.", "published": "2022-10-09 06:32:58", "link": "http://arxiv.org/abs/2210.04185v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Analogy Generation by Prompting Large Language Models: A Case Study of\n  InstructGPT", "abstract": "We propose a novel application of prompting Pre-trained Language Models\n(PLMs) to generate analogies and study how to design effective prompts for two\ntask settings: generating a source concept analogous to a given target concept\n(aka Analogous Concept Generation or ACG), and generating an explanation of the\nsimilarity between a given pair of target concept and source concept (aka\nAnalogous Explanation Generation or AEG). We found that it is feasible to\nprompt InstructGPT to generate meaningful analogies and the best prompts tend\nto be precise imperative statements especially with a low temperature setting.\nWe also systematically analyzed the sensitivity of the InstructGPT model to\nprompt design, temperature, and injected spelling errors, and found that the\nmodel is particularly sensitive to certain variations (e.g., questions vs.\nimperative statements). Further, we conducted human evaluation on 1.4k of the\ngenerated analogies and found that the quality of generations varies\nsubstantially by model size. The largest InstructGPT model can achieve\nhuman-level performance at generating meaningful analogies for a given target\nwhile there is still room for improvement on the AEG task.", "published": "2022-10-09 06:35:14", "link": "http://arxiv.org/abs/2210.04186v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Spread Love Not Hate: Undermining the Importance of Hateful Pre-training\n  for Hate Speech Detection", "abstract": "Pre-training large neural language models, such as BERT, has led to\nimpressive gains on many natural language processing (NLP) tasks. Although this\nmethod has proven to be effective for many domains, it might not always provide\ndesirable benefits. In this paper, we study the effects of hateful pre-training\non low-resource hate speech classification tasks. While previous studies on the\nEnglish language have emphasized its importance, we aim to augment their\nobservations with some non-obvious insights. We evaluate different variations\nof tweet-based BERT models pre-trained on hateful, non-hateful, and mixed\nsubsets of a 40M tweet dataset. This evaluation is carried out for the Indian\nlanguages Hindi and Marathi. This paper is empirical evidence that hateful\npre-training is not the best pre-training option for hate speech detection. We\nshow that pre-training on non-hateful text from the target domain provides\nsimilar or better results. Further, we introduce HindTweetBERT and\nMahaTweetBERT, the first publicly available BERT models pre-trained on Hindi\nand Marathi tweets, respectively. We show that they provide state-of-the-art\nperformance on hate speech classification tasks. We also release hateful BERT\nfor the two languages and a gold hate speech evaluation benchmark HateEval-Hi\nand HateEval-Mr consisting of manually labeled 2000 tweets each. The models and\ndata are available at https://github.com/l3cube-pune/MarathiNLP .", "published": "2022-10-09 13:53:06", "link": "http://arxiv.org/abs/2210.04267v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KSAT: Knowledge-infused Self Attention Transformer -- Integrating\n  Multiple Domain-Specific Contexts", "abstract": "Domain-specific language understanding requires integrating multiple pieces\nof relevant contextual information. For example, we see both suicide and\ndepression-related behavior (multiple contexts) in the text ``I have a gun and\nfeel pretty bad about my life, and it wouldn't be the worst thing if I didn't\nwake up tomorrow''. Domain specificity in self-attention architectures is\nhandled by fine-tuning on excerpts from relevant domain specific resources\n(datasets and external knowledge - medical textbook chapters on mental health\ndiagnosis related to suicide and depression). We propose a modified\nself-attention architecture Knowledge-infused Self Attention Transformer (KSAT)\nthat achieves the integration of multiple domain-specific contexts through the\nuse of external knowledge sources. KSAT introduces knowledge-guided biases in\ndedicated self-attention layers for each knowledge source to accomplish this.\nIn addition, KSAT provides mechanics for controlling the trade-off between\nlearning from data and learning from knowledge. Our quantitative and\nqualitative evaluations show that (1) the KSAT architecture provides novel\nhuman-understandable ways to precisely measure and visualize the contributions\nof the infused domain contexts, and (2) KSAT performs competitively with other\nknowledge-infused baselines and significantly outperforms baselines that use\nfine-tuning for domain-specific tasks.", "published": "2022-10-09 17:23:49", "link": "http://arxiv.org/abs/2210.04307v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Quantifying Social Biases Using Templates is Unreliable", "abstract": "Recently, there has been an increase in efforts to understand how large\nlanguage models (LLMs) propagate and amplify social biases. Several works have\nutilized templates for fairness evaluation, which allow researchers to quantify\nsocial biases in the absence of test sets with protected attribute labels.\nWhile template evaluation can be a convenient and helpful diagnostic tool to\nunderstand model deficiencies, it often uses a simplistic and limited set of\ntemplates. In this paper, we study whether bias measurements are sensitive to\nthe choice of templates used for benchmarking. Specifically, we investigate the\ninstability of bias measurements by manually modifying templates proposed in\nprevious works in a semantically-preserving manner and measuring bias across\nthese modifications. We find that bias values and resulting conclusions vary\nconsiderably across template modifications on four tasks, ranging from an 81%\nreduction (NLI) to a 162% increase (MLM) in (task-specific) bias measurements.\nOur results indicate that quantifying fairness in LLMs, as done in current\npractice, can be brittle and needs to be approached with more care and caution.", "published": "2022-10-09 20:05:29", "link": "http://arxiv.org/abs/2210.04337v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adapting Pretrained Vision-Language Foundational Models to Medical\n  Imaging Domains", "abstract": "Multi-modal foundation models are typically trained on millions of pairs of\nnatural images and text captions, frequently obtained through web-crawling\napproaches. Although such models depict excellent generative capabilities, they\ndo not typically generalize well to specific domains such as medical images\nthat have fundamentally shifted distributions compared to natural images.\nBuilding generative models for medical images that faithfully depict clinical\ncontext may help alleviate the paucity of healthcare datasets. Thus, in this\nstudy, we seek to research and expand the representational capabilities of\nlarge pretrained foundation models to medical concepts, specifically for\nleveraging the Stable Diffusion model to generate domain specific images found\nin medical imaging. We explore the sub-components of the Stable Diffusion\npipeline (the variational autoencoder, the U-Net and the text-encoder) to\nfine-tune the model to generate medical images. We benchmark the efficacy of\nthese efforts using quantitative image quality metrics and qualitative\nradiologist-driven evaluations that accurately represent the clinical content\nof conditional text prompts. Our best-performing model improves upon the stable\ndiffusion baseline and can be conditioned to insert a realistic-looking\nabnormality on a synthetic radiology image, while maintaining a 95% accuracy on\na classifier trained to detect the abnormality.", "published": "2022-10-09 01:43:08", "link": "http://arxiv.org/abs/2210.04133v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language\n  Representation Learning", "abstract": "Multimodal representation learning has shown promising improvements on\nvarious vision-language tasks. Most existing methods excel at building\nglobal-level alignment between vision and language while lacking effective\nfine-grained image-text interaction. In this paper, we propose a jointly masked\nmultimodal modeling method to learn fine-grained multimodal representations.\nOur method performs joint masking on image-text input and integrates both\nimplicit and explicit targets for the masked signals to recover. The implicit\ntarget provides a unified and debiased objective for vision and language, where\nthe model predicts latent multimodal representations of the unmasked input. The\nexplicit target further enriches the multimodal representations by recovering\nhigh-level and semantically meaningful information: momentum visual features of\nimage patches and concepts of word tokens. Through such a masked modeling\nprocess, our model not only learns fine-grained multimodal interaction, but\nalso avoids the semantic gap between high-level representations and low- or\nmid-level prediction targets (e.g. image pixels), thus producing semantically\nrich multimodal representations that perform well on both zero-shot and\nfine-tuned settings. Our pre-trained model (named MAMO) achieves\nstate-of-the-art performance on various downstream vision-language tasks,\nincluding image-text retrieval, visual question answering, visual reasoning,\nand weakly-supervised visual grounding.", "published": "2022-10-09 06:31:15", "link": "http://arxiv.org/abs/2210.04183v3", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text\n  Generation Models", "abstract": "We motivate and introduce CHARD: Clinical Health-Aware Reasoning across\nDimensions, to investigate the capability of text generation models to act as\nimplicit clinical knowledge bases and generate free-flow textual explanations\nabout various health-related conditions across several dimensions. We collect\nand present an associated dataset, CHARDat, consisting of explanations about 52\nhealth conditions across three clinical dimensions. We conduct extensive\nexperiments using BART and T5 along with data augmentation, and perform\nautomatic, human, and qualitative analyses. We show that while our models can\nperform decently, CHARD is very challenging with strong potential for further\nexploration.", "published": "2022-10-09 07:16:58", "link": "http://arxiv.org/abs/2210.04191v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ASDOT: Any-Shot Data-to-Text Generation with Pretrained Language Models", "abstract": "Data-to-text generation is challenging due to the great variety of the input\ndata in terms of domains (e.g., finance vs sports) or schemata (e.g., diverse\npredicates). Recent end-to-end neural methods thus require substantial training\nexamples to learn to disambiguate and describe the data. Yet, real-world\ndata-to-text problems often suffer from various data-scarce issues: one may\nhave access to only a handful of or no training examples, and/or have to rely\non examples in a different domain or schema. To fill this gap, we propose\nAny-Shot Data-to-Text (ASDOT), a new approach flexibly applicable to diverse\nsettings by making efficient use of any given (or no) examples. ASDOT consists\nof two steps, data disambiguation and sentence fusion, both of which are\namenable to be solved with off-the-shelf pretrained language models (LMs) with\noptional finetuning. In the data disambiguation stage, we employ the prompted\nGPT-3 model to understand possibly ambiguous triples from the input data and\nconvert each into a short sentence with reduced ambiguity. The sentence fusion\nstage then uses an LM like T5 to fuse all the resulting sentences into a\ncoherent paragraph as the final description. We evaluate extensively on various\ndatasets in different scenarios, including the zero-/few-/full-shot settings,\nand generalization to unseen predicates and out-of-domain data. Experimental\nresults show that ASDOT consistently achieves significant improvement over\nbaselines, e.g., a 30.81 BLEU gain on the DART dataset under the zero-shot\nsetting.", "published": "2022-10-09 19:17:43", "link": "http://arxiv.org/abs/2210.04325v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years\n  of German Parliamentary Debates", "abstract": "Solidarity is a crucial concept to understand social relations in societies.\nIn this paper, we explore fine-grained solidarity frames to study solidarity\ntowards women and migrants in German parliamentary debates between 1867 and\n2022. Using 2,864 manually annotated text snippets (with a cost exceeding 18k\nEuro), we evaluate large language models (LLMs) like Llama 3, GPT-3.5, and\nGPT-4. We find that GPT-4 outperforms other LLMs, approaching human annotation\nquality. Using GPT-4, we automatically annotate more than 18k further instances\n(with a cost of around 500 Euro) across 155 years and find that solidarity with\nmigrants outweighs anti-solidarity but that frequencies and solidarity types\nshift over time. Most importantly, group-based notions of (anti-)solidarity\nfade in favor of compassionate solidarity, focusing on the vulnerability of\nmigrant groups, and exchange-based anti-solidarity, focusing on the lack of\n(economic) contribution. Our study highlights the interplay of historical\nevents, socio-economic needs, and political ideologies in shaping migration\ndiscourse and social cohesion. We also show that powerful LLMs, if carefully\nprompted, can be cost-effective alternatives to human annotation for hard\nsocial scientific tasks.", "published": "2022-10-09 22:02:58", "link": "http://arxiv.org/abs/2210.04359v3", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "VCSE: Time-Domain Visual-Contextual Speaker Extraction Network", "abstract": "Speaker extraction seeks to extract the target speech in a multi-talker\nscenario given an auxiliary reference. Such reference can be auditory, i.e., a\npre-recorded speech, visual, i.e., lip movements, or contextual, i.e., phonetic\nsequence. References in different modalities provide distinct and complementary\ninformation that could be fused to form top-down attention on the target\nspeaker. Previous studies have introduced visual and contextual modalities in a\nsingle model. In this paper, we propose a two-stage time-domain\nvisual-contextual speaker extraction network named VCSE, which incorporates\nvisual and self-enrolled contextual cues stage by stage to take full advantage\nof every modality. In the first stage, we pre-extract a target speech with\nvisual cues and estimate the underlying phonetic sequence. In the second stage,\nwe refine the pre-extracted target speech with the self-enrolled contextual\ncues. Experimental results on the real-world Lip Reading Sentences 3 (LRS3)\ndatabase demonstrate that our proposed VCSE network consistently outperforms\nother state-of-the-art baselines.", "published": "2022-10-09 12:29:38", "link": "http://arxiv.org/abs/2210.06177v1", "categories": ["cs.CV", "cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
