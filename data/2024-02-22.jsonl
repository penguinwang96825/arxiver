{"title": "Learning to Reduce: Optimal Representations of Structured Data in\n  Prompting Large Language Models", "abstract": "Large Language Models (LLMs) have been widely used as general-purpose AI\nagents showing comparable performance on many downstream tasks. However,\nexisting work shows that it is challenging for LLMs to integrate structured\ndata (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand\nlong text data or select the most relevant evidence prior to inference, and\nboth approaches are not trivial.\n  In this paper, we propose a framework, Learning to Reduce, that fine-tunes a\nlanguage model to generate a reduced version of an input context, given a task\ndescription and context input. The model learns to reduce the input context\nusing On-Policy Reinforcement Learning and aims to improve the reasoning\nperformance of a fixed LLM. Experimental results illustrate that our model not\nonly achieves comparable accuracies in selecting the relevant evidence from an\ninput context, but also shows generalizability on different datasets. We\nfurther show that our model helps improve the LLM's performance on downstream\ntasks especially when the context is long.", "published": "2024-02-22 00:41:23", "link": "http://arxiv.org/abs/2402.14195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Understanding Counseling Conversations: Domain Knowledge and\n  Large Language Models", "abstract": "Understanding the dynamics of counseling conversations is an important task,\nyet it is a challenging NLP problem regardless of the recent advance of\nTransformer-based pre-trained language models. This paper proposes a systematic\napproach to examine the efficacy of domain knowledge and large language models\n(LLMs) in better representing conversations between a crisis counselor and a\nhelp seeker. We empirically show that state-of-the-art language models such as\nTransformer-based models and GPT models fail to predict the conversation\noutcome. To provide richer context to conversations, we incorporate\nhuman-annotated domain knowledge and LLM-generated features; simple integration\nof domain knowledge and LLM features improves the model performance by\napproximately 15%. We argue that both domain knowledge and LLM-generated\nfeatures can be exploited to better characterize counseling conversations when\nthey are used as an additional context to conversations.", "published": "2024-02-22 01:02:37", "link": "http://arxiv.org/abs/2402.14200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Framing in the Presence of Supporting Data: A Case Study in U.S.\n  Economic News", "abstract": "The mainstream media has much leeway in what it chooses to cover and how it\ncovers it. These choices have real-world consequences on what people know and\ntheir subsequent behaviors. However, the lack of objective measures to evaluate\neditorial choices makes research in this area particularly difficult. In this\npaper, we argue that there are newsworthy topics where objective measures exist\nin the form of supporting data and propose a computational framework to analyze\neditorial choices in this setup. We focus on the economy because the reporting\nof economic indicators presents us with a relatively easy way to determine both\nthe selection and framing of various publications. Their values provide a\nground truth of how the economy is doing relative to how the publications\nchoose to cover it. To do this, we define frame prediction as a set of\ninterdependent tasks. At the article level, we learn to identify the reported\nstance towards the general state of the economy. Then, for every numerical\nquantity reported in the article, we learn to identify whether it corresponds\nto an economic indicator and whether it is being reported in a positive or\nnegative way. To perform our analysis, we track six American publishers and\neach article that appeared in the top 10 slots of their landing page between\n2015 and 2023.", "published": "2024-02-22 02:07:21", "link": "http://arxiv.org/abs/2402.14224v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Eagle: Ethical Dataset Given from Real Interactions", "abstract": "Recent studies have demonstrated that large language models (LLMs) have\nethical-related problems such as social biases, lack of moral reasoning, and\ngeneration of offensive content. The existing evaluation metrics and methods to\naddress these ethical challenges use datasets intentionally created by\ninstructing humans to create instances including ethical problems. Therefore,\nthe data does not reflect prompts that users actually provide when utilizing\nLLM services in everyday contexts. This may not lead to the development of safe\nLLMs that can address ethical challenges arising in real-world applications. In\nthis paper, we create Eagle datasets extracted from real interactions between\nChatGPT and users that exhibit social biases, toxicity, and immoral problems.\nOur experiments show that Eagle captures complementary aspects, not covered by\nexisting datasets proposed for evaluation and mitigation of such ethical\nchallenges. Our code is publicly available at\nhttps://huggingface.co/datasets/MasahiroKaneko/eagle.", "published": "2024-02-22 03:46:02", "link": "http://arxiv.org/abs/2402.14258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Qsnail: A Questionnaire Dataset for Sequential Question Generation", "abstract": "The questionnaire is a professional research methodology used for both\nqualitative and quantitative analysis of human opinions, preferences,\nattitudes, and behaviors. However, designing and evaluating questionnaires\ndemands significant effort due to their intricate and complex structure.\nQuestionnaires entail a series of questions that must conform to intricate\nconstraints involving the questions, options, and overall structure.\nSpecifically, the questions should be relevant and specific to the given\nresearch topic and intent. The options should be tailored to the questions,\nensuring they are mutually exclusive, completed, and ordered sensibly.\nMoreover, the sequence of questions should follow a logical order, grouping\nsimilar topics together. As a result, automatically generating questionnaires\npresents a significant challenge and this area has received limited attention\nprimarily due to the scarcity of high-quality datasets. To address these\nissues, we present Qsnail, the first dataset specifically constructed for the\nquestionnaire generation task, which comprises 13,168 human-written\nquestionnaires gathered from online platforms. We further conduct experiments\non Qsnail, and the results reveal that retrieval models and traditional\ngenerative models do not fully align with the given research topic and intents.\nLarge language models, while more closely related to the research topic and\nintents, exhibit significant limitations in terms of diversity and specificity.\nDespite enhancements through the chain-of-thought prompt and finetuning,\nquestionnaires generated by language models still fall short of human-written\nquestionnaires. Therefore, questionnaire generation is challenging and needs to\nbe further explored. The dataset is available at:\nhttps://github.com/LeiyanGithub/qsnail.", "published": "2024-02-22 04:14:10", "link": "http://arxiv.org/abs/2402.14272v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Language Models Act as Knowledge Bases at Scale?", "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating responses to complex queries through large-scale\npre-training. However, the efficacy of these models in memorizing and reasoning\namong large-scale structured knowledge, especially world knowledge that\nexplicitly covers abundant factual information remains questionable. Addressing\nthis gap, our research investigates whether LLMs can effectively store, recall,\nand reason with knowledge on a large scale comparable to latest knowledge bases\n(KBs) such as Wikidata. Specifically, we focus on three crucial aspects to\nstudy the viability: (1) the efficiency of LLMs with different sizes in\nmemorizing the exact knowledge in the large-scale KB; (2) the flexibility of\nrecalling the memorized knowledge in response to natural language queries; (3)\nthe capability to infer new knowledge through reasoning. Our findings indicate\nthat while LLMs hold promise as large-scale KBs capable of retrieving and\nresponding with flexibility, enhancements in their reasoning capabilities are\nnecessary to fully realize their potential.", "published": "2024-02-22 04:20:14", "link": "http://arxiv.org/abs/2402.14273v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Concept Graph Recovery and Question\n  Answering in NLP Education", "abstract": "In the domain of Natural Language Processing (NLP), Large Language Models\n(LLMs) have demonstrated promise in text-generation tasks. However, their\neducational applications, particularly for domain-specific queries, remain\nunderexplored. This study investigates LLMs' capabilities in educational\nscenarios, focusing on concept graph recovery and question-answering (QA). We\nassess LLMs' zero-shot performance in creating domain-specific concept graphs\nand introduce TutorQA, a new expert-verified NLP-focused benchmark for\nscientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA\npairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating\nconcept graphs with LLMs for answering diverse questions. Our results indicate\nthat LLMs' zero-shot concept graph recovery is competitive with supervised\nmethods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs\nachieve up to 26% F1 score enhancement. Moreover, human evaluation and analysis\nshow that CGLLM generates answers with more fine-grained concepts.", "published": "2024-02-22 05:15:27", "link": "http://arxiv.org/abs/2402.14293v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mitigating Biases of Large Language Models in Stance Detection with\n  Counterfactual Augmented Calibration", "abstract": "Stance detection is critical for understanding the underlying position or\nattitude expressed toward a topic. Large language models (LLMs) have\ndemonstrated significant advancements across various natural language\nprocessing tasks including stance detection, however, their performance in\nstance detection is limited by biases and spurious correlations inherent due to\ntheir data-driven nature. Our statistical experiment reveals that LLMs are\nprone to generate biased stances due to sentiment-stance spurious correlations\nand preference towards certain individuals and topics. Furthermore, the results\ndemonstrate a strong negative correlation between stance bias and stance\ndetection performance, underscoring the importance of mitigating bias to\nenhance the utility of LLMs in stance detection. Therefore, in this paper, we\npropose a Counterfactual Augmented Calibration Network (FACTUAL), which a novel\ncalibration network is devised to calibrate potential bias in the stance\nprediction of LLMs. Further, to address the challenge of effectively learning\nbias representations and the difficulty in the generalizability of debiasing,\nwe construct counterfactual augmented data. This approach enhances the\ncalibration network, facilitating the debiasing and out-of-domain\ngeneralization. Experimental results on in-target and zero-shot stance\ndetection tasks show that the proposed FACTUAL can effectively mitigate biases\nof LLMs, achieving state-of-the-art results.", "published": "2024-02-22 05:17:49", "link": "http://arxiv.org/abs/2402.14296v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-modal Stance Detection: New Datasets and Model", "abstract": "Stance detection is a challenging task that aims to identify public opinion\nfrom social media platforms with respect to specific targets. Previous work on\nstance detection largely focused on pure texts. In this paper, we study\nmulti-modal stance detection for tweets consisting of texts and images, which\nare prevalent in today's fast-growing social media platforms where people often\npost multi-modal messages. To this end, we create five new multi-modal stance\ndetection datasets of different domains based on Twitter, in which each example\nconsists of a text and an image. In addition, we propose a simple yet effective\nTargeted Multi-modal Prompt Tuning framework (TMPT), where target information\nis leveraged to learn multi-modal stance features from textual and visual\nmodalities. Experimental results on our five benchmark datasets show that the\nproposed TMPT achieves state-of-the-art performance in multi-modal stance\ndetection.", "published": "2024-02-22 05:24:19", "link": "http://arxiv.org/abs/2402.14298v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize\n  Encoded Knowledge", "abstract": "Large Language Models (LLMs) have recently showcased remarkable\ngeneralizability in various domains. Despite their extensive knowledge, LLMs\nstill face challenges in efficiently utilizing encoded knowledge to develop\naccurate and logical reasoning processes. To mitigate this problem, we\nintroduced Hint-before-Solving Prompting (HSP), which guides the model to\ngenerate hints (e.g., specific knowledge or key ideas) for solving the problem\nand then generate solutions containing intermediate reasoning steps. Since HSP\nis orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied\nHSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results\nof extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs\ndemonstrate that HSP can effectively improve the accuracy of reasoning tasks:\n(1) By applying high-quality hint-enhanced HSP to CoT prompting,\nLlama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free\nLLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned\nLlemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We\nmake our code and dataset publicly available at\n\\url{https://github.com/jinlanfu/HSP}.", "published": "2024-02-22 05:58:03", "link": "http://arxiv.org/abs/2402.14310v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing generalization capability of text ranking models in Polish", "abstract": "Retrieval-augmented generation (RAG) is becoming an increasingly popular\ntechnique for integrating internal knowledge bases with large language models.\nIn a typical RAG pipeline, three models are used, responsible for the\nretrieval, reranking, and generation stages. In this article, we focus on the\nreranking problem for the Polish language, examining the performance of\nrerankers and comparing their results with available retrieval models. We\nconduct a comprehensive evaluation of existing models and those trained by us,\nutilizing a benchmark of 41 diverse information retrieval tasks for the Polish\nlanguage. The results of our experiments show that most models struggle with\nout-of-domain generalization. However, a combination of effective optimization\nmethod and a large training dataset allows for building rerankers that are both\ncompact in size and capable of generalization. The best of our models\nestablishes a new state-of-the-art for reranking in the Polish language,\noutperforming existing models with up to 30 times more parameters.", "published": "2024-02-22 06:21:41", "link": "http://arxiv.org/abs/2402.14318v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Understanding and Patching Compositional Reasoning in LLMs", "abstract": "LLMs have marked a revolutonary shift, yet they falter when faced with\ncompositional reasoning tasks. Our research embarks on a quest to uncover the\nroot causes of compositional reasoning failures of LLMs, uncovering that most\nof them stem from the improperly generated or leveraged implicit reasoning\nresults. Inspired by our empirical findings, we resort to Logit Lens and an\nintervention experiment to dissect the inner hidden states of LLMs. This deep\ndive reveals that implicit reasoning results indeed surface within middle\nlayers and play a causative role in shaping the final explicit reasoning\nresults. Our exploration further locates multi-head self-attention (MHSA)\nmodules within these layers, which emerge as the linchpins in accurate\ngeneration and leveraing of implicit reasoning results. Grounded on the above\nfindings, we develop CREME, a lightweight method to patch errors in\ncompositional reasoning via editing the located MHSA modules. Our empirical\nevidence stands testament to CREME's effectiveness, paving the way for\nautonomously and continuously enhancing compositional reasoning capabilities in\nlanguage models.", "published": "2024-02-22 06:47:56", "link": "http://arxiv.org/abs/2402.14328v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "INSTRUCTIR: A Benchmark for Instruction Following of Information\n  Retrieval Models", "abstract": "Despite the critical need to align search targets with users' intention,\nretrievers often only prioritize query information without delving into the\nusers' intended search context. Enhancing the capability of retrievers to\nunderstand intentions and preferences of users, akin to language model\ninstructions, has the potential to yield more aligned search targets. Prior\nstudies restrict the application of instructions in information retrieval to a\ntask description format, neglecting the broader context of diverse and evolving\nsearch scenarios. Furthermore, the prevailing benchmarks utilized for\nevaluation lack explicit tailoring to assess instruction-following ability,\nthereby hindering progress in this field. In response to these limitations, we\npropose a novel benchmark,INSTRUCTIR, specifically designed to evaluate\ninstruction-following ability in information retrieval tasks. Our approach\nfocuses on user-aligned instructions tailored to each query instance,\nreflecting the diverse characteristics inherent in real-world search scenarios.\nThrough experimental analysis, we observe that retrievers fine-tuned to follow\ntask-style instructions, such as INSTRUCTOR, can underperform compared to their\nnon-instruction-tuned counterparts. This underscores potential overfitting\nissues inherent in constructing retrievers trained on existing\ninstruction-aware retrieval datasets.", "published": "2024-02-22 06:59:50", "link": "http://arxiv.org/abs/2402.14334v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rule or Story, Which is a Better Commonsense Expression for Talking with\n  Large Language Models?", "abstract": "Building machines with commonsense has been a longstanding challenge in NLP\ndue to the reporting bias of commonsense rules and the exposure bias of\nrule-based commonsense reasoning. In contrast, humans convey and pass down\ncommonsense implicitly through stories. This paper investigates the inherent\ncommonsense ability of large language models (LLMs) expressed through\nstorytelling. We systematically investigate and compare stories and rules for\nretrieving and leveraging commonsense in LLMs. Experimental results on 28\ncommonsense QA datasets show that stories outperform rules as the expression\nfor retrieving commonsense from LLMs, exhibiting higher generation confidence\nand commonsense accuracy. Moreover, stories are the more effective commonsense\nexpression for answering questions regarding daily events, while rules are more\neffective for scientific questions. This aligns with the reporting bias of\ncommonsense in text corpora. We further show that the correctness and relevance\nof commonsense stories can be further improved via iterative self-supervised\nfine-tuning. These findings emphasize the importance of using appropriate\nlanguage to express, retrieve, and leverage commonsense for LLMs, highlighting\na promising direction for better exploiting their commonsense abilities.", "published": "2024-02-22 07:55:26", "link": "http://arxiv.org/abs/2402.14355v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable\n  Metrics on Facet-aware Benchmark", "abstract": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs.", "published": "2024-02-22 07:58:29", "link": "http://arxiv.org/abs/2402.14359v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Small Language Models as Effective Guides for Large Language Models in\n  Chinese Relation Extraction", "abstract": "Recently, large language models (LLMs) have been successful in relational\nextraction (RE) tasks, especially in the few-shot learning. An important\nproblem in the field of RE is long-tailed data, while not much attention is\npaid to this problem using LLM approaches. Therefore, in this paper, we propose\nSLCoLM, a model collaboration framework, to mitigate the data long-tail\nproblem. In our framework, we use the ``\\textit{Training-Guide-Predict}''\nstrategy to combine the strengths of small pre-trained language models (SLMs)\nand LLMs, where a task-specific SLM framework acts as a guider, transfers task\nknowledge to the LLM and guides the LLM in performing RE tasks. Our experiments\non an ancient Chinese RE dataset rich in relation types show that the approach\nfacilitates RE of long-tail relation types.", "published": "2024-02-22 08:26:56", "link": "http://arxiv.org/abs/2402.14373v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Novi jezi\u010dki modeli za srpski jezik", "abstract": "The paper will briefly present the development history of transformer-based\nlanguage models for the Serbian language. Several new models for text\ngeneration and vectorization, trained on the resources of the Society for\nLanguage Resources and Technologies, will also be presented. Ten selected\nvectorization models for Serbian, including two new ones, will be compared on\nfour natural language processing tasks. Paper will analyze which models are the\nbest for each selected task, how does their size and the size of their training\nsets affect the performance on those tasks, and what is the optimal setting to\ntrain the best language models for the Serbian language.", "published": "2024-02-22 08:48:21", "link": "http://arxiv.org/abs/2402.14379v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain-of-History Reasoning for Temporal Knowledge Graph Forecasting", "abstract": "Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based\non given histories. Most recent graph-based models excel at capturing\nstructural information within TKGs but lack semantic comprehension abilities.\nNowadays, with the surge of LLMs, the LLM-based TKG prediction model has\nemerged. However, the existing LLM-based model exhibits three shortcomings: (1)\nIt only focuses on the first-order history for prediction while ignoring\nhigh-order historical information, resulting in the provided information for\nLLMs being extremely limited. (2) LLMs struggle with optimal reasoning\nperformance under heavy historical information loads. (3) For TKG prediction,\nthe temporal reasoning capability of LLM alone is limited. To address the first\ntwo challenges, we propose Chain-of-History (CoH) reasoning which explores\nhigh-order histories step-by-step, achieving effective utilization of\nhigh-order historical information for LLMs on TKG prediction. To address the\nthird issue, we design CoH as a plug-and-play module to enhance the performance\nof graph-based models for TKG prediction. Extensive experiments on three\ndatasets and backbones demonstrate the effectiveness of CoH.", "published": "2024-02-22 08:51:39", "link": "http://arxiv.org/abs/2402.14382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Transferring BERT Capabilities from High-Resource to Low-Resource\n  Languages Using Vocabulary Matching", "abstract": "Pre-trained language models have revolutionized the natural language\nunderstanding landscape, most notably BERT (Bidirectional Encoder\nRepresentations from Transformers). However, a significant challenge remains\nfor low-resource languages, where limited data hinders the effective training\nof such models. This work presents a novel approach to bridge this gap by\ntransferring BERT capabilities from high-resource to low-resource languages\nusing vocabulary matching. We conduct experiments on the Silesian and Kashubian\nlanguages and demonstrate the effectiveness of our approach to improve the\nperformance of BERT models even when the target language has minimal training\ndata. Our results highlight the potential of the proposed technique to\neffectively train BERT models for low-resource languages, thus democratizing\naccess to advanced language understanding models.", "published": "2024-02-22 09:49:26", "link": "http://arxiv.org/abs/2402.14408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "J-UniMorph: Japanese Morphological Annotation through the Universal\n  Feature Schema", "abstract": "We introduce a Japanese Morphology dataset, J-UniMorph, developed based on\nthe UniMorph feature schema. This dataset addresses the unique and rich verb\nforms characteristic of the language's agglutinative nature. J-UniMorph\ndistinguishes itself from the existing Japanese subset of UniMorph, which is\nautomatically extracted from Wiktionary. On average, the Wiktionary Edition\nfeatures around 12 inflected forms for each word and is primarily dominated by\ndenominal verbs (i.e., [noun] +suru (do-PRS)). Morphologically, this form is\nequivalent to the verb suru (do). In contrast, J-UniMorph explores a much\nbroader and more frequently used range of verb forms, offering 118 inflected\nforms for each word on average. It includes honorifics, a range of politeness\nlevels, and other linguistic nuances, emphasizing the distinctive\ncharacteristics of the Japanese language. This paper presents detailed\nstatistics and characteristics of J-UniMorph, comparing it with the Wiktionary\nEdition. We release J-UniMorph and its interactive visualizer publicly\navailable, aiming to support cross-linguistic research and various\napplications.", "published": "2024-02-22 09:56:51", "link": "http://arxiv.org/abs/2402.14411v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?", "abstract": "Education that suits the individual learning level is necessary to improve\nstudents' understanding. The first step in achieving this purpose by using\nlarge language models (LLMs) is to adjust the textual difficulty of the\nresponse to students. This work analyzes how LLMs can implicitly adjust text\ndifficulty between user input and its generated text. To conduct the\nexperiments, we created a new dataset from Stack-Overflow to explore the\nperformance of question-answering-based conversation. Experimental results on\nthe Stack-Overflow dataset and the TSCC dataset, including multi-turn\nconversation show that LLMs can implicitly handle text difficulty between user\ninput and its generated response. We also observed that some LLMs can surpass\nhumans in handling text difficulty and the importance of instruction-tuning.", "published": "2024-02-22 11:16:23", "link": "http://arxiv.org/abs/2402.14453v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Annotation and Classification of Relevant Clauses in\n  Terms-and-Conditions Contracts", "abstract": "In this paper, we propose a new annotation scheme to classify different types\nof clauses in Terms-and-Conditions contracts with the ultimate goal of\nsupporting legal experts to quickly identify and assess problematic issues in\nthis type of legal documents. To this end, we built a small corpus of\nTerms-and-Conditions contracts and finalized an annotation scheme of 14\ncategories, eventually reaching an inter-annotator agreement of 0.92. Then, for\n11 of them, we experimented with binary classification tasks using few-shot\nprompting with a multilingual T5 and two fine-tuned versions of two BERT-based\nLLMs for Italian. Our experiments showed the feasibility of automatic\nclassification of our categories by reaching accuracies ranging from .79 to .95\non validation tasks.", "published": "2024-02-22 11:24:45", "link": "http://arxiv.org/abs/2402.14457v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation\n  and Analysis", "abstract": "Causality is fundamental in human cognition and has drawn attention in\ndiverse research fields. With growing volumes of textual data, discerning\ncausalities within text data is crucial, and causal text mining plays a pivotal\nrole in extracting meaningful patterns. This study conducts comprehensive\nevaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce\na benchmark that extends beyond general English datasets, including\ndomain-specific and non-English datasets. We also provide an evaluation\nframework to ensure fair comparisons between ChatGPT and previous approaches.\nFinally, our analysis outlines the limitations and future challenges in\nemploying ChatGPT for causal text mining. Specifically, our analysis reveals\nthat ChatGPT serves as a good starting point for various datasets. However,\nwhen equipped with a sufficient amount of training data, previous models still\nsurpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency\nto falsely recognize non-causal sequences as causal sequences. These issues\nbecome even more pronounced with advanced versions of the model, such as GPT-4.\nIn addition, we highlight the constraints of ChatGPT in handling complex\ncausality types, including both intra/inter-sentential and implicit causality.\nThe model also faces challenges with effectively leveraging in-context learning\nand domain adaptation. We release our code to support further research and\ndevelopment in this field.", "published": "2024-02-22 12:19:04", "link": "http://arxiv.org/abs/2402.14484v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Does the Generator Mind its Contexts? An Analysis of Generative Model\n  Faithfulness under Context Transfer", "abstract": "The present study introduces the knowledge-augmented generator, which is\nspecifically designed to produce information that remains grounded in\ncontextual knowledge, regardless of alterations in the context. Previous\nresearch has predominantly focused on examining hallucinations stemming from\nstatic input, such as in the domains of summarization or machine translation.\nHowever, our investigation delves into the faithfulness of generative question\nanswering in the presence of dynamic knowledge. Our objective is to explore the\nexistence of hallucinations arising from parametric memory when contextual\nknowledge undergoes changes, while also analyzing the underlying causes for\ntheir occurrence. In order to efficiently address this issue, we propose a\nstraightforward yet effective measure for detecting such hallucinations.\nIntriguingly, our investigation uncovers that all models exhibit a tendency to\ngenerate previous answers as hallucinations. To gain deeper insights into the\nunderlying causes of this phenomenon, we conduct a series of experiments that\nverify the critical role played by context in hallucination, both during\ntraining and testing, from various perspectives.", "published": "2024-02-22 12:26:07", "link": "http://arxiv.org/abs/2402.14488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment\n  Pre-training for Noisy Slot Filling Task", "abstract": "In a realistic dialogue system, the input information from users is often\nsubject to various types of input perturbations, which affects the slot-filling\ntask. Although rule-based data augmentation methods have achieved satisfactory\nresults, they fail to exhibit the desired generalization when faced with\nunknown noise disturbances. In this study, we address the challenges posed by\ninput perturbations in slot filling by proposing Noise-BERT, a unified\nPerturbation-Robust Framework with Noise Alignment Pre-training. Our framework\nincorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and\nSentence Noisiness Discrimination, aiming to guide the pre-trained language\nmodel in capturing accurate slot information and noise distribution. During\nfine-tuning, we employ a contrastive learning loss to enhance the semantic\nrepresentation of entities and labels. Additionally, we introduce an\nadversarial attack training strategy to improve the model's robustness.\nExperimental results demonstrate the superiority of our proposed approach over\nstate-of-the-art models, and further analysis confirms its effectiveness and\ngeneralization ability.", "published": "2024-02-22 12:39:50", "link": "http://arxiv.org/abs/2402.14494v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "\"My Answer is C\": First-Token Probabilities Do Not Match Text Answers in\n  Instruction-Tuned Language Models", "abstract": "The open-ended nature of language generation makes the evaluation of\nautoregressive large language models (LLMs) challenging. One common evaluation\napproach uses multiple-choice questions (MCQ) to limit the response space. The\nmodel is then evaluated by ranking the candidate answers by the log probability\nof the first token prediction. However, first-tokens may not consistently\nreflect the final response output, due to model's diverse response styles such\nas starting with \"Sure\" or refusing to answer. Consequently, MCQ evaluation is\nnot indicative of model behaviour when interacting with users. But by how much?\nWe evaluate how aligned first-token evaluation is with the text output along\nseveral dimensions, namely final option choice, refusal rate, choice\ndistribution and robustness under prompt perturbation. Our results show that\nthe two approaches are severely misaligned on all dimensions, reaching mismatch\nrates over 60%. Models heavily fine-tuned on conversational or safety data are\nespecially impacted. Crucially, models remain misaligned even when we\nincreasingly constrain prompts, i.e., force them to start with an option letter\nor example template. Our findings i) underscore the importance of inspecting\nthe text output as well and ii) caution against relying solely on first-token\nevaluation.", "published": "2024-02-22 12:47:33", "link": "http://arxiv.org/abs/2402.14499v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Malaysian English News Decoded: A Linguistic Resource for Named Entity\n  and Relation Extraction", "abstract": "Standard English and Malaysian English exhibit notable differences, posing\nchallenges for natural language processing (NLP) tasks on Malaysian English.\nUnfortunately, most of the existing datasets are mainly based on standard\nEnglish and therefore inadequate for improving NLP tasks in Malaysian English.\nAn experiment using state-of-the-art Named Entity Recognition (NER) solutions\non Malaysian English news articles highlights that they cannot handle\nmorphosyntactic variations in Malaysian English. To the best of our knowledge,\nthere is no annotated dataset available to improvise the model. To address\nthese issues, we constructed a Malaysian English News (MEN) dataset, which\ncontains 200 news articles that are manually annotated with entities and\nrelations. We then fine-tuned the spaCy NER tool and validated that having a\ndataset tailor-made for Malaysian English could improve the performance of NER\nin Malaysian English significantly. This paper presents our effort in the data\nacquisition, annotation methodology, and thorough analysis of the annotated\ndataset. To validate the quality of the annotation, inter-annotator agreement\nwas used, followed by adjudication of disagreements by a subject matter expert.\nUpon completion of these tasks, we managed to develop a dataset with 6,061\nentities and 3,268 relation instances. Finally, we discuss on spaCy fine-tuning\nsetup and analysis on the NER performance. This unique dataset will contribute\nsignificantly to the advancement of NLP research in Malaysian English, allowing\nresearchers to accelerate their progress, particularly in NER and relation\nextraction. The dataset and annotation guideline has been published on Github.", "published": "2024-02-22 13:12:05", "link": "http://arxiv.org/abs/2402.14521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt\n  Politeness on LLM Performance", "abstract": "We investigate the impact of politeness levels in prompts on the performance\nof large language models (LLMs). Polite language in human communications often\ngarners more compliance and effectiveness, while rudeness can cause aversion,\nimpacting response quality. We consider that LLMs mirror human communication\ntraits, suggesting they align with human cultural norms. We assess the impact\nof politeness in prompts on LLMs across English, Chinese, and Japanese tasks.\nWe observed that impolite prompts often result in poor performance, but overly\npolite language does not guarantee better outcomes. The best politeness level\nis different according to the language. This phenomenon suggests that LLMs not\nonly reflect human behavior but are also influenced by language, particularly\nin different cultural contexts. Our findings highlight the need to factor in\npoliteness for cross-cultural natural language processing and LLM usage.", "published": "2024-02-22 13:24:10", "link": "http://arxiv.org/abs/2402.14531v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for\n  GPT-3.5, GPT-4 and Bard", "abstract": "Large Language Models (LLMs) are capable of generating text that is similar\nto or surpasses human quality. However, it is unclear whether LLMs tend to\nexhibit distinctive linguistic styles akin to how human authors do. Through a\ncomprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech\n(POS) distribution, dependency distribution, and sentiment of texts generated\nby three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse\ninputs. The results point to significant linguistic variations which, in turn,\nenable us to attribute a given text to its LLM origin with a favorable 88\\%\naccuracy using a simple off-the-shelf classification model. Theoretical and\npractical implications of this intriguing finding are discussed.", "published": "2024-02-22 13:25:17", "link": "http://arxiv.org/abs/2402.14533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain Generalization via Causal Adjustment for Cross-Domain Sentiment\n  Analysis", "abstract": "Domain adaption has been widely adapted for cross-domain sentiment analysis\nto transfer knowledge from the source domain to the target domain. Whereas,\nmost methods are proposed under the assumption that the target (test) domain is\nknown, making them fail to generalize well on unknown test data that is not\nalways available in practice. In this paper, we focus on the problem of domain\ngeneralization for cross-domain sentiment analysis. Specifically, we propose a\nbackdoor adjustment-based causal model to disentangle the domain-specific and\ndomain-invariant representations that play essential roles in tackling domain\nshift. First, we rethink the cross-domain sentiment analysis task in a causal\nview to model the causal-and-effect relationships among different variables.\nThen, to learn an invariant feature representation, we remove the effect of\ndomain confounders (e.g., domain knowledge) using the backdoor adjustment. A\nseries of experiments over many homologous and diverse datasets show the great\nperformance and robustness of our model by comparing it with the\nstate-of-the-art domain generalization baselines.", "published": "2024-02-22 13:26:56", "link": "http://arxiv.org/abs/2402.14536v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A\n  Survey", "abstract": "Large language models (LLMs) have become the secret ingredient driving\nnumerous industrial applications, showcasing their remarkable versatility\nacross a diverse spectrum of tasks. From natural language processing and\nsentiment analysis to content generation and personalized recommendations,\ntheir unparalleled adaptability has facilitated widespread adoption across\nindustries. This transformative shift driven by LLMs underscores the need to\nexplore the underlying associated challenges and avenues for enhancement in\ntheir utilization. In this paper, our objective is to unravel and evaluate the\nobstacles and opportunities inherent in leveraging LLMs within an industrial\ncontext. To this end, we conduct a survey involving a group of industry\npractitioners, develop four research questions derived from the insights\ngathered, and examine 68 industry papers to address these questions and derive\nmeaningful conclusions.", "published": "2024-02-22 13:52:02", "link": "http://arxiv.org/abs/2402.14558v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named\n  Entity Recognition", "abstract": "Despite the impressive capabilities of large language models (LLMs), their\nperformance on information extraction tasks is still not entirely satisfactory.\nHowever, their remarkable rewriting capabilities and extensive world knowledge\noffer valuable insights to improve these tasks. In this paper, we propose\n$LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot\nNER task. To overcome the limitations of existing data augmentation methods\nthat compromise semantic integrity and address the uncertainty inherent in\nLLM-generated text, we leverage the distinctive characteristics of the NER task\nby augmenting the original data at both the contextual and entity levels. Our\napproach involves employing 14 contextual rewriting strategies, designing\nentity replacements of the same type, and incorporating noise injection to\nenhance robustness. Extensive experiments demonstrate the effectiveness of our\napproach in enhancing NER model performance with limited data. Furthermore,\nadditional analyses provide further evidence supporting the assertion that the\nquality of the data we generate surpasses that of other existing methods.", "published": "2024-02-22 14:19:56", "link": "http://arxiv.org/abs/2402.14568v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Two Counterexamples to Tokenization and the Noiseless Channel", "abstract": "In Tokenization and the Noiseless Channel (Zouhar et al., 2023a), R\\'enyi\nefficiency is suggested as an intrinsic mechanism for evaluating a tokenizer:\nfor NLP tasks, the tokenizer which leads to the highest R\\'enyi efficiency of\nthe unigram distribution should be chosen. The R\\'enyi efficiency is thus\ntreated as a predictor of downstream performance (e.g., predicting BLEU for a\nmachine translation task), without the expensive step of training multiple\nmodels with different tokenizers. Although useful, the predictive power of this\nmetric is not perfect, and the authors note there are additional qualities of a\ngood tokenization scheme that R\\'enyi efficiency alone cannot capture.\n  We describe two variants of BPE tokenization which can arbitrarily increase\nR\\'enyi efficiency while decreasing the downstream model performance. These\ncounterexamples expose cases where R\\'enyi efficiency fails as an intrinsic\ntokenization metric and thus give insight for building more accurate\npredictors.", "published": "2024-02-22 15:03:25", "link": "http://arxiv.org/abs/2402.14614v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Word Splitting on the Semantic Content of Contextualized\n  Word Representations", "abstract": "When deriving contextualized word representations from language models, a\ndecision needs to be made on how to obtain one for out-of-vocabulary (OOV)\nwords that are segmented into subwords. What is the best way to represent these\nwords with a single vector, and are these representations of worse quality than\nthose of in-vocabulary words? We carry out an intrinsic evaluation of\nembeddings from different models on semantic similarity tasks involving OOV\nwords. Our analysis reveals, among other interesting findings, that the quality\nof representations of words that are split is often, but not always, worse than\nthat of the embeddings of known words. Their similarity values, however, must\nbe interpreted with caution.", "published": "2024-02-22 15:04:24", "link": "http://arxiv.org/abs/2402.14616v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cleaner Pretraining Corpus Curation with Neural Web Scraping", "abstract": "The web contains large-scale, diverse, and abundant information to satisfy\nthe information-seeking needs of humans. Through meticulous data collection,\npreprocessing, and curation, webpages can be used as a fundamental data\nresource for language model pretraining. However, when confronted with the\nprogressively revolutionized and intricate nature of webpages,\nrule-based/feature-based web scrapers are becoming increasingly inadequate.\nThis paper presents a simple, fast, and effective Neural web Scraper\n(NeuScraper) to help extract primary and clean text contents from webpages.\nExperimental results show that NeuScraper surpasses the baseline scrapers by\nachieving more than a 20% improvement, demonstrating its potential in\nextracting higher-quality data to facilitate the language model pretraining.\nAll of the code is available at https://github.com/OpenMatch/NeuScraper.", "published": "2024-02-22 16:04:03", "link": "http://arxiv.org/abs/2402.14652v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UFO: a Unified and Flexible Framework for Evaluating Factuality of Large\n  Language Models", "abstract": "Large language models (LLMs) may generate text that lacks consistency with\nhuman knowledge, leading to factual inaccuracies or \\textit{hallucination}.\nExisting research for evaluating the factuality of LLMs involves extracting\nfact claims using an LLM and verifying them against a predefined fact source.\nHowever, these evaluation metrics are task-specific, and not scalable, and the\nsubstitutability of fact sources in different tasks is under-explored. To\naddress these challenges, we categorize four available fact sources:\nhuman-written evidence, reference documents, search engine results, and LLM\nknowledge, along with five text generation tasks containing six representative\ndatasets. Then, we propose \\texttt{UFO}, an LLM-based unified and flexible\nevaluation framework to verify facts against plug-and-play fact sources. We\nimplement five evaluation scenarios based on this framework. Experimental\nresults show that for most QA tasks, human-written evidence and reference\ndocuments are crucial, and they can substitute for each other in\nretrieval-augmented QA tasks. In news fact generation tasks, search engine\nresults and LLM knowledge are essential. Our dataset and code are available at\n\\url{https://github.com/WaldenRUC/UFO}.", "published": "2024-02-22 16:45:32", "link": "http://arxiv.org/abs/2402.14690v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Linguistic Regions in Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated considerable cross-lingual\nalignment and generalization ability. Current research primarily focuses on\nimproving LLMs' cross-lingual generalization capabilities. However, there is\nstill a lack of research on the intrinsic mechanisms of how LLMs achieve\ncross-lingual alignment. From the perspective of region partitioning, this\npaper conducts several investigations on the linguistic competence of LLMs. We\ndiscover a core region in LLMs that corresponds to linguistic competence,\naccounting for approximately 1% of the total model parameters. Removing this\ncore region by setting parameters to zero results in a significant performance\ndecrease across 30 different languages. Furthermore, this core region exhibits\nsignificant dimensional dependence, perturbations to even a single parameter on\nspecific dimensions leading to a loss of linguistic competence. Moreover, we\ndiscover that distinct monolingual regions exist for different languages, and\ndisruption to these specific regions substantially reduces the LLMs'\nproficiency in those corresponding languages. Our research also indicates that\nfreezing the core linguistic region during further pre-training can mitigate\nthe issue of catastrophic forgetting (CF), a common phenomenon observed during\nfurther pre-training of LLMs. Overall, exploring the LLMs' functional regions\nprovides insights into the foundation of their intelligence.", "published": "2024-02-22 16:56:13", "link": "http://arxiv.org/abs/2402.14700v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InfFeed: Influence Functions as a Feedback to Improve the Performance of\n  Subjective Tasks", "abstract": "Recently, influence functions present an apparatus for achieving\nexplainability for deep neural models by quantifying the perturbation of\nindividual train instances that might impact a test prediction. Our objectives\nin this paper are twofold. First we incorporate influence functions as a\nfeedback into the model to improve its performance. Second, in a dataset\nextension exercise, using influence functions to automatically identify data\npoints that have been initially `silver' annotated by some existing method and\nneed to be cross-checked (and corrected) by annotators to improve the model\nperformance. To meet these objectives, in this paper, we introduce InfFeed,\nwhich uses influence functions to compute the influential instances for a\ntarget instance. Toward the first objective, we adjust the label of the target\ninstance based on its influencer(s) label. In doing this, InfFeed outperforms\nthe state-of-the-art baselines (including LLMs) by a maximum macro F1-score\nmargin of almost 4% for hate speech classification, 3.5% for stance\nclassification, and 3% for irony and 2% for sarcasm detection. Toward the\nsecond objective we show that manually re-annotating only those silver\nannotated data points in the extension set that have a negative influence can\nimmensely improve the model performance bringing it very close to the scenario\nwhere all the data points in the extension set have gold labels. This allows\nfor huge reduction of the number of data points that need to be manually\nannotated since out of the silver annotated extension dataset, the influence\nfunction scheme picks up ~1/1000 points that need manual correction.", "published": "2024-02-22 16:59:09", "link": "http://arxiv.org/abs/2402.14702v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An LLM-Enhanced Adversarial Editing System for Lexical Simplification", "abstract": "Lexical Simplification (LS) aims to simplify text at the lexical level.\nExisting methods rely heavily on annotated data, making it challenging to apply\nin low-resource scenarios. In this paper, we propose a novel LS method without\nparallel corpora. This method employs an Adversarial Editing System with\nguidance from a confusion loss and an invariance loss to predict lexical edits\nin the original sentences. Meanwhile, we introduce an innovative LLM-enhanced\nloss to enable the distillation of knowledge from Large Language Models (LLMs)\ninto a small-size LS system. From that, complex words within sentences are\nmasked and a Difficulty-aware Filling module is crafted to replace masked\npositions with simpler words. At last, extensive experimental results and\nanalyses on three benchmark LS datasets demonstrate the effectiveness of our\nproposed method.", "published": "2024-02-22 17:04:30", "link": "http://arxiv.org/abs/2402.14704v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency Annotation of Ottoman Turkish with Multilingual BERT", "abstract": "This study introduces a pretrained large language model-based annotation\nmethodology for the first de dency treebank in Ottoman Turkish. Our\nexperimental results show that, iteratively, i) pseudo-annotating data using a\nmultilingual BERT-based parsing model, ii) manually correcting the\npseudo-annotations, and iii) fine-tuning the parsing model with the corrected\nannotations, we speed up and simplify the challenging dependency annotation\nprocess. The resulting treebank, that will be a part of the Universal\nDependencies (UD) project, will facilitate automated analysis of Ottoman\nTurkish documents, unlocking the linguistic richness embedded in this\nhistorical heritage.", "published": "2024-02-22 17:58:50", "link": "http://arxiv.org/abs/2402.14743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RelayAttention for Efficient Large Language Model Serving with Long\n  System Prompts", "abstract": "A practical large language model (LLM) service may involve a long system\nprompt, which specifies the instructions, examples, and knowledge documents of\nthe task and is reused across requests. However, the long system prompt causes\nthroughput/latency bottlenecks as the cost of generating the next token grows\nw.r.t. the sequence length. This paper aims to improve the efficiency of LLM\nservices that involve long system prompts. Our key observation is that handling\nthese system prompts requires heavily redundant memory accesses in existing\ncausal attention computation algorithms. Specifically, for batched requests,\nthe cached hidden states (\\ie, key-value pairs) of system prompts are\ntransferred from off-chip DRAM to on-chip SRAM multiple times, each\ncorresponding to an individual request. To eliminate such a redundancy, we\npropose RelayAttention, an attention algorithm that allows reading these hidden\nstates from DRAM exactly once for a batch of input tokens. RelayAttention is a\nfree lunch: it maintains the generation quality while requiring no model\nretraining, as it is based on a mathematical reformulation of causal attention.\nWe have observed significant performance improvements to a production-level\nsystem, vLLM, through integration with RelayAttention. The improvements are\neven more profound with longer system prompts.", "published": "2024-02-22 18:58:28", "link": "http://arxiv.org/abs/2402.14808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Simple Averaging: Improving NLP Ensemble Performance with\n  Topological-Data-Analysis-Based Weighting", "abstract": "In machine learning, ensembles are important tools for improving the model\nperformance. In natural language processing specifically, ensembles boost the\nperformance of a method due to multiple large models available in open source.\nHowever, existing approaches mostly rely on simple averaging of predictions by\nensembles with equal weights for each model, ignoring differences in the\nquality and conformity of models. We propose to estimate weights for ensembles\nof NLP models using not only knowledge of their individual performance but also\ntheir similarity to each other. By adopting distance measures based on\nTopological Data Analysis (TDA), we improve our ensemble. The quality improves\nfor both text classification accuracy and relevant uncertainty estimation.", "published": "2024-02-22 00:04:21", "link": "http://arxiv.org/abs/2402.14184v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Assisting in Writing Wikipedia-like Articles From Scratch with Large\n  Language Models", "abstract": "We study how to apply large language models to write grounded and organized\nlong-form articles from scratch, with comparable breadth and depth to Wikipedia\npages. This underexplored problem poses new challenges at the pre-writing\nstage, including how to research the topic and prepare an outline prior to\nwriting. We propose STORM, a writing system for the Synthesis of Topic Outlines\nthrough Retrieval and Multi-perspective Question Asking. STORM models the\npre-writing stage by (1) discovering diverse perspectives in researching the\ngiven topic, (2) simulating conversations where writers carrying different\nperspectives pose questions to a topic expert grounded on trusted Internet\nsources, (3) curating the collected information to create an outline.\n  For evaluation, we curate FreshWiki, a dataset of recent high-quality\nWikipedia articles, and formulate outline assessments to evaluate the\npre-writing stage. We further gather feedback from experienced Wikipedia\neditors. Compared to articles generated by an outline-driven\nretrieval-augmented baseline, more of STORM's articles are deemed to be\norganized (by a 25% absolute increase) and broad in coverage (by 10%). The\nexpert feedback also helps identify new challenges for generating grounded long\narticles, such as source bias transfer and over-association of unrelated facts.", "published": "2024-02-22 01:20:17", "link": "http://arxiv.org/abs/2402.14207v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GATE X-E : A Challenge Set for Gender-Fair Translations from\n  Weakly-Gendered Languages", "abstract": "Neural Machine Translation (NMT) continues to improve in quality and\nadoption, yet the inadvertent perpetuation of gender bias remains a significant\nconcern. Despite numerous studies on gender bias in translations into English\nfrom weakly gendered-languages, there are no benchmarks for evaluating this\nphenomenon or for assessing mitigation strategies. To address this gap, we\nintroduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus,\nthat consists of human translations from Turkish, Hungarian, Finnish, and\nPersian into English. Each translation is accompanied by feminine, masculine,\nand neutral variants. The dataset, which contains between 1250 and 1850\ninstances for each of the four language pairs, features natural sentences with\na wide range of sentence lengths and domains, challenging translation rewriters\non various linguistic phenomena. Additionally, we present a translation gender\nrewriting solution built with GPT-4 and use GATE X-E to evaluate it. We open\nsource our contributions to encourage further research on gender debiasing.", "published": "2024-02-22 04:36:14", "link": "http://arxiv.org/abs/2402.14277v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating the Linguistic Gap with Phonemic Representations for Robust\n  Cross-lingual Transfer", "abstract": "Approaches to improving multilingual language understanding often struggle\nwith significant performance gaps between high-resource and low-resource\nlanguages. While there are efforts to align the languages in a single latent\nspace to mitigate such gaps, how different input-level representations\ninfluence such gaps has not been investigated, particularly with phonemic\ninputs. We hypothesize that the performance gaps are affected by representation\ndiscrepancies between these languages, and revisit the use of phonemic\nrepresentations as a means to mitigate these discrepancies. To demonstrate the\neffectiveness of phonemic representations, we present experiments on three\nrepresentative cross-lingual tasks on 12 languages in total. The results show\nthat phonemic representations exhibit higher similarities between languages\ncompared to orthographic representations, and it consistently outperforms\ngrapheme-based baseline model on languages that are relatively low-resourced.\nWe present quantitative evidence from three cross-lingual tasks that\ndemonstrate the effectiveness of phonemic representations, and it is further\njustified by a theoretical analysis of the cross-lingual performance gap.", "published": "2024-02-22 04:41:52", "link": "http://arxiv.org/abs/2402.14279v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models", "abstract": "We present the TinyLLaVA framework that provides a unified perspective in\ndesigning and analyzing the small-scale Large Multimodal Models (LMMs). We\nempirically study the effects of different vision encoders, connection modules,\nlanguage models, training data and training recipes. Our extensive experiments\nshowed that better quality of data combined with better training recipes,\nsmaller LMMs can consistently achieve on-par performances compared to bigger\nLMMs. Under our framework, we train a family of small-scale LMMs. Our best\nmodel, TinyLLaVA-3.1B, achieves better overall performance against existing 7B\nmodels such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as\nbaselines for future research in terms of data scaling, training setups and\nmodel selections. Our model weights and codes will be made public.", "published": "2024-02-22 05:05:30", "link": "http://arxiv.org/abs/2402.14289v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CEV-LM: Controlled Edit Vector Language Model for Shaping Natural\n  Language Generations", "abstract": "As large-scale language models become the standard for text generation, there\nis a greater need to tailor the generations to be more or less concise,\ntargeted, and informative, depending on the audience/application. Existing\ncontrol approaches primarily adjust the semantic (e.g., emotion, topics),\nstructural (e.g., syntax tree, parts-of-speech), and lexical (e.g.,\nkeyword/phrase inclusion) properties of text, but are insufficient to\naccomplish complex objectives such as pacing which control the complexity and\nreadability of the text. In this paper, we introduce CEV-LM - a lightweight,\nsemi-autoregressive language model that utilizes constrained edit vectors to\ncontrol three complementary metrics (speed, volume, and circuitousness) that\nquantify the shape of text (e.g., pacing of content). We study an extensive set\nof state-of-the-art CTG models and find that CEV-LM provides significantly more\ntargeted and precise control of these three metrics while preserving semantic\ncontent, using less training data, and containing fewer parameters.", "published": "2024-02-22 05:07:31", "link": "http://arxiv.org/abs/2402.14290v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve\n  Knowledge Base Question Answering", "abstract": "Recent progress with LLM-based agents has shown promising results across\nvarious tasks. However, their use in answering questions from knowledge bases\nremains largely unexplored. Implementing a KBQA system using traditional\nmethods is challenging due to the shortage of task-specific training data and\nthe complexity of creating task-focused model structures. In this paper, we\npresent Triad, a unified framework that utilizes an LLM-based agent with three\nroles for KBQA tasks. The agent is assigned three roles to tackle different\nKBQA subtasks: agent as a generalist for mastering various subtasks, as a\ndecision maker for the selection of candidates, and as an advisor for answering\nquestions with knowledge. Our KBQA framework is executed in four phases,\ninvolving the collaboration of the agent's multiple roles. We evaluated the\nperformance of our framework using three benchmark datasets, and the results\nshow that our framework outperforms state-of-the-art systems on the LC-QuAD and\nYAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.", "published": "2024-02-22 06:23:37", "link": "http://arxiv.org/abs/2402.14320v6", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Subobject-level Image Tokenization", "abstract": "Patch-based image tokenization ignores the morphology of the visual world,\nlimiting effective and efficient learning of image understanding. Inspired by\nsubword tokenization, we introduce subobject-level adaptive token segmentation\nand explore several approaches, including superpixel, SAM, and a proposed\nEfficient and PanOptiC (EPOC) image tokenizer. Our EPOC combines boundary\ndetection -- a simple task that can be handled well by a compact model -- with\nwatershed segmentation, which inherently guarantees no pixels are left\nunsegmented. Intrinsic evaluations across 5 datasets demonstrate that EPOC's\nsegmentation aligns well with human annotations of both object- and part-level\nvisual morphology, producing more monosemantic tokens and offering substantial\nefficiency advantages. For extrinsic evaluation, we designed a token embedding\nthat handles arbitrary-shaped tokens, and trained VLMs with different\ntokenizers on 4 datasets of object recognition and detailed captioning. The\nresults reveal that subobject tokenization enables faster convergence and\nbetter generalization while using fewer visual tokens.", "published": "2024-02-22 06:47:44", "link": "http://arxiv.org/abs/2402.14327v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "How Ambiguous Are the Rationales for Natural Language Reasoning? A\n  Simple Approach to Handling Rationale Uncertainty", "abstract": "The quality of rationales is essential in the reasoning capabilities of\nlanguage models. Rationales not only enhance reasoning performance in complex\nnatural language tasks but also justify model decisions. However, obtaining\nimpeccable rationales is often impossible. Our study aims to investigate how\nambiguous rationales play in model performances of natural language reasoning.\nWe first assess the ambiguity of rationales through the lens of entropy and\nuncertainty in model prior beliefs, exploring its impact on task performance.\nWe then propose a simple way to guide models to choose between two different\nreasoning paths depending on the ambiguity of the rationales. Our empirical\nresults demonstrate that this approach leads to robust performance,\nparticularly in adversarial scenarios where rationale quality is inconsistent.", "published": "2024-02-22 07:12:34", "link": "http://arxiv.org/abs/2402.14337v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On the Tip of the Tongue: Analyzing Conceptual Representation in Large\n  Language Models with Reverse-Dictionary Probe", "abstract": "Probing and enhancing large language models' reasoning capacity remains a\ncrucial open question. Here we re-purpose the reverse dictionary task as a case\nstudy to probe LLMs' capacity for conceptual inference. We use in-context\nlearning to guide the models to generate the term for an object concept implied\nin a linguistic description. Models robustly achieve high accuracy in this\ntask, and their representation space encodes information about object\ncategories and fine-grained features. Further experiments suggest that the\nconceptual inference ability as probed by the reverse-dictionary task predicts\nmodel's general reasoning performance across multiple benchmarks, despite\nsimilar syntactic generalization behaviors across models. Explorative analyses\nsuggest that prompting LLMs with description$\\Rightarrow$word examples may\ninduce generalization beyond surface-level differences in task construals and\nfacilitate models on broader commonsense reasoning problems.", "published": "2024-02-22 09:45:26", "link": "http://arxiv.org/abs/2402.14404v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KoCoSa: Korean Context-aware Sarcasm Detection Dataset", "abstract": "Sarcasm is a way of verbal irony where someone says the opposite of what they\nmean, often to ridicule a person, situation, or idea. It is often difficult to\ndetect sarcasm in the dialogue since detecting sarcasm should reflect the\ncontext (i.e., dialogue history). In this paper, we introduce a new dataset for\nthe Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware\nSarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and\nthe labels for this task on the last response. To build the dataset, we propose\nan efficient sarcasm detection dataset generation pipeline: 1) generating new\nsarcastic dialogues from source dialogues with large language models, 2)\nautomatic and manual filtering of abnormal and toxic dialogues, and 3) human\nannotation for the sarcasm detection task. We also provide a simple but\neffective baseline for the Korean sarcasm detection task trained on our\ndataset. Experimental results on the dataset show that our baseline system\noutperforms strong baselines like large language models, such as GPT-3.5, in\nthe Korean sarcasm detection task. We show that the sarcasm detection task\nrelies deeply on the existence of sufficient context. We will release the\ndataset at https://github.com/Yu-billie/KoCoSa_sarcasm_detection.", "published": "2024-02-22 10:17:57", "link": "http://arxiv.org/abs/2402.14428v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Language Model's Guide Through Latent Space", "abstract": "Concept guidance has emerged as a cheap and simple way to control the\nbehavior of language models by probing their hidden representations for concept\nvectors and using them to perturb activations at inference time. While the\nfocus of previous work has largely been on truthfulness, in this paper we\nextend this framework to a richer set of concepts such as appropriateness,\nhumor, creativity and quality, and explore to what degree current detection and\nguidance strategies work in these challenging settings. To facilitate\nevaluation, we develop a novel metric for concept guidance that takes into\naccount both the success of concept elicitation as well as the potential\ndegradation in fluency of the guided model. Our extensive experiments reveal\nthat while some concepts such as truthfulness more easily allow for guidance\nwith current techniques, novel concepts such as appropriateness or humor either\nremain difficult to elicit, need extensive tuning to work, or even experience\nconfusion. Moreover, we find that probes with optimal detection accuracies do\nnot necessarily make for the optimal guides, contradicting previous\nobservations for truthfulness. Our work warrants a deeper investigation into\nthe interplay between detectability, guidability, and the nature of the\nconcept, and we hope that our rich experimental test-bed for guidance research\ninspires stronger follow-up approaches.", "published": "2024-02-22 10:25:14", "link": "http://arxiv.org/abs/2402.14433v1", "categories": ["cs.CL", "cs.AI", "I.2"], "primary_category": "cs.CL"}
{"title": "NLAS-multi: A Multilingual Corpus of Automatically Generated Natural\n  Language Argumentation Schemes", "abstract": "Some of the major limitations identified in the areas of argument mining,\nargument generation, and natural language argument analysis are related to the\ncomplexity of annotating argumentatively rich data, the limited size of these\ncorpora, and the constraints that represent the different languages and domains\nin which these data is annotated. To address these limitations, in this paper\nwe present the following contributions: (i) an effective methodology for the\nautomatic generation of natural language arguments in different topics and\nlanguages, (ii) the largest publicly available corpus of natural language\nargumentation schemes, and (iii) a set of solid baselines and fine-tuned models\nfor the automatic identification of argumentation schemes.", "published": "2024-02-22 11:31:50", "link": "http://arxiv.org/abs/2402.14458v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Data Science with LLMs and Interpretable Models", "abstract": "Recent years have seen important advances in the building of interpretable\nmodels, machine learning models that are designed to be easily understood by\nhumans. In this work, we show that large language models (LLMs) are remarkably\ngood at working with interpretable models, too. In particular, we show that\nLLMs can describe, interpret, and debug Generalized Additive Models (GAMs).\nCombining the flexibility of LLMs with the breadth of statistical patterns\naccurately described by GAMs enables dataset summarization, question answering,\nand model critique. LLMs can also improve the interaction between domain\nexperts and interpretable models, and generate hypotheses about the underlying\nphenomenon. We release \\url{https://github.com/interpretml/TalkToEBM} as an\nopen-source LLM-GAM interface.", "published": "2024-02-22 12:04:15", "link": "http://arxiv.org/abs/2402.14474v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Robust Instruction Tuning on Multimodal Large Language Models", "abstract": "Fine-tuning large language models (LLMs) on multi-task instruction-following\ndata has been proven to be a powerful learning paradigm for improving their\nzero-shot capabilities on new tasks. Recent works about high-quality\ninstruction-following data generation and selection require amounts of human\nlabor to conceive model-understandable instructions for the given tasks and\ncarefully filter the LLM-generated data. In this work, we introduce an\nautomatic instruction augmentation method named INSTRAUG in multimodal tasks.\nIt starts from a handful of basic and straightforward meta instructions but can\nexpand an instruction-following dataset by 30 times. Results on two popular\nmultimodal instructionfollowing benchmarks MULTIINSTRUCT and InstructBLIP show\nthat INSTRAUG can significantly improve the alignment of multimodal large\nlanguage models (MLLMs) across 12 multimodal tasks, which is even equivalent to\nthe benefits of scaling up training data multiple times.", "published": "2024-02-22 12:35:50", "link": "http://arxiv.org/abs/2402.14492v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap\n  for Prompt-Based Large Language Models and Beyond", "abstract": "Task embedding, a meta-learning technique that captures task-specific\ninformation, has gained popularity, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradient-free manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To hardness the\npotential of task embeddings in the era of LLMs, we propose a framework for\nunified task embeddings (FUTE), harmonizing task embeddings from various\nmodels, including smaller language models and LLMs with varied prompts, within\na single vector space. Such uniformity enables comparison and analysis of\nsimilarities amongst different models, broadening the scope and utility of\nexisting task embedding methods in multi-model scenarios, while maintaining\ntheir performance comparable to architecture-specific methods.", "published": "2024-02-22 13:13:31", "link": "http://arxiv.org/abs/2402.14522v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Balanced Data Sampling for Language Model Training with Clustering", "abstract": "Data plays a fundamental role in the training of Large Language Models\n(LLMs). While attention has been paid to the collection and composition of\ndatasets, determining the data sampling strategy in training remains an open\nquestion. Most LLMs are trained with a simple strategy, random sampling.\nHowever, this sampling strategy ignores the unbalanced nature of training data\ndistribution, which can be sub-optimal. In this paper, we propose ClusterClip\nSampling to balance the text distribution of training data for better model\ntraining. Specifically, ClusterClip Sampling utilizes data clustering to\nreflect the data distribution of the training set and balances the common\nsamples and rare samples during training based on the cluster results. A\nrepetition clip operation is introduced to mitigate the overfitting issue led\nby samples from certain clusters. Extensive experiments validate the\neffectiveness of ClusterClip Sampling, which outperforms random sampling and\nother cluster-based sampling variants under various training datasets and large\nlanguage models.", "published": "2024-02-22 13:20:53", "link": "http://arxiv.org/abs/2402.14526v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Less is More: Mitigating Multimodal Hallucination from an EOS Decision\n  Perspective", "abstract": "Large Multimodal Models (LMMs) often suffer from multimodal hallucinations,\nwherein they may create content that is not present in the visual inputs. In\nthis paper, we explore a new angle of this issue: overly detailed training data\nhinders the model's ability to timely terminate generation, leading to\ncontinued outputs beyond visual perception limits. By investigating how the\nmodel decides to terminate generation with EOS, the special end-of-sentence\ntoken, we find that the model assesses the completeness of the entire sequence\nby comparing the generated text with the image. This observation suggests that\nthe model possesses an inherent potential of making proper EOS decisions based\non its visual perception to avoid overly lengthy outputs. To take advantage of\nsuch potential, we explore two methods to mitigate multimodal hallucinations: a\ntraining objective that enables the model to reduce hallucinations by learning\nfrom regular instruction data, and a data filtering strategy to prevent harmful\ntraining data from exacerbating model hallucinations. Both methods\nsignificantly improve the hallucination performance of LMMs, without requiring\nany additional data or knowledge.", "published": "2024-02-22 13:33:13", "link": "http://arxiv.org/abs/2402.14545v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring\n  Mathematical Reasoning of Large Language Models", "abstract": "This paper introduces ConceptMath, a bilingual (English and Chinese),\nfine-grained benchmark that evaluates concept-wise mathematical reasoning of\nLarge Language Models (LLMs). Unlike traditional benchmarks that evaluate\ngeneral mathematical reasoning with an average accuracy, ConceptMath\nsystematically organizes math problems under a hierarchy of math concepts, so\nthat mathematical reasoning can be evaluated at different granularity with\nconcept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range\nof LLMs, and we observe existing LLMs, though achieving high average accuracies\non traditional benchmarks, exhibit significant performance variations across\ndifferent math concepts and may even fail catastrophically on the most basic\nones. Besides, we also introduce an efficient fine-tuning strategy to enhance\nthe weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the\ndevelopers to understand the fine-grained mathematical abilities of their\nmodels and facilitate the growth of foundation models.", "published": "2024-02-22 16:06:49", "link": "http://arxiv.org/abs/2402.14660v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Middleware for LLMs: Tools Are Instrumental for Language Agents in\n  Complex Environments", "abstract": "The applications of large language models (LLMs) have expanded well beyond\nthe confines of text processing, signaling a new era where LLMs are envisioned\nas generalist agents capable of operating within complex environments. These\nenvironments are often highly expansive, making it impossible for the LLM to\nprocess them within its short-term memory. Motivated by recent research on\nextending the capabilities of LLMs with tools, we seek to investigate the\nintriguing potential of tools to augment LLMs in handling such complexity by\nintroducing a novel class of tools, termed middleware, to aid in the proactive\nexploration within these massive environments. Such specialized tools can serve\nas a middleware layer shielding the LLM from environmental complexity. In two\nrepresentative complex environments -- knowledge bases (KBs) and databases --\nwe demonstrate the significant potential of augmenting language agents with\ntools in complex environments. Notably, equipped with the middleware, GPT-4\nachieves 2.8X the performance of the best baseline in tasks requiring access to\ndatabase content and 2.2X in KB tasks. Our findings illuminate the path for\nadvancing language agents in real-world applications.", "published": "2024-02-22 16:18:07", "link": "http://arxiv.org/abs/2402.14672v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Is Self-knowledge and Action Consistent or Not: Investigating Large\n  Language Model's Personality", "abstract": "In this study, we delve into the validity of conventional personality\nquestionnaires in capturing the human-like personality traits of Large Language\nModels (LLMs). Our objective is to assess the congruence between the\npersonality traits LLMs claim to possess and their demonstrated tendencies in\nreal-world scenarios. By conducting an extensive examination of LLM outputs\nagainst observed human response patterns, we aim to understand the disjunction\nbetween self-knowledge and action in LLMs.", "published": "2024-02-22 16:32:08", "link": "http://arxiv.org/abs/2402.14679v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Efficient and Effective Vocabulary Expansion Towards Multilingual Large\n  Language Models", "abstract": "This report introduces \\texttt{EEVE-Korean-v1.0}, a Korean adaptation of\nlarge language models that exhibit remarkable capabilities across English and\nKorean text understanding. Building on recent highly capable but\nEnglish-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts\nare inefficiently processed with English-centric tokenizers, we present an\nefficient and effective vocabulary expansion (EEVE) method, which encompasses\nparameter freezing and subword initialization. In contrast to previous efforts\nthat believe new embeddings require trillions of training tokens, we show that\nour method can significantly boost non-English proficiency within just 2\nbillion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM\nLeaderboard, as of January 2024, our model \\texttt{EEVE-Korean-10.8B-v1.0}\nranks as the leading Korean pre-trained model in the open-source community,\naccording to Hugging Face's leaderboard. We open-source our models on\nHuggingface to empower the open research community in various languages.", "published": "2024-02-22 17:12:39", "link": "http://arxiv.org/abs/2402.14714v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Scaling Efficient LLMs", "abstract": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss to obtain upper and lower bounds on the number of unique\nsequences in a natural training corpus as a function of its size. Our result\nimplies (1) to double the number of skills represented in a training corpus,\nthe corpus must scale more than four fold (2) for efficient LLMs, the number of\nparameters N and the size D of a natural training corpus scale as $N \\propto\nD^{0.44}$; (3) if the number of parameters of an LLM is smaller than the number\nof unique sequences in the training corpus, scaling up can uncover emergent\nskills.", "published": "2024-02-22 18:06:19", "link": "http://arxiv.org/abs/2402.14746v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalizing Reward Modeling for Out-of-Distribution Preference Learning", "abstract": "Preference learning (PL) with large language models (LLMs) aims to align the\nLLMs' generations with human preferences. Previous work on reinforcement\nlearning from human feedback (RLHF) has demonstrated promising results in\nin-distribution PL. However, due to the difficulty of obtaining human feedback,\ndiscretely training reward models for every encountered distribution is\nchallenging. Thus, out-of-distribution (OOD) PL is practically useful for\nenhancing the generalization ability of LLMs with limited preference feedback.\nThis work addresses OOD PL by optimizing a general reward model through a\nmeta-learning approach. During meta-training, a bilevel optimization algorithm\nis utilized to learn a reward model capable of guiding policy learning to align\nwith human preferences across various distributions. When encountering a test\ndistribution, the meta-test procedure conducts regularized policy optimization\nusing the learned reward model for PL. We theoretically demonstrate the\nconvergence rate of the bilevel optimization algorithm under reasonable\nassumptions. Additionally, we conduct experiments on two text generation tasks\nacross 20 held-out domains and outperform a variety of strong baselines across\nvarious evaluation metrics.", "published": "2024-02-22 18:20:33", "link": "http://arxiv.org/abs/2402.14760v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language\n  Models in Multi-Turn Dialogues", "abstract": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities. The data and code are available at\n\\url{https://github.com/mtbench101/mt-bench-101}.", "published": "2024-02-22 18:21:59", "link": "http://arxiv.org/abs/2402.14762v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "2D Matryoshka Sentence Embeddings", "abstract": "Common approaches rely on fixed-length embedding vectors from language models\nas sentence embeddings for downstream tasks such as semantic textual similarity\n(STS). Such methods are limited in their flexibility due to unknown\ncomputational constraints and budgets across various applications. Matryoshka\nRepresentation Learning (MRL) \\cite{aditya2022matryoshka} encodes information\nat finer granularities, i.e., with lower embedding dimensions, to adaptively\naccommodate \\emph{ad hoc} tasks. Similar accuracy can be achieved with a\nsmaller embedding size, leading to speedups in downstream tasks. Despite its\nimproved efficiency, MRL still requires traversing all Transformer layers\nbefore obtaining the embedding, which remains the dominant factor in time and\nmemory consumption. This prompts consideration of whether the fixed number of\nTransformer layers affects representation quality and whether using\nintermediate layers for sentence representation is feasible. In this paper, we\nintroduce a novel sentence embedding model called \\textit{Two-dimensional\nMatryoshka Sentence Embedding} (2DMSE)\\footnote{Our code is available at\n\\url{https://github.com/SeanLee97/AnglE/blob/main/README_2DMSE.md}.}. It\nsupports elastic settings for both embedding sizes and Transformer layers,\noffering greater flexibility and efficiency than MRL. We conduct extensive\nexperiments on STS tasks and downstream applications. The experimental results\ndemonstrate the effectiveness of our proposed model in dynamically supporting\ndifferent embedding sizes and Transformer layers, allowing it to be highly\nadaptable to various scenarios.", "published": "2024-02-22 18:35:05", "link": "http://arxiv.org/abs/2402.14776v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Zero-shot cross-lingual transfer in instruction tuning of large language\n  models", "abstract": "Instruction tuning (IT) is widely used to teach pretrained large language\nmodels (LLMs) to follow arbitrary instructions, but is under-studied in\nmultilingual settings. In this work, we conduct a systematic study of zero-shot\ncross-lingual transfer in IT, when an LLM is instruction-tuned on English-only\ndata and then tested on user prompts in other languages. We advocate for the\nimportance of evaluating various aspects of model responses in multilingual\ninstruction following and investigate the influence of different model\nconfiguration choices. We find that cross-lingual transfer does happen\nsuccessfully in IT even if all stages of model training are English-centric,\nbut only if multiliguality is taken into account in hyperparameter tuning and\nwith large enough IT data. English-trained LLMs are capable of generating\ncorrect-language, comprehensive and helpful responses in other languages, but\nsuffer from low factuality and may occasionally have fluency errors.", "published": "2024-02-22 18:37:33", "link": "http://arxiv.org/abs/2402.14778v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Systematic Decompositional Natural Language Inference Using\n  Informal Logic", "abstract": "Recent language models enable new opportunities for structured reasoning with\ntext, such as the construction of intuitive, proof-like textual entailment\ntrees without relying on brittle formal logic. However, progress in this\ndirection has been hampered by a long-standing lack of a clear protocol for\ndetermining what valid compositional entailment is. This absence causes noisy\ndatasets and limited performance gains by modern neuro-symbolic engines. To\naddress these problems, we formulate a consistent and theoretically grounded\napproach to annotating decompositional entailment and evaluate its impact on\nLLM-based textual inference. We find that our new dataset, RDTE (Recognizing\nDecompositional Textual Entailment), has a substantially higher internal\nconsistency (+9%) than prior decompositional entailment datasets. We also find\nthat training an RDTE-oriented entailment classifier via knowledge distillation\nand employing it in an entailment tree reasoning engine significantly improves\nboth accuracy and proof quality, illustrating the practical benefit of this\nadvance for textual inference.", "published": "2024-02-22 18:55:17", "link": "http://arxiv.org/abs/2402.14798v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying Multiple Personalities in Large Language Models with\n  External Evaluation", "abstract": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs.", "published": "2024-02-22 18:57:20", "link": "http://arxiv.org/abs/2402.14805v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity\n  Tracking", "abstract": "Fine-tuning on generalized tasks such as instruction following, code\ngeneration, and mathematics has been shown to enhance language models'\nperformance on a range of tasks. Nevertheless, explanations of how such\nfine-tuning influences the internal computations in these models remain\nelusive. We study how fine-tuning affects the internal mechanisms implemented\nin language models. As a case study, we explore the property of entity\ntracking, a crucial facet of language comprehension, where models fine-tuned on\nmathematics have substantial performance gains. We identify the mechanism that\nenables entity tracking and show that (i) in both the original model and its\nfine-tuned versions primarily the same circuit implements entity tracking. In\nfact, the entity tracking circuit of the original model on the fine-tuned\nversions performs better than the full original model. (ii) The circuits of all\nthe models implement roughly the same functionality: Entity tracking is\nperformed by tracking the position of the correct entity in both the original\nmodel and its fine-tuned versions. (iii) Performance boost in the fine-tuned\nmodels is primarily attributed to its improved ability to handle the augmented\npositional information. To uncover these findings, we employ: Patch Patching,\nDCM, which automatically detects model components responsible for specific\nsemantics, and CMAP, a new approach for patching activations across models to\nreveal improved mechanisms. Our findings suggest that fine-tuning enhances,\nrather than fundamentally alters, the mechanistic operation of the model.", "published": "2024-02-22 18:59:24", "link": "http://arxiv.org/abs/2402.14811v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PALO: A Polyglot Large Multimodal Model for 5B People", "abstract": "In pursuit of more inclusive Vision-Language Models (VLMs), this study\nintroduces a Large Multilingual Multimodal Model called PALO. PALO offers\nvisual reasoning capabilities in 10 major languages, including English,\nChinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese,\nthat span a total of ~5B people (65% of the world population). Our approach\ninvolves a semi-automated translation approach to adapt the multimodal\ninstruction dataset from English to the target languages using a fine-tuned\nLarge Language Model, thereby ensuring high linguistic fidelity while allowing\nscalability due to minimal manual effort. The incorporation of diverse\ninstruction sets helps us boost overall performance across multiple languages\nespecially those that are underrepresented like Hindi, Arabic, Bengali, and\nUrdu. The resulting models are trained across three scales (1.7B, 7B and 13B\nparameters) to show the generalization and scalability where we observe\nsubstantial improvements compared to strong baselines. We also propose the\nfirst multilingual multimodal benchmark for the forthcoming approaches to\nevaluate their vision-language reasoning capabilities across languages. Code:\nhttps://github.com/mbzuai-oryx/PALO.", "published": "2024-02-22 18:59:58", "link": "http://arxiv.org/abs/2402.14818v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "COBIAS: Contextual Reliability in Bias Assessment", "abstract": "Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augment 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65$, $p = 3.4 *\n10^{-60}$) and can be used to create reliable datasets, which would assist bias\nmitigation works.", "published": "2024-02-22 10:46:11", "link": "http://arxiv.org/abs/2402.14889v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMBind: A Unified Modality-Task Integration Framework", "abstract": "In the multi-modal domain, the dependence of various models on specific input\nformats leads to user confusion and hinders progress. To address this\nchallenge, we introduce \\textbf{LLMBind}, a novel framework designed to unify a\ndiverse array of multi-modal tasks. By harnessing a Mixture-of-Experts (MoE)\nLarge Language Model (LLM), LLMBind processes multi-modal inputs and generates\ntask-specific tokens, enabling the invocation of corresponding models to\naccomplish tasks. This unique approach empowers LLMBind to interpret inputs and\ngenerate outputs across various modalities, including image, text, video, and\naudio. Furthermore, we have constructed an interaction dataset comprising 400k\ninstructions, which unlocks the ability of LLMBind for interactive visual\ngeneration and editing tasks. Extensive experimentation demonstrates that\nLLMBind achieves very superior performance across diverse tasks and outperforms\nexisting models in user evaluations conducted in real-world scenarios.\nMoreover, the adaptability of LLMBind allows for seamless integration with the\nlatest models and extension to new modality tasks, highlighting its potential\nto serve as a unified AI agent for modeling universal modalities.", "published": "2024-02-22 12:36:31", "link": "http://arxiv.org/abs/2402.14891v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Usage-centric Take on Intent Understanding in E-Commerce", "abstract": "Identifying and understanding user intents is a pivotal task for E-Commerce.\nDespite its essential role in product recommendation and business user\nprofiling analysis, intent understanding has not been consistently defined or\naccurately benchmarked. In this paper, we focus on predicative user intents as\n\"how a customer uses a product\", and pose intent understanding as a natural\nlanguage reasoning task, independent of product ontologies. We identify two\nweaknesses of FolkScope, the SOTA E-Commerce Intent Knowledge Graph:\ncategory-rigidity and property-ambiguity. They limit its ability to strongly\nalign user intents with products having the most desirable property, and to\nrecommend useful products across diverse categories. Following these\nobservations, we introduce a Product Recovery Benchmark featuring a novel\nevaluation framework and an example dataset. We further validate the above\nFolkScope weaknesses on this benchmark. Our code and dataset are available at\nhttps://github.com/stayones/Usgae-Centric-Intent-Understanding.", "published": "2024-02-22 18:09:33", "link": "http://arxiv.org/abs/2402.14901v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Tokenization counts: the impact of tokenization on arithmetic in\n  frontier LLMs", "abstract": "Tokenization, the division of input text into input tokens, is an often\noverlooked aspect of the large language model (LLM) pipeline and could be the\nsource of useful or harmful inductive biases. Historically, LLMs have relied on\nbyte pair encoding, without care to specific input domains. With the increased\nuse of LLMs for reasoning, various number-specific tokenization schemes have\nbeen adopted, with popular models like LLaMa and PaLM opting for single-digit\ntokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and\n3-digit numbers. In this work, we study the effect this choice has on numerical\nreasoning through the use of arithmetic tasks. We consider left-to-right and\nright-to-left tokenization for GPT-3.5 and -4, finding that right-to-left\ntokenization (enforced by comma separating numbers at inference time) leads to\nlargely improved performance. Furthermore, we find that model errors when using\nstandard left-to-right tokenization follow stereotyped error patterns,\nsuggesting that model computations are systematic rather than approximate. We\nshow that the model is able to convert between tokenizations easily, thus\nallowing chain-of-thought-inspired approaches to recover performance on\nleft-to-right tokenized inputs. We also find the gap between tokenization\ndirections decreases when models are scaled, possibly indicating that larger\nmodels are better able to override this tokenization-dependent inductive bias.\nIn summary, our work performs the first study of how number tokenization\nchoices lead to differences in model performance on arithmetic tasks,\naccompanied by a thorough analysis of error patterns. We hope this work\ninspires practitioners to more carefully ablate number tokenization-related\nchoices when working towards general models of numerical reasoning.", "published": "2024-02-22 18:14:09", "link": "http://arxiv.org/abs/2402.14903v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Re-Examine Distantly Supervised NER: A New Benchmark and a Simple\n  Approach", "abstract": "Distantly-Supervised Named Entity Recognition (DS-NER) uses knowledge bases\nor dictionaries for annotations, reducing manual efforts but rely on large\nhuman labeled validation set. In this paper, we introduce a real-life DS-NER\ndataset, QTL, where the training data is annotated using domain dictionaries\nand the test data is annotated by domain experts. This dataset has a small\nvalidation set, reflecting real-life scenarios. Existing DS-NER approaches fail\nwhen applied to QTL, which motivate us to re-examine existing DS-NER\napproaches. We found that many of them rely on large validation sets and some\nused test set for tuning inappropriately. To solve this issue, we proposed a\nnew approach, token-level Curriculum-based Positive-Unlabeled Learning (CuPUL),\nwhich uses curriculum learning to order training samples from easy to hard.\nThis method stabilizes training, making it robust and effective on small\nvalidation sets. CuPUL also addresses false negative issues using the\nPositive-Unlabeled learning paradigm, demonstrating improved performance in\nreal-life applications.", "published": "2024-02-22 20:07:02", "link": "http://arxiv.org/abs/2402.14948v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich\n  Reasoning", "abstract": "While Large language models (LLMs) have the capability to iteratively reflect\non their own outputs, recent studies have observed their struggles with\nknowledge-rich problems without access to external resources. In addition to\nthe inefficiency of LLMs in self-assessment, we also observe that LLMs struggle\nto revisit their predictions despite receiving explicit negative feedback.\nTherefore, We propose Mirror, a Multiple-perspective self-reflection method for\nknowledge-rich reasoning, to avoid getting stuck at a particular reflection\niteration. Mirror enables LLMs to reflect from multiple-perspective clues,\nachieved through a heuristic interaction between a Navigator and a Reasoner. It\nguides agents toward diverse yet plausibly reliable reasoning trajectory\nwithout access to ground truth by encouraging (1) diversity of directions\ngenerated by Navigator and (2) agreement among strategically induced\nperturbations in responses generated by the Reasoner. The experiments on five\nreasoning datasets demonstrate that Mirror's superiority over several\ncontemporary self-reflection approaches. Additionally, the ablation study\nstudies clearly indicate that our strategies alleviate the aforementioned\nchallenges.", "published": "2024-02-22 20:57:17", "link": "http://arxiv.org/abs/2402.14963v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced\n  Safety Alignment", "abstract": "Despite the general capabilities of Large Language Models (LLM), these models\nstill request fine-tuning or adaptation with customized data when meeting\nspecific business demands. However, this process inevitably introduces new\nthreats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack)\nunder the setting of Language-Model-as-a-Service (LMaaS), where the model's\nsafety has been significantly compromised by fine-tuning users' uploaded\nexamples contain just a few harmful examples. Though potential defenses have\nbeen proposed that the service providers can integrate safety examples into the\nfine-tuning dataset to reduce safety issues, such approaches require\nincorporating a substantial amount of data, making it inefficient. To\neffectively defend against the FJAttack with limited safety examples under\nLMaaS, we propose the Backdoor Enhanced Safety Alignment method inspired by an\nanalogy with the concept of backdoor attacks. In particular, service providers\nwill construct prefixed safety examples with a secret prompt, acting as a\n\"backdoor trigger\". By integrating prefixed safety examples into the\nfine-tuning dataset, the subsequent fine-tuning process effectively acts as the\n\"backdoor attack\", establishing a strong correlation between the secret prompt\nand safety generations. Consequently, safe responses are ensured once service\nproviders prepend this secret prompt ahead of any user input during inference.\nOur comprehensive experiments demonstrate that through the Backdoor Enhanced\nSafety Alignment with adding as few as 11 prefixed safety examples, the\nmaliciously fine-tuned LLMs will achieve similar safety performance as the\noriginal aligned models without harming the benign performance. Furthermore, we\nalso present the effectiveness of our method in a more practical setting where\nthe fine-tuning data consists of both FJAttack examples and the fine-tuning\ntask data.", "published": "2024-02-22 21:05:18", "link": "http://arxiv.org/abs/2402.14968v3", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "MultiLS: A Multi-task Lexical Simplification Framework", "abstract": "Lexical Simplification (LS) automatically replaces difficult to read words\nfor easier alternatives while preserving a sentence's original meaning. LS is a\nprecursor to Text Simplification with the aim of improving text accessibility\nto various target demographics, including children, second language learners,\nindividuals with reading disabilities or low literacy. Several datasets exist\nfor LS. These LS datasets specialize on one or two sub-tasks within the LS\npipeline. However, as of this moment, no single LS dataset has been developed\nthat covers all LS sub-tasks. We present MultiLS, the first LS framework that\nallows for the creation of a multi-task LS dataset. We also present MultiLS-PT,\nthe first dataset to be created using the MultiLS framework. We demonstrate the\npotential of MultiLS-PT by carrying out all LS sub-tasks of (1). lexical\ncomplexity prediction (LCP), (2). substitute generation, and (3). substitute\nranking for Portuguese. Model performances are reported, ranging from\ntransformer-based models to more recent large language models (LLMs).", "published": "2024-02-22 21:16:18", "link": "http://arxiv.org/abs/2402.14972v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?", "abstract": "Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.", "published": "2024-02-22 22:28:46", "link": "http://arxiv.org/abs/2402.15000v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CommVQA: Situating Visual Question Answering in Communicative Contexts", "abstract": "Current visual question answering (VQA) models tend to be trained and\nevaluated on image-question pairs in isolation. However, the questions people\nask are dependent on their informational needs and prior knowledge about the\nimage content. To evaluate how situating images within naturalistic contexts\nshapes visual questions, we introduce CommVQA, a VQA dataset consisting of\nimages, image descriptions, real-world communicative scenarios where the image\nmight appear (e.g., a travel website), and follow-up questions and answers\nconditioned on the scenario and description. CommVQA, which contains 1000\nimages and 8,949 question-answer pairs, poses a challenge for current models.\nError analyses and a human-subjects study suggest that generated answers still\ncontain high rates of hallucinations, fail to fittingly address unanswerable\nquestions, and don't suitably reflect contextual information. Overall, we show\nthat access to contextual information is essential for solving CommVQA, leading\nto the highest performing VQA model and highlighting the relevance of situating\nsystems within communicative scenarios.", "published": "2024-02-22 22:31:39", "link": "http://arxiv.org/abs/2402.15002v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Ar-Spider: Text-to-SQL in Arabic", "abstract": "In Natural Language Processing (NLP), one of the most important tasks is\ntext-to-SQL semantic parsing, which focuses on enabling users to interact with\nthe database in a more natural manner. In recent years, text-to-SQL has made\nsignificant progress, but most were English-centric. In this paper, we\nintroduce Ar-Spider 1, the first Arabic cross-domain text-to-SQL dataset. Due\nto the unique nature of the language, two major challenges have been\nencountered, namely schema linguistic and SQL structural challenges. In order\nto handle these issues and conduct the experiments, we adopt two baseline\nmodels LGESQL [4] and S2SQL [12], both of which are tested with two\ncross-lingual models to alleviate the effects of schema linguistic and SQL\nstructure linking challenges. The baselines demonstrate decent single-language\nperformance on our Arabic text-to-SQL dataset, Ar-Spider, achieving 62.48% for\nS2SQL and 65.57% for LGESQL, only 8.79% below the highest results achieved by\nthe baselines when trained in English dataset. To achieve better performance on\nArabic text-to-SQL, we propose the context similarity relationship (CSR)\napproach, which results in a significant increase in the overall performance of\nabout 1.52% for S2SQL and 1.06% for LGESQL and closes the gap between Arabic\nand English languages to 7.73%.", "published": "2024-02-22 23:11:17", "link": "http://arxiv.org/abs/2402.15012v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Probabilistically-Sound Beam Search with Masked Language Models", "abstract": "Beam search with masked language models (MLMs) is challenging in part because\njoint probability distributions over sequences are not readily available,\nunlike for autoregressive models. However, estimating such distributions has\nimportant domain-specific applications such as ancient text restoration and\nprotein engineering. Here we present probabilistically-sound methods for beam\nsearch with MLMs. First, we clarify the conditions under which it is\ntheoretically sound to perform text infilling with MLMs using standard beam\nsearch. When these conditions fail, we provide a probabilistically-sound\ninference time modification with no additional computational complexity and\ndemonstrate that it is superior to the aforementioned beam search in the\nexpected conditions. We then present empirical results comparing several\ninfilling approaches with MLMs across several domains. Notably, our method\nprobes the inductive biases of MLMs and explores the surprising contextual\nsensitivity of mask tokens for text infilling.", "published": "2024-02-22 23:36:26", "link": "http://arxiv.org/abs/2402.15020v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language\n  Models", "abstract": "Recent years have witnessed a significant increase in the performance of\nVision and Language tasks. Foundational Vision-Language Models (VLMs), such as\nCLIP, have been leveraged in multiple settings and demonstrated remarkable\nperformance across several tasks. Such models excel at object-centric\nrecognition yet learn text representations that seem invariant to word order,\nfailing to compose known concepts in novel ways. However, no evidence exists\nthat any VLM, including large-scale single-stream models such as GPT-4V,\nidentifies compositions successfully. In this paper, we introduce a framework\nto significantly improve the ability of existing models to encode compositional\nlanguage, with over 10% absolute improvement on compositionality benchmarks,\nwhile maintaining or improving the performance on standard object-recognition\nand retrieval benchmarks. Our code and pre-trained models are publicly\navailable at https://github.com/netflix/clove.", "published": "2024-02-22 23:42:25", "link": "http://arxiv.org/abs/2402.15021v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Adoption to Adaption: Tracing the Diffusion of New Emojis on\n  Twitter", "abstract": "In the rapidly evolving landscape of social media, the introduction of new\nemojis in Unicode release versions presents a structured opportunity to explore\ndigital language evolution. Analyzing a large dataset of sampled English\ntweets, we examine how newly released emojis gain traction and evolve in\nmeaning. We find that community size of early adopters and emoji semantics are\ncrucial in determining their popularity. Certain emojis experienced notable\nshifts in the meanings and sentiment associations during the diffusion process.\nAdditionally, we propose a novel framework utilizing language models to extract\nwords and pre-existing emojis with semantically similar contexts, which\nenhances interpretation of new emojis. The framework demonstrates its\neffectiveness in improving sentiment classification performance by substituting\nunknown new emojis with familiar ones. This study offers a new perspective in\nunderstanding how new language units are adopted, adapted, and integrated into\nthe fabric of online communication.", "published": "2024-02-22 00:24:44", "link": "http://arxiv.org/abs/2402.14187v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "LLM-Assisted Content Conditional Debiasing for Fair Text Embedding", "abstract": "Mitigating biases in machine learning models has become an increasing concern\nin Natural Language Processing (NLP), particularly in developing fair text\nembeddings, which are crucial yet challenging for real-world applications like\nsearch engines. In response, this paper proposes a novel method for learning\nfair text embeddings. First, we define a novel content-conditional equal\ndistance (CCED) fairness for text embeddings, ensuring content-conditional\nindependence between sensitive attributes and text embeddings. Building on\nCCED, we introduce a content-conditional debiasing (CCD) loss to ensure that\nembeddings of texts with different sensitive attributes but identical content\nmaintain the same distance from the embedding of their corresponding neutral\ntext. Additionally, we tackle the issue of insufficient training data by using\nLarge Language Models (LLMs) with instructions to fairly augment texts into\ndifferent sensitive groups. Our extensive evaluations show that our approach\neffectively enhances fairness while maintaining the utility of embeddings.\nFurthermore, our augmented dataset, combined with the CCED metric, serves as an\nnew benchmark for evaluating fairness.", "published": "2024-02-22 01:20:51", "link": "http://arxiv.org/abs/2402.14208v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form\n  Medical Question Answering Applications and Beyond", "abstract": "Uncertainty estimation is crucial for the reliability of safety-critical\nhuman and artificial intelligence (AI) interaction systems, particularly in the\ndomain of healthcare engineering. However, a robust and general uncertainty\nmeasure for free-form answers has not been well-established in open-ended\nmedical question-answering (QA) tasks, where generative inequality introduces a\nlarge number of irrelevant words and sequences within the generated set for\nuncertainty quantification (UQ), which can lead to biases. This paper\nintroduces Word-Sequence Entropy (WSE), a method that calibrates uncertainty at\nboth the word and sequence levels, considering semantic relevance. WSE\nquantifies uncertainty in a way that is more closely aligned with the\nreliability of LLMs during uncertainty quantification (UQ). We compare WSE with\nsix baseline methods on five free-form medical QA datasets, utilizing seven\npopular large language models (LLMs). Experimental results demonstrate that WSE\nexhibits superior performance in UQ under two standard criteria for correctness\nevaluation. Additionally, in terms of real-world medical QA applications, the\nperformance of LLMs is significantly enhanced (e.g., a 6.36% improvement in\nmodel accuracy on the COVID-QA dataset) by employing responses with lower\nuncertainty that are identified by WSE as final answers, without any additional\ntask-specific fine-tuning or architectural modifications.", "published": "2024-02-22 03:46:08", "link": "http://arxiv.org/abs/2402.14259v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Detect Misinformation in Scientific News\n  Reporting?", "abstract": "Scientific facts are often spun in the popular press with the intent to\ninfluence public opinion and action, as was evidenced during the COVID-19\npandemic. Automatic detection of misinformation in the scientific domain is\nchallenging because of the distinct styles of writing in these two media types\nand is still in its nascence. Most research on the validity of scientific\nreporting treats this problem as a claim verification challenge. In doing so,\nsignificant expert human effort is required to generate appropriate claims. Our\nsolution bypasses this step and addresses a more real-world scenario where such\nexplicit, labeled claims may not be available. The central research question of\nthis paper is whether it is possible to use large language models (LLMs) to\ndetect misinformation in scientific reporting. To this end, we first present a\nnew labeled dataset SciNews, containing 2.4k scientific news stories drawn from\ntrusted and untrustworthy sources, paired with related abstracts from the\nCORD-19 database. Our dataset includes both human-written and LLM-generated\nnews articles, making it more comprehensive in terms of capturing the growing\ntrend of using LLMs to generate popular press articles. Then, we identify\ndimensions of scientific validity in science news articles and explore how this\ncan be integrated into the automated detection of scientific misinformation. We\npropose several baseline architectures using LLMs to automatically detect false\nrepresentations of scientific findings in the popular press. For each of these\narchitectures, we use several prompt engineering strategies including\nzero-shot, few-shot, and chain-of-thought prompting. We also test these\narchitectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,\nLlama2-13B.", "published": "2024-02-22 04:07:00", "link": "http://arxiv.org/abs/2402.14268v1", "categories": ["cs.CL", "cs.AI", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Tug-of-War Between Knowledge: Exploring and Resolving Knowledge\n  Conflicts in Retrieval-Augmented Language Models", "abstract": "Retrieval-augmented language models (RALMs) have demonstrated significant\npotential in refining and expanding their internal memory by retrieving\nevidence from external sources. However, RALMs will inevitably encounter\nknowledge conflicts when integrating their internal memory with external\nsources. Knowledge conflicts can ensnare RALMs in a tug-of-war between\nknowledge, limiting their practical applicability. In this paper, we focus on\nexploring and resolving knowledge conflicts in RALMs. First, we present an\nevaluation framework for assessing knowledge conflicts across various\ndimensions. Then, we investigate the behavior and preference of RALMs from the\nfollowing two perspectives: (1) Conflicts between internal memory and external\nsources: We find that stronger RALMs emerge with the Dunning-Kruger effect,\npersistently favoring their faulty internal memory even when correct evidence\nis provided. Besides, RALMs exhibit an availability bias towards common\nknowledge; (2) Conflicts between truthful, irrelevant and misleading evidence:\nWe reveal that RALMs follow the principle of majority rule, leaning towards\nplacing trust in evidence that appears more frequently. Moreover, we find that\nRALMs exhibit confirmation bias, and are more willing to choose evidence that\nis consistent with their internal memory. To solve the challenge of knowledge\nconflicts, we propose a method called Conflict-Disentangle Contrastive Decoding\n(CD2) to better calibrate the model's confidence. Experimental results\ndemonstrate that our CD2 can effectively resolve knowledge conflicts in RALMs.", "published": "2024-02-22 09:51:08", "link": "http://arxiv.org/abs/2402.14409v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Text me the data: Generating Ground Pressure Sequence from Textual\n  Descriptions for HAR", "abstract": "In human activity recognition (HAR), the availability of substantial ground\ntruth is necessary for training efficient models. However, acquiring ground\npressure data through physical sensors itself can be cost-prohibitive,\ntime-consuming. To address this critical need, we introduce Text-to-Pressure\n(T2P), a framework designed to generate extensive ground pressure sequences\nfrom textual descriptions of human activities using deep learning techniques.\nWe show that the combination of vector quantization of sensor data along with\nsimple text conditioned auto regressive strategy allows us to obtain\nhigh-quality generated pressure sequences from textual descriptions with the\nhelp of discrete latent correlation between text and pressure maps. We achieved\ncomparable performance on the consistency between text and generated motion\nwith an R squared value of 0.722, Masked R squared value of 0.892, and FID\nscore of 1.83. Additionally, we trained a HAR model with the the synthesized\ndata and evaluated it on pressure dynamics collected by a real pressure sensor\nwhich is on par with a model trained on only real data. Combining both real and\nsynthesized training data increases the overall macro F1 score by 5.9 percent.", "published": "2024-02-22 10:14:59", "link": "http://arxiv.org/abs/2402.14427v1", "categories": ["cs.LG", "cs.CL", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding\n  Decomposition", "abstract": "We often verbally express emotions in a multifaceted manner, they may vary in\ntheir intensities and may be expressed not just as a single but as a mixture of\nemotions. This wide spectrum of emotions is well-studied in the structural\nmodel of emotions, which represents variety of emotions as derivative products\nof primary emotions with varying degrees of intensity. In this paper, we\npropose an emotional text-to-speech design to simulate a wider spectrum of\nemotions grounded on the structural model. Our proposed design, Daisy-TTS,\nincorporates a prosody encoder to learn emotionally-separable prosody embedding\nas a proxy for emotion. This emotion representation allows the model to\nsimulate: (1) Primary emotions, as learned from the training samples, (2)\nSecondary emotions, as a mixture of primary emotions, (3) Intensity-level, by\nscaling the emotion embedding, and (4) Emotions polarity, by negating the\nemotion embedding. Through a series of perceptual evaluations, Daisy-TTS\ndemonstrated overall higher emotional speech naturalness and emotion\nperceiveability compared to the baseline.", "published": "2024-02-22 13:15:49", "link": "http://arxiv.org/abs/2402.14523v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "OmniPred: Language Models as Universal Regressors", "abstract": "Regression is a powerful tool to accurately predict the outcome metric of a\nsystem given a set of parameters, but has traditionally been restricted to\nmethods which are only applicable to a specific task. In this paper, we propose\nOmniPred, a framework for training language models as universal end-to-end\nregressors over $(x,y)$ data from arbitrary formats. Using data sourced from\nGoogle Vizier, one of the largest proprietary blackbox optimization databases\nin the world, our extensive experiments demonstrate that language models are\ncapable of very precise numerical regression using only textual representations\nof mathematical parameters and values, and if given the opportunity to train at\nscale over multiple tasks, can significantly outperform traditional regression\nmodels.", "published": "2024-02-22 13:36:53", "link": "http://arxiv.org/abs/2402.14547v6", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "primary_category": "cs.LG"}
{"title": "From Keywords to Structured Summaries: Streamlining Scholarly\n  Information Access", "abstract": "This paper highlights the growing importance of information retrieval (IR)\nengines in the scientific community, addressing the inefficiency of traditional\nkeyword-based search engines due to the rising volume of publications. The\nproposed solution involves structured records, underpinning advanced\ninformation technology (IT) tools, including visualization dashboards, to\nrevolutionize how researchers access and filter articles, replacing the\ntraditional text-heavy approach. This vision is exemplified through a proof of\nconcept centered on the \"reproductive number estimate of infectious diseases\"\nresearch theme, using a fine-tuned large language model (LLM) to automate the\ncreation of structured records to populate a backend database that now goes\nbeyond keywords. The result is a next-generation information access system as\nan IR method accessible at https://orkg.org/usecases/r0-estimates.", "published": "2024-02-22 15:10:45", "link": "http://arxiv.org/abs/2402.14622v2", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "RoboScript: Code Generation for Free-Form Manipulation Tasks across Real\n  and Simulation", "abstract": "Rapid progress in high-level task planning and code generation for open-world\nrobot manipulation has been witnessed in Embodied AI. However, previous studies\nput much effort into general common sense reasoning and task planning\ncapabilities of large-scale language or multi-modal models, relatively little\neffort on ensuring the deployability of generated code on real robots, and\nother fundamental components of autonomous robot systems including robot\nperception, motion planning, and control. To bridge this ``ideal-to-real'' gap,\nthis paper presents \\textbf{RobotScript}, a platform for 1) a deployable robot\nmanipulation pipeline powered by code generation; and 2) a code generation\nbenchmark for robot manipulation tasks in free-form natural language. The\nRobotScript platform addresses this gap by emphasizing the unified interface\nwith both simulation and real robots, based on abstraction from the Robot\nOperating System (ROS), ensuring syntax compliance and simulation validation\nwith Gazebo. We demonstrate the adaptability of our code generation framework\nacross multiple robot embodiments, including the Franka and UR5 robot arms, and\nmultiple grippers. Additionally, our benchmark assesses reasoning abilities for\nphysical space and constraints, highlighting the differences between GPT-3.5,\nGPT-4, and Gemini in handling complex physical interactions. Finally, we\npresent a thorough evaluation on the whole system, exploring how each module in\nthe pipeline: code generation, perception, motion planning, and even object\ngeometric properties, impact the overall performance of the system.", "published": "2024-02-22 15:12:00", "link": "http://arxiv.org/abs/2402.14623v1", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "I.2.7; I.2.8; I.2.9; I.2.10"], "primary_category": "cs.RO"}
{"title": "OpenCodeInterpreter: Integrating Code Generation with Execution and\n  Refinement", "abstract": "The introduction of large language models has significantly advanced code\ngeneration. However, open-source models often lack the execution capabilities\nand iterative refinement of advanced systems like the GPT-4 Code Interpreter.\nTo address this, we introduce OpenCodeInterpreter, a family of open-source code\nsystems designed for generating, executing, and iteratively refining code.\nSupported by Code-Feedback, a dataset featuring 68K multi-turn interactions,\nOpenCodeInterpreter integrates execution and human feedback for dynamic code\nrefinement. Our comprehensive evaluation of OpenCodeInterpreter across key\nbenchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus\nreveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves\nan accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and\nMBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6)\nwith synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap\nbetween open-source code generation models and proprietary systems like GPT-4\nCode Interpreter.", "published": "2024-02-22 16:06:23", "link": "http://arxiv.org/abs/2402.14658v3", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Large Language Models as Urban Residents: An LLM Agent Framework for\n  Personal Mobility Generation", "abstract": "This paper introduces a novel approach using Large Language Models (LLMs)\nintegrated into an agent framework for flexible and effective personal mobility\ngeneration. LLMs overcome the limitations of previous models by effectively\nprocessing semantic data and offering versatility in modeling various tasks.\nOur approach addresses three research questions: aligning LLMs with real-world\nurban mobility data, developing reliable activity generation strategies, and\nexploring LLM applications in urban mobility. The key technical contribution is\na novel LLM agent framework that accounts for individual activity patterns and\nmotivations, including a self-consistency approach to align LLMs with\nreal-world activity data and a retrieval-augmented strategy for interpretable\nactivity generation. We evaluate our LLM agent framework and compare it with\nstate-of-the-art personal mobility generation approaches, demonstrating the\neffectiveness of our approach and its potential applications in urban mobility.\nOverall, this study marks the pioneering work of designing an LLM agent\nframework for activity generation based on real-world human activity data,\noffering a promising tool for urban mobility analysis.", "published": "2024-02-22 18:03:14", "link": "http://arxiv.org/abs/2402.14744v3", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for\n  Mixture-of-Experts Large Language Models", "abstract": "A pivotal advancement in the progress of large language models (LLMs) is the\nemergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,\nMoE LLMs can achieve higher performance with fewer parameters, but it is still\nhard to deploy them due to their immense parameter sizes. Different from\nprevious weight pruning methods that rely on specifically designed hardware,\nthis paper mainly aims to enhance the deployment efficiency of MoE LLMs by\nintroducing plug-and-play expert-level sparsification techniques. Specifically,\nwe propose, for the first time to our best knowledge, post-training approaches\nfor task-agnostic and task-specific expert pruning and skipping of MoE LLMs,\ntailored to improve deployment efficiency while maintaining model performance\nacross a wide range of tasks. Extensive experiments show that our proposed\nmethods can simultaneously reduce model sizes and increase the inference speed,\nwhile maintaining satisfactory performance. Data and code will be available at\nhttps://github.com/Lucky-Lance/Expert_Sparsity.", "published": "2024-02-22 18:56:07", "link": "http://arxiv.org/abs/2402.14800v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning", "abstract": "The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.", "published": "2024-02-22 18:59:02", "link": "http://arxiv.org/abs/2402.14809v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient data selection employing Semantic Similarity-based Graph\n  Structures for model training", "abstract": "Recent developments in natural language processing (NLP) have highlighted the\nneed for substantial amounts of data for models to capture textual information\naccurately. This raises concerns regarding the computational resources and time\nrequired for training such models. This paper introduces Semantics for data\nSAliency in Model performance Estimation (SeSaME). It is an efficient data\nsampling mechanism solely based on textual information without passing the data\nthrough a compute-heavy model or other intensive pre-processing\ntransformations. The application of this approach is demonstrated in the use\ncase of low-resource automated speech recognition (ASR) models, which\nexcessively rely on text-to-speech (TTS) calls when using augmented data.\nSeSaME learns to categorize new incoming data points into speech recognition\ndifficulty buckets by employing semantic similarity-based graph structures and\ndiscrete ASR information from homophilous neighbourhoods through message\npassing. The results indicate reliable projections of ASR performance, with a\n93% accuracy increase when using the proposed method compared to random\npredictions, bringing non-trivial information on the impact of textual\nrepresentations in speech models. Furthermore, a series of experiments show\nboth the benefits and challenges of using the ASR information on incoming data\nto fine-tune the model. We report a 7% drop in validation loss compared to\nrandom sampling, 7% WER drop with non-local aggregation when evaluating against\na highly difficult dataset, and 1.8% WER drop with local aggregation and high\nsemantic similarity between datasets.", "published": "2024-02-22 09:43:53", "link": "http://arxiv.org/abs/2402.14888v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Vygotsky Distance: Measure for Benchmark Task Similarity", "abstract": "Evaluation plays a significant role in modern natural language processing.\nMost modern NLP benchmarks consist of arbitrary sets of tasks that neither\nguarantee any generalization potential for the model once applied outside the\ntest set nor try to minimize the resource consumption needed for model\nevaluation. This paper presents a theoretical instrument and a practical\nalgorithm to calculate similarity between benchmark tasks, we call this\nsimilarity measure \"Vygotsky distance\". The core idea of this similarity\nmeasure is that it is based on relative performance of the \"students\" on a\ngiven task, rather that on the properties of the task itself. If two tasks are\nclose to each other in terms of Vygotsky distance the models tend to have\nsimilar relative performance on them. Thus knowing Vygotsky distance between\ntasks one can significantly reduce the number of evaluation tasks while\nmaintaining a high validation quality. Experiments on various benchmarks,\nincluding GLUE, SuperGLUE, CLUE, and RussianSuperGLUE, demonstrate that a vast\nmajority of NLP benchmarks could be at least 40% smaller in terms of the tasks\nincluded. Most importantly, Vygotsky distance could also be used for the\nvalidation of new tasks thus increasing the generalization potential of the\nfuture NLP models.", "published": "2024-02-22 12:00:32", "link": "http://arxiv.org/abs/2402.14890v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T01, 97P80, 97C30, 68Q32", "H.1.1; I.2.4; I.2.6; F.2.0"], "primary_category": "cs.CL"}
{"title": "On Evaluation Protocols for Data Augmentation in a Limited Data Scenario", "abstract": "Textual data augmentation (DA) is a prolific field of study where novel\ntechniques to create artificial data are regularly proposed, and that has\ndemonstrated great efficiency on small data settings, at least for text\nclassification tasks. In this paper, we challenge those results, showing that\nclassical data augmentation (which modify sentences) is simply a way of\nperforming better fine-tuning, and that spending more time doing so before\napplying data augmentation negates its effect. This is a significant\ncontribution as it answers several questions that were left open in recent\nyears, namely~: which DA technique performs best (all of them as long as they\ngenerate data close enough to the training set, as to not impair training) and\nwhy did DA show positive results (facilitates training of network). We further\nshow that zero- and few-shot DA via conversational agents such as ChatGPT or\nLLama2 can increase performances, confirming that this form of data\naugmentation is preferable to classical methods.", "published": "2024-02-22 16:42:37", "link": "http://arxiv.org/abs/2402.14895v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy", "abstract": "Understanding the extent to which Chain-of-Thought (CoT) generations align\nwith a large language model's (LLM) internal computations is critical for\ndeciding whether to trust an LLM's output. As a proxy for CoT faithfulness,\nLanham et al. (2023) propose a metric that measures a model's dependence on its\nCoT for producing an answer. Within a single family of proprietary models, they\nfind that LLMs exhibit a scaling-then-inverse-scaling relationship between\nmodel size and their measure of faithfulness, and that a 13 billion parameter\nmodel exhibits increased faithfulness compared to models ranging from 810\nmillion to 175 billion parameters in size. We evaluate whether these results\ngeneralize as a property of all LLMs. We replicate the experimental setup in\ntheir section focused on scaling experiments with three different families of\nmodels and, under specific conditions, successfully reproduce the scaling\ntrends for CoT faithfulness they report. However, after normalizing the metric\nto account for a model's bias toward certain answer choices, unfaithfulness\ndrops significantly for smaller less-capable models. This normalized\nfaithfulness metric is also strongly correlated ($R^2$=0.74) with accuracy,\nraising doubts about its validity for evaluating faithfulness.", "published": "2024-02-22 17:23:53", "link": "http://arxiv.org/abs/2402.14897v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Watermarking Makes Language Models Radioactive", "abstract": "We investigate the radioactivity of text generated by large language models\n(LLM), i.e. whether it is possible to detect that such synthetic input was used\nto train a subsequent LLM. Current methods like membership inference or active\nIP protection either work only in settings where the suspected text is known or\ndo not provide reliable statistical guarantees. We discover that, on the\ncontrary, it is possible to reliably determine if a language model was trained\non synthetic data if that data is output by a watermarked LLM. Our new methods,\nspecialized for radioactivity, detects with a provable confidence weak\nresiduals of the watermark signal in the fine-tuned LLM. We link the\nradioactivity contamination level to the following properties: the watermark\nrobustness, its proportion in the training set, and the fine-tuning process.\nFor instance, if the suspect model is open-weight, we demonstrate that training\non watermarked instructions can be detected with high confidence ($p$-value $<\n10^{-5}$) even when as little as $5\\%$ of training text is watermarked.", "published": "2024-02-22 18:55:22", "link": "http://arxiv.org/abs/2402.14904v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for\n  On-Device Use Cases", "abstract": "This paper addresses the growing need for efficient large language models\n(LLMs) on mobile devices, driven by increasing cloud costs and latency\nconcerns. We focus on designing top-quality LLMs with fewer than a billion\nparameters, a practical choice for mobile deployment. Contrary to prevailing\nbelief emphasizing the pivotal role of data and parameter quantity in\ndetermining model quality, our investigation underscores the significance of\nmodel architecture for sub-billion scale LLMs. Leveraging deep and thin\narchitectures, coupled with embedding sharing and grouped-query attention\nmechanisms, we establish a strong baseline network denoted as MobileLLM, which\nattains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M\nstate-of-the-art models. Additionally, we propose an immediate block-wise\nweight-sharing approach with no increase in model size and only marginal\nlatency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a\nfurther accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,\nMobileLLM model family shows significant improvements compared to previous\nsub-billion models on chat benchmarks, and demonstrates close correctness to\nLLaMA-v2 7B in API calling tasks, highlighting the capability of small models\nfor common on-device use cases.", "published": "2024-02-22 18:58:55", "link": "http://arxiv.org/abs/2402.14905v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "In-Context Learning of a Linear Transformer Block: Benefits of the MLP\n  Component and One-Step GD Initialization", "abstract": "We study the \\emph{in-context learning} (ICL) ability of a \\emph{Linear\nTransformer Block} (LTB) that combines a linear attention component and a\nlinear multi-layer perceptron (MLP) component. For ICL of linear regression\nwith a Gaussian prior and a \\emph{non-zero mean}, we show that LTB can achieve\nnearly Bayes optimal ICL risk. In contrast, using only linear attention must\nincur an irreducible additive approximation error. Furthermore, we establish a\ncorrespondence between LTB and one-step gradient descent estimators with\nlearnable initialization ($\\mathsf{GD}\\text{-}\\mathbf{\\beta}$), in the sense\nthat every $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimator can be implemented by\nan LTB estimator and every optimal LTB estimator that minimizes the in-class\nICL risk is effectively a $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimator.\nFinally, we show that $\\mathsf{GD}\\text{-}\\mathbf{\\beta}$ estimators can be\nefficiently optimized with gradient flow, despite a non-convex training\nobjective. Our results reveal that LTB achieves ICL by implementing\n$\\mathsf{GD}\\text{-}\\mathbf{\\beta}$, and they highlight the role of MLP layers\nin reducing approximation error.", "published": "2024-02-22 20:26:08", "link": "http://arxiv.org/abs/2402.14951v1", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data", "abstract": "Multimodal Large Language Models (MLLMs) are typically assessed using\nexpensive annotated multimodal benchmarks, which often lag behind the rapidly\nevolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only\nunimodal data to measure inter-modality semantic coherence and inversely\nassesses MLLMs' tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is\nexpected to result in slower benchmark saturation, and avoids the illusion of\nemerging abilities. Inspired by the DrawCeption game, GenCeption begins with a\nnon-textual sample and proceeds through iterative description and generation\nsteps. The semantic drift across iterations is quantified using the GC@T\nmetric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision\nLLMs (VLLMs). Based on the GenCeption method, we establish the MMECeption\nbenchmark for evaluating VLLMs, and compare the performance of several popular\nVLLMs and human annotators. Our empirical results validate GenCeption's\neffectiveness, demonstrating strong correlations with established VLLM\nbenchmarks. VLLMs still significantly lag behind human performance and struggle\nespecially with text-intensive tasks.", "published": "2024-02-22 21:22:04", "link": "http://arxiv.org/abs/2402.14973v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.7; I.4"], "primary_category": "cs.CL"}
{"title": "Optimizing Language Models for Human Preferences is a Causal Inference\n  Problem", "abstract": "As large language models (LLMs) see greater use in academic and commercial\nsettings, there is increasing interest in methods that allow language models to\ngenerate texts aligned with human preferences. In this paper, we present an\ninitial exploration of language model optimization for human preferences from\ndirect outcome datasets, where each sample consists of a text and an associated\nnumerical outcome measuring the reader's response. We first propose that\nlanguage model optimization should be viewed as a causal problem to ensure that\nthe model correctly learns the relationship between the text and the outcome.\nWe formalize this causal language optimization problem, and we develop a\nmethod--causal preference optimization (CPO)--that solves an unbiased surrogate\nobjective for the problem. We further extend CPO with doubly robust CPO\n(DR-CPO), which reduces the variance of the surrogate objective while retaining\nprovably strong guarantees on bias. Finally, we empirically demonstrate the\neffectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human\npreferences on direct outcome data, and we validate the robustness of DR-CPO\nunder difficult confounding conditions.", "published": "2024-02-22 21:36:07", "link": "http://arxiv.org/abs/2402.14979v2", "categories": ["cs.LG", "cs.CL", "stat.ME"], "primary_category": "cs.LG"}
{"title": "tinyBenchmarks: evaluating LLMs with fewer examples", "abstract": "The versatility of large language models (LLMs) led to the creation of\ndiverse benchmarks that thoroughly test a variety of language models'\nabilities. These benchmarks consist of tens of thousands of examples making\nevaluation of LLMs very expensive. In this paper, we investigate strategies to\nreduce the number of evaluations needed to assess the performance of an LLM on\nseveral key benchmarks. For example, we show that to accurately estimate the\nperformance of an LLM on MMLU, a popular multiple-choice QA benchmark\nconsisting of 14K examples, it is sufficient to evaluate this LLM on 100\ncurated examples. We release evaluation tools and tiny versions of popular\nbenchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical\nanalysis demonstrates that these tools and tiny benchmarks are sufficient to\nreliably and efficiently reproduce the original evaluation results.", "published": "2024-02-22 22:05:23", "link": "http://arxiv.org/abs/2402.14992v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "How Important Is Tokenization in French Medical Masked Language Models?", "abstract": "Subword tokenization has become the prevailing standard in the field of\nnatural language processing (NLP) over recent years, primarily due to the\nwidespread utilization of pre-trained language models. This shift began with\nByte-Pair Encoding (BPE) and was later followed by the adoption of\nSentencePiece and WordPiece. While subword tokenization consistently\noutperforms character and word-level tokenization, the precise factors\ncontributing to its success remain unclear. Key aspects such as the optimal\nsegmentation granularity for diverse tasks and languages, the influence of data\nsources on tokenizers, and the role of morphological information in\nIndo-European languages remain insufficiently explored. This is particularly\npertinent for biomedical terminology, characterized by specific rules governing\nmorpheme combinations. Despite the agglutinative nature of biomedical\nterminology, existing language models do not explicitly incorporate this\nknowledge, leading to inconsistent tokenization strategies for common terms. In\nthis paper, we seek to delve into the complexities of subword tokenization in\nFrench biomedical domain across a variety of NLP tasks and pinpoint areas where\nfurther enhancements can be made. We analyze classical tokenization algorithms,\nincluding BPE and SentencePiece, and introduce an original tokenization\nstrategy that integrates morpheme-enriched word segmentation into existing\ntokenization methods.", "published": "2024-02-22 23:11:08", "link": "http://arxiv.org/abs/2402.15010v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Few-Shot Adaptation of Foundation Models via Multitask\n  Finetuning", "abstract": "Foundation models have emerged as a powerful tool for many AI problems.\nDespite the tremendous success of foundation models, effective adaptation to\nnew tasks, particularly those with limited labels, remains an open question and\nlacks theoretical understanding. An emerging solution with recent success in\nvision and NLP involves finetuning a foundation model on a selection of\nrelevant tasks, before its adaptation to a target task with limited labeled\nsamples. In this paper, we study the theoretical justification of this\nmultitask finetuning approach. Our theoretical analysis reveals that with a\ndiverse set of related tasks, this multitask finetuning leads to reduced error\nin the target task, in comparison to directly adapting the same pretrained\nmodel. We quantify the relationship between finetuning tasks and target tasks\nby diversity and consistency metrics, and further propose a practical task\nselection algorithm. We substantiate our theoretical claims with extensive\nempirical evidence. Further, we present results affirming our task selection\nalgorithm adeptly chooses related finetuning tasks, providing advantages to the\nmodel performance on target tasks. We believe our study shed new light on the\neffective adaptation of foundation models to new tasks that lack abundant\nlabels. Our code is available at\nhttps://github.com/OliverXUZY/Foudation-Model_Multitask.", "published": "2024-02-22 23:29:42", "link": "http://arxiv.org/abs/2402.15017v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Unintended Impacts of LLM Alignment on Global Representation", "abstract": "Before being deployed for user-facing applications, developers align Large\nLanguage Models (LLMs) to user preferences through a variety of procedures,\nsuch as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference\nOptimization (DPO). Current evaluations of these procedures focus on benchmarks\nof instruction following, reasoning, and truthfulness. However, human\npreferences are not universal, and aligning to specific preference sets may\nhave unintended effects. We explore how alignment impacts performance along\nthree axes of global representation: English dialects, multilingualism, and\nopinions from and about countries worldwide. Our results show that current\nalignment procedures create disparities between English dialects and global\nopinions. We find alignment improves capabilities in several languages. We\nconclude by discussing design decisions that led to these unintended impacts\nand recommendations for more equitable preference tuning. We make our code and\ndata publicly available on Github.", "published": "2024-02-22 23:31:22", "link": "http://arxiv.org/abs/2402.15018v2", "categories": ["cs.CL", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Unleashing the Power of Imbalanced Modality Information for Multi-modal\n  Knowledge Graph Completion", "abstract": "Multi-modal knowledge graph completion (MMKGC) aims to predict the missing\ntriples in the multi-modal knowledge graphs by incorporating structural,\nvisual, and textual information of entities into the discriminant models. The\ninformation from different modalities will work together to measure the triple\nplausibility. Existing MMKGC methods overlook the imbalance problem of modality\ninformation among entities, resulting in inadequate modal fusion and\ninefficient utilization of the raw modality information. To address the\nmentioned problems, we propose Adaptive Multi-modal Fusion and Modality\nAdversarial Training (AdaMF-MAT) to unleash the power of imbalanced modality\ninformation for MMKGC. AdaMF-MAT achieves multi-modal fusion with adaptive\nmodality weights and further generates adversarial samples by\nmodality-adversarial training to enhance the imbalanced modality information.\nOur approach is a co-design of the MMKGC model and training strategy which can\noutperform 19 recent MMKGC methods and achieve new state-of-the-art results on\nthree public MMKGC benchmarks. Our code and data have been released at\nhttps://github.com/zjukg/AdaMF-MAT.", "published": "2024-02-22 05:48:03", "link": "http://arxiv.org/abs/2402.15444v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.AI"}
{"title": "L+M-24: Building a Dataset for Language + Molecules @ ACL 2024", "abstract": "Language-molecule models have emerged as an exciting direction for molecular\ndiscovery and understanding. However, training these models is challenging due\nto the scarcity of molecule-language pair datasets. At this point, datasets\nhave been released which are 1) small and scraped from existing databases, 2)\nlarge but noisy and constructed by performing entity linking on the scientific\nliterature, and 3) built by converting property prediction datasets to natural\nlanguage using templates. In this document, we detail the $\\textit{L+M-24}$\ndataset, which has been created for the Language + Molecules Workshop shared\ntask at ACL 2024. In particular, $\\textit{L+M-24}$ is designed to focus on\nthree key benefits of natural language in molecule design: compositionality,\nfunctionality, and abstraction.", "published": "2024-02-22 20:11:24", "link": "http://arxiv.org/abs/2403.00791v2", "categories": ["cs.CL", "cs.AI", "q-bio.BM", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Is ChatGPT More Empathetic than Humans?", "abstract": "This paper investigates the empathetic responding capabilities of ChatGPT,\nparticularly its latest iteration, GPT-4, in comparison to human-generated\nresponses to a wide range of emotional scenarios, both positive and negative.\nWe employ a rigorous evaluation methodology, involving a between-groups study\nwith 600 participants, to evaluate the level of empathy in responses generated\nby humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard\napproach and one explicitly detailing empathy's cognitive, affective, and\ncompassionate counterparts. Our findings indicate that the average empathy\nrating of responses generated by ChatGPT exceeds those crafted by humans by\napproximately 10%. Additionally, instructing ChatGPT to incorporate a clear\nunderstanding of empathy in its responses makes the responses align\napproximately 5 times more closely with the expectations of individuals\npossessing a high degree of empathy, compared to human responses. The proposed\nevaluation framework serves as a scalable and adaptable framework to assess the\nempathetic capabilities of newer and updated versions of large language models,\neliminating the need to replicate the current study's results in future\nresearch.", "published": "2024-02-22 09:52:45", "link": "http://arxiv.org/abs/2403.05572v1", "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "cs.HC"}
{"title": "SIMPLOT: Enhancing Chart Question Answering by Distilling Essentials", "abstract": "Recently, interpreting complex charts with logical reasoning has emerged as\nchallenges due to the development of vision-language models. A prior\nstate-of-the-art (SOTA) model has presented an end-to-end method that leverages\nthe vision-language model to convert charts into table format utilizing Large\nLanguage Model (LLM) for reasoning. However, unlike natural images, charts\ncontain a mix of essential and irrelevant information required for chart\nreasoning, and we discover that this characteristic can lower the performance\nof chart-to-table extraction. In this paper, we introduce SIMPLOT, a method\ndesigned to extract only the elements necessary for chart reasoning. The\nproposed method involves two steps: 1) training to mimic a simple plot that\ncontains only the essential information from a complex chart for table\nextraction, followed by 2) performing reasoning based on the table. Our model\nenables accurate chart reasoning without the need for additional annotations or\ndatasets, and its effectiveness is demonstrated through various experiments.\nFurthermore, we propose a novel prompt mimicking how human interpret charts for\nmore accurate reasoning. Our source code is available at\nhttps://github.com/sangwu99/Simplot.", "published": "2024-02-22 14:04:22", "link": "http://arxiv.org/abs/2405.00021v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies\n  with Language Modeling", "abstract": "The therapeutic working alliance is a critical factor in predicting the\nsuccess of psychotherapy treatment. Traditionally, working alliance assessment\nrelies on questionnaires completed by both therapists and patients. In this\npaper, we present COMPASS, a novel framework to directly infer the therapeutic\nworking alliance from the natural language used in psychotherapy sessions. Our\napproach utilizes advanced large language models (LLMs) to analyze transcripts\nof psychotherapy sessions and compare them with distributed representations of\nstatements in the working alliance inventory. Analyzing a dataset of over 950\nsessions covering diverse psychiatric conditions including anxiety, depression,\nschizophrenia, and suicidal tendencies, we demonstrate the effectiveness of our\nmethod in providing fine-grained mapping of patient-therapist alignment\ntrajectories and offering interpretability for clinical psychiatry and in\nidentifying emerging patterns related to the condition being treated. By\nemploying various deep learning-based topic modeling techniques in combination\nwith prompting generative language models, we analyze the topical\ncharacteristics of different psychiatric conditions and their evolution at a\nturn-level resolution. This combined framework enhances the understanding of\ntherapeutic interactions, enabling timely feedback for therapists regarding the\nquality of therapeutic relationships and providing interpretable insights to\nimprove the effectiveness of psychotherapy.", "published": "2024-02-22 16:56:44", "link": "http://arxiv.org/abs/2402.14701v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "IEPile: Unearthing Large-Scale Schema-Based Information Extraction\n  Corpus", "abstract": "Large Language Models (LLMs) demonstrate remarkable potential across various\ndomains; however, they exhibit a significant performance gap in Information\nExtraction (IE). Note that high-quality instruction data is the vital key for\nenhancing the specific capabilities of LLMs, while current IE datasets tend to\nbe small in scale, fragmented, and lack standardized schema. To this end, we\nintroduce IEPile, a comprehensive bilingual (English and Chinese) IE\ninstruction corpus, which contains approximately 0.32B tokens. We construct\nIEPile by collecting and cleaning 33 existing IE datasets, and introduce\nschema-based instruction generation to unearth a large-scale corpus.\nExperimentally, IEPile enhance the performance of LLMs for IE, with notable\nimprovements in zero-shot generalization. We open-source the resource and\npre-trained models, hoping to provide valuable support to the NLP community.", "published": "2024-02-22 17:11:38", "link": "http://arxiv.org/abs/2402.14710v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset", "abstract": "Recent advancements in Large Multimodal Models (LMMs) have shown promising\nresults in mathematical reasoning within visual contexts, with models\napproaching human-level performance on existing benchmarks such as MathVista.\nHowever, we observe significant limitations in the diversity of questions and\nbreadth of subjects covered by these benchmarks. To address this issue, we\npresent the MATH-Vision (MATH-V) dataset, a meticulously curated collection of\n3,040 high-quality mathematical problems with visual contexts sourced from real\nmath competitions. Spanning 16 distinct mathematical disciplines and graded\nacross 5 levels of difficulty, our dataset provides a comprehensive and diverse\nset of challenges for evaluating the mathematical reasoning abilities of LMMs.\nThrough extensive experimentation, we unveil a notable performance gap between\ncurrent LMMs and human performance on MATH-V, underscoring the imperative for\nfurther advancements in LMMs. Moreover, our detailed categorization allows for\na thorough error analysis of LMMs, offering valuable insights to guide future\nresearch and development. The project is available at\nhttps://mathvision-cuhk.github.io", "published": "2024-02-22 18:56:38", "link": "http://arxiv.org/abs/2402.14804v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "math.HO"], "primary_category": "cs.CV"}
{"title": "SICRN: Advancing Speech Enhancement through State Space Model and\n  Inplace Convolution Techniques", "abstract": "Speech enhancement aims to improve speech quality and intelligibility,\nespecially in noisy environments where background noise degrades speech\nsignals. Currently, deep learning methods achieve great success in speech\nenhancement, e.g. the representative convolutional recurrent neural network\n(CRN) and its variants. However, CRN typically employs consecutive downsampling\nand upsampling convolution for frequency modeling, which destroys the inherent\nstructure of the signal over frequency. Additionally, convolutional layers\nlacks of temporal modelling abilities. To address these issues, we propose an\ninnovative module combing a State space model and Inplace Convolution (SIC),\nand to replace the conventional convolution in CRN, called SICRN. Specifically,\na dual-path multidimensional State space model captures the global frequencies\ndependency and long-term temporal dependencies. Meanwhile, the 2D-inplace\nconvolution is used to capture the local structure, which abandons the\ndownsampling and upsampling. Systematic evaluations on the public INTERSPEECH\n2020 DNS challenge dataset demonstrate SICRN's efficacy. Compared to strong\nbaselines, SICRN achieves performance close to state-of-the-art while having\nadvantages in model parameters, computations, and algorithmic delay. The\nproposed SICRN shows great promise for improved speech enhancement.", "published": "2024-02-22 02:08:35", "link": "http://arxiv.org/abs/2402.14225v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Symbolic Music Generation with Non-Differentiable Rule Guided Diffusion", "abstract": "We study the problem of symbolic music generation (e.g., generating piano\nrolls), with a technical focus on non-differentiable rule guidance. Musical\nrules are often expressed in symbolic form on note characteristics, such as\nnote density or chord progression, many of which are non-differentiable which\npose a challenge when using them for guided diffusion. We propose Stochastic\nControl Guidance (SCG), a novel guidance method that only requires forward\nevaluation of rule functions that can work with pre-trained diffusion models in\na plug-and-play way, thus achieving training-free guidance for\nnon-differentiable rules for the first time. Additionally, we introduce a\nlatent diffusion architecture for symbolic music generation with high time\nresolution, which can be composed with SCG in a plug-and-play fashion. Compared\nto standard strong baselines in symbolic music generation, this framework\ndemonstrates marked advancements in music quality and rule-based\ncontrollability, outperforming current state-of-the-art generators in a variety\nof settings. For detailed demonstrations, code and model checkpoints, please\nvisit our project website: https://scg-rule-guided-music.github.io/.", "published": "2024-02-22 04:55:58", "link": "http://arxiv.org/abs/2402.14285v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PeriodGrad: Towards Pitch-Controllable Neural Vocoder Based on a\n  Diffusion Probabilistic Model", "abstract": "This paper presents a neural vocoder based on a denoising diffusion\nprobabilistic model (DDPM) incorporating explicit periodic signals as auxiliary\nconditioning signals. Recently, DDPM-based neural vocoders have gained\nprominence as non-autoregressive models that can generate high-quality\nwaveforms. The neural vocoders based on DDPM have the advantage of training\nwith a simple time-domain loss. In practical applications, such as singing\nvoice synthesis, there is a demand for neural vocoders to generate\nhigh-fidelity speech waveforms with flexible pitch control. However,\nconventional DDPM-based neural vocoders struggle to generate speech waveforms\nunder such conditions. Our proposed model aims to accurately capture the\nperiodic structure of speech waveforms by incorporating explicit periodic\nsignals. Experimental results show that our model improves sound quality and\nprovides better pitch control than conventional DDPM-based neural vocoders.", "published": "2024-02-22 16:47:15", "link": "http://arxiv.org/abs/2402.14692v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Human Brain Exhibits Distinct Patterns When Listening to Fake Versus\n  Real Audio: Preliminary Evidence", "abstract": "In this paper we study the variations in human brain activity when listening\nto real and fake audio. Our preliminary results suggest that the\nrepresentations learned by a state-of-the-art deepfake audio detection\nalgorithm, do not exhibit clear distinct patterns between real and fake audio.\nIn contrast, human brain activity, as measured by EEG, displays distinct\npatterns when individuals are exposed to fake versus real audio. This\npreliminary evidence enables future research directions in areas such as\ndeepfake audio detection.", "published": "2024-02-22 21:44:58", "link": "http://arxiv.org/abs/2402.14982v3", "categories": ["cs.SD", "cs.LG", "eess.AS", "q-bio.NC"], "primary_category": "cs.SD"}
{"title": "Structuring Concept Space with the Musical Circle of Fifths by Utilizing\n  Music Grammar Based Activations", "abstract": "In this paper, we explore the intriguing similarities between the structure\nof a discrete neural network, such as a spiking network, and the composition of\na piano piece. While both involve nodes or notes that are activated\nsequentially or in parallel, the latter benefits from the rich body of music\ntheory to guide meaningful combinations. We propose a novel approach that\nleverages musical grammar to regulate activations in a spiking neural network,\nallowing for the representation of symbols as attractors. By applying rules for\nchord progressions from music theory, we demonstrate how certain activations\nnaturally follow others, akin to the concept of attraction. Furthermore, we\nintroduce the concept of modulating keys to navigate different basins of\nattraction within the network. Ultimately, we show that the map of concepts in\nour model is structured by the musical circle of fifths, highlighting the\npotential for leveraging music theory principles in deep learning algorithms.", "published": "2024-02-22 03:28:25", "link": "http://arxiv.org/abs/2403.00790v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring and Applying Audio-Based Sentiment Analysis in Music", "abstract": "Sentiment analysis is a continuously explored area of text processing that\ndeals with the computational analysis of opinions, sentiments, and subjectivity\nof text. However, this idea is not limited to text and speech, in fact, it\ncould be applied to other modalities. In reality, humans do not express\nthemselves in text as deeply as they do in music. The ability of a\ncomputational model to interpret musical emotions is largely unexplored and\ncould have implications and uses in therapy and musical queuing. In this paper,\ntwo individual tasks are addressed. This study seeks to (1) predict the emotion\nof a musical clip over time and (2) determine the next emotion value after the\nmusic in a time series to ensure seamless transitions. Utilizing data from the\nEmotions in Music Database, which contains clips of songs selected from the\nFree Music Archive annotated with levels of valence and arousal as reported on\nRussel's circumplex model of affect by multiple volunteers, models are trained\nfor both tasks. Overall, the performance of these models reflected that they\nwere able to perform the tasks they were designed for effectively and\naccurately.", "published": "2024-02-22 22:34:06", "link": "http://arxiv.org/abs/2403.17379v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Compression Robust Synthetic Speech Detection Using Patched Spectrogram\n  Transformer", "abstract": "Many deep learning synthetic speech generation tools are readily available.\nThe use of synthetic speech has caused financial fraud, impersonation of\npeople, and misinformation to spread. For this reason forensic methods that can\ndetect synthetic speech have been proposed. Existing methods often overfit on\none dataset and their performance reduces substantially in practical scenarios\nsuch as detecting synthetic speech shared on social platforms. In this paper we\npropose, Patched Spectrogram Synthetic Speech Detection Transformer (PS3DT), a\nsynthetic speech detector that converts a time domain speech signal to a\nmel-spectrogram and processes it in patches using a transformer neural network.\nWe evaluate the detection performance of PS3DT on ASVspoof2019 dataset. Our\nexperiments show that PS3DT performs well on ASVspoof2019 dataset compared to\nother approaches using spectrogram for synthetic speech detection. We also\ninvestigate generalization performance of PS3DT on In-the-Wild dataset. PS3DT\ngeneralizes well than several existing methods on detecting synthetic speech\nfrom an out-of-distribution dataset. We also evaluate robustness of PS3DT to\ndetect telephone quality synthetic speech and synthetic speech shared on social\nplatforms (compressed speech). PS3DT is robust to compression and can detect\ntelephone quality synthetic speech better than several existing methods.", "published": "2024-02-22 01:18:55", "link": "http://arxiv.org/abs/2402.14205v1", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
