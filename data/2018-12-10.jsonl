{"title": "SDNet: Contextualized Attention-based Deep Network for Conversational\n  Question Answering", "abstract": "Conversational question answering (CQA) is a novel QA task that requires\nunderstanding of dialogue context. Different from traditional single-turn\nmachine reading comprehension (MRC) tasks, CQA includes passage comprehension,\ncoreference resolution, and contextual understanding. In this paper, we propose\nan innovated contextualized attention-based deep neural network, SDNet, to fuse\ncontext into traditional MRC models. Our model leverages both inter-attention\nand self-attention to comprehend conversation context and extract relevant\ninformation from passage. Furthermore, we demonstrated a novel method to\nintegrate the latest BERT contextual model. Empirical results show the\neffectiveness of our model, which sets the new state of the art result in CoQA\nleaderboard, outperforming the previous best model by 1.6% F1. Our ensemble\nmodel further improves the result by 2.7% F1.", "published": "2018-12-10 01:43:14", "link": "http://arxiv.org/abs/1812.03593v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inflo: News Categorization and Keyphrase Extraction for Implementation\n  in an Aggregation System", "abstract": "The work herein describes a system for automatic news category and keyphrase\nlabeling, presented in the context of our motivation to improve the speed at\nwhich a user can find relevant and interesting content within an aggregation\nplatform. A set of 12 discrete categories were applied to over 500,000 news\narticles for training a neural network, to be used to facilitate the more\nin-depth task of extracting the most significant keyphrases. The latter was\ndone using three methods: statistical, graphical and numerical, using the\npre-identified category label to improve relevance of extracted phrases. The\nresults are presented in a demo in which the articles are pre-populated via\nNews API, and upon being selected, the category and keyphrase labels will be\ncomputed via the methods explained herein.", "published": "2018-12-10 13:36:09", "link": "http://arxiv.org/abs/1812.03781v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Chat-crowd: A Dialog-based Platform for Visual Layout Composition", "abstract": "In this paper we introduce Chat-crowd, an interactive environment for visual\nlayout composition via conversational interactions. Chat-crowd supports\nmultiple agents with two conversational roles: agents who play the role of a\ndesigner are in charge of placing objects in an editable canvas according to\ninstructions or commands issued by agents with a director role. The system can\nbe integrated with crowdsourcing platforms for both synchronous and\nasynchronous data collection and is equipped with comprehensive quality\ncontrols on the performance of both types of agents. We expect that this system\nwill be useful to build multimodal goal-oriented dialog tasks that require\nspatial and geometric reasoning.", "published": "2018-12-10 20:52:41", "link": "http://arxiv.org/abs/1812.04081v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Pretraining by Backtranslation for End-to-end ASR in Low-Resource\n  Settings", "abstract": "We explore training attention-based encoder-decoder ASR in low-resource\nsettings. These models perform poorly when trained on small amounts of\ntranscribed speech, in part because they depend on having sufficient\ntarget-side text to train the attention and decoder networks. In this paper we\naddress this shortcoming by pretraining our network parameters using only\ntext-based data and transcribed speech from other languages. We analyze the\nrelative contributions of both sources of data. Across 3 test languages, our\ntext-based approach resulted in a 20% average relative improvement over a\ntext-based augmentation technique without pretraining. Using transcribed speech\nfrom nearby languages gives a further 20-30% relative reduction in character\nerror rate.", "published": "2018-12-10 16:57:21", "link": "http://arxiv.org/abs/1812.03919v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "L\u00e9vy Flights of the Collective Imagination", "abstract": "We present a structured random-walk model that captures key aspects of how\npeople communicate in groups. Our model takes the form of a correlated L\\'{e}vy\nflight that quantifies the balance between focused discussion of an idea and\nlong-distance leaps in semantic space. We apply our model to three cases of\nincreasing structural complexity: philosophical texts by Aristotle, Hume, and\nKant; four days of parliamentary debate during the French Revolution; and\nbranching comment trees on the discussion website Reddit. In the philosophical\nand parliamentary cases, the model parameters that describe this balance\nconverge under coarse-graining to limit regions that demonstrate the emergence\nof large-scale structure, a result which is robust to translation between\nlanguages. Meanwhile, we find that the political forum we consider on Reddit\nexhibits a debate-like pattern, while communities dedicated to the discussion\nof science and news show much less temporal order, and may make use of the\nemergent, tree-like topology of comment replies to structure their epistemic\nexplorations. Our model allows us to quantify the ways in which social\ntechnologies such as parliamentary procedures and online commenting systems\nshape the joint exploration of ideas.", "published": "2018-12-10 19:00:09", "link": "http://arxiv.org/abs/1812.04013v1", "categories": ["cs.SI", "cs.CL", "nlin.AO", "physics.soc-ph"], "primary_category": "cs.SI"}
{"title": "Von Mises-Fisher Loss for Training Sequence to Sequence Models with\n  Continuous Outputs", "abstract": "The Softmax function is used in the final layer of nearly all existing\nsequence-to-sequence models for language generation. However, it is usually the\nslowest layer to compute which limits the vocabulary size to a subset of most\nfrequent types; and it has a large memory footprint. We propose a general\ntechnique for replacing the softmax layer with a continuous embedding layer.\nOur primary innovations are a novel probabilistic loss, and a training and\ninference procedure in which we generate a probability distribution over\npre-trained word embeddings, instead of a multinomial distribution over the\nvocabulary obtained via softmax. We evaluate this new class of\nsequence-to-sequence models with continuous outputs on the task of neural\nmachine translation. We show that our models obtain upto 2.5x speed-up in\ntraining time while performing on par with the state-of-the-art models in terms\nof translation quality. These models are capable of handling very large\nvocabularies without compromising on translation quality. They also produce\nmore meaningful errors than in the softmax-based models, as these errors\ntypically lie in a subspace of the vector space of the reference translations.", "published": "2018-12-10 20:00:36", "link": "http://arxiv.org/abs/1812.04616v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Vision-based Navigation with Language-based Assistance via Imitation\n  Learning with Indirect Intervention", "abstract": "We present Vision-based Navigation with Language-based Assistance (VNLA), a\ngrounded vision-language task where an agent with visual perception is guided\nvia language to find objects in photorealistic indoor environments. The task\nemulates a real-world scenario in that (a) the requester may not know how to\nnavigate to the target objects and thus makes requests by only specifying\nhigh-level end-goals, and (b) the agent is capable of sensing when it is lost\nand querying an advisor, who is more qualified at the task, to obtain language\nsubgoals to make progress. To model language-based assistance, we develop a\ngeneral framework termed Imitation Learning with Indirect Intervention (I3L),\nand propose a solution that is effective on the VNLA task. Empirical results\nshow that this approach significantly improves the success rate of the learning\nagent over other baselines in both seen and unseen environments. Our code and\ndata are publicly available at https://github.com/debadeepta/vnla .", "published": "2018-12-10 23:48:25", "link": "http://arxiv.org/abs/1812.04155v4", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.RO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Computationally Efficient and Practically Feasible Two Microphones\n  Blind Speech Separation Method", "abstract": "Traditionally, Blind Speech Separation techniques are computationally\nexpensive as they update the demixing matrix at every time frame index, making\nthem impractical to use in many Real-Time applications. In this paper, a robust\ndata-driven two-microphone sound source localization method is used as a\ncriterion to reduce the computational complexity of the Independent Vector\nAnalysis (IVA) Blind Speech Separation (BSS) method. IVA is used to separate\nconvolutedly mixed speech and noise sources. The practical feasibility of the\nproposed method is proved by implementing it on a smartphone device to separate\nspeech and noise in Real-World scenarios for Hearing-Aid applications. The\nexperimental results with objective and subjective tests reveal the practical\nusability of the developed method in many real-world applications.", "published": "2018-12-10 16:49:47", "link": "http://arxiv.org/abs/1812.03914v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An individualized super Gaussian single microphone Speech Enhancement\n  for hearing aid users with smartphone as an assistive device", "abstract": "In this letter, we derive a new super Gaussian Joint Maximum a Posteriori\nbased single microphone speech enhancement gain function. The developed Speech\nEnhancement method is implemented on a smartphone, and this arrangement\nfunctions as an assistive device to hearing aids. We introduce a tradeoff\nparameter in the derived gain function that allows the smartphone user to\ncustomize their listening preference, by controlling the amount of noise\nsuppression and speech distortion in real-time based on their level of hearing\ncomfort perceived in noisy real world acoustic environment. Objective quality\nand intelligibility measures show the effectiveness of the proposed method in\ncomparison to benchmark techniques considered in this paper. Subjective results\nreflect the usefulness of the developed Speech Enhancement application in\nreal-world noisy conditions at signal to noise ratio levels of 0 dB and 5 dB.", "published": "2018-12-10 16:50:37", "link": "http://arxiv.org/abs/1812.03916v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Impact of Intervals on the Emotional Effect in Western Music", "abstract": "Every art form ultimately aims to invoke an emotional response over the\naudience, and music is no different. While the precise perception of music is a\nhighly subjective topic, there is an agreement in the \"feeling\" of a piece of\nmusic in broad terms. Based on this observation, in this study, we aimed to\ndetermine the emotional feeling associated with short passages of music;\nspecifically by analyzing the melodic aspects. We have used the dataset put\ntogether by Eerola et. al. which is comprised of labeled short passages of film\nmusic. Our initial survey of the dataset indicated that other than \"happy\" and\n\"sad\" labels do not possess a melodic structure. We transcribed the main melody\nof the happy and sad tracks and used the intervals between the notes to\nclassify them. Our experiments have shown that treating a melody as a\nbag-of-intervals do not possess any predictive power whatsoever, whereas\ncounting intervals with respect to the key of the melody yielded a classifier\nwith 85% accuracy.", "published": "2018-12-10 17:49:30", "link": "http://arxiv.org/abs/1812.04723v1", "categories": ["q-bio.NC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "q-bio.NC"}
