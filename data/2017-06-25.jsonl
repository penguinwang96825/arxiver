{"title": "A Deep Neural Architecture for Sentence-level Sentiment Classification\n  in Twitter Social Networking", "abstract": "This paper introduces a novel deep learning framework including a\nlexicon-based approach for sentence-level prediction of sentiment label\ndistribution. We propose to first apply semantic rules and then use a Deep\nConvolutional Neural Network (DeepCNN) for character-level embeddings in order\nto increase information for word-level embedding. After that, a Bidirectional\nLong Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature\nrepresentation from the word-level embedding. We evaluate our approach on three\nTwitter sentiment classification datasets. Experimental results show that our\nmodel can improve the classification accuracy of sentence-level sentiment\nanalysis in Twitter social networking.", "published": "2017-06-25 04:05:09", "link": "http://arxiv.org/abs/1706.08032v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated text summarisation and evidence-based medicine: A survey of\n  two domains", "abstract": "The practice of evidence-based medicine (EBM) urges medical practitioners to\nutilise the latest research evidence when making clinical decisions. Because of\nthe massive and growing volume of published research on various medical topics,\npractitioners often find themselves overloaded with information. As such,\nnatural language processing research has recently commenced exploring\ntechniques for performing medical domain-specific automated text summarisation\n(ATS) techniques-- targeted towards the task of condensing large medical texts.\nHowever, the development of effective summarisation techniques for this task\nrequires cross-domain knowledge. We present a survey of EBM, the\ndomain-specific needs for EBM, automated summarisation techniques, and how they\nhave been applied hitherto. We envision that this survey will serve as a first\nresource for the development of future operational text summarisation\ntechniques for EBM.", "published": "2017-06-25 20:12:28", "link": "http://arxiv.org/abs/1706.08162v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Synonym Discovery with Knowledge Bases", "abstract": "Recognizing entity synonyms from text has become a crucial task in many\nentity-leveraging applications. However, discovering entity synonyms from\ndomain-specific text corpora (e.g., news articles, scientific papers) is rather\nchallenging. Current systems take an entity name string as input to find out\nother names that are synonymous, ignoring the fact that often times a name\nstring can refer to multiple entities (e.g., \"apple\" could refer to both Apple\nInc and the fruit apple). Moreover, most existing methods require training data\nmanually created by domain experts to construct supervised-learning systems. In\nthis paper, we study the problem of automatic synonym discovery with knowledge\nbases, that is, identifying synonyms for knowledge base entities in a given\ndomain-specific corpus. The manually-curated synonyms for each entity stored in\na knowledge base not only form a set of name strings to disambiguate the\nmeaning for each other, but also can serve as \"distant\" supervision to help\ndetermine important features for the task. We propose a novel framework, called\nDPE, to integrate two kinds of mutually-complementing signals for synonym\ndiscovery, i.e., distributional features based on corpus-level statistics and\ntextual patterns based on local contexts. In particular, DPE jointly optimizes\nthe two kinds of signals in conjunction with distant supervision, so that they\ncan mutually enhance each other in the training stage. At the inference stage,\nboth signals will be utilized to discover synonyms for the given entities.\nExperimental results prove the effectiveness of the proposed framework.", "published": "2017-06-25 23:10:26", "link": "http://arxiv.org/abs/1706.08186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have\nbecome ubiquitous to several NLP tasks. A recent line of work uses bilingual\n(two languages) corpora to learn a different vector for each sense of a word,\nby exploiting crosslingual signals to aid sense identification. We present a\nmulti-view Bayesian non-parametric algorithm which improves multi-sense word\nembeddings by (a) using multilingual (i.e., more than two languages) corpora to\nsignificantly improve sense embeddings beyond what one achieves with bilingual\ninformation, and (b) uses a principled approach to learn a variable number of\nsenses per word, in a data-driven manner. Ours is the first approach with the\nability to leverage multilingual corpora efficiently for multi-sense\nrepresentation learning. Experiments show that multilingual training\nsignificantly improves performance over monolingual and bilingual training, by\nallowing us to combine different parallel corpora to leverage multilingual\ncontext. Multilingual training yields comparable performance to a state of the\nart mono-lingual model trained on five times more training data.", "published": "2017-06-25 20:00:54", "link": "http://arxiv.org/abs/1706.08160v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
