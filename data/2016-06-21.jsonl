{"title": "Incremental Parsing with Minimal Features Using Bi-Directional LSTM", "abstract": "Recently, neural network approaches for parsing have largely automated the\ncombination of individual features, but still rely on (often a larger number\nof) atomic features created from human linguistic intuition, and potentially\nomitting important global context. To further reduce feature engineering to the\nbare minimum, we use bi-directional LSTM sentence representations to model a\nparser state with only three sentence positions, which automatically identifies\nimportant aspects of the entire sentence. This model achieves state-of-the-art\nresults among greedy dependency parsers for English. We also introduce a novel\ntransition system for constituency parsing which does not require binarization,\nand together with the above architecture, achieves state-of-the-art results\namong greedy parsers for both English and Chinese.", "published": "2016-06-21 03:20:59", "link": "http://arxiv.org/abs/1606.06406v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Morphological Tagging from Characters for Morphologically Rich\n  Languages", "abstract": "This paper investigates neural character-based morphological tagging for\nlanguages with complex morphology and large tag sets. We systematically explore\na variety of neural architectures (DNN, CNN, CNNHighway, LSTM, BLSTM) to obtain\ncharacter-based word vectors combined with bidirectional LSTMs to model\nacross-word context in an end-to-end setting. We explore supplementary use of\nword-based vectors trained on large amounts of unlabeled data. Our experiments\nfor morphological tagging suggest that for \"simple\" model configurations, the\nchoice of the network architecture (CNN vs. CNNHighway vs. LSTM vs. BLSTM) or\nthe augmentation with pre-trained word embeddings can be important and clearly\nimpact the accuracy. Increasing the model capacity by adding depth, for\nexample, and carefully optimizing the neural networks can lead to substantial\nimprovements, and the differences in accuracy (but not training time) become\nmuch smaller or even negligible. Overall, our best morphological taggers for\nGerman and Czech outperform the best results reported in the literature by a\nlarge margin.", "published": "2016-06-21 16:25:31", "link": "http://arxiv.org/abs/1606.06640v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Correlation-based Intrinsic Evaluation of Word Vector Representations", "abstract": "We introduce QVEC-CCA--an intrinsic evaluation metric for word vector\nrepresentations based on correlations of learned vectors with features\nextracted from linguistic resources. We show that QVEC-CCA scores are an\neffective proxy for a range of extrinsic semantic and syntactic tasks. We also\nshow that the proposed evaluation obtains higher and more consistent\ncorrelations with downstream tasks, compared to existing approaches to\nintrinsic evaluation of word vectors that are based on word similarity.", "published": "2016-06-21 19:12:01", "link": "http://arxiv.org/abs/1606.06710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Topic Aware Neural Response Generation", "abstract": "We consider incorporating topic information into the sequence-to-sequence\nframework to generate informative and interesting responses for chatbots. To\nthis end, we propose a topic aware sequence-to-sequence (TA-Seq2Seq) model. The\nmodel utilizes topics to simulate prior knowledge of human that guides them to\nform informative and interesting responses in conversation, and leverages the\ntopic information in generation by a joint attention mechanism and a biased\ngeneration probability. The joint attention mechanism summarizes the hidden\nvectors of an input message as context vectors by message attention,\nsynthesizes topic vectors by topic attention from the topic words of the\nmessage obtained from a pre-trained LDA model, and let these vectors jointly\naffect the generation of words in decoding. To increase the possibility of\ntopic words appearing in responses, the model modifies the generation\nprobability of topic words by adding an extra probability item to bias the\noverall distribution. Empirical study on both automatic evaluation metrics and\nhuman annotations shows that TA-Seq2Seq can generate more informative and\ninteresting responses, and significantly outperform the-state-of-the-art\nresponse generation models.", "published": "2016-06-21 05:47:59", "link": "http://arxiv.org/abs/1606.08340v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neighborhood Mixture Model for Knowledge Base Completion", "abstract": "Knowledge bases are useful resources for many natural language processing\ntasks, however, they are far from complete. In this paper, we define a novel\nentity representation as a mixture of its neighborhood in the knowledge base\nand apply this technique on TransE-a well-known embedding model for knowledge\nbase completion. Experimental results show that the neighborhood information\nsignificantly helps to improve the results of the TransE model, leading to\nbetter performance than obtained by other state-of-the-art embedding models on\nthree benchmark datasets for triple classification, entity prediction and\nrelation prediction tasks.", "published": "2016-06-21 07:54:35", "link": "http://arxiv.org/abs/1606.06461v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An empirical study on large scale text classification with skip-gram\n  embeddings", "abstract": "We investigate the integration of word embeddings as classification features\nin the setting of large scale text classification. Such representations have\nbeen used in a plethora of tasks, however their application in classification\nscenarios with thousands of classes has not been extensively researched,\npartially due to hardware limitations. In this work, we examine efficient\ncomposition functions to obtain document-level from word-level embeddings and\nwe subsequently investigate their combination with the traditional\none-hot-encoding representations. By presenting empirical evidence on large,\nmulti-class, multi-label classification problems, we demonstrate the efficiency\nand the performance benefits of this combination.", "published": "2016-06-21 15:39:35", "link": "http://arxiv.org/abs/1606.06623v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Criticality in Formal Languages and Statistical Physics", "abstract": "We show that the mutual information between two symbols, as a function of the\nnumber of symbols between the two, decays exponentially in any probabilistic\nregular grammar, but can decay like a power law for a context-free grammar.\nThis result about formal languages is closely related to a well-known result in\nclassical statistical mechanics that there are no phase transitions in\ndimensions fewer than two. It is also related to the emergence of power-law\ncorrelations in turbulence and cosmological inflation through recursive\ngenerative processes. We elucidate these physics connections and comment on\npotential applications of our results to machine learning tasks like training\nartificial recurrent neural networks. Along the way, we introduce a useful\nquantity which we dub the rational mutual information and discuss\ngeneralizations of our claims involving more complicated Bayesian networks.", "published": "2016-06-21 20:00:01", "link": "http://arxiv.org/abs/1606.06737v3", "categories": ["cond-mat.dis-nn", "cs.CL"], "primary_category": "cond-mat.dis-nn"}
{"title": "A Novel Framework to Expedite Systematic Reviews by Automatically\n  Building Information Extraction Training Corpora", "abstract": "A systematic review identifies and collates various clinical studies and\ncompares data elements and results in order to provide an evidence based answer\nfor a particular clinical question. The process is manual and involves lot of\ntime. A tool to automate this process is lacking. The aim of this work is to\ndevelop a framework using natural language processing and machine learning to\nbuild information extraction algorithms to identify data elements in a new\nprimary publication, without having to go through the expensive task of manual\nannotation to build gold standards for each data element type. The system is\ndeveloped in two stages. Initially, it uses information contained in existing\nsystematic reviews to identify the sentences from the PDF files of the included\nreferences that contain specific data elements of interest using a modified\nJaccard similarity measure. These sentences have been treated as labeled data.A\nSupport Vector Machine (SVM) classifier is trained on this labeled data to\nextract data elements of interests from a new article. We conducted experiments\non Cochrane Database systematic reviews related to congestive heart failure\nusing inclusion criteria as an example data element. The empirical results show\nthat the proposed system automatically identifies sentences containing the data\nelement of interest with a high recall (93.75%) and reasonable precision\n(27.05% - which means the reviewers have to read only 3.7 sentences on\naverage). The empirical results suggest that the tool is retrieving valuable\ninformation from the reference articles, even when it is time-consuming to\nidentify them manually. Thus we hope that the tool will be useful for automatic\ndata extraction from biomedical research publications. The future scope of this\nwork is to generalize this information framework for all types of systematic\nreviews.", "published": "2016-06-21 04:56:33", "link": "http://arxiv.org/abs/1606.06424v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Question Relevance in VQA: Identifying Non-Visual And False-Premise\n  Questions", "abstract": "Visual Question Answering (VQA) is the task of answering natural-language\nquestions about images. We introduce the novel problem of determining the\nrelevance of questions to images in VQA. Current VQA models do not reason about\nwhether a question is even related to the given image (e.g. What is the capital\nof Argentina?) or if it requires information from external resources to answer\ncorrectly. This can break the continuity of a dialogue in human-machine\ninteraction. Our approaches for determining relevance are composed of two\nstages. Given an image and a question, (1) we first determine whether the\nquestion is visual or not, (2) if visual, we determine whether the question is\nrelevant to the given image or not. Our approaches, based on LSTM-RNNs, VQA\nmodel uncertainty, and caption-question similarity, are able to outperform\nstrong baselines on both relevance tasks. We also present human studies showing\nthat VQA models augmented with such question relevance reasoning are perceived\nas more intelligent, reasonable, and human-like.", "published": "2016-06-21 15:38:27", "link": "http://arxiv.org/abs/1606.06622v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
