{"title": "Predicting Role Relevance with Minimal Domain Expertise in a Financial\n  Domain", "abstract": "Word embeddings have made enormous inroads in recent years in a wide variety\nof text mining applications. In this paper, we explore a word embedding-based\narchitecture for predicting the relevance of a role between two financial\nentities within the context of natural language sentences. In this extended\nabstract, we propose a pooled approach that uses a collection of sentences to\ntrain word embeddings using the skip-gram word2vec architecture. We use the\nword embeddings to obtain context vectors that are assigned one or more labels\nbased on manual annotations. We train a machine learning classifier using the\nlabeled context vectors, and use the trained classifier to predict contextual\nrole relevance on test data. Our approach serves as a good minimal-expertise\nbaseline for the task as it is simple and intuitive, uses open-source modules,\nrequires little feature crafting effort and performs well across roles.", "published": "2017-04-19 00:55:23", "link": "http://arxiv.org/abs/1704.05571v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dependency resolution and semantic mining using Tree Adjoining Grammars\n  for Tamil Language", "abstract": "Tree adjoining grammars (TAGs) provide an ample tool to capture syntax of\nmany Indian languages. Tamil represents a special challenge to computational\nformalisms as it has extensive agglutinative morphology and a comparatively\ndifficult argument structure. Modelling Tamil syntax and morphology using TAG\nis an interesting problem which has not been in focus even though TAGs are over\n4 decades old, since its inception. Our research with Tamil TAGs have shown us\nthat we can not only represent syntax of the language, but to an extent mine\nout semantics through dependency resolution of the sentence. But in order to\ndemonstrate this phenomenal property, we need to parse Tamil language sentences\nusing TAGs we have built and through parsing obtain a derivation we could use\nto resolve dependencies, thus proving the semantic property. We use an in-house\ndeveloped pseudo lexical TAG chart parser; algorithm given by Schabes and Joshi\n(1988), for generating derivations of sentences. We do not use any statistics\nto rank out ambiguous derivations but rather use all of them to understand the\nmentioned semantic relation with in TAGs for Tamil. We shall also present a\nbrief parser analysis for the completeness of our discussions.", "published": "2017-04-19 05:02:05", "link": "http://arxiv.org/abs/1704.05611v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Multi-task Learning for Text Classification", "abstract": "Neural network models have shown their promising opportunities for multi-task\nlearning, which focus on learning the shared layers to extract the common and\ntask-invariant features. However, in most existing approaches, the extracted\nshared features are prone to be contaminated by task-specific features or the\nnoise brought by other tasks. In this paper, we propose an adversarial\nmulti-task learning framework, alleviating the shared and private latent\nfeature spaces from interfering with each other. We conduct extensive\nexperiments on 16 different text classification tasks, which demonstrates the\nbenefits of our approach. Besides, we show that the shared knowledge learned by\nour proposed model can be regarded as off-the-shelf knowledge and easily\ntransferred to new tasks. The datasets of all 16 tasks are publicly available\nat \\url{http://nlp.fudan.edu.cn/data/}", "published": "2017-04-19 14:17:25", "link": "http://arxiv.org/abs/1704.05742v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Redefining Context Windows for Word Embedding Models: An Experimental\n  Study", "abstract": "Distributional semantic models learn vector representations of words through\nthe contexts they occur in. Although the choice of context (which often takes\nthe form of a sliding window) has a direct influence on the resulting\nembeddings, the exact role of this model component is still not fully\nunderstood. This paper presents a systematic analysis of context windows based\non a set of four distinct hyper-parameters. We train continuous Skip-Gram\nmodels on two English-language corpora for various combinations of these\nhyper-parameters, and evaluate them on both lexical similarity and analogy\ntasks. Notable experimental results are the positive impact of cross-sentential\ncontexts and the surprisingly good performance of right-context windows.", "published": "2017-04-19 15:41:34", "link": "http://arxiv.org/abs/1704.05781v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Global Relation Embedding for Relation Extraction", "abstract": "We study the problem of textual relation embedding with distant supervision.\nTo combat the wrong labeling problem of distant supervision, we propose to\nembed textual relations with global statistics of relations, i.e., the\nco-occurrence statistics of textual and knowledge base relations collected from\nthe entire corpus. This approach turns out to be more robust to the training\nnoise introduced by distant supervision. On a popular relation extraction\ndataset, we show that the learned textual relation embedding can be used to\naugment existing relation extraction models and significantly improve their\nperformance. Most remarkably, for the top 1,000 relational facts discovered by\nthe best existing model, the precision can be improved from 83.9% to 89.3%.", "published": "2017-04-19 23:54:46", "link": "http://arxiv.org/abs/1704.05958v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Answering Complex Questions Using Open Information Extraction", "abstract": "While there has been substantial progress in factoid question-answering (QA),\nanswering complex questions remains challenging, typically requiring both a\nlarge body of knowledge and inference techniques. Open Information Extraction\n(Open IE) provides a way to generate semi-structured knowledge for QA, but to\ndate such knowledge has only been used to answer simple questions with\nretrieval-based methods. We overcome this limitation by presenting a method for\nreasoning with Open IE knowledge, allowing more complex questions to be\nhandled. Using a recently proposed support graph optimization framework for QA,\nwe develop a new inference model for Open IE, in particular one that can work\neffectively with multiple short facts, noise, and the relational structure of\ntuples. Our model significantly outperforms a state-of-the-art structured\nsolver on complex questions of varying difficulty, while also removing the\nreliance on manually curated knowledge.", "published": "2017-04-19 01:07:56", "link": "http://arxiv.org/abs/1704.05572v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Understanding Task Design Trade-offs in Crowdsourced Paraphrase\n  Collection", "abstract": "Linguistically diverse datasets are critical for training and evaluating\nrobust machine learning systems, but data collection is a costly process that\noften requires experts. Crowdsourcing the process of paraphrase generation is\nan effective means of expanding natural language datasets, but there has been\nlimited analysis of the trade-offs that arise when designing tasks. In this\npaper, we present the first systematic study of the key factors in\ncrowdsourcing paraphrase collection. We consider variations in instructions,\nincentives, data domains, and workflows. We manually analyzed paraphrases for\ncorrectness, grammaticality, and linguistic diversity. Our observations provide\nnew insight into the trade-offs between accuracy and diversity in crowd\nresponses that arise as a result of task design, providing guidance for future\nparaphrase generation procedures.", "published": "2017-04-19 14:41:21", "link": "http://arxiv.org/abs/1704.05753v2", "categories": ["cs.CL", "cs.HC", "I.2.7; H.5.0"], "primary_category": "cs.CL"}
{"title": "A Large Self-Annotated Corpus for Sarcasm", "abstract": "We introduce the Self-Annotated Reddit Corpus (SARC), a large corpus for\nsarcasm research and for training and evaluating systems for sarcasm detection.\nThe corpus has 1.3 million sarcastic statements -- 10 times more than any\nprevious dataset -- and many times more instances of non-sarcastic statements,\nallowing for learning in both balanced and unbalanced label regimes. Each\nstatement is furthermore self-annotated -- sarcasm is labeled by the author,\nnot an independent annotator -- and provided with user, topic, and conversation\ncontext. We evaluate the corpus for accuracy, construct benchmarks for sarcasm\ndetection, and evaluate baseline methods.", "published": "2017-04-19 02:01:39", "link": "http://arxiv.org/abs/1704.05579v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Multi-View Networks for Text Classification", "abstract": "We propose a multi-view network for text classification. Our method\nautomatically creates various views of its input text, each taking the form of\nsoft attention weights that distribute the classifier's focus among a set of\nbase features. For a bag-of-words representation, each view focuses on a\ndifferent subset of the text's words. Aggregating many such views results in a\nmore discriminative and robust representation. Through a novel architecture\nthat both stacks and concatenates views, we produce a network that emphasizes\nboth depth and width, allowing training to converge quickly. Using our\nmulti-view architecture, we establish new state-of-the-art accuracies on two\nbenchmark tasks.", "published": "2017-04-19 19:33:38", "link": "http://arxiv.org/abs/1704.05907v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion", "abstract": "Knowledge bases are important resources for a variety of natural language\nprocessing tasks but suffer from incompleteness. We propose a novel embedding\nmodel, \\emph{ITransF}, to perform knowledge base completion. Equipped with a\nsparse attention mechanism, ITransF discovers hidden concepts of relations and\ntransfer statistical strength through the sharing of concepts. Moreover, the\nlearned associations between relations and concepts, which are represented by\nsparse attention vectors, can be interpreted easily. We evaluate ITransF on two\nbenchmark datasets---WN18 and FB15k for knowledge base completion and obtains\nimprovements on both the mean rank and Hits@10 metrics, over all baselines that\ndo not use additional information.", "published": "2017-04-19 19:35:54", "link": "http://arxiv.org/abs/1704.05908v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
