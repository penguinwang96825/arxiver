{"title": "Effectiveness of Self Normalizing Neural Networks for Text\n  Classification", "abstract": "Self Normalizing Neural Networks(SNN) proposed on Feed Forward Neural\nNetworks(FNN) outperform regular FNN architectures in various machine learning\ntasks. Particularly in the domain of Computer Vision, the activation function\nScaled Exponential Linear Units (SELU) proposed for SNNs, perform better than\nother non linear activations such as ReLU. The goal of SNN is to produce a\nnormalized output for a normalized input. Established neural network\narchitectures like feed forward networks and Convolutional Neural Networks(CNN)\nlack the intrinsic nature of normalizing outputs. Hence, requiring additional\nlayers such as Batch Normalization. Despite the success of SNNs, their\ncharacteristic features on other network architectures like CNN haven't been\nexplored, especially in the domain of Natural Language Processing. In this\npaper we aim to show the effectiveness of proposed, Self Normalizing\nConvolutional Neural Networks(SCNN) on text classification. We analyze their\nperformance with the standard CNN architecture used on several text\nclassification datasets. Our experiments demonstrate that SCNN achieves\ncomparable results to standard CNN model with significantly fewer parameters.\nFurthermore it also outperforms CNN with equal number of parameters.", "published": "2019-05-03 18:38:39", "link": "http://arxiv.org/abs/1905.01338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Relatedness on Stack Overflow: The Task, Dataset, and\n  Corpus-inspired Models", "abstract": "Domain-specific community question answering is becoming an integral part of\nprofessions. Finding related questions and answers in these communities can\nsignificantly improve the effectiveness and efficiency of information seeking.\nStack Overflow is one of the most popular communities that is being used by\nmillions of programmers. In this paper, we analyze the problem of predicting\nknowledge unit (question thread) relatedness in Stack Overflow. In particular,\nwe formulate the question relatedness task as a multi-class classification\nproblem with four degrees of relatedness. We present a large-scale dataset with\nmore than 300K pairs. To the best of our knowledge, this dataset is the largest\ndomain-specific dataset for Question-Question relatedness. We present the steps\nthat we took to collect, clean, process, and assure the quality of the dataset.\nThe proposed dataset Stack Overflow is a useful resource to develop novel\nsolutions, specifically data-hungry neural network models, for the prediction\nof relatedness in technical community question-answering forums. We adopt a\nneural network architecture and a traditional model for this task that\neffectively utilize information from different parts of knowledge units to\ncompute the relatedness between them. These models can be used to benchmark\nnovel models, as they perform well in our task and in a closely similar task.", "published": "2019-05-03 01:45:50", "link": "http://arxiv.org/abs/1905.01966v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Auditing ImageNet: Towards a Model-driven Framework for Annotating\n  Demographic Attributes of Large-Scale Image Datasets", "abstract": "The ImageNet dataset ushered in a flood of academic and industry interest in\ndeep learning for computer vision applications. Despite its significant impact,\nthere has not been a comprehensive investigation into the demographic\nattributes of images contained within the dataset. Such a study could lead to\nnew insights on inherent biases within ImageNet, particularly important given\nit is frequently used to pretrain models for a wide variety of computer vision\ntasks. In this work, we introduce a model-driven framework for the automatic\nannotation of apparent age and gender attributes in large-scale image datasets.\nUsing this framework, we conduct the first demographic audit of the 2012\nImageNet Large Scale Visual Recognition Challenge (ILSVRC) subset of ImageNet\nand the \"person\" hierarchical category of ImageNet. We find that 41.62% of\nfaces in ILSVRC appear as female, 1.71% appear as individuals above the age of\n60, and males aged 15 to 29 account for the largest subgroup with 27.11%. We\nnote that the presented model-driven framework is not fair for all\nintersectional groups, so annotation are subject to bias. We present this work\nas the starting point for future development of unbiased annotation models and\nfor the study of downstream effects of imbalances in the demographics of\nImageNet. Code and annotations are available at:\nhttp://bit.ly/ImageNetDemoAudit", "published": "2019-05-03 19:33:02", "link": "http://arxiv.org/abs/1905.01347v2", "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Personalized Query Auto-Completion Through a Lightweight Representation\n  of the User Context", "abstract": "Query Auto-Completion (QAC) is a widely used feature in many domains,\nincluding web and eCommerce search, suggesting full queries based on a prefix\ntyped by the user. QAC has been extensively studied in the literature in the\nrecent years, and it has been consistently shown that adding personalization\nfeatures can significantly improve the performance of QAC. In this work we\npropose a novel method for personalized QAC that uses lightweight embeddings\nlearnt through fastText. We construct an embedding for the user context\nqueries, which are the last few queries issued by the user. We also use the\nsame model to get the embedding for the candidate queries to be ranked. We\nintroduce ranking features that compute the distance between the candidate\nqueries and the context queries in the embedding space. These features are then\ncombined with other commonly used QAC ranking features to learn a ranking\nmodel. We apply our method to a large eCommerce search engine (eBay) and show\nthat the ranker with our proposed feature significantly outperforms the\nbaselines on all of the offline metrics measured, which includes Mean\nReciprocal Rank (MRR), Success Rate (SR), Mean Average Precision (MAP), and\nNormalized Discounted Cumulative Gain (NDCG). Our baselines include the Most\nPopular Completion (MPC) model as well as a ranking model without our proposed\nfeatures. The ranking model with the proposed features results in a $20-30\\%$\nimprovement over the MPC model on all metrics. We obtain up to a $5\\%$\nimprovement over the baseline ranking model for all the sessions, which goes up\nto about $10\\%$ when we restrict to sessions that contain the user context.\nMoreover, our proposed features also significantly outperform text based\npersonalization features studied in the literature before, and adding text\nbased features on top of our proposed embedding based features results only in\nminor improvements.", "published": "2019-05-03 23:28:18", "link": "http://arxiv.org/abs/1905.01386v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Meeting Transcription Using Virtual Microphone Arrays", "abstract": "We describe a system that generates speaker-annotated transcripts of meetings\nby using a virtual microphone array, a set of spatially distributed\nasynchronous recording devices such as laptops and mobile phones. The system is\ncomposed of continuous audio stream alignment, blind beamforming, speech\nrecognition, speaker diarization using prior speaker information, and system\ncombination. When utilizing seven input audio streams, our system achieves a\nword error rate (WER) of 22.3% and comes within 3% of the close-talking\nmicrophone WER on the non-overlapping speech segments. The speaker-attributed\nWER (SAWER) is 26.7%. The relative gains in SAWER over the single-device system\nare 14.8%, 20.3%, and 22.4% for three, five, and seven microphones,\nrespectively. The presented system achieves a 13.6% diarization error rate when\n10% of the speech duration contains more than one speaker. The contribution of\neach component to the overall performance is also investigated, and we validate\nthe system with experiments on the NIST RT-07 conference meeting test set.", "published": "2019-05-03 10:30:17", "link": "http://arxiv.org/abs/1905.02545v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Statistically Principled and Computationally Efficient Approach to\n  Speech Enhancement using Variational Autoencoders", "abstract": "Recent studies have explored the use of deep generative models of speech\nspectra based of variational autoencoders (VAEs), combined with unsupervised\nnoise models, to perform speech enhancement. These studies developed iterative\nalgorithms involving either Gibbs sampling or gradient descent at each step,\nmaking them computationally expensive. This paper proposes a variational\ninference method to iteratively estimate the power spectrogram of the clean\nspeech. Our main contribution is the analytical derivation of the variational\nsteps in which the en-coder of the pre-learned VAE can be used to estimate the\nvaria-tional approximation of the true posterior distribution, using the very\nsame assumption made to train VAEs. Experiments show that the proposed method\nproduces results on par with the afore-mentioned iterative methods using\nsampling, while decreasing the computational cost by a factor 36 to reach a\ngiven performance .", "published": "2019-05-03 14:46:47", "link": "http://arxiv.org/abs/1905.01209v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Deep Tensor Factorization for Spatially-Aware Scene Decomposition", "abstract": "We propose a completely unsupervised method to understand audio scenes\nobserved with random microphone arrangements by decomposing the scene into its\nconstituent sources and their relative presence in each microphone. To this\nend, we formulate a neural network architecture that can be interpreted as a\nnonnegative tensor factorization of a multi-channel audio recording. By\nclustering on the learned network parameters corresponding to channel content,\nwe can learn sources' individual spectral dictionaries and their activation\npatterns over time. Our method allows us to leverage deep learning advances\nlike end-to-end training, while also allowing stochastic minibatch training so\nthat we can feasibly decompose realistic audio scenes that are intractable to\ndecompose using standard methods. This neural network architecture is easily\nextensible to other kinds of tensor factorizations.", "published": "2019-05-03 23:58:56", "link": "http://arxiv.org/abs/1905.01391v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
