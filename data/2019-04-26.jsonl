{"title": "Transformers with convolutional context for ASR", "abstract": "The recent success of transformer networks for neural machine translation and\nother NLP tasks has led to a surge in research work trying to apply it for\nspeech recognition. Recent efforts studied key research questions around ways\nof combining positional embedding with speech features, and stability of\noptimization for large scale learning of transformer networks. In this paper,\nwe propose replacing the sinusoidal positional embedding for transformers with\nconvolutionally learned input representations. These contextual representations\nprovide subsequent transformer blocks with relative positional information\nneeded for discovering long-range relationships between local concepts. The\nproposed system has favorable optimization characteristics where our reported\nresults are produced with fixed learning rate of 1.0 and no warmup steps. The\nproposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech\n``test clean'' and ``test other'' subsets when no extra LM text is provided.", "published": "2019-04-26 03:00:19", "link": "http://arxiv.org/abs/1904.11660v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are We Consistently Biased? Multidimensional Analysis of Biases in\n  Distributional Word Vectors", "abstract": "Word embeddings have recently been shown to reflect many of the pronounced\nsocietal biases (e.g., gender bias or racial bias). Existing studies are,\nhowever, limited in scope and do not investigate the consistency of biases\nacross relevant dimensions like embedding models, types of texts, and different\nlanguages. In this work, we present a systematic study of biases encoded in\ndistributional word vector spaces: we analyze how consistent the bias effects\nare across languages, corpora, and embedding models. Furthermore, we analyze\nthe cross-lingual biases encoded in bilingual embedding spaces, indicative of\nthe effects of bias transfer encompassed in cross-lingual transfer of NLP\nmodels. Our study yields some unexpected findings, e.g., that biases can be\nemphasized or downplayed by different embedding models or that user-generated\ncontent may be less biased than encyclopedic text. We hope our work catalyzes\nbias research in NLP and informs the development of bias reduction techniques.", "published": "2019-04-26 11:56:35", "link": "http://arxiv.org/abs/1904.11783v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextualized Word Embeddings Enhanced Event Temporal Relation\n  Extraction for Story Understanding", "abstract": "Learning causal and temporal relationships between events is an important\nstep towards deeper story and commonsense understanding. Though there are\nabundant datasets annotated with event relations for story comprehension, many\nhave no empirical results associated with them. In this work, we establish\nstrong baselines for event temporal relation extraction on two under-explored\nstory narrative datasets: Richer Event Description (RED) and Causal and\nTemporal Relation Scheme (CaTeRS). To the best of our knowledge, these are the\nfirst results reported on these two datasets. We demonstrate that neural\nnetwork-based models can outperform some strong traditional linguistic\nfeature-based models. We also conduct comparative studies to show the\ncontribution of adopting contextualized word embeddings (BERT) for event\ntemporal relation extraction from stories. Detailed analyses are offered to\nbetter understand the results.", "published": "2019-04-26 17:21:59", "link": "http://arxiv.org/abs/1904.11942v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One-Shot Learning for Text-to-SQL Generation", "abstract": "Most deep learning approaches for text-to-SQL generation are limited to the\nWikiSQL dataset, which only supports very simple queries. Recently,\ntemplate-based and sequence-to-sequence approaches were proposed to support\ncomplex queries, which contain join queries, nested queries, and other types.\nHowever, Finegan-Dollak et al. (2018) demonstrated that both the approaches\nlack the ability to generate SQL of unseen templates. In this paper, we propose\na template-based one-shot learning model for the text-to-SQL generation so that\nthe model can generate SQL of an untrained template based on a single example.\nFirst, we classify the SQL template using the Matching Network that is\naugmented by our novel architecture Candidate Search Network. Then, we fill the\nvariable slots in the predicted template using the Pointer Network. We show\nthat our model outperforms state-of-the-art approaches for various text-to-SQL\ndatasets in two aspects: 1) the SQL generation accuracy for the trained\ntemplates, and 2) the adaptability to the unseen SQL templates based on a\nsingle example without any additional training.", "published": "2019-04-26 06:29:29", "link": "http://arxiv.org/abs/1905.11499v1", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Fake News Early Detection: An Interdisciplinary Study", "abstract": "Massive dissemination of fake news and its potential to erode democracy has\nincreased the demand for accurate fake news detection. Recent advancements in\nthis area have proposed novel techniques that aim to detect fake news by\nexploring how it propagates on social networks. Nevertheless, to detect fake\nnews at an early stage, i.e., when it is published on a news outlet but not yet\nspread on social media, one cannot rely on news propagation information as it\ndoes not exist. Hence, there is a strong need to develop approaches that can\ndetect fake news by focusing on news content. In this paper, a theory-driven\nmodel is proposed for fake news detection. The method investigates news content\nat various levels: lexicon-level, syntax-level, semantic-level and\ndiscourse-level. We represent news at each level, relying on well-established\ntheories in social and forensic psychology. Fake news detection is then\nconducted within a supervised machine learning framework. As an\ninterdisciplinary research, our work explores potential fake news patterns,\nenhances the interpretability in fake news feature engineering, and studies the\nrelationships among fake news, deception/disinformation, and clickbaits.\nExperiments conducted on two real-world datasets indicate the proposed method\ncan outperform the state-of-the-art and enable fake news early detection when\nthere is limited content information.", "published": "2019-04-26 05:52:05", "link": "http://arxiv.org/abs/1904.11679v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Think Again Networks and the Delta Loss", "abstract": "This short paper introduces an abstraction called Think Again Networks\n(ThinkNet) which can be applied to any state-dependent function (such as a\nrecurrent neural network).", "published": "2019-04-26 12:57:25", "link": "http://arxiv.org/abs/1904.11816v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Chinese Word Segmentation with Lexicon and Unlabeled Data via\n  Posterior Regularization", "abstract": "Existing methods for CWS usually rely on a large number of labeled sentences\nto train word segmentation models, which are expensive and time-consuming to\nannotate. Luckily, the unlabeled data is usually easy to collect and many\nhigh-quality Chinese lexicons are off-the-shelf, both of which can provide\nuseful information for CWS. In this paper, we propose a neural approach for\nChinese word segmentation which can exploit both lexicon and unlabeled data.\nOur approach is based on a variant of posterior regularization algorithm, and\nthe unlabeled data and lexicon are incorporated into model training as indirect\nsupervision by regularizing the prediction space of CWS models. Extensive\nexperiments on multiple benchmark datasets in both in-domain and cross-domain\nscenarios validate the effectiveness of our approach.", "published": "2019-04-26 08:21:08", "link": "http://arxiv.org/abs/1905.01963v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Neural Chinese Named Entity Recognition via CNN-LSTM-CRF and Joint\n  Training with Word Segmentation", "abstract": "Chinese named entity recognition (CNER) is an important task in Chinese\nnatural language processing field. However, CNER is very challenging since\nChinese entity names are highly context-dependent. In addition, Chinese texts\nlack delimiters to separate words, making it difficult to identify the boundary\nof entities. Besides, the training data for CNER in many domains is usually\ninsufficient, and annotating enough training data for CNER is very expensive\nand time-consuming. In this paper, we propose a neural approach for CNER.\nFirst, we introduce a CNN-LSTM-CRF neural architecture to capture both local\nand long-distance contexts for CNER. Second, we propose a unified framework to\njointly train CNER and word segmentation models in order to enhance the ability\nof CNER model in identifying entity boundaries. Third, we introduce an\nautomatic method to generate pseudo labeled samples from existing labeled data\nwhich can enrich the training data. Experiments on two benchmark datasets show\nthat our approach can effectively improve the performance of Chinese named\nentity recognition, especially when training data is insufficient.", "published": "2019-04-26 08:09:56", "link": "http://arxiv.org/abs/1905.01964v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Speaker Sincerity Detection based on Covariance Feature Vectors and\n  Ensemble Methods", "abstract": "Automatic measuring of speaker sincerity degree is a novel research problem\nin computational paralinguistics. This paper proposes covariance-based feature\nvectors to model speech and ensembles of support vector regressors to estimate\nthe degree of sincerity of a speaker. The elements of each covariance vector\nare pairwise statistics between the short-term feature components. These\nfeatures are used alone as well as in combination with the ComParE acoustic\nfeature set. The experimental results on the development set of the Sincerity\nSpeech Corpus using a cross-validation procedure have shown an 8.1% relative\nimprovement in the Spearman's correlation coefficient over the baseline system.", "published": "2019-04-26 01:42:41", "link": "http://arxiv.org/abs/1904.11641v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Copy mechanism and tailored training for character-based data-to-text\n  generation", "abstract": "In the last few years, many different methods have been focusing on using\ndeep recurrent neural networks for natural language generation. The most widely\nused sequence-to-sequence neural methods are word-based: as such, they need a\npre-processing step called delexicalization (conversely, relexicalization) to\ndeal with uncommon or unknown words. These forms of processing, however, give\nrise to models that depend on the vocabulary used and are not completely\nneural.\n  In this work, we present an end-to-end sequence-to-sequence model with\nattention mechanism which reads and generates at a character level, no longer\nrequiring delexicalization, tokenization, nor even lowercasing. Moreover, since\ncharacters constitute the common \"building blocks\" of every text, it also\nallows a more general approach to text generation, enabling the possibility to\nexploit transfer learning for training. These skills are obtained thanks to two\nmajor features: (i) the possibility to alternate between the standard\ngeneration mechanism and a copy one, which allows to directly copy input facts\nto produce outputs, and (ii) the use of an original training pipeline that\nfurther improves the quality of the generated texts.\n  We also introduce a new dataset called E2E+, designed to highlight the\ncopying capabilities of character-based models, that is a modified version of\nthe well-known E2E dataset used in the E2E Challenge. We tested our model\naccording to five broadly accepted metrics (including the widely used BLEU),\nshowing that it yields competitive performance with respect to both\ncharacter-based and word-based approaches.", "published": "2019-04-26 13:33:56", "link": "http://arxiv.org/abs/1904.11838v4", "categories": ["cs.LG", "cs.CL", "cs.NE", "stat.ML", "68T50"], "primary_category": "cs.LG"}
{"title": "CoachAI: A Conversational Agent Assisted Health Coaching Platform", "abstract": "Poor lifestyle represents a health risk factor and is the leading cause of\nmorbidity and chronic conditions. The impact of poor lifestyle can be\nsignificantly altered by individual behavior change. Although the current shift\nin healthcare towards a long lasting modifiable behavior, however, with\nincreasing caregiver workload and individuals' continuous needs of care, there\nis a need to ease caregiver's work while ensuring continuous interaction with\nusers. This paper describes the design and validation of CoachAI, a\nconversational agent assisted health coaching system to support health\nintervention delivery to individuals and groups. CoachAI instantiates a text\nbased healthcare chatbot system that bridges the remote human coach and the\nusers. This research provides three main contributions to the preventive\nhealthcare and healthy lifestyle promotion: (1) it presents the conversational\nagent to aid the caregiver; (2) it aims to decrease caregiver's workload and\nenhance care given to users, by handling (automating) repetitive caregiver\ntasks; and (3) it presents a domain independent mobile health conversational\nagent for health intervention delivery. We will discuss our approach and\nanalyze the results of a one month validation study on physical activity,\nhealthy diet and stress management.", "published": "2019-04-26 17:44:04", "link": "http://arxiv.org/abs/1904.11961v1", "categories": ["cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.AI"}
{"title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and\n  Sentences From Natural Supervision", "abstract": "We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns\nvisual concepts, words, and semantic parsing of sentences without explicit\nsupervision on any of them; instead, our model learns by simply looking at\nimages and reading paired questions and answers. Our model builds an\nobject-based scene representation and translates sentences into executable,\nsymbolic programs. To bridge the learning of two modules, we use a\nneuro-symbolic reasoning module that executes these programs on the latent\nscene representation. Analogical to human concept learning, the perception\nmodule learns visual concepts based on the language description of the object\nbeing referred to. Meanwhile, the learned visual concepts facilitate learning\nnew words and parsing new sentences. We use curriculum learning to guide the\nsearching over the large compositional space of images and language. Extensive\nexperiments demonstrate the accuracy and efficiency of our model on learning\nvisual concepts, word representations, and semantic parsing of sentences.\nFurther, our method allows easy generalization to new object attributes,\ncompositions, language concepts, scenes and questions, and even new program\ndomains. It also empowers applications including visual question answering and\nbidirectional image-text retrieval.", "published": "2019-04-26 06:50:54", "link": "http://arxiv.org/abs/1904.12584v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Statistical feature embedding for heart sound classification", "abstract": "Cardiovascular Disease (CVD) is considered as one of the principal causes of\ndeath in the world. Over recent years, this field of study has attracted\nresearchers' attention to investigate heart sounds' patterns for disease\ndiagnostics. In this study, an approach is proposed for normal/abnormal heart\nsound classification on the Physionet challenge 2016 dataset. For the first\ntime, a fixed-length feature vector; called i-vector; is extracted from each\nheart sound using Mel Frequency Cepstral Coefficient (MFCC) features.\nAfterwards, Principal Component Analysis (PCA) transform and Variational\nAutoencoder (VAE) are applied on the i-vector to achieve dimension reduction.\nEventually, the reduced size vector is fed to Gaussian Mixture Models (GMMs)\nand Support Vector Machine (SVM) for classification purpose. Experimental\nresults demonstrate the proposed method could achieve a performance improvement\nof 16% based on Modified Accuracy (MAcc) compared with the baseline system on\nthe Physoinet dataset.", "published": "2019-04-26 16:07:18", "link": "http://arxiv.org/abs/1904.11914v3", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Improving Deep Speech Denoising by Noisy2Noisy Signal Mapping", "abstract": "Existing deep learning-based speech denoising approaches require clean speech\nsignals to be available for training. This paper presents a deep learning-based\napproach to improve speech denoising in real-world audio environments by not\nrequiring the availability of clean speech signals in a self-supervised manner.\nA fully convolutional neural network is trained by using two noisy realizations\nof the same speech signal, one used as the input and the other as the output of\nthe network. Extensive experimentations are conducted to show the superiority\nof the developed deep speech denoising approach over the conventional\nsupervised deep speech denoising approach based on four commonly used\nperformance metrics and also based on actual field-testing outcomes.", "published": "2019-04-26 22:45:44", "link": "http://arxiv.org/abs/1904.12069v2", "categories": ["eess.AS", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
