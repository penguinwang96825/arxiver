{"title": "ArguGPT: evaluating, understanding and identifying argumentative essays\n  generated by GPT models", "abstract": "AI generated content (AIGC) presents considerable challenge to educators\naround the world. Instructors need to be able to detect such text generated by\nlarge language models, either with the naked eye or with the help of some\ntools. There is also growing need to understand the lexical, syntactic and\nstylistic features of AIGC. To address these challenges in English language\nteaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative\nessays generated by 7 GPT models in response to essay prompts from three\nsources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing\ntasks. Machine-generated texts are paired with roughly equal number of\nhuman-written essays with three score levels matched in essay prompts. We then\nhire English instructors to distinguish machine essays from human ones. Results\nshow that when first exposed to machine-generated essays, the instructors only\nhave an accuracy of 61% in detecting them. But the number rises to 67% after\none round of minimal self-training. Next, we perform linguistic analyses of\nthese essays, which show that machines produce sentences with more complex\nsyntactic structures while human essays tend to be lexically more complex.\nFinally, we test existing AIGC detectors and build our own detectors using SVMs\nand RoBERTa. Results suggest that a RoBERTa fine-tuned with the training set of\nArguGPT achieves above 90% accuracy in both essay- and sentence-level\nclassification. To the best of our knowledge, this is the first comprehensive\nanalysis of argumentative essays produced by generative large language models.\nMachine-authored essays in ArguGPT and our models will be made publicly\navailable at https://github.com/huhailinguist/ArguGPT", "published": "2023-04-16 01:50:26", "link": "http://arxiv.org/abs/2304.07666v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Clustering Framework for Unsupervised and Semi-supervised New Intent\n  Discovery", "abstract": "New intent discovery is of great value to natural language processing,\nallowing for a better understanding of user needs and providing friendly\nservices. However, most existing methods struggle to capture the complicated\nsemantics of discrete text representations when limited or no prior knowledge\nof labeled data is available. To tackle this problem, we propose a novel\nclustering framework, USNID, for unsupervised and semi-supervised new intent\ndiscovery, which has three key technologies. First, it fully utilizes\nunsupervised or semi-supervised data to mine shallow semantic similarity\nrelations and provide well-initialized representations for clustering. Second,\nit designs a centroid-guided clustering mechanism to address the issue of\ncluster allocation inconsistency and provide high-quality self-supervised\ntargets for representation learning. Third, it captures high-level semantics in\nunsupervised or semi-supervised data to discover fine-grained intent-wise\nclusters by optimizing both cluster-level and instance-level objectives. We\nalso propose an effective method for estimating the cluster number in\nopen-world scenarios without knowing the number of new intents beforehand.\nUSNID performs exceptionally well on several benchmark intent datasets,\nachieving new state-of-the-art results in unsupervised and semi-supervised new\nintent discovery and demonstrating robust performance with different cluster\nnumbers.", "published": "2023-04-16 05:30:42", "link": "http://arxiv.org/abs/2304.07699v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SikuGPT: A Generative Pre-trained Model for Intelligent Information\n  Processing of Ancient Texts from the Perspective of Digital Humanities", "abstract": "The rapid advance in artificial intelligence technology has facilitated the\nprosperity of digital humanities research. Against such backdrop, research\nmethods need to be transformed in the intelligent processing of ancient texts,\nwhich is a crucial component of digital humanities research, so as to adapt to\nnew development trends in the wave of AIGC. In this study, we propose a GPT\nmodel called SikuGPT based on the corpus of Siku Quanshu. The model's\nperformance in tasks such as intralingual translation and text classification\nexceeds that of other GPT-type models aimed at processing ancient texts.\nSikuGPT's ability to process traditional Chinese ancient texts can help promote\nthe organization of ancient information and knowledge services, as well as the\ninternational dissemination of Chinese ancient culture.", "published": "2023-04-16 13:25:24", "link": "http://arxiv.org/abs/2304.07778v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented\n  Instruction Tuning for Digital Human", "abstract": "In this paper, we present ChatPLUG, a Chinese open-domain dialogue system for\ndigital human applications that instruction finetunes on a wide range of\ndialogue tasks in a unified internet-augmented format. Different from other\nopen-domain dialogue models that focus on large-scale pre-training and scaling\nup model size or dialogue corpus, we aim to build a powerful and practical\ndialogue system for digital human with diverse skills and good multi-task\ngeneralization by internet-augmented instruction tuning. To this end, we first\nconduct large-scale pre-training on both common document corpus and dialogue\ndata with curriculum learning, so as to inject various world knowledge and\ndialogue abilities into ChatPLUG. Then, we collect a wide range of dialogue\ntasks spanning diverse features of knowledge, personality, multi-turn memory,\nand empathy, on which we further instruction tune \\modelname via unified\nnatural language instruction templates. External knowledge from an internet\nsearch is also used during instruction finetuning for alleviating the problem\nof knowledge hallucinations. We show that \\modelname outperforms\nstate-of-the-art Chinese dialogue systems on both automatic and human\nevaluation, and demonstrates strong multi-task generalization on a variety of\ntext understanding and generation tasks. In addition, we deploy \\modelname to\nreal-world applications such as Smart Speaker and Instant Message applications\nwith fast inference. Our models and code will be made publicly available on\nModelScope: https://modelscope.cn/models/damo/ChatPLUG-3.7B and Github:\nhttps://github.com/X-PLUG/ChatPLUG .", "published": "2023-04-16 18:16:35", "link": "http://arxiv.org/abs/2304.07849v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Better Instruction Following Language Models for Chinese:\n  Investigating the Impact of Training Data and Evaluation", "abstract": "Recently, significant public efforts have been directed towards developing\nlow-cost models with capabilities akin to ChatGPT, thereby fostering the growth\nof open-source conversational models. However, there remains a scarcity of\ncomprehensive and in-depth evaluations of these models' performance. In this\nstudy, we examine the influence of training data factors, including quantity,\nquality, and linguistic distribution, on model performance. Our analysis is\ngrounded in several publicly accessible, high-quality instruction datasets, as\nwell as our own Chinese multi-turn conversations. We assess various models\nusing a evaluation set of 1,000 samples, encompassing nine real-world\nscenarios. Our goal is to supplement manual evaluations with quantitative\nanalyses, offering valuable insights for the continued advancement of\nopen-source chat models. Furthermore, to enhance the performance and training\nand inference efficiency of models in the Chinese domain, we extend the\nvocabulary of LLaMA - the model with the closest open-source performance to\nproprietary language models like GPT-3 - and conduct secondary pre-training on\n3.4B Chinese words. We make our model, data, as well as code publicly\navailable.", "published": "2023-04-16 18:37:39", "link": "http://arxiv.org/abs/2304.07854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MisRoB\u00c6RTa: Transformers versus Misinformation", "abstract": "Misinformation is considered a threat to our democratic values and\nprinciples. The spread of such content on social media polarizes society and\nundermines public discourse by distorting public perceptions and generating\nsocial unrest while lacking the rigor of traditional journalism. Transformers\nand transfer learning proved to be state-of-the-art methods for multiple\nwell-known natural language processing tasks. In this paper, we propose\nMisRoB{\\AE}RTa, a novel transformer-based deep neural ensemble architecture for\nmisinformation detection. MisRoB{\\AE}RTa takes advantage of two transformers\n(BART \\& RoBERTa) to improve the classification performance. We also\nbenchmarked and evaluated the performances of multiple transformers on the task\nof misinformation detection. For training and testing, we used a large\nreal-world news articles dataset labeled with 10 classes, addressing two\nshortcomings in the current research: increasing the size of the dataset from\nsmall to large, and moving the focus of fake news detection from binary\nclassification to multi-class classification. For this dataset, we manually\nverified the content of the news articles to ensure that they were correctly\nlabeled. The experimental results show that the accuracy of transformers on the\nmisinformation detection problem was significantly influenced by the method\nemployed to learn the context, dataset size, and vocabulary dimension. We\nobserve empirically that the best accuracy performance among the classification\nmodels that use only one transformer is obtained by BART, while DistilRoBERTa\nobtains the best accuracy in the least amount of time required for fine-tuning\nand training. The proposed MisRoB{\\AE}RTa outperforms the other transformer\nmodels in the task of misinformation detection. To arrive at this conclusion,\nwe performed ample ablation and sensitivity testing with MisRoB{\\AE}RTa on two\ndatasets.", "published": "2023-04-16 12:14:38", "link": "http://arxiv.org/abs/2304.07759v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Evaluation of Neural SPARQL Query Generation from\n  Natural Language Questions", "abstract": "In recent years, the field of neural machine translation (NMT) for SPARQL\nquery generation has witnessed significant growth. Incorporating the copy\nmechanism with traditional encoder-decoder architectures and using pre-trained\nencoder-decoders and large language models have set new performance benchmarks.\nThis paper presents various experiments that replicate and expand upon recent\nNMT-based SPARQL generation studies, comparing pre-trained language models\n(PLMs), non-pre-trained language models (NPLMs), and large language models\n(LLMs), highlighting the impact of question annotation and the copy mechanism\nand testing various fine-tuning methods using LLMs. In particular, we provide a\nsystematic error analysis of the models and test their generalization ability.\nOur study demonstrates that the copy mechanism yields significant performance\nenhancements for most PLMs and NPLMs. Annotating the data is pivotal to\ngenerating correct URIs, with the \"tag-within\" strategy emerging as the most\neffective approach. Additionally, our findings reveal that the primary source\nof errors stems from incorrect URIs in SPARQL queries that are sometimes\nreplaced with hallucinated URIs when using base models. This does not happen\nusing the copy mechanism, but it sometimes leads to selecting wrong URIs among\ncandidates. Finally, the performance of the tested LLMs fell short of achieving\nthe desired outcomes.", "published": "2023-04-16 13:12:26", "link": "http://arxiv.org/abs/2304.07772v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Syntactic Complexity Identification, Measurement, and Reduction Through\n  Controlled Syntactic Simplification", "abstract": "Text simplification is one of the domains in Natural Language Processing\n(NLP) that offers an opportunity to understand the text in a simplified manner\nfor exploration. However, it is always hard to understand and retrieve\nknowledge from unstructured text, which is usually in the form of compound and\ncomplex sentences. There are state-of-the-art neural network-based methods to\nsimplify the sentences for improved readability while replacing words with\nplain English substitutes and summarising the sentences and paragraphs. In the\nKnowledge Graph (KG) creation process from unstructured text, summarising long\nsentences and substituting words is undesirable since this may lead to\ninformation loss. However, KG creation from text requires the extraction of all\npossible facts (triples) with the same mentions as in the text. In this work,\nwe propose a controlled simplification based on the factual information in a\nsentence, i.e., triple. We present a classical syntactic dependency-based\napproach to split and rephrase a compound and complex sentence into a set of\nsimplified sentences. This simplification process will retain the original\nwording with a simple structure of possible domain facts in each sentence,\ni.e., triples. The paper also introduces an algorithm to identify and measure a\nsentence's syntactic complexity (SC), followed by reduction through a\ncontrolled syntactic simplification process. Last, an experiment for a dataset\nre-annotation is also conducted through GPT3; we aim to publish this refined\ncorpus as a resource. This work is accepted and presented in International\nworkshop on Learning with Knowledge Graphs (IWLKG) at WSDM-2023 Conference. The\ncode and data is available at www.github.com/sallmanm/SynSim.", "published": "2023-04-16 13:13:58", "link": "http://arxiv.org/abs/2304.07774v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "It's All in the Embedding! Fake News Detection Using Document Embeddings", "abstract": "With the current shift in the mass media landscape from journalistic rigor to\nsocial media, personalized social media is becoming the new norm. Although the\ndigitalization progress of the media brings many advantages, it also increases\nthe risk of spreading disinformation, misinformation, and malformation through\nthe use of fake news. The emergence of this harmful phenomenon has managed to\npolarize society and manipulate public opinion on particular topics, e.g.,\nelections, vaccinations, etc. Such information propagated on social media can\ndistort public perceptions and generate social unrest while lacking the rigor\nof traditional journalism. Natural Language Processing and Machine Learning\ntechniques are essential for developing efficient tools that can detect fake\nnews. Models that use the context of textual data are essential for resolving\nthe fake news detection problem, as they manage to encode linguistic features\nwithin the vector representation of words. In this paper, we propose a new\napproach that uses document embeddings to build multiple models that accurately\nlabel news articles as reliable or fake. We also present a benchmark on\ndifferent architectures that detect fake news using binary or multi-labeled\nclassification. We evaluated the models on five large news corpora using\naccuracy, precision, and recall. We obtained better results than more complex\nstate-of-the-art Deep Neural Network models. We observe that the most important\nfactor for obtaining high accuracy is the document encoding, not the\nclassification model's complexity.", "published": "2023-04-16 13:30:06", "link": "http://arxiv.org/abs/2304.07781v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and\n  Dictionary-based Named Entity Recognition from Medical Text", "abstract": "Background Medical research generates millions of publications and it is a\ngreat challenge for researchers to utilize this information in full since its\nscale and complexity greatly surpasses human reading capabilities. Automated\ntext mining can help extract and connect information spread across this large\nbody of literature but this technology is not easily accessible to life\nscientists. Results Here, we developed an easy-to-use end-to-end pipeline for\ndeep learning- and dictionary-based named entity recognition (NER) of typical\nentities found in medical research articles, including diseases, cells,\nchemicals, genes/proteins, and species. The pipeline can access and process\nlarge medical research article collections (PubMed, CORD-19) or raw text and\nincorporates a series of deep learning models fine-tuned on the HUNER corpora\ncollection. In addition, the pipeline can perform dictionary-based NER related\nto COVID-19 and other medical topics. Users can also load their own NER models\nand dictionaries to include additional entities. The output consists of\npublication-ready ranked lists and graphs of detected entities and files\ncontaining the annotated texts. An associated script allows rapid inspection of\nthe results for specific entities of interest. As model use cases, the pipeline\nwas deployed on two collections of autophagy-related abstracts from PubMed and\non the CORD19 dataset, a collection of 764 398 research article abstracts\nrelated to COVID-19. Conclusions The NER pipeline we present is applicable in a\nvariety of medical research settings and makes customizable text mining\naccessible to life scientists.", "published": "2023-04-16 15:17:56", "link": "http://arxiv.org/abs/2304.07805v2", "categories": ["q-bio.QM", "cs.CL", "92-04, 92-08, 68T50", "J.3; I.2.7; H.3.3"], "primary_category": "q-bio.QM"}
{"title": "Neural Machine Translation For Low Resource Languages", "abstract": "Neural Machine translation is a challenging task due to the inherent complex\nnature and the fluidity that natural languages bring. Nonetheless, in recent\nyears, it has achieved state-of-the-art performance in several language pairs.\nAlthough, a lot of traction can be seen in the areas of multilingual neural\nmachine translation (MNMT) in the recent years, there are no comprehensive\nsurvey done to identify what approaches work well. The goal of this paper is to\ninvestigate the realm of low resource languages and build a Neural Machine\nTranslation model to achieve state-of-the-art results. The paper looks to build\nupon the mBART language model and explore strategies to augment it with various\nNLP and Deep Learning techniques like back translation and transfer learning.\nThis implementation tries to unpack the architecture of the NMT application and\ndetermine the different components which offers us opportunities to amend the\nsaid application within the purview of the low resource languages problem\nspace.", "published": "2023-04-16 19:27:48", "link": "http://arxiv.org/abs/2304.07869v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sabi\u00e1: Portuguese Large Language Models", "abstract": "As the capabilities of language models continue to advance, it is conceivable\nthat \"one-size-fits-all\" model will remain as the main paradigm. For instance,\ngiven the vast number of languages worldwide, many of which are low-resource,\nthe prevalent practice is to pretrain a single model on multiple languages. In\nthis paper, we add to the growing body of evidence that challenges this\npractice, demonstrating that monolingual pretraining on the target language\nsignificantly improves models already extensively trained on diverse corpora.\nMore specifically, we further pretrain GPT-J and LLaMA models on Portuguese\ntexts using 3% or less of their original pretraining budget. Few-shot\nevaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models\noutperform English-centric and multilingual counterparts by a significant\nmargin. Our best model, Sabi\\'a-65B, performs on par with GPT-3.5-turbo. By\nevaluating on datasets originally conceived in the target language as well as\ntranslated ones, we study the contributions of language-specific pretraining in\nterms of 1) capturing linguistic nuances and structures inherent to the target\nlanguage, and 2) enriching the model's knowledge about a domain or culture. Our\nresults indicate that the majority of the benefits stem from the\ndomain-specific knowledge acquired through monolingual pretraining.", "published": "2023-04-16 20:11:19", "link": "http://arxiv.org/abs/2304.07880v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Solving Math Word Problems by Combining Language Models With Symbolic\n  Solvers", "abstract": "Automatically generating high-quality step-by-step solutions to math word\nproblems has many applications in education. Recently, combining large language\nmodels (LLMs) with external tools to perform complex reasoning and calculation\nhas emerged as a promising direction for solving math word problems, but prior\napproaches such as Program-Aided Language model (PAL) are biased towards simple\nprocedural problems and less effective for problems that require declarative\nreasoning. We propose an approach that combines an LLM that can incrementally\nformalize word problems as a set of variables and equations with an external\nsymbolic solver that can solve the equations. Our approach achieves comparable\naccuracy to the original PAL on the GSM8K benchmark of math word problems and\noutperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more\nchallenging word problems extracted from Algebra textbooks. Our work highlights\nthe benefits of using declarative and incremental representations when\ninterfacing with an external tool for solving complex math word problems. Our\ndata and prompts are publicly available at\nhttps://github.com/joyheyueya/declarative-math-word-problem.", "published": "2023-04-16 04:16:06", "link": "http://arxiv.org/abs/2304.09102v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MLRegTest: A Benchmark for the Machine Learning of Regular Languages", "abstract": "Synthetic datasets constructed from formal languages allow fine-grained\nexamination of the learning and generalization capabilities of machine learning\nsystems for sequence classification. This article presents a new benchmark for\nmachine learning systems on sequence classification called MLRegTest, which\ncontains training, development, and test sets from 1,800 regular languages.\nDifferent kinds of formal languages represent different kinds of long-distance\ndependencies, and correctly identifying long-distance dependencies in sequences\nis a known challenge for ML systems to generalize successfully. MLRegTest\norganizes its languages according to their logical complexity (monadic second\norder, first order, propositional, or monomial expressions) and the kind of\nlogical literals (string, tier-string, subsequence, or combinations thereof).\nThe logical complexity and choice of literal provides a systematic way to\nunderstand different kinds of long-distance dependencies in regular languages,\nand therefore to understand the capacities of different ML systems to learn\nsuch long-distance dependencies. Finally, the performance of different neural\nnetworks (simple RNN, LSTM, GRU, transformer) on MLRegTest is examined. The\nmain conclusion is that performance depends significantly on the kind of test\nset, the class of language, and the neural network architecture.", "published": "2023-04-16 03:49:50", "link": "http://arxiv.org/abs/2304.07687v4", "categories": ["cs.LG", "cs.CL", "cs.FL"], "primary_category": "cs.LG"}
{"title": "VISAR: A Human-AI Argumentative Writing Assistant with Visual\n  Programming and Rapid Draft Prototyping", "abstract": "In argumentative writing, writers must brainstorm hierarchical writing goals,\nensure the persuasiveness of their arguments, and revise and organize their\nplans through drafting. Recent advances in large language models (LLMs) have\nmade interactive text generation through a chat interface (e.g., ChatGPT)\npossible. However, this approach often neglects implicit writing context and\nuser intent, lacks support for user control and autonomy, and provides limited\nassistance for sensemaking and revising writing plans. To address these\nchallenges, we introduce VISAR, an AI-enabled writing assistant system designed\nto help writers brainstorm and revise hierarchical goals within their writing\ncontext, organize argument structures through synchronized text editing and\nvisual programming, and enhance persuasiveness with argumentation spark\nrecommendations. VISAR allows users to explore, experiment with, and validate\ntheir writing plans using automatic draft prototyping. A controlled lab study\nconfirmed the usability and effectiveness of VISAR in facilitating the\nargumentative writing planning process.", "published": "2023-04-16 15:29:03", "link": "http://arxiv.org/abs/2304.07810v2", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "The language of sounds unheard: Exploring musical timbre semantics of\n  large language models", "abstract": "Semantic dimensions of sound have been playing a central role in\nunderstanding the nature of auditory sensory experience as well as the broader\nrelation between perception, language, and meaning. Accordingly, and given the\nrecent proliferation of large language models (LLMs), here we asked whether\nsuch models exhibit an organisation of perceptual semantics similar to those\nobserved in humans. Specifically, we prompted ChatGPT, a chatbot based on a\nstate-of-the-art LLM, to rate musical instrument sounds on a set of 20 semantic\nscales. We elicited multiple responses in separate chats, analogous to having\nmultiple human raters. ChatGPT generated semantic profiles that only partially\ncorrelated with human ratings, yet showed robust agreement along well-known\npsychophysical dimensions of musical sounds such as brightness (bright-dark)\nand pitch height (deep-high). Exploratory factor analysis suggested the same\ndimensionality but different spatial configuration of a latent factor space\nbetween the chatbot and human ratings. Unexpectedly, the chatbot showed degrees\nof internal variability that were comparable in magnitude to that of human\nratings. Our work highlights the potential of LLMs to capture salient\ndimensions of human sensory experience.", "published": "2023-04-16 16:50:25", "link": "http://arxiv.org/abs/2304.07830v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Canvas: End-to-End Kernel Architecture Search in Neural Networks", "abstract": "The demands for higher performance and accuracy in neural networks (NNs)\nnever end. Existing tensor compilation and Neural Architecture Search (NAS)\ntechniques orthogonally optimize the two goals but actually share many\nsimilarities in their concrete strategies. We exploit such opportunities by\ncombining the two into one and make a case for Kernel Architecture Search\n(KAS). KAS reviews NAS from a system perspective and zooms into a more\nfine-grained level to generate neural kernels with both high performance and\ngood accuracy. To demonstrate the potential of KAS, we build an end-to-end\nframework, Canvas, to find high-quality kernels as convolution replacements.\nCanvas samples from a rich set of fine-grained primitives to stochastically and\niteratively construct new kernels and evaluate them according to user-specified\nconstraints. Canvas supports freely adjustable tensor dimension sizes inside\nthe kernel and uses two levels of solvers to satisfy structural legality and\nfully utilize model budgets. The evaluation shows that by replacing standard\nconvolutions with generated new kernels in common NNs, Canvas achieves average\n1.5x speedups compared to the previous state-of-the-art with acceptable\naccuracy loss and search efficiency. Canvas verifies the practicability of KAS\nby rediscovering many manually designed kernels in the past and producing new\nstructures that may inspire future machine learning innovations. For source\ncode and implementation, we open-sourced Canvas at\nhttps://github.com/tsinghua-ideal/Canvas.", "published": "2023-04-16 10:05:42", "link": "http://arxiv.org/abs/2304.07741v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "cs.PF"], "primary_category": "cs.LG"}
{"title": "Fairness in AI and Its Long-Term Implications on Society", "abstract": "Successful deployment of artificial intelligence (AI) in various settings has\nled to numerous positive outcomes for individuals and society. However, AI\nsystems have also been shown to harm parts of the population due to biased\npredictions. AI fairness focuses on mitigating such biases to ensure AI\ndecision making is not discriminatory towards certain groups. We take a closer\nlook at AI fairness and analyze how lack of AI fairness can lead to deepening\nof biases over time and act as a social stressor. More specifically, we discuss\nhow biased models can lead to more negative real-world outcomes for certain\ngroups, which may then become more prevalent by deploying new AI models trained\non increasingly biased data, resulting in a feedback loop. If the issues\npersist, they could be reinforced by interactions with other risks and have\nsevere implications on society in the form of social unrest. We examine current\nstrategies for improving AI fairness, assess their limitations in terms of\nreal-world deployment, and explore potential paths forward to ensure we reap\nAI's benefits without causing society's collapse.", "published": "2023-04-16 11:22:59", "link": "http://arxiv.org/abs/2304.09826v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CY"}
{"title": "A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers", "abstract": "In this paper we propose a novel virtual simulation-pilot engine for speeding\nup air traffic controller (ATCo) training by integrating different\nstate-of-the-art artificial intelligence (AI) based tools. The virtual\nsimulation-pilot engine receives spoken communications from ATCo trainees, and\nit performs automatic speech recognition and understanding. Thus, it goes\nbeyond only transcribing the communication and can also understand its meaning.\nThe output is subsequently sent to a response generator system, which resembles\nthe spoken read back that pilots give to the ATCo trainees. The overall\npipeline is composed of the following submodules: (i) automatic speech\nrecognition (ASR) system that transforms audio into a sequence of words; (ii)\nhigh-level air traffic control (ATC) related entity parser that understands the\ntranscribed voice communication; and (iii) a text-to-speech submodule that\ngenerates a spoken utterance that resembles a pilot based on the situation of\nthe dialogue. Our system employs state-of-the-art AI-based tools such as\nWav2Vec 2.0, Conformer, BERT and Tacotron models. To the best of our knowledge,\nthis is the first work fully based on open-source ATC resources and AI tools.\nIn addition, we have developed a robust and modular system with optional\nsubmodules that can enhance the system's performance by incorporating real-time\nsurveillance data, metadata related to exercises (such as sectors or runways),\nor even introducing a deliberate read-back error to train ATCo trainees to\nidentify them. Our ASR system can reach as low as 5.5% and 15.9% word error\nrates (WER) on high and low-quality ATC audio. We also demonstrate that adding\nsurveillance data into the ASR can yield callsign detection accuracy of more\nthan 96%.", "published": "2023-04-16 17:45:21", "link": "http://arxiv.org/abs/2304.07842v1", "categories": ["eess.AS", "cs.AI", "cs.HC"], "primary_category": "eess.AS"}
