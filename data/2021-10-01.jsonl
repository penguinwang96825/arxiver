{"title": "Span Labeling Approach for Vietnamese and Chinese Word Segmentation", "abstract": "In this paper, we propose a span labeling approach to model n-gram\ninformation for Vietnamese word segmentation, namely SPAN SEG. We compare the\nspan labeling approach with the conditional random field by using encoders with\nthe same architecture. Since Vietnamese and Chinese have similar linguistic\nphenomena, we evaluated the proposed method on the Vietnamese treebank\nbenchmark dataset and five Chinese benchmark datasets. Through our experimental\nresults, the proposed approach SpanSeg achieves higher performance than the\nsequence tagging approach with the state-of-the-art F-score of 98.31% on the\nVietnamese treebank benchmark, when they both apply the contextual pre-trained\nlanguage model XLM-RoBERTa and the predicted word boundary information.\nBesides, we do fine-tuning experiments for the span labeling approach on BERT\nand ZEN pre-trained language model for Chinese with fewer parameters, faster\ninference time, and competitive or higher F-scores than the previous\nstate-of-the-art approach, word segmentation with word-hood memory networks, on\nfive Chinese benchmarks.", "published": "2021-10-01 01:25:50", "link": "http://arxiv.org/abs/2110.00156v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building an Efficient and Effective Retrieval-based Dialogue System via\n  Mutual Learning", "abstract": "Establishing retrieval-based dialogue systems that can select appropriate\nresponses from the pre-built index has gained increasing attention from\nresearchers. For this task, the adoption of pre-trained language models (such\nas BERT) has led to remarkable progress in a number of benchmarks. There exist\ntwo common approaches, including cross-encoders which perform full attention\nover the inputs, and bi-encoders that encode the context and response\nseparately. The former gives considerable improvements in accuracy but is often\ninapplicable in practice for large-scale retrieval given the cost of the full\nattention required for each sample at test time. The latter is efficient for\nbillions of indexes but suffers from sub-optimal performance. In this work, we\npropose to combine the best of both worlds to build a retrieval system.\nSpecifically, we employ a fast bi-encoder to replace the traditional\nfeature-based pre-retrieval model (such as BM25) and set the response\nre-ranking model as a more complicated architecture (such as cross-encoder). To\nfurther improve the effectiveness of our framework, we train the pre-retrieval\nmodel and the re-ranking model at the same time via mutual learning, which\nenables two models to learn from each other throughout the training process. We\nconduct experiments on two benchmarks and evaluation results demonstrate the\nefficiency and effectiveness of our proposed framework.", "published": "2021-10-01 01:32:33", "link": "http://arxiv.org/abs/2110.00159v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BERT4GCN: Using BERT Intermediate Layers to Augment GCN for Aspect-based\n  Sentiment Classification", "abstract": "Graph-based Aspect-based Sentiment Classification (ABSC) approaches have\nyielded state-of-the-art results, expecially when equipped with contextual word\nembedding from pre-training language models (PLMs). However, they ignore\nsequential features of the context and have not yet made the best of PLMs. In\nthis paper, we propose a novel model, BERT4GCN, which integrates the\ngrammatical sequential features from the PLM of BERT, and the syntactic\nknowledge from dependency graphs. BERT4GCN utilizes outputs from intermediate\nlayers of BERT and positional information between words to augment GCN (Graph\nConvolutional Network) to better encode the dependency graphs for the\ndownstream classification. Experimental results demonstrate that the proposed\nBERT4GCN outperforms all state-of-the-art baselines, justifying that augmenting\nGCN with the grammatical features from intermediate layers of BERT can\nsignificantly empower ABSC models.", "published": "2021-10-01 02:03:43", "link": "http://arxiv.org/abs/2110.00171v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Punctuation Restoration for Speech Transcripts via External\n  Data", "abstract": "Automatic Speech Recognition (ASR) systems generally do not produce\npunctuated transcripts. To make transcripts more readable and follow the\nexpected input format for downstream language models, it is necessary to add\npunctuation marks. In this paper, we tackle the punctuation restoration problem\nspecifically for the noisy text (e.g., phone conversation scenarios). To\nleverage the available written text datasets, we introduce a data sampling\ntechnique based on an n-gram language model to sample more training data that\nare similar to our in-domain data. Moreover, we propose a two-stage fine-tuning\napproach that utilizes the sampled external data as well as our in-domain\ndataset for models based on BERT. Extensive experiments show that the proposed\napproach outperforms the baseline with an improvement of 1:12% F1 score.", "published": "2021-10-01 17:40:55", "link": "http://arxiv.org/abs/2110.00560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Expected Validation Performance and Estimation of a Random Variable's\n  Maximum", "abstract": "Research in NLP is often supported by experimental results, and improved\nreporting of such results can lead to better understanding and more\nreproducible science. In this paper we analyze three statistical estimators for\nexpected validation performance, a tool used for reporting performance (e.g.,\naccuracy) as a function of computational budget (e.g., number of hyperparameter\ntuning experiments). Where previous work analyzing such estimators focused on\nthe bias, we also examine the variance and mean squared error (MSE). In both\nsynthetic and realistic scenarios, we evaluate three estimators and find the\nunbiased estimator has the highest variance, and the estimator with the\nsmallest variance has the largest bias; the estimator with the smallest MSE\nstrikes a balance between bias and variance, displaying a classic bias-variance\ntradeoff. We use expected validation performance to compare between different\nmodels, and analyze how frequently each estimator leads to drawing incorrect\nconclusions about which of two models performs best. We find that the two\nbiased estimators lead to the fewest incorrect conclusions, which hints at the\nimportance of minimizing variance and MSE.", "published": "2021-10-01 18:48:47", "link": "http://arxiv.org/abs/2110.00613v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Attentive Constituency Parsing for UCCA-based Semantic Parsing", "abstract": "Semantic parsing provides a way to extract the semantic structure of a text\nthat could be understood by machines. It is utilized in various NLP\napplications that require text comprehension such as summarization and question\nanswering. Graph-based representation is one of the semantic representation\napproaches to express the semantic structure of a text. Such representations\ngenerate expressive and adequate graph-based target structures. In this paper,\nwe focus primarily on UCCA graph-based semantic representation. The paper not\nonly presents the existing approaches proposed for UCCA representation, but\nalso proposes a novel self-attentive neural parsing model for the UCCA\nrepresentation. We present the results for both single-lingual and\ncross-lingual tasks using zero-shot and few-shot learning for low-resource\nlanguages.", "published": "2021-10-01 19:10:18", "link": "http://arxiv.org/abs/2110.00621v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Investigating Robustness of Dialog Models to Popular Figurative Language\n  Constructs", "abstract": "Humans often employ figurative language use in communication, including\nduring interactions with dialog systems. Thus, it is important for real-world\ndialog systems to be able to handle popular figurative language constructs like\nmetaphor and simile. In this work, we analyze the performance of existing\ndialog models in situations where the input dialog context exhibits use of\nfigurative language. We observe large gaps in handling of figurative language\nwhen evaluating the models on two open domain dialog datasets. When faced with\ndialog contexts consisting of figurative language, some models show very large\ndrops in performance compared to contexts without figurative language. We\nencourage future research in dialog modeling to separately analyze and report\nresults on figurative language in order to better test model capabilities\nrelevant to real-world use. Finally, we propose lightweight solutions to help\nexisting models become more robust to figurative language by simply using an\nexternal resource to translate figurative language to literal (non-figurative)\nforms while preserving the meaning to the best extent possible.", "published": "2021-10-01 23:55:16", "link": "http://arxiv.org/abs/2110.00687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural sentence embedding models for semantic similarity estimation in\n  the biomedical domain", "abstract": "BACKGROUND: In this study, we investigated the efficacy of current\nstate-of-the-art neural sentence embedding models for semantic similarity\nestimation of sentences from biomedical literature. We trained different neural\nembedding models on 1.7 million articles from the PubMed Open Access dataset,\nand evaluated them based on a biomedical benchmark set containing 100 sentence\npairs annotated by human experts and a smaller contradiction subset derived\nfrom the original benchmark set.\n  RESULTS: With a Pearson correlation of 0.819, our best unsupervised model\nbased on the Paragraph Vector Distributed Memory algorithm outperforms previous\nstate-of-the-art results achieved on the BIOSSES biomedical benchmark set.\nMoreover, our proposed supervised model that combines different string-based\nsimilarity metrics with a neural embedding model surpasses previous\nontology-dependent supervised state-of-the-art approaches in terms of Pearson's\nr (r=0.871) on the biomedical benchmark set. In contrast to the promising\nresults for the original benchmark, we found our best models' performance on\nthe smaller contradiction subset to be poor.\n  CONCLUSIONS: In this study we highlighted the value of neural network-based\nmodels for semantic similarity estimation in the biomedical domain by showing\nthat they can keep up with and even surpass previous state-of-the-art\napproaches for semantic similarity estimation that depend on the availability\nof laboriously curated ontologies when evaluated on a biomedical benchmark set.\nCapturing contradictions and negations in biomedical sentences, however,\nemerged as an essential area for further work.", "published": "2021-10-01 13:27:44", "link": "http://arxiv.org/abs/2110.15708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Under the Microscope: Interpreting Readability Assessment Models for\n  Filipino", "abstract": "Readability assessment is the process of identifying the level of ease or\ndifficulty of a certain piece of text for its intended audience. Approaches\nhave evolved from the use of arithmetic formulas to more complex\npattern-recognizing models trained using machine learning algorithms. While\nusing these approaches provide competitive results, limited work is done on\nanalyzing how linguistic variables affect model inference quantitatively. In\nthis work, we dissect machine learning-based readability assessment models in\nFilipino by performing global and local model interpretation to understand the\ncontributions of varying linguistic features and discuss its implications in\nthe context of the Filipino language. Results show that using a model trained\nwith top features from global interpretation obtained higher performance than\nthe ones using features selected by Spearman correlation. Likewise, we also\nempirically observed local feature weight boundaries for discriminating reading\ndifficulty at an extremely fine-grained level and their corresponding effects\nif values are perturbed.", "published": "2021-10-01 01:27:10", "link": "http://arxiv.org/abs/2110.00157v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey of Knowledge Enhanced Pre-trained Models", "abstract": "Pre-trained language models learn informative word representations on a\nlarge-scale text corpus through self-supervised learning, which has achieved\npromising performance in fields of natural language processing (NLP) after\nfine-tuning. These models, however, suffer from poor robustness and lack of\ninterpretability. We refer to pre-trained language models with knowledge\ninjection as knowledge-enhanced pre-trained language models (KEPLMs). These\nmodels demonstrate deep understanding and logical reasoning and introduce\ninterpretability. In this survey, we provide a comprehensive overview of KEPLMs\nin NLP. We first discuss the advancements in pre-trained language models and\nknowledge representation learning. Then we systematically categorize existing\nKEPLMs from three different perspectives. Finally, we outline some potential\ndirections of KEPLMs for future research.", "published": "2021-10-01 08:51:58", "link": "http://arxiv.org/abs/2110.00269v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Phonology Recognition in American Sign Language", "abstract": "Inspired by recent developments in natural language processing, we propose a\nnovel approach to sign language processing based on phonological properties\nvalidated by American Sign Language users. By taking advantage of datasets\ncomposed of phonological data and people speaking sign language, we use a\npretrained deep model based on mesh reconstruction to extract the 3D\ncoordinates of the signers keypoints. Then, we train standard statistical and\ndeep machine learning models in order to assign phonological classes to each\ntemporal sequence of coordinates.\n  Our paper introduces the idea of exploiting the phonological properties\nmanually assigned by sign language users to classify videos of people\nperforming signs by regressing a 3D mesh. We establish a new baseline for this\nproblem based on the statistical distribution of 725 different signs. Our\nbest-performing models achieve a micro-averaged F1-score of 58% for the major\nlocation class and 70% for the sign type using statistical and deep learning\nalgorithms, compared to their corresponding baselines of 35% and 39%.", "published": "2021-10-01 14:38:47", "link": "http://arxiv.org/abs/2110.00453v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real\n  Images", "abstract": "While neural symbolic methods demonstrate impressive performance in visual\nquestion answering on synthetic images, their performance suffers on real\nimages. We identify that the long-tail distribution of visual concepts and\nunequal importance of reasoning steps in real data are the two key obstacles\nthat limit the models' real-world potentials. To address these challenges, we\npropose a new paradigm, Calibrating Concepts and Operations (CCO), which\nenables neural symbolic models to capture underlying data characteristics and\nto reason with hierarchical importance. Specifically, we introduce an executor\nwith learnable concept embedding magnitudes for handling distribution\nimbalance, and an operation calibrator for highlighting important operations\nand suppressing redundant ones. Our experiments show CCO substantially boosts\nthe performance of neural symbolic methods on real images. By evaluating models\non the real world dataset GQA, CCO helps the neural symbolic method NSCL\noutperforms its vanilla counterpart by 9.1% (from 47.0% to 56.1%); this result\nalso largely reduces the performance gap between symbolic and non-symbolic\nmethods. Additionally, we create a perturbed test set for better understanding\nand analyzing model performance on real images. Code is available at\nhttps://github.com/Lizw14/CaliCO.git .", "published": "2021-10-01 16:38:32", "link": "http://arxiv.org/abs/2110.00519v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unpacking the Interdependent Systems of Discrimination: Ableist Bias in\n  NLP Systems through an Intersectional Lens", "abstract": "Much of the world's population experiences some form of disability during\ntheir lifetime. Caution must be exercised while designing natural language\nprocessing (NLP) systems to prevent systems from inadvertently perpetuating\nableist bias against people with disabilities, i.e., prejudice that favors\nthose with typical abilities. We report on various analyses based on word\npredictions of a large-scale BERT language model. Statistically significant\nresults demonstrate that people with disabilities can be disadvantaged.\nFindings also explore overlapping forms of discrimination related to\ninterconnected gender and race identities.", "published": "2021-10-01 16:40:58", "link": "http://arxiv.org/abs/2110.00521v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OPAD: An Optimized Policy-based Active Learning Framework for Document\n  Content Analysis", "abstract": "Documents are central to many business systems, and include forms, reports,\ncontracts, invoices or purchase orders. The information in documents is\ntypically in natural language, but can be organized in various layouts and\nformats. There have been recent spurt of interest in understanding document\ncontent with novel deep learning architectures. However, document understanding\ntasks need dense information annotations, which are costly to scale and\ngeneralize. Several active learning techniques have been proposed to reduce the\noverall budget of annotation while maintaining the performance of the\nunderlying deep learning model. However, most of these techniques work only for\nclassification problems. But content detection is a more complex task, and has\nbeen scarcely explored in active learning literature. In this paper, we propose\n\\textit{OPAD}, a novel framework using reinforcement policy for active learning\nin content detection tasks for documents. The proposed framework learns the\nacquisition function to decide the samples to be selected while optimizing\nperformance metrics that the tasks typically have. Furthermore, we extend to\nweak labelling scenarios to further reduce the cost of annotation\nsignificantly. We propose novel rewards to account for class imbalance and user\nfeedback in the annotation interface, to improve the active learning method. We\nshow superior performance of the proposed \\textit{OPAD} framework for active\nlearning for various tasks related to document understanding like layout\nparsing, object detection and named entity recognition. Ablation studies for\nhuman feedback and class imbalance rewards are presented, along with a\ncomparison of annotation times for different approaches.", "published": "2021-10-01 07:40:56", "link": "http://arxiv.org/abs/2110.02069v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "UserIdentifier: Implicit User Representations for Simple and Effective\n  Personalized Sentiment Analysis", "abstract": "Global models are trained to be as generalizable as possible, with user\ninvariance considered desirable since the models are shared across multitudes\nof users. As such, these models are often unable to produce personalized\nresponses for individual users, based on their data. Contrary to widely-used\npersonalization techniques based on few-shot learning, we propose\nUserIdentifier, a novel scheme for training a single shared model for all\nusers. Our approach produces personalized responses by adding fixed,\nnon-trainable user identifiers to the input data. We empirically demonstrate\nthat this proposed method outperforms the prefix-tuning based state-of-the-art\napproach by up to 13%, on a suite of sentiment analysis datasets. We also show\nthat, unlike prior work, this method needs neither any additional model\nparameters nor any extra rounds of few-shot fine-tuning.", "published": "2021-10-01 00:21:33", "link": "http://arxiv.org/abs/2110.00135v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Large-scale ASR Domain Adaptation using Self- and Semi-supervised\n  Learning", "abstract": "Self- and semi-supervised learning methods have been actively investigated to\nreduce labeled training data or enhance the model performance. However, the\napproach mostly focus on in-domain performance for public datasets. In this\nstudy, we utilize the combination of self- and semi-supervised learning methods\nto solve unseen domain adaptation problem in a large-scale production setting\nfor online ASR model. This approach demonstrates that using the source domain\ndata with a small fraction of the target domain data (3%) can recover the\nperformance gap compared to a full data baseline: relative 13.5% WER\nimprovement for target domain data.", "published": "2021-10-01 01:48:33", "link": "http://arxiv.org/abs/2110.00165v5", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Learning to Ask for Data-Efficient Event Argument Extraction", "abstract": "Event argument extraction (EAE) is an important task for information\nextraction to discover specific argument roles. In this study, we cast EAE as a\nquestion-based cloze task and empirically analyze fixed discrete token template\nperformance. As generating human-annotated question templates is often\ntime-consuming and labor-intensive, we further propose a novel approach called\n\"Learning to Ask,\" which can learn optimized question templates for EAE without\nhuman annotations. Experiments using the ACE-2005 dataset demonstrate that our\nmethod based on optimized questions achieves state-of-the-art performance in\nboth the few-shot and supervised settings.", "published": "2021-10-01 15:22:37", "link": "http://arxiv.org/abs/2110.00479v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LEMON: Explainable Entity Matching", "abstract": "State-of-the-art entity matching (EM) methods are hard to interpret, and\nthere is significant value in bringing explainable AI to EM. Unfortunately,\nmost popular explainability methods do not work well out of the box for EM and\nneed adaptation. In this paper, we identify three challenges of applying local\npost hoc feature attribution methods to entity matching: cross-record\ninteraction effects, non-match explanations, and variation in sensitivity. We\npropose our novel model-agnostic and schema-flexible method LEMON that\naddresses all three challenges by (i) producing dual explanations to avoid\ncross-record interaction effects, (ii) introducing the novel concept of\nattribution potential to explain how two records could have matched, and (iii)\nautomatically choosing explanation granularity to match the sensitivity of the\nmatcher and record pair in question. Experiments on public datasets demonstrate\nthat the proposed method is more faithful to the matcher and does a better job\nof helping users understand the decision boundary of the matcher than previous\nwork. Furthermore, user studies show that the rate at which human subjects can\nconstruct counterfactual examples after seeing an explanation from our proposed\nmethod increases from 54% to 64% for matches and from 15% to 49% for\nnon-matches compared to explanations from a standard adaptation of LIME.", "published": "2021-10-01 16:32:16", "link": "http://arxiv.org/abs/2110.00516v2", "categories": ["cs.DB", "cs.CL", "cs.HC", "cs.LG"], "primary_category": "cs.DB"}
{"title": "TEACh: Task-driven Embodied Agents that Chat", "abstract": "Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.", "published": "2021-10-01 17:00:14", "link": "http://arxiv.org/abs/2110.00534v3", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Natural language understanding for logical games", "abstract": "We developed a system able to automatically solve logical puzzles in natural\nlanguage. Our solution is composed by a parser and an inference module. The\nparser translates the text into first order logic (FOL), while the MACE4 model\nfinder is used to compute the models of the given FOL theory. We also empower\nour software agent with the capability to provide Yes/No answers to natural\nlanguage questions related to each puzzle. Moreover, in line with Explainalbe\nArtificial Intelligence (XAI), the agent can back its answer, providing a\ngraphical representation of the proof. The advantage of using reasoning for\nNatural Language Understanding (NLU) instead of Machine learning is that the\nuser can obtain an explanation of the reasoning chain. We illustrate how the\nsystem performs on various types of natural language puzzles, including 382\nknights and knaves puzzles. These features together with the overall\nperformance rate of 80.89\\% makes the proposed solution an improvement upon\nsimilar solvers for natural language understanding in the puzzles domain.", "published": "2021-10-01 17:36:14", "link": "http://arxiv.org/abs/2110.00558v1", "categories": ["cs.CL", "cs.AI", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Sentiment and structure in word co-occurrence networks on Twitter", "abstract": "We explore the relationship between context and happiness scores in political\ntweets using word co-occurrence networks, where nodes in the network are the\nwords, and the weight of an edge is the number of tweets in the corpus for\nwhich the two connected words co-occur. In particular, we consider tweets with\nhashtags #imwithher and #crookedhillary, both relating to Hillary Clinton's\npresidential bid in 2016. We then analyze the network properties in conjunction\nwith the word scores by comparing with null models to separate the effects of\nthe network structure and the score distribution. Neutral words are found to be\ndominant and most words, regardless of polarity, tend to co-occur with neutral\nwords. We do not observe any score homophily among positive and negative words.\nHowever, when we perform network backboning, community detection results in\nword groupings with meaningful narratives, and the happiness scores of the\nwords in each group correspond to its respective theme. Thus, although we\nobserve no clear relationship between happiness scores and co-occurrence at the\nnode or edge level, a community-centric approach can isolate themes of\ncompeting sentiments in a corpus.", "published": "2021-10-01 18:00:02", "link": "http://arxiv.org/abs/2110.00587v1", "categories": ["cs.CL", "cs.CY", "cs.SI", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "ALBU: An approximate Loopy Belief message passing algorithm for LDA to\n  improve performance on small data sets", "abstract": "Variational Bayes (VB) applied to latent Dirichlet allocation (LDA) has\nbecome the most popular algorithm for aspect modeling. While sufficiently\nsuccessful in text topic extraction from large corpora, VB is less successful\nin identifying aspects in the presence of limited data. We present a novel\nvariational message passing algorithm as applied to Latent Dirichlet Allocation\n(LDA) and compare it with the gold standard VB and collapsed Gibbs sampling. In\nsituations where marginalisation leads to non-conjugate messages, we use ideas\nfrom sampling to derive approximate update equations. In cases where conjugacy\nholds, Loopy Belief update (LBU) (also known as Lauritzen-Spiegelhalter) is\nused. Our algorithm, ALBU (approximate LBU), has strong similarities with\nVariational Message Passing (VMP) (which is the message passing variant of VB).\nTo compare the performance of the algorithms in the presence of limited data,\nwe use data sets consisting of tweets and news groups. Additionally, to perform\nmore fine grained evaluations and comparisons, we use simulations that enable\ncomparisons with the ground truth via Kullback-Leibler divergence (KLD). Using\ncoherence measures for the text corpora and KLD with the simulations we show\nthat ALBU learns latent distributions more accurately than does VB, especially\nfor smaller data sets.", "published": "2021-10-01 19:55:12", "link": "http://arxiv.org/abs/2110.00635v2", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML", "62F15", "G.3"], "primary_category": "cs.LG"}
{"title": "Low Frequency Names Exhibit Bias and Overfitting in Contextualizing\n  Language Models", "abstract": "We use a dataset of U.S. first names with labels based on predominant gender\nand racial group to examine the effect of training corpus frequency on\ntokenization, contextualization, similarity to initial representation, and bias\nin BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white\nnames are less frequent in the training corpora of these four language models.\nWe find that infrequent names are more self-similar across contexts, with\nSpearman's r between frequency and self-similarity as low as -.763. Infrequent\nnames are also less similar to initial representation, with Spearman's r\nbetween frequency and linear centered kernel alignment (CKA) similarity to\ninitial representation as high as .702. Moreover, we find Spearman's r between\nracial bias and name frequency in BERT of .492, indicating that lower-frequency\nminority group names are more associated with unpleasantness. Representations\nof infrequent names undergo more processing, but are more self-similar,\nindicating that models rely on less context-informed representations of\nuncommon and minority names which are overfit to a lower number of observed\ncontexts.", "published": "2021-10-01 22:44:31", "link": "http://arxiv.org/abs/2110.00672v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Speech Technology for Everyone: Automatic Speech Recognition for\n  Non-Native English with Transfer Learning", "abstract": "To address the performance gap of English ASR models on L2 English speakers,\nwe evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;\nXu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,\n2018) under different training settings. We compare \\textbf{(a)} models trained\nwith a combination of diverse accents to ones trained with only specific\naccents and \\textbf{(b)} results from different single-accent models. Our\nexperiments demonstrate the promise of developing ASR models for non-native\nEnglish speakers, even with small amounts of L2 training data and even without\na language model. Our models also excel in the zero-shot setting where we train\non multiple L2 datasets and test on a blind L2 test set.", "published": "2021-10-01 23:11:00", "link": "http://arxiv.org/abs/2110.00678v3", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "SALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic\n  Sound Event Localization and Detection", "abstract": "Sound event localization and detection (SELD) consists of two subtasks, which\nare sound event detection and direction-of-arrival estimation. While sound\nevent detection mainly relies on time-frequency patterns to distinguish\ndifferent sound classes, direction-of-arrival estimation uses amplitude and/or\nphase differences between microphones to estimate source directions. As a\nresult, it is often difficult to jointly optimize these two subtasks. We\npropose a novel feature called Spatial cue-Augmented Log-SpectrogrAm (SALSA)\nwith exact time-frequency mapping between the signal power and the source\ndirectional cues, which is crucial for resolving overlapping sound sources. The\nSALSA feature consists of multichannel log-spectrograms stacked along with the\nnormalized principal eigenvector of the spatial covariance matrix at each\ncorresponding time-frequency bin. Depending on the microphone array format, the\nprincipal eigenvector can be normalized differently to extract amplitude and/or\nphase differences between the microphones. As a result, SALSA features are\napplicable for different microphone array formats such as first-order\nambisonics (FOA) and multichannel microphone array (MIC). Experimental results\non the TAU-NIGENS Spatial Sound Events 2021 dataset with directional\ninterferences showed that SALSA features outperformed other state-of-the-art\nfeatures. Specifically, the use of SALSA features in the FOA format increased\nthe F1 score and localization recall by 6% each, compared to the multichannel\nlog-mel spectrograms with intensity vectors. For the MIC format, using SALSA\nfeatures increased F1 score and localization recall by 16% and 7%,\nrespectively, compared to using multichannel log-mel spectrograms with\ngeneralized cross-correlation spectra.", "published": "2021-10-01 09:01:35", "link": "http://arxiv.org/abs/2110.00275v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Low-Distortion Target Estimates for Improved Speech\n  Enhancement", "abstract": "A promising approach for multi-microphone speech separation involves two deep\nneural networks (DNN), where the predicted target speech from the first DNN is\nused to compute signal statistics for time-invariant minimum variance\ndistortionless response (MVDR) beamforming, and the MVDR result is then used as\nextra features for the second DNN to predict target speech. Previous studies\nsuggested that the MVDR result can provide complementary information for the\nsecond DNN to better predict target speech. However, on fixed-geometry arrays,\nboth DNNs can take in, for example, the real and imaginary (RI) components of\nthe multi-channel mixture as features to leverage the spatial and spectral\ninformation for enhancement. It is not explained clearly why the linear MVDR\nresult can be complementary and why it is still needed, considering that the\nDNNs and the beamformer use the same input, and the DNNs perform non-linear\nfiltering and could render the linear filtering of MVDR unnecessary. Similarly,\nin monaural cases, one can replace the MVDR beamformer with a monaural weighted\nprediction error (WPE) filter. Although the linear WPE filter and the DNNs use\nthe same mixture RI components as input, the WPE result is found to\nsignificantly improve the second DNN. This study provides a novel explanation\nfrom the perspective of the low-distortion nature of such algorithms, and finds\nthat they can consistently improve phase estimation. Equipped with this\nunderstanding, we investigate several low-distortion target estimation\nalgorithms including several beamformers, WPE, forward convolutive prediction,\nand their combinations, and use their results as extra features to train the\nsecond network to achieve better enhancement. Evaluation results on single- and\nmulti-microphone speech dereverberation and enhancement tasks indicate the\neffectiveness of the proposed approach, and the validity of the proposed view.", "published": "2021-10-01 17:53:40", "link": "http://arxiv.org/abs/2110.00570v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Incremental Layer-wise Self-Supervised Learning for Efficient Speech\n  Domain Adaptation On Device", "abstract": "Streaming end-to-end speech recognition models have been widely applied to\nmobile devices and show significant improvement in efficiency. These models are\ntypically trained on the server using transcribed speech data. However, the\nserver data distribution can be very different from the data distribution on\nuser devices, which could affect the model performance. There are two main\nchallenges for on device training, limited reliable labels and limited training\nmemory. While self-supervised learning algorithms can mitigate the mismatch\nbetween domains using unlabeled data, they are not applicable on mobile devices\ndirectly because of the memory constraint. In this paper, we propose an\nincremental layer-wise self-supervised learning algorithm for efficient speech\ndomain adaptation on mobile devices, in which only one layer is updated at a\ntime. Extensive experimental results demonstrate that the proposed algorithm\nobtains a Word Error Rate (WER) on the target domain $24.2\\%$ better than\nsupervised baseline and costs $89.7\\%$ less training memory than the end-to-end\nself-supervised learning algorithm.", "published": "2021-10-01 01:22:38", "link": "http://arxiv.org/abs/2110.00155v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Ensemble-based Multi-Criteria Decision Making Method for COVID-19\n  Cough Classification", "abstract": "The objectives of this research are analysing the performance of the\nstate-of-the-art machine learning techniques for classifying COVID-19 from\ncough sound and identifying the model(s) that consistently perform well across\ndifferent cough datasets. Different performance evaluation metrics (such as\nprecision, sensitivity, specificity, AUC, accuracy, etc.) make it difficult to\nselect the best performance model. To address this issue, in this paper, we\npropose an ensemble-based multi-criteria decision making (MCDM) method for\nselecting top performance machine learning technique(s) for COVID-19 cough\nclassification. We use four cough datasets, namely Cambridge, Coswara, Virufy,\nand NoCoCoDa to verify the proposed method. At first, our proposed method uses\nthe audio features of cough samples and then applies machine learning (ML)\ntechniques to classify them as COVID-19 or non-COVID-19. Then, we consider a\nmulti-criteria decision-making (MCDM) method that combines ensemble\ntechnologies (i.e., soft and hard) to select the best model. In MCDM, we use\nthe technique for order preference by similarity to ideal solution (TOPSIS) for\nranking purposes, while entropy is applied to calculate evaluation criteria\nweights. In addition, we apply the feature reduction process through recursive\nfeature elimination with cross-validation under different estimators. The\nresults of our empirical evaluations show that the proposed method outperforms\nthe state-of-the-art models.", "published": "2021-10-01 16:19:50", "link": "http://arxiv.org/abs/2110.00508v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
