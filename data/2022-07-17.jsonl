{"title": "Towards Explainability in NLP: Analyzing and Calculating Word Saliency\n  through Word Properties", "abstract": "The wide use of black-box models in natural language processing brings great\nchallenges to the understanding of the decision basis, the trustworthiness of\nthe prediction results, and the improvement of the model performance. The words\nin text samples have properties that reflect their semantics and contextual\ninformation, such as the part of speech, the position, etc. These properties\nmay have certain relationships with the word saliency, which is of great help\nfor studying the explainability of the model predictions. In this paper, we\nexplore the relationships between the word saliency and the word properties.\nAccording to the analysis results, we further establish a mapping model,\nSeq2Saliency, from the words in a text sample and their properties to the\nsaliency values based on the idea of sequence tagging. In addition, we\nestablish a new dataset called PrSalM, which contains each word in the text\nsamples, the word properties, and the word saliency values. The experimental\nevaluations are conducted to analyze the saliency of words with different\nproperties. The effectiveness of the Seq2Saliency model is verified.", "published": "2022-07-17 06:02:48", "link": "http://arxiv.org/abs/2207.08083v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "United States Politicians' Tone Became More Negative with 2016 Primary\n  Campaigns", "abstract": "There is a widespread belief that the tone of US political language has\nbecome more negative recently, in particular when Donald Trump entered\npolitics. At the same time, there is disagreement as to whether Trump changed\nor merely continued previous trends. To date, data-driven evidence regarding\nthese questions is scarce, partly due to the difficulty of obtaining a\ncomprehensive, longitudinal record of politicians' utterances. Here we apply\npsycholinguistic tools to a novel, comprehensive corpus of 24 million quotes\nfrom online news attributed to 18,627 US politicians in order to analyze how\nthe tone of US politicians' language evolved between 2008 and 2020. We show\nthat, whereas the frequency of negative emotion words had decreased\ncontinuously during Obama's tenure, it suddenly and lastingly increased with\nthe 2016 primary campaigns, by 1.6 pre-campaign standard deviations, or 8% of\nthe pre-campaign mean, in a pattern that emerges across parties. The effect\nsize drops by 40% when omitting Trump's quotes, and by 50% when averaging over\nspeakers rather than quotes, implying that prominent speakers, and Trump in\nparticular, have disproportionately, though not exclusively, contributed to the\nrise in negative language. This work provides the first large-scale data-driven\nevidence of a drastic shift toward a more negative political tone following\nTrump's campaign start as a catalyst, with important implications for the\ndebate about the state of US politics.", "published": "2022-07-17 08:41:14", "link": "http://arxiv.org/abs/2207.08112v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ELECTRA is a Zero-Shot Learner, Too", "abstract": "Recently, for few-shot or even zero-shot learning, the new paradigm\n\"pre-train, prompt, and predict\" has achieved remarkable achievements compared\nwith the \"pre-train, fine-tune\" paradigm. After the success of prompt-based\nGPT-3, a series of masked language model (MLM)-based (e.g., BERT, RoBERTa)\nprompt learning methods became popular and widely used. However, another\nefficient pre-trained discriminative model, ELECTRA, has probably been\nneglected. In this paper, we attempt to accomplish several NLP tasks in the\nzero-shot scenario using a novel our proposed replaced token detection\n(RTD)-based prompt learning method. Experimental results show that ELECTRA\nmodel based on RTD-prompt learning achieves surprisingly state-of-the-art\nzero-shot performance. Numerically, compared to MLM-RoBERTa-large and\nMLM-BERT-large, our RTD-ELECTRA-large has an average of about 8.4% and 13.7%\nimprovement on all 15 tasks. Especially on the SST-2 task, our\nRTD-ELECTRA-large achieves an astonishing 90.1% accuracy without any training\ndata. Overall, compared to the pre-trained masked language models, the\npre-trained replaced token detection model performs better in zero-shot\nlearning. The source code is available at:\nhttps://github.com/nishiwen1214/RTD-ELECTRA.", "published": "2022-07-17 11:20:58", "link": "http://arxiv.org/abs/2207.08141v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Natural language processing for clusterization of genes according to\n  their functions", "abstract": "There are hundreds of methods for analysis of data obtained in\nmRNA-sequencing. The most of them are focused on small number of genes. In this\nstudy, we propose an approach that reduces the analysis of several thousand\ngenes to analysis of several clusters. The list of genes is enriched with\ninformation from open databases. Then, the descriptions are encoded as vectors\nusing the pretrained language model (BERT) and some text processing approaches.\nThe encoded gene function pass through the dimensionality reduction and\nclusterization. Aiming to find the most efficient pipeline, 180 cases of\npipeline with different methods in the major pipeline steps were analyzed. The\nperformance was evaluated with clusterization indexes and expert review of the\nresults.", "published": "2022-07-17 12:59:34", "link": "http://arxiv.org/abs/2207.08162v1", "categories": ["cs.CL", "68T50, 92-08"], "primary_category": "cs.CL"}
{"title": "An Overview of Distant Supervision for Relation Extraction with a Focus\n  on Denoising and Pre-training Methods", "abstract": "Relation Extraction (RE) is a foundational task of natural language\nprocessing. RE seeks to transform raw, unstructured text into structured\nknowledge by identifying relational information between entity pairs found in\ntext. RE has numerous uses, such as knowledge graph completion, text\nsummarization, question-answering, and search querying. The history of RE\nmethods can be roughly organized into four phases: pattern-based RE,\nstatistical-based RE, neural-based RE, and large language model-based RE. This\nsurvey begins with an overview of a few exemplary works in the earlier phases\nof RE, highlighting limitations and shortcomings to contextualize progress.\nNext, we review popular benchmarks and critically examine metrics used to\nassess RE performance. We then discuss distant supervision, a paradigm that has\nshaped the development of modern RE methods. Lastly, we review recent RE works\nfocusing on denoising and pre-training methods.", "published": "2022-07-17 21:02:04", "link": "http://arxiv.org/abs/2207.08286v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "A Spoken Drug Prescription Dataset in French for Spoken Language\n  Understanding", "abstract": "Spoken medical dialogue systems are increasingly attracting interest to\nenhance access to healthcare services and improve quality and traceability of\npatient care. In this paper, we focus on medical drug prescriptions acquired on\nsmartphones through spoken dialogue. Such systems would facilitate the\ntraceability of care and would free clinicians' time. However, there is a lack\nof speech corpora to develop such systems since most of the related corpora are\nin text form and in English. To facilitate the research and development of\nspoken medical dialogue systems, we present, to the best of our knowledge, the\nfirst spoken medical drug prescriptions corpus, named PxSLU. It contains 4\nhours of transcribed and annotated dialogues of drug prescriptions in French\nacquired through an experiment with 55 participants experts and non-experts in\nprescriptions. We also present some experiments that demonstrate the interest\nof this corpus for the evaluation and development of medical dialogue systems.", "published": "2022-07-17 21:18:03", "link": "http://arxiv.org/abs/2207.08292v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Similarity is More Valuable than Character Similarity: An\n  Empirical Study for Chinese Spell Checking", "abstract": "Chinese Spell Checking (CSC) task aims to detect and correct Chinese spelling\nerrors. Recently, related researches focus on introducing character similarity\nfrom confusion set to enhance the CSC models, ignoring the context of\ncharacters that contain richer information. To make better use of contextual\ninformation, we propose a simple yet effective Curriculum Learning (CL)\nframework for the CSC task. With the help of our model-agnostic CL framework,\nexisting CSC models will be trained from easy to difficult as humans learn\nChinese characters and achieve further performance improvements. Extensive\nexperiments and detailed analyses on widely used SIGHAN datasets show that our\nmethod outperforms previous state-of-the-art methods. More instructively, our\nstudy empirically suggests that contextual similarity is more valuable than\ncharacter similarity for the CSC task.", "published": "2022-07-17 03:12:27", "link": "http://arxiv.org/abs/2207.09217v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Context Pattern Generation for Entity Set Expansion", "abstract": "Entity Set Expansion (ESE) is a valuable task that aims to find entities of\nthe target semantic class described by given seed entities. Various Natural\nLanguage Processing (NLP) and Information Retrieval (IR) downstream\napplications have benefited from ESE due to its ability to discover knowledge.\nAlthough existing corpus-based ESE methods have achieved great progress, they\nstill rely on corpora with high-quality entity information annotated, because\nmost of them need to obtain the context patterns through the position of the\nentity in a sentence. Therefore, the quality of the given corpora and their\nentity annotation has become the bottleneck that limits the performance of such\nmethods. To overcome this dilemma and make the ESE models free from the\ndependence on entity annotation, our work aims to explore a new ESE paradigm,\nnamely corpus-independent ESE. Specifically, we devise a context pattern\ngeneration module that utilizes autoregressive language models (e.g., GPT-2) to\nautomatically generate high-quality context patterns for entities. In addition,\nwe propose the GAPA, a novel ESE framework that leverages the aforementioned\nGenerAted PAtterns to expand target entities. Extensive experiments and\ndetailed analyses on three widely used datasets demonstrate the effectiveness\nof our method. All the codes of our experiments are available at\nhttps://github.com/geekjuruo/GAPA.", "published": "2022-07-17 06:50:35", "link": "http://arxiv.org/abs/2207.08087v4", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Aspect-specific Context Modeling for Aspect-based Sentiment Analysis", "abstract": "Aspect-based sentiment analysis (ABSA) aims at predicting sentiment polarity\n(SC) or extracting opinion span (OE) expressed towards a given aspect. Previous\nwork in ABSA mostly relies on rather complicated aspect-specific feature\ninduction. Recently, pretrained language models (PLMs), e.g., BERT, have been\nused as context modeling layers to simplify the feature induction structures\nand achieve state-of-the-art performance. However, such PLM-based context\nmodeling can be not that aspect-specific. Therefore, a key question is left\nunder-explored: how the aspect-specific context can be better modeled through\nPLMs? To answer the question, we attempt to enhance aspect-specific context\nmodeling with PLM in a non-intrusive manner. We propose three aspect-specific\ninput transformations, namely aspect companion, aspect prompt, and aspect\nmarker. Informed by these transformations, non-intrusive aspect-specific PLMs\ncan be achieved to promote the PLM to pay more attention to the aspect-specific\ncontext in a sentence. Additionally, we craft an adversarial benchmark for ABSA\n(advABSA) to see how aspect-specific modeling can impact model robustness.\nExtensive experimental results on standard and adversarial benchmarks for SC\nand OE demonstrate the effectiveness and robustness of the proposed method,\nyielding new state-of-the-art performance on OE and competitive performance on\nSC.", "published": "2022-07-17 07:22:19", "link": "http://arxiv.org/abs/2207.08099v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Multibias-mitigated and Sentiment Knowledge Enriched Transformer for\n  Debiasing in Multimodal Conversational Emotion Recognition", "abstract": "Multimodal emotion recognition in conversations (mERC) is an active research\ntopic in natural language processing (NLP), which aims to predict human's\nemotional states in communications of multiple modalities, e,g., natural\nlanguage and facial gestures. Innumerable implicit prejudices and\npreconceptions fill human language and conversations, leading to the question\nof whether the current data-driven mERC approaches produce a biased error. For\nexample, such approaches may offer higher emotional scores on the utterances by\nfemales than males. In addition, the existing debias models mainly focus on\ngender or race, where multibias mitigation is still an unexplored task in mERC.\nIn this work, we take the first step to solve these issues by proposing a\nseries of approaches to mitigate five typical kinds of bias in textual\nutterances (i.e., gender, age, race, religion and LGBTQ+) and visual\nrepresentations (i.e, gender and age), followed by a Multibias-Mitigated and\nsentiment Knowledge Enriched bi-modal Transformer (MMKET). Comprehensive\nexperimental results show the effectiveness of the proposed model and prove\nthat the debias operation has a great impact on the classification performance\nfor mERC. We hope our study will benefit the development of bias mitigation in\nmERC and related emotion studies.", "published": "2022-07-17 08:16:49", "link": "http://arxiv.org/abs/2207.08104v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RT-KGD: Relation Transition Aware Knowledge-Grounded Dialogue Generation", "abstract": "Grounding dialogue system with external knowledge is a promising way to\nimprove the quality of responses. Most existing works adopt knowledge graphs\n(KGs) as the external resources, paying attention to the contribution of\nentities in the last utterance of the dialogue for context understanding and\nresponse generation. Nevertheless, the correlations between knowledge implied\nin the multi-turn context and the transition regularities between relations in\nKGs are under-explored. To this end, we propose a Relation Transition aware\nKnowledge-Grounded Dialogue Generation model (RT-KGD). Specifically, inspired\nby the latent logic of human conversation, our model integrates dialogue-level\nrelation transition regularities with turn-level entity semantic information.\nIn this manner, the interaction between knowledge is considered to produce\nabundant clues for predicting the appropriate knowledge and generating coherent\nresponses. The experimental results on both automatic evaluation and manual\nevaluation indicate that our model outperforms state-of-the-art baselines.", "published": "2022-07-17 16:07:38", "link": "http://arxiv.org/abs/2207.08212v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Context-Sensitive Word Embedding Approach for The Detection of Troll\n  Tweets", "abstract": "In this study, we aimed to address the growing concern of trolling behavior\non social media by developing and evaluating a set of model architectures for\nthe automatic detection of troll tweets. Utilizing deep learning techniques and\npre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated\nthe performance of each architecture using metrics such as classification\naccuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo\nembedding methods performed better than the GloVe method, likely due to their\nability to provide contextualized word embeddings that better capture the\nnuances and subtleties of language use in online social media. Additionally, we\nfound that CNN and GRU encoders performed similarly in terms of F1 score and\nAUC, suggesting their effectiveness in extracting relevant information from\ninput text. The best-performing method was found to be an ELMo-based\narchitecture that employed a GRU classifier, with an AUC score of 0.929. This\nresearch highlights the importance of utilizing contextualized word embeddings\nand appropriate encoder methods in the task of troll tweet detection, which can\nassist social-based systems in improving their performance in identifying and\naddressing trolling behavior on their platforms.", "published": "2022-07-17 17:12:16", "link": "http://arxiv.org/abs/2207.08230v4", "categories": ["cs.CL", "cs.AI", "I.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Effectiveness of French Language Models on Abstractive Dialogue\n  Summarization Task", "abstract": "Pre-trained language models have established the state-of-the-art on various\nnatural language processing tasks, including dialogue summarization, which\nallows the reader to quickly access key information from long conversations in\nmeetings, interviews or phone calls. However, such dialogues are still\ndifficult to handle with current models because the spontaneity of the language\ninvolves expressions that are rarely present in the corpora used for\npre-training the language models. Moreover, the vast majority of the work\naccomplished in this field has been focused on English. In this work, we\npresent a study on the summarization of spontaneous oral dialogues in French\nusing several language specific pre-trained models: BARThez, and BelGPT-2, as\nwell as multilingual pre-trained models: mBART, mBARThez, and mT5. Experiments\nwere performed on the DECODA (Call Center) dialogue corpus whose task is to\ngenerate abstractive synopses from call center conversations between a caller\nand one or several agents depending on the situation. Results show that the\nBARThez models offer the best performance far above the previous\nstate-of-the-art on DECODA. We further discuss the limits of such pre-trained\nmodels and the challenges that must be addressed for summarizing spontaneous\ndialogues.", "published": "2022-07-17 21:43:18", "link": "http://arxiv.org/abs/2207.08305v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can large language models reason about medical questions?", "abstract": "Although large language models (LLMs) often produce impressive outputs, it\nremains unclear how they perform in real-world scenarios requiring strong\nreasoning skills and expert domain knowledge. We set out to investigate whether\nclose- and open-source models (GPT-3.5, LLama-2, etc.) can be applied to answer\nand reason about difficult real-world-based questions. We focus on three\npopular medical benchmarks (MedQA-USMLE, MedMCQA, and PubMedQA) and multiple\nprompting scenarios: Chain-of-Thought (CoT, think step-by-step), few-shot and\nretrieval augmentation. Based on an expert annotation of the generated CoTs, we\nfound that InstructGPT can often read, reason and recall expert knowledge.\nLast, by leveraging advances in prompt engineering (few-shot and ensemble\nmethods), we demonstrated that GPT-3.5 not only yields calibrated predictive\ndistributions, but also reaches the passing score on three datasets:\nMedQA-USMLE 60.2%, MedMCQA 62.7% and PubMedQA 78.2%. Open-source models are\nclosing the gap: Llama-2 70B also passed the MedQA-USMLE with 62.5% accuracy.", "published": "2022-07-17 11:24:44", "link": "http://arxiv.org/abs/2207.08143v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.1; I.2.7"], "primary_category": "cs.CL"}
{"title": "End-to-End Spoken Language Understanding: Performance analyses of a\n  voice command task in a low resource setting", "abstract": "Spoken Language Understanding (SLU) is a core task in most human-machine\ninteraction systems. With the emergence of smart homes, smart phones and smart\nspeakers, SLU has become a key technology for the industry. In a classical SLU\napproach, an Automatic Speech Recognition (ASR) module transcribes the speech\nsignal into a textual representation from which a Natural Language\nUnderstanding (NLU) module extracts semantic information. Recently End-to-End\nSLU (E2E SLU) based on Deep Neural Networks has gained momentum since it\nbenefits from the joint optimization of the ASR and the NLU parts, hence\nlimiting the cascade of error effect of the pipeline architecture. However,\nlittle is known about the actual linguistic properties used by E2E models to\npredict concepts and intents from speech input. In this paper, we present a\nstudy identifying the signal features and other linguistic properties used by\nan E2E model to perform the SLU task. The study is carried out in the\napplication domain of a smart home that has to handle non-English (here French)\nvoice commands. The results show that a good E2E SLU performance does not\nalways require a perfect ASR capability. Furthermore, the results show the\nsuperior capabilities of the E2E model in handling background noise and\nsyntactic variation compared to the pipeline model. Finally, a finer-grained\nanalysis suggests that the E2E model uses the pitch information of the input\nsignal to identify voice command concepts. The results and methodology outlined\nin this paper provide a springboard for further analyses of E2E models in\nspeech processing.", "published": "2022-07-17 13:51:56", "link": "http://arxiv.org/abs/2207.08179v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Zero-Shot Temporal Action Detection via Vision-Language Prompting", "abstract": "Existing temporal action detection (TAD) methods rely on large training data\nincluding segment-level annotations, limited to recognizing previously seen\nclasses alone during inference. Collecting and annotating a large training set\nfor each class of interest is costly and hence unscalable. Zero-shot TAD\n(ZS-TAD) resolves this obstacle by enabling a pre-trained model to recognize\nany unseen action classes. Meanwhile, ZS-TAD is also much more challenging with\nsignificantly less investigation. Inspired by the success of zero-shot image\nclassification aided by vision-language (ViL) models such as CLIP, we aim to\ntackle the more complex TAD task. An intuitive method is to integrate an\noff-the-shelf proposal detector with CLIP style classification. However, due to\nthe sequential localization (e.g, proposal generation) and classification\ndesign, it is prone to localization error propagation. To overcome this\nproblem, in this paper we propose a novel zero-Shot Temporal Action detection\nmodel via Vision-LanguagE prompting (STALE). Such a novel design effectively\neliminates the dependence between localization and classification by breaking\nthe route for error propagation in-between. We further introduce an interaction\nmechanism between classification and localization for improved optimization.\nExtensive experiments on standard ZS-TAD video benchmarks show that our STALE\nsignificantly outperforms state-of-the-art alternatives. Besides, our model\nalso yields superior results on supervised TAD over recent strong competitors.\nThe PyTorch implementation of STALE is available at\nhttps://github.com/sauradip/STALE.", "published": "2022-07-17 13:59:46", "link": "http://arxiv.org/abs/2207.08184v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Representation Learning of Image Schema", "abstract": "Image schema is a recurrent pattern of reasoning where one entity is mapped\ninto another. Image schema is similar to conceptual metaphor and is also\nrelated to metaphoric gesture. Our main goal is to generate metaphoric gestures\nfor an Embodied Conversational Agent.\n  We propose a technique to learn the vector representation of image schemas.\nAs far as we are aware of, this is the first work which addresses that problem.\nOur technique uses Ravenet et al's algorithm which we use to compute the image\nschemas from the text input and also BERT and SenseBERT which we use as the\nbase word embedding technique to calculate the final vector representation of\nthe image schema. Our representation learning technique works by clustering:\nword embedding vectors which belong to the same image schema should be\nrelatively closer to each other, and thus form a cluster.\n  With the image schemas representable as vectors, it also becomes possible to\nhave a notion that some image schemas are closer or more similar to each other\nthan to the others because the distance between the vectors is a proxy of the\ndissimilarity between the corresponding image schemas. Therefore, after\nobtaining the vector representation of the image schemas, we calculate the\ndistances between those vectors. Based on these, we create visualizations to\nillustrate the relative distances between the different image schemas.", "published": "2022-07-17 18:42:37", "link": "http://arxiv.org/abs/2207.08256v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Multi-channel target speech enhancement based on ERB-scaled spatial\n  coherence features", "abstract": "Recently, speech enhancement technologies that are based on deep learning\nhave received considerable research attention. If the spatial information in\nmicrophone signals is exploited, microphone arrays can be advantageous under\nsome adverse acoustic conditions compared with single-microphone systems.\nHowever, multichannel speech enhancement is often performed in the short-time\nFourier transform (STFT) domain, which renders the enhancement approach\ncomputationally expensive. To remedy this problem, we propose a novel\nequivalent rectangular bandwidth (ERB)-scaled spatial coherence feature that is\ndependent on the target speaker activity between two ERB bands. Experiments\nconducted using a four-microphone array in a reverberant environment, which\ninvolved speech interference, demonstrated the efficacy of the proposed system.\nThis study also demonstrated that a network that was trained with the\nERB-scaled spatial feature was robust against variations in the geometry and\nnumber of the microphones in the array.", "published": "2022-07-17 09:53:06", "link": "http://arxiv.org/abs/2207.08126v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Improving spatial cues for hearables using a parameterized binaural CDR\n  estimator", "abstract": "We investigate a speech enhancement method based on the binaural\ncoherence-to-diffuse power ratio (CDR), which preserves auditory spatial cues\nfor maskers and a broadside target. Conventional CDR estimators typically rely\non a mathematical coherence model of the desired signal and/or diffuse noise\nfield in their formulation, which may influence their accuracy in natural\nenvironments. This work proposes a new robust and parameterized directional\nbinaural CDR estimator. The estimator is calculated in the time-frequency\ndomain and is based on a geometrical interpretation of the spatial coherence\nfunction between the binaural microphone signals. The binaural performance of\nthe new CDR estimator is compared with three state-of-the-art CDR estimators in\ncocktail-party-like environments and has shown improvements in terms of several\nobjective speech quality metrics such as PESQ and SRMR. We also discuss the\nbenefits of the parameterizable CDR estimator for varying sound environments\nand briefly reflect on several informal subjective evaluations using a\nlow-latency real-time framework.", "published": "2022-07-17 22:56:24", "link": "http://arxiv.org/abs/2207.08314v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Smart speaker design and implementation with biometric authentication\n  and advanced voice interaction capability", "abstract": "Advancements in semiconductor technology have reduced dimensions and cost\nwhile improving the performance and capacity of chipsets. In addition,\nadvancement in the AI frameworks and libraries brings possibilities to\naccommodate more AI at the resource-constrained edge of consumer IoT devices.\nSensors are nowadays an integral part of our environment which provide\ncontinuous data streams to build intelligent applications. An example could be\na smart home scenario with multiple interconnected devices. In such smart\nenvironments, for convenience and quick access to web-based service and\npersonal information such as calendars, notes, emails, reminders, banking, etc,\nusers link third-party skills or skills from the Amazon store to their smart\nspeakers. Also, in current smart home scenarios, several smart home products\nsuch as smart security cameras, video doorbells, smart plugs, smart carbon\nmonoxide monitors, and smart door locks, etc. are interlinked to a modern smart\nspeaker via means of custom skill addition. Since smart speakers are linked to\nsuch services and devices via the smart speaker user's account. They can be\nused by anyone with physical access to the smart speaker via voice commands. If\ndone so, the data privacy, home security and other aspects of the user get\ncompromised. Recently launched, Tensor Cam's AI Camera, Toshiba's Symbio,\nFacebook's Portal are camera-enabled smart speakers with AI functionalities.\nAlthough they are camera-enabled, yet they do not have an authentication scheme\nin addition to calling out the wake-word. This paper provides an overview of\ncybersecurity risks faced by smart speaker users due to lack of authentication\nscheme and discusses the development of a state-of-the-art camera-enabled,\nmicrophone array-based modern Alexa smart speaker prototype to address these\nrisks.", "published": "2022-07-17 04:55:24", "link": "http://arxiv.org/abs/2207.10811v1", "categories": ["cs.CR", "cs.SD", "eess.AS"], "primary_category": "cs.CR"}
