{"title": "Leveraging Deep Neural Networks and Knowledge Graphs for Entity\n  Disambiguation", "abstract": "Entity Disambiguation aims to link mentions of ambiguous entities to a\nknowledge base (e.g., Wikipedia). Modeling topical coherence is crucial for\nthis task based on the assumption that information from the same semantic\ncontext tends to belong to the same topic. This paper presents a novel deep\nsemantic relatedness model (DSRM) based on deep neural networks (DNN) and\nsemantic knowledge graphs (KGs) to measure entity semantic relatedness for\ntopical coherence modeling. The DSRM is directly trained on large-scale KGs and\nit maps heterogeneous types of knowledge of an entity from KGs to numerical\nfeature vectors in a latent space such that the distance between two\nsemantically-related entities is minimized. Compared with the state-of-the-art\nrelatedness approach proposed by (Milne and Witten, 2008a), the DSRM obtains\n19.4% and 24.5% reductions in entity disambiguation errors on two publicly\navailable datasets respectively.", "published": "2015-04-28 22:47:25", "link": "http://arxiv.org/abs/1504.07678v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reader-Aware Multi-Document Summarization via Sparse Coding", "abstract": "We propose a new MDS paradigm called reader-aware multi-document\nsummarization (RA-MDS). Specifically, a set of reader comments associated with\nthe news reports are also collected. The generated summaries from the reports\nfor the event should be salient according to not only the reports but also the\nreader comments. To tackle this RA-MDS problem, we propose a\nsparse-coding-based method that is able to calculate the salience of the text\nunits by jointly considering news reports and reader comments. Another\nreader-aware characteristic of our framework is to improve linguistic quality\nvia entity rewriting. The rewriting consideration is jointly assessed together\nwith other summarization requirements under a unified optimization model. To\nsupport the generation of compressive summaries via optimization, we explore a\nfiner syntactic unit, namely, noun/verb phrase. In this work, we also generate\na data set for conducting RA-MDS. Extensive experiments on this data set and\nsome classical data sets demonstrate the effectiveness of our proposed\napproach.", "published": "2015-04-28 01:34:33", "link": "http://arxiv.org/abs/1504.07324v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CommentWatcher: An Open Source Web-based platform for analyzing\n  discussions on web forums", "abstract": "We present CommentWatcher, an open source tool aimed at analyzing discussions\non web forums. Constructed as a web platform, CommentWatcher features automatic\nmass fetching of user posts from forum on multiple sites, extracting topics,\nvisualizing the topics as an expression cloud and exploring their temporal\nevolution. The underlying social network of users is simultaneously constructed\nusing the citation relations between users and visualized as a graph structure.\nOur platform addresses the issues of the diversity and dynamics of structures\nof webpages hosting the forums by implementing a parser architecture that is\nindependent of the HTML structure of webpages. This allows easy on-the-fly\nadding of new websites. Two types of users are targeted: end users who seek to\nstudy the discussed topics and their temporal evolution, and researchers in\nneed of establishing a forum benchmark dataset and comparing the performances\nof analysis tools.", "published": "2015-04-28 13:18:00", "link": "http://arxiv.org/abs/1504.07459v1", "categories": ["cs.CL", "cs.SI", "H.3.5; I.2.7; H.3.5"], "primary_category": "cs.CL"}
{"title": "Lexical Translation Model Using a Deep Neural Network Architecture", "abstract": "In this paper we combine the advantages of a model using global source\nsentence contexts, the Discriminative Word Lexicon, and neural networks. By\nusing deep neural networks instead of the linear maximum entropy model in the\nDiscriminative Word Lexicon models, we are able to leverage dependencies\nbetween different source words due to the non-linearity. Furthermore, the\nmodels for different target words can share parameters and therefore data\nsparsity problems are effectively reduced.\n  By using this approach in a state-of-the-art translation system, we can\nimprove the performance by up to 0.5 BLEU points for three different language\npairs on the TED translation task.", "published": "2015-04-28 09:43:40", "link": "http://arxiv.org/abs/1504.07395v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
