{"title": "Misgendering and Assuming Gender in Machine Translation when Working\n  with Low-Resource Languages", "abstract": "This chapter focuses on gender-related errors in machine translation (MT) in\nthe context of low-resource languages. We begin by explaining what low-resource\nlanguages are, examining the inseparable social and computational factors that\ncreate such linguistic hierarchies. We demonstrate through a case study of our\nmother tongue Bengali, a global language spoken by almost 300 million people\nbut still classified as low-resource, how gender is assumed and inferred in\ntranslations to and from the high(est)-resource English when no such\ninformation is provided in source texts. We discuss the postcolonial and\nsocietal impacts of such errors leading to linguistic erasure and\nrepresentational harms, and conclude by discussing potential solutions towards\nuplifting languages by providing them more agency in MT conversations.", "published": "2024-01-24 00:58:30", "link": "http://arxiv.org/abs/2401.13165v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert\n  Judgments For Open-Domain Question Answering", "abstract": "Question answering (QA) can only make progress if we know if an answer is\ncorrect, but for many of the most challenging and interesting QA examples,\ncurrent evaluation metrics to determine answer equivalence (AE) often do not\nalign with human judgments, particularly more verbose, free-form answers from\nlarge language models (LLM). There are two challenges: a lack of data and that\nmodels are too big: LLM-based scorers can correlate better with human judges,\nbut this task has only been tested on limited QA datasets, and even when\navailable, update of the model is limited because LLMs are large and often\nexpensive. We rectify both of these issues by providing clear and consistent\nguidelines for evaluating AE in machine QA adopted from professional human QA\ncontests. We also introduce a combination of standard evaluation and a more\nefficient, robust, and lightweight discriminate AE classifier-based matching\nmethod (CFMatch, smaller than 1 MB), trained and validated to more accurately\nevaluate answer correctness in accordance with adopted expert AE rules that are\nmore aligned with human judgments.", "published": "2024-01-24 01:30:25", "link": "http://arxiv.org/abs/2401.13170v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ULTRA: Unleash LLMs' Potential for Event Argument Extraction through\n  Hierarchical Modeling and Pair-wise Self-Refinement", "abstract": "Structural extraction of events within discourse is critical since it avails\na deeper understanding of communication patterns and behavior trends. Event\nargument extraction (EAE), at the core of event-centric understanding, is the\ntask of identifying role-specific text spans (i.e., arguments) for a given\nevent. Document-level EAE (DocEAE) focuses on arguments that are scattered\nacross an entire document. In this work, we explore open-source Large Language\nModels (LLMs) for DocEAE, and propose ULTRA, a hierarchical framework that\nextracts event arguments more cost-effectively. Further, it alleviates the\npositional bias issue intrinsic to LLMs. ULTRA sequentially reads text chunks\nof a document to generate a candidate argument set, upon which non-pertinent\ncandidates are dropped through self-refinement. We introduce LEAFER to address\nthe challenge LLMs face in locating the exact boundary of an argument. ULTRA\noutperforms strong baselines, including strong supervised models and ChatGPT,\nby 9.8% when evaluated by Exact Match (EM).", "published": "2024-01-24 04:13:28", "link": "http://arxiv.org/abs/2401.13218v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SEER: Facilitating Structured Reasoning and Explanation via\n  Reinforcement Learning", "abstract": "Elucidating the reasoning process with structured explanations from question\nto answer is crucial, as it significantly enhances the interpretability,\ntraceability, and trustworthiness of question-answering (QA) systems. However,\nstructured explanations demand models to perform intricately structured\nreasoning, which poses great challenges. Most existing methods focus on\nsingle-step reasoning through supervised learning, ignoring logical\ndependencies between steps. Moreover, existing reinforcement learning (RL)\nbased methods overlook the structured relationships, underutilizing the\npotential of RL in structured reasoning. In this paper, we propose SEER, a\nnovel method that maximizes a structure-based return to facilitate structured\nreasoning and explanation. Our proposed structure-based return precisely\ndescribes the hierarchical and branching structure inherent in structured\nreasoning, effectively capturing the intricate relationships between different\nreasoning steps. In addition, we introduce a fine-grained reward function to\nmeticulously delineate diverse reasoning steps. Extensive experiments show that\nSEER significantly outperforms state-of-the-art methods, achieving an absolute\nimprovement of 6.9% over RL-based methods on EntailmentBank, a 4.4% average\nimprovement on STREET benchmark, and exhibiting outstanding efficiency and\ncross-dataset generalization performance. Our code is available at\nhttps://github.com/Chen-GX/SEER.", "published": "2024-01-24 06:10:51", "link": "http://arxiv.org/abs/2401.13246v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaLA-500: Massive Language Adaptation of Large Language Models", "abstract": "Large language models (LLMs) have advanced the state of the art in natural\nlanguage processing. However, their predominant design for English or a limited\nset of languages creates a substantial gap in their effectiveness for\nlow-resource languages. To bridge this gap, we introduce MaLA-500, a novel\nlarge language model designed to cover an extensive range of 534 languages. To\ntrain MaLA-500, we employ vocabulary extension and continued pretraining on\nLLaMA 2 with Glot500-c. Our intrinsic evaluation demonstrates that MaLA-500 is\nbetter at predicting the given texts of low-resource languages than existing\nmultilingual LLMs. Moreover, the extrinsic evaluation of in-context learning\nshows that MaLA-500 outperforms previous LLMs on SIB200 and Taxi1500 by a\nsignificant margin, i.e., 11.68% and 4.82% marco-average accuracy across\nlanguages. We release MaLA-500 at https://huggingface.co/MaLA-LM", "published": "2024-01-24 08:57:39", "link": "http://arxiv.org/abs/2401.13303v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can GPT-3.5 Generate and Code Discharge Summaries?", "abstract": "Objective: To investigate GPT-3.5 in generating and coding medical documents\nwith ICD-10 codes for data augmentation on low-resources labels.\n  Materials and Methods: Employing GPT-3.5 we generated and coded 9,606\ndischarge summaries based on lists of ICD-10 code descriptions of patients with\ninfrequent (generation) codes within the MIMIC-IV dataset. Combined with the\nbaseline training set, this formed an augmented training set. Neural coding\nmodels were trained on baseline and augmented data and evaluated on a MIMIC-IV\ntest set. We report micro- and macro-F1 scores on the full codeset, generation\ncodes, and their families. Weak Hierarchical Confusion Matrices were employed\nto determine within-family and outside-of-family coding errors in the latter\ncodesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided\nself-generated data and real MIMIC-IV data. Clinical professionals evaluated\nthe clinical acceptability of the generated documents.\n  Results: Augmentation slightly hinders the overall performance of the models\nbut improves performance for the generation candidate codes and their families,\nincluding one unseen in the baseline training data. Augmented models display\nlower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the\nprompted descriptions, but performs poorly on real data. Evaluators note the\ncorrectness of generated concepts while suffering in variety, supporting\ninformation, and narrative.\n  Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.\nAugmentation positively affects generation code families but mainly benefits\ncodes with existing examples. Augmentation reduces out-of-family errors.\nDischarge summaries generated by GPT-3.5 state prompted concepts correctly but\nlack variety, and authenticity in narratives. They are unsuitable for clinical\npractice.", "published": "2024-01-24 15:10:13", "link": "http://arxiv.org/abs/2401.13512v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Malaysian Language Model Based on Mistral for Enhanced Local\n  Language Understanding", "abstract": "In this paper, we present significant advancements in the pretraining of\nMistral 7B, a large-scale language model, using a dataset of 32.6 GB,\nequivalent to 1.1 billion tokens. We explore the impact of extending the\ncontext length, releasing models with context lengths of 4096 and 32768 tokens,\nand further refining performance with a specialized 16384 context length\ninstruction-tuned model, we called it Malaysian Mistral.\n  Our experiments demonstrate the efficacy of continue pretraining and the\ninfluence of extended context lengths on Mistral 7B's language understanding\ncapabilities. Additionally, we release a model specifically tuned with a 16384\ncontext length instruction, showcasing its potential for capturing nuanced\nlanguage intricacies.\n  Furthermore, our research contributes to the benchmarking of Malaysian\nMistral against prominent language models, including ChatGPT3.5 and Claude 2.\nWe present compelling results indicating Malaysian Mistral's superior\nperformance on Tatabahasa (Malay grammar) test set, particularly when\nfine-tuned with instructions.\n  All models released at\nhttps://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c", "published": "2024-01-24 16:21:28", "link": "http://arxiv.org/abs/2401.13565v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Consistency Guided Knowledge Retrieval and Denoising in LLMs for\n  Zero-shot Document-level Relation Triplet Extraction", "abstract": "Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in\ninformation systems that aims to simultaneously extract entities with semantic\nrelations from a document. Existing methods heavily rely on a substantial\namount of fully labeled data. However, collecting and annotating data for newly\nemerging relations is time-consuming and labor-intensive. Recent advanced Large\nLanguage Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text\ngeneration capabilities, inspiring us to explore an alternative approach for\nobtaining auto-labeled documents with new relations. In this paper, we propose\na Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework,\nwhich generates labeled data by retrieval and denoising knowledge from LLMs,\ncalled GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide\nChatGPT to generate labeled long-text data step by step. To improve the quality\nof synthetic data, we propose a denoising strategy based on the consistency of\ncross-document knowledge. Leveraging our denoised synthetic data, we proceed to\nfine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.\nWe perform experiments for both zero-shot document-level relation and triplet\nextraction on two public datasets. The experimental results illustrate that our\nGenRDK framework outperforms strong baselines.", "published": "2024-01-24 17:04:28", "link": "http://arxiv.org/abs/2401.13598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MM-LLMs: Recent Advances in MultiModal Large Language Models", "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone\nsubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs or\noutputs via cost-effective training strategies. The resulting models not only\npreserve the inherent reasoning and decision-making capabilities of LLMs but\nalso empower a diverse range of MM tasks. In this paper, we provide a\ncomprehensive survey aimed at facilitating further research of MM-LLMs.\nInitially, we outline general design formulations for model architecture and\ntraining pipeline. Subsequently, we introduce a taxonomy encompassing 126\nMM-LLMs, each characterized by its specific formulations. Furthermore, we\nreview the performance of selected MM-LLMs on mainstream benchmarks and\nsummarize key training recipes to enhance the potency of MM-LLMs. Finally, we\nexplore promising directions for MM-LLMs while concurrently maintaining a\nreal-time tracking website for the latest developments in the field. We hope\nthat this survey contributes to the ongoing advancement of the MM-LLMs domain.", "published": "2024-01-24 17:10:45", "link": "http://arxiv.org/abs/2401.13601v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DenoSent: A Denoising Objective for Self-Supervised Sentence\n  Representation Learning", "abstract": "Contrastive-learning-based methods have dominated sentence representation\nlearning. These methods regularize the representation space by pulling similar\nsentence representations closer and pushing away the dissimilar ones and have\nbeen proven effective in various NLP tasks, e.g., semantic textual similarity\n(STS) tasks. However, it is challenging for these methods to learn fine-grained\nsemantics as they only learn from the inter-sentence perspective, i.e., their\nsupervision signal comes from the relationship between data samples. In this\nwork, we propose a novel denoising objective that inherits from another\nperspective, i.e., the intra-sentence perspective. By introducing both discrete\nand continuous noise, we generate noisy sentences and then train our model to\nrestore them to their original form. Our empirical evaluations demonstrate that\nthis approach delivers competitive results on both semantic textual similarity\n(STS) and a wide range of transfer tasks, standing up well in comparison to\ncontrastive-learning-based methods. Notably, the proposed intra-sentence\ndenoising objective complements existing inter-sentence contrastive\nmethodologies and can be integrated with them to further enhance performance.\nOur code is available at https://github.com/xinghaow99/DenoSent.", "published": "2024-01-24 17:48:45", "link": "http://arxiv.org/abs/2401.13621v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Unified Approach to Emotion Detection and Task-Oriented Dialogue\n  Modeling", "abstract": "In current text-based task-oriented dialogue (TOD) systems, user emotion\ndetection (ED) is often overlooked or is typically treated as a separate and\nindependent task, requiring additional training. In contrast, our work\ndemonstrates that seamlessly unifying ED and TOD modeling brings about mutual\nbenefits, and is therefore an alternative to be considered. Our method consists\nin augmenting SimpleToD, an end-to-end TOD system, by extending belief state\ntracking to include ED, relying on a single language model. We evaluate our\napproach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ\nannotated with emotions. Our results reveal a general increase in performance\nfor ED and task results. Our findings also indicate that user emotions provide\nuseful contextual conditioning for system responses, and can be leveraged to\nfurther refine responses in terms of empathy.", "published": "2024-01-24 20:17:11", "link": "http://arxiv.org/abs/2401.13789v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "APT-Pipe: A Prompt-Tuning Tool for Social Data Annotation using ChatGPT", "abstract": "Recent research has highlighted the potential of LLM applications, like\nChatGPT, for performing label annotation on social computing text. However, it\nis already well known that performance hinges on the quality of the input\nprompts. To address this, there has been a flurry of research into prompt\ntuning -- techniques and guidelines that attempt to improve the quality of\nprompts. Yet these largely rely on manual effort and prior knowledge of the\ndataset being annotated. To address this limitation, we propose APT-Pipe, an\nautomated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts\nto enhance ChatGPT's text classification performance on any given dataset. We\nimplement APT-Pipe and test it across twelve distinct text classification\ndatasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher\nweighted F1-score on nine out of twelve experimented datasets, with an\nimprovement of 7.01% on average. We further highlight APT-Pipe's flexibility as\na framework by showing how it can be extended to support additional tuning\nmechanisms.", "published": "2024-01-24 10:09:11", "link": "http://arxiv.org/abs/2402.01697v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced\n  Token Detection", "abstract": "Pre-training large language models is known to be extremely resource\nintensive and often times inefficient, under-utilizing the information\nencapsulated in the training text sequences. In this paper, we present SpacTor,\na new training procedure consisting of (1) a hybrid objective combining span\ncorruption (SC) and token replacement detection (RTD), and (2) a two-stage\ncurriculum that optimizes the hybrid objective over the initial $\\tau$\niterations, then transitions to standard SC loss. We show empirically that the\neffectiveness of the hybrid objective is tied to the two-stage pre-training\nschedule, and provide extensive analysis on why this is the case. In our\nexperiments with encoder-decoder architectures (T5) on a variety of NLP tasks,\nSpacTor-T5 yields the same downstream performance as standard SC pre-training,\nwhile enabling a 50% reduction in pre-training iterations and 40% reduction in\ntotal FLOPs. Alternatively, given the same amount of computing budget, we find\nthat SpacTor results in significantly improved downstream benchmark\nperformance.", "published": "2024-01-24 00:36:13", "link": "http://arxiv.org/abs/2401.13160v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over\n  Tabular and Textual Data", "abstract": "In this work, we address question answering (QA) over a hybrid of tabular and\ntextual data that are very common content on the Web (e.g. SEC filings), where\ndiscrete reasoning capabilities are often required. Recently, large language\nmodels (LLMs) like GPT-4 have demonstrated strong multi-step reasoning\ncapabilities. We then consider harnessing the amazing power of LLMs to solve\nour task. We abstract a Step-wise Pipeline for tabular and textual QA, which\nconsists of three key steps, including Extractor, Reasoner and Executor, and\ninitially design an instruction to instantiate the pipeline and validate that\nGPT-4 outperforms all existing methods. However, utilizing an online LLM like\nGPT-4 holds various challenges in terms of cost, latency, and data security\nrisk, which motivates us to specialize smaller LLMs in this task. We develop a\nTAT-LLM language model by fine-tuning LLaMA 2 with the training data generated\nautomatically from existing expert-annotated datasets following the Step-wise\nPipeline. The experimental results have verified that our TAT-LLM model can\noutperform all baseline models, including the previous best fine-tuned models\nand very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.", "published": "2024-01-24 04:28:50", "link": "http://arxiv.org/abs/2401.13223v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n  Personalized Dialogue Systems", "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.", "published": "2024-01-24 06:50:20", "link": "http://arxiv.org/abs/2401.13256v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can AI Assistants Know What They Don't Know?", "abstract": "Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.", "published": "2024-01-24 07:34:55", "link": "http://arxiv.org/abs/2401.13275v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Explainable Harmful Meme Detection through Multimodal Debate\n  between Large Language Models", "abstract": "The age of social media is flooded with Internet memes, necessitating a clear\ngrasp and effective identification of harmful ones. This task presents a\nsignificant challenge due to the implicit meaning embedded in memes, which is\nnot explicitly conveyed through the surface text and image. However, existing\nharmful meme detection methods do not present readable explanations that unveil\nsuch implicit meaning to support their detection decisions. In this paper, we\npropose an explainable approach to detect harmful memes, achieved through\nreasoning over conflicting rationales from both harmless and harmful positions.\nSpecifically, inspired by the powerful capacity of Large Language Models (LLMs)\non text generation and reasoning, we first elicit multimodal debate between\nLLMs to generate the explanations derived from the contradictory arguments.\nThen we propose to fine-tune a small language model as the debate judge for\nharmfulness inference, to facilitate multimodal fusion between the harmfulness\nrationales and the intrinsic multimodal information within memes. In this way,\nour model is empowered to perform dialectical reasoning over intricate and\nimplicit harm-indicative patterns, utilizing multimodal explanations\noriginating from both harmless and harmful arguments. Extensive experiments on\nthree public meme datasets demonstrate that our harmful meme detection approach\nachieves much better performance than state-of-the-art methods and exhibits a\nsuperior capacity for explaining the meme harmfulness of the model predictions.", "published": "2024-01-24 08:37:16", "link": "http://arxiv.org/abs/2401.13298v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document\n  Understanding with Instructions", "abstract": "We study the problem of completing various visual document understanding\n(VDU) tasks, e.g., question answering and information extraction, on real-world\ndocuments through human-written instructions. To this end, we propose\nInstructDoc, the first large-scale collection of 30 publicly available VDU\ndatasets, each with diverse instructions in a unified format, which covers a\nwide range of 12 tasks and includes open document types/formats. Furthermore,\nto enhance the generalization performance on VDU tasks, we design a new\ninstruction-based document reading and understanding model, InstructDr, that\nconnects document images, image encoders, and large language models (LLMs)\nthrough a trainable bridging module. Experiments demonstrate that InstructDr\ncan effectively adapt to new VDU datasets, tasks, and domains via given\ninstructions and outperforms existing multimodal LLMs and ChatGPT without\nspecific training.", "published": "2024-01-24 09:09:37", "link": "http://arxiv.org/abs/2401.13313v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Text Categorization Can Enhance Domain-Agnostic Stopword Extraction", "abstract": "This paper investigates the role of text categorization in streamlining\nstopword extraction in natural language processing (NLP), specifically focusing\non nine African languages alongside French. By leveraging the MasakhaNEWS,\nAfrican Stopwords Project, and MasakhaPOS datasets, our findings emphasize that\ntext categorization effectively identifies domain-agnostic stopwords with over\n80% detection success rate for most examined languages. Nevertheless,\nlinguistic variances result in lower detection rates for certain languages.\nInterestingly, we find that while over 40% of stopwords are common across news\ncategories, less than 15% are unique to a single category. Uncommon stopwords\nadd depth to text but their classification as stopwords depends on context.\nTherefore combining statistical and linguistic approaches creates comprehensive\nstopword lists, highlighting the value of our hybrid method. This research\nenhances NLP for African languages and underscores the importance of text\ncategorization in stopword extraction.", "published": "2024-01-24 11:52:05", "link": "http://arxiv.org/abs/2401.13398v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Stateful Knowledge Exploration: A Novel Paradigm for\n  Integrating Knowledge Graphs with Large Language Models", "abstract": "Large Language Models (LLMs) have shown impressive capabilities, yet updating\ntheir knowledge remains a significant challenge, often leading to outdated or\ninaccurate responses. A proposed solution is the integration of external\nknowledge bases, such as knowledge graphs, with LLMs. Most existing methods use\na paradigm that treats the question as the objective, with relevant knowledge\nbeing incrementally retrieved from the knowledge graph. However, this strategy\nfrequently experiences an mismatch in the granularity of knowledge between the\ntarget question and the entities and relations being retrieved. As a result,\nthe information in the question cannot precisely correspond to the retrieved\nknowledge. This may cause redundant exploration or omission of vital knowledge,\nthereby leading to enhanced computational consumption and reduced retrieval\naccuracy. In this paper, we propose a novel paradigm of fine-grained stateful\nknowledge exploration, which addresses the `information granularity mismatch'\nissue. We extract fine-grained information from questions and explore the\nsemantic mapping between this information and the knowledge in graph. By\ndynamically updating the mapping records, we avoid redundant exploration and\nensure no pertinent information is overlooked, thereby reducing computational\noverhead and improving the accuracy of knowledge exploration. The use of\nfine-grained information also eliminates the need for a priori knowledge, a\ncommon requirement in existing methods. Experiments on multiple datasets\nrevealed that our paradigm surpasses current advanced methods in knowledge\nretrieval while significantly reducing the average number of LLM invocations.", "published": "2024-01-24 13:36:50", "link": "http://arxiv.org/abs/2401.13444v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Graph Guided Question Answer Generation for Procedural\n  Question-Answering", "abstract": "In this paper, we focus on task-specific question answering (QA). To this\nend, we introduce a method for generating exhaustive and high-quality training\ndata, which allows us to train compact (e.g., run on a mobile device),\ntask-specific QA models that are competitive against GPT variants. The key\ntechnological enabler is a novel mechanism for automatic question-answer\ngeneration from procedural text which can ingest large amounts of textual\ninstructions and produce exhaustive in-domain QA training data. While current\nQA data generation methods can produce well-formed and varied data, their\nnon-exhaustive nature is sub-optimal for training a QA model. In contrast, we\nleverage the highly structured aspect of procedural text and represent each\nstep and the overall flow of the procedure as graphs. We then condition on\ngraph nodes to automatically generate QA pairs in an exhaustive and\ncontrollable manner. Comprehensive evaluations of our method show that: 1)\nsmall models trained with our data achieve excellent performance on the target\nQA task, even exceeding that of GPT3 and ChatGPT despite being several orders\nof magnitude smaller. 2) semantic coverage is the key indicator for downstream\nQA performance. Crucially, while large language models excel at syntactic\ndiversity, this does not necessarily result in improvements on the end QA\nmodel. In contrast, the higher semantic coverage provided by our method is\ncritical for QA performance.", "published": "2024-01-24 17:01:42", "link": "http://arxiv.org/abs/2401.13594v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MambaByte: Token-free Selective State Space Model", "abstract": "Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.", "published": "2024-01-24 18:53:53", "link": "http://arxiv.org/abs/2401.13660v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automated Root Causing of Cloud Incidents using In-Context Learning with\n  GPT-4", "abstract": "Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis\nprocess for cloud services, requiring on-call engineers to identify the primary\nissues and implement corrective actions to prevent future recurrences.\nImproving the incident RCA process is vital for minimizing service downtime,\ncustomer impact and manual toil. Recent advances in artificial intelligence\nhave introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which\nhave proven effective in tackling various AIOps problems, ranging from code\nauthoring to incident management. Nonetheless, the GPT-4 model's immense size\npresents challenges when trying to fine-tune it on user data because of the\nsignificant GPU resource demand and the necessity for continuous model\nfine-tuning with the emergence of new data. To address the high cost of\nfine-tuning LLM, we propose an in-context learning approach for automated root\ncausing, which eliminates the need for fine-tuning. We conduct extensive study\nover 100,000 production incidents, comparing several large language models\nusing multiple metrics. The results reveal that our in-context learning\napproach outperforms the previous fine-tuned large language models such as\nGPT-3 by an average of 24.8\\% across all metrics, with an impressive 49.7\\%\nimprovement over the zero-shot model. Moreover, human evaluation involving\nactual incident owners demonstrates its superiority over the fine-tuned model,\nachieving a 43.5\\% improvement in correctness and an 8.7\\% enhancement in\nreadability. The impressive results demonstrate the viability of utilizing a\nvanilla GPT model for the RCA task, thereby avoiding the high computational and\nmaintenance costs associated with a fine-tuned model.", "published": "2024-01-24 21:02:07", "link": "http://arxiv.org/abs/2401.13810v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery\n  and Guidance", "abstract": "Large Language Models (LLMs) have recently showcased remarkable reasoning\nabilities. However, larger models often surpass their smaller counterparts in\nreasoning tasks, posing the challenge of effectively transferring these\ncapabilities from larger models. Existing approaches heavily rely on extensive\nfine-tuning data or continuous interactions with a superior teacher LLM during\ninference. We introduce a principle-based teacher-student framework called\n``Teaching via Principle Discovery'' (TPD) to address these limitations.\nInspired by human learning mechanisms, TPD mimics the interaction between a\nteacher and a student using a principle-based approach. The teacher LLM\ngenerates problem-solving instructions and corrective principles based on the\nstudent LLM's errors. These principles guide the refinement of instructions and\nthe selection of instructive examples from a validation set. This enables the\nstudent model to learn from both the teacher's guidance and its own mistakes.\nOnce the student model begins making inferences, TPD requires no further\nintervention from the teacher LLM or humans. Through extensive experiments\nacross eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared\nto standard chain-of-thought prompting, TPD significantly improves the student\nmodel's performance, achieving $6.2\\%$ improvement on average.", "published": "2024-01-24 23:11:33", "link": "http://arxiv.org/abs/2401.13849v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Better Call GPT, Comparing Large Language Models Against Lawyers", "abstract": "This paper presents a groundbreaking comparison between Large Language Models\nand traditional legal contract reviewers, Junior Lawyers and Legal Process\nOutsourcers. We dissect whether LLMs can outperform humans in accuracy, speed,\nand cost efficiency during contract review. Our empirical analysis benchmarks\nLLMs against a ground truth set by Senior Lawyers, uncovering that advanced\nmodels match or exceed human accuracy in determining legal issues. In speed,\nLLMs complete reviews in mere seconds, eclipsing the hours required by their\nhuman counterparts. Cost wise, LLMs operate at a fraction of the price,\noffering a staggering 99.97 percent reduction in cost over traditional methods.\nThese results are not just statistics, they signal a seismic shift in legal\npractice. LLMs stand poised to disrupt the legal industry, enhancing\naccessibility and efficiency of legal services. Our research asserts that the\nera of LLM dominance in legal contract review is upon us, challenging the\nstatus quo and calling for a reimagined future of legal workflows.", "published": "2024-01-24 03:53:28", "link": "http://arxiv.org/abs/2401.16212v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text\n  Classification", "abstract": "Hierarchical text classification (HTC) is a complex subtask under multi-label\ntext classification, characterized by a hierarchical label taxonomy and data\nimbalance. The best-performing models aim to learn a static representation by\ncombining document and hierarchical label information. However, the relevance\nof document sections can vary based on the hierarchy level, necessitating a\ndynamic document representation. To address this, we propose HiGen, a\ntext-generation-based framework utilizing language models to encode dynamic\ntext representations. We introduce a level-guided loss function to capture the\nrelationship between text and label name semantics. Our approach incorporates a\ntask-specific pretraining strategy, adapting the language model to in-domain\nknowledge and significantly enhancing performance for classes with limited\nexamples. Furthermore, we present a new and valuable dataset called ENZYME,\ndesigned for HTC, which comprises articles from PubMed with the goal of\npredicting Enzyme Commission (EC) numbers. Through extensive experiments on the\nENZYME dataset and the widely recognized WOS and NYT datasets, our methodology\ndemonstrates superior performance, surpassing existing approaches while\nefficiently handling data and mitigating class imbalance. The data and code\nwill be released publicly.", "published": "2024-01-24 04:44:42", "link": "http://arxiv.org/abs/2402.01696v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large language model empowered participatory urban planning", "abstract": "Participatory urban planning is the mainstream of modern urban planning and\ninvolves the active engagement of different stakeholders. However, the\ntraditional participatory paradigm encounters challenges in time and manpower,\nwhile the generative planning tools fail to provide adjustable and inclusive\nsolutions. This research introduces an innovative urban planning approach\nintegrating Large Language Models (LLMs) within the participatory process. The\nframework, based on the crafted LLM agent, consists of role-play, collaborative\ngeneration, and feedback iteration, solving a community-level land-use task\ncatering to 1000 distinct interests. Empirical experiments in diverse urban\ncommunities exhibit LLM's adaptability and effectiveness across varied planning\nscenarios. The results were evaluated on four metrics, surpassing human experts\nin satisfaction and inclusion, and rivaling state-of-the-art reinforcement\nlearning methods in service and ecology. Further analysis shows the advantage\nof LLM agents in providing adjustable and inclusive solutions with natural\nlanguage reasoning and strong scalability. While implementing the recent\nadvancements in emulating human behavior for planning, this work envisions both\nplanners and citizens benefiting from low-cost, efficient LLM agents, which is\ncrucial for enhancing participation and realizing participatory urban planning.", "published": "2024-01-24 10:50:01", "link": "http://arxiv.org/abs/2402.01698v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Question answering systems for health professionals at the point of care\n  -- a systematic review", "abstract": "Objective: Question answering (QA) systems have the potential to improve the\nquality of clinical care by providing health professionals with the latest and\nmost relevant evidence. However, QA systems have not been widely adopted. This\nsystematic review aims to characterize current medical QA systems, assess their\nsuitability for healthcare, and identify areas of improvement.\n  Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library,\nACL Anthology and forward and backward citations on 7th February 2023. We\nincluded peer-reviewed journal and conference papers describing the design and\nevaluation of biomedical QA systems. Two reviewers screened titles, abstracts,\nand full-text articles. We conducted a narrative synthesis and risk of bias\nassessment for each study. We assessed the utility of biomedical QA systems.\n  Results: We included 79 studies and identified themes, including question\nrealism, answer reliability, answer utility, clinical specialism, systems,\nusability, and evaluation methods. Clinicians' questions used to train and\nevaluate QA systems were restricted to certain sources, types and complexity\nlevels. No system communicated confidence levels in the answers or sources.\nMany studies suffered from high risks of bias and applicability concerns. Only\n8 studies completely satisfied any criterion for clinical utility, and only 7\nreported user evaluations. Most systems were built with limited input from\nclinicians.\n  Discussion: While machine learning methods have led to increased accuracy,\nmost studies imperfectly reflected real-world healthcare information needs. Key\nresearch priorities include developing more realistic healthcare QA datasets\nand considering the reliability of answer sources, rather than merely focusing\non accuracy.", "published": "2024-01-24 13:47:39", "link": "http://arxiv.org/abs/2402.01700v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Fluent dreaming for language models", "abstract": "Feature visualization, also known as \"dreaming\", offers insights into vision\nmodels by optimizing the inputs to maximize a neuron's activation or other\ninternal component. However, dreaming has not been successfully applied to\nlanguage models because the input space is discrete. We extend Greedy\nCoordinate Gradient, a method from the language model adversarial attack\nliterature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO\noptimizes the input prompt to simultaneously maximize the Pareto frontier\nbetween a chosen internal feature and prompt fluency, enabling fluent dreaming\nfor language models. We demonstrate dreaming with neurons, output logits and\narbitrary directions in activation space. We measure the fluency of the\nresulting prompts and compare language model dreaming with max-activating\ndataset examples. Critically, fluent dreaming allows automatically exploring\nthe behavior of model internals in reaction to mildly out-of-distribution\nprompts. Code for running EPO is available at\nhttps://github.com/Confirm-Solutions/dreamy. A companion page demonstrating\ncode usage is at https://confirmlabs.org/posts/dreamy.html", "published": "2024-01-24 17:57:12", "link": "http://arxiv.org/abs/2402.01702v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents", "abstract": "Evaluating Large Language Models (LLMs) as general-purpose agents is\nessential for understanding their capabilities and facilitating their\nintegration into practical applications. However, the evaluation process\npresents substantial challenges. A primary obstacle is the benchmarking of\nagent performance across diverse scenarios within a unified framework,\nespecially in maintaining partially-observable environments and ensuring\nmulti-round interactions. Moreover, current evaluation frameworks mostly focus\non the final success rate, revealing few insights during the process and\nfailing to provide a deep understanding of the model abilities. To address\nthese challenges, we introduce AgentBoard, a pioneering comprehensive benchmark\nand accompanied open-source evaluation framework tailored to analytical\nevaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric\nthat captures incremental advancements as well as a comprehensive evaluation\ntoolkit that features easy assessment of agents for multi-faceted analysis.\nThis not only sheds light on the capabilities and limitations of LLM agents but\nalso propels the interpretability of their performance to the forefront.\nUltimately, AgentBoard serves as a step towards demystifying agent behaviors\nand accelerating the development of stronger LLM agents.", "published": "2024-01-24 01:51:00", "link": "http://arxiv.org/abs/2401.13178v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MLLMReID: Multimodal Large Language Model-based Person Re-identification", "abstract": "Multimodal large language models (MLLM) have achieved satisfactory results in\nmany tasks. However, their performance in the task of ReID (ReID) has not been\nexplored to date. This paper will investigate how to adapt them for the task of\nReID. An intuitive idea is to fine-tune MLLM with ReID image-text datasets, and\nthen use their visual encoder as a backbone for ReID. However, there still\nexist two apparent issues: (1) Designing instructions for ReID, MLLMs may\noverfit specific instructions, and designing a variety of instructions will\nlead to higher costs. (2) When fine-tuning the visual encoder of a MLLM, it is\nnot trained synchronously with the ReID task. As a result, the effectiveness of\nthe visual encoder fine-tuning cannot be directly reflected in the performance\nof the ReID task. To address these problems, this paper proposes MLLMReID:\nMultimodal Large Language Model-based ReID. Firstly, we proposed Common\nInstruction, a simple approach that leverages the essence ability of LLMs to\ncontinue writing, avoiding complex and diverse instruction design. Secondly, we\npropose a multi-task learning-based synchronization module to ensure that the\nvisual encoder of the MLLM is trained synchronously with the ReID task. The\nexperimental results demonstrate the superiority of our method.", "published": "2024-01-24 03:07:26", "link": "http://arxiv.org/abs/2401.13201v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "LPNL: Scalable Link Prediction with Large Language Models", "abstract": "Exploring the application of large language models (LLMs) to graph learning\nis a emerging endeavor. However, the vast amount of information inherent in\nlarge graphs poses significant challenges to this process. This work focuses on\nthe link prediction task and introduces $\\textbf{LPNL}$ (Link Prediction via\nNatural Language), a framework based on large language models designed for\nscalable link prediction on large-scale heterogeneous graphs. We design novel\nprompts for link prediction that articulate graph details in natural language.\nWe propose a two-stage sampling pipeline to extract crucial information from\nthe graphs, and a divide-and-conquer strategy to control the input tokens\nwithin predefined limits, addressing the challenge of overwhelming information.\nWe fine-tune a T5 model based on our self-supervised learning designed for link\nprediction. Extensive experimental results demonstrate that LPNL outperforms\nmultiple advanced baselines in link prediction tasks on large-scale graphs.", "published": "2024-01-24 04:50:16", "link": "http://arxiv.org/abs/2401.13227v3", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "From Random to Informed Data Selection: A Diversity-Based Approach to\n  Optimize Human Annotation and Few-Shot Learning", "abstract": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.", "published": "2024-01-24 04:57:32", "link": "http://arxiv.org/abs/2401.13229v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion,\n  Asr Error Detection, and Asr Error Correction", "abstract": "The prevalent approach in speech emotion recognition (SER) involves\nintegrating both audio and textual information to comprehensively identify the\nspeaker's emotion, with the text generally obtained through automatic speech\nrecognition (ASR). An essential issue of this approach is that ASR errors from\nthe text modality can worsen the performance of SER. Previous studies have\nproposed using an auxiliary ASR error detection task to adaptively assign\nweights of each word in ASR hypotheses. However, this approach has limited\nimprovement potential because it does not address the coherence of semantic\ninformation in the text. Additionally, the inherent heterogeneity of different\nmodalities leads to distribution gaps between their representations, making\ntheir fusion challenging. Therefore, in this paper, we incorporate two\nauxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to\nenhance the semantic coherence of ASR text, and further introduce a novel\nmulti-modal fusion (MF) method to learn shared representations across\nmodalities. We refer to our method as MF-AED-AEC. Experimental results indicate\nthat MF-AED-AEC significantly outperforms the baseline model by a margin of\n4.1\\%.", "published": "2024-01-24 06:55:55", "link": "http://arxiv.org/abs/2401.13260v2", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken\n  Question Answering", "abstract": "Spoken Question Answering (SQA) is essential for machines to reply to user's\nquestion by finding the answer span within a given spoken passage. SQA has been\npreviously achieved without ASR to avoid recognition errors and\nOut-of-Vocabulary (OOV) problems. However, the real-world problem of\nOpen-domain SQA (openSQA), in which the machine needs to first retrieve\npassages that possibly contain the answer from a spoken archive in addition,\nwas never considered. This paper proposes the first known end-to-end framework,\nSpeech Dense Passage Retriever (SpeechDPR), for the retrieval component of the\nopenSQA problem. SpeechDPR learns a sentence-level semantic representation by\ndistilling knowledge from the cascading model of unsupervised ASR (UASR) and\ntext dense retriever (TDR). No manually transcribed speech data is needed.\nInitial experiments showed performance comparable to the cascading model of\nUASR and TDR, and significantly better when UASR was poor, verifying this\napproach is more robust to speech recognition errors.", "published": "2024-01-24 14:08:38", "link": "http://arxiv.org/abs/2401.13463v3", "categories": ["cs.CL", "cs.IR", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval", "abstract": "Multi-modal information retrieval (MMIR) is a rapidly evolving field, where\nsignificant progress, particularly in image-text pairing, has been made through\nadvanced representation learning and cross-modality alignment research.\nHowever, current benchmarks for evaluating MMIR performance in image-text\npairing within the scientific domain show a notable gap, where chart and table\nimages described in scholarly language usually do not play a significant role.\nTo bridge this gap, we develop a specialised scientific MMIR (SciMMIR)\nbenchmark by leveraging open-access paper collections to extract data relevant\nto the scientific domain. This benchmark comprises 530K meticulously curated\nimage-text pairs, extracted from figures and tables with detailed captions in\nscientific documents. We further annotate the image-text pairs with two-level\nsubset-subcategory hierarchy annotations to facilitate a more comprehensive\nevaluation of the baselines. We conducted zero-shot and fine-tuning evaluations\non prominent multi-modal image-captioning and visual language models, such as\nCLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific\ndomain, including the impact of pre-training and fine-tuning settings and the\ninfluence of the visual and textual encoders. All our data and checkpoints are\npublicly available at https://github.com/Wusiwei0410/SciMMIR.", "published": "2024-01-24 14:23:12", "link": "http://arxiv.org/abs/2401.13478v2", "categories": ["cs.IR", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.IR"}
{"title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment", "abstract": "Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- speaks to the interdependent process of cultural creation: creative ideas\nare built upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI and that participants may knowingly adopt AI ideas when the\ntask is difficult. Our findings suggest that introducing AI ideas may increase\ncollective diversity but not individual creativity.", "published": "2024-01-24 14:29:39", "link": "http://arxiv.org/abs/2401.13481v2", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.CY"}
{"title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation", "abstract": "Benefiting from effective speech modeling, current Speech Large Language\nModels (SLLMs) have demonstrated exceptional capabilities in in-context speech\ngeneration and efficient generalization to unseen speakers. However, the\nprevailing information modeling process is encumbered by certain redundancies,\nleading to inefficiencies in speech generation. We propose Chain-of-Information\nGeneration (CoIG), a method for decoupling semantic and perceptual information\nin large-scale speech generation. Building on this, we develop SpeechGPT-Gen,\nan 8-billion-parameter SLLM efficient in semantic and perceptual information\nmodeling. It comprises an autoregressive model based on LLM for semantic\ninformation modeling and a non-autoregressive model employing flow matching for\nperceptual information modeling. Additionally, we introduce the novel approach\nof infusing semantic information into the prior distribution to enhance the\nefficiency of flow matching. Extensive experimental results demonstrate that\nSpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice\nconversion, and speech-to-speech dialogue, underscoring CoIG's remarkable\nproficiency in capturing and modeling speech's semantic and perceptual\ndimensions. Code and models are available at\nhttps://github.com/0nutation/SpeechGPT.", "published": "2024-01-24 15:25:01", "link": "http://arxiv.org/abs/2401.13527v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Instruction Fine-Tuning: Does Prompt Loss Matter?", "abstract": "We present a novel study analyzing the effects of various prompt loss token\nweights (PLW) for supervised instruction fine-tuning (SIFT). While\nprompt-masking (PLW = 0) is common for SIFT, some fine-tuning APIs support\nfractional PLWs and suggest that using a small non-zero PLW can help stabilize\nlearning when fine-tuning on short-completion data. However, there has never\nbeen a study confirming this claim, and OpenAI, a major cloud-based SIFT\nprovider, recently removed this parameter from their fine-tuning API. We found\nthat performance of models fine-tuned on short-completion data had a\nstatistically-significant negative quadratic relationship with PLW. Using small\nvalues (0.01 - 0.5) of PLW produced better results on multiple-choice and\nshort-generation benchmarks (outperforming models fine-tuned on long-completion\ndata) while large values (~ 1.0) of PLW produced better results on\nlong-generation benchmarks. We explained this effect and verified its\nimportance through additional experiments. This research serves as a warning to\nAPI providers about the importance of providing a PLW parameter for SIFT.", "published": "2024-01-24 16:51:23", "link": "http://arxiv.org/abs/2401.13586v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Evaluation of General Large Language Models in Contextually Assessing\n  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record\n  Notes", "abstract": "The field of healthcare has increasingly turned its focus towards Large\nLanguage Models (LLMs) due to their remarkable performance. However, their\nperformance in actual clinical applications has been underexplored. Traditional\nevaluations based on question-answering tasks don't fully capture the nuanced\ncontexts. This gap highlights the need for more in-depth and practical\nassessments of LLMs in real-world healthcare settings. Objective: We sought to\nevaluate the performance of LLMs in the complex clinical context of adult\ncritical care medicine using systematic and comprehensible analytic methods,\nincluding clinician annotation and adjudication. Methods: We investigated the\nperformance of three general LLMs in understanding and processing real-world\nclinical notes. Concepts from 150 clinical notes were identified by MetaMap and\nthen labeled by 9 clinicians. Each LLM's proficiency was evaluated by\nidentifying the temporality and negation of these concepts using different\nprompts for an in-depth analysis. Results: GPT-4 showed overall superior\nperformance compared to other LLMs. In contrast, both GPT-3.5 and\ntext-davinci-003 exhibit enhanced performance when the appropriate prompting\nstrategies are employed. The GPT family models have demonstrated considerable\nefficiency, evidenced by their cost-effectiveness and time-saving capabilities.\nConclusion: A comprehensive qualitative performance evaluation framework for\nLLMs is developed and operationalized. This framework goes beyond singular\nperformance aspects. With expert annotations, this methodology not only\nvalidates LLMs' capabilities in processing complex medical data but also\nestablishes a benchmark for future LLM evaluations across specialized domains.", "published": "2024-01-24 16:52:37", "link": "http://arxiv.org/abs/2401.13588v1", "categories": ["cs.CL", "cs.AI", "cs.SE"], "primary_category": "cs.CL"}
{"title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web\n  Tasks", "abstract": "Autonomous agents capable of planning, reasoning, and executing actions on\nthe web offer a promising avenue for automating computer tasks. However, the\nmajority of existing benchmarks primarily focus on text-based agents,\nneglecting many natural tasks that require visual information to effectively\nsolve. Given that most computer interfaces cater to human perception, visual\ninformation often augments textual data in ways that text-only models struggle\nto harness effectively. To bridge this gap, we introduce VisualWebArena, a\nbenchmark designed to assess the performance of multimodal web agents on\nrealistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set\nof diverse and complex web-based tasks that evaluate various capabilities of\nautonomous multimodal agents. To perform on this benchmark, agents need to\naccurately process image-text inputs, interpret natural language instructions,\nand execute actions on websites to accomplish user-defined objectives. We\nconduct an extensive evaluation of state-of-the-art LLM-based autonomous\nagents, including several multimodal models. Through extensive quantitative and\nqualitative analysis, we identify several limitations of text-only LLM agents,\nand reveal gaps in the capabilities of state-of-the-art multimodal language\nagents. VisualWebArena provides a framework for evaluating multimodal\nautonomous language agents, and offers insights towards building stronger\nautonomous agents for the web. Our code, baseline models, and data is publicly\navailable at https://jykoh.com/vwa.", "published": "2024-01-24 18:35:21", "link": "http://arxiv.org/abs/2401.13649v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Investigating the Efficacy of Large Language Models for Code Clone\n  Detection", "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in various\nnatural language processing and software engineering tasks, such as code\ngeneration. The LLMs are mainly utilized in the prompt-based zero/few-shot\nparadigm to guide the model in accomplishing the task. GPT-based models are one\nof the popular ones studied for tasks such as code comment generation or test\ngeneration. These tasks are `generative' tasks. However, there is limited\nresearch on the usage of LLMs for `non-generative' tasks such as classification\nusing the prompt-based paradigm. In this preliminary exploratory study, we\ninvestigated the applicability of LLMs for Code Clone Detection (CCD), a\nnon-generative task. By building a mono-lingual and cross-lingual CCD dataset\nderived from CodeNet, we first investigated two different prompts using ChatGPT\nto detect Type-4 code clones in Java-Java and Java-Ruby pairs in a zero-shot\nsetting. We then conducted an analysis to understand the strengths and\nweaknesses of ChatGPT in CCD. ChatGPT surpasses the baselines in cross-language\nCCD attaining an F1-score of 0.877 and achieves comparable performance to fully\nfine-tuned models for mono-lingual CCD, with an F1-score of 0.878. Also, the\nprompt and the difficulty level of the problems has an impact on the\nperformance of ChatGPT. Finally we provide insights and future directions based\non our initial analysis", "published": "2024-01-24 20:43:36", "link": "http://arxiv.org/abs/2401.13802v3", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.SE"}
{"title": "What Large Language Models Know and What People Think They Know", "abstract": "As artificial intelligence (AI) systems, particularly large language models\n(LLMs), become increasingly integrated into decision-making processes, the\nability to trust their outputs is crucial. To earn human trust, LLMs must be\nwell calibrated such that they can accurately assess and communicate the\nlikelihood of their predictions being correct. Whereas recent work has focused\non LLMs' internal confidence, less is understood about how effectively they\nconvey uncertainty to users. Here we explore the calibration gap, which refers\nto the difference between human confidence in LLM-generated answers and the\nmodels' actual confidence, and the discrimination gap, which reflects how well\nhumans and models can distinguish between correct and incorrect answers. Our\nexperiments with multiple-choice and short-answer questions reveal that users\ntend to overestimate the accuracy of LLM responses when provided with default\nexplanations. Moreover, longer explanations increased user confidence, even\nwhen the extra length did not improve answer accuracy. By adjusting LLM\nexplanations to better reflect the models' internal confidence, both the\ncalibration gap and the discrimination gap narrowed, significantly improving\nuser perception of LLM accuracy. These findings underscore the importance of\naccurate uncertainty communication and highlight the effect of explanation\nlength in influencing user trust in AI-assisted decision-making environments.\nCode and Data can be found at https://osf.io/y7pr6/ . Journal publication can\nbe found on Nature Machine Intelligence at\nhttps://www.nature.com/articles/s42256-024-00976-7 .", "published": "2024-01-24 22:21:04", "link": "http://arxiv.org/abs/2401.13835v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Language-Guided World Models: A Model-Based Approach to AI Control", "abstract": "This paper introduces the concept of Language-Guided World Models (LWMs) --\nprobabilistic models that can simulate environments by reading texts. Agents\nequipped with these models provide humans with more extensive and efficient\ncontrol, allowing them to simultaneously alter agent behaviors in multiple\ntasks via natural verbal communication. In this work, we take initial steps in\ndeveloping robust LWMs that can generalize to compositionally novel language\ndescriptions. We design a challenging world modeling benchmark based on the\ngame of MESSENGER (Hanjie et al., 2021), featuring evaluation settings that\nrequire varying degrees of compositional generalization. Our experiments reveal\nthe lack of generalizability of the state-of-the-art Transformer model, as it\noffers marginal improvements in simulation quality over a no-text baseline. We\ndevise a more robust model by fusing the Transformer with the EMMA attention\nmechanism (Hanjie et al., 2021). Our model substantially outperforms the\nTransformer and approaches the performance of a model with an oracle semantic\nparsing and grounding capability. To demonstrate the practicality of this model\nin improving AI safety and transparency, we simulate a scenario in which the\nmodel enables an agent to present plans to a human before execution, and to\nrevise plans based on their language feedback.", "published": "2024-01-24 03:11:36", "link": "http://arxiv.org/abs/2402.01695v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Steering Language Models with Game-Theoretic Solvers", "abstract": "Mathematical models of interactions among rational agents have long been\nstudied in game theory. However these interactions are often over a small set\nof discrete game actions which is very different from how humans communicate in\nnatural language. To bridge this gap, we introduce a framework that allows\nequilibrium solvers to work over the space of natural language dialogue\ngenerated by large language models (LLMs). Specifically, by modelling the\nplayers, strategies and payoffs in a \"game\" of dialogue, we create a binding\nfrom natural language interactions to the conventional symbolic logic of game\ntheory. Given this binding, we can ask existing game-theoretic algorithms to\nprovide us with strategic solutions (e.g., what string an LLM should generate\nto maximize payoff in the face of strategic partners or opponents), giving us\npredictors of stable, rational conversational strategies. We focus on three\ndomains that require different negotiation strategies: scheduling meetings,\ntrading fruit and debate, and evaluate an LLM's generated language when guided\nby solvers. We see that LLMs that follow game-theory solvers result in dialogue\ngenerations that are less exploitable than the control (no guidance from\nsolvers), and the language generated results in higher rewards, in all\nnegotiation domains. We discuss future implications of this work, and how\ngame-theoretic solvers that can leverage the expressivity of natural language\ncan open up a new avenue of guiding language research.", "published": "2024-01-24 22:22:00", "link": "http://arxiv.org/abs/2402.01704v3", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL"}
{"title": "Position: AI/ML Influencers Have a Place in the Academic Process", "abstract": "As the number of accepted papers at AI and ML conferences reaches into the\nthousands, it has become unclear how researchers access and read research\npublications. In this paper, we investigate the role of social media\ninfluencers in enhancing the visibility of machine learning research,\nparticularly the citation counts of papers they share. We have compiled a\ncomprehensive dataset of over 8,000 papers, spanning tweets from December 2018\nto October 2023, alongside controls precisely matched by 9 key covariates. Our\nstatistical and causal inference analysis reveals a significant increase in\ncitations for papers endorsed by these influencers, with median citation counts\n2-3 times higher than those of the control group. Additionally, the study\ndelves into the geographic, gender, and institutional diversity of highlighted\nauthors. Given these findings, we advocate for a responsible approach to\ncuration, encouraging influencers to uphold the journalistic standard that\nincludes showcasing diverse research topics, authors, and institutions.", "published": "2024-01-24 20:05:49", "link": "http://arxiv.org/abs/2401.13782v3", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SI"], "primary_category": "cs.DL"}
{"title": "SCNet: Sparse Compression Network for Music Source Separation", "abstract": "Deep learning-based methods have made significant achievements in music\nsource separation. However, obtaining good results while maintaining a low\nmodel complexity remains challenging in super wide-band music source\nseparation. Previous works either overlook the differences in subbands or\ninadequately address the problem of information loss when generating subband\nfeatures. In this paper, we propose SCNet, a novel frequency-domain network to\nexplicitly split the spectrogram of the mixture into several subbands and\nintroduce a sparsity-based encoder to model different frequency bands. We use a\nhigher compression ratio on subbands with less information to improve the\ninformation density and focus on modeling subbands with more information. In\nthis way, the separation performance can be significantly improved using lower\ncomputational consumption. Experiment results show that the proposed model\nachieves a signal to distortion ratio (SDR) of 9.0 dB on the MUSDB18-HQ dataset\nwithout using extra data, which outperforms state-of-the-art methods.\nSpecifically, SCNet's CPU inference time is only 48% of HT Demucs, one of the\nprevious state-of-the-art models.", "published": "2024-01-24 07:35:37", "link": "http://arxiv.org/abs/2401.13276v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "MOS-FAD: Improving Fake Audio Detection Via Automatic Mean Opinion Score\n  Prediction", "abstract": "Automatic Mean Opinion Score (MOS) prediction is employed to evaluate the\nquality of synthetic speech. This study extends the application of predicted\nMOS to the task of Fake Audio Detection (FAD), as we expect that MOS can be\nused to assess how close synthesized speech is to the natural human voice. We\npropose MOS-FAD, where MOS can be leveraged at two key points in FAD: training\ndata selection and model fusion. In training data selection, we demonstrate\nthat MOS enables effective filtering of samples from unbalanced datasets. In\nthe model fusion, our results demonstrate that incorporating MOS as a gating\nmechanism in FAD model fusion enhances overall performance.", "published": "2024-01-24 06:19:44", "link": "http://arxiv.org/abs/2401.13249v2", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Perceptually-motivated Spatial Audio Codec for Higher-Order Ambisonics\n  Compression", "abstract": "Scene-based spatial audio formats, such as Ambisonics, are playback system\nagnostic and may therefore be favoured for delivering immersive audio\nexperiences to a wide range of (potentially unknown) devices. The number of\nchannels required to deliver high spatial resolution Ambisonic audio, however,\ncan be prohibitive for low-bandwidth applications. Therefore, this paper\nproposes a compression codec, which is based upon the parametric higher-order\nDirectional Audio Coding (HO-DirAC) model. The encoder downmixes the\nhigher-order Ambisonic (HOA) input audio into a reduced number of signals,\nwhich are accompanied by perceptually-motivated scene parameters. The downmixed\naudio is coded using a perceptual audio coder, whereas the parameters are\ngrouped into perceptual bands, quantized, and downsampled. On the decoder side,\nlow Ambisonic orders are fully recovered. Not fully recoverable HOA components\nare synthesized according to the parameters. The results of a listening test\nindicate that the proposed parametric spatial audio codec can improve the\nadopted perceptual audio coder, especially at low to medium-high bitrates, when\napplied to fifth-order HOA signals.", "published": "2024-01-24 11:55:53", "link": "http://arxiv.org/abs/2401.13401v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A Phoneme-Scale Assessment of Multichannel Speech Enhancement Algorithms", "abstract": "In the intricate acoustic landscapes where speech intelligibility is\nchallenged by noise and reverberation, multichannel speech enhancement emerges\nas a promising solution for individuals with hearing loss. Such algorithms are\ncommonly evaluated at the utterance level. However, this approach overlooks the\ngranular acoustic nuances revealed by phoneme-specific analysis, potentially\nobscuring key insights into their performance. This paper presents an in-depth\nphoneme-scale evaluation of 3 state-of-the-art multichannel speech enhancement\nalgorithms. These algorithms -- FasNet, MVDR, and Tango -- are extensively\nevaluated across different noise conditions and spatial setups, employing\nrealistic acoustic simulations with measured room impulse responses, and\nleveraging diversity offered by multiple microphones in a binaural hearing\nsetup. The study emphasizes the fine-grained phoneme-level analysis, revealing\nthat while some phonemes like plosives are heavily impacted by environmental\nacoustics and challenging to deal with by the algorithms, others like nasals\nand sibilants see substantial improvements after enhancement. These\ninvestigations demonstrate important improvements in phoneme clarity in noisy\nconditions, with insights that could drive the development of more personalized\nand phoneme-aware hearing aid technologies.", "published": "2024-01-24 16:08:21", "link": "http://arxiv.org/abs/2401.13548v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Bayesian adaptive learning to latent variables via Variational Bayes and\n  Maximum a Posteriori", "abstract": "In this work, we aim to establish a Bayesian adaptive learning framework by\nfocusing on estimating latent variables in deep neural network (DNN) models.\nLatent variables indeed encode both transferable distributional information and\nstructural relationships. Thus the distributions of the source latent variables\n(prior) can be combined with the knowledge learned from the target data\n(likelihood) to yield the distributions of the target latent variables\n(posterior) with the goal of addressing acoustic mismatches between training\nand testing conditions. The prior knowledge transfer is accomplished through\nVariational Bayes (VB). In addition, we also investigate Maximum a Posteriori\n(MAP) based Bayesian adaptation. Experimental results on device adaptation in\nacoustic scene classification show that our proposed approaches can obtain good\nimprovements on target devices, and consistently outperforms other cut-edging\nalgorithms.", "published": "2024-01-24 19:27:27", "link": "http://arxiv.org/abs/2401.13766v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired\n  Users using Intermediate ASR Features and Human Memory Models", "abstract": "Neural networks have been successfully used for non-intrusive speech\nintelligibility prediction. Recently, the use of feature representations\nsourced from intermediate layers of pre-trained self-supervised and\nweakly-supervised models has been found to be particularly useful for this\ntask. This work combines the use of Whisper ASR decoder layer representations\nas neural network input features with an exemplar-based, psychologically\nmotivated model of human memory to predict human intelligibility ratings for\nhearing-aid users. Substantial performance improvement over an established\nintrusive HASPI baseline system is found, including on enhancement systems and\nlisteners unseen in the training data, with a root mean squared error of 25.3\ncompared with the baseline of 28.7.", "published": "2024-01-24 17:31:07", "link": "http://arxiv.org/abs/2401.13611v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scaling NVIDIA's Multi-speaker Multi-lingual TTS Systems with Zero-Shot\n  TTS to Indic Languages", "abstract": "In this paper, we describe the TTS models developed by NVIDIA for the\nMMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024\nChallenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by\ntraining additionally on 5 minutes of target speaker data. In Track 3, we\nutilize P-Flow to perform zero-shot TTS by training on the challenge dataset as\nwell as external datasets. We use HiFi-GAN vocoders for all submissions.\nRAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on\nTrack 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS)\nof 3.62.", "published": "2024-01-24 23:18:33", "link": "http://arxiv.org/abs/2401.13851v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech foundation models on intelligibility prediction for\n  hearing-impaired listeners", "abstract": "Speech foundation models (SFMs) have been benchmarked on many speech\nprocessing tasks, often achieving state-of-the-art performance with minimal\nadaptation. However, the SFM paradigm has been significantly less explored for\napplications of interest to the speech perception community. In this paper we\npresent a systematic evaluation of 10 SFMs on one such application: Speech\nintelligibility prediction. We focus on the non-intrusive setup of the Clarity\nPrediction Challenge 2 (CPC2), where the task is to predict the percentage of\nwords correctly perceived by hearing-impaired listeners from speech-in-noise\nrecordings. We propose a simple method that learns a lightweight specialized\nprediction head on top of frozen SFMs to approach the problem. Our results\nreveal statistically significant differences in performance across SFMs. Our\nmethod resulted in the winning submission in the CPC2, demonstrating its\npromise for speech perception applications.", "published": "2024-01-24 18:26:52", "link": "http://arxiv.org/abs/2401.14289v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver\n  Interaction in Los Angeles", "abstract": "Interactions between the government officials and civilians affect public\nwellbeing and the state legitimacy that is necessary for the functioning of\ndemocratic society. Police officers, the most visible and contacted agents of\nthe state, interact with the public more than 20 million times a year during\ntraffic stops. Today, these interactions are regularly recorded by body-worn\ncameras (BWCs), which are lauded as a means to enhance police accountability\nand improve police-public interactions. However, the timely analysis of these\nrecordings is hampered by a lack of reliable automated tools that can enable\nthe analysis of these complex and contested police-public interactions. This\narticle proposes an approach to developing new multi-perspective, multimodal\nmachine learning (ML) tools to analyze the audio, video, and transcript\ninformation from this BWC footage. Our approach begins by identifying the\naspects of communication most salient to different stakeholders, including both\ncommunity members and police officers. We move away from modeling approaches\nbuilt around the existence of a single ground truth and instead utilize new\nadvances in soft labeling to incorporate variation in how different observers\nperceive the same interactions. We argue that this inclusive approach to the\nconceptualization and design of new ML tools is broadly applicable to the study\nof communication and development of analytic tools across domains of human\ninteraction, including education, medicine, and the workplace.", "published": "2024-01-24 19:56:20", "link": "http://arxiv.org/abs/2402.01703v3", "categories": ["cs.CY", "cs.AI", "cs.LG", "eess.AS", "I.2.0; I.2.7"], "primary_category": "cs.CY"}
{"title": "Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific\n  Input Representation and Diffusion Outpainting", "abstract": "Synthesizing performing guitar sound is a highly challenging task due to the\npolyphony and high variability in expression. Recently, deep generative models\nhave shown promising results in synthesizing expressive polyphonic instrument\nsounds from music scores, often using a generic MIDI input. In this work, we\npropose an expressive acoustic guitar sound synthesis model with a customized\ninput representation to the instrument, which we call guitarroll. We implement\nthe proposed approach using diffusion-based outpainting which can generate\naudio with long-term consistency. To overcome the lack of MIDI/audio-paired\ndatasets, we used not only an existing guitar dataset but also collected data\nfrom a high quality sample-based guitar synthesizer. Through quantitative and\nqualitative evaluations, we show that our proposed model has higher audio\nquality than the baseline model and generates more realistic timbre sounds than\nthe previous leading work.", "published": "2024-01-24 14:44:01", "link": "http://arxiv.org/abs/2401.13498v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
