{"title": "Knowledge Integration for Disease Characterization: A Breast Cancer Example", "abstract": "With the rapid advancements in cancer research, the information that is useful for characterizing disease, staging tumors, and creating treatment and survivorship plans has been changing at a pace that creates challenges when physicians try to remain current. One example involves increasing usage of biomarkers when characterizing the pathologic prognostic stage of a breast tumor. We present our semantic technology approach to support cancer characterization and demonstrate it in our end-to-end prototype system that collects the newest breast cancer staging criteria from authoritative oncology manuals to construct an ontology for breast cancer. Using a tool we developed that utilizes this ontology, physician-facing applications can be used to quickly stage a new patient to support identifying risks, treatment options, and monitoring plans based on authoritative and best practice guidelines. Physicians can also re-stage existing patients or patient populations, allowing them to find patients whose stage has changed in a given patient cohort. As new guidelines emerge, using our proposed mechanism, which is grounded by semantic technologies for ingesting new data from staging manuals, we have created an enriched cancer staging ontology that integrates relevant data from several sources with very little human intervention.", "published": "2018-07-20 18:26:29", "link": "http://arxiv.org/abs/1807.07991v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Boosting algorithms for uplift modeling", "abstract": "Uplift modeling is an area of machine learning which aims at predicting the causal effect of some action on a given individual. The action may be a medical procedure, marketing campaign, or any other circumstance controlled by the experimenter. Building an uplift model requires two training sets: the treatment group, where individuals have been subject to the action, and the control group, where no action has been performed. An uplift model allows then to assess the gain resulting from taking the action on a given individual, such as the increase in probability of patient recovery or of a product being purchased. This paper describes an adaptation of the well-known boosting techniques to the uplift modeling case. We formulate three desirable properties which an uplift boosting algorithm should have. Since all three properties cannot be satisfied simultaneously, we propose three uplift boosting algorithms, each satisfying two of them. Experiments demonstrate the usefulness of the proposed methods, which often dramatically improve performance of the base models and are thus new and powerful tools for uplift modeling.", "published": "2018-07-20 15:57:27", "link": "http://arxiv.org/abs/1807.07909v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Decentralized Task Allocation in Multi-Robot Systems via Bipartite Graph Matching Augmented with Fuzzy Clustering", "abstract": "Robotic systems, working together as a team, are becoming valuable players in different real-world applications, from disaster response to warehouse fulfillment services. Centralized solutions for coordinating multi-robot teams often suffer from poor scalability and vulnerability to communication disruptions. This paper develops a decentralized multi-agent task allocation (Dec-MATA) algorithm for multi-robot applications. The task planning problem is posed as a maximum-weighted matching of a bipartite graph, the solution of which using the blossom algorithm allows each robot to autonomously identify the optimal sequence of tasks it should undertake. The graph weights are determined based on a soft clustering process, which also plays a problem decomposition role seeking to reduce the complexity of the individual-agents' task assignment problems. To evaluate the new Dec-MATA algorithm, a series of case studies (of varying complexity) are performed, with tasks being distributed randomly over an observable 2D environment. A centralized approach, based on a state-of-the-art MILP formulation of the multi-Traveling Salesman problem is used for comparative analysis. While getting within 7-28% of the optimal cost obtained by the centralized algorithm, the Dec-MATA algorithm is found to be 1-3 orders of magnitude faster and minimally sensitive to task-to-robot ratios, unlike the centralized algorithm.", "published": "2018-07-20 17:59:58", "link": "http://arxiv.org/abs/1807.07957v1", "categories": ["cs.MA", "cs.AI", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Atomic Swaptions: Cryptocurrency Derivatives", "abstract": "The atomic swap protocol allows for the exchange of cryptocurrencies on different blockchains without the need to trust a third-party. However, market participants who desire to hold derivative assets such as options or futures would also benefit from trustless exchange. In this paper I propose the atomic swaption, which extends the atomic swap to allow for such exchanges. Crucially, atomic swaptions do not require the use of oracles. I also introduce the margin contract, which provides the ability to create leveraged and short positions. Lastly, I discuss how atomic swaptions may be routed on the Lightning Network.", "published": "2018-07-20 08:07:06", "link": "http://arxiv.org/abs/1807.08644v2", "categories": ["cs.CR", "q-fin.TR"], "primary_category": "cs.CR"}
{"title": "Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors", "abstract": "We study the problem of generating adversarial examples in a black-box setting in which only loss-oracle access to a model is available. We introduce a framework that conceptually unifies much of the existing work on black-box attacks, and we demonstrate that the current state-of-the-art methods are optimal in a natural sense. Despite this optimality, we show how to improve black-box attacks by bringing a new element into the problem: gradient priors. We give a bandit optimization-based algorithm that allows us to seamlessly integrate any such priors, and we explicitly identify and incorporate two examples. The resulting methods use two to four times fewer queries and fail two to five times less often than the current state-of-the-art.", "published": "2018-07-20 18:00:27", "link": "http://arxiv.org/abs/1807.07978v3", "categories": ["stat.ML", "cs.CR", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Escaping the Curse of Dimensionality in Similarity Learning: Efficient Frank-Wolfe Algorithm and Generalization Bounds", "abstract": "Similarity and metric learning provides a principled approach to construct a task-specific similarity from weakly supervised data. However, these methods are subject to the curse of dimensionality: as the number of features grows large, poor generalization is to be expected and training becomes intractable due to high computational and memory costs. In this paper, we propose a similarity learning method that can efficiently deal with high-dimensional sparse data. This is achieved through a parameterization of similarity functions by convex combinations of sparse rank-one matrices, together with the use of a greedy approximate Frank-Wolfe algorithm which provides an efficient way to control the number of active features. We show that the convergence rate of the algorithm, as well as its time and memory complexity, are independent of the data dimension. We further provide a theoretical justification of our modeling choices through an analysis of the generalization error, which depends logarithmically on the sparsity of the solution rather than on the number of features. Our experiments on datasets with up to one million features demonstrate the ability of our approach to generalize well despite the high dimensionality as well as its superiority compared to several competing methods.", "published": "2018-07-20 11:18:00", "link": "http://arxiv.org/abs/1807.07789v4", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Principal Flow Patterns across renewable electricity networks", "abstract": "Using Principal Component Analysis (PCA), the nodal injection and line flow patterns in a network model of a future highly renewable European electricity system are investigated. It is shown that the number of principal components needed to describe 95$\\%$ of the nodal power injection variance first increases with the spatial resolution of the system representation. The number of relevant components then saturates at around 76 components for network sizes larger than 512 nodes, which can be related to the correlation length of wind patterns over Europe. Remarkably, the application of PCA to the transmission line power flow statistics shows that irrespective of the spatial scale of the system representation a very low number of only 8 principal flow patterns is sufficient to capture 95$\\%$ of the corresponding spatio-temporal variance. This result can be theoretically explained by a particular alignment of some principal injection patterns with topological patterns inherent to the network structure of the European transmission system.", "published": "2018-07-20 10:15:44", "link": "http://arxiv.org/abs/1807.07771v2", "categories": ["eess.SP", "physics.soc-ph"], "primary_category": "eess.SP"}
