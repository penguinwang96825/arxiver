{"title": "Abstractive and Extractive Text Summarization using Document Context\n  Vector and Recurrent Neural Networks", "abstract": "Sequence to sequence (Seq2Seq) learning has recently been used for\nabstractive and extractive summarization. In current study, Seq2Seq models have\nbeen used for eBay product description summarization. We propose a novel\nDocument-Context based Seq2Seq models using RNNs for abstractive and extractive\nsummarizations. Intuitively, this is similar to humans reading the title,\nabstract or any other contextual information before reading the document. This\ngives humans a high-level idea of what the document is about. We use this idea\nand propose that Seq2Seq models should be started with contextual information\nat the first time-step of the input to obtain better summaries. In this manner,\nthe output summaries are more document centric, than being generic, overcoming\none of the major hurdles of using generative models. We generate\ndocument-context from user-behavior and seller provided information. We train\nand evaluate our models on human-extracted-golden-summaries. The\ndocument-contextual Seq2Seq models outperform standard Seq2Seq models.\nMoreover, generating human extracted summaries is prohibitively expensive to\nscale, we therefore propose a semi-supervised technique for extracting\napproximate summaries and using it for training Seq2Seq models at scale.\nSemi-supervised models are evaluated against human extracted summaries and are\nfound to be of similar efficacy. We provide side by side comparison for\nabstractive and extractive summarizers (contextual and non-contextual) on same\nevaluation dataset. Overall, we provide methodologies to use and evaluate the\nproposed techniques for large document summarization. Furthermore, we found\nthese techniques to be highly effective, which is not the case with existing\ntechniques.", "published": "2018-07-20 18:56:32", "link": "http://arxiv.org/abs/1807.08000v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Functorial Language-Games", "abstract": "In categorical compositional semantics of natural language one studies\nfunctors from a category of grammatical derivations (such as a Lambek pregroup)\nto a semantic category (such as real vector spaces). We compositionally build\ngame-theoretic semantics of sentences by taking the semantic category to be the\ncategory whose morphisms are open games. This requires some modifications to\nthe grammar category to compensate for the failure of open games to form a\ncompact closed category. We illustrate the theory using simple examples of\nWittgenstein's language-games.", "published": "2018-07-20 13:18:15", "link": "http://arxiv.org/abs/1807.07828v2", "categories": ["cs.LO", "cs.CL"], "primary_category": "cs.LO"}
{"title": "Twitter Sentiment Analysis via Bi-sense Emoji Embedding and\n  Attention-based LSTM", "abstract": "Sentiment analysis on large-scale social media data is important to bridge\nthe gaps between social media contents and real world activities including\npolitical election prediction, individual and public emotional status\nmonitoring and analysis, and so on. Although textual sentiment analysis has\nbeen well studied based on platforms such as Twitter and Instagram, analysis of\nthe role of extensive emoji uses in sentiment analysis remains light. In this\npaper, we propose a novel scheme for Twitter sentiment analysis with extra\nattention on emojis. We first learn bi-sense emoji embeddings under positive\nand negative sentimental tweets individually, and then train a sentiment\nclassifier by attending on these bi-sense emoji embeddings with an\nattention-based long short-term memory network (LSTM). Our experiments show\nthat the bi-sense embedding is effective for extracting sentiment-aware\nembeddings of emojis and outperforms the state-of-the-art models. We also\nvisualize the attentions to show that the bi-sense emoji embedding provides\nbetter guidance on the attention mechanism to obtain a more robust\nunderstanding of the semantics and sentiments.", "published": "2018-07-20 04:09:08", "link": "http://arxiv.org/abs/1807.07961v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Question-Aware Sentence Gating Networks for Question and Answering", "abstract": "Machine comprehension question answering, which finds an answer to the\nquestion given a passage, involves high-level reasoning processes of\nunderstanding and tracking the relevant contents across various semantic units\nsuch as words, phrases, and sentences in a document. This paper proposes the\nnovel question-aware sentence gating networks that directly incorporate the\nsentence-level information into word-level encoding processes. To this end, our\nmodel first learns question-aware sentence representations and then dynamically\ncombines them with word-level representations, resulting in semantically\nmeaningful word representations for QA tasks. Experimental results demonstrate\nthat our approach consistently improves the accuracy over existing baseline\napproaches on various QA datasets and bears the wide applicability to other\nneural network-based QA models.", "published": "2018-07-20 07:35:43", "link": "http://arxiv.org/abs/1807.07964v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Learning Representations for Soft Skill Matching", "abstract": "Employers actively look for talents having not only specific hard skills but\nalso various soft skills. To analyze the soft skill demands on the job market,\nit is important to be able to detect soft skill phrases from job advertisements\nautomatically. However, a naive matching of soft skill phrases can lead to\nfalse positive matches when a soft skill phrase, such as friendly, is used to\ndescribe a company, a team, or another entity, rather than a desired candidate.\n  In this paper, we propose a phrase-matching-based approach which\ndifferentiates between soft skill phrases referring to a candidate vs.\nsomething else. The disambiguation is formulated as a binary text\nclassification problem where the prediction is made for the potential soft\nskill based on the context where it occurs. To inform the model about the soft\nskill for which the prediction is made, we develop several approaches,\nincluding soft skill masking and soft skill tagging.\n  We compare several neural network based approaches, including CNN, LSTM and\nHierarchical Attention Model. The proposed tagging-based input representation\nusing LSTM achieved the highest recall of 83.92% on the job dataset when fixing\na precision to 95%.", "published": "2018-07-20 08:40:10", "link": "http://arxiv.org/abs/1807.07741v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Twitter Sentiment Analysis System", "abstract": "Social media is increasingly used by humans to express their feelings and\nopinions in the form of short text messages. Detecting sentiments in the text\nhas a wide range of applications including identifying anxiety or depression of\nindividuals and measuring well-being or mood of a community. Sentiments can be\nexpressed in many ways that can be seen such as facial expression and gestures,\nspeech and by written text. Sentiment Analysis in text documents is essentially\na content-based classification problem involving concepts from the domains of\nNatural Language Processing as well as Machine Learning. In this paper,\nsentiment recognition based on textual data and the techniques used in\nsentiment analysis are discussed.", "published": "2018-07-20 09:19:08", "link": "http://arxiv.org/abs/1807.07752v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Generalized Vector Space Model for Ontology-Based Information\n  Retrieval", "abstract": "Named entities (NE) are objects that are referred to by names such as people,\norganizations and locations. Named entities and keywords are important to the\nmeaning of a document. We propose a generalized vector space model that\ncombines named entities and keywords. In the model, we take into account\ndifferent ontological features of named entities, namely, aliases, classes and\nidentifiers. Moreover, we use entity classes to represent the latent\ninformation of interrogative words in Wh-queries, which are ignored in\ntraditional keyword-based searching. We have implemented and tested the\nproposed model on a TREC dataset, as presented and discussed in the paper.", "published": "2018-07-20 10:37:31", "link": "http://arxiv.org/abs/1807.07779v1", "categories": ["cs.IR", "cs.CL", "cs.DB"], "primary_category": "cs.IR"}
{"title": "An Efficient End-to-End Neural Model for Handwritten Text Recognition", "abstract": "Offline handwritten text recognition from images is an important problem for\nenterprises attempting to digitize large volumes of handmarked scanned\ndocuments/reports. Deep recurrent models such as Multi-dimensional LSTMs have\nbeen shown to yield superior performance over traditional Hidden Markov Model\nbased approaches that suffer from the Markov assumption and therefore lack the\nrepresentational power of RNNs. In this paper we introduce a novel approach\nthat combines a deep convolutional network with a recurrent Encoder-Decoder\nnetwork to map an image to a sequence of characters corresponding to the text\npresent in the image. The entire model is trained end-to-end using Focal Loss,\nan improvement over the standard Cross-Entropy loss that addresses the class\nimbalance problem, inherent to text recognition. To enhance the decoding\ncapacity of the model, Beam Search algorithm is employed which searches for the\nbest sequence out of a set of hypotheses based on a joint distribution of\nindividual characters. Our model takes as input a downsampled version of the\noriginal image thereby making it both computationally and memory efficient. The\nexperimental results were benchmarked against two publicly available datasets,\nIAM and RIMES. We surpass the state-of-the-art word level accuracy on the\nevaluation set of both datasets by 3.5% & 1.1%, respectively.", "published": "2018-07-20 09:55:09", "link": "http://arxiv.org/abs/1807.07965v2", "categories": ["cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Visitors to urban greenspace have higher sentiment and lower negativity\n  on Twitter", "abstract": "With more people living in cities, we are witnessing a decline in exposure to\nnature. A growing body of research has demonstrated an association between\nnature contact and improved mood. Here, we used Twitter and the Hedonometer, a\nworld analysis tool, to investigate how sentiment, or the estimated happiness\nof the words people write, varied before, during, and after visits to San\nFrancisco's urban park system. We found that sentiment was substantially higher\nduring park visits and remained elevated for several hours following the visit.\nLeveraging differences in vegetative cover across park types, we explored how\ndifferent types of outdoor public spaces may contribute to subjective\nwell-being. Tweets during visits to Regional Parks, which are greener and have\ngreater vegetative cover, exhibited larger increases in sentiment than tweets\nduring visits to Civic Plazas and Squares. Finally, we analyzed word\nfrequencies to explore several mechanisms theorized to link nature exposure\nwith mental and cognitive benefits. Negation words such as 'no', 'not', and\n'don't' decreased in frequency during visits to urban parks. These results can\nbe used by urban planners and public health officials to better target nature\ncontact recommendations for growing urban populations.", "published": "2018-07-20 18:09:44", "link": "http://arxiv.org/abs/1807.07982v2", "categories": ["cs.SI", "cs.CL", "cs.CY"], "primary_category": "cs.SI"}
{"title": "A Fully Convolutional Neural Network Approach to End-to-End Speech\n  Enhancement", "abstract": "This paper will describe a novel approach to the cocktail party problem that\nrelies on a fully convolutional neural network (FCN) architecture. The FCN\ntakes noisy audio data as input and performs nonlinear, filtering operations to\nproduce clean audio data of the target speech at the output. Our method learns\na model for one specific speaker, and is then able to extract that speakers\nvoice from babble background noise. Results from experimentation indicate the\nability to generalize to new speakers and robustness to new noise environments\nof varying signal-to-noise ratios. A potential application of this method would\nbe for use in hearing aids. A pre-trained model could be quickly fine tuned for\nan individuals family members and close friends, and deployed onto a hearing\naid to assist listeners in noisy environments.", "published": "2018-07-20 01:20:06", "link": "http://arxiv.org/abs/1807.07959v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
