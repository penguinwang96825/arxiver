{"title": "Hierarchical Latent Word Clustering", "abstract": "This paper presents a new Bayesian non-parametric model by extending the\nusage of Hierarchical Dirichlet Allocation to extract tree structured word\nclusters from text data. The inference algorithm of the model collects words in\na cluster if they share similar distribution over documents. In our\nexperiments, we observed meaningful hierarchical structures on NIPS corpus and\nradiology reports collected from public repositories.", "published": "2016-01-20 23:31:58", "link": "http://arxiv.org/abs/1601.05472v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improved Spoken Document Summarization with Coverage Modeling Techniques", "abstract": "Extractive summarization aims at selecting a set of indicative sentences from\na source document as a summary that can express the major theme of the\ndocument. A general consensus on extractive summarization is that both\nrelevance and coverage are critical issues to address. The existing methods\ndesigned to model coverage can be characterized by either reducing redundancy\nor increasing diversity in the summary. Maximal margin relevance (MMR) is a\nwidely-cited method since it takes both relevance and redundancy into account\nwhen generating a summary for a given document. In addition to MMR, there is\nonly a dearth of research concentrating on reducing redundancy or increasing\ndiversity for the spoken document summarization task, as far as we are aware.\nMotivated by these observations, two major contributions are presented in this\npaper. First, in contrast to MMR, which considers coverage by reducing\nredundancy, we propose two novel coverage-based methods, which directly\nincrease diversity. With the proposed methods, a set of representative\nsentences, which not only are relevant to the given document but also cover\nmost of the important sub-themes of the document, can be selected\nautomatically. Second, we make a step forward to plug in several\ndocument/sentence representation methods into the proposed framework to further\nenhance the summarization performance. A series of empirical evaluations\ndemonstrate the effectiveness of our proposed methods.", "published": "2016-01-20 08:26:07", "link": "http://arxiv.org/abs/1601.05194v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Semantic Word Clusters Using Signed Normalized Graph Cuts", "abstract": "Vector space representations of words capture many aspects of word\nsimilarity, but such methods tend to make vector spaces in which antonyms (as\nwell as synonyms) are close to each other. We present a new signed spectral\nnormalized graph cut algorithm, signed clustering, that overlays existing\nthesauri upon distributionally derived vector representations of words, so that\nantonym relationships between word pairs are represented by negative weights.\nOur signed clustering algorithm produces clusters of words which simultaneously\ncapture distributional and synonym relations. We evaluate these clusters\nagainst the SimLex-999 dataset (Hill et al.,2014) of human judgments of word\npair similarities, and also show the benefit of using our clusters to predict\nthe sentiment of a given text.", "published": "2016-01-20 20:37:47", "link": "http://arxiv.org/abs/1601.05403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
