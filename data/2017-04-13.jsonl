{"title": "Incremental Skip-gram Model with Negative Sampling", "abstract": "This paper explores an incremental training strategy for the skip-gram model\nwith negative sampling (SGNS) from both empirical and theoretical perspectives.\nExisting methods of neural word embeddings, including SGNS, are multi-pass\nalgorithms and thus cannot perform incremental model update. To address this\nproblem, we present a simple incremental extension of SGNS and provide a\nthorough theoretical analysis to demonstrate its validity. Empirical\nexperiments demonstrated the correctness of the theoretical analysis as well as\nthe practical usefulness of the incremental algorithm.", "published": "2017-04-13 00:36:33", "link": "http://arxiv.org/abs/1704.03956v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mobile Keyboard Input Decoding with Finite-State Transducers", "abstract": "We propose a finite-state transducer (FST) representation for the models used\nto decode keyboard inputs on mobile devices. Drawing from learnings from the\nfield of speech recognition, we describe a decoding framework that can satisfy\nthe strict memory and latency constraints of keyboard input. We extend this\nframework to support functionalities typically not present in speech\nrecognition, such as literal decoding, autocorrections, word completions, and\nnext word predictions.\n  We describe the general framework of what we call for short the keyboard \"FST\ndecoder\" as well as the implementation details that are new compared to a\nspeech FST decoder. We demonstrate that the FST decoder enables new UX features\nsuch as post-corrections. Finally, we sketch how this decoder can support\nadvanced features such as personalization and contextualization.", "published": "2017-04-13 04:00:07", "link": "http://arxiv.org/abs/1704.03987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Neural Model for User Geolocation and Lexical Dialectology", "abstract": "We propose a simple yet effective text- based user geolocation model based on\na neural network with one hidden layer, which achieves state of the art\nperformance over three Twitter benchmark geolocation datasets, in addition to\nproducing word and phrase embeddings in the hidden layer that we show to be\nuseful for detecting dialectal terms. As part of our analysis of dialectal\nterms, we release DAREDS, a dataset for evaluating dialect term detection\nmethods.", "published": "2017-04-13 06:35:55", "link": "http://arxiv.org/abs/1704.04008v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-lingual and cross-domain discourse segmentation of entire\n  documents", "abstract": "Discourse segmentation is a crucial step in building end-to-end discourse\nparsers. However, discourse segmenters only exist for a few languages and\ndomains. Typically they only detect intra-sentential segment boundaries,\nassuming gold standard sentence and token segmentation, and relying on\nhigh-quality syntactic parses and rich heuristics that are not generally\navailable across languages and domains. In this paper, we propose statistical\ndiscourse segmenters for five languages and three domains that do not rely on\ngold pre-annotations. We also consider the problem of learning discourse\nsegmenters when no labeled data is available for a language. Our fully\nsupervised system obtains 89.5% F1 for English newswire, with slight drops in\nperformance on other domains, and we report supervised and unsupervised\n(cross-lingual) results for five languages in total.", "published": "2017-04-13 12:54:30", "link": "http://arxiv.org/abs/1704.04100v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Joint Multilingual Sentence Representations with Neural Machine\n  Translation", "abstract": "In this paper, we use the framework of neural machine translation to learn\njoint sentence representations across six very different languages. Our aim is\nthat a representation which is independent of the language, is likely to\ncapture the underlying semantics. We define a new cross-lingual similarity\nmeasure, compare up to 1.4M sentence representations and study the\ncharacteristics of close sentences. We provide experimental evidence that\nsentences that are close in embedding space are indeed semantically highly\nrelated, but often have quite different structure and syntax. These relations\nalso hold when comparing sentences in different languages.", "published": "2017-04-13 14:40:40", "link": "http://arxiv.org/abs/1704.04154v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Room for improvement in automatic image description: an error analysis", "abstract": "In recent years we have seen rapid and significant progress in automatic\nimage description but what are the open problems in this area? Most work has\nbeen evaluated using text-based similarity metrics, which only indicate that\nthere have been improvements, without explaining what has improved. In this\npaper, we present a detailed error analysis of the descriptions generated by a\nstate-of-the-art attention-based model. Our analysis operates on two levels:\nfirst we check the descriptions for accuracy, and then we categorize the types\nof errors we observe in the inaccurate descriptions. We find only 20% of the\ndescriptions are free from errors, and surprisingly that 26% are unrelated to\nthe image. Finally, we manually correct the most frequently occurring error\ntypes (e.g. gender identification) to estimate the performance reward for\naddressing these errors, observing gains of 0.2--1 BLEU point per type.", "published": "2017-04-13 16:21:18", "link": "http://arxiv.org/abs/1704.04198v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identity and Granularity of Events in Text", "abstract": "In this paper we describe a method to detect event descrip- tions in\ndifferent news articles and to model the semantics of events and their\ncomponents using RDF representations. We compare these descriptions to solve a\ncross-document event coreference task. Our com- ponent approach to event\nsemantics defines identity and granularity of events at different levels. It\nperforms close to state-of-the-art approaches on the cross-document event\ncoreference task, while outperforming other works when assuming similar quality\nof event detection. We demonstrate how granularity and identity are\ninterconnected and we discuss how se- mantic anomaly could be used to define\ndifferences between coreference, subevent and topical relations.", "published": "2017-04-13 19:23:43", "link": "http://arxiv.org/abs/1704.04259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Latent Representations for Speech Generation and Transformation", "abstract": "An ability to model a generative process and learn a latent representation\nfor speech in an unsupervised fashion will be crucial to process vast\nquantities of unlabelled speech data. Recently, deep probabilistic generative\nmodels such as Variational Autoencoders (VAEs) have achieved tremendous success\nin modeling natural images. In this paper, we apply a convolutional VAE to\nmodel the generative process of natural speech. We derive latent space\narithmetic operations to disentangle learned latent representations. We\ndemonstrate the capability of our model to modify the phonetic content or the\nspeaker identity for speech segments using the derived operations, without the\nneed for parallel supervisory data.", "published": "2017-04-13 17:41:11", "link": "http://arxiv.org/abs/1704.04222v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
