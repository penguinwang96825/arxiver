{"title": "Agreement-based Joint Training for Bidirectional Attention-based Neural\n  Machine Translation", "abstract": "The attentional mechanism has proven to be effective in improving end-to-end\nneural machine translation. However, due to the intricate structural divergence\nbetween natural languages, unidirectional attention-based models might only\ncapture partial aspects of attentional regularities. We propose agreement-based\njoint training for bidirectional attention-based end-to-end neural machine\ntranslation. Instead of training source-to-target and target-to-source\ntranslation models independently,our approach encourages the two complementary\nmodels to agree on word alignment matrices on the same training data.\nExperiments on Chinese-English and English-French translation tasks show that\nagreement-based joint training significantly improves both alignment and\ntranslation quality over independent training.", "published": "2015-12-15 04:55:06", "link": "http://arxiv.org/abs/1512.04650v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strategies for Training Large Vocabulary Neural Language Models", "abstract": "Training neural network language models over large vocabularies is still\ncomputationally very costly compared to count-based models such as Kneser-Ney.\nAt the same time, neural language models are gaining popularity for many\napplications such as speech recognition and machine translation whose success\ndepends on scalability. We present a systematic comparison of strategies to\nrepresent and train large vocabularies, including softmax, hierarchical\nsoftmax, target sampling, noise contrastive estimation and self normalization.\nWe further extend self normalization to be a proper estimator of likelihood and\nintroduce an efficient variant of softmax. We evaluate each method on three\npopular benchmarks, examining performance on rare words, the speed/accuracy\ntrade-off and complementarity to Kneser-Ney.", "published": "2015-12-15 19:29:01", "link": "http://arxiv.org/abs/1512.04906v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "An Operator for Entity Extraction in MapReduce", "abstract": "Dictionary-based entity extraction involves finding mentions of dictionary\nentities in text. Text mentions are often noisy, containing spurious or missing\nwords. Efficient algorithms for detecting approximate entity mentions follow\none of two general techniques. The first approach is to build an index on the\nentities and perform index lookups of document substrings. The second approach\nrecognizes that the number of substrings generated from documents can explode\nto large numbers, to get around this, they use a filter to prune many such\nsubstrings which do not match any dictionary entity and then only verify the\nremaining substrings if they are entity mentions of dictionary entities, by\nmeans of a text join. The choice between the index-based approach and the\nfilter & verification-based approach is a case-to-case decision as the best\napproach depends on the characteristics of the input entity dictionary, for\nexample frequency of entity mentions. Choosing the right approach for the\nsetting can make a substantial difference in execution time. Making this choice\nis however non-trivial as there are parameters within each of the approaches\nthat make the space of possible approaches very large. In this paper, we\npresent a cost-based operator for making the choice among execution plans for\nentity extraction. Since we need to deal with large dictionaries and even\nlarger large datasets, our operator is developed for implementations of\nMapReduce distributed algorithms.", "published": "2015-12-15 21:23:20", "link": "http://arxiv.org/abs/1512.04973v1", "categories": ["cs.DB", "cs.CL", "68T50"], "primary_category": "cs.DB"}
{"title": "Joint Image-Text News Topic Detection and Tracking with And-Or Graph\n  Representation", "abstract": "In this paper, we aim to develop a method for automatically detecting and\ntracking topics in broadcast news. We present a hierarchical And-Or graph (AOG)\nto jointly represent the latent structure of both texts and visuals. The AOG\nembeds a context sensitive grammar that can describe the hierarchical\ncomposition of news topics by semantic elements about people involved, related\nplaces and what happened, and model contextual relationships between elements\nin the hierarchy. We detect news topics through a cluster sampling process\nwhich groups stories about closely related events. Swendsen-Wang Cuts (SWC), an\neffective cluster sampling algorithm, is adopted for traversing the solution\nspace and obtaining optimal clustering solutions by maximizing a Bayesian\nposterior probability. Topics are tracked to deal with the continuously updated\nnews streams. We generate topic trajectories to show how topics emerge, evolve\nand disappear over time. The experimental results show that our method can\nexplicitly describe the textual and visual data in news videos and produce\nmeaningful topic trajectories. Our method achieves superior performance\ncompared to state-of-the-art methods on both a public dataset Reuters-21578 and\na self-collected dataset named UCLA Broadcast News Dataset.", "published": "2015-12-15 10:01:37", "link": "http://arxiv.org/abs/1512.04701v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
{"title": "Towards Evaluation of Cultural-scale Claims in Light of Topic Model\n  Sampling Effects", "abstract": "Cultural-scale models of full text documents are prone to over-interpretation\nby researchers making unintentionally strong socio-linguistic claims (Pechenick\net al., 2015) without recognizing that even large digital libraries are merely\nsamples of all the books ever produced. In this study, we test the sensitivity\nof the topic models to the sampling process by taking random samples of books\nin the Hathi Trust Digital Library from different areas of the Library of\nCongress Classification Outline. For each classification area, we train several\ntopic models over the entire class with different random seeds, generating a\nset of spanning models. Then, we train topic models on random samples of books\nfrom the classification area, generating a set of sample models. Finally, we\nperform a topic alignment between each pair of models by computing the\nJensen-Shannon distance (JSD) between the word probability distributions for\neach topic. We take two measures on each model alignment: alignment distance\nand topic overlap. We find that sample models with a large sample size\ntypically have an alignment distance that falls in the range of the alignment\ndistance between spanning models. Unsurprisingly, as sample size increases,\nalignment distance decreases. We also find that the topic overlap increases as\nsample size increases. However, the decomposition of these measures by sample\nsize differs by number of topics and by classification area. We speculate that\nthese measures could be used to find classes which have a common \"canon\"\ndiscussed among all books in the area, as shown by high topic overlap and low\nalignment distance even in small sample sizes.", "published": "2015-12-15 23:07:58", "link": "http://arxiv.org/abs/1512.05004v3", "categories": ["cs.DL", "cs.CL", "cs.IR"], "primary_category": "cs.DL"}
