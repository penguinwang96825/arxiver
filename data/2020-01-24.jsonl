{"title": "Exploration Based Language Learning for Text-Based Games", "abstract": "This work presents an exploration and imitation-learning-based agent capable\nof state-of-the-art performance in playing text-based computer games.\nText-based computer games describe their world to the player through natural\nlanguage and expect the player to interact with the game using text. These\ngames are of interest as they can be seen as a testbed for language\nunderstanding, problem-solving, and language generation by artificial agents.\nMoreover, they provide a learning environment in which these skills can be\nacquired through interactions with an environment rather than using fixed\ncorpora. One aspect that makes these games particularly challenging for\nlearning agents is the combinatorially large action space. Existing methods for\nsolving text-based games are limited to games that are either very simple or\nhave an action space restricted to a predetermined set of admissible actions.\nIn this work, we propose to use the exploration approach of Go-Explore for\nsolving text-based games. More specifically, in an initial exploration phase,\nwe first extract trajectories with high rewards, after which we train a policy\nto solve the game by imitating these trajectories. Our experiments show that\nthis approach outperforms existing solutions in solving text-based games, and\nit is more sample efficient in terms of the number of interactions with the\nenvironment. Moreover, we show that the learned policy can generalize better\nthan existing solutions to unseen games without using any restriction on the\naction space.", "published": "2020-01-24 03:03:51", "link": "http://arxiv.org/abs/2001.08868v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Iterative Approach for Identifying Complaint Based Tweets in Social\n  Media Platforms", "abstract": "Twitter is a social media platform where users express opinions over a\nvariety of issues. Posts offering grievances or complaints can be utilized by\nprivate/ public organizations to improve their service and promptly gauge a\nlow-cost assessment. In this paper, we propose an iterative methodology which\naims to identify complaint based posts pertaining to the transport domain. We\nperform comprehensive evaluations along with releasing a novel dataset for the\nresearch purposes.", "published": "2020-01-24 22:23:22", "link": "http://arxiv.org/abs/2001.09215v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Compressing Language Models using Doped Kronecker Products", "abstract": "Kronecker Products (KP) have been used to compress IoT RNN Applications by\n15-38x compression factors, achieving better results than traditional\ncompression methods. However when KP is applied to large Natural Language\nProcessing tasks, it leads to significant accuracy loss (approx 26%). This\npaper proposes a way to recover accuracy otherwise lost when applying KP to\nlarge NLP tasks, by allowing additional degrees of freedom in the KP matrix.\nMore formally, we propose doping, a process of adding an extremely sparse\noverlay matrix on top of the pre-defined KP structure. We call this compression\nmethod doped kronecker product compression. To train these models, we present a\nnew solution to the phenomenon of co-matrix adaption (CMA), which uses a new\nregularization scheme called co matrix dropout regularization (CMR). We present\nexperimental results that demonstrate compression of a large language model\nwith LSTM layers of size 25 MB by 25x with 1.4% loss in perplexity score. At\n25x compression, an equivalent pruned network leads to 7.9% loss in perplexity\nscore, while HMD and LMF lead to 15% and 27% loss in perplexity score\nrespectively.", "published": "2020-01-24 06:07:21", "link": "http://arxiv.org/abs/2001.08896v5", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition\n  using Deep Bidirectional Transformers", "abstract": "Conversational agents such as Cortana, Alexa and Siri are continuously\nworking on increasing their capabilities by adding new domains. The support of\na new domain includes the design and development of a number of NLU components\nfor domain classification, intents classification and slots tagging (including\nnamed entity recognition). Each component only performs well when trained on a\nlarge amount of labeled data. Second, these components are deployed on\nlimited-memory devices which requires some model compression. Third, for some\ndomains such as the health domain, it is hard to find a single training data\nset that covers all the required slot types. To overcome these mentioned\nproblems, we present a multi-task transformer-based neural architecture for\nslot tagging. We consider the training of a slot tagger using multiple data\nsets covering different slot types as a multi-task learning problem. The\nexperimental results on the biomedical domain have shown that the proposed\napproach outperforms the previous state-of-the-art systems for slot tagging on\nthe different benchmark biomedical datasets in terms of (time and memory)\nefficiency and effectiveness. The output slot tagger can be used by the\nconversational agent to better identify entities in the input utterances.", "published": "2020-01-24 07:16:32", "link": "http://arxiv.org/abs/2001.08904v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector\n  Elimination", "abstract": "We develop a novel method, called PoWER-BERT, for improving the inference\ntime of the popular BERT model, while maintaining the accuracy. It works by: a)\nexploiting redundancy pertaining to word-vectors (intermediate encoder outputs)\nand eliminating the redundant vectors. b) determining which word-vectors to\neliminate by developing a strategy for measuring their significance, based on\nthe self-attention mechanism. c) learning how many word-vectors to eliminate by\naugmenting the BERT model and the loss function. Experiments on the standard\nGLUE benchmark shows that PoWER-BERT achieves up to 4.5x reduction in inference\ntime over BERT with <1% loss in accuracy. We show that PoWER-BERT offers\nsignificantly better trade-off between accuracy and inference time compared to\nprior methods. We demonstrate that our method attains up to 6.8x reduction in\ninference time with <1% loss in accuracy when applied over ALBERT, a highly\ncompressed version of BERT. The code for PoWER-BERT is publicly available at\nhttps://github.com/IBM/PoWER-BERT.", "published": "2020-01-24 11:36:12", "link": "http://arxiv.org/abs/2001.08950v5", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval", "abstract": "We introduce TV show Retrieval (TVR), a new multimodal retrieval dataset. TVR\nrequires systems to understand both videos and their associated subtitle\n(dialogue) texts, making it more realistic. The dataset contains 109K queries\ncollected on 21.8K videos from 6 TV shows of diverse genres, where each query\nis associated with a tight temporal window. The queries are also labeled with\nquery types that indicate whether each of them is more related to video or\nsubtitle or both, allowing for in-depth analysis of the dataset and the methods\nthat built on top of it. Strict qualification and post-annotation verification\ntests are applied to ensure the quality of the collected data. Further, we\npresent several baselines and a novel Cross-modal Moment Localization (XML )\nnetwork for multimodal moment retrieval tasks. The proposed XML model uses a\nlate fusion design with a novel Convolutional Start-End detector (ConvSE),\nsurpassing baselines by a large margin and with better efficiency, providing a\nstrong starting point for future work. We have also collected additional\ndescriptions for each annotated moment in TVR to form a new multimodal\ncaptioning dataset with 262K captions, named TV show Caption (TVC). Both\ndatasets are publicly available. TVR: https://tvr.cs.unc.edu, TVC:\nhttps://tvr.cs.unc.edu/tvc.html.", "published": "2020-01-24 17:09:39", "link": "http://arxiv.org/abs/2001.09099v2", "categories": ["cs.CV", "cs.CL", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Semi-supervised ASR by End-to-end Self-training", "abstract": "While deep learning based end-to-end automatic speech recognition (ASR)\nsystems have greatly simplified modeling pipelines, they suffer from the data\nsparsity issue. In this work, we propose a self-training method with an\nend-to-end system for semi-supervised ASR. Starting from a Connectionist\nTemporal Classification (CTC) system trained on the supervised data, we\niteratively generate pseudo-labels on a mini-batch of unsupervised utterances\nwith the current model, and use the pseudo-labels to augment the supervised\ndata for immediate model update. Our method retains the simplicity of\nend-to-end ASR systems, and can be seen as performing alternating optimization\nover a well-defined learning objective. We also perform empirical\ninvestigations of our method, regarding the effect of data augmentation,\ndecoding beamsize for pseudo-label generation, and freshness of pseudo-labels.\nOn a commonly used semi-supervised ASR setting with the WSJ corpus, our method\ngives 14.4% relative WER improvement over a carefully-trained base system with\ndata augmentation, reducing the performance gap between the base system and the\noracle system by 50%.", "published": "2020-01-24 18:22:57", "link": "http://arxiv.org/abs/2001.09128v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Data Techniques For Online End-to-end Speech Recognition", "abstract": "Practitioners often need to build ASR systems for new use cases in a short\namount of time, given limited in-domain data. While recently developed\nend-to-end methods largely simplify the modeling pipelines, they still suffer\nfrom the data sparsity issue. In this work, we explore a few\nsimple-to-implement techniques for building online ASR systems in an end-to-end\nfashion, with a small amount of transcribed data in the target domain. These\ntechniques include data augmentation in the target domain, domain adaptation\nusing models previously trained on a large source domain, and knowledge\ndistillation on non-transcribed target domain data, using an adapted\nbi-directional model as the teacher; they are applicable in real scenarios with\ndifferent types of resources. Our experiments demonstrate that each technique\nis independently useful in the improvement of the online ASR performance in the\ntarget domain.", "published": "2020-01-24 22:59:46", "link": "http://arxiv.org/abs/2001.09221v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Towards Graph Representation Learning in Emergent Communication", "abstract": "Recent findings in neuroscience suggest that the human brain represents\ninformation in a geometric structure (for instance, through conceptual spaces).\nIn order to communicate, we flatten the complex representation of entities and\ntheir attributes into a single word or a sentence. In this paper we use graph\nconvolutional networks to support the evolution of language and cooperation in\nmulti-agent systems. Motivated by an image-based referential game, we propose a\ngraph referential game with varying degrees of complexity, and we provide\nstrong baseline models that exhibit desirable properties in terms of language\nemergence and cooperation. We show that the emerged communication protocol is\nrobust, that the agents uncover the true factors of variation in the game, and\nthat they learn to generalize beyond the samples encountered during training.", "published": "2020-01-24 15:55:59", "link": "http://arxiv.org/abs/2001.09063v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning Multi-instrument Classification with Partial Labels", "abstract": "Multi-instrument recognition is the task of predicting the presence or\nabsence of different instruments within an audio clip. A considerable challenge\nin applying deep learning to multi-instrument recognition is the scarcity of\nlabeled data. OpenMIC is a recent dataset containing 20K polyphonic audio\nclips. The dataset is weakly labeled, in that only the presence or absence of\ninstruments is known for each clip, while the onset and offset times are\nunknown. The dataset is also partially labeled, in that only a subset of\ninstruments are labeled for each clip.\n  In this work, we investigate the use of attention-based recurrent neural\nnetworks to address the weakly-labeled problem. We also use different data\naugmentation methods to mitigate the partially-labeled problem. Our experiments\nshow that our approach achieves state-of-the-art results on the OpenMIC\nmulti-instrument recognition task.", "published": "2020-01-24 02:34:31", "link": "http://arxiv.org/abs/2001.08864v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Low-rank Gradient Approximation For Memory-Efficient On-device Training\n  of Deep Neural Network", "abstract": "Training machine learning models on mobile devices has the potential of\nimproving both privacy and accuracy of the models. However, one of the major\nobstacles to achieving this goal is the memory limitation of mobile devices.\nReducing training memory enables models with high-dimensional weight matrices,\nlike automatic speech recognition (ASR) models, to be trained on-device. In\nthis paper, we propose approximating the gradient matrices of deep neural\nnetworks using a low-rank parameterization as an avenue to save training\nmemory. The low-rank gradient approximation enables more advanced,\nmemory-intensive optimization techniques to be run on device. Our experimental\nresults show that we can reduce the training memory by about 33.0% for Adam\noptimization. It uses comparable memory to momentum optimization and achieves a\n4.5% relative lower word error rate on an ASR personalization task.", "published": "2020-01-24 05:12:18", "link": "http://arxiv.org/abs/2001.08885v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Performance of a Deep Neural Network at Detecting North Atlantic Right\n  Whale Upcalls", "abstract": "Passive acoustics provides a powerful tool for monitoring the endangered\nNorth Atlantic right whale ($Eubalaena$ $glacialis$), but robust detection\nalgorithms are needed to handle diverse and variable acoustic conditions and\ndifferences in recording techniques and equipment. Here, we investigate the\npotential of deep neural networks for addressing this need. ResNet, an\narchitecture commonly used for image recognition, is trained to recognize the\ntime-frequency representation of the characteristic North Atlantic right whale\nupcall. The network is trained on several thousand examples recorded at various\nlocations in the Gulf of St.\\ Lawrence in 2018 and 2019, using different\nequipment and deployment techniques. Used as a detection algorithm on fifty\n30-minute recordings from the years 2015-2017 containing over one thousand\nupcalls, the network achieves recalls up to 80%, while maintaining a precision\nof 90%. Importantly, the performance of the network improves as more variance\nis introduced into the training dataset, whereas the opposite trend is observed\nusing a conventional linear discriminant analysis approach. Our work\ndemonstrates that deep neural networks can be trained to identify North\nAtlantic right whale upcalls under diverse and variable conditions with a\nperformance that compares favorably to that of existing algorithms.", "published": "2020-01-24 18:22:27", "link": "http://arxiv.org/abs/2001.09127v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
