{"title": "EmotionX-HSU: Adopting Pre-trained BERT for Emotion Classification", "abstract": "This paper describes our approach to the EmotionX-2019, the shared task of\nSocialNLP 2019. To detect emotion for each utterance of two datasets from the\nTV show Friends and Facebook chat log EmotionPush, we propose two-step deep\nlearning based methodology: (i) encode each of the utterance into a sequence of\nvectors that represent its meaning; and (ii) use a simply softmax classifier to\npredict one of the emotions amongst four candidates that an utterance may\ncarry. Notice that the source of labeled utterances is not rich, we utilise a\nwell-trained model, known as BERT, to transfer part of the knowledge learned\nfrom a large amount of corpus to our model. We then focus on fine-tuning our\nmodel until it well fits to the in-domain data. The performance of the proposed\nmodel is evaluated by micro-F1 scores, i.e., 79.1% and 86.2% for the testsets\nof Friends and EmotionPush, respectively. Our model ranks 3rd among 11\nsubmissions.", "published": "2019-07-23 03:05:39", "link": "http://arxiv.org/abs/1907.09669v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Select, Track, and Generate for Data-to-Text", "abstract": "We propose a data-to-text generation model with two modules, one for tracking\nand the other for text generation. Our tracking module selects and keeps track\nof salient information and memorizes which record has been mentioned. Our\ngeneration module generates a summary conditioned on the state of tracking\nmodule. Our model is considered to simulate the human-like writing process that\ngradually selects the information by determining the intermediate variables\nwhile writing the summary. In addition, we also explore the effectiveness of\nthe writer information for generation. Experimental results show that our model\noutperforms existing models in all evaluation metrics even without writer\ninformation. Incorporating writer information further improves the performance,\ncontributing to content planning and surface realization.", "published": "2019-07-23 05:31:54", "link": "http://arxiv.org/abs/1907.09699v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Unsupervised Grammatical Error Correction using Statistical\n  Machine Translation with Synthetic Comparable Corpus", "abstract": "We introduce unsupervised techniques based on phrase-based statistical\nmachine translation for grammatical error correction (GEC) trained on a pseudo\nlearner corpus created by Google Translation. We verified our GEC system\nthrough experiments on various GEC dataset, includi ng a low resource track of\nthe shared task at Building Educational Applications 2019 (BEA 2019). As a\nresult, we achieved an F_0.5 score of 28.31 points with the test data of the\nlow resource track.", "published": "2019-07-23 07:15:23", "link": "http://arxiv.org/abs/1907.09724v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling question asking using neural program generation", "abstract": "People ask questions that are far richer, more informative, and more creative\nthan current AI systems. We propose a neuro-symbolic framework for modeling\nhuman question asking, which represents questions as formal programs and\ngenerates programs with an encoder-decoder based deep neural network. From\nextensive experiments using an information-search game, we show that our method\ncan predict which questions humans are likely to ask in unconstrained settings.\nWe also propose a novel grammar-based question generation framework trained\nwith reinforcement learning, which is able to generate creative questions\nwithout supervised human data.", "published": "2019-07-23 14:20:21", "link": "http://arxiv.org/abs/1907.09899v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CMU-01 at the SIGMORPHON 2019 Shared Task on Crosslinguality and Context\n  in Morphology", "abstract": "This paper presents the submission by the CMU-01 team to the SIGMORPHON 2019\ntask 2 of Morphological Analysis and Lemmatization in Context. This task\nrequires us to produce the lemma and morpho-syntactic description of each token\nin a sequence, for 107 treebanks. We approach this task with a hierarchical\nneural conditional random field (CRF) model which predicts each coarse-grained\nfeature (eg. POS, Case, etc.) independently. However, most treebanks are\nunder-resourced, thus making it challenging to train deep neural models for\nthem. Hence, we propose a multi-lingual transfer training regime where we\ntransfer from multiple related languages that share similar typology.", "published": "2019-07-23 21:05:37", "link": "http://arxiv.org/abs/1907.10129v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dr.Quad at MEDIQA 2019: Towards Textual Inference and Question\n  Entailment using contextualized representations", "abstract": "This paper presents the submissions by Team Dr.Quad to the ACL-BioNLP 2019\nshared task on Textual Inference and Question Entailment in the Medical Domain.\nOur system is based on the prior work Liu et al. (2019) which uses a multi-task\nobjective function for textual entailment. In this work, we explore different\nstrategies for generalizing state-of-the-art language understanding models to\nthe specialized medical domain. Our results on the shared task demonstrate that\nincorporating domain knowledge through data augmentation is a powerful strategy\nfor addressing challenges posed by specialized domains such as medicine.", "published": "2019-07-23 21:18:41", "link": "http://arxiv.org/abs/1907.10136v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Web for Machine Translation: Challenges and Directions", "abstract": "A large number of machine translation approaches have recently been developed\nto facilitate the fluid migration of content across languages. However, the\nliterature suggests that many obstacles must still be dealt with to achieve\nbetter automatic translations. One of these obstacles is lexical and syntactic\nambiguity. A promising way of overcoming this problem is using Semantic Web\ntechnologies. This article is an extended abstract of our systematic review on\nmachine translation approaches that rely on Semantic Web technologies for\nimproving the translation of texts. Overall, we present the challenges and\nopportunities in the use of Semantic Web technologies in Machine Translation.\nMoreover, our research suggests that while Semantic Web technologies can\nenhance the quality of machine translation outputs for various problems, the\ncombination of both is still in its infancy.", "published": "2019-07-23 15:49:20", "link": "http://arxiv.org/abs/1907.10676v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MacNet: Transferring Knowledge from Machine Comprehension to\n  Sequence-to-Sequence Models", "abstract": "Machine Comprehension (MC) is one of the core problems in natural language\nprocessing, requiring both understanding of the natural language and knowledge\nabout the world. Rapid progress has been made since the release of several\nbenchmark datasets, and recently the state-of-the-art models even surpass human\nperformance on the well-known SQuAD evaluation. In this paper, we transfer\nknowledge learned from machine comprehension to the sequence-to-sequence tasks\nto deepen the understanding of the text. We propose MacNet: a novel\nencoder-decoder supplementary architecture to the widely used attention-based\nsequence-to-sequence models. Experiments on neural machine translation (NMT)\nand abstractive text summarization show that our proposed framework can\nsignificantly improve the performance of the baseline models, and our method\nfor the abstractive text summarization achieves the state-of-the-art results on\nthe Gigaword dataset.", "published": "2019-07-23 04:38:09", "link": "http://arxiv.org/abs/1908.01816v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Happiness Entailment: Automating Suggestions for Well-Being", "abstract": "Understanding what makes people happy is a central topic in psychology. Prior\nwork has mostly focused on developing self-reporting assessment tools for\nindividuals and relies on experts to analyze the periodic reported assessments.\nOne of the goals of the analysis is to understand what actions are necessary to\nencourage modifications in the behaviors of the individuals to improve their\noverall well-being. In this paper, we outline a complementary approach; on the\nassumption that the user journals her happy moments as short texts, a system\ncan analyze these texts and propose sustainable suggestions for the user that\nmay lead to an overall improvement in her well-being. We prototype one\nnecessary component of such a system, the Happiness Entailment Recognition\n(HER) module, which takes as input a short text describing an event, a\ncandidate suggestion, and outputs a determination about whether the suggestion\nis more likely to be good for this user based on the event described. This\ncomponent is implemented as a neural network model with two encoders, one for\nthe user input and one for the candidate actionable suggestion, with additional\nlayers to capture psychologically significant features in the happy moment and\nsuggestion.", "published": "2019-07-23 17:46:02", "link": "http://arxiv.org/abs/1907.10036v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Structured Knowledge Discovery from Massive Text Corpus", "abstract": "Nowadays, with the booming development of the Internet, people benefit from\nits convenience due to its open and sharing nature. A large volume of natural\nlanguage texts is being generated by users in various forms, such as search\nqueries, documents, and social media posts. As the unstructured text corpus is\nusually noisy and messy, it becomes imperative to correctly identify and\naccurately annotate structured information in order to obtain meaningful\ninsights or better understand unstructured texts. On the other hand, the\nexisting structured information, which embodies our knowledge such as entity or\nconcept relations, often suffers from incompleteness or quality-related issues.\nGiven a gigantic collection of texts which offers rich semantic information, it\nis also important to harness the massiveness of the unannotated text corpus to\nexpand and refine existing structured knowledge with fewer annotation efforts.\n  In this dissertation, I will introduce principles, models, and algorithms for\neffective structured knowledge discovery from the massive text corpus. We are\ngenerally interested in obtaining insights and better understanding\nunstructured texts with the help of structured annotations or by\nstructure-aware modeling. Also, given the existing structured knowledge, we are\ninterested in expanding its scale and improving its quality harnessing the\nmassiveness of the text corpus. In particular, four problems are studied in\nthis dissertation: Structured Intent Detection for Natural Language\nUnderstanding, Structure-aware Natural Language Modeling, Generative Structured\nKnowledge Expansion, and Synonym Refinement on Structured Knowledge.", "published": "2019-07-23 20:45:41", "link": "http://arxiv.org/abs/1908.01837v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Pre-Learning Environment Representations for Data-Efficient Neural\n  Instruction Following", "abstract": "We consider the problem of learning to map from natural language instructions\nto state transitions (actions) in a data-efficient manner. Our method takes\ninspiration from the idea that it should be easier to ground language to\nconcepts that have already been formed through pre-linguistic observation. We\naugment a baseline instruction-following learner with an initial\nenvironment-learning phase that uses observations of language-free state\ntransitions to induce a suitable latent representation of actions before\nprocessing the instruction-following training data. We show that mapping to\npre-learned representations substantially improves performance over systems\nwhose representations are learned from limited instructional data alone.", "published": "2019-07-23 03:11:07", "link": "http://arxiv.org/abs/1907.09671v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discourse Marker Augmented Network with Reinforcement Learning for\n  Natural Language Inference", "abstract": "Natural Language Inference (NLI), also known as Recognizing Textual\nEntailment (RTE), is one of the most important problems in natural language\nprocessing. It requires to infer the logical relationship between two given\nsentences. While current approaches mostly focus on the interaction\narchitectures of the sentences, in this paper, we propose to transfer knowledge\nfrom some important discourse markers to augment the quality of the NLI model.\nWe observe that people usually use some discourse markers such as \"so\" or \"but\"\nto represent the logical relationship between two sentences. These words\npotentially have deep connections with the meanings of the sentences, thus can\nbe utilized to help improve the representations of them. Moreover, we use\nreinforcement learning to optimize a new objective function with a reward\ndefined by the property of the NLI datasets to make full use of the labels\ninformation. Experiments show that our method achieves the state-of-the-art\nperformance on several large-scale datasets.", "published": "2019-07-23 04:27:57", "link": "http://arxiv.org/abs/1907.09692v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "2D-CTC for Scene Text Recognition", "abstract": "Scene text recognition has been an important, active research topic in\ncomputer vision for years. Previous approaches mainly consider text as 1D\nsignals and cast scene text recognition as a sequence prediction problem, by\nfeat of CTC or attention based encoder-decoder framework, which is originally\ndesigned for speech recognition. However, different from speech voices, which\nare 1D signals, text instances are essentially distributed in 2D image spaces.\nTo adhere to and make use of the 2D nature of text for higher recognition\naccuracy, we extend the vanilla CTC model to a second dimension, thus creating\n2D-CTC. 2D-CTC can adaptively concentrate on most relevant features while\nexcluding the impact from clutters and noises in the background; It can also\nnaturally handle text instances with various forms (horizontal, oriented and\ncurved) while giving more interpretable intermediate predictions. The\nexperiments on standard benchmarks for scene text recognition, such as IIIT-5K,\nICDAR 2015, SVP-Perspective, and CUTE80, demonstrate that the proposed 2D-CTC\nmodel outperforms state-of-the-art methods on the text of both regular and\nirregular shapes. Moreover, 2D-CTC exhibits its superiority over prior art on\ntraining and testing speed. Our implementation and models of 2D-CTC will be\nmade publicly available soon later.", "published": "2019-07-23 05:55:28", "link": "http://arxiv.org/abs/1907.09705v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Position Focused Attention Network for Image-Text Matching", "abstract": "Image-text matching tasks have recently attracted a lot of attention in the\ncomputer vision field. The key point of this cross-domain problem is how to\naccurately measure the similarity between the visual and the textual contents,\nwhich demands a fine understanding of both modalities. In this paper, we\npropose a novel position focused attention network (PFAN) to investigate the\nrelation between the visual and the textual views. In this work, we integrate\nthe object position clue to enhance the visual-text joint-embedding learning.\nWe first split the images into blocks, by which we infer the relative position\nof region in the image. Then, an attention mechanism is proposed to model the\nrelations between the image region and blocks and generate the valuable\nposition feature, which will be further utilized to enhance the region\nexpression and model a more reliable relationship between the visual image and\nthe textual sentence. Experiments on the popular datasets Flickr30K and MS-COCO\nshow the effectiveness of the proposed method. Besides the public datasets, we\nalso conduct experiments on our collected practical large-scale news dataset\n(Tencent-News) to validate the practical application value of proposed method.\nAs far as we know, this is the first attempt to test the performance on the\npractical application. Our method achieves the state-of-art performance on all\nof these three datasets.", "published": "2019-07-23 08:23:42", "link": "http://arxiv.org/abs/1907.09748v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Overview and Results: CL-SciSumm Shared Task 2019", "abstract": "The CL-SciSumm Shared Task is the first medium-scale shared task on\nscientific document summarization in the computational linguistics~(CL) domain.\nIn 2019, it comprised three tasks: (1A) identifying relationships between\nciting documents and the referred document, (1B) classifying the discourse\nfacets, and (2) generating the abstractive summary. The dataset comprised 40\nannotated sets of citing and reference papers of the CL-SciSumm 2018 corpus and\n1000 more from the SciSummNet dataset. All papers are from the open access\nresearch papers in the CL domain. This overview describes the participation and\nthe official results of the CL-SciSumm 2019 Shared Task, organized as a part of\nthe 42nd Annual Conference of the Special Interest Group in Information\nRetrieval (SIGIR), held in Paris, France in July 2019. We compare the\nparticipating systems in terms of two evaluation metrics and discuss the use of\nROUGE as an evaluation metric. The annotated dataset used for this shared task\nand the scripts used for evaluation can be accessed and used by the community\nat: https://github.com/WING-NUS/scisumm-corpus.", "published": "2019-07-23 13:06:01", "link": "http://arxiv.org/abs/1907.09854v1", "categories": ["cs.CL", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Structured Fusion Networks for Dialog", "abstract": "Neural dialog models have exhibited strong performance, however their\nend-to-end nature lacks a representation of the explicit structure of dialog.\nThis results in a loss of generalizability, controllability and a data-hungry\nnature. Conversely, more traditional dialog systems do have strong models of\nexplicit structure. This paper introduces several approaches for explicitly\nincorporating structure into neural models of dialog. Structured Fusion\nNetworks first learn neural dialog modules corresponding to the structured\ncomponents of traditional dialog systems and then incorporate these modules in\na higher-level generative model. Structured Fusion Networks obtain strong\nresults on the MultiWOZ dataset, both with and without reinforcement learning.\nStructured Fusion Networks are shown to have several valuable properties,\nincluding better domain generalizability, improved performance in reduced data\nscenarios and robustness to divergence during reinforcement learning.", "published": "2019-07-23 17:20:13", "link": "http://arxiv.org/abs/1907.10016v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Optimal Transport-based Alignment of Learned Character Representations\n  for String Similarity", "abstract": "String similarity models are vital for record linkage, entity resolution, and\nsearch. In this work, we present STANCE --a learned model for computing the\nsimilarity of two strings. Our approach encodes the characters of each string,\naligns the encodings using Sinkhorn Iteration (alignment is posed as an\ninstance of optimal transport) and scores the alignment with a convolutional\nneural network. We evaluate STANCE's ability to detect whether two strings can\nrefer to the same entity--a task we term alias detection. We construct five new\nalias detection datasets (and make them publicly available). We show that\nSTANCE or one of its variants outperforms both state-of-the-art and classic,\nparameter-free similarity models on four of the five datasets. We also\ndemonstrate STANCE's ability to improve downstream tasks by applying it to an\ninstance of cross-document coreference and show that it leads to a 2.8 point\nimprovement in B^3 F1 over the previous state-of-the-art approach.", "published": "2019-07-23 22:41:22", "link": "http://arxiv.org/abs/1907.10165v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Log Complex Color for Visual Pattern Recognition of Total Sound", "abstract": "While traditional audio visualization methods depict amplitude intensities\nvs. time, such as in a time-frequency spectrogram, and while some may use\ncomplex phase information to augment the amplitude representation, such as in a\nreassigned spectrogram, the phase data are not generally represented in their\nown right. By plotting amplitude intensity as brightness/saturation and\nphase-cycles as hue-variations, our complex spectrogram method displays both\namplitude and phase information simultaneously, making such images canonical\nvisual representations of the source wave. As a result, the original sound may\nbe precisely reconstructed (down to the original phases) from an image, simply\nby reversing our process. This allows humans to apply our highly developed\nvisual pattern recognition skills to complete audio data in new way.", "published": "2019-07-23 15:03:32", "link": "http://arxiv.org/abs/1907.09936v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Discriminative Learning for Monaural Speech Separation Using Deep\n  Embedding Features", "abstract": "Deep clustering (DC) and utterance-level permutation invariant training\n(uPIT) have been demonstrated promising for speaker-independent speech\nseparation. DC is usually formulated as two-step processes: embedding learning\nand embedding clustering, which results in complex separation pipelines and a\nhuge obstacle in directly optimizing the actual separation objectives. As for\nuPIT, it only minimizes the chosen permutation with the lowest mean square\nerror, doesn't discriminate it with other permutations. In this paper, we\npropose a discriminative learning method for speaker-independent speech\nseparation using deep embedding features. Firstly, a DC network is trained to\nextract deep embedding features, which contain each source's information and\nhave an advantage in discriminating each target speakers. Then these features\nare used as the input for uPIT to directly separate the different sources.\nFinally, uPIT and DC are jointly trained, which directly optimizes the actual\nseparation objectives. Moreover, in order to maximize the distance of each\npermutation, the discriminative learning is applied to fine tuning the whole\nmodel. Our experiments are conducted on WSJ0-2mix dataset. Experimental results\nshow that the proposed models achieve better performances than DC and uPIT for\nspeaker-independent speech separation.", "published": "2019-07-23 14:00:06", "link": "http://arxiv.org/abs/1907.09884v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "NONOTO: A Model-agnostic Web Interface for Interactive Music Composition\n  by Inpainting", "abstract": "Inpainting-based generative modeling allows for stimulating human-machine\ninteractions by letting users perform stylistically coherent local editions to\nan object using a statistical model. We present NONOTO, a new interface for\ninteractive music generation based on inpainting models. It is aimed both at\nresearchers, by offering a simple and flexible API allowing them to connect\ntheir own models with the interface, and at musicians by providing\nindustry-standard features such as audio playback, real-time MIDI output and\nstraightforward synchronization with DAWs using Ableton Link.", "published": "2019-07-23 13:47:46", "link": "http://arxiv.org/abs/1907.10380v1", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
{"title": "LSTM based Similarity Measurement with Spectral Clustering for Speaker\n  Diarization", "abstract": "More and more neural network approaches have achieved considerable\nimprovement upon submodules of speaker diarization system, including speaker\nchange detection and segment-wise speaker embedding extraction. Still, in the\nclustering stage, traditional algorithms like probabilistic linear discriminant\nanalysis (PLDA) are widely used for scoring the similarity between two speech\nsegments. In this paper, we propose a supervised method to measure the\nsimilarity matrix between all segments of an audio recording with sequential\nbidirectional long short-term memory networks (Bi-LSTM). Spectral clustering is\napplied on top of the similarity matrix to further improve the performance.\nExperimental results show that our system significantly outperforms the\nstate-of-the-art methods and achieves a diarization error rate of 6.63% on the\nNIST SRE 2000 CALLHOME database.", "published": "2019-07-23 04:17:58", "link": "http://arxiv.org/abs/1907.10393v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "EmoBed: Strengthening Monomodal Emotion Recognition via Training with\n  Crossmodal Emotion Embeddings", "abstract": "Despite remarkable advances in emotion recognition, they are severely\nrestrained from either the essentially limited property of the employed single\nmodality, or the synchronous presence of all involved multiple modalities.\nMotivated by this, we propose a novel crossmodal emotion embedding framework\ncalled EmoBed, which aims to leverage the knowledge from other auxiliary\nmodalities to improve the performance of an emotion recognition system at hand.\nThe framework generally includes two main learning components, i. e., joint\nmultimodal training and crossmodal training. Both of them tend to explore the\nunderlying semantic emotion information but with a shared recognition network\nor with a shared emotion embedding space, respectively. In doing this, the\nenhanced system trained with this approach can efficiently make use of the\ncomplementary information from other modalities. Nevertheless, the presence of\nthese auxiliary modalities is not demanded during inference. To empirically\ninvestigate the effectiveness and robustness of the proposed framework, we\nperform extensive experiments on the two benchmark databases RECOLA and\nOMG-Emotion for the tasks of dimensional emotion regression and categorical\nemotion classification, respectively. The obtained results show that the\nproposed framework significantly outperforms related baselines in monomodal\ninference, and are also competitive or superior to the recently reported\nsystems, which emphasises the importance of the proposed crossmodal learning\nfor emotion recognition.", "published": "2019-07-23 07:55:29", "link": "http://arxiv.org/abs/1907.10428v1", "categories": ["cs.LG", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
