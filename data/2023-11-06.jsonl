{"title": "Tailoring Self-Rationalizers with Multi-Reward Distillation", "abstract": "Large language models (LMs) are capable of generating free-text rationales to\naid question answering. However, prior work 1) suggests that useful\nself-rationalization is emergent only at significant scales (e.g., 175B\nparameter GPT-3); and 2) focuses largely on downstream performance, ignoring\nthe semantics of the rationales themselves, e.g., are they faithful, true, and\nhelpful for humans? In this work, we enable small-scale LMs (approx. 200x\nsmaller than GPT-3) to generate rationales that not only improve downstream\ntask performance, but are also more plausible, consistent, and diverse,\nassessed both by automatic and human evaluation. Our method, MaRio\n(Multi-rewArd RatIOnalization), is a multi-reward conditioned\nself-rationalization algorithm that optimizes multiple distinct properties like\nplausibility, diversity and consistency. Results on five difficult\nquestion-answering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC\nshow that not only does MaRio improve task accuracy, but it also improves the\nself-rationalization quality of small LMs across the aforementioned axes better\nthan a supervised fine-tuning (SFT) baseline. Extensive human evaluations\nconfirm that MaRio rationales are preferred vs. SFT rationales, as well as\nqualitative improvements in plausibility and consistency.", "published": "2023-11-06 00:20:11", "link": "http://arxiv.org/abs/2311.02805v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Machine Translation with Large Language Models: A Preliminary\n  Study with Cooperative Decoding", "abstract": "Contemporary translation engines based on the encoder-decoder framework have\nmade significant strides in development. However, the emergence of Large\nLanguage Models (LLMs) has disrupted their position by presenting the potential\nfor achieving superior translation quality. To uncover the circumstances in\nwhich LLMs excel and explore how their strengths can be harnessed to enhance\ntranslation quality, we first conduct a comprehensive analysis to assess the\nstrengths and limitations of various commercial NMT systems and MT-oriented\nLLMs. Our findings indicate that neither NMT nor MT-oriented LLMs alone can\neffectively address all the translation issues, but MT-oriented LLMs show\npromise as a complementary solution to NMT systems. Building upon these\ninsights, we propose Cooperative Decoding (CoDec), which treats NMT systems as\na pretranslation model and MT-oriented LLMs as a supplemental solution to\nhandle complex scenarios beyond the capability of NMT alone. Experimental\nresults on the WMT22 test sets and a newly collected test set WebCrawl\ndemonstrate the effectiveness and efficiency of CoDec, highlighting its\npotential as a robust solution for combining NMT systems with MT-oriented LLMs\nin the field of machine translation.", "published": "2023-11-06 03:41:57", "link": "http://arxiv.org/abs/2311.02851v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Less than One-shot: Named Entity Recognition via Extremely Weak\n  Supervision", "abstract": "We study the named entity recognition (NER) problem under the extremely weak\nsupervision (XWS) setting, where only one example entity per type is given in a\ncontext-free way. While one can see that XWS is lighter than one-shot in terms\nof the amount of supervision, we propose a novel method X-NER that can\noutperform the state-of-the-art one-shot NER methods. We first mine entity\nspans that are similar to the example entities from an unlabelled training\ncorpus. Instead of utilizing entity span representations from language models,\nwe find it more effective to compare the context distributions before and after\nthe span is replaced by the entity example. We then leverage the top-ranked\nspans as pseudo-labels to train an NER tagger. Extensive experiments and\nanalyses on 4 NER datasets show the superior end-to-end NER performance of\nX-NER, outperforming the state-of-the-art few-shot methods with 1-shot\nsupervision and ChatGPT annotations significantly. Finally, our X-NER possesses\nseveral notable properties, such as inheriting the cross-lingual abilities of\nthe underlying language models.", "published": "2023-11-06 04:20:42", "link": "http://arxiv.org/abs/2311.02861v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data", "abstract": "Text-to-SQL aims to automate the process of generating SQL queries on a\ndatabase from natural language text. In this work, we propose \"SQLPrompt\",\ntailored to improve the few-shot prompting capabilities of Text-to-SQL for\nLarge Language Models (LLMs). Our methods include innovative prompt design,\nexecution-based consistency decoding strategy which selects the SQL with the\nmost consistent execution outcome among other SQL proposals, and a method that\naims to improve performance by diversifying the SQL proposals during\nconsistency selection with different prompt designs (\"MixPrompt\") and\nfoundation models (\"MixLLMs\"). We show that \\emph{SQLPrompt} outperforms\nprevious approaches for in-context learning with few labeled data by a large\nmargin, closing the gap with finetuning state-of-the-art with thousands of\nlabeled data.", "published": "2023-11-06 05:24:06", "link": "http://arxiv.org/abs/2311.02883v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PhoGPT: Generative Pre-training for Vietnamese", "abstract": "We open-source a state-of-the-art 4B-parameter generative model series for\nVietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and\nits chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B\nparameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens,\nwith an 8192 context length, employing a vocabulary of 20480 token types. The\nchat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning\nPhoGPT-4B on a dataset of 70K instructional prompts and their responses, along\nwith an additional 290K conversations. In addition, we also demonstrate its\nsuperior performance compared to previous open-source models. Our PhoGPT models\nare available at: https://github.com/VinAIResearch/PhoGPT", "published": "2023-11-06 08:26:14", "link": "http://arxiv.org/abs/2311.02945v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapting Pre-trained Generative Models for Extractive Question Answering", "abstract": "Pre-trained Generative models such as BART, T5, etc. have gained prominence\nas a preferred method for text generation in various natural language\nprocessing tasks, including abstractive long-form question answering (QA) and\nsummarization. However, the potential of generative models in extractive QA\ntasks, where discriminative models are commonly employed, remains largely\nunexplored. Discriminative models often encounter challenges associated with\nlabel sparsity, particularly when only a small portion of the context contains\nthe answer. The challenge is more pronounced for multi-span answers. In this\nwork, we introduce a novel approach that uses the power of pre-trained\ngenerative models to address extractive QA tasks by generating indexes\ncorresponding to context tokens or sentences that form part of the answer.\nThrough comprehensive evaluations on multiple extractive QA datasets, including\nMultiSpanQA, BioASQ, MASHQA, and WikiQA, we demonstrate the superior\nperformance of our proposed approach compared to existing state-of-the-art\nmodels.", "published": "2023-11-06 09:01:02", "link": "http://arxiv.org/abs/2311.02961v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Transformer-Based Reverse Dictionary Model for Quality\n  Estimation of Definitions", "abstract": "In the last years, several variants of transformers have emerged. In this\npaper, we compare different transformer-based models for solving the reverse\ndictionary task and explore their use in the context of a serious game called\nThe Dictionary Game.", "published": "2023-11-06 09:42:44", "link": "http://arxiv.org/abs/2311.02985v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting agreement in multi-party dialogue: evaluating speaker\n  diarisation versus a procedural baseline to enhance user engagement", "abstract": "Conversational agents participating in multi-party interactions face\nsignificant challenges in dialogue state tracking, since the identity of the\nspeaker adds significant contextual meaning. It is common to utilise\ndiarisation models to identify the speaker. However, it is not clear if these\nare accurate enough to correctly identify specific conversational events such\nas agreement or disagreement during a real-time interaction. This study uses a\ncooperative quiz, where the conversational agent acts as quiz-show host, to\ndetermine whether diarisation or a frequency-and-proximity-based method is more\naccurate at determining agreement, and whether this translates to feelings of\nengagement from the players. Experimental results show that our procedural\nsystem was more engaging to players, and was more accurate at detecting\nagreement, reaching an average accuracy of 0.44 compared to 0.28 for the\ndiarised system.", "published": "2023-11-06 11:00:44", "link": "http://arxiv.org/abs/2311.03021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Agreement in Multi-party Conversational AI", "abstract": "Today, conversational systems are expected to handle conversations in\nmulti-party settings, especially within Socially Assistive Robots (SARs).\nHowever, practical usability remains difficult as there are additional\nchallenges to overcome, such as speaker recognition, addressee recognition, and\ncomplex turn-taking. In this paper, we present our work on a multi-party\nconversational system, which invites two users to play a trivia quiz game. The\nsystem detects users' agreement or disagreement on a final answer and responds\naccordingly. Our evaluation includes both performance and user assessment\nresults, with a focus on detecting user agreement. Our annotated transcripts\nand the code for the proposed system have been released open-source on GitHub.", "published": "2023-11-06 11:04:39", "link": "http://arxiv.org/abs/2311.03026v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla\n  Lemmatizer", "abstract": "Lemmatization holds significance in both natural language processing (NLP)\nand linguistics, as it effectively decreases data density and aids in\ncomprehending contextual meaning. However, due to the highly inflected nature\nand morphological richness, lemmatization in Bangla text poses a complex\nchallenge. In this study, we propose linguistic rules for lemmatization and\nutilize a dictionary along with the rules to design a lemmatizer specifically\nfor Bangla. Our system aims to lemmatize words based on their parts of speech\nclass within a given sentence. Unlike previous rule-based approaches, we\nanalyzed the suffix marker occurrence according to the morpho-syntactic values\nand then utilized sequences of suffix markers instead of entire suffixes. To\ndevelop our rules, we analyze a large corpus of Bangla text from various\ndomains, sources, and time periods to observe the word formation of inflected\nwords. The lemmatizer achieves an accuracy of 96.36% when tested against a\nmanually annotated test dataset by trained linguists and demonstrates\ncompetitive performance on three previously published Bangla lemmatization\ndatasets. We are making the code and datasets publicly available at\nhttps://github.com/eblict-gigatech/BanLemma in order to contribute to the\nfurther advancement of Bangla NLP.", "published": "2023-11-06 13:02:07", "link": "http://arxiv.org/abs/2311.03078v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Architectural Sweet Spots for Modeling Human Label Variation by the\n  Example of Argument Quality: It's Best to Relate Perspectives!", "abstract": "Many annotation tasks in natural language processing are highly subjective in\nthat there can be different valid and justified perspectives on what is a\nproper label for a given example. This also applies to the judgment of argument\nquality, where the assignment of a single ground truth is often questionable.\nAt the same time, there are generally accepted concepts behind argumentation\nthat form a common ground. To best represent the interplay of individual and\nshared perspectives, we consider a continuum of approaches ranging from models\nthat fully aggregate perspectives into a majority label to \"share\nnothing\"-architectures in which each annotator is considered in isolation from\nall other annotators. In between these extremes, inspired by models used in the\nfield of recommender systems, we investigate the extent to which architectures\nthat include layers to model the relations between different annotators are\nbeneficial for predicting single-annotator labels. By means of two tasks of\nargument quality classification (argument concreteness and validity/novelty of\nconclusions), we show that recommender architectures increase the averaged\nannotator-individual F$_1$-scores up to $43\\%$ over a majority label model. Our\nfindings indicate that approaches to subjectivity can benefit from relating\nindividual perspectives.", "published": "2023-11-06 14:47:48", "link": "http://arxiv.org/abs/2311.03153v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Model-based Counterfactual Generator for Gender Bias Mitigation", "abstract": "Counterfactual Data Augmentation (CDA) has been one of the preferred\ntechniques for mitigating gender bias in natural language models. CDA\ntechniques have mostly employed word substitution based on dictionaries.\nAlthough such dictionary-based CDA techniques have been shown to significantly\nimprove the mitigation of gender bias, in this paper, we highlight some\nlimitations of such dictionary-based counterfactual data augmentation\ntechniques, such as susceptibility to ungrammatical compositions, and lack of\ngeneralization outside the set of predefined dictionary words. Model-based\nsolutions can alleviate these problems, yet the lack of qualitative parallel\ntraining data hinders development in this direction. Therefore, we propose a\ncombination of data processing techniques and a bi-objective training regime to\ndevelop a model-based solution for generating counterfactuals to mitigate\ngender bias. We implemented our proposed solution and performed an empirical\nevaluation which shows how our model alleviates the shortcomings of\ndictionary-based solutions.", "published": "2023-11-06 15:25:30", "link": "http://arxiv.org/abs/2311.03186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Safurai-Csharp: Harnessing Synthetic Data to improve language-specific\n  Code LLM", "abstract": "This paper introduces Safurai-Csharp, an open-source model designed to\nspecialize in the generation, completion, and debugging of C# code.\nSafurai-Csharp is built upon the novel CodeLlama 34B model and leverages the\nEvolInstruct technique, creating a refined and expanded dataset for its\nfine-tuning process. The results of its performance, a notable score of 56.33%\non the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity\nto streamline developers' workflows and aid code learning. It shows promise in\nsetting new stakes in the landscape of open-source C# LLMs and hopes to inspire\nmore inclusive and wide-ranging development in the field of language-specific\nLLMs.", "published": "2023-11-06 16:31:48", "link": "http://arxiv.org/abs/2311.03243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Ziya2: Data-centric Learning is All LLMs Need", "abstract": "Various large language models (LLMs) have been proposed in recent years,\nincluding closed- and open-source ones, continually setting new records on\nmultiple benchmarks. However, the development of LLMs still faces several\nissues, such as high cost of training models from scratch, and continual\npre-training leading to catastrophic forgetting, etc. Although many such issues\nare addressed along the line of research on LLMs, an important yet practical\nlimitation is that many studies overly pursue enlarging model sizes without\ncomprehensively analyzing and optimizing the use of pre-training data in their\nlearning process, as well as appropriate organization and leveraging of such\ndata in training LLMs under cost-effective settings. In this work, we propose\nZiya2, a model with 13 billion parameters adopting LLaMA2 as the foundation\nmodel, and further pre-trained on 700 billion tokens, where we focus on\npre-training techniques and use data-centric optimization to enhance the\nlearning process of Ziya2 on different stages. We define three data attributes\nand firstly establish data-centric scaling laws to illustrate how different\ndata impacts LLMs. Experiments show that Ziya2 significantly outperforms other\nmodels in multiple benchmarks especially with promising results compared to\nrepresentative open-source ones. Ziya2 (Base) is released at\nhttps://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and\nhttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.", "published": "2023-11-06 17:49:34", "link": "http://arxiv.org/abs/2311.03301v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tackling Concept Shift in Text Classification using Entailment-style\n  Modeling", "abstract": "Pre-trained language models (PLMs) have seen tremendous success in text\nclassification (TC) problems in the context of Natural Language Processing\n(NLP). In many real-world text classification tasks, the class definitions\nbeing learned do not remain constant but rather change with time - this is\nknown as Concept Shift. Most techniques for handling concept shift rely on\nretraining the old classifiers with the newly labelled data. However, given the\namount of training data required to fine-tune large DL models for the new\nconcepts, the associated labelling costs can be prohibitively expensive and\ntime consuming. In this work, we propose a reformulation, converting vanilla\nclassification into an entailment-style problem that requires significantly\nless data to re-train the text classifier to adapt to new concepts. We\ndemonstrate the effectiveness of our proposed method on both real world &\nsynthetic datasets achieving absolute F1 gains upto 7% and 40% respectively in\nfew-shot settings. Further, upon deployment, our solution also helped save 75%\nof labeling costs overall.", "published": "2023-11-06 18:15:36", "link": "http://arxiv.org/abs/2311.03320v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spoken Dialogue System for Medical Prescription Acquisition on\n  Smartphone: Development, Corpus and Evaluation", "abstract": "Hospital information systems (HIS) have become an essential part of\nhealthcare institutions and now incorporate prescribing support software.\nPrescription support software allows for structured information capture, which\nimproves the safety, appropriateness and efficiency of prescriptions and\nreduces the number of adverse drug events (ADEs). However, such a system\nincreases the amount of time physicians spend at a computer entering\ninformation instead of providing medical care. In addition, any new visiting\nclinician must learn to manage complex interfaces since each HIS has its own\ninterfaces. In this paper, we present a natural language interface for\ne-prescribing software in the form of a spoken dialogue system accessible on a\nsmartphone. This system allows prescribers to record their prescriptions\nverbally, a form of interaction closer to their usual practice. The system\nextracts the formal representation of the prescription ready to be checked by\nthe prescribing software and uses the dialogue to request mandatory\ninformation, correct errors or warn of particular situations. Since, to the\nbest of our knowledge, there is no existing voice-based prescription dialogue\nsystem, we present the system developed in a low-resource environment, focusing\non dialogue modeling, semantic extraction and data augmentation. The system was\nevaluated in the wild with 55 participants. This evaluation showed that our\nsystem has an average prescription time of 66.15 seconds for physicians and\n35.64 seconds for other experts, and a task success rate of 76\\% for physicians\nand 72\\% for other experts. All evaluation data were recorded and annotated to\nform PxCorpus, the first spoken drug prescription corpus that has been made\nfully available to the community\n(\\url{https://doi.org/10.5281/zenodo.6524162}).", "published": "2023-11-06 20:36:55", "link": "http://arxiv.org/abs/2311.03510v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Quantifying Uncertainty in Natural Language Explanations of Large\n  Language Models", "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for\nseveral high-stakes natural language processing (NLP) applications. Recent\nprompting works claim to elicit intermediate reasoning steps and key tokens\nthat serve as proxy explanations for LLM predictions. However, there is no\ncertainty whether these explanations are reliable and reflect the LLMs\nbehavior. In this work, we make one of the first attempts at quantifying the\nuncertainty in explanations of LLMs. To this end, we propose two novel metrics\n-- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to\nquantify the uncertainty of generated explanations. While verbalized\nuncertainty involves prompting the LLM to express its confidence in its\nexplanations, probing uncertainty leverages sample and model perturbations as a\nmeans to quantify the uncertainty. Our empirical analysis of benchmark datasets\nreveals that verbalized uncertainty is not a reliable estimate of explanation\nconfidence. Further, we show that the probing uncertainty estimates are\ncorrelated with the faithfulness of an explanation, with lower uncertainty\ncorresponding to explanations with higher faithfulness. Our study provides\ninsights into the challenges and opportunities of quantifying uncertainty in\nLLM explanations, contributing to the broader discussion of the trustworthiness\nof foundation models.", "published": "2023-11-06 21:14:40", "link": "http://arxiv.org/abs/2311.03533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Dimensions of Online Conflict: Towards Modeling Agonism", "abstract": "Agonism plays a vital role in democratic dialogue by fostering diverse\nperspectives and robust discussions. Within the realm of online conflict there\nis another type: hateful antagonism, which undermines constructive dialogue.\nDetecting conflict online is central to platform moderation and monetization.\nIt is also vital for democratic dialogue, but only when it takes the form of\nagonism. To model these two types of conflict, we collected Twitter\nconversations related to trending controversial topics. We introduce a\ncomprehensive annotation schema for labelling different dimensions of conflict\nin the conversations, such as the source of conflict, the target, and the\nrhetorical strategies deployed. Using this schema, we annotated approximately\n4,000 conversations with multiple labels. We then trained both logistic\nregression and transformer-based models on the dataset, incorporating context\nfrom the conversation, including the number of participants and the structure\nof the interactions. Results show that contextual labels are helpful in\nidentifying conflict and make the models robust to variations in topic. Our\nresearch contributes a conceptualization of different dimensions of conflict, a\nrichly annotated dataset, and promising results that can contribute to content\nmoderation.", "published": "2023-11-06 22:34:17", "link": "http://arxiv.org/abs/2311.03584v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STONYBOOK: A System and Resource for Large-Scale Analysis of Novels", "abstract": "Books have historically been the primary mechanism through which narratives\nare transmitted. We have developed a collection of resources for the\nlarge-scale analysis of novels, including: (1) an open source end-to-end NLP\nanalysis pipeline for the annotation of novels into a standard XML format, (2)\na collection of 49,207 distinct cleaned and annotated novels, and (3) a\ndatabase with an associated web interface for the large-scale aggregate\nanalysis of these literary works. We describe the major functionalities\nprovided in the annotation system along with their utilities. We present\nsamples of analysis artifacts from our website, such as visualizations of\ncharacter occurrences and interactions, similar books, representative\nvocabulary, part of speech statistics, and readability metrics. We also\ndescribe the use of the annotated format in qualitative and quantitative\nanalysis across large corpora of novels.", "published": "2023-11-06 23:46:40", "link": "http://arxiv.org/abs/2311.03614v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Worker Perspectives into MTurk Annotation Practices for\n  NLP", "abstract": "Current practices regarding data collection for natural language processing\non Amazon Mechanical Turk (MTurk) often rely on a combination of studies on\ndata quality and heuristics shared among NLP researchers. However, without\nconsidering the perspectives of MTurk workers, these approaches are susceptible\nto issues regarding workers' rights and poor response quality. We conducted a\ncritical literature review and a survey of MTurk workers aimed at addressing\nopen questions regarding best practices for fair payment, worker privacy, data\nquality, and considering worker incentives. We found that worker preferences\nare often at odds with received wisdom among NLP researchers. Surveyed workers\npreferred reliable, reasonable payments over uncertain, very high payments;\nreported frequently lying on demographic questions; and expressed frustration\nat having work rejected with no explanation. We also found that workers view\nsome quality control methods, such as requiring minimum response times or\nMaster's qualifications, as biased and largely ineffective. Based on the survey\nresults, we provide recommendations on how future NLP studies may better\naccount for MTurk workers' experiences in order to respect workers' rights and\nimprove data quality.", "published": "2023-11-06 00:06:11", "link": "http://arxiv.org/abs/2311.02802v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Co-training and Co-distillation for Quality Improvement and Compression\n  of Language Models", "abstract": "Knowledge Distillation (KD) compresses computationally expensive pre-trained\nlanguage models (PLMs) by transferring their knowledge to smaller models,\nallowing their use in resource-constrained or real-time settings. However, most\nsmaller models fail to surpass the performance of the original larger model,\nresulting in sacrificing performance to improve inference speed. To address\nthis issue, we propose Co-Training and Co-Distillation (CTCD), a novel\nframework that improves performance and inference speed together by co-training\ntwo models while mutually distilling knowledge. The CTCD framework successfully\nachieves this based on two significant findings: 1) Distilling knowledge from\nthe smaller model to the larger model during co-training improves the\nperformance of the larger model. 2) The enhanced performance of the larger\nmodel further boosts the performance of the smaller model. The CTCD framework\nshows promise as it can be combined with existing techniques like architecture\ndesign or data augmentation, replacing one-way KD methods, to achieve further\nperformance improvement. Extensive ablation studies demonstrate the\neffectiveness of CTCD, and the small model distilled by CTCD outperforms the\noriginal larger model by a significant margin of 1.66 on the GLUE benchmark.", "published": "2023-11-06 03:29:00", "link": "http://arxiv.org/abs/2311.02849v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-Context Learning for Knowledge Base Question Answering for Unmanned\n  Systems based on Large Language Models", "abstract": "Knowledge Base Question Answering (KBQA) aims to answer factoid questions\nbased on knowledge bases. However, generating the most appropriate knowledge\nbase query code based on Natural Language Questions (NLQ) poses a significant\nchallenge in KBQA. In this work, we focus on the CCKS2023 Competition of\nQuestion Answering with Knowledge Graph Inference for Unmanned Systems.\nInspired by the recent success of large language models (LLMs) like ChatGPT and\nGPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL)\ngeneration framework to generate the most appropriate CQL based on the given\nNLQ. Our generative framework contains six parts: an auxiliary model predicting\nthe syntax-related information of CQL based on the given NLQ, a proper noun\nmatcher extracting proper nouns from the given NLQ, a demonstration example\nselector retrieving similar examples of the input sample, a prompt constructor\ndesigning the input template of ChatGPT, a ChatGPT-based generation model\ngenerating the CQL, and an ensemble model to obtain the final answers from\ndiversified outputs. With our ChatGPT-based CQL generation framework, we\nachieved the second place in the CCKS 2023 Question Answering with Knowledge\nGraph Inference for Unmanned Systems competition, achieving an F1-score of\n0.92676.", "published": "2023-11-06 08:52:11", "link": "http://arxiv.org/abs/2311.02956v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Zero-shot Bilingual App Reviews Mining with Large Language Models", "abstract": "App reviews from app stores are crucial for improving software requirements.\nA large number of valuable reviews are continually being posted, describing\nsoftware problems and expected features. Effectively utilizing user reviews\nnecessitates the extraction of relevant information, as well as their\nsubsequent summarization. Due to the substantial volume of user reviews, manual\nanalysis is arduous. Various approaches based on natural language processing\n(NLP) have been proposed for automatic user review mining. However, the\nmajority of them requires a manually crafted dataset to train their models,\nwhich limits their usage in real-world scenarios. In this work, we propose\nMini-BAR, a tool that integrates large language models (LLMs) to perform\nzero-shot mining of user reviews in both English and French. Specifically,\nMini-BAR is designed to (i) classify the user reviews, (ii) cluster similar\nreviews together, (iii) generate an abstractive summary for each cluster and\n(iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we\ncreated a dataset containing 6,000 English and 6,000 French annotated user\nreviews and conducted extensive experiments. Preliminary results demonstrate\nthe effectiveness and efficiency of Mini-BAR in requirement engineering by\nanalyzing bilingual app reviews. (Replication package containing the code,\ndataset, and experiment setups on https://github.com/Jl-wei/mini-bar )", "published": "2023-11-06 12:36:46", "link": "http://arxiv.org/abs/2311.03058v1", "categories": ["cs.CL", "cs.SE"], "primary_category": "cs.CL"}
{"title": "A Simple yet Efficient Ensemble Approach for AI-generated Text Detection", "abstract": "Recent Large Language Models (LLMs) have demonstrated remarkable capabilities\nin generating text that closely resembles human writing across wide range of\nstyles and genres. However, such capabilities are prone to potential abuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. Hence, it is essential to build automated approaches capable of\ndistinguishing between artificially generated text and human-authored text. In\nthis paper, we propose a simple yet efficient solution to this problem by\nensembling predictions from multiple constituent LLMs. Compared to previous\nstate-of-the-art approaches, which are perplexity-based or uses ensembles with\na number of LLMs, our condensed ensembling approach uses only two constituent\nLLMs to achieve comparable performance. Experiments conducted on four benchmark\ndatasets for generative text classification show performance improvements in\nthe range of 0.5 to 100\\% compared to previous state-of-the-art approaches. We\nalso study the influence that the training data from individual LLMs have on\nmodel performance. We found that substituting commercially-restrictive\nGenerative Pre-trained Transformer (GPT) data with data generated from other\nopen language models such as Falcon, Large Language Model Meta AI (LLaMA2), and\nMosaic Pretrained Transformers (MPT) is a feasible alternative when developing\ngenerative text detectors. Furthermore, to demonstrate zero-shot\ngeneralization, we experimented with an English essays dataset, and results\nsuggest that our ensembling approach can handle new data effectively.", "published": "2023-11-06 13:11:02", "link": "http://arxiv.org/abs/2311.03084v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Language Models are Super Mario: Absorbing Abilities from Homologous\n  Models as a Free Lunch", "abstract": "In this paper, we unveil that Language Models (LMs) can acquire new\ncapabilities by assimilating parameters from homologous models without\nretraining or GPUs. We first introduce DARE to set most delta parameters (i.e.,\nthe disparity between fine-tuned and pre-trained parameters) to zeros without\naffecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly\nDrops delta parameters with a ratio $p$ And REscales the remaining ones by $1 /\n(1 - p)$ to approximate the original embeddings. Then, we use DARE as a\nversatile plug-in to sparsify delta parameters of multiple SFT homologous\nmodels for mitigating parameter interference and merge them into a single model\nby parameter fusing. We experiment with encoder- and decoder-based LMs, showing\nthat: (1) SFT delta parameter value ranges are typically small (within 0.002)\nwith extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of\nthem; (2) DARE can merge multiple task-specific LMs into one LM with diverse\ncapabilities. Notably, this phenomenon is more pronounced in large-scale LMs,\nwhere the merged LM reveals the potential to surpass the performance of any\nsource LM, providing a new discovery. We also utilize DARE to create a merged\nLM that ranks first among models with 7 billion parameters on the Open LLM\nLeaderboard.", "published": "2023-11-06 13:43:07", "link": "http://arxiv.org/abs/2311.03099v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Findings of the WMT 2023 Shared Task on Discourse-Level Literary\n  Translation: A Fresh Orb in the Cosmos of LLMs", "abstract": "Translating literary works has perennially stood as an elusive dream in\nmachine translation (MT), a journey steeped in intricate challenges. To foster\nprogress in this domain, we hold a new shared task at WMT 2023, the first\nedition of the Discourse-Level Literary Translation. First, we (Tencent AI Lab\nand China Literature Ltd.) release a copyrighted and document-level\nChinese-English web novel corpus. Furthermore, we put forth an\nindustry-endorsed criteria to guide human evaluation process. This year, we\ntotally received 14 submissions from 7 academia and industry teams. We employ\nboth automatic and human evaluations to measure the performance of the\nsubmitted systems. The official ranking of the systems is based on the overall\nhuman judgments. In addition, our extensive analysis reveals a series of\ninteresting findings on literary and discourse-aware MT. We release data,\nsystem outputs, and leaderboard at\nhttp://www2.statmt.org/wmt23/literary-translation-task.html.", "published": "2023-11-06 14:23:49", "link": "http://arxiv.org/abs/2311.03127v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection\n  in Arabic Text", "abstract": "We present an overview of the ArAIEval shared task, organized as part of the\nfirst ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two\ntasks over Arabic text: (i) persuasion technique detection, focusing on\nidentifying persuasion techniques in tweets and news articles, and (ii)\ndisinformation detection in binary and multiclass setups over tweets. A total\nof 20 teams participated in the final evaluation phase, with 14 and 16 teams\nparticipating in Tasks 1 and 2, respectively. Across both tasks, we observed\nthat fine-tuning transformer models such as AraBERT was at the core of the\nmajority of the participating systems. We provide a description of the task\nsetup, including a description of the dataset construction and the evaluation\nsetup. We further give a brief overview of the participating systems. All\ndatasets and evaluation scripts from the shared task are released to the\nresearch community. (https://araieval.gitlab.io/) We hope this will enable\nfurther research on these important tasks in Arabic.", "published": "2023-11-06 15:21:19", "link": "http://arxiv.org/abs/2311.03179v1", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition", "abstract": "One of the major challenges for developing automatic speech recognition (ASR)\nfor low-resource languages is the limited access to labeled data with\ndomain-specific variations. In this study, we propose a pseudo-labeling\napproach to develop a large-scale domain-agnostic ASR dataset. With the\nproposed methodology, we developed a 20k+ hours labeled Bangla speech dataset\ncovering diverse topics, speaking styles, dialects, noisy environments, and\nconversational scenarios. We then exploited the developed corpus to design a\nconformer-based ASR system. We benchmarked the trained ASR with publicly\navailable datasets and compared it with other available models. To investigate\nthe efficacy, we designed and developed a human-annotated domain-agnostic test\nset composed of news, telephony, and conversational data among others. Our\nresults demonstrate the efficacy of the model trained on psuedo-label data for\nthe designed test-set along with publicly-available Bangla datasets. The\nexperimental resources will be publicly\navailable.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)", "published": "2023-11-06 15:37:14", "link": "http://arxiv.org/abs/2311.03196v1", "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "An Efficient Self-Supervised Cross-View Training For Sentence Embedding", "abstract": "Self-supervised sentence representation learning is the task of constructing\nan embedding space for sentences without relying on human annotation efforts.\nOne straightforward approach is to finetune a pretrained language model (PLM)\nwith a representation learning method such as contrastive learning. While this\napproach achieves impressive performance on larger PLMs, the performance\nrapidly degrades as the number of parameters decreases. In this paper, we\npropose a framework called Self-supervised Cross-View Training (SCT) to narrow\nthe performance gap between large and small PLMs. To evaluate the effectiveness\nof SCT, we compare it to 5 baseline and state-of-the-art competitors on seven\nSemantic Textual Similarity (STS) benchmarks using 5 PLMs with the number of\nparameters ranging from 4M to 340M. The experimental results show that STC\noutperforms the competitors for PLMs with less than 100M parameters in 18 of 21\ncases.", "published": "2023-11-06 16:12:25", "link": "http://arxiv.org/abs/2311.03228v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers", "abstract": "Generative approaches powered by large language models (LLMs) have\ndemonstrated emergent abilities in tasks that require complex reasoning\nabilities. Yet the generative nature still makes the generated content suffer\nfrom hallucinations, thus unsuitable for entity-centric tasks like entity\nlinking (EL) requiring precise entity predictions over a large knowledge base.\nWe present Instructed Generative Entity Linker (INSGENEL), the first approach\nthat enables casual language models to perform entity linking over knowledge\nbases. Several methods to equip language models with EL capability were\nproposed in this work, including (i) a sequence-to-sequence training EL\nobjective with instruction-tuning, (ii) a novel generative EL framework based\non a light-weight potential mention retriever that frees the model from heavy\nand non-parallelizable decoding, achieving 4$\\times$ speedup without compromise\non linking metrics. INSGENEL outperforms previous generative alternatives with\n+6.8 F1 points gain on average, also with a huge advantage in training data\nefficiency and training compute consumption. In addition, our skillfully\nengineered in-context learning (ICL) framework for EL still lags behind\nINSGENEL significantly, reaffirming that the EL task remains a persistent\nhurdle for general LLMs.", "published": "2023-11-06 16:38:51", "link": "http://arxiv.org/abs/2311.03250v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Coherent Entity Disambiguation via Modeling Topic and Categorical\n  Dependency", "abstract": "Previous entity disambiguation (ED) methods adopt a discriminative paradigm,\nwhere prediction is made based on matching scores between mention context and\ncandidate entities using length-limited encoders. However, these methods often\nstruggle to capture explicit discourse-level dependencies, resulting in\nincoherent predictions at the abstract level (e.g. topic or category). We\npropose CoherentED, an ED system equipped with novel designs aimed at enhancing\nthe coherence of entity predictions. Our method first introduces an\nunsupervised variational autoencoder (VAE) to extract latent topic vectors of\ncontext sentences. This approach not only allows the encoder to handle longer\ndocuments more effectively, conserves valuable input space, but also keeps a\ntopic-level coherence. Additionally, we incorporate an external category\nmemory, enabling the system to retrieve relevant categories for undecided\nmentions. By employing step-by-step entity decisions, this design facilitates\nthe modeling of entity-entity interactions, thereby maintaining maximum\ncoherence at the category level. We achieve new state-of-the-art results on\npopular ED benchmarks, with an average improvement of 1.3 F1 points. Our model\ndemonstrates particularly outstanding performance on challenging long-text\nscenarios.", "published": "2023-11-06 16:40:13", "link": "http://arxiv.org/abs/2311.03253v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unraveling Downstream Gender Bias from Large Language Models: A Study on\n  AI Educational Writing Assistance", "abstract": "Large Language Models (LLMs) are increasingly utilized in educational tasks\nsuch as providing writing suggestions to students. Despite their potential,\nLLMs are known to harbor inherent biases which may negatively impact learners.\nPrevious studies have investigated bias in models and data representations\nseparately, neglecting the potential impact of LLM bias on human writing. In\nthis paper, we investigate how bias transfers through an AI writing support\npipeline. We conduct a large-scale user study with 231 students writing\nbusiness case peer reviews in German. Students are divided into five groups\nwith different levels of writing support: one classroom group with\nfeature-based suggestions and four groups recruited from Prolific -- a control\ngroup with no assistance, two groups with suggestions from fine-tuned GPT-2 and\nGPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using\nGenBit gender bias analysis, Word Embedding Association Tests (WEAT), and\nSentence Embedding Association Test (SEAT) we evaluate the gender bias at\nvarious stages of the pipeline: in model embeddings, in suggestions generated\nby the models, and in reviews written by students. Our results demonstrate that\nthere is no significant difference in gender bias between the resulting peer\nreviews of groups with and without LLM suggestions. Our research is therefore\noptimistic about the use of AI writing support in the classroom, showcasing a\ncontext where bias in LLMs does not transfer to students' responses.", "published": "2023-11-06 18:01:34", "link": "http://arxiv.org/abs/2311.03311v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase", "abstract": "In-Context Learning (ICL) combined with pre-trained large language models has\nachieved promising results on various NLP tasks. However, ICL requires\nhigh-quality annotated demonstrations which might not be available in\nreal-world scenarios. To overcome this limitation, we propose \\textbf{D}ata\n\\textbf{A}ugmentation for \\textbf{I}n-Context \\textbf{L}earning\n(\\textbf{DAIL}). DAIL leverages the intuition that large language models are\nmore familiar with the content generated by themselves. It first utilizes the\nlanguage model to generate paraphrases of the test sample and employs majority\nvoting to determine the final result based on individual predictions. Our\nextensive empirical evaluation shows that DAIL outperforms the standard ICL\nmethod and other ensemble-based methods in the low-resource scenario.\nAdditionally, we explore the use of voting consistency as a confidence score of\nthe model when the logits of predictions are inaccessible. We believe our work\nwill stimulate further research on ICL in low-resource settings.", "published": "2023-11-06 18:12:55", "link": "http://arxiv.org/abs/2311.03319v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "In-Context Exemplars as Clues to Retrieving from Large Associative\n  Memory", "abstract": "Recently, large language models (LLMs) have made remarkable progress in\nnatural language processing. The most representative ability of LLMs is\nin-context learning (ICL), which enables LLMs to learn patterns from in-context\nexemplars without training. The performance of ICL greatly depends on the\nexemplars used. However, how to choose exemplars remains unclear due to the\nlack of understanding of how in-context learning works. In this paper, we\npresent a novel perspective on ICL by conceptualizing it as contextual\nretrieval from a model of associative memory. We establish a theoretical\nframework of ICL based on Hopfield Networks. Based on our framework, we look\ninto how in-context exemplars influence the performance of ICL and propose more\nefficient active exemplar selection. Our study sheds new light on the mechanism\nof ICL by connecting it to memory retrieval, with potential implications for\nadvancing the understanding of LLMs.", "published": "2023-11-06 20:13:29", "link": "http://arxiv.org/abs/2311.03498v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Context Unlocks Emotions: Text-based Emotion Classification Dataset\n  Auditing with Large Language Models", "abstract": "The lack of contextual information in text data can make the annotation\nprocess of text-based emotion classification datasets challenging. As a result,\nsuch datasets often contain labels that fail to consider all the relevant\nemotions in the vocabulary. This misalignment between text inputs and labels\ncan degrade the performance of machine learning models trained on top of them.\nAs re-annotating entire datasets is a costly and time-consuming task that\ncannot be done at scale, we propose to use the expressive capabilities of large\nlanguage models to synthesize additional context for input text to increase its\nalignment with the annotated emotional labels. In this work, we propose a\nformal definition of textual context to motivate a prompting strategy to\nenhance such contextual information. We provide both human and empirical\nevaluation to demonstrate the efficacy of the enhanced context. Our method\nimproves alignment between inputs and their human-annotated labels from both an\nempirical and human-evaluated standpoint.", "published": "2023-11-06 21:34:49", "link": "http://arxiv.org/abs/2311.03551v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT4All: An Ecosystem of Open Source Compressed Language Models", "abstract": "Large language models (LLMs) have recently achieved human-level performance\non a range of professional and academic benchmarks. The accessibility of these\nmodels has lagged behind their performance. State-of-the-art LLMs require\ncostly infrastructure; are only accessible via rate-limited, geo-locked, and\ncensored web interfaces; and lack publicly available code and technical\nreports. In this paper, we tell the story of GPT4All, a popular open source\nrepository that aims to democratize access to LLMs. We outline the technical\ndetails of the original GPT4All model family, as well as the evolution of the\nGPT4All project from a single model into a fully fledged open source ecosystem.\nIt is our hope that this paper acts as both a technical overview of the\noriginal GPT4All models as well as a case study on the subsequent growth of the\nGPT4All open source ecosystem.", "published": "2023-11-06 23:50:20", "link": "http://arxiv.org/abs/2311.04931v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "QualEval: Qualitative Evaluation for Model Improvement", "abstract": "Quantitative evaluation metrics have traditionally been pivotal in gauging\nthe advancements of artificial intelligence systems, including large language\nmodels (LLMs). However, these metrics have inherent limitations. Given the\nintricate nature of real-world tasks, a single scalar to quantify and compare\nis insufficient to capture the fine-grained nuances of model behavior. Metrics\nserve only as a way to compare and benchmark models, and do not yield\nactionable diagnostics, thus making the model improvement process challenging.\nModel developers find themselves amid extensive manual efforts involving\nsifting through vast datasets and attempting hit-or-miss adjustments to\ntraining data or setups. In this work, we address the shortcomings of\nquantitative metrics by proposing QualEval, which augments quantitative scalar\nmetrics with automated qualitative evaluation as a vehicle for model\nimprovement. QualEval uses a powerful LLM reasoner and our novel flexible\nlinear programming solver to generate human-readable insights that when\napplied, accelerate model improvement. The insights are backed by a\ncomprehensive dashboard with fine-grained visualizations and\nhuman-interpretable analyses. We corroborate the faithfulness of QualEval by\ndemonstrating that leveraging its insights, for example, improves the absolute\nperformance of the Llama 2 model by up to 15% points relative on a challenging\ndialogue task (DialogSum) when compared to baselines. QualEval successfully\nincreases the pace of model development, thus in essence serving as a\ndata-scientist-in-a-box. Given the focus on critiquing and improving current\nevaluation metrics, our method serves as a refreshingly new technique for both\nmodel evaluation and improvement.", "published": "2023-11-06 00:21:44", "link": "http://arxiv.org/abs/2311.02807v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Retrieval-Augmented Code Generation for Universal Information Extraction", "abstract": "Information Extraction (IE) aims to extract structural knowledge (e.g.,\nentities, relations, events) from natural language texts, which brings\nchallenges to existing methods due to task-specific schemas and complex text\nexpressions. Code, as a typical kind of formalized language, is capable of\ndescribing structural knowledge under various schemas in a universal way. On\nthe other hand, Large Language Models (LLMs) trained on both codes and texts\nhave demonstrated powerful capabilities of transforming texts into codes, which\nprovides a feasible solution to IE tasks. Therefore, in this paper, we propose\na universal retrieval-augmented code generation framework based on LLMs, called\nCode4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define\ntask-specific schemas of various structural knowledge in a universal way. By so\ndoing, extracting knowledge under these schemas can be transformed into\ngenerating codes that instantiate the predefined Python classes with the\ninformation in texts. To generate these codes more precisely, Code4UIE adopts\nthe in-context learning mechanism to instruct LLMs with examples. In order to\nobtain appropriate examples for different tasks, Code4UIE explores several\nexample retrieval strategies, which can retrieve examples semantically similar\nto the given texts. Extensive experiments on five representative IE tasks\nacross nine datasets demonstrate the effectiveness of the Code4UIE framework.", "published": "2023-11-06 09:03:21", "link": "http://arxiv.org/abs/2311.02962v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Injecting Categorical Labels and Syntactic Information into Biomedical\n  NER", "abstract": "We present a simple approach to improve biomedical named entity recognition\n(NER) by injecting categorical labels and Part-of-speech (POS) information into\nthe model. We use two approaches, in the first approach, we first train a\nsequence-level classifier to classify the sentences into categories to obtain\nthe sentence-level tags (categorical labels). The sequence classifier is\nmodeled as an entailment problem by modifying the labels as a natural language\ntemplate. This helps to improve the accuracy of the classifier. Further, this\nlabel information is injected into the NER model. In this paper, we demonstrate\neffective ways to represent and inject these labels and POS attributes into the\nNER model. In the second approach, we jointly learn the categorical labels and\nNER labels. Here we also inject the POS tags into the model to increase the\nsyntactic context of the model. Experiments on three benchmark datasets show\nthat incorporating categorical label information with syntactic context is\nquite useful and outperforms baseline BERT-based models.", "published": "2023-11-06 14:03:59", "link": "http://arxiv.org/abs/2311.03113v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for\n  Propaganda and Disinformation Detection", "abstract": "The spread of disinformation and propagandistic content poses a threat to\nsocietal harmony, undermining informed decision-making and trust in reliable\nsources. Online platforms often serve as breeding grounds for such content, and\nmalicious actors exploit the vulnerabilities of audiences to shape public\nopinion. Although there have been research efforts aimed at the automatic\nidentification of disinformation and propaganda in social media content, there\nremain challenges in terms of performance. The ArAIEval shared task aims to\nfurther research on these particular issues within the context of the Arabic\nlanguage. In this paper, we discuss our participation in these shared tasks. We\ncompeted in subtasks 1A and 2A, where our submitted system secured positions\n9th and 10th, respectively. Our experiments consist of fine-tuning transformer\nmodels and using zero- and few-shot learning with GPT-4.", "published": "2023-11-06 15:24:18", "link": "http://arxiv.org/abs/2311.03184v1", "categories": ["cs.CL", "cs.AI", "cs.SI", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Mini Minds: Exploring Bebeshka and Zlata Baby Models", "abstract": "In this paper, we describe the University of Lyon 2 submission to the\nStrict-Small track of the BabyLM competition. The shared task is created with\nan emphasis on small-scale language modelling from scratch on limited-size data\nand human language acquisition. Dataset released for the Strict-Small track has\n10M words, which is comparable to children's vocabulary size. We approach the\ntask with an architecture search, minimizing masked language modelling loss on\nthe data of the shared task. Having found an optimal configuration, we\nintroduce two small-size language models (LMs) that were submitted for\nevaluation, a 4-layer encoder with 8 attention heads and a 6-layer decoder\nmodel with 12 heads which we term Bebeshka and Zlata, respectively. Despite\nbeing half the scale of the baseline LMs, our proposed models achieve\ncomparable performance. We further explore the applicability of small-scale\nlanguage models in tasks involving moral judgment, aligning their predictions\nwith human values. These findings highlight the potential of compact LMs in\naddressing practical language understanding tasks.", "published": "2023-11-06 16:01:10", "link": "http://arxiv.org/abs/2311.03216v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic\n  Decision-Making with AI Agents", "abstract": "This paper introduces Alympics (Olympics for Agents), a systematic simulation\nframework utilizing Large Language Model (LLM) agents for game theory research.\nAlympics creates a versatile platform for studying complex game theory\nproblems, bridging the gap between theoretical game theory and empirical\ninvestigations by providing a controlled environment for simulating human-like\nstrategic interactions with LLM agents. In our pilot case study, the \"Water\nAllocation Challenge,\" we explore Alympics through a challenging strategic game\nfocused on the multi-round auction on scarce survival resources. This study\ndemonstrates the framework's ability to qualitatively and quantitatively\nanalyze game determinants, strategies, and outcomes. Additionally, we conduct a\ncomprehensive human assessment and an in-depth evaluation of LLM agents in\nstrategic decision-making scenarios. Our findings not only expand the\nunderstanding of LLM agents' proficiency in emulating human strategic behavior\nbut also highlight their potential in advancing game theory knowledge, thereby\nenriching our understanding of both game theory and empowering further research\ninto strategic decision-making domains with LLM agents. Codes, prompts, and all\nrelated resources are available at https://github.com/microsoft/Alympics.", "published": "2023-11-06 16:03:46", "link": "http://arxiv.org/abs/2311.03220v4", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL"}
{"title": "p-Laplacian Transformer", "abstract": "$p$-Laplacian regularization, rooted in graph and image signal processing,\nintroduces a parameter $p$ to control the regularization effect on these data.\nSmaller values of $p$ promote sparsity and interpretability, while larger\nvalues encourage smoother solutions. In this paper, we first show that the\nself-attention mechanism obtains the minimal Laplacian regularization ($p=2$)\nand encourages the smoothness in the architecture. However, the smoothness is\nnot suitable for the heterophilic structure of self-attention in transformers\nwhere attention weights between tokens that are in close proximity and\nnon-close ones are assigned indistinguishably. From that insight, we then\npropose a novel class of transformers, namely the $p$-Laplacian Transformer\n(p-LaT), which leverages $p$-Laplacian regularization framework to harness the\nheterophilic features within self-attention layers. In particular, low $p$\nvalues will effectively assign higher attention weights to tokens that are in\nclose proximity to the current token being processed. We empirically\ndemonstrate the advantages of p-LaT over the baseline transformers on a wide\nrange of benchmark datasets.", "published": "2023-11-06 16:25:56", "link": "http://arxiv.org/abs/2311.03235v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Holistic Analysis of Hallucination in GPT-4V(ision): Bias and\n  Interference Challenges", "abstract": "While GPT-4V(ision) impressively models both visual and textual information\nsimultaneously, it's hallucination behavior has not been systematically\nassessed. To bridge this gap, we introduce a new benchmark, namely, the Bias\nand Interference Challenges in Visual Language Models (Bingo). This benchmark\nis designed to evaluate and shed light on the two common types of\nhallucinations in visual language models: bias and interference. Here, bias\nrefers to the model's tendency to hallucinate certain types of responses,\npossibly due to imbalance in its training data. Interference pertains to\nscenarios where the judgment of GPT-4V(ision) can be disrupted due to how the\ntext prompt is phrased or how the input image is presented. We identify a\nnotable regional bias, whereby GPT-4V(ision) is better at interpreting Western\nimages or images with English writing compared to images from other countries\nor containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to\nleading questions and is often confused when interpreting multiple images\ntogether. Popular mitigation approaches, such as self-correction and\nchain-of-thought reasoning, are not effective in resolving these challenges. We\nalso identified similar biases and interference vulnerabilities with LLaVA and\nBard. Our results characterize the hallucination challenges in GPT-4V(ision)\nand state-of-the-art visual-language models, and highlight the need for new\nsolutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.", "published": "2023-11-06 17:26:59", "link": "http://arxiv.org/abs/2311.03287v2", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Scalable and Transferable Black-Box Jailbreaks for Language Models via\n  Persona Modulation", "abstract": "Despite efforts to align large language models to produce harmless responses,\nthey are still vulnerable to jailbreak prompts that elicit unrestricted\nbehaviour. In this work, we investigate persona modulation as a black-box\njailbreaking method to steer a target model to take on personalities that are\nwilling to comply with harmful instructions. Rather than manually crafting\nprompts for each persona, we automate the generation of jailbreaks using a\nlanguage model assistant. We demonstrate a range of harmful completions made\npossible by persona modulation, including detailed instructions for\nsynthesising methamphetamine, building a bomb, and laundering money. These\nautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is\n185 times larger than before modulation (0.23%). These prompts also transfer to\nClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,\nrespectively. Our work reveals yet another vulnerability in commercial large\nlanguage models and highlights the need for more comprehensive safeguards.", "published": "2023-11-06 18:55:18", "link": "http://arxiv.org/abs/2311.03348v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Text Augmentations with R-drop for Classification of Tweets Self\n  Reporting Covid-19", "abstract": "This paper presents models created for the Social Media Mining for Health\n2023 shared task. Our team addressed the first task, classifying tweets that\nself-report Covid-19 diagnosis. Our approach involves a classification model\nthat incorporates diverse textual augmentations and utilizes R-drop to augment\ndata and mitigate overfitting, boosting model efficacy. Our leading model,\nenhanced with R-drop and augmentations like synonym substitution, reserved\nwords, and back translations, outperforms the task mean and median scores. Our\nsystem achieves an impressive F1 score of 0.877 on the test set.", "published": "2023-11-06 14:18:16", "link": "http://arxiv.org/abs/2311.03420v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Measuring Adversarial Datasets", "abstract": "In the era of widespread public use of AI systems across various domains,\nensuring adversarial robustness has become increasingly vital to maintain\nsafety and prevent undesirable errors. Researchers have curated various\nadversarial datasets (through perturbations) for capturing model deficiencies\nthat cannot be revealed in standard benchmark datasets. However, little is\nknown about how these adversarial examples differ from the original data\npoints, and there is still no methodology to measure the intended and\nunintended consequences of those adversarial transformations. In this research,\nwe conducted a systematic survey of existing quantifiable metrics that describe\ntext instances in NLP tasks, among dimensions of difficulty, diversity, and\ndisagreement. We selected several current adversarial effect datasets and\ncompared the distributions between the original and their adversarial\ncounterparts. The results provide valuable insights into what makes these\ndatasets more challenging from a metrics perspective and whether they align\nwith underlying assumptions.", "published": "2023-11-06 22:08:16", "link": "http://arxiv.org/abs/2311.03566v1", "categories": ["cs.LG", "cs.CL", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Can LLMs Follow Simple Rules?", "abstract": "As Large Language Models (LLMs) are deployed with increasing real-world\nresponsibilities, it is important to be able to specify and constrain the\nbehavior of these systems in a reliable manner. Model developers may wish to\nset explicit rules for the model, such as \"do not generate abusive content\",\nbut these may be circumvented by jailbreaking techniques. Existing evaluations\nof adversarial attacks and defenses on LLMs generally require either expensive\nmanual review or unreliable heuristic checks. To address this issue, we propose\nRule-following Language Evaluation Scenarios (RuLES), a programmatic framework\nfor measuring rule-following ability in LLMs. RuLES consists of 14 simple text\nscenarios in which the model is instructed to obey various rules while\ninteracting with the user. Each scenario has a programmatic evaluation function\nto determine whether the model has broken any rules in a conversation. Our\nevaluations of proprietary and open models show that almost all current models\nstruggle to follow scenario rules, even on straightforward test cases. We also\ndemonstrate that simple optimization attacks suffice to significantly increase\nfailure rates on test cases. We conclude by exploring two potential avenues for\nimprovement: test-time steering and supervised fine-tuning.", "published": "2023-11-06 08:50:29", "link": "http://arxiv.org/abs/2311.04235v3", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Performance Prediction of Data-Driven Knowledge summarization of High\n  Entropy Alloys (HEAs) literature implementing Natural Language Processing\n  algorithms", "abstract": "The ability to interpret spoken language is connected to natural language\nprocessing. It involves teaching the AI how words relate to one another, how\nthey are meant to be used, and in what settings. The goal of natural language\nprocessing (NLP) is to get a machine intelligence to process words the same way\na human brain does. This enables machine intelligence to interpret, arrange,\nand comprehend textual data by processing the natural language. The technology\ncan comprehend what is communicated, whether it be through speech or writing\nbecause AI pro-cesses language more quickly than humans can. In the present\nstudy, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic\nAnalysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the\nfirst time for the knowledge summarization purpose of the High Entropy Alloys\n(HEAs). The performance prediction of these algorithms is made by using the\nBLEU score and ROUGE score. The results showed that the Luhn algorithm has the\nhighest accuracy score for the knowledge summarization tasks compared to the\nother used algorithms.", "published": "2023-11-06 16:22:32", "link": "http://arxiv.org/abs/2311.07584v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.IT", "cs.LG", "math.IT"], "primary_category": "cs.CL"}
{"title": "HRTF Estimation in the Wild", "abstract": "Head Related Transfer Functions (HRTFs) play a crucial role in creating\nimmersive spatial audio experiences. However, HRTFs differ significantly from\nperson to person, and traditional methods for estimating personalized HRTFs are\nexpensive, time-consuming, and require specialized equipment. We imagine a\nworld where your personalized HRTF can be determined by capturing data through\nearbuds in everyday environments. In this paper, we propose a novel approach\nfor deriving personalized HRTFs that only relies on in-the-wild binaural\nrecordings and head tracking data. By analyzing how sounds change as the user\nrotates their head through different environments with different noise sources,\nwe can accurately estimate their personalized HRTF. Our results show that our\npredicted HRTFs closely match ground-truth HRTFs measured in an anechoic\nchamber. Furthermore, listening studies demonstrate that our personalized HRTFs\nsignificantly improve sound localization and reduce front-back confusion in\nvirtual environments. Our approach offers an efficient and accessible method\nfor deriving personalized HRTFs and has the potential to greatly improve\nspatial audio experiences.", "published": "2023-11-06 22:01:18", "link": "http://arxiv.org/abs/2311.03560v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic\n  Token Prediction", "abstract": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We\nuse discretized semantic tokens acquired from wav2vec2.0 embeddings, which\nmakes it easy to adopt a neural transducer for the TTS framework enjoying its\nmonotonic alignment constraints. The proposed model first generates aligned\nsemantic tokens using the neural transducer, then synthesizes a speech sample\nfrom the semantic tokens using a non-autoregressive(NAR) speech generator. This\ndecoupled framework alleviates the training complexity of TTS and allows each\nstage to focus on 1) linguistic and alignment modeling and 2) fine-grained\nacoustic modeling, respectively. Experimental results on the zero-shot adaptive\nTTS show that the proposed model exceeds the baselines in speech quality and\nspeaker similarity via objective and subjective measures. We also investigate\nthe inference speed and prosody controllability of our proposed model, showing\nthe potential of the neural transducer for TTS frameworks.", "published": "2023-11-06 06:13:39", "link": "http://arxiv.org/abs/2311.02898v2", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "A Foundation Model for Music Informatics", "abstract": "This paper investigates foundation models tailored for music informatics, a\ndomain currently challenged by the scarcity of labeled data and generalization\nissues. To this end, we conduct an in-depth comparative study among various\nfoundation model variants, examining key determinants such as model\narchitectures, tokenization methods, temporal resolution, data, and model\nscalability. This research aims to bridge the existing knowledge gap by\nelucidating how these individual factors contribute to the success of\nfoundation models in music informatics. Employing a careful evaluation\nframework, we assess the performance of these models across diverse downstream\ntasks in music information retrieval, with a particular focus on token-level\nand sequence-level classification. Our results reveal that our model\ndemonstrates robust performance, surpassing existing models in specific key\nmetrics. These findings contribute to the understanding of self-supervised\nlearning in music informatics and pave the way for developing more effective\nand versatile foundation models in the field. A pretrained version of our model\nis publicly available to foster reproducibility and future research.", "published": "2023-11-06 18:12:27", "link": "http://arxiv.org/abs/2311.03318v1", "categories": ["cs.SD", "cs.IR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Personalizing Keyword Spotting with Speaker Information", "abstract": "Keyword spotting systems often struggle to generalize to a diverse population\nwith various accents and age groups. To address this challenge, we propose a\nnovel approach that integrates speaker information into keyword spotting using\nFeature-wise Linear Modulation (FiLM), a recent method for learning from\nmultiple sources of information. We explore both Text-Dependent and\nText-Independent speaker recognition systems to extract speaker information,\nand we experiment on extracting this information from both the input audio and\npre-enrolled user audio. We evaluate our systems on a diverse dataset and\nachieve a substantial improvement in keyword detection accuracy, particularly\namong underrepresented speaker groups. Moreover, our proposed approach only\nrequires a small 1% increase in the number of parameters, with a minimum impact\non latency and computational cost, which makes it a practical solution for\nreal-world applications.", "published": "2023-11-06 12:16:06", "link": "http://arxiv.org/abs/2311.03419v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Combinatorial Hodge Theory in Simplicial Signal Processing -- DAFx2023\n  Lecture Notes", "abstract": "Lecture notes of a tutorial on Combinatorial Hodge Theory in Simplicial\nSignal Processing held at international conference for digital audio effects\n(DAFx-23) in Copenhagen, Denmark.", "published": "2023-11-06 19:11:40", "link": "http://arxiv.org/abs/2311.03469v2", "categories": ["eess.SP", "cs.SD", "eess.AS", "math.CO"], "primary_category": "eess.SP"}
{"title": "MFAAN: Unveiling Audio Deepfakes with a Multi-Feature Authenticity\n  Network", "abstract": "In the contemporary digital age, the proliferation of deepfakes presents a\nformidable challenge to the sanctity of information dissemination. Audio\ndeepfakes, in particular, can be deceptively realistic, posing significant\nrisks in misinformation campaigns. To address this threat, we introduce the\nMulti-Feature Audio Authenticity Network (MFAAN), an advanced architecture\ntailored for the detection of fabricated audio content. MFAAN incorporates\nmultiple parallel paths designed to harness the strengths of different audio\nrepresentations, including Mel-frequency cepstral coefficients (MFCC),\nlinear-frequency cepstral coefficients (LFCC), and Chroma Short Time Fourier\nTransform (Chroma-STFT). By synergistically fusing these features, MFAAN\nachieves a nuanced understanding of audio content, facilitating robust\ndifferentiation between genuine and manipulated recordings. Preliminary\nevaluations of MFAAN on two benchmark datasets, 'In-the-Wild' Audio Deepfake\nData and The Fake-or-Real Dataset, demonstrate its superior performance,\nachieving accuracies of 98.93% and 94.47% respectively. Such results not only\nunderscore the efficacy of MFAAN but also highlight its potential as a pivotal\ntool in the ongoing battle against deepfake audio content.", "published": "2023-11-06 20:32:39", "link": "http://arxiv.org/abs/2311.03509v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SoundCam: A Dataset for Finding Humans Using Room Acoustics", "abstract": "A room's acoustic properties are a product of the room's geometry, the\nobjects within the room, and their specific positions. A room's acoustic\nproperties can be characterized by its impulse response (RIR) between a source\nand listener location, or roughly inferred from recordings of natural signals\npresent in the room. Variations in the positions of objects in a room can\neffect measurable changes in the room's acoustic properties, as characterized\nby the RIR. Existing datasets of RIRs either do not systematically vary\npositions of objects in an environment, or they consist of only simulated RIRs.\nWe present SoundCam, the largest dataset of unique RIRs from in-the-wild rooms\npublicly released to date. It includes 5,000 10-channel real-world measurements\nof room impulse responses and 2,000 10-channel recordings of music in three\ndifferent rooms, including a controlled acoustic lab, an in-the-wild living\nroom, and a conference room, with different humans in positions throughout each\nroom. We show that these measurements can be used for interesting tasks, such\nas detecting and identifying humans, and tracking their positions.", "published": "2023-11-06 20:51:16", "link": "http://arxiv.org/abs/2311.03517v2", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
