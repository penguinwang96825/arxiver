{"title": "Arithmetic Word Problem Solver using Frame Identification", "abstract": "Automatic Word problem solving has always posed a great challenge for the NLP\ncommunity. Usually a word problem is a narrative comprising of a few sentences\nand a question is asked about a quantity referred in the sentences. Solving\nword problem involves reasoning across sentences, identification of operations,\ntheir order, relevant quantities and discarding irrelevant quantities. In this\npaper, we present a novel approach for automatic arithmetic word problem\nsolving. Our approach starts with frame identification. Each frame can either\nbe classified as a state or an action frame. The frame identification is\ndependent on the verb in a sentence. Every frame is unique and is identified by\nits slots. The slots are filled using dependency parsed output of a sentence.\nThe slots are entity holder, entity, quantity of the entity, recipient,\nadditional information like place, time. The slots and frames helps to identify\nthe type of question asked and the entity referred. Action frames act on state\nframe(s) which causes a change in quantities of the state frames. The frames\nare then used to build a graph where any change in quantities can be propagated\nto the neighboring nodes. Most of the current solvers can only answer questions\nrelated to the quantity, while our system can answer different kinds of\nquestions like `who', `what' other than the quantity related questions `how\nmany'.\n  There are three major contributions of this paper. 1. Frame Annotated Corpus\n(with a frame annotation tool) 2. Frame Identification Module 3. A new easily\nunderstandable Framework for word problem solving", "published": "2018-08-09 05:57:13", "link": "http://arxiv.org/abs/1808.03028v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Sentiment and Emotion Analysis for Computational Literary\n  Studies", "abstract": "Emotions are a crucial part of compelling narratives: literature tells us\nabout people with goals, desires, passions, and intentions. Emotion analysis is\npart of the broader and larger field of sentiment analysis, and receives\nincreasing attention in literary studies. In the past, the affective dimension\nof literature was mainly studied in the context of literary hermeneutics.\nHowever, with the emergence of the research field known as Digital Humanities\n(DH), some studies of emotions in a literary context have taken a computational\nturn. Given the fact that DH is still being formed as a field, this direction\nof research can be rendered relatively new. In this survey, we offer an\noverview of the existing body of research on emotion analysis as applied to\nliterature. The research under review deals with a variety of topics including\ntracking dramatic changes of a plot development, network analysis of a literary\ntext, and understanding the emotionality of texts, among other topics.", "published": "2018-08-09 13:12:07", "link": "http://arxiv.org/abs/1808.03137v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Kannada POS Tagger Using Machine Learning and Neural Network\n  Models", "abstract": "POS Tagging serves as a preliminary task for many NLP applications. Kannada\nis a relatively poor Indian language with very limited number of quality NLP\ntools available for use. An accurate and reliable POS Tagger is essential for\nmany NLP tasks like shallow parsing, dependency parsing, sentiment analysis,\nnamed entity recognition. We present a statistical POS tagger for Kannada using\ndifferent machine learning and neural network models. Our Kannada POS tagger\noutperforms the state-of-the-art Kannada POS tagger by 6%. Our contribution in\nthis paper is three folds - building a generic POS Tagger, comparing the\nperformances of different modeling techniques, exploring the use of character\nand word embeddings together for Kannada POS Tagging.", "published": "2018-08-09 14:16:30", "link": "http://arxiv.org/abs/1808.03175v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-Mixed Sentiment Analysis Using Machine Learning and Neural Network\n  Approaches", "abstract": "Sentiment Analysis for Indian Languages (SAIL)-Code Mixed tools contest aimed\nat identifying the sentence level sentiment polarity of the code-mixed dataset\nof Indian languages pairs (Hi-En, Ben-Hi-En). Hi-En dataset is henceforth\nreferred to as HI-EN and Ben-Hi-En dataset as BN-EN respectively. For this, we\nsubmitted four models for sentiment analysis of code-mixed HI-EN and BN-EN\ndatasets. The first model was an ensemble voting classifier consisting of three\nclassifiers - linear SVM, logistic regression and random forests while the\nsecond one was a linear SVM. Both the models used TF-IDF feature vectors of\ncharacter n-grams where n ranged from 2 to 6. We used scikit-learn (sklearn)\nmachine learning library for implementing both the approaches. Run1 was\nobtained from the voting classifier and Run2 used the linear SVM model for\nproducing the results. Out of the four submitted outputs Run2 outperformed Run1\nin both the datasets. We finished first in the contest for both HI-EN with an\nF-score of 0.569 and BN-EN with an F-score of 0.526.", "published": "2018-08-09 18:38:16", "link": "http://arxiv.org/abs/1808.03299v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient human-like semantic representations via the Information\n  Bottleneck principle", "abstract": "Maintaining efficient semantic representations of the environment is a major\nchallenge both for humans and for machines. While human languages represent\nuseful solutions to this problem, it is not yet clear what computational\nprinciple could give rise to similar solutions in machines. In this work we\npropose an answer to this open question. We suggest that languages compress\npercepts into words by optimizing the Information Bottleneck (IB) tradeoff\nbetween the complexity and accuracy of their lexicons. We present empirical\nevidence that this principle may give rise to human-like semantic\nrepresentations, by exploring how human languages categorize colors. We show\nthat color naming systems across languages are near-optimal in the IB sense,\nand that these natural systems are similar to artificial IB color naming\nsystems with a single tradeoff parameter controlling the cross-language\nvariability. In addition, the IB systems evolve through a sequence of\nstructural phase transitions, demonstrating a possible adaptation process. This\nwork thus identifies a computational principle that characterizes human\nsemantic systems, and that could usefully inform semantic representations in\nmachines.", "published": "2018-08-09 21:44:58", "link": "http://arxiv.org/abs/1808.03353v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Learning Based Natural Language Processing for End to End Speech\n  Translation", "abstract": "Deep Learning methods employ multiple processing layers to learn hierarchial\nrepresentations of data. They have already been deployed in a humongous number\nof applications and have produced state-of-the-art results. Recently with the\ngrowth in processing power of computers to be able to do high dimensional\ntensor calculations, Natural Language Processing (NLP) applications have been\ngiven a significant boost in terms of efficiency as well as accuracy. In this\npaper, we will take a look at various signal processing techniques and then\napplication of them to produce a speech-to-text system using Deep Recurrent\nNeural Networks.", "published": "2018-08-09 14:21:35", "link": "http://arxiv.org/abs/1808.04459v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentimental Content Analysis and Knowledge Extraction from News Articles", "abstract": "In web era, since technology has revolutionized mankind life, plenty of data\nand information are published on the Internet each day. For instance, news\nagencies publish news on their websites all over the world. These raw data\ncould be an important resource for knowledge extraction. These shared data\ncontain emotions (i.e., positive, neutral or negative) toward various topics;\ntherefore, sentimental content extraction could be a beneficial task in many\naspects. Extracting the sentiment of news illustrates highly valuable\ninformation about the events over a period of time, the viewpoint of a media or\nnews agency to these events. In this paper an attempt is made to propose an\napproach for news analysis and extracting useful knowledge from them. Firstly,\nwe attempt to extract a noise robust sentiment of news documents; therefore,\nthe news associated to six countries: United State, United Kingdom, Germany,\nCanada, France and Australia in 5 different news categories: Politics, Sports,\nBusiness, Entertainment and Technology are downloaded. In this paper we compare\nthe condition of different countries in each 5 news topics based on the\nextracted sentiments and emotional contents in news documents. Moreover, we\npropose an approach to reduce the bulky news data to extract the hottest topics\nand news titles as a knowledge. Eventually, we generate a word model to map\neach word to a fixed-size vector by Word2Vec in order to understand the\nrelations between words in our collected news database.", "published": "2018-08-09 05:42:49", "link": "http://arxiv.org/abs/1808.03027v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Image Inspired Poetry Generation in XiaoIce", "abstract": "Vision is a common source of inspiration for poetry. The objects and the\nsentimental imprints that one perceives from an image may lead to various\nfeelings depending on the reader. In this paper, we present a system of poetry\ngeneration from images to mimic the process. Given an image, we first extract a\nfew keywords representing objects and sentiments perceived from the image.\nThese keywords are then expanded to related ones based on their associations in\nhuman written poems. Finally, verses are generated gradually from the keywords\nusing recurrent neural networks trained on existing poems. Our approach is\nevaluated by human assessors and compared to other generation baselines. The\nresults show that our method can generate poems that are more artistic than the\nbaseline methods. This is one of the few attempts to generate poetry from\nimages. By deploying our proposed approach, XiaoIce has already generated more\nthan 12 million poems for users since its release in July 2017. A book of its\npoems has been published by Cheers Publishing, which claimed that the book is\nthe first-ever poetry collection written by an AI in human history.", "published": "2018-08-09 11:17:38", "link": "http://arxiv.org/abs/1808.03090v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Fast Flexible Function Dispatch in Julia", "abstract": "Technical computing is a challenging application area for programming\nlanguages to address. This is evinced by the unusually large number of\nspecialized languages in the area (e.g. MATLAB, R), and the complexity of\ncommon software stacks, often involving multiple languages and custom code\ngenerators. We believe this is ultimately due to key characteristics of the\ndomain: highly complex operators, a need for extensive code specialization for\nperformance, and a desire for permissive high-level programming styles allowing\nproductive experimentation. The Julia language attempts to provide a more\neffective structure for this kind of programming by allowing programmers to\nexpress complex polymorphic behaviors using dynamic multiple dispatch over\nparametric types. The forms of extension and reuse permitted by this paradigm\nhave proven valuable for technical computing. We report on how this approach\nhas allowed domain experts to express useful abstractions while simultaneously\nproviding a natural path to better performance for high-level technical code.", "published": "2018-08-09 23:09:16", "link": "http://arxiv.org/abs/1808.03370v1", "categories": ["cs.PL", "cs.CL", "cs.MS", "68N15", "D.3.3; G.4"], "primary_category": "cs.PL"}
{"title": "Character-Level Language Modeling with Deeper Self-Attention", "abstract": "LSTMs and other RNN variants have shown strong performance on character-level\nlanguage modeling. These models are typically trained using truncated\nbackpropagation through time, and it is common to assume that their success\nstems from their ability to remember long-term contexts. In this paper, we show\nthat a deep (64-layer) transformer model with fixed context outperforms RNN\nvariants by a large margin, achieving state of the art on two popular\nbenchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good\nresults at this depth, we show that it is important to add auxiliary losses,\nboth at intermediate network layers and intermediate sequence positions.", "published": "2018-08-09 18:44:38", "link": "http://arxiv.org/abs/1808.04444v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Rhythm-Flexible Voice Conversion without Parallel Data Using Cycle-GAN\n  over Phoneme Posteriorgram Sequences", "abstract": "Speaking rate refers to the average number of phonemes within some unit time,\nwhile the rhythmic patterns refer to duration distributions for realizations of\ndifferent phonemes within different phonetic structures. Both are key\ncomponents of prosody in speech, which is different for different speakers.\nModels like cycle-consistent adversarial network (Cycle-GAN) and variational\nauto-encoder (VAE) have been successfully applied to voice conversion tasks\nwithout parallel data. However, due to the neural network architectures and\nfeature vectors chosen for these approaches, the length of the predicted\nutterance has to be fixed to that of the input utterance, which limits the\nflexibility in mimicking the speaking rates and rhythmic patterns for the\ntarget speaker. On the other hand, sequence-to-sequence learning model was used\nto remove the above length constraint, but parallel training data are needed.\nIn this paper, we propose an approach utilizing sequence-to-sequence model\ntrained with unsupervised Cycle-GAN to perform the transformation between the\nphoneme posteriorgram sequences for different speakers. In this way, the length\nconstraint mentioned above is removed to offer rhythm-flexible voice conversion\nwithout requiring parallel data. Preliminary evaluation on two datasets showed\nvery encouraging results.", "published": "2018-08-09 12:32:23", "link": "http://arxiv.org/abs/1808.03113v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
