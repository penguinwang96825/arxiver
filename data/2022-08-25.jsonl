{"title": "A logical theory for conditional weak ontic necessity based on context\n  update", "abstract": "Weak ontic necessity is the ontic necessity expressed by ``should'' or\n``ought to'' in English. An example of it is ``I should be dead by now''. A\nfeature of this necessity is whether it holds does not have anything to do with\nwhether its prejacent holds. In this paper, we present a logical theory for\nconditional weak ontic necessity based on context update. A context is a set of\nordered defaults, determining expected possible states of the present world.\nSentences are evaluated with respect to contexts. When evaluating the\nconditional weak ontic necessity with respect to a context, we first update the\ncontext with the antecedent, then check whether the consequent holds with\nrespect to the updated context. The logic is complete. Our theory combines\npremise semantics and update semantics for conditionals.", "published": "2022-08-25 07:57:44", "link": "http://arxiv.org/abs/2208.11917v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A logical theory for strong and weak ontic necessities in branching time", "abstract": "Ontic necessities are those modalities universally quantifying over domains\nof ontic possibilities, whose ``existence'' is independent of our knowledge. An\nontic necessity, called the weak ontic necessity, causes interesting questions.\nAn example for it is ``I should be dead by now''. A feature of this necessity\nis whether it holds at a state has nothing to do with whether its prejacent\nholds at the state. Is there a weak epistemic necessity expressed by\n``should''? Is there a strong ontic necessity expressed by ``must''? How do we\nmake sense of the strong and weak ontic necessities formally? In this paper, we\ndo the following work. Firstly, we recognize strong/weak ontic/epistemic\nnecessities and give our general ideas about them. Secondly, we present a\ncomplete logical theory for the strong and weak ontic necessities in branching\ntime. This theory is based on the following approach. The weak ontic necessity\nquantifies over a domain of expected timelines, determined by the agent's\nsystem of ontic rules. The strong ontic necessity quantifies over a domain of\naccepted timelines, determined by undefeatable ontic rules.", "published": "2022-08-25 08:06:44", "link": "http://arxiv.org/abs/2208.11922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for\n  Natural Language Processing Tasks", "abstract": "Indigenous African languages are categorized as under-served in Natural\nLanguage Processing. They therefore experience poor digital inclusivity and\ninformation access. The processing challenge with such languages has been how\nto use machine learning and deep learning models without the requisite data.\nThe Kencorpus project intends to bridge this gap by collecting and storing text\nand speech data that is good enough for data-driven solutions in applications\nsuch as machine translation, question answering and transcription in\nmultilingual communities. The Kencorpus dataset is a text and speech corpus for\nthree languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data\ncollection was done by researchers from communities, schools, media, and\npublishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442\ntexts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of\nSpeech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively)\nwere developed. We developed 7,537 Question-Answer pairs for Swahili and\ncreated a text translation set of 13,400 sentences from Dholuo and Luhya into\nSwahili. The datasets are useful for downstream machine learning tasks such as\nmodel training and translation. We also developed two proof of concept systems:\nfor Kiswahili speech-to-text and machine learning system for Question Answering\ntask, with results of 18.87% word error rate and 80% Exact Match (EM)\nrespectively. These initial results give great promise to the usability of\nKencorpus to the machine learning community. Kencorpus is one of few public\ndomain corpora for these three low resource languages and forms a basis of\nlearning and sharing experiences for similar works especially for low resource\nlanguages.", "published": "2022-08-25 13:27:14", "link": "http://arxiv.org/abs/2208.12081v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training a T5 Using Lab-sized Resources", "abstract": "Training large neural language models on large datasets is resource- and\ntime-intensive. These requirements create a barrier to entry, where those with\nfewer resources cannot build competitive models. This paper presents various\ntechniques for making it possible to (a) train a large language model using\nresources that a modest research lab might have, and (b) train it in a\nreasonable amount of time. We provide concrete recommendations for\npractitioners, which we illustrate with a case study: a T5 model for Danish,\nthe first for this language.", "published": "2022-08-25 13:55:16", "link": "http://arxiv.org/abs/2208.12097v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shortcut Learning of Large Language Models in Natural Language\n  Understanding", "abstract": "Large language models (LLMs) have achieved state-of-the-art performance on a\nseries of natural language understanding tasks. However, these LLMs might rely\non dataset bias and artifacts as shortcuts for prediction. This has\nsignificantly affected their generalizability and adversarial robustness. In\nthis paper, we provide a review of recent developments that address the\nshortcut learning and robustness challenge of LLMs. We first introduce the\nconcepts of shortcut learning of language models. We then introduce methods to\nidentify shortcut learning behavior in language models, characterize the\nreasons for shortcut learning, as well as introduce mitigation solutions.\nFinally, we discuss key research challenges and potential research directions\nin order to advance the field of LLMs.", "published": "2022-08-25 03:51:39", "link": "http://arxiv.org/abs/2208.11857v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Deep Learning-based approaches for automatic detection of shell nouns\n  and evaluation on WikiText-2", "abstract": "In some areas, such as Cognitive Linguistics, researchers are still using\ntraditional techniques based on manual rules and patterns. Since the definition\nof shell noun is rather subjective and there are many exceptions, this\ntime-consuming work had to be done by hand in the past when Deep Learning\ntechniques were not mature enough. With the increasing number of networked\nlanguages, these rules are becoming less useful. However, there is a better\nalternative now. With the development of Deep Learning, pre-trained language\nmodels have provided a good technical basis for Natural Language Processing.\nAutomated processes based on Deep Learning approaches are more in line with\nmodern needs. This paper collaborates across borders to propose two Neural\nNetwork models for the automatic detection of shell nouns and experiment on the\nWikiText-2 dataset. The proposed approaches not only allow the entire process\nto be automated, but the precision has reached 94% even on completely unseen\narticles, comparable to that of human annotators. This shows that the\nperformance and generalization ability of the model is good enough to be used\nfor research purposes. Many new nouns are found that fit the definition of\nshell noun very well. All discovered shell nouns as well as pre-trained models\nand code are available on GitHub.", "published": "2022-08-25 04:35:30", "link": "http://arxiv.org/abs/2208.11867v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-Modality Gated Attention Fusion for Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis is an important research task to predict the\nsentiment score based on the different modality data from a specific opinion\nvideo. Many previous pieces of research have proved the significance of\nutilizing the shared and unique information across different modalities.\nHowever, the high-order combined signals from multimodal data would also help\nextract satisfied representations. In this paper, we propose CMGA, a\nCross-Modality Gated Attention fusion model for MSA that tends to make adequate\ninteraction across different modality pairs. CMGA also adds a forget gate to\nfilter the noisy and redundant signals introduced in the interaction procedure.\nWe experiment on two benchmark datasets in MSA, MOSI, and MOSEI, illustrating\nthe performance of CMGA over several baseline models. We also conduct the\nablation study to demonstrate the function of different components inside CMGA.", "published": "2022-08-25 06:48:56", "link": "http://arxiv.org/abs/2208.11893v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Compact Pretraining Approach for Neural Language Models", "abstract": "Domain adaptation for large neural language models (NLMs) is coupled with\nmassive amounts of unstructured data in the pretraining phase. In this study,\nhowever, we show that pretrained NLMs learn in-domain information more\neffectively and faster from a compact subset of the data that focuses on the\nkey information in the domain. We construct these compact subsets from the\nunstructured data using a combination of abstractive summaries and extractive\nkeywords. In particular, we rely on BART to generate abstractive summaries, and\nKeyBERT to extract keywords from these summaries (or the original unstructured\ntext directly). We evaluate our approach using six different settings: three\ndatasets combined with two distinct NLMs. Our results reveal that the\ntask-specific classifiers trained on top of NLMs pretrained using our method\noutperform methods based on traditional pretraining, i.e., random masking on\nthe entire data, as well as methods without pretraining. Further, we show that\nour strategy reduces pretraining time by up to five times compared to vanilla\npretraining. The code for all of our experiments is publicly available at\nhttps://github.com/shahriargolchin/compact-pretraining.", "published": "2022-08-25 22:43:47", "link": "http://arxiv.org/abs/2208.12367v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Reality and the Limits of Language Data: Aligning LLMs with Human\n  Norms", "abstract": "Recent advancements in Large Language Models (LLMs) harness linguistic\nassociations in vast natural language data for practical applications. However,\ntheir ability to understand the physical world using only language data remains\na question. After reviewing existing protocols, we explore this question using\na novel and tightly controlled reasoning test (ART) and compare human norms\nagainst versions of GPT-3. Our findings highlight the categories of\ncommon-sense relations models that could learn directly from data and areas of\nweakness. GPT-3 offers evidence for verbal reasoning on a par with human\nsubjects for several relations including Synonymy, Antonymy, and Default\ninheritance, Without reinforcement learning from human judgements, it appears\nGPT-3 performs at the lower end of the reference interval for Has-part and\nContained-in. Weaknesses were observed also in affordance characteristics\nthrough Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs\nwith symbolic world grounding is a promising direction to address associative\nlearning.", "published": "2022-08-25 10:21:23", "link": "http://arxiv.org/abs/2208.11981v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automatic Mapping of Unstructured Cyber Threat Intelligence: An\n  Experimental Study", "abstract": "Proactive approaches to security, such as adversary emulation, leverage\ninformation about threat actors and their techniques (Cyber Threat\nIntelligence, CTI). However, most CTI still comes in unstructured forms (i.e.,\nnatural language), such as incident reports and leaked documents. To support\nproactive security efforts, we present an experimental study on the automatic\nclassification of unstructured CTI into attack techniques using machine\nlearning (ML). We contribute with two new datasets for CTI analysis, and we\nevaluate several ML models, including both traditional and deep learning-based\nones. We present several lessons learned about how ML can perform at this task,\nwhich classifiers perform best and under which conditions, which are the main\ncauses of classification errors, and the challenges ahead for CTI analysis.", "published": "2022-08-25 15:01:42", "link": "http://arxiv.org/abs/2208.12144v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Contrastive Audio-Language Learning for Music", "abstract": "As one of the most intuitive interfaces known to humans, natural language has\nthe potential to mediate many tasks that involve human-computer interaction,\nespecially in application-focused fields like Music Information Retrieval. In\nthis work, we explore cross-modal learning in an attempt to bridge audio and\nlanguage in the music domain. To this end, we propose MusCALL, a framework for\nMusic Contrastive Audio-Language Learning. Our approach consists of a\ndual-encoder architecture that learns the alignment between pairs of music\naudio and descriptive sentences, producing multimodal embeddings that can be\nused for text-to-audio and audio-to-text retrieval out-of-the-box. Thanks to\nthis property, MusCALL can be transferred to virtually any task that can be\ncast as text-based retrieval. Our experiments show that our method performs\nsignificantly better than the baselines at retrieving audio that matches a\ntextual description and, conversely, text that matches an audio query. We also\ndemonstrate that the multimodal alignment capability of our model can be\nsuccessfully extended to the zero-shot transfer scenario for genre\nclassification and auto-tagging on two public datasets.", "published": "2022-08-25 16:55:15", "link": "http://arxiv.org/abs/2208.12208v1", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multimedia Generative Script Learning for Task Planning", "abstract": "Goal-oriented generative script learning aims to generate subsequent steps to\nreach a particular goal, which is an essential task to assist robots or humans\nin performing stereotypical activities. An important aspect of this process is\nthe ability to capture historical states visually, which provides detailed\ninformation that is not covered by text and will guide subsequent steps.\nTherefore, we propose a new task, Multimedia Generative Script Learning, to\ngenerate subsequent steps by tracking historical states in both text and vision\nmodalities, as well as presenting the first benchmark containing 5,652 tasks\nand 79,089 multimedia steps. This task is challenging in three aspects: the\nmultimedia challenge of capturing the visual states in images, the induction\nchallenge of performing unseen tasks, and the diversity challenge of covering\ndifferent information in individual steps. We propose to encode visual state\nchanges through a selective multimedia encoder to address the multimedia\nchallenge, transfer knowledge from previously observed tasks using a\nretrieval-augmented decoder to overcome the induction challenge, and further\npresent distinct information at each step by optimizing a diversity-oriented\ncontrastive learning objective. We define metrics to evaluate both generation\nand inductive quality. Experiment results demonstrate that our approach\nsignificantly outperforms strong baselines.", "published": "2022-08-25 19:04:28", "link": "http://arxiv.org/abs/2208.12306v3", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Digital Audio Tampering Detection Based on ENF Spatio-temporal Features\n  Representation Learning", "abstract": "Most digital audio tampering detection methods based on electrical network\nfrequency (ENF) only utilize the static spatial information of ENF, ignoring\nthe variation of ENF in time series, which limit the ability of ENF feature\nrepresentation and reduce the accuracy of tampering detection. This paper\nproposes a new method for digital audio tampering detection based on ENF\nspatio-temporal features representation learning. A parallel spatio-temporal\nnetwork model is constructed using CNN and BiLSTM, which deeply extracts ENF\nspatial feature information and ENF temporal feature information to enhance the\nfeature representation capability to improve the tampering detection accuracy.\nIn order to extract the spatial and temporal features of the ENF, this paper\nfirstly uses digital audio high-precision Discrete Fourier Transform analysis\nto extract the phase sequences of the ENF. The unequal phase series is divided\ninto frames by adaptive frame shifting to obtain feature matrices of the same\nsize to represent the spatial features of the ENF. At the same time, the phase\nsequences are divided into frames based on ENF time changes information to\nrepresent the temporal features of the ENF. Then deep spatial and temporal\nfeatures are further extracted using CNN and BiLSTM respectively, and an\nattention mechanism is used to adaptively assign weights to the deep spatial\nand temporal features to obtain spatio-temporal features with stronger\nrepresentation capability. Finally, the deep neural network is used to\ndetermine whether the audio has been tampered with. The experimental results\nshow that the proposed method improves the accuracy by 2.12%-7.12% compared\nwith state-of-the-art methods under the public database Carioca, New Spanish.", "published": "2022-08-25 08:01:02", "link": "http://arxiv.org/abs/2208.11920v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Interpretable Multimodal Emotion Recognition using Hybrid Fusion of\n  Speech and Image Data", "abstract": "This paper proposes a multimodal emotion recognition system based on hybrid\nfusion that classifies the emotions depicted by speech utterances and\ncorresponding images into discrete classes. A new interpretability technique\nhas been developed to identify the important speech & image features leading to\nthe prediction of particular emotion classes. The proposed system's\narchitecture has been determined through intensive ablation studies. It fuses\nthe speech & image features and then combines speech, image, and intermediate\nfusion outputs. The proposed interpretability technique incorporates the divide\n& conquer approach to compute shapely values denoting each speech & image\nfeature's importance. We have also constructed a large-scale dataset (IIT-R\nSIER dataset), consisting of speech utterances, corresponding images, and class\nlabels, i.e., 'anger,' 'happy,' 'hate,' and 'sad.' The proposed system has\nachieved 83.29% accuracy for emotion recognition. The enhanced performance of\nthe proposed system advocates the importance of utilizing complementary\ninformation from multiple modalities for emotion recognition.", "published": "2022-08-25 04:43:34", "link": "http://arxiv.org/abs/2208.11868v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Decoding speech perception from non-invasive brain recordings", "abstract": "Decoding speech from brain activity is a long-awaited goal in both healthcare\nand neuroscience. Invasive devices have recently led to major milestones in\nthat regard: deep learning algorithms trained on intracranial recordings now\nstart to decode elementary linguistic features (e.g. letters, words,\nspectrograms). However, extending this approach to natural speech and\nnon-invasive brain recordings remains a major challenge. Here, we introduce a\nmodel trained with contrastive-learning to decode self-supervised\nrepresentations of perceived speech from the non-invasive recordings of a large\ncohort of healthy individuals. To evaluate this approach, we curate and\nintegrate four public datasets, encompassing 175 volunteers recorded with\nmagneto- or electro-encephalography (M/EEG), while they listened to short\nstories and isolated sentences. The results show that our model can identify,\nfrom 3 seconds of MEG signals, the corresponding speech segment with up to 41%\naccuracy out of more than 1,000 distinct possibilities on average across\nparticipants, and more than 80% in the very best participants - a performance\nthat allows the decoding of words and phrases absent from the training set. The\ncomparison of our model to a variety of baselines highlights the importance of\n(i) a contrastive objective, (ii) pretrained representations of speech and\n(iii) a common convolutional architecture simultaneously trained across\nmultiple participants. Finally, the analysis of the decoder's predictions\nsuggests that they primarily depend on lexical and contextual semantic\nrepresentations. Overall, this effective decoding of perceived speech from\nnon-invasive recordings delineates a promising path to decode language from\nbrain activity, without putting patients at risk for brain surgery.", "published": "2022-08-25 10:01:43", "link": "http://arxiv.org/abs/2208.12266v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "q-bio.NC"], "primary_category": "eess.AS"}
{"title": "Spatio-Temporal Representation Learning Enhanced Source Cell-phone\n  Recognition from Speech Recordings", "abstract": "The existing source cell-phone recognition method lacks the long-term feature\ncharacterization of the source device, resulting in inaccurate representation\nof the source cell-phone related features which leads to insufficient\nrecognition accuracy. In this paper, we propose a source cell-phone recognition\nmethod based on spatio-temporal representation learning, which includes two\nmain parts: extraction of sequential Gaussian mean matrix features and\nconstruction of a recognition model based on spatio-temporal representation\nlearning. In the feature extraction part, based on the analysis of time-series\nrepresentation of recording source signals, we extract sequential Gaussian mean\nmatrix with long-term and short-term representation ability by using the\nsensitivity of Gaussian mixture model to data distribution. In the model\nconstruction part, we design a structured spatio-temporal representation\nlearning network C3D-BiLSTM to fully characterize the spatio-temporal\ninformation, combine 3D convolutional network and bidirectional long short-term\nmemory network for short-term spectral information and long-time fluctuation\ninformation representation learning, and achieve accurate recognition of\ncell-phones by fusing spatio-temporal feature information of recording source\nsignals. The method achieves an average accuracy of 99.03% for the closed-set\nrecognition of 45 cell-phones under the CCNU\\_Mobile dataset, and 98.18% in\nsmall sample size experiments, with recognition performance better than the\nexisting state-of-the-art methods. The experimental results show that the\nmethod exhibits excellent recognition performance in multi-class cell-phones\nrecognition.", "published": "2022-08-25 07:47:41", "link": "http://arxiv.org/abs/2208.12753v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Study on Broadcast Networks for Music Genre Classification", "abstract": "Due to the increased demand for music streaming/recommender services and the\nrecent developments of music information retrieval frameworks, Music Genre\nClassification (MGC) has attracted the community's attention. However,\nconvolutional-based approaches are known to lack the ability to efficiently\nencode and localize temporal features. In this paper, we study the\nbroadcast-based neural networks aiming to improve the localization and\ngeneralizability under a small set of parameters (about 180k) and investigate\ntwelve variants of broadcast networks discussing the effect of block\nconfiguration, pooling method, activation function, normalization mechanism,\nlabel smoothing, channel interdependency, LSTM block inclusion, and variants of\ninception schemes. Our computational experiments using relevant datasets such\nas GTZAN, Extended Ballroom, HOMBURG, and Free Music Archive (FMA) show\nstate-of-the-art classification accuracies in Music Genre Classification. Our\napproach offers insights and the potential to enable compact and generalizable\nbroadcast networks for music and audio classification.", "published": "2022-08-25 13:36:43", "link": "http://arxiv.org/abs/2208.12086v1", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "The ReprGesture entry to the GENEA Challenge 2022", "abstract": "This paper describes the ReprGesture entry to the Generation and Evaluation\nof Non-verbal Behaviour for Embodied Agents (GENEA) challenge 2022. The GENEA\nchallenge provides the processed datasets and performs crowdsourced evaluations\nto compare the performance of different gesture generation systems. In this\npaper, we explore an automatic gesture generation system based on multimodal\nrepresentation learning. We use WavLM features for audio, FastText features for\ntext and position and rotation matrix features for gesture. Each modality is\nprojected to two distinct subspaces: modality-invariant and modality-specific.\nTo learn inter-modality-invariant commonalities and capture the characters of\nmodality-specific representations, gradient reversal layer based adversarial\nclassifier and modality reconstruction decoders are used during training. The\ngesture decoder generates proper gestures using all representations and\nfeatures related to the rhythm in the audio. Our code, pre-trained models and\ndemo are available at https://github.com/YoungSeng/ReprGesture.", "published": "2022-08-25 14:50:50", "link": "http://arxiv.org/abs/2208.12133v1", "categories": ["cs.HC", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.HC"}
