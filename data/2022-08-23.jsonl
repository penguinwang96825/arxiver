{"title": "Few-Shot Table-to-Text Generation with Prefix-Controlled Generator", "abstract": "Neural table-to-text generation approaches are data-hungry, limiting their\nadaptation for low-resource real-world applications. Previous works mostly\nresort to Pre-trained Language Models (PLMs) to generate fluent summaries of a\ntable. However, they often contain hallucinated contents due to the\nuncontrolled nature of PLMs. Moreover, the topological differences between\ntables and sequences are rarely studied. Last but not least, fine-tuning on\nPLMs with a handful of instances may lead to over-fitting and catastrophic\nforgetting. To alleviate these problems, we propose a prompt-based approach,\nPrefix-Controlled Generator (i.e., PCG), for few-shot table-to-text generation.\nWe prepend a task-specific prefix for a PLM to make the table structure better\nfit the pre-trained input. In addition, we generate an input-specific prefix to\ncontrol the factual contents and word order of the generated text. Both\nautomatic and human evaluations on different domains (humans, books and songs)\nof the Wikibio dataset show substantial improvements over baseline approaches.", "published": "2022-08-23 03:23:26", "link": "http://arxiv.org/abs/2208.10709v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Dynamic Contextualised Word Embeddings via Template-based\n  Temporal Adaptation", "abstract": "Dynamic contextualised word embeddings (DCWEs) represent the temporal\nsemantic variations of words. We propose a method for learning DCWEs by\ntime-adapting a pretrained Masked Language Model (MLM) using time-sensitive\ntemplates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively\nat two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised\nmethod to select (a) \\emph{pivot} terms related to both $C_1$ and $C_2$, and\n(b) \\emph{anchor} terms that are associated with a specific pivot term in each\nindividual snapshot. We then generate prompts by filling manually compiled\ntemplates using the extracted pivot and anchor terms. Moreover, we propose an\nautomatic method to learn time-sensitive templates from $C_1$ and $C_2$,\nwithout requiring any human supervision. Next, we use the generated prompts to\nadapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple\nexperiments show that our proposed method reduces the perplexity of test\nsentences in $C_2$, outperforming the current state-of-the-art.", "published": "2022-08-23 05:12:27", "link": "http://arxiv.org/abs/2208.10734v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational valency lexica and Homeric formularity", "abstract": "Distributional semantics, the quantitative study of meaning variation and\nchange through corpus collocations, is currently one of the most productive\nresearch areas in computational linguistics. The wider availability of big data\nand of reproducible algorithms for analysis has boosted its application to\nliving languages in recent years. But can we use distributional semantics to\nstudy a language with such a limited corpus as ancient Greek? And can this\napproach tell us something about such vexed questions in classical studies as\nthe language and composition of the Homeric poems? Our paper will compare the\nsemantic flexibility of formulae involving transitive verbs in archaic Greek\nepic to similar verb phrases in a non-formulaic corpus, in order to detect\nunique patterns of variation in formulae. To address this, we present AGVaLex,\na computational valency lexicon for ancient Greek automatically extracted from\nthe Ancient Greek Dependency Treebank. The lexicon contains quantitative\ncorpus-driven morphological, syntactic and lexical information about verbs and\ntheir arguments, such as objects, subjects, and prepositional phrases, and has\na wide range of applications for the study of the language of ancient Greek\nauthors.", "published": "2022-08-23 08:03:16", "link": "http://arxiv.org/abs/2208.10795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MATra: A Multilingual Attentive Transliteration System for Indian\n  Scripts", "abstract": "Transliteration is a task in the domain of NLP where the output word is a\nsimilar-sounding word written using the letters of any foreign language. Today\nthis system has been developed for several language pairs that involve English\nas either the source or target word and deployed in several places like Google\nTranslate and chatbots. However, there is very little research done in the\nfield of Indic languages transliterated to other Indic languages. This paper\ndemonstrates a multilingual model based on transformers (with some\nmodifications) that can give noticeably higher performance and accuracy than\nall existing models in this domain and get much better results than\nstate-of-the-art models. This paper shows a model that can perform\ntransliteration between any pair among the following five languages - English,\nHindi, Bengali, Kannada and Tamil. It is applicable in scenarios where language\nis a barrier to communication in any written task. The model beats the\nstate-of-the-art (for all pairs among the five mentioned languages - English,\nHindi, Bengali, Kannada, and Tamil) and achieves a top-1 accuracy score of\n80.7%, about 29.5% higher than the best current results. Furthermore, the model\nachieves 93.5% in terms of Phonetic Accuracy (transliteration is primarily a\nphonetic/sound-based task).", "published": "2022-08-23 08:14:29", "link": "http://arxiv.org/abs/2208.10801v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Better Masking for Better Language Model Pre-training", "abstract": "Masked Language Modeling (MLM) has been widely used as the denoising\nobjective in pre-training language models (PrLMs). Existing PrLMs commonly\nadopt a Random-Token Masking strategy where a fixed masking ratio is applied\nand different contents are masked by an equal probability throughout the entire\ntraining. However, the model may receive complicated impact from pre-training\nstatus, which changes accordingly as training time goes on. In this paper, we\nshow that such time-invariant MLM settings on masking ratio and masked content\nare unlikely to deliver an optimal outcome, which motivates us to explore the\ninfluence of time-variant MLM settings. We propose two scheduled masking\napproaches that adaptively tune the masking ratio and masked content in\ndifferent training stages, which improves the pre-training efficiency and\neffectiveness verified on the downstream tasks. Our work is a pioneer study on\ntime-variant masking strategy on ratio and content and gives a better\nunderstanding of how masking ratio and masked content influence the MLM\npre-training.", "published": "2022-08-23 08:27:52", "link": "http://arxiv.org/abs/2208.10806v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Question Answering via Answer Diversifying", "abstract": "Unsupervised question answering is an attractive task due to its independence\non labeled data. Previous works usually make use of heuristic rules as well as\npre-trained models to construct data and train QA models. However, most of\nthese works regard named entity (NE) as the only answer type, which ignores the\nhigh diversity of answers in the real world. To tackle this problem, we propose\na novel unsupervised method by diversifying answers, named DiverseQA.\nSpecifically, the proposed method is composed of three modules: data\nconstruction, data augmentation and denoising filter. Firstly, the data\nconstruction module extends the extracted named entity into a longer sentence\nconstituent as the new answer span to construct a QA dataset with diverse\nanswers. Secondly, the data augmentation module adopts an answer-type dependent\ndata augmentation process via adversarial training in the embedding level.\nThirdly, the denoising filter module is designed to alleviate the noise in the\nconstructed data. Extensive experiments show that the proposed method\noutperforms previous unsupervised models on five benchmark datasets, including\nSQuADv1.1, NewsQA, TriviaQA, BioASQ, and DuoRC. Besides, the proposed method\nshows strong performance in the few-shot learning setting.", "published": "2022-08-23 08:57:00", "link": "http://arxiv.org/abs/2208.10813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Personality Consistency in Conversation by Persona Extending", "abstract": "Endowing chatbots with a consistent personality plays a vital role for agents\nto deliver human-like interactions. However, existing personalized approaches\ncommonly generate responses in light of static predefined personas depicted\nwith textual description, which may severely restrict the interactivity of\nhuman and the chatbot, especially when the agent needs to answer the query\nexcluded in the predefined personas, which is so-called out-of-predefined\npersona problem (named OOP for simplicity). To alleviate the problem, in this\npaper we propose a novel retrieval-to-prediction paradigm consisting of two\nsubcomponents, namely, (1) Persona Retrieval Model (PRM), it retrieves a\npersona from a global collection based on a Natural Language Inference (NLI)\nmodel, the inferred persona is consistent with the predefined personas; and (2)\nPosterior-scored Transformer (PS-Transformer), it adopts a persona posterior\ndistribution that further considers the actual personas used in the ground\nresponse, maximally mitigating the gap between training and inferring.\nFurthermore, we present a dataset called IT-ConvAI2 that first highlights the\nOOP problem in personalized dialogue. Extensive experiments on both IT-ConvAI2\nand ConvAI2 demonstrate that our proposed model yields considerable\nimprovements in both automatic metrics and human evaluations.", "published": "2022-08-23 09:00:58", "link": "http://arxiv.org/abs/2208.10816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GenTUS: Simulating User Behaviour and Language in Task-oriented\n  Dialogues with Generative Transformers", "abstract": "User simulators (USs) are commonly used to train task-oriented dialogue\nsystems (DSs) via reinforcement learning. The interactions often take place on\nsemantic level for efficiency, but there is still a gap from semantic actions\nto natural language, which causes a mismatch between training and deployment\nenvironment. Incorporating a natural language generation (NLG) module with USs\nduring training can partly deal with this problem. However, since the policy\nand NLG of USs are optimised separately, these simulated user utterances may\nnot be natural enough in a given context. In this work, we propose a generative\ntransformer-based user simulator (GenTUS). GenTUS consists of an\nencoder-decoder structure, which means it can optimise both the user policy and\nnatural language generation jointly. GenTUS generates both semantic actions and\nnatural language utterances, preserving interpretability and enhancing language\nvariation. In addition, by representing the inputs and outputs as word\nsequences and by using a large pre-trained language model we can achieve\ngeneralisability in feature representation. We evaluate GenTUS with automatic\nmetrics and human evaluation. Our results show that GenTUS generates more\nnatural language and is able to transfer to an unseen ontology in a zero-shot\nfashion. In addition, its behaviour can be further shaped with reinforcement\nlearning opening the door to training specialised user simulators.", "published": "2022-08-23 09:01:17", "link": "http://arxiv.org/abs/2208.10817v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense\n  Reasoning", "abstract": "Commonsense reasoning is an appealing topic in natural language processing\n(NLP) as it plays a fundamental role in supporting the human-like actions of\nNLP systems. With large-scale language models as the backbone, unsupervised\npre-training on numerous corpora shows the potential to capture commonsense\nknowledge. Current pre-trained language model (PLM)-based reasoning follows the\ntraditional practice using perplexity metric. However, commonsense reasoning is\nmore than existing probability evaluation, which is biased by word frequency.\nThis paper reconsiders the nature of commonsense reasoning and proposes a novel\ncommonsense reasoning metric, Non-Replacement Confidence (NRC). In detail, it\nworks on PLMs according to the Replaced Token Detection (RTD) pre-training\nobjective in ELECTRA, in which the corruption detection objective reflects the\nconfidence on contextual integrity that is more relevant to commonsense\nreasoning than existing probability. Our proposed novel method boosts zero-shot\nperformance on two commonsense reasoning benchmark datasets and further seven\ncommonsense question-answering datasets. Our analysis shows that pre-endowed\ncommonsense knowledge, especially for RTD-based PLMs, is essential in\ndownstream reasoning.", "published": "2022-08-23 14:42:14", "link": "http://arxiv.org/abs/2208.11007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Query-Response Interactions by Multi-tasks in Semantic Search for\n  Chatbot Candidate Retrieval", "abstract": "Semantic search for candidate retrieval is an important yet neglected problem\nin retrieval-based Chatbots, which aims to select a bunch of candidate\nresponses efficiently from a large pool. The existing bottleneck is to ensure\nthe model architecture having two points: 1) rich interactions between a query\nand a response to produce query-relevant responses; 2) ability of separately\nprojecting the query and the response into latent spaces to apply efficiently\nin semantic search during online inference. To tackle this problem, we propose\na novel approach, called Multitask-based Semantic Search Neural Network (MSSNN)\nfor candidate retrieval, which accomplishes query-response interactions through\nmulti-tasks. The method employs a Seq2Seq modeling task to learn a good query\nencoder, and then performs a word prediction task to build response embeddings,\nfinally conducts a simple matching model to form the dot-product scorer.\nExperimental studies have demonstrated the potential of the proposed approach.", "published": "2022-08-23 15:07:35", "link": "http://arxiv.org/abs/2208.11018v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bitext Mining for Low-Resource Languages via Contrastive Learning", "abstract": "Mining high-quality bitexts for low-resource languages is challenging. This\npaper shows that sentence representation of language models fine-tuned with\nmultiple negatives ranking loss, a contrastive objective, helps retrieve clean\nbitexts. Experiments show that parallel data mined from our approach\nsubstantially outperform the previous state-of-the-art method on low resource\nlanguages Khmer and Pashto.", "published": "2022-08-23 21:07:42", "link": "http://arxiv.org/abs/2208.11194v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "K-MHaS: A Multi-label Hate Speech Detection Dataset in Korean Online\n  News Comment", "abstract": "Online hate speech detection has become an important issue due to the growth\nof online content, but resources in languages other than English are extremely\nlimited. We introduce K-MHaS, a new multi-label dataset for hate speech\ndetection that effectively handles Korean language patterns. The dataset\nconsists of 109k utterances from news comments and provides a multi-label\nclassification using 1 to 4 labels, and handles subjectivity and\nintersectionality. We evaluate strong baseline experiments on K-MHaS using\nKorean-BERT-based language models with six different metrics. KR-BERT with a\nsub-character tokenizer outperforms others, recognizing decomposed characters\nin each hate speech class.", "published": "2022-08-23 02:10:53", "link": "http://arxiv.org/abs/2208.10684v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "We Are in This Together: Quantifying Community Subjective Wellbeing and\n  Resilience", "abstract": "The COVID-19 pandemic disrupted everyone's life across the world. In this\nwork, we characterize the subjective wellbeing patterns of 112 cities across\nthe United States during the pandemic prior to vaccine availability, as\nexhibited in subreddits corresponding to the cities. We quantify subjective\nwellbeing using positive and negative affect. We then measure the pandemic's\nimpact by comparing a community's observed wellbeing with its expected\nwellbeing, as forecasted by time series models derived from prior to the\npandemic.We show that general community traits reflected in language can be\npredictive of community resilience. We predict how the pandemic would impact\nthe wellbeing of each community based on linguistic and interaction features\nfrom normal times \\textit{before} the pandemic. We find that communities with\ninteraction characteristics corresponding to more closely connected users and\nhigher engagement were less likely to be significantly impacted. Notably, we\nfind that communities that talked more about social ties normally experienced\nin-person, such as friends, family, and affiliations, were actually more likely\nto be impacted. Additionally, we use the same features to also predict how\nquickly each community would recover after the initial onset of the pandemic.\nWe similarly find that communities that talked more about family, affiliations,\nand identifying as part of a group had a slower recovery.", "published": "2022-08-23 06:57:05", "link": "http://arxiv.org/abs/2208.10766v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Multimodal Crop Type Classification Fusing Multi-Spectral Satellite Time\n  Series with Farmers Crop Rotations and Local Crop Distribution", "abstract": "Accurate, detailed, and timely crop type mapping is a very valuable\ninformation for the institutions in order to create more accurate policies\naccording to the needs of the citizens. In the last decade, the amount of\navailable data dramatically increased, whether it can come from Remote Sensing\n(using Copernicus Sentinel-2 data) or directly from the farmers (providing\nin-situ crop information throughout the years and information on crop\nrotation). Nevertheless, the majority of the studies are restricted to the use\nof one modality (Remote Sensing data or crop rotation) and never fuse the Earth\nObservation data with domain knowledge like crop rotations. Moreover, when they\nuse Earth Observation data they are mainly restrained to one year of data, not\ntaking into account the past years. In this context, we propose to tackle a\nland use and crop type classification task using three data types, by using a\nHierarchical Deep Learning algorithm modeling the crop rotations like a\nlanguage model, the satellite signals like a speech signal and using the crop\ndistribution as additional context vector. We obtained very promising results\ncompared to classical approaches with significant performances, increasing the\nAccuracy by 5.1 points in a 28-class setting (.948), and the micro-F1 by 9.6\npoints in a 10-class setting (.887) using only a set of crop of interests\nselected by an expert. We finally proposed a data-augmentation technique to\nallow the model to classify the crop before the end of the season, which works\nsurprisingly well in a multimodal setting.", "published": "2022-08-23 09:41:09", "link": "http://arxiv.org/abs/2208.10838v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "CLOWER: A Pre-trained Language Model with Contrastive Learning over Word\n  and Character Representations", "abstract": "Pre-trained Language Models (PLMs) have achieved remarkable performance gains\nacross numerous downstream tasks in natural language understanding. Various\nChinese PLMs have been successively proposed for learning better Chinese\nlanguage representation. However, most current models use Chinese characters as\ninputs and are not able to encode semantic information contained in Chinese\nwords. While recent pre-trained models incorporate both words and characters\nsimultaneously, they usually suffer from deficient semantic interactions and\nfail to capture the semantic relation between words and characters. To address\nthe above issues, we propose a simple yet effective PLM CLOWER, which adopts\nthe Contrastive Learning Over Word and charactER representations. In\nparticular, CLOWER implicitly encodes the coarse-grained information (i.e.,\nwords) into the fine-grained representations (i.e., characters) through\ncontrastive learning on multi-grained information. CLOWER is of great value in\nrealistic scenarios since it can be easily incorporated into any existing\nfine-grained based PLMs without modifying the production pipelines.Extensive\nexperiments conducted on a range of downstream tasks demonstrate the superior\nperformance of CLOWER over several state-of-the-art baselines.", "published": "2022-08-23 09:52:34", "link": "http://arxiv.org/abs/2208.10844v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Don't Take it Personally: Analyzing Gender and Age Differences in\n  Ratings of Online Humor", "abstract": "Computational humor detection systems rarely model the subjectivity of humor\nresponses, or consider alternative reactions to humor - namely offense. We\nanalyzed a large dataset of humor and offense ratings by male and female\nannotators of different age groups. We find that women link these two concepts\nmore strongly than men, and they tend to give lower humor ratings and higher\noffense scores. We also find that the correlation between humor and offense\nincreases with age. Although there were no gender or age differences in humor\ndetection, women and older annotators signalled that they did not understand\njoke texts more often than men. We discuss implications for computational humor\ndetection and downstream tasks.", "published": "2022-08-23 12:04:36", "link": "http://arxiv.org/abs/2208.10898v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Flat Multi-modal Interaction Transformer for Named Entity Recognition", "abstract": "Multi-modal named entity recognition (MNER) aims at identifying entity spans\nand recognizing their categories in social media posts with the aid of images.\nHowever, in dominant MNER approaches, the interaction of different modalities\nis usually carried out through the alternation of self-attention and\ncross-attention or over-reliance on the gating machine, which results in\nimprecise and biased correspondence between fine-grained semantic units of text\nand image. To address this issue, we propose a Flat Multi-modal Interaction\nTransformer (FMIT) for MNER. Specifically, we first utilize noun phrases in\nsentences and general domain words to obtain visual cues. Then, we transform\nthe fine-grained semantic representation of the vision and text into a unified\nlattice structure and design a novel relative position encoding to match\ndifferent modalities in Transformer. Meanwhile, we propose to leverage entity\nboundary detection as an auxiliary task to alleviate visual bias. Experiments\nshow that our methods achieve the new state-of-the-art performance on two\nbenchmark datasets.", "published": "2022-08-23 15:25:44", "link": "http://arxiv.org/abs/2208.11039v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Prompting as Probing: Using Language Models for Knowledge Base\n  Construction", "abstract": "Language Models (LMs) have proven to be useful in various downstream\napplications, such as summarisation, translation, question answering and text\nclassification. LMs are becoming increasingly important tools in Artificial\nIntelligence, because of the vast quantity of information they can store. In\nthis work, we present ProP (Prompting as Probing), which utilizes GPT-3, a\nlarge Language Model originally proposed by OpenAI in 2020, to perform the task\nof Knowledge Base Construction (KBC). ProP implements a multi-step approach\nthat combines a variety of prompting techniques to achieve this. Our results\nshow that manual prompt curation is essential, that the LM must be encouraged\nto give answer sets of variable lengths, in particular including empty answer\nsets, that true/false questions are a useful device to increase precision on\nsuggestions generated by the LM, that the size of the LM is a crucial factor,\nand that a dictionary of entity aliases improves the LM score. Our evaluation\nstudy indicates that these proposed techniques can substantially enhance the\nquality of the final predictions: ProP won track 2 of the LM-KBC competition,\noutperforming the baseline by 36.4 percentage points. Our implementation is\navailable on https://github.com/HEmile/iswc-challenge.", "published": "2022-08-23 16:03:50", "link": "http://arxiv.org/abs/2208.11057v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FlexER: Flexible Entity Resolution for Multiple Intents", "abstract": "Entity resolution, a longstanding problem of data cleaning and integration,\naims at identifying data records that represent the same real-world entity.\nExisting approaches treat entity resolution as a universal task, assuming the\nexistence of a single interpretation of a real-world entity and focusing only\non finding matched records, separating corresponding from non-corresponding\nones, with respect to this single interpretation. However, in real-world\nscenarios, where entity resolution is part of a more general data project,\ndownstream applications may have varying interpretations of real-world entities\nrelating, for example, to various user needs. In what follows, we introduce the\nproblem of multiple intents entity resolution (MIER), an extension to the\nuniversal (single intent) entity resolution task. As a solution, we propose\nFlexER, utilizing contemporary solutions to universal entity resolution tasks\nto solve multiple intents entity resolution. FlexER addresses the problem as a\nmulti-label classification problem. It combines intent-based representations of\ntuple pairs using a multiplex graph representation that serves as an input to a\ngraph neural network (GNN). FlexER learns intent representations and improves\nthe outcome to multiple resolution problems. A large-scale empirical evaluation\nintroduces a new benchmark and, using also two well-known benchmarks, shows\nthat FlexER effectively solves the MIER problem and outperforms the\nstate-of-the-art for a universal entity resolution.", "published": "2022-08-23 15:52:52", "link": "http://arxiv.org/abs/2209.07569v2", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Ordinal analysis of lexical patterns", "abstract": "Words are fundamental linguistic units that connect thoughts and things\nthrough meaning. However, words do not appear independently in a text sequence.\nThe existence of syntactic rules induces correlations among neighboring words.\nUsing an ordinal pattern approach, we present an analysis of lexical\nstatistical connections for 11 major languages. We find that the diverse\nmanners that languages utilize to express word relations give rise to unique\npattern structural distributions. Furthermore, fluctuations of these pattern\ndistributions for a given language can allow us to determine both the\nhistorical period when the text was written and its author. Taken together, our\nresults emphasize the relevance of ordinal time series analysis in linguistic\ntypology, historical linguistics and stylometry.", "published": "2022-08-23 20:03:27", "link": "http://arxiv.org/abs/2208.11175v2", "categories": ["cs.CL", "cond-mat.stat-mech", "physics.soc-ph"], "primary_category": "cs.CL"}
{"title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors,\n  and Lessons Learned", "abstract": "We describe our early efforts to red team language models in order to\nsimultaneously discover, measure, and attempt to reduce their potentially\nharmful outputs. We make three main contributions. First, we investigate\nscaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B\nparameters) and 4 model types: a plain language model (LM); an LM prompted to\nbe helpful, honest, and harmless; an LM with rejection sampling; and a model\ntrained to be helpful and harmless using reinforcement learning from human\nfeedback (RLHF). We find that the RLHF models are increasingly difficult to red\nteam as they scale, and we find a flat trend with scale for the other model\ntypes. Second, we release our dataset of 38,961 red team attacks for others to\nanalyze and learn from. We provide our own analysis of the data and find a\nvariety of harmful outputs, which range from offensive language to more subtly\nharmful non-violent unethical outputs. Third, we exhaustively describe our\ninstructions, processes, statistical methodologies, and uncertainty about red\nteaming. We hope that this transparency accelerates our ability to work\ntogether as a community in order to develop shared norms, practices, and\ntechnical standards for how to red team language models.", "published": "2022-08-23 23:37:14", "link": "http://arxiv.org/abs/2209.07858v2", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio\n  Textures", "abstract": "Standard evaluation metrics such as the Inception score and Fr\\'echet Audio\nDistance provide a general audio quality distance metric between the\nsynthesized audio and reference clean audio. However, the sensitivity of these\nmetrics to variations in the statistical parameters that define an audio\ntexture is not well studied. In this work, we provide a systematic study of the\nsensitivity of some of the existing audio quality evaluation metrics to\nparameter variations in audio textures. Furthermore, we also study three more\npotentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram\nmatrix based distance, (b) an Accumulated Gram metric using a summarized\nversion of the Gram matrices, and (c) a cochlear-model based statistical\nfeatures metric. These metrics use deep features that summarize the statistics\nof any given audio texture, thus being inherently sensitive to variations in\nthe statistical parameters that define an audio texture. We study and evaluate\nthe sensitivity of existing standard metrics as well as Gram matrix and\ncochlear-model based metrics to control-parameter variations in audio textures\nacross a wide range of texture and parameter types, and validate with\nsubjective evaluation. We find that each of the metrics is sensitive to\ndifferent sets of texture-parameter types. This is the first step towards\ninvestigating objective metrics for assessing parameter sensitivity in audio\ntextures.", "published": "2022-08-23 05:40:53", "link": "http://arxiv.org/abs/2208.10743v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fall Detection from Audios with Audio Transformers", "abstract": "Fall detection for the elderly is a well-researched problem with several\nproposed solutions, including wearable and non-wearable techniques. While the\nexisting techniques have excellent detection rates, their adoption by the\ntarget population is lacking due to the need for wearing devices and user\nprivacy concerns. Our paper provides a novel, non-wearable, non-intrusive, and\nscalable solution for fall detection, deployed on an autonomous mobile robot\nequipped with a microphone. The proposed method uses ambient sound input\nrecorded in people's homes. We specifically target the bathroom environment as\nit is highly prone to falls and where existing techniques cannot be deployed\nwithout jeopardizing user privacy. The present work develops a solution based\non a Transformer architecture that takes noisy sound input from bathrooms and\nclassifies it into fall/no-fall class with an accuracy of 0.8673. Further, the\nproposed approach is extendable to other indoor environments, besides bathrooms\nand is suitable for deploying in elderly homes, hospitals, and rehabilitation\nfacilities without requiring the user to wear any device or be constantly\n\"watched\" by the sensors.", "published": "2022-08-23 00:24:19", "link": "http://arxiv.org/abs/2208.10659v1", "categories": ["cs.SD", "cs.LG", "cs.RO", "eess.AS"], "primary_category": "cs.SD"}
{"title": "In-Air Imaging Sonar Sensor Network with Real-Time Processing Using GPUs", "abstract": "For autonomous navigation and robotic applications, sensing the environment\ncorrectly is crucial. Many sensing modalities for this purpose exist. In recent\nyears, one such modality that is being used is in-air imaging sonar. It is\nideal in complex environments with rough conditions such as dust or fog.\nHowever, like with most sensing modalities, to sense the full environment\naround the mobile platform, multiple such sensors are needed to capture the\nfull 360-degree range. Currently the processing algorithms used to create this\ndata are insufficient to do so for multiple sensors at a reasonably fast update\nrate. Furthermore, a flexible and robust framework is needed to easily\nimplement multiple imaging sonar sensors into any setup and serve multiple\napplication types for the data. In this paper we present a sensor network\nframework designed for this novel sensing modality. Furthermore, an\nimplementation of the processing algorithm on a Graphics Processing Unit is\nproposed to potentially decrease the computing time to allow for real-time\nprocessing of one or more imaging sonar sensors at a sufficiently high update\nrate.", "published": "2022-08-23 09:46:18", "link": "http://arxiv.org/abs/2208.10839v1", "categories": ["cs.CV", "cs.NI", "eess.AS"], "primary_category": "cs.CV"}
{"title": "StyleTalker: One-shot Style-based Audio-driven Talking Head Video\n  Generation", "abstract": "We propose StyleTalker, a novel audio-driven talking head generation model\nthat can synthesize a video of a talking person from a single reference image\nwith accurately audio-synced lip shapes, realistic head poses, and eye blinks.\nSpecifically, by leveraging a pretrained image generator and an image encoder,\nwe estimate the latent codes of the talking head video that faithfully reflects\nthe given audio. This is made possible with several newly devised components:\n1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A\nconditional sequential variational autoencoder that learns the latent motion\nspace disentangled from the lip movements, such that we can independently\nmanipulate the motions and lip movements while preserving the identity. 3) An\nauto-regressive prior augmented with normalizing flow to learn a complex\naudio-to-motion multi-modal latent space. Equipped with these components,\nStyleTalker can generate talking head videos not only in a motion-controllable\nway when another motion source video is given but also in a completely\naudio-driven manner by inferring realistic motions from the input audio.\nThrough extensive experiments and user studies, we show that our model is able\nto synthesize talking head videos with impressive perceptual quality which are\naccurately lip-synced with the input audios, largely outperforming\nstate-of-the-art baselines.", "published": "2022-08-23 12:49:01", "link": "http://arxiv.org/abs/2208.10922v2", "categories": ["cs.CV", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.CV"}
