{"title": "Large Language Models Perform Diagnostic Reasoning", "abstract": "We explore the extension of chain-of-thought (CoT) prompting to medical\nreasoning for the task of automatic diagnosis. Motivated by doctors' underlying\nreasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical\nresults demonstrate that by simply prompting large language models trained only\non general text corpus with two DR-CoT exemplars, the diagnostic accuracy\nimproves by 15% comparing to standard prompting. Moreover, the gap reaches a\npronounced 18% in out-domain settings. Our findings suggest expert-knowledge\nreasoning in large language models can be elicited through proper promptings.", "published": "2023-07-18 01:43:00", "link": "http://arxiv.org/abs/2307.08922v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Teach model to answer questions after comprehending the document", "abstract": "Multi-choice Machine Reading Comprehension (MRC) is a challenging extension\nof Natural Language Processing (NLP) that requires the ability to comprehend\nthe semantics and logical relationships between entities in a given text. The\nMRC task has traditionally been viewed as a process of answering questions\nbased on the given text. This single-stage approach has often led the network\nto concentrate on generating the correct answer, potentially neglecting the\ncomprehension of the text itself. As a result, many prevalent models have faced\nchallenges in performing well on this task when dealing with longer texts. In\nthis paper, we propose a two-stage knowledge distillation method that teaches\nthe model to better comprehend the document by dividing the MRC task into two\nseparate stages. Our experimental results show that the student model, when\nequipped with our method, achieves significant improvements, demonstrating the\neffectiveness of our method.", "published": "2023-07-18 02:38:02", "link": "http://arxiv.org/abs/2307.08931v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the (In)Effectiveness of Large Language Models for Chinese Text\n  Correction", "abstract": "Recently, the development and progress of Large Language Models (LLMs) have\namazed the entire Artificial Intelligence community. Benefiting from their\nemergent abilities, LLMs have attracted more and more researchers to study\ntheir capabilities and performance on various downstream Natural Language\nProcessing (NLP) tasks. While marveling at LLMs' incredible performance on all\nkinds of tasks, we notice that they also have excellent multilingual processing\ncapabilities, such as Chinese. To explore the Chinese processing ability of\nLLMs, we focus on Chinese Text Correction, a fundamental and challenging\nChinese NLP task. Specifically, we evaluate various representative LLMs on the\nChinese Grammatical Error Correction (CGEC) and Chinese Spelling Check (CSC)\ntasks, which are two main Chinese Text Correction scenarios. Additionally, we\nalso fine-tune LLMs for Chinese Text Correction to better observe the potential\ncapabilities of LLMs. From extensive analyses and comparisons with previous\nstate-of-the-art small models, we empirically find that the LLMs currently have\nboth amazing performance and unsatisfactory behavior for Chinese Text\nCorrection. We believe our findings will promote the landing and application of\nLLMs in the Chinese NLP community.", "published": "2023-07-18 06:48:52", "link": "http://arxiv.org/abs/2307.09007v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards a Neural Era in Dialogue Management for Collaboration: A\n  Literature Survey", "abstract": "Dialogue-based human-AI collaboration can revolutionize collaborative\nproblem-solving, creative exploration, and social support. To realize this\ngoal, the development of automated agents proficient in skills such as\nnegotiating, following instructions, establishing common ground, and\nprogressing shared tasks is essential. This survey begins by reviewing the\nevolution of dialogue management paradigms in collaborative dialogue systems,\nfrom traditional handcrafted and information-state based methods to AI\nplanning-inspired approaches. It then shifts focus to contemporary data-driven\ndialogue management techniques, which seek to transfer deep learning successes\nfrom form-filling and open-domain settings to collaborative contexts. The paper\nproceeds to analyze a selected set of recent works that apply neural approaches\nto collaborative dialogue management, spotlighting prevailing trends in the\nfield. This survey hopes to provide foundational background for future\nadvancements in collaborative dialogue management, particularly as the dialogue\nsystems community continues to embrace the potential of large language models.", "published": "2023-07-18 07:20:43", "link": "http://arxiv.org/abs/2307.09021v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention over pre-trained Sentence Embeddings for Long Document\n  Classification", "abstract": "Despite being the current de-facto models in most NLP tasks, transformers are\noften limited to short sequences due to their quadratic attention complexity on\nthe number of tokens. Several attempts to address this issue were studied,\neither by reducing the cost of the self-attention computation or by modeling\nsmaller sequences and combining them through a recurrence mechanism or using a\nnew transformer model. In this paper, we suggest to take advantage of\npre-trained sentence transformers to start from semantically meaningful\nembeddings of the individual sentences, and then combine them through a small\nattention layer that scales linearly with the document length. We report the\nresults obtained by this simple architecture on three standard document\nclassification datasets. When compared with the current state-of-the-art models\nusing standard fine-tuning, the studied method obtains competitive results\n(even if there is no clear best model in this configuration). We also showcase\nthat the studied architecture obtains better results when freezing the\nunderlying transformers. A configuration that is useful when we need to avoid\ncomplete fine-tuning (e.g. when the same frozen transformer is shared by\ndifferent applications). Finally, two additional experiments are provided to\nfurther evaluate the relevancy of the studied architecture over simpler\nbaselines.", "published": "2023-07-18 09:06:35", "link": "http://arxiv.org/abs/2307.09084v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling Gender Bias in Terms of Profession Across LLMs: Analyzing and\n  Addressing Sociological Implications", "abstract": "Gender bias in artificial intelligence (AI) and natural language processing\nhas garnered significant attention due to its potential impact on societal\nperceptions and biases. This research paper aims to analyze gender bias in\nLarge Language Models (LLMs) with a focus on multiple comparisons between GPT-2\nand GPT-3.5, some prominent language models, to better understand its\nimplications. Through a comprehensive literature review, the study examines\nexisting research on gender bias in AI language models and identifies gaps in\nthe current knowledge. The methodology involves collecting and preprocessing\ndata from GPT-2 and GPT-3.5, and employing in-depth quantitative analysis\ntechniques to evaluate gender bias in the generated text. The findings shed\nlight on gendered word associations, language usage, and biased narratives\npresent in the outputs of these Large Language Models. The discussion explores\nthe ethical implications of gender bias and its potential consequences on\nsocial perceptions and marginalized communities. Additionally, the paper\npresents strategies for reducing gender bias in LLMs, including algorithmic\napproaches and data augmentation techniques. The research highlights the\nimportance of interdisciplinary collaborations and the role of sociological\nstudies in mitigating gender bias in AI models. By addressing these issues, we\ncan pave the way for more inclusive and unbiased AI systems that have a\npositive impact on society.", "published": "2023-07-18 11:38:45", "link": "http://arxiv.org/abs/2307.09162v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text vectorization via transformer-based language models and n-gram\n  perplexities", "abstract": "As the probability (and thus perplexity) of a text is calculated based on the\nproduct of the probabilities of individual tokens, it may happen that one\nunlikely token significantly reduces the probability (i.e., increase the\nperplexity) of some otherwise highly probable input, while potentially\nrepresenting a simple typographical error. Also, given that perplexity is a\nscalar value that refers to the entire input, information about the probability\ndistribution within it is lost in the calculation (a relatively good text that\nhas one unlikely token and another text in which each token is equally likely\nthey can have the same perplexity value), especially for longer texts. As an\nalternative to scalar perplexity this research proposes a simple algorithm used\nto calculate vector values based on n-gram perplexities within the input. Such\nrepresentations consider the previously mentioned aspects, and instead of a\nunique value, the relative perplexity of each text token is calculated, and\nthese values are combined into a single vector representing the input.", "published": "2023-07-18 13:38:39", "link": "http://arxiv.org/abs/2307.09255v1", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Linearized Relative Positional Encoding", "abstract": "Relative positional encoding is widely used in vanilla and linear\ntransformers to represent positional information. However, existing encoding\nmethods of a vanilla transformer are not always directly applicable to a linear\ntransformer, because the latter requires a decomposition of the query and key\nrepresentations into separate kernel functions. Nevertheless, principles for\ndesigning encoding methods suitable for linear transformers remain\nunderstudied. In this work, we put together a variety of existing linear\nrelative positional encoding approaches under a canonical form and further\npropose a family of linear relative positional encoding algorithms via unitary\ntransformation. Our formulation leads to a principled framework that can be\nused to develop new relative positional encoding methods that preserve linear\nspace-time complexity. Equipped with different models, the proposed linearized\nrelative positional encoding (LRPE) family derives effective encoding for\nvarious applications. Experiments show that compared with existing methods,\nLRPE achieves state-of-the-art performance in language modeling, text\nclassification, and image classification. Meanwhile, it emphasizes a general\nparadigm for designing broadly more relative positional encoding methods that\nare applicable to linear transformers. The code is available at\nhttps://github.com/OpenNLPLab/Lrpe.", "published": "2023-07-18 13:56:43", "link": "http://arxiv.org/abs/2307.09270v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pseudo Outlier Exposure for Out-of-Distribution Detection using\n  Pretrained Transformers", "abstract": "For real-world language applications, detecting an out-of-distribution (OOD)\nsample is helpful to alert users or reject such unreliable samples. However,\nmodern over-parameterized language models often produce overconfident\npredictions for both in-distribution (ID) and OOD samples. In particular,\nlanguage models suffer from OOD samples with a similar semantic representation\nto ID samples since these OOD samples lie near the ID manifold. A rejection\nnetwork can be trained with ID and diverse outlier samples to detect test OOD\nsamples, but explicitly collecting auxiliary OOD datasets brings an additional\nburden for data collection. In this paper, we propose a simple but effective\nmethod called Pseudo Outlier Exposure (POE) that constructs a surrogate OOD\ndataset by sequentially masking tokens related to ID classes. The surrogate OOD\nsample introduced by POE shows a similar representation to ID data, which is\nmost effective in training a rejection network. Our method does not require any\nexternal OOD data and can be easily implemented within off-the-shelf\nTransformers. A comprehensive comparison with state-of-the-art algorithms\ndemonstrates POE's competitiveness on several text classification benchmarks.", "published": "2023-07-18 17:29:23", "link": "http://arxiv.org/abs/2307.09455v2", "categories": ["cs.CL", "68T50"], "primary_category": "cs.CL"}
{"title": "Can Model Fusing Help Transformers in Long Document Classification? An\n  Empirical Study", "abstract": "Text classification is an area of research which has been studied over the\nyears in Natural Language Processing (NLP). Adapting NLP to multiple domains\nhas introduced many new challenges for text classification and one of them is\nlong document classification. While state-of-the-art transformer models provide\nexcellent results in text classification, most of them have limitations in the\nmaximum sequence length of the input sequence. The majority of the transformer\nmodels are limited to 512 tokens, and therefore, they struggle with long\ndocument classification problems. In this research, we explore on employing\nModel Fusing for long document classification while comparing the results with\nwell-known BERT and Longformer architectures.", "published": "2023-07-18 18:21:26", "link": "http://arxiv.org/abs/2307.09532v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mutual Reinforcement Effects in Japanese Sentence Classification and\n  Named Entity Recognition Tasks", "abstract": "Information extraction(IE) is a crucial subfield within natural language\nprocessing. However, for the traditionally segmented approach to sentence\nclassification and Named Entity Recognition, the intricate interactions between\nthese individual subtasks remain largely uninvestigated. In this study, we\npropose an integrative analysis, converging sentence classification with Named\nEntity Recognition, with the objective to unveil and comprehend the mutual\nreinforcement effect within these two information extraction subtasks. To\nachieve this, we introduce a Sentence Classification and Named Entity\nRecognition Multi-task (SCNM) approach that combines Sentence Classification\n(SC) and Named Entity Recognition (NER). We develop a Sentence-to-Label\nGeneration (SLG) framework for SCNM and construct a Wikipedia dataset\ncontaining both SC and NER. Using a format converter, we unify input formats\nand employ a generative model to generate SC-labels, NER-labels, and associated\ntext segments. We propose a Constraint Mechanism (CM) to improve generated\nformat accuracy. Our results show SC accuracy increased by 1.13 points and NER\nby 1.06 points in SCNM compared to standalone tasks, with CM raising format\naccuracy from 63.61 to 100. The findings indicate mutual reinforcement effects\nbetween SC and NER, and integration enhances both tasks' performance. We\nadditionally implemented the SLG framework on single SC task. It yielded\nsuperior accuracies compared to the baseline on two distinct Japanese SC\ndatasets. Notably, in the experiment of few-shot learning, SLG framework shows\nmuch better performance than fine-tune method. These empirical findings\ncontribute additional evidence to affirm the efficacy of the SLG framework.", "published": "2023-07-18 14:30:36", "link": "http://arxiv.org/abs/2307.10291v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Integrated NPL Approach to Sentiment Analysis in Satisfaction Surveys", "abstract": "The research project aims to apply an integrated approach to natural language\nprocessing NLP to satisfaction surveys. It will focus on understanding and\nextracting relevant information from survey responses, analyzing feelings, and\nidentifying recurring word patterns. NLP techniques will be used to determine\nemotional polarity, classify responses into positive, negative, or neutral\ncategories, and use opinion mining to highlight participants opinions. This\napproach will help identify the most relevant aspects for participants and\nunderstand their opinions in relation to those specific aspects. A key\ncomponent of the research project will be the analysis of word patterns in\nsatisfaction survey responses using NPL. This analysis will provide a deeper\nunderstanding of feelings, opinions, and themes and trends present in\nrespondents responses. The results obtained from this approach can be used to\nidentify areas for improvement, understand respondents preferences, and make\nstrategic decisions based on analysis to improve respondent satisfaction.", "published": "2023-07-18 00:23:35", "link": "http://arxiv.org/abs/2307.11771v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MLP Fusion: Towards Efficient Fine-tuning of Dense and\n  Mixture-of-Experts Language Models", "abstract": "Fine-tuning a pre-trained language model (PLM) emerges as the predominant\nstrategy in many natural language processing applications. However, this\nprocess is known to be expensive, especially on edge devices with low computing\npower. While general approaches (e.g. quantization and distillation) have been\nwidely studied to reduce the compute/memory of PLM fine-tuning, one-shot\ncompression techniques specifically designed for fine-tuning remain largely\nunexplored. In this paper, we investigate the neural tangent kernel\n(NTK)--which reveals the gradient descent dynamics of neural networks--of the\nmultilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight\nPLM through NTK-approximating MLP fusion. By incorporating NTK into the\ncompression process, MLP Fusion not only preserves the original model's output\nbut also maintains its training dynamics. To achieve this, we reconsider the\nMLP as a bundle of sub-MLPs and cluster them into a given number of centroids,\nwhich can then be restored as a compressed MLP and surprisingly well\napproximate the NTK of the original PLM. Our approach is applicable to both\nstandard MLP modules and Mixture-of-Experts (MoE) modules in PLMs,\ndemonstrating its scalability and versatility. Additionally, we provide\ntheoretical derivations to demonstrate how the proposed compression preserves\nthe NTK. Extensive experiments of PLM fine-tuning on both natural language\nunderstanding and generation tasks are provided to verify the effectiveness of\nMLP fusion. Our code is available at https://github.com/weitianxin/MLP_Fusion.", "published": "2023-07-18 03:12:51", "link": "http://arxiv.org/abs/2307.08941v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Mitigating Label Bias via Decoupled Confident Learning", "abstract": "Growing concerns regarding algorithmic fairness have led to a surge in\nmethodologies to mitigate algorithmic bias. However, such methodologies largely\nassume that observed labels in training data are correct. This is problematic\nbecause bias in labels is pervasive across important domains, including\nhealthcare, hiring, and content moderation. In particular, human-generated\nlabels are prone to encoding societal biases. While the presence of labeling\nbias has been discussed conceptually, there is a lack of methodologies to\naddress this problem. We propose a pruning method -- Decoupled Confident\nLearning (DeCoLe) -- specifically designed to mitigate label bias. After\nillustrating its performance on a synthetic dataset, we apply DeCoLe in the\ncontext of hate speech detection, where label bias has been recognized as an\nimportant challenge, and show that it successfully identifies biased labels and\noutperforms competing approaches.", "published": "2023-07-18 03:28:03", "link": "http://arxiv.org/abs/2307.08945v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Improving Text Semantic Similarity Modeling through a 3D Siamese Network", "abstract": "Siamese networks have gained popularity as a method for modeling text\nsemantic similarity. Traditional methods rely on pooling operation to compress\nthe semantic representations from Transformer blocks in encoding, resulting in\ntwo-dimensional semantic vectors and the loss of hierarchical semantic\ninformation from Transformer blocks. Moreover, this limited structure of\nsemantic vectors is akin to a flattened landscape, which restricts the methods\nthat can be applied in downstream modeling, as they can only navigate this flat\nterrain. To address this issue, we propose a novel 3D Siamese network for text\nsemantic similarity modeling, which maps semantic information to a\nhigher-dimensional space. The three-dimensional semantic tensors not only\nretains more precise spatial and feature domain information but also provides\nthe necessary structural condition for comprehensive downstream modeling\nstrategies to capture them. Leveraging this structural advantage, we introduce\nseveral modules to reinforce this 3D framework, focusing on three aspects:\nfeature extraction, attention, and feature fusion. Our extensive experiments on\nfour text semantic similarity benchmarks demonstrate the effectiveness and\nefficiency of our 3D Siamese Network.", "published": "2023-07-18 14:11:58", "link": "http://arxiv.org/abs/2307.09274v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.", "published": "2023-07-18 14:31:57", "link": "http://arxiv.org/abs/2307.09288v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ZeQR: Zero-shot Query Reformulation for Conversational Search", "abstract": "As the popularity of voice assistants continues to surge, conversational\nsearch has gained increased attention in Information Retrieval. However, data\nsparsity issues in conversational search significantly hinder the progress of\nsupervised conversational search methods. Consequently, researchers are\nfocusing more on zero-shot conversational search approaches. Nevertheless,\nexisting zero-shot methods face three primary limitations: they are not\nuniversally applicable to all retrievers, their effectiveness lacks sufficient\nexplainability, and they struggle to resolve common conversational ambiguities\ncaused by omission. To address these limitations, we introduce a novel\nZero-shot Query Reformulation (or Query Rewriting) (ZeQR) framework that\nreformulates queries based on previous dialogue contexts without requiring\nsupervision from conversational search data. Specifically, our framework\nutilizes language models designed for machine reading comprehension tasks to\nexplicitly resolve two common ambiguities: coreference and omission, in raw\nqueries. In comparison to existing zero-shot methods, our approach is\nuniversally applicable to any retriever without additional adaptation or\nindexing. It also provides greater explainability and effectively enhances\nquery intent understanding because ambiguities are explicitly and proactively\nresolved. Through extensive experiments on four TREC conversational datasets,\nwe demonstrate the effectiveness of our method, which consistently outperforms\nstate-of-the-art baselines.", "published": "2023-07-18 16:05:25", "link": "http://arxiv.org/abs/2307.09384v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation\n  Evaluation", "abstract": "Research in Image Generation has recently made significant progress,\nparticularly boosted by the introduction of Vision-Language models which are\nable to produce high-quality visual content based on textual inputs. Despite\nongoing advancements in terms of generation quality and realism, no methodical\nframeworks have been defined yet to quantitatively measure the quality of the\ngenerated content and the adherence with the prompted requests: so far, only\nhuman-based evaluations have been adopted for quality satisfaction and for\ncomparing different generative methods. We introduce a novel automated method\nfor Visual Concept Evaluation (ViCE), i.e. to assess consistency between a\ngenerated/edited image and the corresponding prompt/instructions, with a\nprocess inspired by the human cognitive behaviour. ViCE combines the strengths\nof Large Language Models (LLMs) and Visual Question Answering (VQA) into a\nunified pipeline, aiming to replicate the human cognitive process in quality\nassessment. This method outlines visual concepts, formulates image-specific\nverification questions, utilizes the Q&A system to investigate the image, and\nscores the combined outcome. Although this brave new hypothesis of mimicking\nhumans in the image evaluation process is in its preliminary assessment stage,\nresults are promising and open the door to a new form of automatic evaluation\nwhich could have significant impact as the image generation or the image target\nediting tasks become more and more sophisticated.", "published": "2023-07-18 16:33:30", "link": "http://arxiv.org/abs/2307.09416v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring\n  Instruction Tuning", "abstract": "Human-AI interactivity is a critical aspect that reflects the usability of\nmultimodal large language models (MLLMs). However, existing end-to-end MLLMs\nonly allow users to interact with them through language instructions, leading\nto the limitation of the interactive accuracy and efficiency. In this study, we\npresent precise referring instructions that utilize diverse reference\nrepresentations such as points and boxes as referring prompts to refer to the\nspecial region. This enables MLLMs to focus on the region of interest and\nachieve finer-grained interaction. Based on precise referring instruction, we\npropose ChatSpot, a unified end-to-end multimodal large language model that\nsupports diverse forms of interactivity including mouse clicks, drag-and-drop,\nand drawing boxes, which provides a more flexible and seamless interactive\nexperience. We also construct a multi-grained vision-language\ninstruction-following dataset based on existing datasets and GPT-4 generating.\nFurthermore, we design a series of evaluation tasks to assess the effectiveness\nof region recognition and interaction. Experimental results showcase ChatSpot's\npromising performance.", "published": "2023-07-18 17:56:06", "link": "http://arxiv.org/abs/2307.09474v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Integration of Large Language Models and Federated Learning", "abstract": "As the parameter size of Large Language Models (LLMs) continues to expand,\nthere is an urgent need to address the scarcity of high-quality data. In\nresponse, existing research has attempted to make a breakthrough by\nincorporating Federated Learning (FL) into LLMs. Conversely, considering the\noutstanding performance of LLMs in task generalization, researchers have also\ntried applying LLMs within FL to tackle challenges in relevant domains. The\ncomplementarity between LLMs and FL has already ignited widespread research\ninterest. In this paper, we aim to deeply explore the integration of LLMs and\nFL. We propose a research framework, dividing the fusion of LLMs and FL into\nthree parts: the combination of LLM sub-technologies with FL, the integration\nof FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We\nfirst provide a comprehensive review of the current state of research in the\ndomain of LLMs combined with FL, including their typical applications,\nintegration advantages, challenges faced, and future directions for resolution.\nSubsequently, we discuss the practical applications of the combination of LLMs\nand FL in critical scenarios such as healthcare, finance, and education, and\nprovide new perspectives and insights into future research directions for LLMs\nand FL.", "published": "2023-07-18 02:09:14", "link": "http://arxiv.org/abs/2307.08925v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "How is ChatGPT's behavior changing over time?", "abstract": "GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)\nservices. However, when and how these models are updated over time is opaque.\nHere, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on\nseveral diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3)\nopinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating\ncode, 6) US Medical License tests, and 7) visual reasoning. We find that the\nperformance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.\nFor example, GPT-4 (March 2023) was reasonable at identifying prime vs.\ncomposite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same\nquestions (51% accuracy). This is partly explained by a drop in GPT-4's amenity\nto follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in\nJune than in March in this task. GPT-4 became less willing to answer sensitive\nquestions and opinion survey questions in June than in March. GPT-4 performed\nbetter at multi-hop questions in June than in March, while GPT-3.5's\nperformance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting\nmistakes in code generation in June than in March. We provide evidence that\nGPT-4's ability to follow user instructions has decreased over time, which is\none common factor behind the many behavior drifts. Overall, our findings show\nthat the behavior of the \"same\" LLM service can change substantially in a\nrelatively short amount of time, highlighting the need for continuous\nmonitoring of LLMs.", "published": "2023-07-18 06:56:08", "link": "http://arxiv.org/abs/2307.09009v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring acceptance of autonomous vehicle policies using KeyBERT and\n  SNA: Targeting engineering students", "abstract": "This study aims to explore user acceptance of Autonomous Vehicle (AV)\npolicies with improved text-mining methods. Recently, South Korean policymakers\nhave viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as\nnext-generation means of transportation that will reduce the cost of\ntransporting passengers and goods. They support the construction of V2I and V2V\ncommunication infrastructures for ADC and recognize that ADR is equivalent to\npedestrians to promote its deployment into sidewalks. To fill the gap where\nend-user acceptance of these policies is not well considered, this study\napplied two text-mining methods to the comments of graduate students in the\nfields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is\nthe Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient,\nand the other is the Contextual Semantic Network Analysis (C-SNA) based on both\nKeyBERT, which extracts keywords that contextually represent the comments, and\ndouble cosine similarity. The reason for comparing these approaches is to\nbalance interest not only in the implications for the AV policies but also in\nthe need to apply quality text mining to this research domain. Significantly,\nthe limitation of frequency-based text mining, which does not reflect textual\ncontext, and the trade-off of adjusting thresholds in Semantic Network Analysis\n(SNA) were considered. As the results of comparing the two approaches, the\nC-SNA provided the information necessary to understand users' voices using\nfewer nodes and features than the CNA. The users who pre-emptively understood\nthe AV policies based on their engineering literacy and the given texts\nrevealed potential risks of the AV accident policies. This study adds\nsuggestions to manage these risks to support the successful deployment of AVs\non public roads.", "published": "2023-07-18 07:03:29", "link": "http://arxiv.org/abs/2307.09014v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.SI"}
{"title": "Text-guided Image Restoration and Semantic Enhancement for Text-to-Image\n  Person Retrieval", "abstract": "The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific\nperson images according to the given textual descriptions. A primary challenge\nin this task is bridging the substantial representational gap between visual\nand textual modalities. The prevailing methods map texts and images into\nunified embedding space for matching, while the intricate semantic\ncorrespondences between texts and images are still not effectively constructed.\nTo address this issue, we propose a novel TIPR framework to build fine-grained\ninteractions and alignment between person images and the corresponding texts.\nSpecifically, via fine-tuning the Contrastive Language-Image Pre-training\n(CLIP) model, a visual-textual dual encoder is firstly constructed, to\npreliminarily align the image and text features. Secondly, a Text-guided Image\nRestoration (TIR) auxiliary task is proposed to map abstract textual entities\nto specific image regions, improving the alignment between local textual and\nvisual embeddings. Additionally, a cross-modal triplet loss is presented to\nhandle hard samples, and further enhance the model's discriminability for minor\ndifferences. Moreover, a pruning-based text data augmentation approach is\nproposed to enhance focus on essential elements in descriptions, thereby\navoiding excessive model attention to less significant information. The\nexperimental results show our proposed method outperforms state-of-the-art\nmethods on three popular benchmark datasets, and the code will be made publicly\navailable at https://github.com/Delong-liu-bupt/SEN.", "published": "2023-07-18 08:23:46", "link": "http://arxiv.org/abs/2307.09059v4", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Automated Ableism: An Exploration of Explicit Disability Biases in\n  Sentiment and Toxicity Analysis Models", "abstract": "We analyze sentiment analysis and toxicity detection models to detect the\npresence of explicit bias against people with disability (PWD). We employ the\nbias identification framework of Perturbation Sensitivity Analysis to examine\nconversations related to PWD on social media platforms, specifically Twitter\nand Reddit, in order to gain insight into how disability bias is disseminated\nin real-world social settings. We then create the \\textit{Bias Identification\nTest in Sentiment} (BITS) corpus to quantify explicit disability bias in any\nsentiment analysis and toxicity detection models. Our study utilizes BITS to\nuncover significant biases in four open AIaaS (AI as a Service) sentiment\nanalysis tools, namely TextBlob, VADER, Google Cloud Natural Language API,\nDistilBERT and two toxicity detection models, namely two versions of\nToxic-BERT. Our findings indicate that all of these models exhibit\nstatistically significant explicit bias against PWD.", "published": "2023-07-18 12:45:54", "link": "http://arxiv.org/abs/2307.09209v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model\n  in Data Science", "abstract": "Recent advancements in NLP have witnessed the groundbreaking impact of\npretrained models, yielding impressive outcomes across various tasks. This\nstudy seeks to extend the power of pretraining methodologies to facilitating\nthe prediction over tables in data science, a domain traditionally overlooked,\nyet inherently challenging due to the plethora of table schemas intrinsic to\ndifferent tasks. The primary research questions underpinning this work revolve\naround the establishment of a universal pretraining protocol for tables with\nvaried structures, the generalizability and transferability of learned\nknowledge across tasks, the adaptation to diverse downstream applications, and\nthe incorporation of incremental columns over time. In response to these\nchallenges, we introduce UniTabE, a straightforward yet effective method\ndesigned to process tables in a uniform manner, devoid of constraints imposed\nby specific table structures. UniTabE's core concept relies on representing\neach basic table element with a module, termed TabUnit. This is subsequently\nfollowed by a Transformer encoder to refine the representation. Moreover, our\nmodel is designed to facilitate pretraining and finetuning through the\nutilization of free-form prompts. In order to implement the pretraining phase,\nwe curated an expansive tabular dataset comprising approximately 13B samples,\nmeticulously gathered from the Kaggle platform. This research primarily centers\non classification and regression tasks involving tabular data, and conducts\nrigorous experimental testing and analyses to validate the effectiveness of our\nmethodology. The experimental results demonstrate UniTabE's superior\nperformance against several baselines across massive benchmarks. This,\ntherefore, underscores UniTabE's potential to significantly enhance the\nsemantic representation of tabular data, thereby marking a significant stride\nfor tabular data analysis.", "published": "2023-07-18 13:28:31", "link": "http://arxiv.org/abs/2307.09249v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Selective Generation for Controllable Language Models", "abstract": "Trustworthiness of generative language models (GLMs) is crucial in their\ndeployment to critical decision making systems. Hence, certified risk control\nmethods such as selective prediction and conformal prediction have been applied\nto mitigating the hallucination problem in various supervised downstream tasks.\nHowever, the lack of appropriate correctness metric hinders applying such\nprincipled methods to language generation tasks. In this paper, we circumvent\nthis problem by leveraging the concept of textual entailment to evaluate the\ncorrectness of the generated sequence, and propose two selective generation\nalgorithms which control the false discovery rate with respect to the textual\nentailment relation (FDR-E) with a theoretical guarantee:\n$\\texttt{SGen}^{\\texttt{Sup}}$ and $\\texttt{SGen}^{\\texttt{Semi}}$.\n$\\texttt{SGen}^{\\texttt{Sup}}$, a direct modification of the selective\nprediction, is a supervised learning algorithm which exploits\nentailment-labeled data, annotated by humans. Since human annotation is costly,\nwe further propose a semi-supervised version, $\\texttt{SGen}^{\\texttt{Semi}}$,\nwhich fully utilizes the unlabeled data by pseudo-labeling, leveraging an\nentailment set function learned via conformal prediction. Furthermore,\n$\\texttt{SGen}^{\\texttt{Semi}}$ enables to use more general class of selection\nfunctions, neuro-selection functions, and provides users with an optimal\nselection function class given multiple candidates. Finally, we demonstrate the\nefficacy of the $\\texttt{SGen}$ family in achieving a desired FDR-E level with\ncomparable selection efficiency to those from baselines on both open and closed\nsource GLMs. Code and datasets are provided at\nhttps://github.com/ml-postech/selective-generation.", "published": "2023-07-18 13:36:24", "link": "http://arxiv.org/abs/2307.09254v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-Modal Discussion Transformer: Integrating Text, Images and Graph\n  Transformers to Detect Hate Speech on Social Media", "abstract": "We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor\ndetecting hate speech in online social networks such as Reddit discussions. In\ncontrast to traditional comment-only methods, our approach to labelling a\ncomment as hate speech involves a holistic analysis of text and images grounded\nin the discussion context. This is done by leveraging graph transformers to\ncapture the contextual relationships in the discussion surrounding a comment\nand grounding the interwoven fusion layers that combine text and image\nembeddings instead of processing modalities separately. To evaluate our work,\nwe present a new dataset, HatefulDiscussions, comprising complete multi-modal\ndiscussions from multiple online communities on Reddit. We compare the\nperformance of our model to baselines that only process individual comments and\nconduct extensive ablation studies.", "published": "2023-07-18 14:57:12", "link": "http://arxiv.org/abs/2307.09312v4", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SI"], "primary_category": "cs.CL"}
{"title": "A comparative analysis of SRGAN models", "abstract": "In this study, we evaluate the performance of multiple state-of-the-art SRGAN\n(Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN\nand EDSR, on a benchmark dataset of real-world images which undergo degradation\nusing a pipeline. Our results show that some models seem to significantly\nincrease the resolution of the input images while preserving their visual\nquality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE\nmodel from huggingface outperforms the remaining candidate models in terms of\nboth quantitative metrics and subjective visual quality assessments with least\ncompute overhead. Specifically, EDSR generates images with higher peak\nsignal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and\nare seen to return high quality OCR results with Tesseract OCR engine. These\nfindings suggest that EDSR is a robust and effective approach for single-image\nsuper-resolution and may be particularly well-suited for applications where\nhigh-quality visual fidelity is critical and optimized compute.", "published": "2023-07-18 17:35:45", "link": "http://arxiv.org/abs/2307.09456v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Overthinking the Truth: Understanding how Language Models Process False\n  Demonstrations", "abstract": "Modern language models can imitate complex patterns through few-shot\nlearning, enabling them to complete challenging tasks without fine-tuning.\nHowever, imitation can also lead models to reproduce inaccuracies or harmful\ncontent if present in the context. We study harmful imitation through the lens\nof a model's internal representations, and identify two related phenomena:\n\"overthinking\" and \"false induction heads\". The first phenomenon, overthinking,\nappears when we decode predictions from intermediate layers, given correct vs.\nincorrect few-shot demonstrations. At early layers, both demonstrations induce\nsimilar model behavior, but the behavior diverges sharply at some \"critical\nlayer\", after which the accuracy given incorrect demonstrations progressively\ndecreases. The second phenomenon, false induction heads, are a possible\nmechanistic cause of overthinking: these are heads in late layers that attend\nto and copy false information from previous demonstrations, and whose ablation\nreduces overthinking. Beyond scientific understanding, our results suggest that\nstudying intermediate model computations could be a promising avenue for\nunderstanding and guarding against harmful model behaviors.", "published": "2023-07-18 17:56:50", "link": "http://arxiv.org/abs/2307.09476v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning\n  Fine-tuning", "abstract": "In this work, we propose a method to create domain-sensitive speech\nrecognition models that utilize textual domain information by conditioning its\ngeneration on a given text prompt. This is accomplished by fine-tuning a\npre-trained, end-to-end model (Whisper) to learn from demonstrations with\nprompt examples. We show that this ability can be generalized to different\ndomains and even various prompt contexts, with our model gaining a Word Error\nRate (WER) reduction of up to 33% on unseen datasets from various domains, such\nas medical conversation, air traffic control communication, and financial\nmeetings. Considering the limited availability of audio-transcript pair data,\nwe further extend our method to text-only fine-tuning to achieve domain\nsensitivity as well as domain adaptation. We demonstrate that our text-only\nfine-tuned model can also attend to various prompt contexts, with the model\nreaching the most WER reduction of 29% on the medical conversation dataset.", "published": "2023-07-18 06:45:43", "link": "http://arxiv.org/abs/2307.10274v2", "categories": ["eess.AS", "cs.CL", "cs.LG"], "primary_category": "eess.AS"}
{"title": "The Language Labyrinth: Constructive Critique on the Terminology Used in\n  the AI Discourse", "abstract": "In the interdisciplinary field of artificial intelligence (AI) the problem of\nclear terminology is especially momentous. This paper claims, that AI debates\nare still characterised by a lack of critical distance to metaphors like\n'training', 'learning' or 'deciding'. As consequence, reflections regarding\nresponsibility or potential use-cases are greatly distorted. Yet, if relevant\ndecision-makers are convinced that AI can develop an 'understanding' or\nproperly 'interpret' issues, its regular use for sensitive tasks like deciding\nabout social benefits or judging court cases looms. The chapter argues its\nclaim by analysing central notions of the AI debate and tries to contribute by\nproposing more fitting terminology and hereby enabling more fruitful debates.\nIt is a conceptual work at the intersection of critical computer science and\nphilosophy of language.", "published": "2023-07-18 14:32:21", "link": "http://arxiv.org/abs/2307.10292v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.NE", "K.4.0; K.4.1; K.4.2; I.2.0; I.2.1; K.4.3"], "primary_category": "cs.CY"}
{"title": "Analyzing sports commentary in order to automatically recognize events\n  and extract insights", "abstract": "In this paper, we carefully investigate how we can use multiple different\nNatural Language Processing techniques and methods in order to automatically\nrecognize the main actions in sports events. We aim to extract insights by\nanalyzing live sport commentaries from different sources and by classifying\nthese major actions into different categories. We also study if sentiment\nanalysis could help detect these main actions.", "published": "2023-07-18 18:51:06", "link": "http://arxiv.org/abs/2307.10303v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment\n  enabled by Large Language Models", "abstract": "The task of entity alignment between knowledge graphs (KGs) aims to identify\nevery pair of entities from two different KGs that represent the same entity.\nMany machine learning-based methods have been proposed for this task. However,\nto our best knowledge, existing methods all require manually crafted seed\nalignments, which are expensive to obtain. In this paper, we propose the first\nfully automatic alignment method named AutoAlign, which does not require any\nmanually crafted seed alignments. Specifically, for predicate embeddings,\nAutoAlign constructs a predicate-proximity-graph with the help of large\nlanguage models to automatically capture the similarity between predicates\nacross two KGs. For entity embeddings, AutoAlign first computes the entity\nembeddings of each KG independently using TransE, and then shifts the two KGs'\nentity embeddings into the same vector space by computing the similarity\nbetween entities based on their attributes. Thus, both predicate alignment and\nentity alignment can be done without manually crafted seed alignments.\nAutoAlign is not only fully automatic, but also highly effective. Experiments\nusing real-world KGs show that AutoAlign improves the performance of entity\nalignment significantly compared to state-of-the-art methods.", "published": "2023-07-18 04:43:24", "link": "http://arxiv.org/abs/2307.11772v3", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Efficient representation of head-related transfer functions in\n  continuous space-frequency domains", "abstract": "Utilizing spherical harmonic (SH) domain has been established as the default\nmethod of obtaining continuity over space in head-related transfer functions\n(HRTFs). This paper concerns different variants of extending this solution by\nreplacing SHs with four-dimensional (4D) continuous functional models in which\nfrequency is imagined as another physical dimension. Recently developed\nhyperspherical harmonic (HSH) representation is compared with models defined in\nspherindrical coordinate system by merging SHs with one-dimensional basis\nfunctions. The efficiency of both approaches is evaluated based on the\nreproduction errors for individual HRTFs from HUTUBS database, including\ndetailed analysis of its dependency on chosen orders of approximation in\nfrequency and space. Employing continuous functional models defined in 4D\ncoordinate systems allows HRTF magnitude spectra to be expressed as a small set\nof coefficients which can be decoded back into values at any direction and\nfrequency. The best performance was noted for HSHs and SHs merged with reverse\nFourier-Bessel series, with the former featuring better compression abilities,\nachieving slightly higher accuracy for low number of coefficients. The\npresented models can serve multiple purposes, such as interpolation,\ncompression or parametrization for machine learning applications, and can be\napplied not only to HRTFs but also to other types of directivity functions,\ne.g. sound source directivity.", "published": "2023-07-18 15:38:24", "link": "http://arxiv.org/abs/2307.09352v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Interpretable Timbre Synthesis using Variational Autoencoders\n  Regularized on Timbre Descriptors", "abstract": "Controllable timbre synthesis has been a subject of research for several\ndecades, and deep neural networks have been the most successful in this area.\nDeep generative models such as Variational Autoencoders (VAEs) have the ability\nto generate a high-level representation of audio while providing a structured\nlatent space. Despite their advantages, the interpretability of these latent\nspaces in terms of human perception is often limited. To address this\nlimitation and enhance the control over timbre generation, we propose a\nregularized VAE-based latent space that incorporates timbre descriptors.\nMoreover, we suggest a more concise representation of sound by utilizing its\nharmonic content, in order to minimize the dimensionality of the latent space.", "published": "2023-07-18 11:46:13", "link": "http://arxiv.org/abs/2307.10283v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "On Computing In the Network: Covid-19 Coughs Detection Case Study", "abstract": "Computing in the network (COIN) is a promising technology that allows\nprocessing to be carried out within network devices such as switches and\nnetwork interface cards. Time sensitive application can achieve their quality\nof service (QoS) target by flexibly distributing the caching and computing\ntasks in the cloud-edge-mist continuum. This paper highlights the advantages of\nin-network computing, comparing to edge computing, in terms of latency and\ntraffic filtering. We consider a critical use case related to Covid-19 alert\napplication in an airport setting. Arriving travelers are monitored through\ncough analysis so that potentially infected cases can be detected and isolated\nfor medical tests. A performance comparison has been done between an\narchitecture using in-network computing and another one using edge computing.\nWe show using simulations that in-network computing outperforms edge computing\nin terms of Round Trip Time (RTT) and traffic filtering.", "published": "2023-07-18 00:13:39", "link": "http://arxiv.org/abs/2307.08902v1", "categories": ["cs.NI", "cs.SD", "eess.AS"], "primary_category": "cs.NI"}
{"title": "OxfordVGG Submission to the EGO4D AV Transcription Challenge", "abstract": "This report presents the technical details of our submission on the EGO4D\nAudio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the\nOxfordVGG team. We present WhisperX, a system for efficient speech\ntranscription of long-form audio with word-level time alignment, along with two\ntext normalisers which are publicly available. Our final submission obtained\n56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the\nleaderboard. All baseline codes and models are available on\nhttps://github.com/m-bain/whisperX.", "published": "2023-07-18 06:48:39", "link": "http://arxiv.org/abs/2307.09006v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Detecting Throat Cancer from Speech Signals using Machine Learning: A\n  Scoping Literature Review", "abstract": "Introduction: Cases of throat cancer are rising worldwide. With survival\ndecreasing significantly at later stages, early detection is vital. Artificial\nintelligence (AI) and machine learning (ML) have the potential to detect throat\ncancer from patient speech, facilitating earlier diagnosis and reducing the\nburden on overstretched healthcare systems. However, no comprehensive review\nhas explored the use of AI and ML for detecting throat cancer from speech. This\nreview aims to fill this gap by evaluating how these technologies perform and\nidentifying issues that need to be addressed in future research. Materials and\nMethods: We conducted a scoping literature review across three databases:\nScopus, Web of Science, and PubMed. We included articles that classified speech\nusing machine learning and specified the inclusion of throat cancer patients in\ntheir data. Articles were categorized based on whether they performed binary or\nmulti-class classification. Results: We found 27 articles fitting our inclusion\ncriteria, 12 performing binary classification, 13 performing multi-class\nclassification, and two that do both binary and multiclass classification. The\nmost common classification method used was neural networks, and the most\nfrequently extracted feature was mel-spectrograms. We also documented\npre-processing methods and classifier performance. We compared each article\nagainst the TRIPOD-AI checklist, which showed a significant lack of open\nscience, with only one article sharing code and only three using open-access\ndata. Conclusion: Open-source code is essential for external validation and\nfurther development in this field. Our review indicates that no single method\nor specific feature consistently outperforms others in detecting throat cancer\nfrom speech. Future research should focus on standardizing methodologies and\nimproving the reproducibility of results.", "published": "2023-07-18 13:06:17", "link": "http://arxiv.org/abs/2307.09230v3", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "FlexiAST: Flexibility is What AST Needs", "abstract": "The objective of this work is to give patch-size flexibility to Audio\nSpectrogram Transformers (AST). Recent advancements in ASTs have shown superior\nperformance in various audio-based tasks. However, the performance of standard\nASTs degrades drastically when evaluated using different patch sizes from that\nused during training. As a result, AST models are typically re-trained to\naccommodate changes in patch sizes. To overcome this limitation, this paper\nproposes a training procedure to provide flexibility to standard AST models\nwithout architectural changes, allowing them to work with various patch sizes\nat the inference stage - FlexiAST. This proposed training approach simply\nutilizes random patch size selection and resizing of patch and positional\nembedding weights. Our experiments show that FlexiAST gives similar performance\nto standard AST models while maintaining its evaluation ability at various\npatch sizes on different datasets for audio classification tasks.", "published": "2023-07-18 14:30:47", "link": "http://arxiv.org/abs/2307.09286v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SLMGAN: Exploiting Speech Language Model Representations for\n  Unsupervised Zero-Shot Voice Conversion in GANs", "abstract": "In recent years, large-scale pre-trained speech language models (SLMs) have\ndemonstrated remarkable advancements in various generative speech modeling\napplications, such as text-to-speech synthesis, voice conversion, and speech\nenhancement. These applications typically involve mapping text or speech inputs\nto pre-trained SLM representations, from which target speech is decoded. This\npaper introduces a new approach, SLMGAN, to leverage SLM representations for\ndiscriminative tasks within the generative adversarial network (GAN) framework,\nspecifically for voice conversion. Building upon StarGANv2-VC, we add our novel\nSLM-based WavLM discriminators on top of the mel-based discriminators along\nwith our newly designed SLM feature matching loss function, resulting in an\nunsupervised zero-shot voice conversion system that does not require text\nlabels during training. Subjective evaluation results show that SLMGAN\noutperforms existing state-of-the-art zero-shot voice conversion models in\nterms of naturalness and achieves comparable similarity, highlighting the\npotential of SLM-based discriminators for related applications.", "published": "2023-07-18 17:09:15", "link": "http://arxiv.org/abs/2307.09435v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "JAZZVAR: A Dataset of Variations found within Solo Piano Performances of\n  Jazz Standards for Music Overpainting", "abstract": "Jazz pianists often uniquely interpret jazz standards. Passages from these\ninterpretations can be viewed as sections of variation. We manually extracted\nsuch variations from solo jazz piano performances. The JAZZVAR dataset is a\ncollection of 502 pairs of Variation and Original MIDI segments. Each Variation\nin the dataset is accompanied by a corresponding Original segment containing\nthe melody and chords from the original jazz standard. Our approach differs\nfrom many existing jazz datasets in the music information retrieval (MIR)\ncommunity, which often focus on improvisation sections within jazz\nperformances. In this paper, we outline the curation process for obtaining and\nsorting the repertoire, the pipeline for creating the Original and Variation\npairs, and our analysis of the dataset. We also introduce a new generative\nmusic task, Music Overpainting, and present a baseline Transformer model\ntrained on the JAZZVAR dataset for this task. Other potential applications of\nour dataset include expressive performance analysis and performer\nidentification.", "published": "2023-07-18 22:48:54", "link": "http://arxiv.org/abs/2307.09670v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
