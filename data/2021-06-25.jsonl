{"title": "JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE\n  2021", "abstract": "COLIEE is an annual competition in automatic computerized legal text\nprocessing. Automatic legal document processing is an ambitious goal, and the\nstructure and semantics of the law are often far more complex than everyday\nlanguage. In this article, we survey and report our methods and experimental\nresults in using deep learning in legal document processing. The results show\nthe difficulties as well as potentials in this family of approaches.", "published": "2021-06-25 03:31:12", "link": "http://arxiv.org/abs/2106.13405v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained\n  Language Models for Domains", "abstract": "Large pre-trained models have achieved great success in many natural language\nprocessing tasks. However, when they are applied in specific domains, these\nmodels suffer from domain shift and bring challenges in fine-tuning and online\nserving for latency and capacity constraints. In this paper, we present a\ngeneral approach to developing small, fast and effective pre-trained models for\nspecific domains. This is achieved by adapting the off-the-shelf general\npre-trained models and performing task-agnostic knowledge distillation in\ntarget domains. Specifically, we propose domain-specific vocabulary expansion\nin the adaptation stage and employ corpus level occurrence probability to\nchoose the size of incremental vocabulary automatically. Then we systematically\nexplore different strategies to compress the large pre-trained models for\nspecific domains. We conduct our experiments in the biomedical and computer\nscience domain. The experimental results demonstrate that our approach achieves\nbetter performance over the BERT BASE model in domain-specific tasks while 3.3x\nsmaller and 5.1x faster than BERT BASE. The code and pre-trained models are\navailable at https://aka.ms/adalm.", "published": "2021-06-25 07:37:05", "link": "http://arxiv.org/abs/2106.13474v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Manually Annotated Spelling Error Corpus for Amharic", "abstract": "This paper presents a manually annotated spelling error corpus for Amharic,\nlingua franca in Ethiopia. The corpus is designed to be used for the evaluation\nof spelling error detection and correction. The misspellings are tagged as\nnon-word and real-word errors. In addition, the contextual information\navailable in the corpus makes it useful in dealing with both types of spelling\nerrors.", "published": "2021-06-25 09:27:42", "link": "http://arxiv.org/abs/2106.13521v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Representation of Word Meanings in Context: A Case Study\n  on Homonymy and Synonymy", "abstract": "This paper presents a multilingual study of word meaning representations in\ncontext. We assess the ability of both static and contextualized models to\nadequately represent different lexical-semantic relations, such as homonymy and\nsynonymy. To do so, we created a new multilingual dataset that allows us to\nperform a controlled evaluation of several factors such as the impact of the\nsurrounding context or the overlap between words, conveying the same or\ndifferent senses. A systematic assessment on four scenarios shows that the best\nmonolingual models based on Transformers can adequately disambiguate homonyms\nin context. However, as they rely heavily on context, these models fail at\nrepresenting words with different senses when occurring in similar sentences.\nExperiments are performed in Galician, Portuguese, English, and Spanish, and\nboth the dataset (with more than 3,000 evaluation items) and new models are\nfreely released with this study.", "published": "2021-06-25 10:54:23", "link": "http://arxiv.org/abs/2106.13553v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models are Good Translators", "abstract": "Recent years have witnessed the rapid advance in neural machine translation\n(NMT), the core of which lies in the encoder-decoder architecture. Inspired by\nthe recent progress of large-scale pre-trained language models on machine\ntranslation in a limited scenario, we firstly demonstrate that a single\nlanguage model (LM4MT) can achieve comparable performance with strong\nencoder-decoder NMT models on standard machine translation benchmarks, using\nthe same training data and similar amount of model parameters. LM4MT can also\neasily utilize source-side texts as additional supervision. Though modeling the\nsource- and target-language texts with the same mechanism, LM4MT can provide\nunified representations for both source and target sentences, which can better\ntransfer knowledge across languages. Extensive experiments on pivot-based and\nzero-shot translation tasks show that LM4MT can outperform the encoder-decoder\nNMT model by a large margin.", "published": "2021-06-25 13:30:29", "link": "http://arxiv.org/abs/2106.13627v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Sample Replacements for ELECTRA Pre-Training", "abstract": "ELECTRA pretrains a discriminator to detect replaced tokens, where the\nreplacements are sampled from a generator trained with masked language\nmodeling. Despite the compelling performance, ELECTRA suffers from the\nfollowing two issues. First, there is no direct feedback loop from\ndiscriminator to generator, which renders replacement sampling inefficient.\nSecond, the generator's prediction tends to be over-confident along with\ntraining, making replacements biased to correct tokens. In this paper, we\npropose two methods to improve replacement sampling for ELECTRA pre-training.\nSpecifically, we augment sampling with a hardness prediction mechanism, so that\nthe generator can encourage the discriminator to learn what it has not\nacquired. We also prove that efficient sampling reduces the training variance\nof the discriminator. Moreover, we propose to use a focal loss for the\ngenerator in order to relieve oversampling of correct tokens as replacements.\nExperimental results show that our method improves ELECTRA pre-training on\nvarious downstream tasks.", "published": "2021-06-25 15:51:55", "link": "http://arxiv.org/abs/2106.13715v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeltaLM: Encoder-Decoder Pre-training for Language Generation and\n  Translation by Augmenting Pretrained Multilingual Encoders", "abstract": "While pretrained encoders have achieved success in various natural language\nunderstanding (NLU) tasks, there is a gap between these pretrained encoders and\nnatural language generation (NLG). NLG tasks are often based on the\nencoder-decoder framework, where the pretrained encoders can only benefit part\nof it. To reduce this gap, we introduce DeltaLM, a pretrained multilingual\nencoder-decoder model that regards the decoder as the task layer of\noff-the-shelf pretrained encoders. Specifically, we augment the pretrained\nmultilingual encoder with a decoder and pre-train it in a self-supervised way.\nTo take advantage of both the large-scale monolingual data and bilingual data,\nwe adopt the span corruption and translation span corruption as the\npre-training tasks. Experiments show that DeltaLM outperforms various strong\nbaselines on both natural language generation and translation tasks, including\nmachine translation, abstractive text summarization, data-to-text, and question\ngeneration. The code and pretrained models are available at\n\\url{https://aka.ms/deltalm}.", "published": "2021-06-25 16:12:10", "link": "http://arxiv.org/abs/2106.13736v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44\n  Languages", "abstract": "Contemporary works on abstractive text summarization have focused primarily\non high-resource languages like English, mostly due to the limited availability\nof datasets for low/mid-resource ones. In this work, we present XL-Sum, a\ncomprehensive and diverse dataset comprising 1 million professionally annotated\narticle-summary pairs from BBC, extracted using a set of carefully designed\nheuristics. The dataset covers 44 languages ranging from low to high-resource,\nfor many of which no public dataset is currently available. XL-Sum is highly\nabstractive, concise, and of high quality, as indicated by human and intrinsic\nevaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model,\nwith XL-Sum and experiment on multilingual and low-resource summarization\ntasks. XL-Sum induces competitive results compared to the ones obtained using\nsimilar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10\nlanguages we benchmark on, with some of them exceeding 15, as obtained by\nmultilingual training. Additionally, training on low-resource languages\nindividually also provides competitive performance. To the best of our\nknowledge, XL-Sum is the largest abstractive summarization dataset in terms of\nthe number of samples collected from a single source and the number of\nlanguages covered. We are releasing our dataset and models to encourage future\nresearch on multilingual abstractive summarization. The resources can be found\nat \\url{https://github.com/csebuetnlp/xl-sum}.", "published": "2021-06-25 18:00:24", "link": "http://arxiv.org/abs/2106.13822v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Persian Rhetorical Structure Theory", "abstract": "Over the past years, interest in discourse analysis and discourse parsing has\nsteadily grown, and many discourse-annotated corpora and, as a result,\ndiscourse parsers have been built. In this paper, we present a\ndiscourse-annotated corpus for the Persian language built in the framework of\nRhetorical Structure Theory as well as a discourse parser built upon the DPLP\nparser, an open-source discourse parser. Our corpus consists of 150\njournalistic texts, each text having an average of around 400 words. Corpus\ntexts were annotated using 18 discourse relations and based on the annotation\nguideline of the English RST Discourse Treebank corpus. Our text-level\ndiscourse parser is trained using gold segmentation and is built upon the DPLP\ndiscourse parser, which uses a large-margin transition-based approach to solve\nthe problem of discourse parsing. The performance of our discourse parser in\nspan (S), nuclearity (N) and relation (R) detection is around 78%, 64%, 44%\nrespectively, in terms of F1 measure.", "published": "2021-06-25 18:15:47", "link": "http://arxiv.org/abs/2106.13833v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Semantic Parsing Natural Language into Relational Algebra", "abstract": "Natural interface to database (NLIDB) has been researched a lot during the\npast decades. In the core of NLIDB, is a semantic parser used to convert\nnatural language into SQL. Solutions from traditional NLP methodology focuses\non grammar rule pattern learning and pairing via intermediate logic forms.\nAlthough those methods give an acceptable performance on certain specific\ndatabase and parsing tasks, they are hard to generalize and scale. On the other\nhand, recent progress in neural deep learning seems to provide a promising\ndirection towards building a general NLIDB system. Unlike the traditional\napproach, those neural methodologies treat the parsing problem as a\nsequence-to-sequence learning problem. In this paper, we experimented on\nseveral sequence-to-sequence learning models and evaluate their performance on\ngeneral database parsing task.", "published": "2021-06-25 19:36:02", "link": "http://arxiv.org/abs/2106.13858v1", "categories": ["cs.CL", "I.1.3"], "primary_category": "cs.CL"}
{"title": "A Source-Criticism Debiasing Method for GloVe Embeddings", "abstract": "It is well-documented that word embeddings trained on large public corpora\nconsistently exhibit known human social biases. Although many methods for\ndebiasing exist, almost all fixate on completely eliminating biased information\nfrom the embeddings and often diminish training set size in the process. In\nthis paper, we present a simple yet effective method for debiasing GloVe word\nembeddings (Pennington et al., 2014) which works by incorporating explicit\ninformation about training set bias rather than removing biased data outright.\nOur method runs quickly and efficiently with the help of a fast bias gradient\napproximation method from Brunet et al. (2019). As our approach is akin to the\nnotion of 'source criticism' in the humanities, we term our method\nSource-Critical GloVe (SC-GloVe). We show that SC-GloVe reduces the effect size\non Word Embedding Association Test (WEAT) sets without sacrificing training\ndata or TOP-1 performance.", "published": "2021-06-25 01:31:02", "link": "http://arxiv.org/abs/2106.13382v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ParaLaw Nets -- Cross-lingual Sentence-level Pretraining for Legal Text\n  Processing", "abstract": "Ambiguity is a characteristic of natural language, which makes expression\nideas flexible. However, in a domain that requires accurate statements, it\nbecomes a barrier. Specifically, a single word can have many meanings and\nmultiple words can have the same meaning. When translating a text into a\nforeign language, the translator needs to determine the exact meaning of each\nelement in the original sentence to produce the correct translation sentence.\nFrom that observation, in this paper, we propose ParaLaw Nets, a pretrained\nmodel family using sentence-level cross-lingual information to reduce ambiguity\nand increase the performance in legal text processing. This approach achieved\nthe best result in the Question Answering task of COLIEE-2021.", "published": "2021-06-25 03:21:57", "link": "http://arxiv.org/abs/2106.13403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Privileged Zero-Shot AutoML", "abstract": "This work improves the quality of automated machine learning (AutoML) systems\nby using dataset and function descriptions while significantly decreasing\ncomputation time from minutes to milliseconds by using a zero-shot approach.\nGiven a new dataset and a well-defined machine learning task, humans begin by\nreading a description of the dataset and documentation for the algorithms to be\nused. This work is the first to use these textual descriptions, which we call\nprivileged information, for AutoML. We use a pre-trained Transformer model to\nprocess the privileged text and demonstrate that using this information\nimproves AutoML performance. Thus, our approach leverages the progress of\nunsupervised representation learning in natural language processing to provide\na significant boost to AutoML. We demonstrate that using only textual\ndescriptions of the data and functions achieves reasonable classification\nperformance, and adding textual descriptions to data meta-features improves\nclassification across tabular datasets. To achieve zero-shot AutoML we train a\ngraph neural network with these description embeddings and the data\nmeta-features. Each node represents a training dataset, which we use to predict\nthe best machine learning pipeline for a new test dataset in a zero-shot\nfashion. Our zero-shot approach rapidly predicts a high-quality pipeline for a\nsupervised learning task and dataset. In contrast, most AutoML systems require\ntens or hundreds of pipeline evaluations. We show that zero-shot AutoML reduces\nrunning and prediction times from minutes to milliseconds, consistently across\ndatasets. By speeding up AutoML by orders of magnitude this work demonstrates\nreal-time AutoML.", "published": "2021-06-25 16:31:05", "link": "http://arxiv.org/abs/2106.13743v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Domain-Specific Pretraining for Vertical Search: Case Study on\n  Biomedical Literature", "abstract": "Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.", "published": "2021-06-25 01:02:55", "link": "http://arxiv.org/abs/2106.13375v2", "categories": ["cs.IR", "cs.CL", "cs.DL"], "primary_category": "cs.IR"}
{"title": "Fine-grained Geolocation Prediction of Tweets with Human Machine\n  Collaboration", "abstract": "Twitter is a useful resource to analyze peoples' opinions on various topics.\nOften these topics are correlated or associated with locations from where these\nTweet posts are made. For example, restaurant owners may need to know where\ntheir target customers eat with respect to the sentiment of the posts made\nrelated to food, policy planners may need to analyze citizens' opinion on\nrelevant issues such as crime, safety, congestion, etc. with respect to\nspecific parts of the city, or county or state. As promising as this is, less\nthan $1\\%$ of the crawled Tweet posts come with geolocation tags. That makes\naccurate prediction of Tweet posts for the non geo-tagged tweets very critical\nto analyze data in various domains. In this research, we utilized millions of\nTwitter posts and end-users domain expertise to build a set of deep neural\nnetwork models using natural language processing (NLP) techniques, that\npredicts the geolocation of non geo-tagged Tweet posts at various level of\ngranularities such as neighborhood, zipcode, and longitude with latitudes. With\nmultiple neural architecture experiments, and a collaborative human-machine\nworkflow design, our ongoing work on geolocation detection shows promising\nresults that empower end-users to correlate relationship between variables of\nchoice with the location information.", "published": "2021-06-25 03:51:02", "link": "http://arxiv.org/abs/2106.13411v1", "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "cs.LG"}
{"title": "Preliminary study on using vector quantization latent spaces for TTS/VC\n  systems with consistent performance", "abstract": "Generally speaking, the main objective when training a neural speech\nsynthesis system is to synthesize natural and expressive speech from the output\nlayer of the neural network without much attention given to the hidden layers.\nHowever, by learning useful latent representation, the system can be used for\nmany more practical scenarios. In this paper, we investigate the use of\nquantized vectors to model the latent linguistic embedding and compare it with\nthe continuous counterpart. By enforcing different policies over the latent\nspaces in the training, we are able to obtain a latent linguistic embedding\nthat takes on different properties while having a similar performance in terms\nof quality and speaker similarity. Our experiments show that the voice cloning\nsystem built with vector quantization has only a small degradation in terms of\nperceptive evaluations, but has a discrete latent space that is useful for\nreducing the representation bit-rate, which is desirable for data transferring,\nor limiting the information leaking, which is important for speaker\nanonymization and other tasks of that nature.", "published": "2021-06-25 07:51:35", "link": "http://arxiv.org/abs/2106.13479v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Knowledge-Grounded Self-Rationalization via Extractive and Natural\n  Language Explanations", "abstract": "Models that generate extractive rationales (i.e., subsets of features) or\nnatural language explanations (NLEs) for their predictions are important for\nexplainable AI. While an extractive rationale provides a quick view of the\nfeatures most responsible for a prediction, an NLE allows for a comprehensive\ndescription of the decision-making process behind a prediction. However,\ncurrent models that generate the best extractive rationales or NLEs often fall\nbehind the state-of-the-art (SOTA) in terms of task performance. In this work,\nwe bridge this gap by introducing RExC, a self-rationalizing framework that\ngrounds its predictions and two complementary types of explanations (NLEs and\nextractive rationales) in background knowledge. Our framework improves over\nprevious methods by: (i) reaching SOTA task performance while also providing\nexplanations, (ii) providing two types of explanations, while existing models\nusually provide only one type, and (iii) beating by a large margin the previous\nSOTA in terms of quality of both types of explanations. Furthermore, a\nperturbation analysis in RExC shows a high degree of association between\nexplanations and predictions, a necessary property of faithful explanations.", "published": "2021-06-25 20:31:33", "link": "http://arxiv.org/abs/2106.13876v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Few-Shot Learning with Frozen Language Models", "abstract": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.", "published": "2021-06-25 21:07:09", "link": "http://arxiv.org/abs/2106.13884v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "iReason: Multimodal Commonsense Reasoning using Videos and Natural\n  Language with Interpretability", "abstract": "Causality knowledge is vital to building robust AI systems. Deep learning\nmodels often perform poorly on tasks that require causal reasoning, which is\noften derived using some form of commonsense knowledge not immediately\navailable in the input but implicitly inferred by humans. Prior work has\nunraveled spurious observational biases that models fall prey to in the absence\nof causality. While language representation models preserve contextual\nknowledge within learned embeddings, they do not factor in causal relationships\nduring training. By blending causal relationships with the input features to an\nexisting model that performs visual cognition tasks (such as scene\nunderstanding, video captioning, video question-answering, etc.), better\nperformance can be achieved owing to the insight causal relationships bring\nabout. Recently, several models have been proposed that have tackled the task\nof mining causal data from either the visual or textual modality. However,\nthere does not exist widespread research that mines causal relationships by\njuxtaposing the visual and language modalities. While images offer a rich and\neasy-to-process resource for us to mine causality knowledge from, videos are\ndenser and consist of naturally time-ordered events. Also, textual information\noffers details that could be implicit in videos. We propose iReason, a\nframework that infers visual-semantic commonsense knowledge using both videos\nand natural language captions. Furthermore, iReason's architecture integrates a\ncausal rationalization module to aid the process of interpretability, error\nanalysis and bias detection. We demonstrate the effectiveness of iReason using\na two-pronged comparative analysis with language representation learning models\n(BERT, GPT-2) as well as current state-of-the-art multimodal causality models.", "published": "2021-06-25 02:56:34", "link": "http://arxiv.org/abs/2107.10300v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Basis-MelGAN: Efficient Neural Vocoder Based on Audio Decomposition", "abstract": "Recent studies have shown that neural vocoders based on generative\nadversarial network (GAN) can generate audios with high quality. While GAN\nbased neural vocoders have shown to be computationally much more efficient than\nthose based on autoregressive predictions, the real-time generation of the\nhighest quality audio on CPU is still a very challenging task. One major\ncomputation of all GAN-based neural vocoders comes from the stacked upsampling\nlayers, which were designed to match the length of the waveform's length of\noutput and temporal resolution. Meanwhile, the computational complexity of\nupsampling networks is closely correlated with the numbers of samples generated\nfor each window. To reduce the computation of upsampling layers, we propose a\nnew GAN based neural vocoder called Basis-MelGAN where the raw audio samples\nare decomposed with a learned basis and their associated weights. As the\nprediction targets of Basis-MelGAN are the weight values associated with each\nlearned basis instead of the raw audio samples, the upsampling layers in\nBasis-MelGAN can be designed with much simpler networks. Compared with other\nGAN based neural vocoders, the proposed Basis-MelGAN could produce comparable\nhigh-quality audio but significantly reduced computational complexity from\nHiFi-GAN V1's 17.74 GFLOPs to 7.95 GFLOPs.", "published": "2021-06-25 04:18:33", "link": "http://arxiv.org/abs/2106.13419v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Online Self-Attentive Gated RNNs for Real-Time Speaker Separation", "abstract": "Deep neural networks have recently shown great success in the task of blind\nsource separation, both under monaural and binaural settings. Although these\nmethods were shown to produce high-quality separations, they were mainly\napplied under offline settings, in which the model has access to the full input\nsignal while separating the signal. In this study, we convert a non-causal\nstate-of-the-art separation model into a causal and real-time model and\nevaluate its performance under both online and offline settings. We compare the\nperformance of the proposed model to several baseline methods under anechoic,\nnoisy, and noisy-reverberant recording conditions while exploring both monaural\nand binaural inputs and outputs. Our findings shed light on the relative\ndifference between causal and non-causal models when performing separation. Our\nstateful implementation for online separation leads to a minor drop in\nperformance compared to the offline model; 0.8dB for monaural inputs and 0.3dB\nfor binaural inputs while reaching a real-time factor of 0.65. Samples can be\nfound under the following link:\nhttps://kwanum.github.io/sagrnnc-stream-results/.", "published": "2021-06-25 08:16:02", "link": "http://arxiv.org/abs/2106.13493v2", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluation of Deep-Learning-Based Voice Activity Detectors and Room\n  Impulse Response Models in Reverberant Environments", "abstract": "State-of-the-art deep-learning-based voice activity detectors (VADs) are\noften trained with anechoic data. However, real acoustic environments are\ngenerally reverberant, which causes the performance to significantly\ndeteriorate. To mitigate this mismatch between training data and real data, we\nsimulate an augmented training set that contains nearly five million\nutterances. This extension comprises of anechoic utterances and their\nreverberant modifications, generated by convolutions of the anechoic utterances\nwith a variety of room impulse responses (RIRs). We consider five different\nmodels to generate RIRs, and five different VADs that are trained with the\naugmented training set. We test all trained systems in three different real\nreverberant environments. Experimental results show $20\\%$ increase on average\nin accuracy, precision and recall for all detectors and response models,\ncompared to anechoic training. Furthermore, one of the RIR models consistently\nyields better performance than the other models, for all the tested VADs.\nAdditionally, one of the VADs consistently outperformed the other VADs in all\nexperiments.", "published": "2021-06-25 09:05:38", "link": "http://arxiv.org/abs/2106.13511v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Phoneme-aware and Channel-wise Attentive Learning for Text\n  DependentSpeaker Verification", "abstract": "This paper proposes a multi-task learning network with phoneme-aware and\nchannel-wise attentive learning strategies for text-dependent Speaker\nVerification (SV). In the proposed structure, the frame-level multi-task\nlearning along with the segment-level adversarial learning is adopted for\nspeaker embedding extraction. The phoneme-aware attentive pooling is exploited\non frame-level features in the main network for speaker classifier, with the\ncorresponding posterior probability for the phoneme distribution in the\nauxiliary subnet. Further, the introduction of Squeeze and Excitation\n(SE-block) performs dynamic channel-wise feature recalibration, which improves\nthe representational ability. The proposed method exploits speaker\nidiosyncrasies associated with pass-phrases, and is further improved by the\nphoneme-aware attentive pooling and SE-block from temporal and channel-wise\naspects, respectively. The experiments conducted on RSR2015 Part 1 database\nconfirm that the proposed system achieves outstanding results for textdependent\nSV.", "published": "2021-06-25 09:11:18", "link": "http://arxiv.org/abs/2106.13514v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Residual Echo Suppression with A Tunable Tradeoff Between Signal\n  Distortion and Echo Suppression", "abstract": "In this paper, we propose a residual echo suppression method using a UNet\nneural network that directly maps the outputs of a linear acoustic echo\ncanceler to the desired signal in the spectral domain. This system embeds a\ndesign parameter that allows a tunable tradeoff between the desired-signal\ndistortion and residual echo suppression in double-talk scenarios. The system\nemploys 136 thousand parameters, and requires 1.6 Giga floating-point\noperations per second and 10 Mega-bytes of memory. The implementation satisfies\nboth the timing requirements of the AEC challenge and the computational and\nmemory limitations of on-device applications. Experiments are conducted with\n161~h of data from the AEC challenge database and from real independent\nrecordings. We demonstrate the performance of the proposed system in real-life\nconditions and compare it with two competing methods regarding echo suppression\nand desired-signal distortion, generalization to various environments, and\nrobustness to high echo levels.", "published": "2021-06-25 09:49:18", "link": "http://arxiv.org/abs/2106.13531v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-Modal Knowledge Distillation Method for Automatic Cued Speech\n  Recognition", "abstract": "Cued Speech (CS) is a visual communication system for the deaf or hearing\nimpaired people. It combines lip movements with hand cues to obtain a complete\nphonetic repertoire. Current deep learning based methods on automatic CS\nrecognition suffer from a common problem, which is the data scarcity. Until\nnow, there are only two public single speaker datasets for French (238\nsentences) and British English (97 sentences). In this work, we propose a\ncross-modal knowledge distillation method with teacher-student structure, which\ntransfers audio speech information to CS to overcome the limited data problem.\nFirstly, we pretrain a teacher model for CS recognition with a large amount of\nopen source audio speech data, and simultaneously pretrain the feature\nextractors for lips and hands using CS data. Then, we distill the knowledge\nfrom teacher model to the student model with frame-level and sequence-level\ndistillation strategies. Importantly, for frame-level, we exploit multi-task\nlearning to weigh losses automatically, to obtain the balance coefficient.\nBesides, we establish a five-speaker British English CS dataset for the first\ntime. The proposed method is evaluated on French and British English CS\ndatasets, showing superior CS recognition performance to the state-of-the-art\n(SOTA) by a large margin.", "published": "2021-06-25 15:12:45", "link": "http://arxiv.org/abs/2106.13686v1", "categories": ["cs.MM", "cs.SD", "eess.AS", "eess.IV"], "primary_category": "cs.MM"}
{"title": "Nonlinear Acoustic Echo Cancellation with Deep Learning", "abstract": "We propose a nonlinear acoustic echo cancellation system, which aims to model\nthe echo path from the far-end signal to the near-end microphone in two parts.\nInspired by the physical behavior of modern hands-free devices, we first\nintroduce a novel neural network architecture that is specifically designed to\nmodel the nonlinear distortions these devices induce between receiving and\nplaying the far-end signal. To account for variations between devices, we\nconstruct this network with trainable memory length and nonlinear activation\nfunctions that are not parameterized in advance, but are rather optimized\nduring the training stage using the training data. Second, the network is\nsucceeded by a standard adaptive linear filter that constantly tracks the echo\npath between the loudspeaker output and the microphone. During training, the\nnetwork and filter are jointly optimized to learn the network parameters. This\nsystem requires 17 thousand parameters that consume 500 Million floating-point\noperations per second and 40 Kilo-bytes of memory. It also satisfies hands-free\ncommunication timing requirements on a standard neural processor, which renders\nit adequate for embedding on hands-free communication devices. Using 280 hours\nof real and synthetic data, experiments show advantageous performance compared\nto competing methods.", "published": "2021-06-25 16:44:51", "link": "http://arxiv.org/abs/2106.13754v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Voice Activity Detection for Transient Noisy Environment Based on\n  Diffusion Nets", "abstract": "We address voice activity detection in acoustic environments of transients\nand stationary noises, which often occur in real life scenarios. We exploit\nunique spatial patterns of speech and non-speech audio frames by independently\nlearning their underlying geometric structure. This process is done through a\ndeep encoder-decoder based neural network architecture. This structure involves\nan encoder that maps spectral features with temporal information to their\nlow-dimensional representations, which are generated by applying the diffusion\nmaps method. The encoder feeds a decoder that maps the embedded data back into\nthe high-dimensional space. A deep neural network, which is trained to separate\nspeech from non-speech frames, is obtained by concatenating the decoder to the\nencoder, resembling the known Diffusion nets architecture. Experimental results\nshow enhanced performance compared to competing voice activity detection\nmethods. The improvement is achieved in both accuracy, robustness and\ngeneralization ability. Our model performs in a real-time manner and can be\nintegrated into audio-based communication systems. We also present a batch\nalgorithm which obtains an even higher accuracy for off-line applications.", "published": "2021-06-25 17:05:26", "link": "http://arxiv.org/abs/2106.13763v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Transflower: probabilistic autoregressive dance generation with\n  multimodal attention", "abstract": "Dance requires skillful composition of complex movements that follow\nrhythmic, tonal and timbral features of music. Formally, generating dance\nconditioned on a piece of music can be expressed as a problem of modelling a\nhigh-dimensional continuous motion signal, conditioned on an audio signal. In\nthis work we make two contributions to tackle this problem. First, we present a\nnovel probabilistic autoregressive architecture that models the distribution\nover future poses with a normalizing flow conditioned on previous poses as well\nas music context, using a multimodal transformer encoder. Second, we introduce\nthe currently largest 3D dance-motion dataset, obtained with a variety of\nmotion-capture technologies, and including both professional and casual\ndancers. Using this dataset, we compare our new model against two baselines,\nvia objective metrics and a user study, and show that both the ability to model\na probability distribution, as well as being able to attend over a large motion\nand music context are necessary to produce interesting, diverse, and realistic\ndance that matches the music.", "published": "2021-06-25 20:14:28", "link": "http://arxiv.org/abs/2106.13871v3", "categories": ["cs.SD", "cs.GR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
