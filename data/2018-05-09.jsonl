{"title": "Character-level Chinese-English Translation through ASCII Encoding", "abstract": "Character-level Neural Machine Translation (NMT) models have recently\nachieved impressive results on many language pairs. They mainly do well for\nIndo-European language pairs, where the languages share the same writing\nsystem. However, for translating between Chinese and English, the gap between\nthe two different writing systems poses a major challenge because of a lack of\nsystematic correspondence between the individual linguistic units. In this\npaper, we enable character-level NMT for Chinese, by breaking down Chinese\ncharacters into linguistic units similar to that of Indo-European languages. We\nuse the Wubi encoding scheme, which preserves the original shape and semantic\ninformation of the characters, while also being reversible. We show promising\nresults from training Wubi-based models on the character- and subword-level\nwith recurrent as well as convolutional models.", "published": "2018-05-09 00:44:59", "link": "http://arxiv.org/abs/1805.03330v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LearningWord Embeddings for Low-resource Languages by PU Learning", "abstract": "Word embedding is a key component in many downstream applications in\nprocessing natural languages. Existing approaches often assume the existence of\na large collection of text for learning effective word embedding. However, such\na corpus may not be available for some low-resource languages. In this paper,\nwe study how to effectively learn a word embedding model on a corpus with only\na few million tokens. In such a situation, the co-occurrence matrix is sparse\nas the co-occurrences of many word pairs are unobserved. In contrast to\nexisting approaches often only sample a few unobserved word pairs as negative\nsamples, we argue that the zero entries in the co-occurrence matrix also\nprovide valuable information. We then design a Positive-Unlabeled Learning\n(PU-Learning) approach to factorize the co-occurrence matrix and validate the\nproposed approaches in four different languages.", "published": "2018-05-09 04:05:37", "link": "http://arxiv.org/abs/1805.03366v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Three tree priors and five datasets: A study of the effect of tree\n  priors in Indo-European phylogenetics", "abstract": "The age of the root of the Indo-European language family has received much\nattention since the application of Bayesian phylogenetic methods by Gray and\nAtkinson(2003). The root age of the Indo-European family has tended to decrease\nfrom an age that supported the Anatolian origin hypothesis to an age that\nsupports the Steppe origin hypothesis with the application of new models (Chang\net al., 2015). However, none of the published work in the Indo-European\nphylogenetics studied the effect of tree priors on phylogenetic analyses of the\nIndo-European family. In this paper, I intend to fill this gap by exploring the\neffect of tree priors on different aspects of the Indo-European family's\nphylogenetic inference. I apply three tree priors---Uniform, Fossilized\nBirth-Death (FBD), and Coalescent---to five publicly available datasets of the\nIndo-European language family. I evaluate the posterior distribution of the\ntrees from the Bayesian analysis using Bayes Factor, and find that there is\nsupport for the Steppe origin hypothesis in the case of two tree priors. I\nreport the median and 95% highest posterior density (HPD) interval of the root\nages for all the three tree priors. A model comparison suggested that either\nUniform prior or FBD prior is more suitable than the Coalescent prior to the\ndatasets belonging to the Indo-European language family.", "published": "2018-05-09 07:56:06", "link": "http://arxiv.org/abs/1805.03645v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Article Commenting: the Task and Dataset", "abstract": "Comments of online articles provide extended views and improve user\nengagement. Automatically making comments thus become a valuable functionality\nfor online forums, intelligent chatbots, etc. This paper proposes the new task\nof automatic article commenting, and introduces a large-scale Chinese dataset\nwith millions of real comments and a human-annotated subset characterizing the\ncomments' varying quality. Incorporating the human bias of comment quality, we\nfurther develop automatic metrics that generalize a broad set of popular\nreference-based metrics and exhibit greatly improved correlations with human\nevaluations.", "published": "2018-05-09 18:00:15", "link": "http://arxiv.org/abs/1805.03668v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Incorporating Subword Information into Matrix Factorization Word\n  Embeddings", "abstract": "The positive effect of adding subword information to word embeddings has been\ndemonstrated for predictive models. In this paper we investigate whether\nsimilar benefits can also be derived from incorporating subwords into counting\nmodels. We evaluate the impact of different types of subwords (n-grams and\nunsupervised morphemes), with results confirming the importance of subword\ninformation in learning representations of rare and out-of-vocabulary words.", "published": "2018-05-09 19:54:58", "link": "http://arxiv.org/abs/1805.03710v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation Decoding with Terminology Constraints", "abstract": "Despite the impressive quality improvements yielded by neural machine\ntranslation (NMT) systems, controlling their translation output to adhere to\nuser-provided terminology constraints remains an open problem. We describe our\napproach to constrained neural decoding based on finite-state machines and\nmulti-stack decoding which supports target-side constraints as well as\nconstraints with corresponding aligned input text spans. We demonstrate the\nperformance of our framework on multiple translation tasks and motivate the\nneed for constrained decoding with attentions as a means of reducing\nmisplacement and duplication when translating user constraints.", "published": "2018-05-09 22:47:30", "link": "http://arxiv.org/abs/1805.03750v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive User-Oriented Direct Load-Control of Residential Flexible\n  Devices", "abstract": "Demand Response (DR) schemes are effective tools to maintain a dynamic\nbalance in energy markets with higher integration of fluctuating renewable\nenergy sources. DR schemes can be used to harness residential devices'\nflexibility and to utilize it to achieve social and financial objectives.\nHowever, existing DR schemes suffer from low user participation as they fail at\ntaking into account the users' requirements. First, DR schemes are highly\ndemanding for the users, as users need to provide direct information, e.g. via\nsurveys, on their energy consumption preferences. Second, the user utility\nmodels based on these surveys are hard-coded and do not adapt over time. Third,\nthe existing scheduling techniques require the users to input their energy\nrequirements on a daily basis. As an alternative, this paper proposes a DR\nscheme for user-oriented direct load-control of residential appliances\noperations. Instead of relying on user surveys to evaluate the user utility, we\npropose an online data-driven approach for estimating user utility functions,\npurely based on available load consumption data, that adaptively models the\nusers' preference over time. Our scheme is based on a day-ahead scheduling\ntechnique that transparently prescribes the users with optimal device operation\nschedules that take into account both financial benefits and user-perceived\nquality of service. To model day-ahead user energy demand and flexibility, we\npropose a probabilistic approach for generating flexibility models under\nuncertainty. Results on both real-world and simulated datasets show that our DR\nscheme can provide significant financial benefits while preserving the\nuser-perceived quality of service.", "published": "2018-05-09 12:36:04", "link": "http://arxiv.org/abs/1805.05470v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Improving End-of-turn Detection in Spoken Dialogues by Detecting Speaker\n  Intentions as a Secondary Task", "abstract": "This work focuses on the use of acoustic cues for modeling turn-taking in\ndyadic spoken dialogues. Previous work has shown that speaker intentions (e.g.,\nasking a question, uttering a backchannel, etc.) can influence turn-taking\nbehavior and are good predictors of turn-transitions in spoken dialogues.\nHowever, speaker intentions are not readily available for use by automated\nsystems at run-time; making it difficult to use this information to anticipate\na turn-transition. To this end, we propose a multi-task neural approach for\npredicting turn- transitions and speaker intentions simultaneously. Our results\nshow that adding the auxiliary task of speaker intention prediction improves\nthe performance of turn-transition prediction in spoken dialogues, without\nrelying on additional input features during run-time.", "published": "2018-05-09 05:38:14", "link": "http://arxiv.org/abs/1805.06511v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Opinion Fraud Detection via Neural Autoencoder Decision Forest", "abstract": "Online reviews play an important role in influencing buyers' daily purchase\ndecisions. However, fake and meaningless reviews, which cannot reflect users'\ngenuine purchase experience and opinions, widely exist on the Web and pose\ngreat challenges for users to make right choices. Therefore,it is desirable to\nbuild a fair model that evaluates the quality of products by distinguishing\nspamming reviews. We present an end-to-end trainable unified model to leverage\nthe appealing properties from Autoencoder and random forest. A stochastic\ndecision tree model is implemented to guide the global parameter learning\nprocess. Extensive experiments were conducted on a large Amazon review dataset.\nThe proposed model consistently outperforms a series of compared methods.", "published": "2018-05-09 05:44:19", "link": "http://arxiv.org/abs/1805.03379v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Decoding Decoders: Finding Optimal Representation Spaces for\n  Unsupervised Similarity Tasks", "abstract": "Experimental evidence indicates that simple models outperform complex deep\nnetworks on many unsupervised similarity tasks. We provide a simple yet\nrigorous explanation for this behaviour by introducing the concept of an\noptimal representation space, in which semantically close symbols are mapped to\nrepresentations that are close under a similarity measure induced by the\nmodel's objective function. In addition, we present a straightforward procedure\nthat, without any retraining or architectural modifications, allows deep\nrecurrent models to perform equally well (and sometimes better) when compared\nto shallow models. To validate our analysis, we conduct a set of consistent\nempirical evaluations and introduce several new sentence embedding models in\nthe process. Even though this work is presented within the context of natural\nlanguage processing, the insights are readily applicable to other domains that\nrely on distributed representations for transfer tasks.", "published": "2018-05-09 09:41:51", "link": "http://arxiv.org/abs/1805.03435v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for\n  Abstractive Text Summarization", "abstract": "In this paper, we propose a deep learning approach to tackle the automatic\nsummarization tasks by incorporating topic information into the convolutional\nsequence-to-sequence (ConvS2S) model and using self-critical sequence training\n(SCST) for optimization. Through jointly attending to topics and word-level\nalignment, our approach can improve coherence, diversity, and informativeness\nof generated summaries via a biased probability generation mechanism. On the\nother hand, reinforcement training, like SCST, directly optimizes the proposed\nmodel with respect to the non-differentiable metric ROUGE, which also avoids\nthe exposure bias during inference. We carry out the experimental evaluation\nwith state-of-the-art methods over the Gigaword, DUC-2004, and LCSTS datasets.\nThe empirical results demonstrate the superiority of our proposed method in the\nabstractive summarization.", "published": "2018-05-09 16:56:41", "link": "http://arxiv.org/abs/1805.03616v3", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "On the Limitations of Unsupervised Bilingual Dictionary Induction", "abstract": "Unsupervised machine translation---i.e., not assuming any cross-lingual\nsupervision signal, whether a dictionary, translations, or comparable\ncorpora---seems impossible, but nevertheless, Lample et al. (2018) recently\nproposed a fully unsupervised machine translation (MT) model. The model relies\nheavily on an adversarial, unsupervised alignment of word embedding spaces for\nbilingual dictionary induction (Conneau et al., 2018), which we examine here.\nOur results identify the limitations of current unsupervised MT: unsupervised\nbilingual dictionary induction performs much worse on morphologically rich\nlanguages that are not dependent marking, when monolingual corpora from\ndifferent domains or different embedding algorithms are used. We show that a\nsimple trick, exploiting a weak supervision signal from identical words,\nenables more robust induction, and establish a near-perfect correlation between\nunsupervised bilingual dictionary induction performance and a previously\nunexplored graph similarity metric.", "published": "2018-05-09 17:08:03", "link": "http://arxiv.org/abs/1805.03620v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Adversarial Contrastive Estimation", "abstract": "Learning by contrasting positive and negative samples is a general strategy\nadopted by many methods. Noise contrastive estimation (NCE) for word embeddings\nand translating embeddings for knowledge graphs are examples in NLP employing\nthis approach. In this work, we view contrastive learning as an abstraction of\nall such methods and augment the negative sampler into a mixture distribution\ncontaining an adversarially learned sampler. The resulting adaptive sampler\nfinds harder negative examples, which forces the main model to learn a better\nrepresentation of the data. We evaluate our proposal on learning word\nembeddings, order embeddings and knowledge graph embeddings and observe both\nfaster convergence and improved results on multiple metrics.", "published": "2018-05-09 04:06:30", "link": "http://arxiv.org/abs/1805.03642v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Long Short-Term Memory as a Dynamically Computed Element-wise Weighted\n  Sum", "abstract": "LSTMs were introduced to combat vanishing gradients in simple RNNs by\naugmenting them with gated additive recurrent connections. We present an\nalternative view to explain the success of LSTMs: the gates themselves are\nversatile recurrent models that provide more representational power than\npreviously appreciated. We do this by decoupling the LSTM's gates from the\nembedded simple RNN, producing a new class of RNNs where the recurrence\ncomputes an element-wise weighted sum of context-independent functions of the\ninput. Ablations on a range of problems demonstrate that the gating mechanism\nalone performs as well as an LSTM in most settings, strongly suggesting that\nthe gates are doing much more in practice than just alleviating vanishing\ngradients.", "published": "2018-05-09 20:05:58", "link": "http://arxiv.org/abs/1805.03716v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Speaker Recognition using Deep Belief Networks", "abstract": "Short time spectral features such as mel frequency cepstral\ncoefficients(MFCCs) have been previously deployed in state of the art speaker\nrecognition systems, however lesser heed has been paid to short term spectral\nfeatures that can be learned by generative learning models from speech signals.\nHigher dimensional encoders such as deep belief networks (DBNs) could improve\nperformance in speaker recognition tasks by better modelling the statistical\nstructure of sound waves. In this paper, we use short term spectral features\nlearnt from the DBN augmented with MFCC features to perform the task of speaker\nrecognition. Using our features, we achieved a recognition accuracy of 0.95 as\ncompared to 0.90 when using standalone MFCC features on the ELSDSR dataset.", "published": "2018-05-09 14:34:40", "link": "http://arxiv.org/abs/1805.08865v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "End-to-End Polyphonic Sound Event Detection Using Convolutional\n  Recurrent Neural Networks with Learned Time-Frequency Representation Input", "abstract": "Sound event detection systems typically consist of two stages: extracting\nhand-crafted features from the raw audio waveform, and learning a mapping\nbetween these features and the target sound events using a classifier.\nRecently, the focus of sound event detection research has been mostly shifted\nto the latter stage using standard features such as mel spectrogram as the\ninput for classifiers such as deep neural networks. In this work, we utilize\nend-to-end approach and propose to combine these two stages in a single deep\nneural network classifier. The feature extraction over the raw waveform is\nconducted by a feedforward layer block, whose parameters are initialized to\nextract the time-frequency representations. The feature extraction parameters\nare updated during training, resulting with a representation that is optimized\nfor the specific task. This feature extraction block is followed by (and\njointly trained with) a convolutional recurrent network, which has recently\ngiven state-of-the-art results in many sound recognition tasks. The proposed\nsystem does not outperform a convolutional recurrent network with fixed\nhand-crafted features. The final magnitude spectrum characteristics of the\nfeature extraction block parameters indicate that the most relevant information\nfor the given task is contained in 0 - 3 kHz frequency range, and this is also\nsupported by the empirical results on the SED performance.", "published": "2018-05-09 15:10:57", "link": "http://arxiv.org/abs/1805.03647v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
