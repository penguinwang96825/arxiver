{"title": "NLPDove at SemEval-2020 Task 12: Improving Offensive Language Detection\n  with Cross-lingual Transfer", "abstract": "This paper describes our approach to the task of identifying offensive\nlanguages in a multilingual setting. We investigate two data augmentation\nstrategies: using additional semi-supervised labels with different thresholds\nand cross-lingual transfer with data selection. Leveraging the semi-supervised\ndataset resulted in performance improvements compared to the baseline trained\nsolely with the manually-annotated dataset. We propose a new metric,\nTranslation Embedding Distance, to measure the transferability of instances for\ncross-lingual data selection. We also introduce various preprocessing steps\ntailored for social media text along with methods to fine-tune the pre-trained\nmultilingual BERT (mBERT) for offensive language identification. Our\nmultilingual systems achieved competitive results in Greek, Danish, and Turkish\nat OffensEval 2020.", "published": "2020-08-04 06:20:50", "link": "http://arxiv.org/abs/2008.01354v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey of Orthographic Information in Machine Translation", "abstract": "Machine translation is one of the applications of natural language processing\nwhich has been explored in different languages. Recently researchers started\npaying attention towards machine translation for resource-poor languages and\nclosely related languages. A widespread and underlying problem for these\nmachine translation systems is the variation in orthographic conventions which\ncauses many issues to traditional approaches. Two languages written in two\ndifferent orthographies are not easily comparable, but orthographic information\ncan also be used to improve the machine translation system. This article offers\na survey of research regarding orthography's influence on machine translation\nof under-resourced languages. It introduces under-resourced languages in terms\nof machine translation and how orthographic information can be utilised to\nimprove machine translation. We describe previous work in this area, discussing\nwhat underlying assumptions were made, and showing how orthographic knowledge\nimproves the performance of machine translation of under-resourced languages.\nWe discuss different types of machine translation and demonstrate a recent\ntrend that seeks to link orthographic information with well-established machine\ntranslation methods. Considerable attention is given to current efforts of\ncognates information at different levels of machine translation and the lessons\nthat can be drawn from this. Additionally, multilingual neural machine\ntranslation of closely related languages is given a particular focus in this\nsurvey. This article ends with a discussion of the way forward in machine\ntranslation with orthographic information, focusing on multilingual settings\nand bilingual lexicon induction.", "published": "2020-08-04 07:59:02", "link": "http://arxiv.org/abs/2008.01391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taking Notes on the Fly Helps BERT Pre-training", "abstract": "How to make unsupervised language pre-training more efficient and less\nresource-intensive is an important research direction in NLP. In this paper, we\nfocus on improving the efficiency of language pre-training methods through\nproviding better data utilization. It is well-known that in language data\ncorpus, words follow a heavy-tail distribution. A large proportion of words\nappear only very few times and the embeddings of rare words are usually poorly\noptimized. We argue that such embeddings carry inadequate semantic signals,\nwhich could make the data utilization inefficient and slow down the\npre-training of the entire model. To mitigate this problem, we propose Taking\nNotes on the Fly (TNF), which takes notes for rare words on the fly during\npre-training to help the model understand them when they occur next time.\nSpecifically, TNF maintains a note dictionary and saves a rare word's\ncontextual information in it as notes when the rare word occurs in a sentence.\nWhen the same rare word occurs again during training, the note information\nsaved beforehand can be employed to enhance the semantics of the current\nsentence. By doing so, TNF provides better data utilization since\ncross-sentence information is employed to cover the inadequate semantics caused\nby rare words in the sentences. We implement TNF on both BERT and ELECTRA to\ncheck its efficiency and effectiveness. Experimental results show that TNF's\ntraining time is $60\\%$ less than its backbone pre-training models when\nreaching the same performance. When trained with the same number of iterations,\nTNF outperforms its backbone methods on most of downstream tasks and the\naverage GLUE score. Source code is attached in the supplementary material.", "published": "2020-08-04 11:25:09", "link": "http://arxiv.org/abs/2008.01466v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete\n  Utterance Restoration", "abstract": "Dialogue systems in open domain have achieved great success due to the easily\nobtained single-turn corpus and the development of deep learning, but the\nmulti-turn scenario is still a challenge because of the frequent coreference\nand information omission. In this paper, we investigate the incomplete\nutterance restoration which has brought general improvement over multi-turn\ndialogue systems in recent studies. Meanwhile, jointly inspired by the\nautoregression for text generation and the sequence labeling for text editing,\nwe propose a novel semi autoregressive generator (SARG) with the high\nefficiency and flexibility. Moreover, experiments on two benchmarks show that\nour proposed model significantly outperforms the state-of-the-art models in\nterms of quality and inference speed.", "published": "2020-08-04 11:52:20", "link": "http://arxiv.org/abs/2008.01474v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Select, Extract and Generate: Neural Keyphrase Generation with\n  Layer-wise Coverage Attention", "abstract": "Natural language processing techniques have demonstrated promising results in\nkeyphrase generation. However, one of the major challenges in \\emph{neural}\nkeyphrase generation is processing long documents using deep neural networks.\nGenerally, documents are truncated before given as inputs to neural networks.\nConsequently, the models may miss essential points conveyed in the target\ndocument. To overcome this limitation, we propose \\emph{SEG-Net}, a neural\nkeyphrase generation model that is composed of two major components, (1) a\nselector that selects the salient sentences in a document and (2) an\nextractor-generator that jointly extracts and generates keyphrases from the\nselected sentences. SEG-Net uses Transformer, a self-attentive architecture, as\nthe basic building block with a novel \\emph{layer-wise} coverage attention to\nsummarize most of the points discussed in the document. The experimental\nresults on seven keyphrase generation benchmarks from scientific and web\ndocuments demonstrate that SEG-Net outperforms the state-of-the-art neural\ngenerative methods by a large margin.", "published": "2020-08-04 18:00:07", "link": "http://arxiv.org/abs/2008.01739v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automated Topical Component Extraction Using Neural Network Attention\n  Scores from Source-based Essay Scoring", "abstract": "While automated essay scoring (AES) can reliably grade essays at scale,\nautomated writing evaluation (AWE) additionally provides formative feedback to\nguide essay revision. However, a neural AES typically does not provide useful\nfeature representations for supporting AWE. This paper presents a method for\nlinking AWE and neural AES, by extracting Topical Components (TCs) representing\nevidence from a source text using the intermediate output of attention layers.\nWe evaluate performance using a feature-based AES requiring TCs. Results show\nthat performance is comparable whether using automatically or manually\nconstructed TCs for 1) representing essays as rubric-based features, 2) grading\nessays.", "published": "2020-08-04 20:13:51", "link": "http://arxiv.org/abs/2008.01809v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An improved Bayesian TRIE based model for SMS text normalization", "abstract": "Normalization of SMS text, commonly known as texting language, is being\npursued for more than a decade. A probabilistic approach based on the Trie data\nstructure was proposed in literature which was found to be better performing\nthan HMM based approaches proposed earlier in predicting the correct\nalternative for an out-of-lexicon word. However, success of the Trie based\napproach depends largely on how correctly the underlying probabilities of word\noccurrences are estimated. In this work we propose a structural modification to\nthe existing Trie-based model along with a novel training algorithm and\nprobability generation scheme. We prove two theorems on statistical properties\nof the proposed Trie and use them to claim that is an unbiased and consistent\nestimator of the occurrence probabilities of the words. We further fuse our\nmodel into the paradigm of noisy channel based error correction and provide a\nheuristic to go beyond a Damerau Levenshtein distance of one. We also run\nsimulations to support our claims and show superiority of the proposed scheme\nover previous works.", "published": "2020-08-04 03:01:23", "link": "http://arxiv.org/abs/2008.01297v2", "categories": ["cs.CL", "cs.DS"], "primary_category": "cs.CL"}
{"title": "Weakly Supervised Construction of ASR Systems with Massive Video Data", "abstract": "Building Automatic Speech Recognition (ASR) systems from scratch is\nsignificantly challenging, mostly due to the time-consuming and\nfinancially-expensive process of annotating a large amount of audio data with\ntranscripts. Although several unsupervised pre-training models have been\nproposed, applying such models directly might still be sub-optimal if more\nlabeled, training data could be obtained without a large cost. In this paper,\nwe present a weakly supervised framework for constructing ASR systems with\nmassive video data. As videos often contain human-speech audios aligned with\nsubtitles, we consider videos as an important knowledge source, and propose an\neffective approach to extract high-quality audios aligned with transcripts from\nvideos based on Optical Character Recognition (OCR). The underlying ASR model\ncan be fine-tuned to fit any domain-specific target training datasets after\nweakly supervised pre-training. Extensive experiments show that our framework\ncan easily produce state-of-the-art results on six public datasets for Mandarin\nspeech recognition.", "published": "2020-08-04 03:11:32", "link": "http://arxiv.org/abs/2008.01300v2", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued\n  Prediction", "abstract": "Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.", "published": "2020-08-04 07:21:36", "link": "http://arxiv.org/abs/2008.01377v4", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Prompt Agnostic Essay Scorer: A Domain Generalization Approach to\n  Cross-prompt Automated Essay Scoring", "abstract": "Cross-prompt automated essay scoring (AES) requires the system to use non\ntarget-prompt essays to award scores to a target-prompt essay. Since obtaining\na large quantity of pre-graded essays to a particular prompt is often difficult\nand unrealistic, the task of cross-prompt AES is vital for the development of\nreal-world AES systems, yet it remains an under-explored area of research.\nModels designed for prompt-specific AES rely heavily on prompt-specific\nknowledge and perform poorly in the cross-prompt setting, whereas current\napproaches to cross-prompt AES either require a certain quantity of labelled\ntarget-prompt essays or require a large quantity of unlabelled target-prompt\nessays to perform transfer learning in a multi-step manner. To address these\nissues, we introduce Prompt Agnostic Essay Scorer (PAES) for cross-prompt AES.\nOur method requires no access to labelled or unlabelled target-prompt data\nduring training and is a single-stage approach. PAES is easy to apply in\npractice and achieves state-of-the-art performance on the Automated Student\nAssessment Prize (ASAP) dataset.", "published": "2020-08-04 10:17:38", "link": "http://arxiv.org/abs/2008.01441v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"This is Houston. Say again, please\". The Behavox system for the\n  Apollo-11 Fearless Steps Challenge (phase II)", "abstract": "We describe the speech activity detection (SAD), speaker diarization (SD),\nand automatic speech recognition (ASR) experiments conducted by the Behavox\nteam for the Interspeech 2020 Fearless Steps Challenge (FSC-2). A relatively\nsmall amount of labeled data, a large variety of speakers and channel\ndistortions, specific lexicon and speaking style resulted in high error rates\non the systems which involved this data. In addition to approximately 36 hours\nof annotated NASA mission recordings, the organizers provided a much larger but\nunlabeled 19k hour Apollo-11 corpus that we also explore for semi-supervised\ntraining of ASR acoustic and language models, observing more than 17% relative\nword error rate improvement compared to training on the FSC-2 data only. We\nalso compare several SAD and SD systems to approach the most difficult tracks\nof the challenge (track 1 for diarization and ASR), where long 30-minute audio\nrecordings are provided for evaluation without segmentation or speaker\ninformation. For all systems, we report substantial performance improvements\ncompared to the FSC-2 baseline systems, and achieved a first-place ranking for\nSD and ASR and fourth-place for SAD in the challenge.", "published": "2020-08-04 13:18:28", "link": "http://arxiv.org/abs/2008.01504v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Word meaning in minds and machines", "abstract": "Machines have achieved a broad and growing set of linguistic competencies,\nthanks to recent progress in Natural Language Processing (NLP). Psychologists\nhave shown increasing interest in such models, comparing their output to\npsychological judgments such as similarity, association, priming, and\ncomprehension, raising the question of whether the models could serve as\npsychological theories. In this article, we compare how humans and machines\nrepresent the meaning of words. We argue that contemporary NLP systems are\nfairly successful models of human word similarity, but they fall short in many\nother respects. Current models are too strongly linked to the text-based\npatterns in large corpora, and too weakly linked to the desires, goals, and\nbeliefs that people express through words. Word meanings must also be grounded\nin perception and action and be capable of flexible combinations in ways that\ncurrent systems are not. We discuss more promising approaches to grounding NLP\nsystems and argue that they will be more successful with a more human-like,\nconceptual basis for word meaning.", "published": "2020-08-04 18:45:49", "link": "http://arxiv.org/abs/2008.01766v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Effective Transfer Learning for Identifying Similar Questions: Matching\n  User Questions to COVID-19 FAQs", "abstract": "People increasingly search online for answers to their medical questions but\nthe rate at which medical questions are asked online significantly exceeds the\ncapacity of qualified people to answer them. This leaves many questions\nunanswered or inadequately answered. Many of these questions are not unique,\nand reliable identification of similar questions would enable more efficient\nand effective question answering schema. COVID-19 has only exacerbated this\nproblem. Almost every government agency and healthcare organization has tried\nto meet the informational need of users by building online FAQs, but there is\nno way for people to ask their question and know if it is answered on one of\nthese pages. While many research efforts have focused on the problem of general\nquestion similarity, these approaches do not generalize well to domains that\nrequire expert knowledge to determine semantic similarity, such as the medical\ndomain. In this paper, we show how a double fine-tuning approach of pretraining\na neural network on medical question-answer pairs followed by fine-tuning on\nmedical question-question pairs is a particularly useful intermediate task for\nthe ultimate goal of determining medical question similarity. While other\npretraining tasks yield an accuracy below 78.7% on this task, our model\nachieves an accuracy of 82.6% with the same number of training examples, an\naccuracy of 80.0% with a much smaller training set, and an accuracy of 84.5%\nwhen the full corpus of medical question-answer data is used. We also describe\na currently live system that uses the trained model to match user questions to\nCOVID-related FAQs.", "published": "2020-08-04 18:20:04", "link": "http://arxiv.org/abs/2008.13546v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "I-AID: Identifying Actionable Information from Disaster-related Tweets", "abstract": "Social media plays a significant role in disaster management by providing\nvaluable data about affected people, donations and help requests. Recent\nstudies highlight the need to filter information on social media into\nfine-grained content labels. However, identifying useful information from\nmassive amounts of social media posts during a crisis is a challenging task. In\nthis paper, we propose I-AID, a multimodel approach to automatically categorize\ntweets into multi-label information types and filter critical information from\nthe enormous volume of social media data. I-AID incorporates three main\ncomponents: i) a BERT-based encoder to capture the semantics of a tweet and\nrepresent as a low-dimensional vector, ii) a graph attention network (GAT) to\napprehend correlations between tweets' words/entities and the corresponding\ninformation types, and iii) a Relation Network as a learnable distance metric\nto compute the similarity between tweets and their corresponding information\ntypes in a supervised way. We conducted several experiments on two real\npublicly-available datasets. Our results indicate that I-AID outperforms\nstate-of-the-art approaches in terms of weighted average F1 score by +6% and\n+4% on the TREC-IS dataset and COVID-19 Tweets, respectively.", "published": "2020-08-04 19:07:50", "link": "http://arxiv.org/abs/2008.13544v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Intra-class variation reduction of speaker representation in\n  disentanglement framework", "abstract": "In this paper, we propose an effective training strategy to ex-tract robust\nspeaker representations from a speech signal. Oneof the key challenges in\nspeaker recognition tasks is to learnlatent representations or embeddings\ncontaining solely speakercharacteristic information in order to be robust in\nterms of intra-speaker variations. By modifying the network architecture\ntogenerate both speaker-related and speaker-unrelated representa-tions, we\nexploit a learning criterion which minimizes the mu-tual information between\nthese disentangled embeddings. Wealso introduce an identity change loss\ncriterion which utilizes areconstruction error to different utterances spoken\nby the samespeaker. Since the proposed criteria reduce the variation ofspeaker\ncharacteristics caused by changes in background envi-ronment or spoken content,\nthe resulting embeddings of eachspeaker become more consistent. The\neffectiveness of the pro-posed method is demonstrated through two tasks;\ndisentangle-ment performance, and improvement of speaker recognition ac-curacy\ncompared to the baseline model on a benchmark dataset,VoxCeleb1. Ablation\nstudies also show the impact of each cri-terion on overall performance.", "published": "2020-08-04 05:55:49", "link": "http://arxiv.org/abs/2008.01348v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic Composition of Guitar Tabs by Transformers and Groove Modeling", "abstract": "Deep learning algorithms are increasingly developed for learning to compose\nmusic in the form of MIDI files. However, whether such algorithms work well for\ncomposing guitar tabs, which are quite different from MIDIs, remain relatively\nunexplored. To address this, we build a model for composing fingerstyle guitar\ntabs with Transformer-XL, a neural sequence model architecture. With this\nmodel, we investigate the following research questions. First, whether the\nneural net generates note sequences with meaningful note-string combinations,\nwhich is important for the guitar but not other instruments such as the piano.\nSecond, whether it generates compositions with coherent rhythmic groove,\ncrucial for fingerstyle guitar music. And, finally, how pleasant the composed\nmusic is in comparison to real, human-made compositions. Our work provides\npreliminary empirical evidence of the promise of deep learning for tab\ncomposition, and suggests areas for future study.", "published": "2020-08-04 09:34:33", "link": "http://arxiv.org/abs/2008.01431v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Expressive TTS Training with Frame and Style Reconstruction Loss", "abstract": "We propose a novel training strategy for Tacotron-based text-to-speech (TTS)\nsystem to improve the expressiveness of speech. One of the key challenges in\nprosody modeling is the lack of reference that makes explicit modeling\ndifficult. The proposed technique doesn't require prosody annotations from\ntraining data. It doesn't attempt to model prosody explicitly either, but\nrather encodes the association between input text and its prosody styles using\na Tacotron-based TTS framework. Our proposed idea marks a departure from the\nstyle token paradigm where prosody is explicitly modeled by a bank of prosody\nembeddings. The proposed training strategy adopts a combination of two\nobjective functions: 1) frame level reconstruction loss, that is calculated\nbetween the synthesized and target spectral features; 2) utterance level style\nreconstruction loss, that is calculated between the deep style features of\nsynthesized and target speech. The proposed style reconstruction loss is\nformulated as a perceptual loss to ensure that utterance level speech style is\ntaken into consideration during training. Experiments show that the proposed\ntraining strategy achieves remarkable performance and outperforms a\nstate-of-the-art baseline in both naturalness and expressiveness. To our best\nknowledge, this is the first study to incorporate utterance level perceptual\nquality as a loss function into Tacotron training for improved expressiveness.", "published": "2020-08-04 12:40:49", "link": "http://arxiv.org/abs/2008.01490v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MIRNet: Learning multiple identities representations in overlapped\n  speech", "abstract": "Many approaches can derive information about a single speaker's identity from\nthe speech by learning to recognize consistent characteristics of acoustic\nparameters. However, it is challenging to determine identity information when\nthere are multiple concurrent speakers in a given signal. In this paper, we\npropose a novel deep speaker representation strategy that can reliably extract\nmultiple speaker identities from an overlapped speech. We design a network that\ncan extract a high-level embedding that contains information about each\nspeaker's identity from a given mixture. Unlike conventional approaches that\nneed reference acoustic features for training, our proposed algorithm only\nrequires the speaker identity labels of the overlapped speech segments. We\ndemonstrate the effectiveness and usefulness of our algorithm in a speaker\nverification task and a speech separation system conditioned on the target\nspeaker embeddings obtained through the proposed method.", "published": "2020-08-04 16:55:14", "link": "http://arxiv.org/abs/2008.01698v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker dependent acoustic-to-articulatory inversion using real-time MRI\n  of the vocal tract", "abstract": "Acoustic-to-articulatory inversion (AAI) methods estimate articulatory\nmovements from the acoustic speech signal, which can be useful in several tasks\nsuch as speech recognition, synthesis, talking heads and language tutoring.\nMost earlier inversion studies are based on point-tracking articulatory\ntechniques (e.g. EMA or XRMB). The advantage of rtMRI is that it provides\ndynamic information about the full midsagittal plane of the upper airway, with\na high 'relative' spatial resolution. In this work, we estimated midsagittal\nrtMRI images of the vocal tract for speaker dependent AAI, using MGC-LSP\nspectral features as input. We applied FC-DNNs, CNNs and recurrent neural\nnetworks, and have shown that LSTMs are the most suitable for this task. As\nobjective evaluation we measured normalized MSE, Structural Similarity Index\n(SSIM) and its complex wavelet version (CW-SSIM). The results indicate that the\ncombination of FC-DNNs and LSTMs can achieve smooth generated MR images of the\nvocal tract, which are similar to the original MRI recordings (average CW-SSIM:\n0.94).", "published": "2020-08-04 04:23:03", "link": "http://arxiv.org/abs/2008.02098v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "The Jazz Transformer on the Front Line: Exploring the Shortcomings of\n  AI-composed Music through Quantitative Measures", "abstract": "This paper presents the Jazz Transformer, a generative model that utilizes a\nneural sequence model called the Transformer-XL for modeling lead sheets of\nJazz music. Moreover, the model endeavors to incorporate structural events\npresent in the Weimar Jazz Database (WJazzD) for inducing structures in the\ngenerated music. While we are able to reduce the training loss to a low value,\nour listening test suggests however a clear gap between the average ratings of\nthe generated and real compositions. We therefore go one step further and\nconduct a series of computational analysis of the generated compositions from\ndifferent perspectives. This includes analyzing the statistics of the pitch\nclass, grooving, and chord progression, assessing the structureness of the\nmusic with the help of the fitness scape plot, and evaluating the model's\nunderstanding of Jazz music through a MIREX-like continuation prediction task.\nOur work presents in an analytical manner why machine-generated music to date\nstill falls short of the artwork of humanity, and sets some goals for future\nwork on automatic composition to further pursue.", "published": "2020-08-04 03:32:59", "link": "http://arxiv.org/abs/2008.01307v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Timbre latent space: exploration and creative aspects", "abstract": "Recent studies show the ability of unsupervised models to learn invertible\naudio representations using Auto-Encoders. They enable high-quality sound\nsynthesis but a limited control since the latent spaces do not disentangle\ntimbre properties. The emergence of disentangled representations was studied in\nVariational Auto-Encoders (VAEs), and has been applied to audio. Using an\nadditional perceptual regularization can align such latent representation with\nthe previously established multi-dimensional timbre spaces, while allowing\ncontinuous inference and synthesis. Alternatively, some specific sound\nattributes can be learned as control variables while unsupervised dimensions\naccount for the remaining features. New possibilities for timbre manipulations\nare enabled with generative neural networks, although the exploration and the\ncreative use of their representations remain little. The following experiments\nare led in cooperation with two composers and propose new creative directions\nto explore latent sound synthesis of musical timbres, using specifically\ndesigned interfaces (Max/MSP, Pure Data) or mappings for descriptor-based\nsynthesis.", "published": "2020-08-04 07:08:04", "link": "http://arxiv.org/abs/2008.01370v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Neural Granular Sound Synthesis", "abstract": "Granular sound synthesis is a popular audio generation technique based on\nrearranging sequences of small waveform windows. In order to control the\nsynthesis, all grains in a given corpus are analyzed through a set of acoustic\ndescriptors. This provides a representation reflecting some form of local\nsimilarities across the grains. However, the quality of this grain space is\nbound by that of the descriptors. Its traversal is not continuously invertible\nto signal and does not render any structured temporality.\n  We demonstrate that generative neural networks can implement granular\nsynthesis while alleviating most of its shortcomings. We efficiently replace\nits audio descriptor basis by a probabilistic latent space learned with a\nVariational Auto-Encoder. In this setting the learned grain space is\ninvertible, meaning that we can continuously synthesize sound when traversing\nits dimensions. It also implies that original grains are not stored for\nsynthesis. Another major advantage of our approach is to learn structured paths\ninside this latent space by training a higher-level temporal embedding over\narranged grain sequences.\n  The model can be applied to many types of libraries, including pitched notes\nor unpitched drums and environmental noises. We report experiments on the\ncommon granular synthesis processes as well as novel ones such as conditional\nsampling and morphing.", "published": "2020-08-04 08:08:00", "link": "http://arxiv.org/abs/2008.01393v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Music SketchNet: Controllable Music Generation via Factorized\n  Representations of Pitch and Rhythm", "abstract": "Drawing an analogy with automatic image completion systems, we propose Music\nSketchNet, a neural network framework that allows users to specify partial\nmusical ideas guiding automatic music generation. We focus on generating the\nmissing measures in incomplete monophonic musical pieces, conditioned on\nsurrounding context, and optionally guided by user-specified pitch and rhythm\nsnippets. First, we introduce SketchVAE, a novel variational autoencoder that\nexplicitly factorizes rhythm and pitch contour to form the basis of our\nproposed model. Then we introduce two discriminative architectures,\nSketchInpainter and SketchConnector, that in conjunction perform the guided\nmusic completion, filling in representations for the missing measures\nconditioned on surrounding context and user-specified snippets. We evaluate\nSketchNet on a standard dataset of Irish folk music and compare with models\nfrom recent works. When used for music completion, our approach outperforms the\nstate-of-the-art both in terms of objective metrics and subjective listening\ntests. Finally, we demonstrate that our model can successfully incorporate\nuser-specified snippets during the generation process.", "published": "2020-08-04 02:49:57", "link": "http://arxiv.org/abs/2008.01291v1", "categories": ["cs.LG", "cs.MM", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Exact, Parallelizable Dynamic Time Warping Alignment with Linear Memory", "abstract": "Audio alignment is a fundamental preprocessing step in many MIR pipelines.\nFor two audio clips with M and N frames, respectively, the most popular\napproach, dynamic time warping (DTW), has O(MN) requirements in both memory and\ncomputation, which is prohibitive for frame-level alignments at reasonable\nrates. To address this, a variety of memory efficient algorithms exist to\napproximate the optimal alignment under the DTW cost. To our knowledge,\nhowever, no exact algorithms exist that are guaranteed to break the quadratic\nmemory barrier. In this work, we present a divide and conquer algorithm that\ncomputes the exact globally optimal DTW alignment using O(M+N) memory. Its\nruntime is still O(MN), trading off memory for a 2x increase in computation.\nHowever, the algorithm can be parallelized up to a factor of min(M, N) with the\nsame memory constraints, so it can still run more efficiently than the textbook\nversion with an adequate GPU. We use our algorithm to compute exact alignments\non a collection of orchestral music, which we use as ground truth to benchmark\nthe alignment accuracy of several popular approximate alignment schemes at\nscales that were not previously possible.", "published": "2020-08-04 15:00:33", "link": "http://arxiv.org/abs/2008.02734v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "cs.MM", "eess.AS", "H.5.5; H.3.3; F.2.1"], "primary_category": "cs.SD"}
