{"title": "A Study of Association Measures and their Combination for Arabic MWT\n  Extraction", "abstract": "Automatic Multi-Word Term (MWT) extraction is a very important issue to many\napplications, such as information retrieval, question answering, and text\ncategorization. Although many methods have been used for MWT extraction in\nEnglish and other European languages, few studies have been applied to Arabic.\nIn this paper, we propose a novel, hybrid method which combines linguistic and\nstatistical approaches for Arabic Multi-Word Term extraction. The main\ncontribution of our method is to consider contextual information and both\ntermhood and unithood for association measures at the statistical filtering\nstep. In addition, our technique takes into account the problem of MWT\nvariation in the linguistic filtering step. The performance of the proposed\nstatistical measure (NLC-value) is evaluated using an Arabic environment corpus\nby comparing it with some existing competitors. Experimental results show that\nour NLC-value measure outperforms the other ones in term of precision for both\nbi-grams and tri-grams.", "published": "2014-09-10 09:52:41", "link": "http://arxiv.org/abs/1409.3005v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words", "abstract": "This paper presents a new model of WordNet that is used to disambiguate the\ncorrect sense of polysemy word based on the clue words. The related words for\neach sense of a polysemy word as well as single sense word are referred to as\nthe clue words. The conventional WordNet organizes nouns, verbs, adjectives and\nadverbs together into sets of synonyms called synsets each expressing a\ndifferent concept. In contrast to the structure of WordNet, we developed a new\nmodel of WordNet that organizes the different senses of polysemy words as well\nas the single sense words based on the clue words. These clue words for each\nsense of a polysemy word as well as for single sense word are used to\ndisambiguate the correct meaning of the polysemy word in the given context\nusing knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word\ncan be a noun, verb, adjective or adverb.", "published": "2014-09-10 19:01:18", "link": "http://arxiv.org/abs/1409.3512v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequence to Sequence Learning with Neural Networks", "abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT'14 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM's performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.", "published": "2014-09-10 19:55:35", "link": "http://arxiv.org/abs/1409.3215v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "abstract": "It has always been a burden to the users of statistical topic models to\npredetermine the right number of topics, which is a key parameter of most topic\nmodels. Conventionally, automatic selection of this parameter is done through\neither statistical model selection (e.g., cross-validation, AIC, or BIC) or\nBayesian nonparametric models (e.g., hierarchical Dirichlet process). These\nmethods either rely on repeated runs of the inference algorithm to search\nthrough a large range of parameter values which does not suit the mining of big\ndata, or replace this parameter with alternative parameters that are less\nintuitive and still hard to be determined. In this paper, we explore to\n\"eliminate\" this parameter from a new perspective. We first present a\nnonparametric treatment of the PLSA model named nonparametric probabilistic\nlatent semantic analysis (nPLSA). The inference procedure of nPLSA allows for\nthe exploration and comparison of different numbers of topics within a single\nexecution, yet remains as simple as that of PLSA. This is achieved by\nsubstituting the parameter of the number of topics with an alternative\nparameter that is the minimal goodness of fit of a document. We show that the\nnew parameter can be further eliminated by two parameter-free treatments:\neither by monitoring the diversity among the discovered topics or by a weak\nsupervision from users in the form of an exemplar topic. The parameter-free\ntopic model finds the appropriate number of topics when the diversity among the\ndiscovered topics is maximized, or when the granularity of the discovered\ntopics matches the exemplar topic. Experiments on both synthetic and real data\nprove that the parameter-free topic model extracts topics with a comparable\nquality comparing to classical topic models with \"manual transmission\". The\nquality of the topics outperforms those extracted through classical Bayesian\nnonparametric models.", "published": "2014-09-10 08:41:35", "link": "http://arxiv.org/abs/1409.2993v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Collaborative Deep Learning for Recommender Systems", "abstract": "Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.", "published": "2014-09-10 03:05:22", "link": "http://arxiv.org/abs/1409.2944v2", "categories": ["cs.LG", "cs.CL", "cs.IR", "cs.NE", "stat.ML"], "primary_category": "cs.LG"}
