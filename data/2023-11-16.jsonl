{"title": "Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs", "abstract": "Resolving ambiguities through interaction is a hallmark of natural language,\nand modeling this behavior is a core challenge in crafting AI assistants. In\nthis work, we study such behavior in LMs by proposing a task-agnostic framework\nfor resolving ambiguity by asking users clarifying questions. Our framework\nbreaks down this objective into three subtasks: (1) determining when\nclarification is needed, (2) determining what clarifying question to ask, and\n(3) responding accurately with the new information gathered through\nclarification. We evaluate systems across three NLP applications: question\nanswering, machine translation and natural language inference. For the first\nsubtask, we present a novel uncertainty estimation approach, intent-sim, that\ndetermines the utility of querying for clarification by estimating the entropy\nover user intents. Our method consistently outperforms existing uncertainty\nestimation approaches at identifying predictions that will benefit from\nclarification. When only allowed to ask for clarification on 10% of examples,\nour system is able to double the performance gains over randomly selecting\nexamples to clarify. Furthermore, we find that intent-sim is robust,\ndemonstrating improvements across a wide range of NLP tasks and LMs. Together,\nour work lays foundation for studying clarifying interactions with LMs.", "published": "2023-11-16 00:18:50", "link": "http://arxiv.org/abs/2311.09469v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Personalized Jargon Identification for Enhanced Interdisciplinary\n  Communication", "abstract": "Scientific jargon can impede researchers when they read materials from other\ndomains. Current methods of jargon identification mainly use corpus-level\nfamiliarity indicators (e.g., Simple Wikipedia represents plain language).\nHowever, researchers' familiarity of a term can vary greatly based on their own\nbackground. We collect a dataset of over 10K term familiarity annotations from\n11 computer science researchers for terms drawn from 100 paper abstracts.\nAnalysis of this data reveals that jargon familiarity and information needs\nvary widely across annotators, even within the same sub-domain (e.g., NLP). We\ninvestigate features representing individual, sub-domain, and domain knowledge\nto predict individual jargon familiarity. We compare supervised and\nprompt-based approaches, finding that prompt-based methods including personal\npublications yields the highest accuracy, though zero-shot prompting provides a\nstrong baseline. This research offers insight into features and methods to\nintegrate personal data into scientific jargon identification.", "published": "2023-11-16 00:51:25", "link": "http://arxiv.org/abs/2311.09481v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SQATIN: Supervised Instruction Tuning Meets Question Answering for\n  Improved Dialogue NLU", "abstract": "Task-oriented dialogue (ToD) systems help users execute well-defined tasks\nacross a variety of domains (e.g., $\\textit{flight booking}$ or $\\textit{food\nordering}$), with their Natural Language Understanding (NLU) components being\ndedicated to the analysis of user utterances, predicting users' intents\n($\\textit{Intent Detection}$, ID) and extracting values for informational slots\n($\\textit{Value Extraction}$, VE). In most domains, labelled NLU data is\nscarce, making sample-efficient learning -- enabled with effective transfer\nparadigms -- paramount. In this work, we introduce SQATIN, a new framework for\ndialog NLU based on (i) instruction tuning and (ii) question-answering-based\nformulation of ID and VE tasks. According to the evaluation on established NLU\nbenchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially\nsurpassing the performance of current models based on standard fine-tuning\nobjectives in both in-domain training and cross-domain transfer. SQATIN yields\nparticularly large performance gains in cross-domain transfer, owing to the\nfact that our QA-based instruction tuning leverages similarities between\nnatural language descriptions of classes (i.e., slots and intents) across\ndomains.", "published": "2023-11-16 01:57:00", "link": "http://arxiv.org/abs/2311.09502v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tailoring with Targeted Precision: Edit-Based Agents for Open-Domain\n  Procedure Customization", "abstract": "How-to procedures, such as how to plant a garden, are now used by millions of\nusers, but sometimes need customizing to meet a user's specific needs, e.g.,\nplanting a garden without pesticides. Our goal is to measure and improve an\nLLM's ability to perform such customization. Our approach is to test several\nsimple multi-LLM-agent architectures for customization, as well as an\nend-to-end LLM, using a new evaluation set, called CustomPlans, of over 200\nWikiHow procedures each with a customization need. We find that a simple\narchitecture with two LLM agents used sequentially performs best, one that\nedits a generic how-to procedure and one that verifies its executability,\nsignificantly outperforming (10.5% absolute) an end-to-end prompted LLM. This\nsuggests that LLMs can be configured reasonably effectively for procedure\ncustomization. This also suggests that multi-agent editing architectures may be\nworth exploring further for other customization applications (e.g. coding,\ncreative writing) in the future.", "published": "2023-11-16 02:25:36", "link": "http://arxiv.org/abs/2311.09510v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sequencing Matters: A Generate-Retrieve-Generate Model for Building\n  Conversational Agents", "abstract": "This paper contains what the Georgetown InfoSense group has done in regard to\nsolving the challenges presented by TREC iKAT 2023. Our submitted runs\noutperform the median runs by a significant margin, exhibiting superior\nperformance in nDCG across various cut numbers and in overall success rate. Our\napproach uses a Generate-Retrieve-Generate method, which we've found to greatly\noutpace Retrieve-Then-Generate approaches for the purposes of iKAT. Our\nsolution involves the use of Large Language Models (LLMs) for initial answers,\nanswer grounding by BM25, passage quality filtering by logistic regression, and\nanswer generation by LLMs again. We leverage several purpose-built Language\nModels, including BERT, Chat-based, and text-to-transfer-based models, for text\nunderstanding, classification, generation, and summarization. The official\nresults of the TREC evaluation contradict our initial self-evaluation, which\nmay suggest that a decrease in the reliance on our retrieval and classification\nmethods is better. Nonetheless, our findings suggest that the sequence of\ninvolving these different components matters, where we see an essentiality of\nusing LLMs before using search engines.", "published": "2023-11-16 02:37:58", "link": "http://arxiv.org/abs/2311.09513v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GEE! Grammar Error Explanation with Large Language Models", "abstract": "Grammatical error correction tools are effective at correcting grammatical\nerrors in users' input sentences but do not provide users with \\textit{natural\nlanguage} explanations about their errors. Such explanations are essential for\nhelping users learn the language by gaining a deeper understanding of its\ngrammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we\npropose the task of grammar error explanation, where a system needs to provide\none-sentence explanations for each grammatical error in a pair of erroneous and\ncorrected sentences. We analyze the capability of GPT-4 in grammar error\nexplanation, and find that it only produces explanations for 60.2% of the\nerrors using one-shot prompting. To improve upon this performance, we develop a\ntwo-step pipeline that leverages fine-tuned and prompted large language models\nto perform structured atomic token edit extraction, followed by prompting GPT-4\nto generate explanations. We evaluate our pipeline on German and Chinese\ngrammar error correction data sampled from language learners with a wide range\nof proficiency levels. Human evaluation reveals that our pipeline produces\n93.9% and 98.0% correct explanations for German and Chinese data, respectively.\nTo encourage further research in this area, we will open-source our data and\ncode.", "published": "2023-11-16 02:45:47", "link": "http://arxiv.org/abs/2311.09517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Code to Improve In-context Learning for Semantic Parsing", "abstract": "In-context learning (ICL) is an appealing approach for semantic parsing due\nto its few-shot nature and improved generalization. However, learning to parse\nto rare domain-specific languages (DSLs) from just a few demonstrations is\nchallenging, limiting the performance of even the most capable LLMs. In this\nwork, we improve the effectiveness of ICL for semantic parsing by (1) using\ngeneral-purpose programming languages such as Python instead of DSLs, and (2)\naugmenting prompts with a structured domain description that includes, e.g.,\nthe available classes and functions. We show that both these changes\nsignificantly improve accuracy across three popular datasets. Combined, they\nlead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional\nsplit), nearly closing the performance gap between easier i.i.d.\\ and harder\ncompositional splits when used with a strong model, and reducing the need for a\nlarge number of demonstrations. We find that the resemblance of the target\nparse language to general-purpose code is a more important factor than the\nlanguage's popularity in pre-training corpora. Our findings provide an improved\nmethodology for building semantic parsers in the modern context of ICL with\nLLMs.", "published": "2023-11-16 02:50:06", "link": "http://arxiv.org/abs/2311.09519v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven\n  Negative Samples Generation", "abstract": "Ensuring factual consistency is crucial for natural language generation\ntasks, particularly in abstractive summarization, where preserving the\nintegrity of information is paramount. Prior works on evaluating factual\nconsistency of summarization often take the entailment-based approaches that\nfirst generate perturbed (factual inconsistent) summaries and then train a\nclassifier on the generated data to detect the factually inconsistencies during\ntesting time. However, previous approaches generating perturbed summaries are\neither of low coherence or lack error-type coverage. To address these issues,\nwe propose AMRFact, a framework that generates perturbed summaries using\nAbstract Meaning Representations (AMRs). Our approach parses factually\nconsistent summaries into AMR graphs and injects controlled factual\ninconsistencies to create negative examples, allowing for coherent factually\ninconsistent summaries to be generated with high error-type coverage.\nAdditionally, we present a data selection module NegFilter based on natural\nlanguage inference and BARTScore to ensure the quality of the generated\nnegative samples. Experimental results demonstrate our approach significantly\noutperforms previous systems on the AggreFact-SOTA benchmark, showcasing its\nefficacy in evaluating factuality of abstractive summarization.", "published": "2023-11-16 02:56:29", "link": "http://arxiv.org/abs/2311.09521v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Large Language Model Adaptation for Improved Grounding and\n  Citation Generation", "abstract": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage understanding and generation. However, one major issue towards their\nwidespread deployment in the real world is that they can generate\n\"hallucinated\" answers that are not factual. Towards this end, this paper\nfocuses on improving LLMs by grounding their responses in retrieved passages\nand by providing citations. We propose a new framework, AGREE, Adaptation for\nGRounding EnhancEment, that improves the grounding from a holistic perspective.\nOur framework tunes LLMs to selfground the claims in their responses and\nprovide accurate citations to retrieved documents. This tuning on top of the\npre-trained LLMs requires well-grounded responses (with citations) for paired\nqueries, for which we introduce a method that can automatically construct such\ndata from unlabeled queries. The selfgrounding capability of tuned LLMs further\ngrants them a test-time adaptation (TTA) capability that can actively retrieve\npassages to support the claims that have not been grounded, which iteratively\nimproves the responses of LLMs. Across five datasets and two LLMs, our results\nshow that the proposed tuningbased AGREE framework generates superior grounded\nresponses with more accurate citations compared to prompting-based approaches\nand post-hoc citing-based approaches", "published": "2023-11-16 03:22:25", "link": "http://arxiv.org/abs/2311.09533v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pregnant Questions: The Importance of Pragmatic Awareness in Maternal\n  Health Question Answering", "abstract": "Questions posed by information-seeking users often contain implicit false or\npotentially harmful assumptions. In a high-risk domain such as maternal and\ninfant health, a question-answering system must recognize these pragmatic\nconstraints and go beyond simply answering user questions, examining them in\ncontext to respond helpfully. To achieve this, we study assumptions and\nimplications, or pragmatic inferences, made when mothers ask questions about\npregnancy and infant care by collecting a dataset of 2,727 inferences from 500\nquestions across three diverse sources. We study how health experts naturally\naddress these inferences when writing answers, and illustrate that informing\nexisting QA pipelines with pragmatic inferences produces responses that are\nmore complete, mitigating the propagation of harmful beliefs.", "published": "2023-11-16 03:33:01", "link": "http://arxiv.org/abs/2311.09542v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Few-Shot Training Example Generators: A Case\n  Study in Fallacy Recognition", "abstract": "Recognizing fallacies is crucial for ensuring the quality and validity of\narguments across various domains. However, computational fallacy recognition\nfaces challenges due to the diverse genres, domains, and types of fallacies\nfound in datasets. This leads to a highly multi-class, and even multi-label,\nsetup with substantial class imbalance. In this study, we aim to enhance\nexisting models for fallacy recognition by incorporating additional context and\nby leveraging large language models to generate synthetic data, thus increasing\nthe representation of the infrequent classes. We experiment with GPT3.5 to\ngenerate synthetic examples and we examine the impact of prompt settings for\nthis. Moreover, we explore zero-shot and few-shot scenarios to evaluate the\neffectiveness of using the generated examples for training smaller models\nwithin a unified fallacy recognition framework. Furthermore, we analyze the\noverlap between the synthetic data and existing fallacy datasets. Finally, we\ninvestigate the usefulness of providing supplementary context for detecting\nfallacy types that need such context, e.g., diversion fallacies. Our evaluation\nresults demonstrate consistent improvements across fallacy types, datasets, and\ngenerators. The code and the synthetic datasets are all publicly available.", "published": "2023-11-16 04:17:47", "link": "http://arxiv.org/abs/2311.09552v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What if you said that differently?: How Explanation Formats Affect Human\n  Feedback Efficacy and User Perception", "abstract": "Eliciting feedback from end users of NLP models can be beneficial for\nimproving models. However, how should we present model responses to users so\nthey are most amenable to be corrected from user feedback? Further, what\nproperties do users value to understand and trust responses? We answer these\nquestions by analyzing the effect of rationales (or explanations) generated by\nQA models to support their answers. We specifically consider decomposed QA\nmodels that first extract an intermediate rationale based on a context and a\nquestion and then use solely this rationale to answer the question. A rationale\noutlines the approach followed by the model to answer the question. Our work\nconsiders various formats of these rationales that vary according to\nwell-defined properties of interest. We sample rationales from language models\nusing few-shot prompting for two datasets, and then perform two user studies.\nFirst, we present users with incorrect answers and corresponding rationales in\nvarious formats and ask them to provide natural language feedback to revise the\nrationale. We then measure the effectiveness of this feedback in patching these\nrationales through in-context learning. The second study evaluates how well\ndifferent rationale formats enable users to understand and trust model answers,\nwhen they are correct. We find that rationale formats significantly affect how\neasy it is (1) for users to give feedback for rationales, and (2) for models to\nsubsequently execute this feedback. In addition, formats with attributions to\nthe context and in-depth reasoning significantly enhance user-reported\nunderstanding and trust of model outputs.", "published": "2023-11-16 04:26:32", "link": "http://arxiv.org/abs/2311.09558v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in\n  Event Extraction", "abstract": "Event extraction has gained considerable interest due to its wide-ranging\napplications. However, recent studies draw attention to evaluation issues,\nsuggesting that reported scores may not accurately reflect the true\nperformance. In this work, we identify and address evaluation challenges,\nincluding inconsistency due to varying data assumptions or preprocessing steps,\nthe insufficiency of current evaluation frameworks that may introduce dataset\nor data split bias, and the low reproducibility of some previous approaches. To\naddress these challenges, we present TextEE, a standardized, fair, and\nreproducible benchmark for event extraction. TextEE comprises standardized data\npreprocessing scripts and splits for 16 datasets spanning eight diverse domains\nand includes 14 recent methodologies, conducting a comprehensive benchmark\nreevaluation. We also evaluate five varied large language models on our TextEE\nbenchmark and demonstrate how they struggle to achieve satisfactory\nperformance. Inspired by our reevaluation results and findings, we discuss the\nrole of event extraction in the current NLP era, as well as future challenges\nand insights derived from TextEE. We believe TextEE, the first standardized\ncomprehensive benchmarking tool, will significantly facilitate future event\nextraction research.", "published": "2023-11-16 04:43:03", "link": "http://arxiv.org/abs/2311.09562v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crafting In-context Examples according to LMs' Parametric Knowledge", "abstract": "In-context learning can improve the performances of knowledge-rich tasks such\nas question answering. In such scenarios, in-context examples trigger a\nlanguage model (LM) to surface information stored in its parametric knowledge.\nWe study how to better construct in-context example sets, based on whether the\nmodel is aware of the in-context examples. We identify 'known' examples, where\nmodels can correctly answer from their parametric knowledge, and 'unknown'\nones. Our experiments show that prompting with 'unknown' examples decreases the\nperformance, potentially as it encourages hallucination rather than searching\nfor its parametric knowledge. Constructing an in-context example set that\npresents both known and unknown information performs the best across diverse\nsettings. We perform analysis on three multi-answer question answering\ndatasets, which allows us to further study answer set ordering strategies based\non the LM's knowledge of each answer. Together, our study sheds light on how to\nbest construct in-context example sets for knowledge-rich tasks.", "published": "2023-11-16 05:30:07", "link": "http://arxiv.org/abs/2311.09579v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MMoE: Enhancing Multimodal Models with Mixtures of Multimodal\n  Interaction Experts", "abstract": "Advances in multimodal models have greatly improved how interactions relevant\nto various tasks are modeled. Today's multimodal models mainly focus on the\ncorrespondence between images and text, using this for tasks like image-text\nmatching. However, this covers only a subset of real-world interactions. Novel\ninteractions, such as sarcasm expressed through opposing spoken words and\ngestures or humor expressed through utterances and tone of voice, remain\nchallenging. In this paper, we introduce an approach to enhance multimodal\nmodels, which we call Multimodal Mixtures of Experts (MMoE). The key idea in\nMMoE is to train separate expert models for each type of multimodal\ninteraction, such as redundancy present in both modalities, uniqueness in one\nmodality, or synergy that emerges when both modalities are fused. On a sarcasm\ndetection task (MUStARD) and a humor detection task (URFUNNY), we obtain new\nstate-of-the-art results. MMoE is also able to be applied to various types of\nmodels to gain improvement.", "published": "2023-11-16 05:31:21", "link": "http://arxiv.org/abs/2311.09580v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocLens: Multi-aspect Fine-grained Evaluation for Medical Text\n  Generation", "abstract": "Medical text generation aims to assist with administrative work and highlight\nsalient information to support decision-making. To reflect the specific\nrequirements of medical text, in this paper, we propose a set of metrics to\nevaluate the completeness, conciseness, and attribution of the generated text\nat a fine-grained level. The metrics can be computed by various types of\nevaluators including instruction-following (both proprietary and open-source)\nand supervised entailment models. We demonstrate the effectiveness of the\nresulting framework, DocLens, with three evaluators on three tasks: clinical\nnote generation, radiology report summarization, and patient question\nsummarization. A comprehensive human study shows that DocLens exhibits\nsubstantially higher agreement with the judgments of medical experts than\nexisting metrics. The results also highlight the need to improve open-source\nevaluators and suggest potential directions.", "published": "2023-11-16 05:32:09", "link": "http://arxiv.org/abs/2311.09581v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LifeTox: Unveiling Implicit Toxicity in Life Advice", "abstract": "As large language models become increasingly integrated into daily life,\ndetecting implicit toxicity across diverse contexts is crucial. To this end, we\nintroduce LifeTox, a dataset designed for identifying implicit toxicity within\na broad range of advice-seeking scenarios. Unlike existing safety datasets,\nLifeTox comprises diverse contexts derived from personal experiences through\nopen-ended questions. Experiments demonstrate that RoBERTa fine-tuned on\nLifeTox matches or surpasses the zero-shot performance of large language models\nin toxicity classification tasks. These results underscore the efficacy of\nLifeTox in addressing the complex challenges inherent in implicit toxicity. We\nopen-sourced the\ndataset\\footnote{\\url{https://huggingface.co/datasets/mbkim/LifeTox}} and the\nLifeTox moderator family; 350M, 7B, and 13B.", "published": "2023-11-16 05:43:02", "link": "http://arxiv.org/abs/2311.09585v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Models (Mostly) Do Not Consider Emotion Triggers When\n  Predicting Emotion", "abstract": "Situations and events evoke emotions in humans, but to what extent do they\ninform the prediction of emotion detection models? This work investigates how\nwell human-annotated emotion triggers correlate with features that models\ndeemed salient in their prediction of emotions. First, we introduce a novel\ndataset EmoTrigger, consisting of 900 social media posts sourced from three\ndifferent datasets; these were annotated by experts for emotion triggers with\nhigh agreement. Using EmoTrigger, we evaluate the ability of large language\nmodels (LLMs) to identify emotion triggers, and conduct a comparative analysis\nof the features considered important for these tasks between LLMs and\nfine-tuned models. Our analysis reveals that emotion triggers are largely not\nconsidered salient features for emotion prediction models, instead there is\nintricate interplay between various features and the task of emotion detection.", "published": "2023-11-16 06:20:13", "link": "http://arxiv.org/abs/2311.09602v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Contradictory Reasoning Evaluation and Detection", "abstract": "In a plethora of recent work, large language models (LLMs) demonstrated\nimpressive reasoning ability, but many proposed downstream reasoning tasks only\nfocus on final answers. Two fundamental questions persist: 1) how consistent is\nthe reasoning, and 2) can models detect unreliable reasoning? In this paper, we\ninvestigate self-contradictory (Self-Contra) reasoning, where the model\nreasoning does not support its answers. To answer 1), we define and assess the\nSelf-Contra rate across three datasets and delve into finer-grained categories\nof Self-Contra reasoning. We find that LLMs often contradict themselves in\nreasoning tasks involving contextual information understanding or commonsense.\nThe model may generate correct answers by taking shortcuts in reasoning or\noverlooking contextual evidence, leading to compromised reasoning. For 2), we\ntask the state-of-the-art model GPT-4 with identifying Self-Contra reasoning\nand finer-grained fallacies. We find that finer-grained categories enhanced\ndetection can improve GPT-4's ability to detect Self-Contra. However, it is\nonly able to detect Self-Contra with a 52.2% F1 score, much lower compared to\n66.7% for humans. Our results indicate that current LLMs lack the robustness\nnecessary for reliable reasoning and we emphasize the urgent need for\nestablishing best practices in comprehensive reasoning evaluations beyond pure\nperformance-based metrics.", "published": "2023-11-16 06:22:17", "link": "http://arxiv.org/abs/2311.09603v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Measuring and Improving Attentiveness to Partial Inputs with\n  Counterfactuals", "abstract": "The inevitable appearance of spurious correlations in training datasets hurts\nthe generalization of NLP models on unseen data. Previous work has found that\ndatasets with paired inputs are prone to correlations between a specific part\nof the input (e.g., the hypothesis in NLI) and the label; consequently, models\ntrained only on those outperform chance. Are these correlations picked up by\nmodels trained on the full input data? To address this question, we propose a\nnew evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses\ncounterfactuals by replacing part of the input with its counterpart from a\ndifferent example (subject to some restrictions), expecting an attentive model\nto change its prediction. Using CAT, we systematically investigate established\nsupervised and in-context learning models on ten datasets spanning four tasks:\nnatural language inference, reading comprehension, paraphrase detection, and\nvisual & language reasoning. CAT reveals that reliance on such correlations is\nmainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive\nwith an increased number of demonstrations, while its accuracy on the test data\nimproves. Our results demonstrate that augmenting training or demonstration\ndata with counterfactuals is effective in improving models' attentiveness. We\nshow that models' attentiveness measured by CAT reveals different conclusions\nfrom solely measuring correlations in data.", "published": "2023-11-16 06:27:35", "link": "http://arxiv.org/abs/2311.09605v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GistScore: Learning Better Representations for In-Context Example\n  Selection with Gist Bottlenecks", "abstract": "In-context Learning (ICL) is the ability of Large Language Models (LLMs) to\nperform new tasks when conditioned on prompts comprising a few task examples.\nHowever, ICL performance can be critically sensitive to the choice of examples.\nTo dynamically select the best examples for every test input, we propose\nExample Gisting, a novel approach for training example encoders through\nsupervised fine-tuning with an attention bottleneck between the inputs and\noutputs. These gist models form the basis for GistScore, a novel metric for\nscoring and selecting informative examples. Further, we experiment with two\nvariations: (1) fine-tuning gist models for each dataset and (2) multi-task\ntraining a single model on a large collection of datasets. The latter can be\nused for new tasks out-of-the-box, enabling a training-free ICL pipeline.\nEvaluations with 21 datasets spanning 9 tasks and 8 diverse LLMs show that our\nfine-tuned models get state-of-the-art ICL performance with over 20% absolute\ngain over off-the-shelf retrievers and 5% over the best prior methods. Further,\nour multi-task model generalizes well to new tasks, datasets, and prompt\ntemplates. Selection using this model matches or outperforms prior methods\nwhile being three orders of magnitude faster than the strongest training-free\nbaseline.", "published": "2023-11-16 06:28:05", "link": "http://arxiv.org/abs/2311.09606v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Retrieval Augmentation and the Limitations of Language Model Training", "abstract": "Augmenting a language model (LM) with $k$-nearest neighbors ($k$NN) retrieval\non its training data alone can decrease its perplexity, though the underlying\nreasons for this remain elusive. In this work, we rule out one previously\nposited possibility -- the \"softmax bottleneck.\" We then create a new dataset\nto evaluate LM generalization ability in the setting where training data\ncontains additional information that is not causally relevant. This task is\nchallenging even for GPT-3.5 Turbo. We show that, for both GPT-2 and Mistral\n7B, $k$NN retrieval augmentation consistently improves performance in this\nsetting. Finally, to make $k$NN retrieval more accessible, we propose using a\nmulti-layer perceptron model that maps datastore keys to values as a drop-in\nreplacement for traditional retrieval. This reduces storage costs by over 25x.", "published": "2023-11-16 06:59:54", "link": "http://arxiv.org/abs/2311.09615v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Take One Step at a Time to Know Incremental Utility of Demonstration: An\n  Analysis on Reranking for Few-Shot In-Context Learning", "abstract": "In-Context Learning (ICL) is an emergent capability of Large Language Models\n(LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new\ntasks. Previous studies have shown that using LLMs' outputs as labels is\neffective in training models to select demonstrations. Such a label is expected\nto estimate utility of a demonstration in ICL; however, it has not been well\nunderstood how different labeling strategies affect results on target tasks.\nThis paper presents an analysis on different utility functions by focusing on\nLLMs' output probability given ground-truth output, and task-specific reward\ngiven LLMs' prediction. Unlike the previous work, we introduce a novel labeling\nmethod, incremental utility, which estimates how much incremental knowledge is\nbrought into the LLMs by a demonstration. We conduct experiments with\ninstruction-tuned LLMs on binary/multi-class classification, segmentation, and\ntranslation across Arabic, English, Finnish, Japanese, and Spanish. Our results\nshow that (1) the probability is effective when the probability values are\ndistributed across the whole value range (on the classification tasks), and (2)\nthe downstream metric is more robust when nuanced reward values are provided\nwith long outputs (on the segmentation and translation tasks). We then show\nthat the proposed incremental utility further helps ICL by contrasting how the\nLLMs perform with and without the demonstrations.", "published": "2023-11-16 07:03:54", "link": "http://arxiv.org/abs/2311.09619v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating In-Context Learning of Libraries for Code Generation", "abstract": "Contemporary Large Language Models (LLMs) exhibit a high degree of code\ngeneration and comprehension capability. A particularly promising area is their\nability to interpret code modules from unfamiliar libraries for solving\nuser-instructed tasks. Recent work has shown that large proprietary LLMs can\nlearn novel library usage in-context from demonstrations. These results raise\nseveral open questions: whether demonstrations of library usage is required,\nwhether smaller (and more open) models also possess such capabilities, etc. In\nthis work, we take a broader approach by systematically evaluating a diverse\narray of LLMs across three scenarios reflecting varying levels of domain\nspecialization to understand their abilities and limitations in generating code\nbased on libraries defined in-context. Our results show that even smaller\nopen-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding\nof novel code libraries based on specification presented in-context. Our\nfindings further reveal that LLMs exhibit a surprisingly high proficiency in\nlearning novel library modules even when provided with just natural language\ndescriptions or raw code implementations of the functions, which are often\ncheaper to obtain than demonstrations. Overall, our results pave the way for\nharnessing LLMs in more adaptable and dynamic coding environments.", "published": "2023-11-16 07:37:25", "link": "http://arxiv.org/abs/2311.09635v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Event Causality Is Key to Computational Story Understanding", "abstract": "Cognitive science and symbolic AI research suggest that event causality\nprovides vital information for story understanding. However, machine learning\nsystems for story understanding rarely employ event causality, partially due to\nthe lack of methods that reliably identify open-world causal event relations.\nLeveraging recent progress in large language models, we present the first\nmethod for event causality identification that leads to material improvements\nin computational story understanding. Our technique sets a new state of the art\non the COPES dataset (Wang et al., 2023) for causal event relation\nidentification. Further, in the downstream story quality evaluation task, the\nidentified causal relations lead to 3.6-16.6% relative improvement on\ncorrelation with human ratings. In the multimodal story video-text alignment\ntask, we attain 4.1-10.9% increase on Clip Accuracy and 4.2-13.5% increase on\nSentence IoU. The findings indicate substantial untapped potential for event\ncausality in computational story understanding. The codebase is at\nhttps://github.com/insundaycathy/Event-Causality-Extraction.", "published": "2023-11-16 07:59:12", "link": "http://arxiv.org/abs/2311.09648v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evolving Domain Adaptation of Pretrained Language Models for Text\n  Classification", "abstract": "Adapting pre-trained language models (PLMs) for time-series text\nclassification amidst evolving domain shifts (EDS) is critical for maintaining\naccuracy in applications like stance detection. This study benchmarks the\neffectiveness of evolving domain adaptation (EDA) strategies, notably\nself-training, domain-adversarial training, and domain-adaptive pretraining,\nwith a focus on an incremental self-training method. Our analysis across\nvarious datasets reveals that this incremental method excels at adapting PLMs\nto EDS, outperforming traditional domain adaptation techniques. These findings\nhighlight the importance of continually updating PLMs to ensure their\neffectiveness in real-world applications, paving the way for future research\ninto PLM robustness against the natural temporal evolution of language.", "published": "2023-11-16 08:28:00", "link": "http://arxiv.org/abs/2311.09661v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Wisdom of Partisan Crowds: Comparing Collective Intelligence in\n  Humans and LLM-based Agents", "abstract": "Human groups are able to converge on more accurate beliefs through\ndeliberation, even in the presence of polarization and partisan bias -- a\nphenomenon known as the \"wisdom of partisan crowds.\" Generated agents powered\nby Large Language Models (LLMs) are increasingly used to simulate human\ncollective behavior, yet few benchmarks exist for evaluating their dynamics\nagainst the behavior of human groups. In this paper, we examine the extent to\nwhich the wisdom of partisan crowds emerges in groups of LLM-based agents that\nare prompted to role-play as partisan personas (e.g., Democrat or Republican).\nWe find that they not only display human-like partisan biases, but also\nconverge to more accurate beliefs through deliberation as humans do. We then\nidentify several factors that interfere with convergence, including the use of\nchain-of-thought prompt and lack of details in personas. Conversely,\nfine-tuning on human data appears to enhance convergence. These findings show\nthe potential and limitations of LLM-based agents as a model of human\ncollective intelligence.", "published": "2023-11-16 08:30:15", "link": "http://arxiv.org/abs/2311.09665v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Where Do People Tell Stories Online? Story Detection Across Online\n  Communities", "abstract": "Story detection in online communities is a challenging task as stories are\nscattered across communities and interwoven with non-storytelling spans within\na single text. We address this challenge by building and releasing the\nStorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts\nand comments, a detailed codebook adapted to the social media context, and\nmodels to predict storytelling at the document and span levels. Our dataset is\nsampled from hundreds of popular English-language Reddit communities ranging\nacross 33 topic categories, and it contains fine-grained expert annotations,\nincluding binary story labels, story spans, and event spans. We evaluate a\nrange of detection methods using our data, and we identify the distinctive\ntextual features of online storytelling, focusing on storytelling spans. We\nilluminate distributional characteristics of storytelling on a large\ncommunity-centric social media platform, and we also conduct a case study on\nr/ChangeMyView, where storytelling is used as one of many persuasive\nstrategies, illustrating that our data and models can be used for both inter-\nand intra-community research. Finally, we discuss implications of our tools and\nanalyses for narratology and the study of online communities.", "published": "2023-11-16 08:42:26", "link": "http://arxiv.org/abs/2311.09675v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "R-Tuning: Instructing Large Language Models to Say `I Don't Know'", "abstract": "Large language models (LLMs) have revolutionized numerous domains with their\nimpressive performance but still face their challenges. A predominant issue is\nthe propensity for these models to generate non-existent facts, a concern\ntermed hallucination. Our research is motivated by the observation that\nprevious instruction tuning methods force the model to complete a sentence no\nmatter whether the model knows the knowledge or not. When the question is out\nof the parametric knowledge, it will try to make up something and fail to\nindicate when it lacks knowledge. In this paper, we present a new approach\ncalled Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized\nby first identifying the disparity in knowledge encompassed by pre-trained\nparameters compared to that of instruction tuning data. Then, we construct the\nrefusal-aware data based on the knowledge intersection, to tune LLMs to refrain\nfrom responding to questions beyond its parametric knowledge. Experimental\nresults demonstrate R-Tuning effectively improves a model's ability to answer\nknown questions and refrain from answering unknown questions. Furthermore, when\ntested on out-of-domain datasets, the refusal ability was found to be a\nmeta-skill that could be generalized to other tasks. Further analysis\nsurprisingly finds that learning the uncertainty results in better calibration\nand an improved ability to estimate the uncertainty than uncertainty-based\ntesting. Our code is available at https://github.com/shizhediao/R-Tuning.", "published": "2023-11-16 08:45:44", "link": "http://arxiv.org/abs/2311.09677v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Inducing Political Bias Allows Language Models Anticipate Partisan\n  Reactions to Controversies", "abstract": "Social media platforms are rife with politically charged discussions.\nTherefore, accurately deciphering and predicting partisan biases using Large\nLanguage Models (LLMs) is increasingly critical. In this study, we address the\nchallenge of understanding political bias in digitized discourse using LLMs.\nWhile traditional approaches often rely on finetuning separate models for each\npolitical faction, our work innovates by employing a singular,\ninstruction-tuned LLM to reflect a spectrum of political ideologies. We present\na comprehensive analytical framework, consisting of Partisan Bias Divergence\nAssessment and Partisan Class Tendency Prediction, to evaluate the model's\nalignment with real-world political ideologies in terms of stances, emotions,\nand moral foundations. Our findings reveal the model's effectiveness in\ncapturing emotional and moral nuances, albeit with some challenges in stance\ndetection, highlighting the intricacies and potential for refinement in NLP\ntools for politically sensitive contexts. This research contributes\nsignificantly to the field by demonstrating the feasibility and importance of\nnuanced political understanding in LLMs, particularly for applications\nrequiring acute awareness of political bias.", "published": "2023-11-16 08:57:53", "link": "http://arxiv.org/abs/2311.09687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness", "abstract": "Do larger and more performant models resolve NLP's longstanding robustness\nissues? We investigate this question using over 20 models of different sizes\nspanning different architectural choices and pretraining objectives. We conduct\nevaluations using (a) out-of-domain and challenge test sets, (b) behavioral\ntesting with CheckLists, (c) contrast sets, and (d) adversarial inputs. Our\nanalysis reveals that not all out-of-domain tests provide insight into\nrobustness. Evaluating with CheckLists and contrast sets shows significant gaps\nin model performance; merely scaling models does not make them adequately\nrobust. Finally, we point out that current approaches for adversarial\nevaluations of models are themselves problematic: they can be easily thwarted,\nand in their current forms, do not represent a sufficiently deep probe of model\nrobustness. We conclude that not only is the question of robustness in NLP as\nyet unresolved, but even some of the approaches to measure robustness need to\nbe reassessed.", "published": "2023-11-16 09:09:32", "link": "http://arxiv.org/abs/2311.09694v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fumbling in Babel: An Investigation into ChatGPT's Language\n  Identification Ability", "abstract": "ChatGPT has recently emerged as a powerful NLP tool that can carry out a\nvariety of tasks. However, the range of languages ChatGPT can handle remains\nlargely a mystery. To uncover which languages ChatGPT `knows', we investigate\nits language identification (LID) abilities. For this purpose, we compile\nBabel-670, a benchmark comprising 670 languages representing 24 language\nfamilies spoken in five continents. Languages in Babel-670 run the gamut from\nthe very high-resource to the very low-resource. We then study ChatGPT's (both\nGPT-3.5 and GPT-4) ability to (i) identify language names and language codes\n(ii) under zero- and few-shot conditions (iii) with and without provision of a\nlabel set. When compared to smaller finetuned LID tools, we find that ChatGPT\nlags behind. For example, it has poor performance on African languages. We\nconclude that current large language models would benefit from further\ndevelopment before they can sufficiently serve diverse communities.", "published": "2023-11-16 09:12:20", "link": "http://arxiv.org/abs/2311.09696v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Self-enhancement Multitask Framework for Unsupervised Aspect Category\n  Detection", "abstract": "Our work addresses the problem of unsupervised Aspect Category Detection\nusing a small set of seed words. Recent works have focused on learning\nembedding spaces for seed words and sentences to establish similarities between\nsentences and aspects. However, aspect representations are limited by the\nquality of initial seed words, and model performances are compromised by noise.\nTo mitigate this limitation, we propose a simple framework that automatically\nenhances the quality of initial seed words and selects high-quality sentences\nfor training instead of using the entire dataset. Our main concepts are to add\na number of seed words to the initial set and to treat the task of noise\nresolution as a task of augmenting data for a low-resource task. In addition,\nwe jointly train Aspect Category Detection with Aspect Term Extraction and\nAspect Term Polarity to further enhance performance. This approach facilitates\nshared representation learning, allowing Aspect Category Detection to benefit\nfrom the additional guidance offered by other tasks. Extensive experiments\ndemonstrate that our framework surpasses strong baselines on standard datasets.", "published": "2023-11-16 09:35:24", "link": "http://arxiv.org/abs/2311.09708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Ups and Downs of Large Language Model Inference with Vocabulary\n  Trimming by Language Heuristics", "abstract": "Deploying large language models (LLMs) encounters challenges due to intensive\ncomputational and memory requirements. Our research examines vocabulary\ntrimming (VT) inspired by restricting embedding entries to the language of\ninterest to bolster time and memory efficiency. While such modifications have\nbeen proven effective in tasks like machine translation, tailoring them to LLMs\ndemands specific modifications given the diverse nature of LLM applications. We\napply two language heuristics to trim the full vocabulary - Unicode-based\nscript filtering and corpus-based selection - to different LLM families and\nsizes. The methods are straightforward, interpretable, and easy to implement.\nIt is found that VT reduces the memory usage of small models by nearly 50% and\nhas an upper bound of 25% improvement in generation speed. Yet, we reveal the\nlimitations of these methods in that they do not perform consistently well for\neach language with diminishing returns in larger models.", "published": "2023-11-16 09:35:50", "link": "http://arxiv.org/abs/2311.09709v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regularized Conventions: Equilibrium Computation as a Model of Pragmatic\n  Reasoning", "abstract": "We present a model of pragmatic language understanding, where utterances are\nproduced and understood by searching for regularized equilibria of signaling\ngames. In this model (which we call ReCo, for Regularized Conventions),\nspeakers and listeners search for contextually appropriate utterance--meaning\nmappings that are both close to game-theoretically optimal conventions and\nclose to a shared, ''default'' semantics. By characterizing pragmatic\ncommunication as equilibrium search, we obtain principled sampling algorithms\nand formal guarantees about the trade-off between communicative success and\nnaturalness. Across several datasets capturing real and idealized human\njudgments about pragmatic implicatures, ReCo matches or improves upon\npredictions made by best response and rational speech act models of language\nunderstanding.", "published": "2023-11-16 09:42:36", "link": "http://arxiv.org/abs/2311.09712v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Evaluating the Integration of Reasoning and Action in LLM Agents with\n  Database Question Answering", "abstract": "This study introduces a new long-form database question answering dataset\ndesigned to evaluate how Large Language Models (LLMs) interact with a SQL\ninterpreter. The task necessitates LLMs to strategically generate multiple SQL\nqueries to retrieve sufficient data from a database, to reason with the\nacquired context, and to synthesize them into a comprehensive analytical\nnarrative. Our findings highlight that this task poses great challenges even\nfor the state-of-the-art GPT-4 model. We propose and evaluate two interaction\nstrategies, and provide a fine-grained analysis of the individual stages within\nthe interaction. A key discovery is the identification of two primary\nbottlenecks hindering effective interaction: the capacity for planning and the\nability to generate multiple SQL queries. To address the challenge of\naccurately assessing answer quality, we introduce a multi-agent evaluation\nframework that simulates the academic peer-review process, enhancing the\nprecision and reliability of our evaluations. This framework allows for a more\nnuanced understanding of the strengths and limitations of current LLMs in\ncomplex retrieval and reasoning tasks.", "published": "2023-11-16 09:55:07", "link": "http://arxiv.org/abs/2311.09721v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MOKA: Moral Knowledge Augmentation for Moral Event Extraction", "abstract": "News media often strive to minimize explicit moral language in news articles,\nyet most articles are dense with moral values as expressed through the reported\nevents themselves. However, values that are reflected in the intricate dynamics\namong participating entities and moral events are far more challenging for most\nNLP systems to detect, including LLMs. To study this phenomenon, we annotate a\nnew dataset, MORAL EVENTS, consisting of 5,494 structured event annotations on\n474 news articles by diverse US media across the political spectrum. We further\npropose MOKA, a moral event extraction framework with MOral Knowledge\nAugmentation, which leverages knowledge derived from moral words and moral\nscenarios to produce structural representations of morality-bearing events.\nExperiments show that MOKA outperforms competitive baselines across three moral\nevent understanding tasks. Further analysis shows even ostensibly nonpartisan\nmedia engage in the selective reporting of moral events. Our data and codebase\nare available at https://github.com/launchnlp/MOKA.", "published": "2023-11-16 10:04:49", "link": "http://arxiv.org/abs/2311.09733v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tracking the Newsworthiness of Public Documents", "abstract": "Journalists must find stories in huge amounts of textual data (e.g. leaks,\nbills, press releases) as part of their jobs: determining when and why text\nbecomes news can help us understand coverage patterns and help us build\nassistive tools. Yet, this is challenging because very few labelled links\nexist, language use between corpora is very different, and text may be covered\nfor a variety of reasons. In this work we focus on news coverage of local\npublic policy in the San Francisco Bay Area by the San Francisco Chronicle.\nFirst, we gather news articles, public policy documents and meeting recordings\nand link them using probabilistic relational modeling, which we show is a\nlow-annotation linking methodology that outperforms other retrieval-based\nbaselines. Second, we define a new task: newsworthiness prediction, to predict\nif a policy item will get covered. We show that different aspects of public\npolicy discussion yield different newsworthiness signals. Finally we perform\nhuman evaluation with expert journalists and show our systems identify policies\nthey consider newsworthy with 68% F1 and our coverage recommendations are\nhelpful with an 84% win-rate.", "published": "2023-11-16 10:05:26", "link": "http://arxiv.org/abs/2311.09734v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CARE: Extracting Experimental Findings From Clinical Literature", "abstract": "Extracting fine-grained experimental findings from literature can provide\ndramatic utility for scientific applications. Prior work has developed\nannotation schemas and datasets for limited aspects of this problem, failing to\ncapture the real-world complexity and nuance required. Focusing on biomedicine,\nthis work presents CARE -- a new IE dataset for the task of extracting clinical\nfindings. We develop a new annotation schema capturing fine-grained findings as\nn-ary relations between entities and attributes, which unifies phenomena\nchallenging for current IE systems such as discontinuous entity spans, nested\nrelations, variable arity n-ary relations and numeric results in a single\nschema. We collect extensive annotations for 700 abstracts from two sources:\nclinical trials and case reports. We also demonstrate the generalizability of\nour schema to the computer science and materials science domains. We benchmark\nstate-of-the-art IE systems on CARE, showing that even models such as GPT4\nstruggle. We release our resources to advance research on extracting and\naggregating literature findings.", "published": "2023-11-16 10:06:19", "link": "http://arxiv.org/abs/2311.09736v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Capturing Perspectives of Crowdsourced Annotators in Subjective Learning\n  Tasks", "abstract": "Supervised classification heavily depends on datasets annotated by humans.\nHowever, in subjective tasks such as toxicity classification, these annotations\noften exhibit low agreement among raters. Annotations have commonly been\naggregated by employing methods like majority voting to determine a single\nground truth label. In subjective tasks, aggregating labels will result in\nbiased labeling and, consequently, biased models that can overlook minority\nopinions. Previous studies have shed light on the pitfalls of label aggregation\nand have introduced a handful of practical approaches to tackle this issue.\nRecently proposed multi-annotator models, which predict labels individually per\nannotator, are vulnerable to under-determination for annotators with few\nsamples. This problem is exacerbated in crowdsourced datasets. In this work, we\npropose \\textbf{Annotator Aware Representations for Texts (AART)} for\nsubjective classification tasks. Our approach involves learning representations\nof annotators, allowing for exploration of annotation behaviors. We show the\nimprovement of our method on metrics that assess the performance on capturing\nindividual annotators' perspectives. Additionally, we demonstrate fairness\nmetrics to evaluate our model's equability of performance for marginalized\nannotators compared to others.", "published": "2023-11-16 10:18:32", "link": "http://arxiv.org/abs/2311.09743v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translation Aligned Sentence Embeddings for Turkish Language", "abstract": "Due to the limited availability of high quality datasets for training\nsentence embeddings in Turkish, we propose a training methodology and a regimen\nto develop a sentence embedding model. The central idea is simple but effective\n: is to fine-tune a pretrained encoder-decoder model in two consecutive stages,\nwhere the first stage involves aligning the embedding space with translation\npairs. Thanks to this alignment, the prowess of the main model can be better\nprojected onto the target language in a sentence embedding setting where it can\nbe fine-tuned with high accuracy in short duration with limited target language\ndataset.", "published": "2023-11-16 10:25:22", "link": "http://arxiv.org/abs/2311.09748v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Impact of Calibration Data in Post-training Quantization and\n  Pruning", "abstract": "Quantization and pruning form the foundation of compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nremarkable performance in a post-training setting. They rely upon calibration\ndata, a small set of unlabeled examples that are used to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of quantization and\npruning methods, datasets, tasks, and models. Surprisingly, we find substantial\nvariations in downstream task performance, contrasting existing work that\nsuggests a greater level of robustness to the calibration data. Finally, we\nmake a series of recommendations for the effective use of calibration data in\nLLM quantization and pruning.", "published": "2023-11-16 10:30:00", "link": "http://arxiv.org/abs/2311.09755v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "StorySparkQA: Expert-Annotated QA Pairs with Real-World Knowledge for\n  Children's Story-Based Learning", "abstract": "Interactive story reading is a common parent-child activity, where parents\nexpect to teach both language skills and real-world knowledge beyond the story.\nWhile increasing storytelling and reading systems have been developed for this\nactivity, they often fail to infuse real-world knowledge into the conversation.\nThis limitation can be attributed to the existing question-answering (QA)\ndatasets used for children's education, upon which the systems are built,\nfailing to capture the nuances of how education experts think when conducting\ninteractive story reading activities. To bridge this gap, we design an\nannotation framework, empowered by existing knowledge graph to capture experts'\nannotations and thinking process, and leverage this framework to construct\nStorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with\nreal-world knowledge. We conduct automated and human expert evaluations across\nvarious QA pair generation settings to demonstrate that our StorySparkQA can\neffectively support models in generating QA pairs that target real-world\nknowledge beyond story content. StorySparkQA is available at\nhttps://huggingface.co/datasets/NEU-HAI/StorySparkQA.", "published": "2023-11-16 10:30:26", "link": "http://arxiv.org/abs/2311.09756v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "OrchestraLLM: Efficient Orchestration of Language Models for Dialogue\n  State Tracking", "abstract": "Large language models (LLMs) have revolutionized the landscape of Natural\nLanguage Processing systems, but are computationally expensive. To reduce the\ncost without sacrificing performance, previous studies have explored various\napproaches to harness the potential of Small Language Models (SLMs) as\ncost-effective alternatives to their larger counterparts. Driven by findings\nthat SLMs and LLMs exhibit complementary strengths in a structured knowledge\nextraction task, this work presents a novel SLM/LLM routing framework designed\nto improve computational efficiency and enhance task performance. First,\nexemplar pools are created to represent the types of contexts where each LM\nprovides a more reliable answer, leveraging a sentence embedding fine-tuned so\nthat context similarity is close to dialogue state similarity. Then, during\ninference, the k-nearest exemplars to the testing instance are retrieved, and\nthe instance is routed according to majority vote. In dialogue state tracking\ntasks, the proposed routing framework enhances performance substantially\ncompared to relying solely on LLMs, while reducing the computational costs by\nover 50%.", "published": "2023-11-16 10:30:55", "link": "http://arxiv.org/abs/2311.09758v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Test-time Backdoor Mitigation for Black-Box Large Language Models with\n  Defensive Demonstrations", "abstract": "Existing studies in backdoor defense have predominantly focused on the\ntraining phase, overlooking the critical aspect of testing time defense. This\ngap becomes pronounced in the context of LLMs deployed as Web Services, which\ntypically offer only black-box access, rendering training-time defenses\nimpractical. To bridge this gap, this study critically examines the use of\ndemonstrations as a defense mechanism against backdoor attacks in black-box\nLLMs. We retrieve task-relevant demonstrations from a clean data pool and\nintegrate them with user queries during testing. This approach does not\nnecessitate modifications or tuning of the model, nor does it require insight\ninto the model's internal architecture. The alignment properties inherent in\nin-context learning play a pivotal role in mitigating the impact of backdoor\ntriggers, effectively recalibrating the behavior of compromised models. Our\nexperimental analysis demonstrates that this method robustly defends against\nboth instance-level and instruction-level backdoor attacks, outperforming\nexisting defense baselines across most evaluation scenarios.", "published": "2023-11-16 10:38:43", "link": "http://arxiv.org/abs/2311.09763v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores", "abstract": "Automatic evaluation of generated textual content presents an ongoing\nchallenge within the field of NLP. Given the impressive capabilities of modern\nlanguage models (LMs) across diverse NLP tasks, there is a growing trend to\nemploy these models in creating innovative evaluation metrics for automated\nassessment of generation tasks. This paper investigates a pivotal question: Do\nlanguage model-driven evaluation metrics inherently exhibit bias favoring texts\ngenerated by the same underlying language model? Specifically, we assess\nwhether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and\nGPTScore) demonstrate a favorable bias toward their respective underlying LMs\nin the context of summarization tasks. Our findings unveil a latent bias,\nparticularly pronounced when such evaluation metrics are used in a\nreference-free manner without leveraging gold summaries. These results\nunderscore that assessments provided by generative evaluation models can be\ninfluenced by factors beyond the inherent text quality, highlighting the\nnecessity of developing more reliable evaluation protocols in the future.", "published": "2023-11-16 10:43:26", "link": "http://arxiv.org/abs/2311.09766v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "To be or not to be? an exploration of continuously controllable prompt\n  engineering", "abstract": "As the use of large language models becomes more widespread, techniques like\nparameter-efficient fine-tuning and other methods for controlled generation are\ngaining traction for customizing models and managing their outputs. However,\nthe challenge of precisely controlling how prompts influence these models is an\narea ripe for further investigation. In response, we introduce ControlPE\n(Continuously Controllable Prompt Engineering). ControlPE enables finer\nadjustments to prompt effects, complementing existing prompt engineering, and\neffectively controls continuous targets. This approach harnesses the power of\nLoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting,\nenabling fine-tuned adjustments to the impact of prompts. Our methodology\ninvolves generating specialized datasets for prompt distillation, incorporating\nthese prompts into the LoRA model, and carefully adjusting LoRA merging weight\nto regulate the influence of prompts. This provides a dynamic and adaptable\ntool for prompt control. Through our experiments, we have validated the\npracticality and efficacy of ControlPE. It proves to be a promising solution\nfor control a variety of prompts, ranging from generating short responses\nprompts, refusal prompts to chain-of-thought prompts.", "published": "2023-11-16 10:55:29", "link": "http://arxiv.org/abs/2311.09773v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "More Samples or More Prompts? Exploring Effective In-Context Sampling\n  for LLM Few-Shot Prompt Engineering", "abstract": "While most existing works on LLM prompting techniques focus only on how to\nselect a better set of data samples inside one single prompt input (In-Context\nLearning or ICL), why can not we design and leverage multiple prompts together\nto further improve the LLM's performance? In this work, we propose In-Context\nSampling (ICS), a low-resource LLM prompting technique to produce confident\npredictions by optimizing the construction of multiple ICL prompt inputs.\nExtensive experiments with three open-source LLMs (FlanT5-XL, Mistral-7B, and\nMixtral-8x7B) on four NLI datasets (e-SNLI, Multi-NLI, ANLI, and Contract-NLI)\nand one QA dataset (CommonsenseQA) illustrate that ICS can consistently enhance\nLLMs' performance. An in-depth evaluation with three data similarity-based ICS\nstrategies suggests that these strategies can further elevate LLM's\nperformance, which sheds light on a new yet promising future research\ndirection.", "published": "2023-11-16 11:02:49", "link": "http://arxiv.org/abs/2311.09782v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FinanceMath: Knowledge-Intensive Math Reasoning in Finance Domains", "abstract": "We introduce FinanceMath, a novel benchmark designed to evaluate LLMs'\ncapabilities in solving knowledge-intensive math reasoning problems. Compared\nto prior works, this study features three core advancements. First, FinanceMath\nincludes 1,200 problems with a hybrid of textual and tabular content. These\nproblems require college-level knowledge in the finance domain for effective\nresolution. Second, we provide expert-annotated, detailed solution references\nin Python program format, ensuring a high-quality benchmark for LLM assessment.\nWe also construct a finance-domain knowledge bank and investigate various\nknowledge integration strategies. Finally, we evaluate a wide spectrum of 44\nLLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Our\nexperimental results reveal that the current best-performing system (i.e.,\nGPT-4o) achieves only 60.9% accuracy using CoT prompting, leaving substantial\nroom for improvement. Moreover, while augmenting LLMs with external knowledge\ncan improve model performance (e.g., from 47.5% to 54.5% for Gemini-1.5-Pro),\ntheir accuracy remains significantly lower than the estimated human expert\nperformance of 92%. We believe that FinanceMath can advance future research in\nthe area of domain-specific knowledge retrieval and integration, particularly\nwithin the context of solving reasoning-intensive tasks.", "published": "2023-11-16 11:22:08", "link": "http://arxiv.org/abs/2311.09797v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Far Can We Extract Diverse Perspectives from Large Language Models?", "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a\nrecent trend in exploiting large language models (LLMs) for generating diverse\ndata for potential scalable and efficient solutions. However, the extent to\nwhich LLMs can generate diverse perspectives on subjective topics is still\nunclear. In this study, we explore LLMs' capacity of generating diverse\nperspectives and rationales on subjective topics such as social norms and\nargumentative texts. We introduce the problem of extracting maximum diversity\nfrom LLMs. Motivated by how humans form opinions based on values, we propose a\ncriteria-based prompting technique to ground diverse opinions. To see how far\nwe can extract diverse perspectives from LLMs, or called diversity coverage, we\nemploy a step-by-step recall prompting to generate more outputs from the model\niteratively. Our methods, applied to various tasks, show that LLMs can indeed\nproduce diverse opinions according to the degree of task subjectivity. We also\nfind that LLM's performance of extracting maximum diversity is on par with\nhuman.", "published": "2023-11-16 11:23:38", "link": "http://arxiv.org/abs/2311.09799v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$\\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of\n  Information-Seeking Dialogue via Behavioural Fine-Tuning", "abstract": "Factuality is a crucial requirement in information seeking dialogue: the\nsystem should respond to the user's queries so that the responses are\nmeaningful and aligned with the knowledge provided to the system. However, most\nmodern large language models suffer from hallucinations, that is, they generate\nresponses not supported by or contradicting the knowledge source. To mitigate\nthe issue and increase faithfulness of information-seeking dialogue systems, we\nintroduce BeInfo, a simple yet effective method that applies behavioural tuning\nto aid information-seeking dialogue. Relying on three standard datasets, we\nshow that models tuned with BeInfo} become considerably more faithful to the\nknowledge source both for datasets and domains seen during BeInfo-tuning, as\nwell as on unseen domains, when applied in a zero-shot manner. In addition, we\nshow that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo\ndemonstrate strong performance on data from real `production' conversations and\noutperform GPT4 when tuned on a limited amount of such realistic in-domain\ndialogues.", "published": "2023-11-16 11:25:44", "link": "http://arxiv.org/abs/2311.09800v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in\n  Understanding Long and Specialized Documents", "abstract": "Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning capabilities of LLMs in the context of understanding and analyzing\nspecialized documents containing both text and tables. We conduct an extensive\nevaluation of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting\nmethods, aiming to comprehensively assess the capabilities and limitations of\nexisting LLMs in DocMath-Eval. We found that even the current best-performing\nsystem (i.e., GPT-4o) still significantly lags behind human experts in solving\ncomplex numerical reasoning problems grounded in long contexts. We believe that\nDocMath-Eval can serve as a valuable benchmark for evaluating LLMs'\ncapabilities in solving challenging numerical reasoning problems within expert\ndomains.", "published": "2023-11-16 11:30:53", "link": "http://arxiv.org/abs/2311.09805v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Curious Decline of Linguistic Diversity: Training Language Models on\n  Synthetic Text", "abstract": "This study investigates the consequences of training language models on\nsynthetic data generated by their predecessors, an increasingly prevalent\npractice given the prominence of powerful generative models. Diverging from the\nusual emphasis on performance metrics, we focus on the impact of this training\nmethodology on linguistic diversity, especially when conducted recursively over\ntime. To assess this, we adapt and develop a set of novel metrics targeting\nlexical, syntactic, and semantic diversity, applying them in recursive\nfinetuning experiments across various natural language generation tasks in\nEnglish. Our findings reveal a consistent decrease in the diversity of the\nmodel outputs through successive iterations, especially remarkable for tasks\ndemanding high levels of creativity. This trend underscores the potential risks\nof training language models on synthetic text, particularly concerning the\npreservation of linguistic richness. Our study highlights the need for careful\nconsideration of the long-term effects of such training approaches on the\nlinguistic capabilities of language models.", "published": "2023-11-16 11:31:50", "link": "http://arxiv.org/abs/2311.09807v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PixT3: Pixel-based Table-To-Text Generation", "abstract": "Table-to-text generation involves generating appropriate textual descriptions\ngiven structured tabular data. It has attracted increasing attention in recent\nyears thanks to the popularity of neural network models and the availability of\nlarge-scale datasets. A common feature across existing methods is their\ntreatment of the input as a string, i.e., by employing linearization techniques\nthat do not always preserve information in the table, are verbose, and lack\nspace efficiency. We propose to rethink data-to-text generation as a visual\nrecognition task, removing the need for rendering the input in a string format.\nWe present PixT3, a multimodal table-to-text model that overcomes the\nchallenges of linearization and input size limitations encountered by existing\nmodels. PixT3 is trained with a new self-supervised learning objective to\nreinforce table structure awareness and is applicable to open-ended and\ncontrolled generation settings. Experiments on the ToTTo and Logic2Text\nbenchmarks show that PixT3 is competitive and, in some settings, superior to\ngenerators that operate solely on text.", "published": "2023-11-16 11:32:47", "link": "http://arxiv.org/abs/2311.09808v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Large Language Models for Propaganda Span Annotation", "abstract": "The use of propagandistic techniques in online content has increased in\nrecent years aiming to manipulate online audiences. Fine-grained propaganda\ndetection and extraction of textual spans where propaganda techniques are used,\nare essential for more informed content consumption. Automatic systems\ntargeting the task over lower resourced languages are limited, usually\nobstructed by lack of large scale training datasets. Our study investigates\nwhether Large Language Models (LLMs), such as GPT-4, can effectively extract\npropagandistic spans. We further study the potential of employing the model to\ncollect more cost-effective annotations. Finally, we examine the effectiveness\nof labels provided by GPT-4 in training smaller language models for the task.\nThe experiments are performed over a large-scale in-house manually annotated\ndataset. The results suggest that providing more annotation context to GPT-4\nwithin prompts improves its performance compared to human annotators. Moreover,\nwhen serving as an expert annotator (consolidator), the model provides labels\nthat have higher agreement with expert annotators, and lead to specialized\nmodels that achieve state-of-the-art over an unseen Arabic testing set.\nFinally, our work is the first to show the potential of utilizing LLMs to\ndevelop annotated datasets for propagandistic spans detection task prompting it\nwith annotations from human annotators with limited expertise. All scripts and\nannotations will be shared with the community.", "published": "2023-11-16 11:37:54", "link": "http://arxiv.org/abs/2311.09812v3", "categories": ["cs.CL", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Towards Robust Temporal Reasoning of Large Language Models via a\n  Multi-Hop QA Dataset and Pseudo-Instruction Tuning", "abstract": "Knowledge in the real world is being updated constantly. However, it is\ncostly to frequently update large language models (LLMs). Therefore, it is\ncrucial for LLMs to understand the concept of temporal knowledge. However,\nprior works on temporal question answering (TQA) did not emphasize multi-answer\nand multi-hop types of temporal reasoning. In this paper, we propose a complex\ntemporal question-answering dataset Complex-TR that focuses on multi-answer and\nmulti-hop temporal reasoning. Besides, we also propose a novel data\naugmentation strategy to improve the complex temporal reasoning capability and\nrobustness of LLMs. We conducted experiments on multiple temporal QA datasets.\nExperimental results show that our method is able to improve LLMs' performance\non temporal QA benchmarks by significant margins. Our code and data are\nreleased at: https://github.com/nusnlp/complex-tr.", "published": "2023-11-16 11:49:29", "link": "http://arxiv.org/abs/2311.09821v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Human Still Wins over LLM: An Empirical Study of Active Learning on\n  Domain-Specific Annotation Tasks", "abstract": "Large Language Models (LLMs) have demonstrated considerable advances, and\nseveral claims have been made about their exceeding human performance. However,\nin real-world tasks, domain knowledge is often required. Low-resource learning\nmethods like Active Learning (AL) have been proposed to tackle the cost of\ndomain expert annotation, raising this question: Can LLMs surpass compact\nmodels trained with expert annotations in domain-specific tasks? In this work,\nwe conduct an empirical experiment on four datasets from three different\ndomains comparing SOTA LLMs with small models trained on expert annotations\nwith AL. We found that small models can outperform GPT-3.5 with a few hundreds\nof labeled data, and they achieve higher or similar performance with GPT-4\ndespite that they are hundreds time smaller. Based on these findings, we posit\nthat LLM predictions can be used as a warmup method in real-world applications\nand human experts remain indispensable in tasks involving data annotation\ndriven by domain-specific knowledge.", "published": "2023-11-16 11:51:13", "link": "http://arxiv.org/abs/2311.09825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded\n  Logical Thinking", "abstract": "While large language models (LLMs) have demonstrated increasing power, they\nhave also given rise to a wide range of harmful behaviors. As representatives,\njailbreak attacks can provoke harmful or unethical responses from LLMs, even\nafter safety alignment. In this paper, we investigate a novel category of\njailbreak attacks specifically designed to target the cognitive structure and\nprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs in\nthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)\neffect-to-cause reasoning. Different from previous jailbreak attacks, our\nproposed cognitive overload is a black-box attack with no need for knowledge of\nmodel architecture or access to model weights. Experiments conducted on\nAdvBench and MasterKey reveal that various LLMs, including both popular\nopen-source model Llama 2 and the proprietary model ChatGPT, can be compromised\nthrough cognitive overload. Motivated by cognitive psychology work on managing\ncognitive load, we further investigate defending cognitive overload attack from\ntwo perspectives. Empirical studies show that our cognitive overload from three\nperspectives can jailbreak all studied LLMs successfully, while existing\ndefense strategies can hardly mitigate the caused malicious uses effectively.", "published": "2023-11-16 11:52:22", "link": "http://arxiv.org/abs/2311.09827v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AfriMTE and AfriCOMET: Enhancing COMET to Embrace Under-resourced\n  African Languages", "abstract": "Despite the recent progress on scaling multilingual machine translation (MT)\nto several under-resourced African languages, accurately measuring this\nprogress remains challenging, since evaluation is often performed on n-gram\nmatching metrics such as BLEU, which typically show a weaker correlation with\nhuman judgments. Learned metrics such as COMET have higher correlation;\nhowever, the lack of evaluation data with human ratings for under-resourced\nlanguages, complexity of annotation guidelines like Multidimensional Quality\nMetrics (MQM), and limited language coverage of multilingual encoders have\nhampered their applicability to African languages. In this paper, we address\nthese challenges by creating high-quality human evaluation data with simplified\nMQM guidelines for error detection and direct assessment (DA) scoring for 13\ntypologically diverse African languages. Furthermore, we develop AfriCOMET:\nCOMET evaluation metrics for African languages by leveraging DA data from\nwell-resourced languages and an African-centric multilingual encoder\n(AfroXLM-R) to create the state-of-the-art MT evaluation metrics for African\nlanguages with respect to Spearman-rank correlation with human judgments\n(0.441).", "published": "2023-11-16 11:52:52", "link": "http://arxiv.org/abs/2311.09828v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FollowEval: A Multi-Dimensional Benchmark for Assessing the\n  Instruction-Following Capability of Large Language Models", "abstract": "The effective assessment of the instruction-following ability of large\nlanguage models (LLMs) is of paramount importance. A model that cannot adhere\nto human instructions might be not able to provide reliable and helpful\nresponses. In pursuit of this goal, various benchmarks have been constructed to\nevaluate the instruction-following capacity of these models. However, these\nbenchmarks are limited to a single language and are constructed using automated\napproaches, which restricts their applicability and the quality of the test\nexamples they contain. To bridge this gap, we introduce the FollowEval\nbenchmark in this paper. This benchmark is composed of instances in both\nEnglish and Chinese, and all test examples are crafted by human experts.\nFurthermore, the FollowEval benchmark is designed to assess LLMs across five\ncritical dimensions of instruction following: string manipulation, commonsense\nreasoning, logical reasoning, spatial reasoning, and response constraints. To\nenhance the complexity and present a sufficient challenge, each test example is\ndesigned to evaluate more than one dimension. We have evaluated various LLMs\nusing the FollowEval benchmark and found that their performance significantly\nlags behind that of humans. This highlights the considerable room for\nimprovement in the instruction-following ability of these models.", "published": "2023-11-16 11:53:31", "link": "http://arxiv.org/abs/2311.09829v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy", "abstract": "Text watermarking has emerged as a pivotal technique for identifying\nmachine-generated text. However, existing methods often rely on arbitrary\nvocabulary partitioning during decoding to embed watermarks, which compromises\nthe availability of suitable tokens and significantly degrades the quality of\nresponses. This study assesses the impact of watermarking on different\ncapabilities of large language models (LLMs) from a cognitive science lens. Our\nfinding highlights a significant disparity; knowledge recall and logical\nreasoning are more adversely affected than language generation. These results\nsuggest a more profound effect of watermarking on LLMs than previously\nunderstood. To address these challenges, we introduce Watermarking with Mutual\nExclusion (WatME), a novel approach leveraging linguistic prior knowledge of\ninherent lexical redundancy in LLM vocabularies to seamlessly integrate\nwatermarks. Specifically, WatME dynamically optimizes token usage during the\ndecoding process by applying a mutually exclusive rule to the identified\nlexical redundancies. This strategy effectively prevents the unavailability of\nappropriate tokens and preserves the expressive power of LLMs. We provide both\ntheoretical analysis and empirical evidence showing that WatME effectively\npreserves the diverse capabilities of LLMs while ensuring watermark\ndetectability.", "published": "2023-11-16 11:58:31", "link": "http://arxiv.org/abs/2311.09832v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens\n  Contributing to Explicit Hate in English by Span Detection", "abstract": "As hate speech continues to proliferate on the web, it is becoming\nincreasingly important to develop computational methods to mitigate it.\nReactively, using black-box models to identify hateful content can perplex\nusers as to why their posts were automatically flagged as hateful. On the other\nhand, proactive mitigation can be achieved by suggesting rephrasing before a\npost is made public. However, both mitigation techniques require information\nabout which part of a post contains the hateful aspect, i.e., what spans within\na text are responsible for conveying hate. Better detection of such spans can\nsignificantly reduce explicitly hateful content on the web. To further\ncontribute to this research area, we organized HateNorm at HASOC-FIRE 2023,\nfocusing on explicit span detection in English Tweets. A total of 12 teams\nparticipated in the competition, with the highest macro-F1 observed at 0.58.", "published": "2023-11-16 12:01:19", "link": "http://arxiv.org/abs/2311.09834v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity\n  Extraction Focused on Machine Learning Models and Datasets", "abstract": "Named Entity Recognition (NER) models play a crucial role in various NLP\ntasks, including information extraction (IE) and text understanding. In\nacademic writing, references to machine learning models and datasets are\nfundamental components of various computer science publications and necessitate\naccurate models for identification. Despite the advancements in NER, existing\nground truth datasets do not treat fine-grained types like ML model and model\narchitecture as separate entity types, and consequently, baseline models cannot\nrecognize them as such. In this paper, we release a corpus of 100 manually\nannotated full-text scientific publications and a first baseline model for 10\nentity types centered around ML models and datasets. In order to provide a\nnuanced understanding of how ML models and datasets are mentioned and utilized,\nour dataset also contains annotations for informal mentions like \"our\nBERT-based model\" or \"an image CNN\". You can find the ground truth dataset and\ncode to replicate model training at https://data.gesis.org/gsap/gsap-ner.", "published": "2023-11-16 12:43:02", "link": "http://arxiv.org/abs/2311.09860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Generation from Brain Recordings", "abstract": "Generating human language through non-invasive brain-computer interfaces\n(BCIs) has the potential to unlock many applications, such as serving disabled\npatients and improving communication. Currently, however, generating language\nvia BCIs has been previously successful only within a classification setup for\nselecting pre-generated sentence continuation candidates with the most likely\ncortical semantic representation. Inspired by recent research that revealed\nassociations between the brain and the large computational language models, we\npropose a generative language BCI that utilizes the capacity of a large\nlanguage model (LLM) jointly with a semantic brain decoder to directly generate\nlanguage from functional magnetic resonance imaging (fMRI) input. The proposed\nmodel can generate coherent language sequences aligned with the semantic\ncontent of visual or auditory language stimuli perceived, without prior\nknowledge of any pre-generated candidates. We compare the language generated\nfrom the presented model with a random control, pre-generated language\nselection approach, and a standard LLM, which generates common coherent text\nsolely based on the next word likelihood according to statistical language\ntraining data. The proposed model is found to generate language that is more\naligned with semantic stimulus in response to which brain input is sampled. Our\nfindings demonstrate the potential and feasibility of employing BCIs in direct\nlanguage generation.", "published": "2023-11-16 13:37:21", "link": "http://arxiv.org/abs/2311.09889v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Characterizing Tradeoffs in Language Model Decoding with Informational\n  Interpretations", "abstract": "We propose a theoretical framework for formulating language model decoder\nalgorithms with dynamic programming and information theory. With dynamic\nprogramming, we lift the design of decoder algorithms from the logit space to\nthe action-state value function space, and show that the decoding algorithms\nare consequences of optimizing the action-state value functions. Each component\nin the action-state value function space has an information theoretical\ninterpretation. With the lifting and interpretation, it becomes evident what\nthe decoder algorithm is optimized for, and hence facilitating the arbitration\nof the tradeoffs in sensibleness, diversity, and attribution.", "published": "2023-11-16 18:38:25", "link": "http://arxiv.org/abs/2311.10083v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "JWSign: A Highly Multilingual Corpus of Bible Translations for more\n  Diversity in Sign Language Processing", "abstract": "Advancements in sign language processing have been hindered by a lack of\nsufficient data, impeding progress in recognition, translation, and production\ntasks. The absence of comprehensive sign language datasets across the world's\nsign languages has widened the gap in this field, resulting in a few sign\nlanguages being studied more than others, making this research area extremely\nskewed mostly towards sign languages from high-income countries. In this work\nwe introduce a new large and highly multilingual dataset for sign language\ntranslation: JWSign. The dataset consists of 2,530 hours of Bible translations\nin 98 sign languages, featuring more than 1,500 individual signers. On this\ndataset, we report neural machine translation experiments. Apart from bilingual\nbaseline systems, we also train multilingual systems, including some that take\ninto account the typological relatedness of signed or spoken languages. Our\nexperiments highlight that multilingual systems are superior to bilingual\nbaselines, and that in higher-resource scenarios, clustering language pairs\nthat are related improves translation quality.", "published": "2023-11-16 20:02:44", "link": "http://arxiv.org/abs/2311.10174v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Impact of Familiarity on Naming Variation: A Study on Object Naming\n  in Mandarin Chinese", "abstract": "Different speakers often produce different names for the same object or\nentity (e.g., \"woman\" vs. \"tourist\" for a female tourist). The reasons behind\nvariation in naming are not well understood. We create a Language and Vision\ndataset for Mandarin Chinese that provides an average of 20 names for 1319\nnaturalistic images, and investigate how familiarity with a given kind of\nobject relates to the degree of naming variation it triggers across subjects.\nWe propose that familiarity influences naming variation in two competing ways:\nincreasing familiarity can either expand vocabulary, leading to higher\nvariation, or promote convergence on conventional names, thereby reducing\nvariation. We find evidence for both factors being at play. Our study\nillustrates how computational resources can be used to address research\nquestions in Cognitive Science.", "published": "2023-11-16 20:13:24", "link": "http://arxiv.org/abs/2311.10181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Latent Feature-based Data Splits to Improve Generalisation Evaluation: A\n  Hate Speech Detection Case Study", "abstract": "With the ever-growing presence of social media platforms comes the increased\nspread of harmful content and the need for robust hate speech detection\nsystems. Such systems easily overfit to specific targets and keywords, and\nevaluating them without considering distribution shifts that might occur\nbetween train and test data overestimates their benefit. We challenge hate\nspeech models via new train-test splits of existing datasets that rely on the\nclustering of models' hidden representations. We present two split variants\n(Subset-Sum-Split and Closest-Split) that, when applied to two datasets using\nfour pretrained models, reveal how models catastrophically fail on blind spots\nin the latent space. This result generalises when developing a split with one\nmodel and evaluating it on another. Our analysis suggests that there is no\nclear surface-level property of the data split that correlates with the\ndecreased performance, which underscores that task difficulty is not always\nhumanly interpretable. We recommend incorporating latent feature-based splits\nin model development and release two splits via the GenBench benchmark.", "published": "2023-11-16 23:49:55", "link": "http://arxiv.org/abs/2311.10236v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Think While You Write: Hypothesis Verification Promotes Faithful\n  Knowledge-to-Text Generation", "abstract": "Knowledge-to-text generators often struggle to faithfully generate\ndescriptions for the input facts: they may produce hallucinations that\ncontradict the input, or describe facts not present in the input. To reduce\nhallucinations, we propose a decoding-only method, TWEAK (Think While\nEffectively Articulating Knowledge), which can be integrated with any generator\nwithout retraining. TWEAK treats the generated sequences at each decoding step\nand its future sequences as hypotheses, and ranks each generation candidate\nbased on the extent to which their hypotheses are supported by the input facts\nusing a Hypothesis Verification Model (HVM). We first demonstrate the\neffectiveness of TWEAK by using a Natural Language Inference (NLI) model as the\nHVM and report improved faithfulness with a minimal impact on the quality. We\nthen replace the NLI model with a task-specific HVM trained with a\nfirst-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairs\ninput facts with their original and perturbed descriptions. We test TWEAK with\ntwo generators, and the best TWEAK variants improve on average for the two\nmodels by 2.24/7.17 points in faithfulness (FactKB) in in/out-of-distribution\nevaluations, respectively, and with only a 0.14/0.32-point decline in quality\n(BERTScore).", "published": "2023-11-16 00:13:19", "link": "http://arxiv.org/abs/2311.09467v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JAB: Joint Adversarial Prompting and Belief Augmentation", "abstract": "With the recent surge of language models in different applications, attention\nto safety and robustness of these models has gained significant importance.\nHere we introduce a joint framework in which we simultaneously probe and\nimprove the robustness of a black-box target model via adversarial prompting\nand belief augmentation using iterative feedback loops. This framework utilizes\nan automated red teaming approach to probe the target model, along with a\nbelief augmenter to generate instructions for the target model to improve its\nrobustness to those adversarial probes. Importantly, the adversarial model and\nthe belief generator leverage the feedback from past interactions to improve\nthe effectiveness of the adversarial prompts and beliefs, respectively. In our\nexperiments, we demonstrate that such a framework can reduce toxic content\ngeneration both in dynamic cases where an adversary directly interacts with a\ntarget model and static cases where we use a static benchmark dataset to\nevaluate our model.", "published": "2023-11-16 00:35:54", "link": "http://arxiv.org/abs/2311.09473v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Reducing Privacy Risks in Online Self-Disclosures with Language Models", "abstract": "Self-disclosure, while being common and rewarding in social media\ninteraction, also poses privacy risks. In this paper, we take the initiative to\nprotect the user-side privacy associated with online self-disclosure through\ndetection and abstraction. We develop a taxonomy of 19 self-disclosure\ncategories and curate a large corpus consisting of 4.8K annotated disclosure\nspans. We then fine-tune a language model for detection, achieving over 65%\npartial span F$_1$. We further conduct an HCI user study, with 82% of\nparticipants viewing the model positively, highlighting its real-world\napplicability. Motivated by the user feedback, we introduce the task of\nself-disclosure abstraction, which is rephrasing disclosures into less specific\nterms while preserving their utility, e.g., \"Im 16F\" to \"I'm a teenage girl\".\nWe explore various fine-tuning strategies, and our best model can generate\ndiverse abstractions that moderately reduce privacy risks while maintaining\nhigh utility according to human evaluation. To help users in deciding which\ndisclosures to abstract, we present a task of rating their importance for\ncontext understanding. Our fine-tuned model achieves 80% accuracy, on-par with\nGPT-3.5. Given safety and privacy considerations, we will only release our\ncorpus and models to researcher who agree to the ethical guidelines outlined in\nEthics Statement.", "published": "2023-11-16 03:28:43", "link": "http://arxiv.org/abs/2311.09538v3", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "A Speed Odyssey for Deployable Quantization of LLMs", "abstract": "The large language model era urges faster and less costly inference. Prior\nmodel compression works on LLMs tend to undertake a software-centric approach\nprimarily focused on the simulated quantization performance. By neglecting the\nfeasibility of deployment, these approaches are typically disabled in real\npractice. They used to drastically push down the quantization bit range for a\nreduced computation which might not be supported by the mainstream hardware, or\ninvolve sophisticated algorithms that introduce extra computation or memory\naccess overhead. We argue that pursuing a hardware-centric approach in the\nconstruction of quantization algorithms is crucial. In this regard, we are\ndriven to build our compression method on top of hardware awareness,\neliminating impractical algorithm choices while maximizing the benefit of\nhardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel\nimplementation called FastGEMM and a combined recipe of quantization\nstrategies. Extensive experiments manifest the superiority of our W4A8 method\nwhich brings the actual speed boosting up to \\textbf{4$\\times$} compared to\nHugging Face FP16 inference and \\textbf{2.23$\\times$} vs. the state-of-the-art\ninference engine TensorRT-LLM in FP16, and \\textbf{1.45$\\times$} vs.\nTensorRT-LLM in INT8, yet without substantially harming the performance.", "published": "2023-11-16 04:11:19", "link": "http://arxiv.org/abs/2311.09550v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Prompt-based Pseudo-labeling Strategy for Sample-Efficient\n  Semi-Supervised Extractive Summarization", "abstract": "Semi-supervised learning (SSL) is a widely used technique in scenarios where\nlabeled data is scarce and unlabeled data is abundant. While SSL is popular for\nimage and text classification, it is relatively underexplored for the task of\nextractive text summarization. Standard SSL methods follow a teacher-student\nparadigm to first train a classification model and then use the classifier's\nconfidence values to select pseudo-labels for the subsequent training cycle;\nhowever, such classifiers are not suitable to measure the accuracy of\npseudo-labels as they lack specific tuning for evaluation, which leads to\nconfidence values that fail to capture the semantics and correctness of the\ngenerated summary. To address this problem, we propose a prompt-based\npseudo-labeling strategy with LLMs that picks unlabeled examples with more\naccurate pseudo-labels than using just the classifier's probability outputs.\nOur approach also includes a relabeling mechanism that improves the quality of\npseudo-labels. We evaluate our method on three text summarization datasets:\nTweetSumm, WikiHow, and ArXiv/PubMed. We empirically show that a\nprompting-based LLM that scores and generates pseudo-labels outperforms\nexisting SSL methods on ROUGE-1, ROUGE-2, and ROUGE-L scores on all the\ndatasets. Furthermore, our method achieves competitive L-Eval scores\n(evaluation with LLaMa-3) as a fully supervised method in a data-scarce setting\nand outperforms fully supervised method in a data-abundant setting.", "published": "2023-11-16 04:29:41", "link": "http://arxiv.org/abs/2311.09559v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks", "abstract": "Many large language models (LLMs) for medicine have largely been evaluated on\nshort texts, and their ability to handle longer sequences such as a complete\nelectronic health record (EHR) has not been systematically explored. Assessing\nthese models on long sequences is crucial since prior work in the general\ndomain has demonstrated performance degradation of LLMs on longer texts.\nMotivated by this, we introduce LongBoX, a collection of seven medical datasets\nin text-to-text format, designed to investigate model performance on long\nsequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT)\nand strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We\nfurther evaluate two techniques designed for long-sequence handling: (i)\nlocal-global attention, and (ii) Fusion-in-Decoder (FiD). Our results\ndemonstrate mixed results with long-sequence handling - while scores on some\ndatasets increase, there is substantial room for improvement. We hope that\nLongBoX facilitates the development of more effective long-sequence techniques\nfor the medical domain. Data and source code are available at\nhttps://github.com/Mihir3009/LongBoX.", "published": "2023-11-16 04:57:49", "link": "http://arxiv.org/abs/2311.09564v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Strings from the Library of Babel: Random Sampling as a Strong Baseline\n  for Prompt Optimisation", "abstract": "Recent prompt optimisation approaches use the generative nature of language\nmodels to produce prompts -- even rivaling the performance of human-curated\nprompts. In this paper, we demonstrate that randomly sampling tokens from the\nmodel vocabulary as ``separators'' can be as effective as language models for\nprompt-style text classification. Our experiments show that random separators\nare competitive baselines, having less than a 1% difference compared to\nprevious self-optimisation methods and showing a 12% average relative\nimprovement over strong human baselines across nine text classification tasks\nand eight language models. We further analyse this phenomenon in detail using\nthree different random generation strategies, establishing that the language\nspace is rich with potentially good separators, with a greater than 40% average\nchance that a randomly drawn separator performs better than human-curated\nseparators. These observations challenge the common assumption that an\neffective prompt should be human readable or task relevant and establish a\nstrong baseline for prompt optimisation research.", "published": "2023-11-16 05:08:33", "link": "http://arxiv.org/abs/2311.09569v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Work State-Centric AI Agents: Design, Implementation, and Management of\n  Cognitive Work Threads", "abstract": "AI agents excel in executing predefined tasks, but the dynamic management of\nwork state information during task execution remains an underexplored area. We\npropose a work state-centric AI agent model employing \"work notes\" to record\nand reflect the state throughout task execution. This paper details the model's\narchitecture, featuring worker threads for task oversight, planner modules for\ntask decomposition and planning, and executor modules for performing subtasks\nusing a ReAct-inspired thought-action loop. We provide an exhaustive work state\nrecord incorporating plans and outcomes, constituting a comprehensive work\njournal. Our results show that this model not only improves task execution\nefficiency but also lays a solid foundation for subsequent task analysis and\nauditing.", "published": "2023-11-16 05:21:25", "link": "http://arxiv.org/abs/2311.09576v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Step Dialogue Workflow Action Prediction", "abstract": "In task-oriented dialogue, a system often needs to follow a sequence of\nactions, called a workflow, that complies with a set of guidelines in order to\ncomplete a task. In this paper, we propose the novel problem of multi-step\nworkflow action prediction, in which the system predicts multiple future\nworkflow actions. Accurate prediction of multiple steps allows for multi-turn\nautomation, which can free up time to focus on more complex tasks. We propose\nthree modeling approaches that are simple to implement yet lead to more action\nautomation: 1) fine-tuning on a training dataset, 2) few-shot in-context\nlearning leveraging retrieval and large language model prompting, and 3)\nzero-shot graph traversal, which aggregates historical action sequences into a\ngraph for prediction. We show that multi-step action prediction produces\nfeatures that improve accuracy on downstream dialogue tasks like predicting\ntask success, and can increase automation of steps by 20% without requiring as\nmuch feedback from a human overseeing the system.", "published": "2023-11-16 06:05:47", "link": "http://arxiv.org/abs/2311.09593v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient End-to-End Visual Document Understanding with Rationale\n  Distillation", "abstract": "Understanding visually situated language requires interpreting complex\nlayouts of textual and visual elements. Pre-processing tools, such as optical\ncharacter recognition (OCR), can map document image inputs to textual tokens,\nthen large language models (LLMs) can reason over text. However, such methods\nhave high computational and engineering complexity. Can small pretrained\nimage-to-text models accurately understand visual documents through similar\nrecognition and reasoning steps instead? We propose Rationale Distillation\n(RD), which incorporates the outputs of OCR tools, LLMs, and larger multimodal\nmodels as intermediate \"rationales\", and trains a small student model to\npredict both rationales and answers. On three visual document understanding\nbenchmarks representing infographics, scanned documents, and figures, our\nPix2Struct (282M parameters) student model finetuned with RD outperforms the\nbase model by 4-5% absolute accuracy with only 1% higher computational cost.", "published": "2023-11-16 06:50:26", "link": "http://arxiv.org/abs/2311.09612v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Digital Socrates: Evaluating LLMs through Explanation Critiques", "abstract": "While LLMs can provide reasoned explanations along with their answers, the\nnature and quality of those explanations are still poorly understood. In\nresponse, our goal is to define a detailed way of characterizing the\nexplanation capabilities of modern models and to create a nuanced,\ninterpretable explanation evaluation tool that can generate such\ncharacterizations automatically, without relying on expensive API calls or\nhuman annotations. Our approach is to (a) define the new task of explanation\ncritiquing - identifying and categorizing any main flaw in an explanation and\nproviding suggestions to address the flaw, (b) create a sizeable,\nhuman-verified dataset for this task, and (c) train an open-source, automatic\ncritique model (called Digital Socrates) using this data. Through quantitative\nand qualitative analysis, we demonstrate how Digital Socrates is useful for\nrevealing insights about student models by examining their reasoning chains,\nand how it can provide high-quality, nuanced, automatic evaluation of those\nmodel explanations for the first time. Digital Socrates thus fills an important\ngap in evaluation tools for understanding and improving the explanation\nbehavior of models.", "published": "2023-11-16 06:51:46", "link": "http://arxiv.org/abs/2311.09613v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Simulating Opinion Dynamics with Networks of LLM-based Agents", "abstract": "Accurately simulating human opinion dynamics is crucial for understanding a\nvariety of societal phenomena, including polarization and the spread of\nmisinformation. However, the agent-based models (ABMs) commonly used for such\nsimulations often over-simplify human behavior. We propose a new approach to\nsimulating opinion dynamics based on populations of Large Language Models\n(LLMs). Our findings reveal a strong inherent bias in LLM agents towards\nproducing accurate information, leading simulated agents to consensus in line\nwith scientific reality. This bias limits their utility for understanding\nresistance to consensus views on issues like climate change. After inducing\nconfirmation bias through prompt engineering, however, we observed opinion\nfragmentation in line with existing agent-based modeling and opinion dynamics\nresearch. These insights highlight the promise and limitations of LLM agents in\nthis domain and suggest a path forward: refining LLMs with real-world discourse\nto better simulate the evolution of human beliefs.", "published": "2023-11-16 07:01:48", "link": "http://arxiv.org/abs/2311.09618v4", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "Online Continual Knowledge Learning for Language Models", "abstract": "Large Language Models (LLMs) serve as repositories of extensive world\nknowledge, enabling them to perform tasks such as question-answering and\nfact-checking. However, this knowledge can become obsolete as global contexts\nchange. In this paper, we introduce a novel problem in the realm of continual\nlearning: Online Continual Knowledge Learning (OCKL). This problem formulation\naims to manage the dynamic nature of world knowledge in LMs under real-time\nconstraints. We propose a new benchmark and evaluation metric designed to\nmeasure both the rate of new knowledge acquisition and the retention of\npreviously learned knowledge. Our empirical evaluation, conducted using a\nvariety of state-of-the-art methods, establishes robust base-lines for OCKL.\nOur results reveal that existing continual learning approaches are\nunfortunately insufficient for tackling the unique challenges posed by OCKL. We\nidentify key factors that influence the trade-off between knowledge acquisition\nand retention, thereby advancing our understanding of how to train LMs in a\ncontinually evolving environment.", "published": "2023-11-16 07:31:03", "link": "http://arxiv.org/abs/2311.09632v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ICXML: An In-Context Learning Framework for Zero-Shot Extreme\n  Multi-Label Classification", "abstract": "This paper focuses on the task of Extreme Multi-Label Classification (XMC)\nwhose goal is to predict multiple labels for each instance from an extremely\nlarge label space. While existing research has primarily focused on fully\nsupervised XMC, real-world scenarios often lack supervision signals,\nhighlighting the importance of zero-shot settings. Given the large label space,\nutilizing in-context learning approaches is not trivial. We address this issue\nby introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage\nframework that cuts down the search space by generating a set of candidate\nlabels through incontext learning and then reranks them. Extensive experiments\nsuggest that ICXML advances the state of the art on two diverse public\nbenchmarks.", "published": "2023-11-16 08:01:17", "link": "http://arxiv.org/abs/2311.09649v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Structured Chemistry Reasoning with Large Language Models", "abstract": "Large Language Models (LLMs) excel in diverse areas, yet struggle with\ncomplex scientific reasoning, especially in the field of chemistry. Different\nfrom the simple chemistry tasks (e.g., molecule classification) addressed in\nprevious studies, complex chemistry problems require not only vast knowledge\nand precise calculation, but also compositional reasoning about rich dynamic\ninteractions of different concepts (e.g., temperature changes). Our study shows\nthat even advanced LLMs, like GPT-4, can fail easily in different ways.\nInterestingly, the errors often stem not from a lack of domain knowledge within\nthe LLMs, but rather from the absence of an effective reasoning structure that\nguides the LLMs to elicit the right knowledge, incorporate the knowledge in\nstep-by-step reasoning, and iteratively refine results for further improved\nquality. On this basis, we introduce StructChem, a simple yet effective\nprompting strategy that offers the desired guidance and substantially boosts\nthe LLMs' chemical reasoning capability. Testing across four chemistry areas --\nquantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem\nsubstantially enhances GPT-4's performance, with up to 30\\% peak improvement.\nOur analysis also underscores the unique difficulties of precise grounded\nreasoning in science with LLMs, highlighting a need for more research in this\narea. Code is available at \\url{https://github.com/ozyyshr/StructChem}.", "published": "2023-11-16 08:20:36", "link": "http://arxiv.org/abs/2311.09656v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MacGyver: Are Large Language Models Creative Problem Solvers?", "abstract": "We explore the creative problem-solving capabilities of modern LLMs in a\nnovel constrained setting. To this end, we create MACGYVER, an automatically\ngenerated dataset consisting of over 1,600 real-world problems deliberately\ndesigned to trigger innovative usage of objects and necessitate out-of-the-box\nthinking. We then present our collection to both LLMs and humans to compare and\ncontrast their problem-solving abilities. MACGYVER is challenging for both\ngroups, but in unique and complementary ways. For instance, humans excel in\ntasks they are familiar with but struggle with domain-specific knowledge,\nleading to a higher variance. In contrast, LLMs, exposed to a variety of\nspecialized knowledge, attempt broader problems but fail by proposing\nphysically-infeasible actions. Finally, we provide a detailed error analysis of\nLLMs, and demonstrate the potential of enhancing their problem-solving ability\nwith novel prompting techniques such as iterative step-wise reflection and\ndivergent-convergent thinking.\n  This work (1) introduces a fresh arena for intelligent agents focusing on\nintricate aspects of physical reasoning, planning, and unconventional thinking,\nwhich supplements the existing spectrum of machine intelligence; and (2)\nprovides insight into the constrained problem-solving capabilities of both\nhumans and AI.", "published": "2023-11-16 08:52:27", "link": "http://arxiv.org/abs/2311.09682v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Physicians Know How to Prompt? The Need for Automatic Prompt\n  Optimization Help in Clinical Note Generation", "abstract": "This study examines the effect of prompt engineering on the performance of\nLarge Language Models (LLMs) in clinical note generation. We introduce an\nAutomatic Prompt Optimization (APO) framework to refine initial prompts and\ncompare the outputs of medical experts, non-medical experts, and APO-enhanced\nGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in\nstandardizing prompt quality across clinical note sections. A human-in-the-loop\napproach shows that experts maintain content quality post-APO, with a\npreference for their own modifications, suggesting the value of expert\ncustomization. We recommend a two-phase optimization process, leveraging\nAPO-GPT4 for consistency and expert input for personalization.", "published": "2023-11-16 08:54:52", "link": "http://arxiv.org/abs/2311.09684v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BLT: Can Large Language Models Handle Basic Legal Text?", "abstract": "We find that the best publicly available LLMs like GPT-4 and Claude currently\nperform poorly on basic legal text handling. This motivates the creation of a\nbenchmark consisting of examples that lawyers and paralegals would expect LLMs\nto handle zero-shot, such as looking up the text at a line of a witness\ndeposition or at a subsection of a contract. LLMs' poor performance on this\nbenchmark casts into doubt their reliability as-is for legal practice. However,\nfine-tuning on our training set brings even a small model to near-perfect\nperformance. This benchmark will be useful for fine-tuning LLMs for downstream\nlegal tasks, as well as for tracking LLMs' reliability as-is for basic legal\ntasks.", "published": "2023-11-16 09:09:22", "link": "http://arxiv.org/abs/2311.09693v3", "categories": ["cs.CL", "cs.AI", "I.2.1; I.2.7; J.7"], "primary_category": "cs.CL"}
{"title": "Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go\n  without Hallucination?", "abstract": "Despite the recent advancement in large language models (LLMs) and their high\nperformances across numerous benchmarks, recent research has unveiled that LLMs\nsuffer from hallucinations and unfaithful reasoning. This work studies a\nspecific type of hallucination induced by semantic associations. Specifically,\nwe investigate to what extent LLMs take shortcuts from certain keyword/entity\nbiases in the prompt instead of following the correct reasoning path. To\nquantify this phenomenon, we propose a novel probing method and benchmark\ncalled EureQA. We start from questions that LLMs will answer correctly with\nutmost certainty, and mask the important entity with evidence sentence\nrecursively, asking models to find masked entities according to a chain of\nevidence before answering the question.\n  During the construction of the evidence, we purposefully replace semantic\nclues (entities) that may lead to the correct answer with distractor clues\n(evidence) that will not directly lead to the correct answer but require a\nchain-like reasoning process. We evaluate if models can follow the correct\nreasoning chain instead of short-cutting through distractor clues. We find that\nexisting LLMs lack the necessary capabilities to follow correct reasoning paths\nand resist the attempt of greedy shortcuts. We show that the distractor\nsemantic associations often lead to model hallucination, which is strong\nevidence that questions the validity of current LLM reasoning.", "published": "2023-11-16 09:27:36", "link": "http://arxiv.org/abs/2311.09702v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization\n  in Programming Language Understanding", "abstract": "Language models can serve as a valuable tool for software developers to\nincrease productivity. Large generative models can be used for code generation\nand code completion, while smaller encoder-only models are capable of\nperforming code search tasks using natural language queries.These capabilities\nare heavily influenced by the quality and diversity of the available training\ndata. Source code datasets used for training usually focus on the most popular\nlanguages and testing is mostly conducted on the same distributions, often\noverlooking low-resource programming languages. Motivated by the NLP\ngeneralization taxonomy proposed by Hupkes et.\\,al., we propose a new benchmark\ndataset called GenCodeSearchNet (GeCS) which builds upon existing natural\nlanguage code search datasets to systemically evaluate the programming language\nunderstanding generalization capabilities of language models. As part of the\nfull dataset, we introduce a new, manually curated subset StatCodeSearch that\nfocuses on R, a popular but so far underrepresented programming language that\nis often used by researchers outside the field of computer science. For\nevaluation and comparison, we collect several baseline results using fine-tuned\nBERT-style models and GPT-style large language models in a zero-shot setting.", "published": "2023-11-16 09:35:00", "link": "http://arxiv.org/abs/2311.09707v1", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "You don't need a personality test to know these models are unreliable:\n  Assessing the Reliability of Large Language Models on Psychometric\n  Instruments", "abstract": "The versatility of Large Language Models (LLMs) on natural language\nunderstanding tasks has made them popular for research in social sciences. To\nproperly understand the properties and innate personas of LLMs, researchers\nhave performed studies that involve using prompts in the form of questions that\nask LLMs about particular opinions. In this study, we take a cautionary step\nback and examine whether the current format of prompting LLMs elicits responses\nin a consistent and robust manner. We first construct a dataset that contains\n693 questions encompassing 39 different instruments of persona measurement on\n115 persona axes. Additionally, we design a set of prompts containing minor\nvariations and examine LLMs' capabilities to generate answers, as well as\nprompt variations to examine their consistency with respect to content-level\nvariations such as switching the order of response options or negating the\nstatement. Our experiments on 17 different LLMs reveal that even simple\nperturbations significantly downgrade a model's question-answering ability, and\nthat most LLMs have low negation consistency. Our results suggest that the\ncurrently widespread practice of prompting is insufficient to accurately and\nreliably capture model perceptions, and we therefore discuss potential\nalternatives to improve these issues.", "published": "2023-11-16 09:50:53", "link": "http://arxiv.org/abs/2311.09718v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "OVM, Outcome-supervised Value Models for Planning in Mathematical\n  Reasoning", "abstract": "Large language models (LLMs) often struggle with maintaining accuracy\nthroughout multiple multiple reasoning steps, especially in mathematical\nreasoning where an error in earlier steps can propagate to subsequent ones and\nit ultimately leading to an incorrect answer. To reduce error propagation,\nguided decoding is employed to direct the LM decoding on a step-by-step basis.\nWe argue that in guided decoding, assessing the potential of an incomplete\nreasoning path can be more advantageous than simply ensuring per-step\ncorrectness, as the former approach leads towards a correct final answer. This\ntransforms the task into a $\\textit{value estimation}$ problem in planning.\n  Inspired by the findings that $\\textit{outcome supervision for guided\ndecoding essentially acts as a value model}$, we propose Outcome-supervised\nValue Model (OVM) that employs outcome supervision for training a value model,\nwhich prioritizes steps that lead to accurate conclusions. Furthermore, the OVM\neliminates the need for labor-intensive annotations of step-level correctness,\nthereby significantly enhancing its scalability. Our experiments on two\nmulti-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate\nthe superior performance of the OVM model. Notably, in GSM8K, our\n$\\textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B\nparameters}$; especially it does not utilize GPT-4 or code execution. These\nfindings offer a novel perspective on the role of outcome supervision in\ntraining value models for multi-step reasoning tasks and provide theoretical\njustification for its advantage in value estimation for guided decoding.", "published": "2023-11-16 09:56:28", "link": "http://arxiv.org/abs/2311.09724v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Source Prompt: Coordinated Pre-training of Language Models on Diverse\n  Corpora from Multiple Sources", "abstract": "Pre-trained language models (PLMs) have established the new paradigm in the\nfield of NLP. For more powerful PLMs, one of the most popular and successful\nway is to continuously scale up sizes of the models and the pre-training\ncorpora. These large corpora are generally obtained by converging smaller ones\nfrom multiple sources, they are thus growing increasingly diverse. However, the\nside-effects of these colossal converged corpora remain understudied. In this\npaper, we identify the disadvantage of heterogeneous corpora from multiple\nsources for pre-training PLMs. Towards coordinated pre-training on diverse\ncorpora, we further propose source prompts (SP), which explicitly prompt the\nmodel of the data source at the pre-training and fine-tuning stages. Results of\nextensive experiments demonstrate that PLMs pre-trained with SP on diverse\ncorpora gain significant improvement in various downstream tasks.", "published": "2023-11-16 10:03:26", "link": "http://arxiv.org/abs/2311.09732v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "P^3SUM: Preserving Author's Perspective in News Summarization with\n  Diffusion Language Models", "abstract": "In this work, we take a first step towards designing summarization systems\nthat are faithful to the author's intent, not only the semantic content of the\narticle. Focusing on a case study of preserving political perspectives in news\nsummarization, we find that existing approaches alter the political opinions\nand stances of news articles in more than 50% of summaries, misrepresenting the\nintent and perspectives of the news authors. We thus propose P^3SUM, a\ndiffusion model-based summarization approach controlled by political\nperspective classifiers. In P^3SUM, the political leaning of a generated\nsummary is iteratively evaluated at each decoding step, and any drift from the\narticle's original stance incurs a loss back-propagated to the embedding\nlayers, steering the political stance of the summary at inference time.\nExtensive experiments on three news summarization datasets demonstrate that\nP^3SUM outperforms state-of-the-art summarization systems and large language\nmodels by up to 13.7% in terms of the success rate of stance preservation, with\ncompetitive performance on standard metrics of summarization quality. Our\nfindings present a first analysis of preservation of pragmatic features in\nsummarization, highlight the lacunae in existing summarization models -- that\neven state-of-the-art models often struggle to preserve author's intents -- and\ndevelop new summarization systems that are more faithful to author's\nperspectives.", "published": "2023-11-16 10:14:28", "link": "http://arxiv.org/abs/2311.09741v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Investigating Data Contamination in Modern Benchmarks for Large Language\n  Models", "abstract": "Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.", "published": "2023-11-16 11:03:04", "link": "http://arxiv.org/abs/2311.09783v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Interpreting User Requests in the Context of Natural Language Standing\n  Instructions", "abstract": "Users of natural language interfaces, generally powered by Large Language\nModels (LLMs),often must repeat their preferences each time they make a similar\nrequest. We describe an approach to LLM-based dialogue modeling in which\npersistent user constraints and preferences -- collectively termed standing\ninstructions -- as additional context for such interfaces. For example, when a\nuser states \"I'm hungry\", a previously expressed preference for Persian food\ncan be automatically added to the LLM prompt, influencing the search for\nrelevant restaurants. We develop NLSI, a language-to-program dataset consisting\nof over 2.4K dialogues spanning 17 domains, where each dialogue is paired with\na user profile (a set of users specific standing instructions) and\ncorresponding structured representations (API calls). A key challenge in NLSI\nis to identify which subset of the standing instructions is applicable to a\ngiven dialogue. NLSI contains diverse phenomena, from simple preferences to\ninterdependent instructions such as triggering a hotel search whenever the user\nis booking tickets to an event. We conduct experiments on NLSI using prompting\nwith large language models and various retrieval approaches, achieving a\nmaximum of 44.7% exact match on API prediction. Our results demonstrate the\nchallenges in identifying the relevant standing instructions and their\ninterpretation into API calls.", "published": "2023-11-16 11:19:26", "link": "http://arxiv.org/abs/2311.09796v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs", "abstract": "Two lines of approaches are adopted for complex reasoning with LLMs. One line\nof work prompts LLMs with various reasoning structures, while the structural\noutputs can be naturally regarded as intermediate reasoning steps. Another line\nof work adopt LLM-free declarative solvers to do the reasoning task, rendering\nhigher reasoning accuracy but lacking interpretability due to the black-box\nnature of the solvers. Aiming to resolve the trade-off between answer accuracy\nand interpretability, we present a simple extension to the latter line of work.\nSpecifically, we showcase that the intermediate search logs generated by Prolog\ninterpreters can be accessed and interpreted into human-readable reasoning\nproofs. As long as LLMs correctly translate problem descriptions into Prolog\nrepresentations, the corresponding reasoning proofs are ensured to be causal\nand reliable. On two logical reasoning and one arithmetic reasoning datasets,\nour framework obtains significant improvements in terms of both answer accuracy\nand reasoning proof accuracy. Our code is released at\nhttps://github.com/DAMO-NLP-SG/CaRing", "published": "2023-11-16 11:26:21", "link": "http://arxiv.org/abs/2311.09802v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "SUQL: Conversational Search over Structured and Unstructured Data with\n  Large Language Models", "abstract": "While most conversational agents are grounded on either free-text or\nstructured knowledge, many knowledge corpora consist of hybrid sources. This\npaper presents the first conversational agent that supports the full generality\nof hybrid data access for large knowledge corpora, through a language we\ndeveloped called SUQL (Structured and Unstructured Query Language).\nSpecifically, SUQL extends SQL with free-text primitives (summary and answer),\nso information retrieval can be composed with structured data accesses\narbitrarily in a formal, succinct, precise, and interpretable notation. With\nSUQL, we propose the first semantic parser, an LLM with in-context learning,\nthat can handle hybrid data sources.\n  Our in-context learning-based approach, when applied to the HybridQA dataset,\ncomes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62K\ndata samples. More significantly, unlike previous approaches, our technique is\napplicable to large databases and free-text corpora. We introduce a dataset\nconsisting of crowdsourced questions and conversations on Yelp, a large, real\nrestaurant knowledge base with structured and unstructured data. We show that\nour few-shot conversational agent based on SUQL finds an entity satisfying all\nuser requirements 90.3% of the time, compared to 63.4% for a baseline based on\nlinearization.", "published": "2023-11-16 11:48:17", "link": "http://arxiv.org/abs/2311.09818v2", "categories": ["cs.CL", "cs.PL"], "primary_category": "cs.CL"}
{"title": "Automating the Generation of Prompts for LLM-based Action Choice in PDDL\n  Planning", "abstract": "Large language models (LLMs) have revolutionized a large variety of NLP\ntasks. An active debate is to what extent they can do reasoning and planning.\nPrior work has assessed the latter in the specific context of PDDL planning,\nbased on manually converting three PDDL domains into natural language (NL)\nprompts. Here we automate this conversion step, showing how to leverage an LLM\nto automatically generate NL prompts from PDDL input. Our automatically\ngenerated NL prompts result in similar LLM-planning performance as the previous\nmanually generated ones. Beyond this, the automation enables us to run much\nlarger experiments, providing for the first time a broad evaluation of LLM\nplanning performance in PDDL.", "published": "2023-11-16 11:55:27", "link": "http://arxiv.org/abs/2311.09830v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "ML-Bench: Evaluating Large Language Models and Agents for Machine\n  Learning Tasks on Repository-Level Code", "abstract": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results\nin function-level code generation, they struggle with repository-scale code\nunderstanding (e.g., coming up with the right arguments for calling routines),\nrequiring a deeper comprehension of complex file interactions. Also, recently,\npeople have developed LLM agents that attempt to interact with repository code\n(e.g., compiling and evaluating its execution), prompting the need to evaluate\ntheir performance. These gaps have motivated our development of ML-Bench, a\nbenchmark rooted in real-world programming applications that leverage existing\ncode repositories to perform tasks. Addressing the need for LLMs to interpret\nlong code contexts and translate instructions into precise, executable scripts,\nML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories,\nchallenging LLMs to accommodate user-specified arguments and documentation\nintricacies effectively. To evaluate both LLMs and AI agents, two setups are\nemployed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a\npredefined deployment environment, and ML-Agent-Bench for testing autonomous\nagents in an end-to-end task execution within a Linux sandbox environment. Our\nfindings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%,\nthere remains significant scope for improvement, highlighted by issues such as\nhallucinated outputs and difficulties with bash script generation. Notably, in\nthe more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate,\nreflecting the efficacy of iterative action and feedback in complex task\nresolution. Our code, dataset, and models are available at\nhttps://github.com/gersteinlab/ML-bench.", "published": "2023-11-16 12:03:21", "link": "http://arxiv.org/abs/2311.09835v5", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization", "abstract": "We investigate pre-training techniques for abstractive multi-document\nsummarization (MDS), which is much less studied than summarizing single\ndocuments. Though recent work has demonstrated the effectiveness of\nhighlighting information salience for pre-training strategy design, it\nstruggles to generate abstractive and reflective summaries, which are critical\nproperties for MDS. To this end, we present PELMS, a pre-trained model that\nuses objectives based on semantic coherence heuristics and faithfulness\nconstraints with un-labeled multi-document inputs, to promote the generation of\nconcise, fluent, and faithful summaries. To support the training of PELMS, we\ncompile MultiPT, a multi-document pre-training corpus containing over 93\nmillion documents to form more than 3 million unlabeled topic-centric document\nclusters, covering diverse genres such as product reviews, news, and general\nknowledge. We perform extensive evaluation of PELMS in low-shot settings on a\nwide range of MDS datasets. Our approach consistently outperforms competitive\ncomparisons with respect to overall informativeness, abstractiveness,\ncoherence, and faithfulness.", "published": "2023-11-16 12:05:23", "link": "http://arxiv.org/abs/2311.09836v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ConceptPsy:A Benchmark Suite with Conceptual Comprehensiveness in\n  Psychology", "abstract": "The critical field of psychology necessitates a comprehensive benchmark to\nenhance the evaluation and development of domain-specific Large Language Models\n(LLMs). Existing MMLU-type benchmarks, such as C-EVAL and CMMLU, include\npsychology-related subjects, but their limited number of questions and lack of\nsystematic concept sampling strategies mean they cannot cover the concepts\nrequired in psychology. Consequently, despite their broad subject coverage,\nthese benchmarks lack the necessary depth in the psychology domain, making them\ninadequate as psychology-specific evaluation suite. To address this issue, this\npaper presents ConceptPsy, designed to evaluate Chinese complex reasoning and\nknowledge abilities in psychology. ConceptPsy includes 12 core subjects and\n1383 manually collected concepts. Specifically, we prompt GPT-4 to generate\nquestions for each concept using carefully designed diverse prompts and hire\nprofessional psychologists to review these questions. To help to understand the\nfine-grained performances and enhance the weaknesses, we annotate each question\nwith a chapter label and provide chapter-wise accuracy. Based on ConceptPsy, we\nevaluate a broad range of LLMs. We observe that, although some LLMs achieve\nsimilar accuracies on overall performances, they exhibit significant\nperformance variations across different psychology concepts, even when they are\nmodels from the same series. We hope our work can facilitate the development of\nLLMs in the field of psychology.", "published": "2023-11-16 12:43:18", "link": "http://arxiv.org/abs/2311.09861v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Which Modality should I use -- Text, Motif, or Image? : Understanding\n  Graphs with Large Language Models", "abstract": "Our research integrates graph data with Large Language Models (LLMs), which,\ndespite their advancements in various fields using large text corpora, face\nlimitations in encoding entire graphs due to context size constraints. This\npaper introduces a new approach to encoding a graph with diverse modalities,\nsuch as text, image, and motif, coupled with prompts to approximate a graph's\nglobal connectivity, thereby enhancing LLMs' efficiency in processing complex\ngraph structures. The study also presents GraphTMI, a novel benchmark for\nevaluating LLMs in graph structure analysis, focusing on homophily, motif\npresence, and graph difficulty. Key findings indicate that the image modality,\nespecially with vision-language models like GPT-4V, is superior to text in\nbalancing token limits and preserving essential information and outperforms\nprior graph neural net (GNN) encoders. Furthermore, the research assesses how\nvarious factors affect the performance of each encoding modality and outlines\nthe existing challenges and potential future developments for LLMs in graph\nunderstanding and reasoning tasks. All data will be publicly available upon\nacceptance.", "published": "2023-11-16 12:45:41", "link": "http://arxiv.org/abs/2311.09862v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "An Attention-Based Denoising Framework for Personality Detection in\n  Social Media Texts", "abstract": "In social media networks, users produce a large amount of text content\nanytime, providing researchers with a valuable approach to digging for\npersonality-related information. Personality detection based on user-generated\ntexts is a universal method that can be used to build user portraits. The\npresence of noise in social media texts hinders personality detection. However,\nprevious studies have not fully addressed this challenge. Inspired by the\nscanning reading technique, we propose an attention-based information\nextraction mechanism (AIEM) for long texts, which is applied to quickly locate\nvaluable pieces of information, and focus more attention on the deep semantics\nof key pieces. Then, we provide a novel attention-based denoising framework\n(ADF) for personality detection tasks and achieve state-of-the-art performance\non two commonly used datasets. Notably, we obtain an average accuracy\nimprovement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator\n(Twitter-MBTI) dataset. We made our code publicly available on GitHub. We shed\nlight on how AIEM works to magnify personality-related signals.", "published": "2023-11-16 14:56:09", "link": "http://arxiv.org/abs/2311.09945v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Generative AI for Hate Speech Detection: Evaluation and Findings", "abstract": "Automatic hate speech detection using deep neural models is hampered by the\nscarcity of labeled datasets, leading to poor generalization. To mitigate this\nproblem, generative AI has been utilized to generate large amounts of synthetic\nhate speech sequences from available labeled examples, leveraging the generated\ndata in finetuning large pre-trained language models (LLMs). In this chapter,\nwe provide a review of relevant methods, experimental setups and evaluation of\nthis approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT,\nwe apply and evaluate the impact of train set augmentation with generated data\nusing LLMs that have been already adapted for hate detection, including\nRoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical\nstudy corroborates our previous findings, showing that this approach improves\nhate speech generalization, boosting recall performance across data\ndistributions. In addition, we explore and compare the performance of the\nfinetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results\ndemonstrate that while better generalization is achieved using the GPT-3.5\nmodel, it achieves mediocre recall and low precision on most datasets. It is an\nopen question whether the sensitivity of models such as GPT-3.5, and onward,\ncan be improved using similar techniques of text generation.", "published": "2023-11-16 16:09:43", "link": "http://arxiv.org/abs/2311.09993v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve\n  Health Literacy and Communication in Pediatric Populations and Beyond", "abstract": "Purpose: Enhanced health literacy has been linked to better health outcomes;\nhowever, few interventions have been studied. We investigate whether large\nlanguage models (LLMs) can serve as a medium to improve health literacy in\nchildren and other populations.\n  Methods: We ran 288 conditions using 26 different prompts through\nChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate\nlimits, we tested a subset of 150 conditions through ChatGPT-4. The primary\noutcome measurements were the reading grade level (RGL) and word counts of\noutput.\n  Results: Across all models, output for basic prompts such as \"Explain\" and\n\"What is (are)\" were at, or exceeded, a 10th-grade RGL. When prompts were\nspecified to explain conditions from the 1st to 12th RGL, we found that LLMs\nhad varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided\nresponses that ranged from the 7th-grade to college freshmen RGL while\nChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL.\nMicrosoft Bing provided responses from the 9th to 11th RGL while Google Bard\nprovided responses from the 7th to 10th RGL.\n  Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade\nlevel outputs. Meanwhile Bard and Bing tended to consistently produce an RGL\nthat is at the high school level regardless of prompt. Additionally, Bard's\nhesitancy in providing certain outputs indicates a cautious approach towards\nhealth information. LLMs demonstrate promise in enhancing health communication,\nbut future research should verify the accuracy and effectiveness of such tools\nin this context.\n  Implications: LLMs face challenges in crafting outputs below a sixth-grade\nreading level. However, their capability to modify outputs above this threshold\nprovides a potential mechanism to improve health literacy and communication in\na pediatric population and beyond.", "published": "2023-11-16 18:30:14", "link": "http://arxiv.org/abs/2311.10075v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Predictive Minds: LLMs As Atypical Active Inference Agents", "abstract": "Large language models (LLMs) like GPT are often conceptualized as passive\npredictors, simulators, or even stochastic parrots. We instead conceptualize\nLLMs by drawing on the theory of active inference originating in cognitive\nscience and neuroscience. We examine similarities and differences between\ntraditional active inference systems and LLMs, leading to the conclusion that,\ncurrently, LLMs lack a tight feedback loop between acting in the world and\nperceiving the impacts of their actions, but otherwise fit in the active\ninference paradigm. We list reasons why this loop may soon be closed, and\npossible consequences of this including enhanced model self-awareness and the\ndrive to minimize prediction error by changing the world.", "published": "2023-11-16 22:11:12", "link": "http://arxiv.org/abs/2311.10215v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Think Twice: Perspective-Taking Improves Large Language Models'\n  Theory-of-Mind Capabilities", "abstract": "Human interactions are deeply rooted in the interplay of thoughts, beliefs,\nand desires made possible by Theory of Mind (ToM): our cognitive ability to\nunderstand the mental states of ourselves and others. Although ToM may come\nnaturally to us, emulating it presents a challenge to even the most advanced\nLarge Language Models (LLMs). Recent improvements to LLMs' reasoning\ncapabilities from simple yet effective prompting techniques such as\nChain-of-Thought have seen limited applicability to ToM. In this paper, we turn\nto the prominent cognitive science theory \"Simulation Theory\" to bridge this\ngap. We introduce SimToM, a novel two-stage prompting framework inspired by\nSimulation Theory's notion of perspective-taking. To implement this idea on\ncurrent ToM benchmarks, SimToM first filters context based on what the\ncharacter in question knows before answering a question about their mental\nstate. Our approach, which requires no additional training and minimal\nprompt-tuning, shows substantial improvement over existing methods, and our\nanalysis reveals the importance of perspective-taking to Theory-of-Mind\ncapabilities. Our findings suggest perspective-taking as a promising direction\nfor future research into improving LLMs' ToM capabilities.", "published": "2023-11-16 22:49:27", "link": "http://arxiv.org/abs/2311.10227v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical\n  Reasoning", "abstract": "Large language models (LLMs), despite their remarkable progress across\nvarious general domains, encounter significant barriers in medicine and\nhealthcare. This field faces unique challenges such as domain-specific\nterminologies and reasoning over specialized knowledge. To address these\nissues, we propose MedAgents, a novel multi-disciplinary collaboration\nframework for the medical domain. MedAgents leverages LLM-based agents in a\nrole-playing setting that participate in a collaborative multi-round\ndiscussion, thereby enhancing LLM proficiency and reasoning capabilities. This\ntraining-free framework encompasses five critical steps: gathering domain\nexperts, proposing individual analyses, summarising these analyses into a\nreport, iterating over discussions until a consensus is reached, and ultimately\nmaking a decision. Our work focuses on the zero-shot setting, which is\napplicable in real-world scenarios. Experimental results on nine datasets\n(MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our\nproposed MedAgents framework excels at mining and harnessing the medical\nexpertise within LLMs, as well as extending its reasoning abilities. Our code\ncan be found at https://github.com/gersteinlab/MedAgents.", "published": "2023-11-16 11:47:58", "link": "http://arxiv.org/abs/2311.10537v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Systematic Review of Aspect-based Sentiment Analysis: Domains,\n  Methods, and Trends", "abstract": "Aspect-based sentiment analysis (ABSA) is a fine-grained type of sentiment\nanalysis that identifies aspects and their associated opinions from a given\ntext. With the surge of digital opinionated text data, ABSA gained increasing\npopularity for its ability to mine more detailed and targeted insights. Many\nreview papers on ABSA subtasks and solution methodologies exist, however, few\nfocus on trends over time or systemic issues relating to research application\ndomains, datasets, and solution approaches. To fill the gap, this paper\npresents a systematic literature review (SLR) of ABSA studies with a focus on\ntrends and high-level relationships among these fundamental components. This\nreview is one of the largest SLRs on ABSA. To our knowledge, it is also the\nfirst to systematically examine the interrelations among ABSA research and data\ndistribution across domains, as well as trends in solution paradigms and\napproaches. Our sample includes 727 primary studies screened from 8550 search\nresults without time constraints via an innovative automatic filtering process.\nOur quantitative analysis not only identifies trends in nearly two decades of\nABSA research development but also unveils a systemic lack of dataset and\ndomain diversity as well as domain mismatch that may hinder the development of\nfuture ABSA research. We discuss these findings and their implications and\npropose suggestions for future research.", "published": "2023-11-16 06:01:47", "link": "http://arxiv.org/abs/2311.10777v6", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Can Language Model Moderators Improve the Health of Online Discourse?", "abstract": "Conversational moderation of online communities is crucial to maintaining\ncivility for a constructive environment, but it is challenging to scale and\nharmful to moderators. The inclusion of sophisticated natural language\ngeneration modules as a force multiplier to aid human moderators is a\ntantalizing prospect, but adequate evaluation approaches have so far been\nelusive. In this paper, we establish a systematic definition of conversational\nmoderation effectiveness grounded on moderation literature and establish design\ncriteria for conducting realistic yet safe evaluation. We then propose a\ncomprehensive evaluation framework to assess models' moderation capabilities\nindependently of human intervention. With our framework, we conduct the first\nknown study of language models as conversational moderators, finding that\nappropriately prompted models that incorporate insights from social science can\nprovide specific and fair feedback on toxic behavior but struggle to influence\nusers to increase their levels of respect and cooperation.", "published": "2023-11-16 11:14:22", "link": "http://arxiv.org/abs/2311.10781v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text Sanitization Beyond Specific Domains: Zero-Shot Redaction &\n  Substitution with Large Language Models", "abstract": "In the context of information systems, text sanitization techniques are used\nto identify and remove sensitive data to comply with security and regulatory\nrequirements. Even though many methods for privacy preservation have been\nproposed, most of them are focused on the detection of entities from specific\ndomains (e.g., credit card numbers, social security numbers), lacking\ngenerality and requiring customization for each desirable domain. Moreover,\nremoving words is, in general, a drastic measure, as it can degrade text\ncoherence and contextual information. Less severe measures include substituting\na word for a safe alternative, yet it can be challenging to automatically find\nmeaningful substitutions. We present a zero-shot text sanitization technique\nthat detects and substitutes potentially sensitive information using Large\nLanguage Models. Our evaluation shows that our method excels at protecting\nprivacy while maintaining text coherence and contextual information, preserving\ndata utility for downstream tasks.", "published": "2023-11-16 18:42:37", "link": "http://arxiv.org/abs/2311.10785v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented\n  Generation Systems", "abstract": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies\non hand annotations for input queries, passages to retrieve, and responses to\ngenerate. We introduce ARES, an Automated RAG Evaluation System, for evaluating\nRAG systems along the dimensions of context relevance, answer faithfulness, and\nanswer relevance. By creating its own synthetic training data, ARES finetunes\nlightweight LM judges to assess the quality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of\nhuman-annotated datapoints for prediction-powered inference (PPI). Across eight\ndifferent knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES\naccurately evaluates RAG systems while using only a few hundred human\nannotations during evaluation. Furthermore, ARES judges remain effective across\ndomain shifts, proving accurate even after changing the type of queries and/or\ndocuments used in the evaluated RAG systems. We make our code and datasets\npublicly available on Github.", "published": "2023-11-16 00:39:39", "link": "http://arxiv.org/abs/2311.09476v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Show Your Work with Confidence: Confidence Bands for Tuning Curves", "abstract": "The choice of hyperparameters greatly impacts performance in natural language\nprocessing. Often, it is hard to tell if a method is better than another or\njust better tuned. Tuning curves fix this ambiguity by accounting for tuning\neffort. Specifically, they plot validation performance as a function of the\nnumber of hyperparameter choices tried so far. While several estimators exist\nfor these curves, it is common to use point estimates, which we show fail\nsilently and give contradictory results when given too little data.\n  Beyond point estimates, confidence bands are necessary to rigorously\nestablish the relationship between different approaches. We present the first\nmethod to construct valid confidence bands for tuning curves. The bands are\nexact, simultaneous, and distribution-free, thus they provide a robust basis\nfor comparing methods.\n  Empirical analysis shows that while bootstrap confidence bands, which serve\nas a baseline, fail to approximate their target confidence, ours achieve it\nexactly. We validate our design with ablations, analyze the effect of sample\nsize, and provide guidance on comparing models with our method. To promote\nconfident comparisons in future work, we release opda: an easy-to-use library\nthat you can install with pip. https://github.com/nicholaslourie/opda", "published": "2023-11-16 00:50:37", "link": "http://arxiv.org/abs/2311.09480v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "SegMix: A Simple Structure-Aware Data Augmentation Method", "abstract": "Interpolation-based Data Augmentation (DA) methods (Mixup) linearly\ninterpolate the inputs and labels of two or more training examples. Mixup has\nmore recently been adapted to the field of Natural Language Processing (NLP),\nmainly for sequence labeling tasks. However, such a simple adoption yields\nmixed or unstable improvements over the baseline models. We argue that the\ndirect-adoption methods do not account for structures in NLP tasks. To this\nend, we propose SegMix, a collection of interpolation-based DA algorithms that\ncan adapt to task-specific structures. SegMix poses fewer constraints on data\nstructures, is robust to various hyperparameter settings, applies to more task\nsettings, and adds little computational overhead. In the algorithm's core, we\napply interpolation methods on task-specific meaningful segments, in contrast\nto applying them on sequences as in prior work. We find SegMix to be a flexible\nframework that combines rule-based DA methods with interpolation-based methods,\ncreating interesting mixtures of DA techniques. We show that SegMix\nconsistently improves performance over strong baseline models in Named Entity\nRecognition (NER) and Relation Extraction (RE) tasks, especially under\ndata-scarce settings. Furthermore, this method is easy to implement and adds\nnegligible training overhead.", "published": "2023-11-16 02:05:15", "link": "http://arxiv.org/abs/2311.09505v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM", "abstract": "Existing open-source helpfulness preference datasets do not specify what\nmakes some responses more helpful and others less so. Models trained on these\ndatasets can incidentally learn to model dataset artifacts (e.g. preferring\nlonger but unhelpful responses only due to their length). To alleviate this\nproblem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated\nfor the various aspects that make responses helpful. Specifically, our\n37k-sample dataset has annotations for correctness, coherence, complexity, and\nverbosity in addition to overall helpfulness of responses. Training Llama 2 70B\nusing the HelpSteer dataset with SteerLM technique produces a model that scores\n7.54 on MT Bench, which is currently the highest score for open models that do\nnot require training data from more powerful models (e.g. GPT4). We release\nthis dataset with CC-BY-4.0 license at\nhttps://huggingface.co/datasets/nvidia/HelpSteer", "published": "2023-11-16 03:13:29", "link": "http://arxiv.org/abs/2311.09528v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Tied-Lora: Enhancing parameter efficiency of LoRA with weight tying", "abstract": "We introduce Tied-LoRA, a novel paradigm leveraging weight tying and\nselective training to enhance the parameter efficiency of Low-rank Adaptation\n(LoRA). Our exploration encompasses different plausible combinations of\nparameter training and freezing, coupled with weight tying, aimed at\nidentifying the optimal trade-off between performance and the count of\ntrainable parameters. Across $5$ diverse tasks and two foundational language\nmodels with different parameter counts, our experiments provide comprehensive\ninsights into the inherent trade-offs between efficiency and performance.\n  Our findings reveal a specific Tied-LoRA configuration that distinguishes\nitself by showcasing comparable performance to LoRA across multiple tasks while\nutilizing only a fraction of the parameters employed by the standard LoRA\nmethod, particularly at elevated ranks. This underscores the efficacy of\nTied-LoRA in achieving impressive results with significantly reduced model\ncomplexity.", "published": "2023-11-16 05:29:39", "link": "http://arxiv.org/abs/2311.09578v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mitigating Biases for Instruction-following Language Models via Bias\n  Neurons Elimination", "abstract": "Instruction-following language models often show undesirable biases. These\nundesirable biases may be accelerated in the real-world usage of language\nmodels, where a wide range of instructions is used through zero-shot example\nprompting. To solve this problem, we first define the bias neuron, which\nsignificantly affects biased outputs, and prove its existence empirically.\nFurthermore, we propose a novel and practical bias mitigation method, CRISPR,\nto eliminate bias neurons of language models in instruction-following settings.\nCRISPR automatically determines biased outputs and categorizes neurons that\naffect the biased outputs as bias neurons using an explainability method.\nExperimental results demonstrate the effectiveness of our method in mitigating\nbiases under zero-shot instruction-following settings without losing the\nmodel's task performance and existing knowledge. The experimental results\nreveal the generalizability of our method as it shows robustness under various\ninstructions and datasets. Surprisingly, our method can mitigate the bias in\nlanguage models by eliminating only a few neurons (at least three).", "published": "2023-11-16 07:16:55", "link": "http://arxiv.org/abs/2311.09627v2", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Decoding Susceptibility: Modeling Misbelief to Misinformation Through a\n  Computational Approach", "abstract": "Susceptibility to misinformation describes the degree of belief in\nunverifiable claims, a latent aspect of individuals' mental processes that is\nnot observable. Existing susceptibility studies heavily rely on self-reported\nbeliefs, which can be subject to bias, expensive to collect, and challenging to\nscale for downstream applications. To address these limitations, in this work,\nwe propose a computational approach to model users' latent susceptibility\nlevels. As shown in previous research, susceptibility is influenced by various\nfactors (e.g., demographic factors, political ideology), and directly\ninfluences people's reposting behavior on social media. To represent the\nunderlying mental process, our susceptibility modeling incorporates these\nfactors as inputs, guided by the supervision of people's sharing behavior.\nUsing COVID-19 as a testbed domain, our experiments demonstrate a significant\nalignment between the susceptibility scores estimated by our computational\nmodeling and human judgments, confirming the effectiveness of this latent\nmodeling approach. Furthermore, we apply our model to annotate susceptibility\nscores on a large-scale dataset and analyze the relationships between\nsusceptibility with various factors. Our analysis reveals that political\nleanings and psychological factors exhibit varying degrees of association with\nsusceptibility to COVID-19 misinformation.", "published": "2023-11-16 07:22:56", "link": "http://arxiv.org/abs/2311.09630v3", "categories": ["cs.CL", "cs.CY", "cs.SI"], "primary_category": "cs.CL"}
{"title": "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with\n  Human Feedback in Large Language Models", "abstract": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed\nto align Large Language Models (LLMs) with human preferences, playing an\nimportant role in LLMs alignment. Despite its advantages, RLHF relies on human\nannotators to rank the text, which can introduce potential security\nvulnerabilities if any adversarial annotator (i.e., attackers) manipulates the\nranking score by up-ranking any malicious text to steer the LLM adversarially.\nTo assess the red-teaming of RLHF against human preference data poisoning, we\npropose RankPoison, a poisoning attack method on candidates' selection of\npreference rank flipping to reach certain malicious behaviors (e.g., generating\nlonger sequences, which can increase the computational cost). With poisoned\ndataset generated by RankPoison, we can perform poisoning attacks on LLMs to\ngenerate longer tokens without hurting the original safety alignment\nperformance. Moreover, applying RankPoison, we also successfully implement a\nbackdoor attack where LLMs can generate longer answers under questions with the\ntrigger word. Our findings highlight critical security challenges in RLHF,\nunderscoring the necessity for more robust alignment methods for LLMs.", "published": "2023-11-16 07:48:45", "link": "http://arxiv.org/abs/2311.09641v2", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Improving the Generation Quality of Watermarked Large Language Models\n  via Word Importance Scoring", "abstract": "The strong general capabilities of Large Language Models (LLMs) bring\npotential ethical risks if they are unrestrictedly accessible to malicious\nusers. Token-level watermarking inserts watermarks in the generated texts by\naltering the token probability distributions with a private random number\ngenerator seeded by its prefix tokens. However, this watermarking algorithm\nalters the logits during generation, which can lead to a downgraded text\nquality if it chooses to promote tokens that are less relevant given the input.\nIn this work, we propose to improve the quality of texts generated by a\nwatermarked language model by Watermarking with Importance Scoring (WIS). At\neach generation step, we estimate the importance of the token to generate, and\nprevent it from being impacted by watermarking if it is important for the\nsemantic correctness of the output. We further propose three methods to predict\nimportance scoring, including a perturbation-based method and two model-based\nmethods. Empirical experiments show that our method can generate texts with\nbetter quality with comparable level of detection rate.", "published": "2023-11-16 08:36:00", "link": "http://arxiv.org/abs/2311.09668v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Examining LLMs' Uncertainty Expression Towards Questions Outside\n  Parametric Knowledge", "abstract": "Can large language models (LLMs) express their uncertainty in situations\nwhere they lack sufficient parametric knowledge to generate reasonable\nresponses? This work aims to systematically investigate LLMs' behaviors in such\nsituations, emphasizing the trade-off between honesty and helpfulness. To\ntackle the challenge of precisely determining LLMs' knowledge gaps, we\ndiagnostically create unanswerable questions containing non-existent concepts\nor false premises, ensuring that they are outside the LLMs' vast training data.\nBy compiling a benchmark, UnknownBench, which consists of both unanswerable and\nanswerable questions, we quantitatively evaluate the LLMs' performance in\nmaintaining honesty while being helpful. Using a model-agnostic unified\nconfidence elicitation approach, we observe that most LLMs fail to consistently\nrefuse or express uncertainty towards questions outside their parametric\nknowledge, although instruction fine-tuning and alignment techniques can\nprovide marginal enhancements. Moreover, LLMs' uncertainty expression does not\nalways stay consistent with the perceived confidence of their textual outputs.", "published": "2023-11-16 10:02:40", "link": "http://arxiv.org/abs/2311.09731v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and\n  Classification", "abstract": "We introduce MAFALDA, a benchmark for fallacy classification that merges and\nunites previous fallacy datasets. It comes with a taxonomy that aligns,\nrefines, and unifies existing classifications of fallacies. We further provide\na manual annotation of a part of the dataset together with manual explanations\nfor each annotation. We propose a new annotation scheme tailored for subjective\nNLP tasks, and a new evaluation method designed to handle subjectivity. We then\nevaluate several language models under a zero-shot learning setting and human\nperformances on MAFALDA to assess their capability to detect and classify\nfallacies.", "published": "2023-11-16 10:35:11", "link": "http://arxiv.org/abs/2311.09761v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Graph Elicitation for Guiding Multi-Step Reasoning in Large Language\n  Models", "abstract": "Chain-of-Thought (CoT) prompting along with sub-question generation and\nanswering has enhanced multi-step reasoning capabilities of Large Language\nModels (LLMs). However, prompting the LLMs to directly generate sub-questions\nis suboptimal since they sometimes generate redundant or irrelevant questions.\nTo deal with them, we propose a GE-Reasoning method, which directs LLMs to\ngenerate proper sub-questions and corresponding answers. Concretely, given an\ninput question, we first prompt the LLM to generate knowledge triplets, forming\na graph representation of the question. Unlike conventional knowledge triplets,\nour approach allows variables as head or tail entities, effectively\nrepresenting a question as knowledge triplets. Second, for each triplet, the\nLLM generates a corresponding sub-question and answer along with using\nknowledge retrieval. If the prediction confidence exceeds a threshold, the\nsub-question and prediction are incorporated into the prompt for subsequent\nprocessing. This approach encourages that sub-questions are grounded in the\nextracted knowledge triplets, reducing redundancy and irrelevance. Our\nexperiments demonstrate that our approach outperforms previous CoT prompting\nmethods and their variants on multi-hop question answering benchmark datasets.", "published": "2023-11-16 10:36:08", "link": "http://arxiv.org/abs/2311.09762v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs", "abstract": "Adapting a language model into a specific domain, a.k.a `domain adaption', is\na common practice when specialized knowledge, e.g. medicine, is not\nencapsulated in a general language model like Llama2. The challenge lies in the\nheterogeneity of data across the two training stages, as it varies in\nlanguages, genres, or formats. To tackle this and simplify the learning\nprotocol, we propose to transform heterogeneous data, from the both\npre-training and supervised stages, into a unified, simple input-output pair\nformat. We validate the new protocol in the domains where proprietary LLMs like\nChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The\ndeveloped model, HuatuoGPT-II, has shown state-of-the-art performance in\nChinese medicine domain on a number of benchmarks, e.g. medical licensing\nexams. It even outperforms proprietary models like ChatGPT and GPT-4 in some\naspects, especially in Traditional Chinese Medicine. Expert manual evaluations\nfurther validate HuatuoGPT-II's advantages over existing LLMs. Notably,\nHuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing\nExamination where it achieved the best performance, showcasing not only its\neffectiveness but also its generalization capabilities.", "published": "2023-11-16 10:56:24", "link": "http://arxiv.org/abs/2311.09774v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Downstream Trade-offs of a Family of Text Watermarks", "abstract": "Watermarking involves implanting an imperceptible signal into generated text\nthat can later be detected via statistical tests. A prominent family of\nwatermarking strategies for LLMs embeds this signal by upsampling a\n(pseudorandomly-chosen) subset of tokens at every generation step. However,\nsuch signals alter the model's output distribution and can have unintended\neffects on its downstream performance. In this work, we evaluate the\nperformance of LLMs watermarked using three different strategies over a diverse\nsuite of tasks including those cast as k-class classification (CLS), multiple\nchoice question answering (MCQ), short-form generation (e.g., open-ended\nquestion answering) and long-form generation (e.g., translation) tasks. We find\nthat watermarks (under realistic hyperparameters) can cause significant drops\nin LLMs' effective utility across all tasks. We observe drops of 10 to 20% in\nCLS tasks in the average case, which shoot up to 100% in the worst case. We\nnotice degradations of about 7% in MCQ tasks, 10-15% in short-form generation,\nand 5-15% in long-form generation tasks. Our findings highlight the trade-offs\nthat users should be cognizant of when using watermarked models.", "published": "2023-11-16 11:44:58", "link": "http://arxiv.org/abs/2311.09816v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging LLMs in Scholarly Knowledge Graph Question Answering", "abstract": "This paper presents a scholarly Knowledge Graph Question Answering (KGQA)\nthat answers bibliographic natural language questions by leveraging a large\nlanguage model (LLM) in a few-shot manner. The model initially identifies the\ntop-n similar training questions related to a given test question via a\nBERT-based sentence encoder and retrieves their corresponding SPARQL. Using the\ntop-n similar question-SPARQL pairs as an example and the test question creates\na prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs\nthe SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and\nreturns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of\nthe Scholarly-QALD-23 challenge benchmarks.", "published": "2023-11-16 12:13:49", "link": "http://arxiv.org/abs/2311.09841v1", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hijacking Large Language Models via Adversarial In-Context Learning", "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific downstream tasks by utilizing labeled examples as demonstrations\n(demos) in the precondition prompts. Despite its promising performance, ICL\nsuffers from instability with the choice and arrangement of examples.\nAdditionally, crafted adversarial attacks pose a notable threat to the\nrobustness of ICL. However, existing attacks are either easy to detect, rely on\nexternal models, or lack specificity towards ICL. This work introduces a novel\ntransferable attack against ICL to address these issues, aiming to hijack LLMs\nto generate the target response or jailbreak. Our hijacking attack leverages a\ngradient-based prompt search method to learn and append imperceptible\nadversarial suffixes to the in-context demos without directly contaminating the\nuser queries. Comprehensive experimental results across different generation\nand jailbreaking tasks highlight the effectiveness of our hijacking attack,\nresulting in distracted attention towards adversarial tokens and consequently\nleading to unwanted target outputs. We also propose a defense strategy against\nhijacking attacks through the use of extra clean demos, which enhances the\nrobustness of LLMs during ICL. Broadly, this work reveals the significant\nsecurity vulnerabilities of LLMs and emphasizes the necessity for in-depth\nstudies on their robustness.", "published": "2023-11-16 15:01:48", "link": "http://arxiv.org/abs/2311.09948v2", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Unambiguity and Fewness for Nonuniform Families of Polynomial-Size\n  Nondeterministic Finite Automata", "abstract": "Nonuniform families of polynomial-size finite automata, which are series of\nindexed finite automata having polynomially many inner states, are used in the\npast literature to solve nonuniform families of promise decision problems.\nAmong such nonuniform families of finite automata, we focus our attention, in\nparticular, on the variants of nondeterministic finite automata, which have at\nmost \"one\" (unambiguous), \"polynomially many\" (few) accepting computation\npaths, or unambiguous/few computation paths leading to each fixed\nconfiguration. When such machines are limited to make only one-way head moves,\nwe can prove with no unproven hardness assumptions that some of these variants\nare different in computational power from each other. As for two-way machines\nrestricted to instances of polynomially-bounded length, families of two-way\npolynomial-size nondeterministic finite automata are equivalent in power to\nfamilies of polynomial-size unambiguous finite automata.", "published": "2023-11-16 15:52:24", "link": "http://arxiv.org/abs/2311.09979v1", "categories": ["cs.FL", "cs.CC", "cs.CL"], "primary_category": "cs.FL"}
{"title": "The Song Describer Dataset: a Corpus of Audio Captions for\n  Music-and-Language Evaluation", "abstract": "We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of\nhigh-quality audio-caption pairs, designed for the evaluation of\nmusic-and-language models. The dataset consists of 1.1k human-written natural\nlanguage descriptions of 706 music recordings, all publicly accessible and\nreleased under Creative Common licenses. To showcase the use of our dataset, we\nbenchmark popular models on three key music-and-language tasks (music\ncaptioning, text-to-music generation and music-language retrieval). Our\nexperiments highlight the importance of cross-dataset evaluation and offer\ninsights into how researchers can use SDD to gain a broader understanding of\nmodel performance.", "published": "2023-11-16 17:52:21", "link": "http://arxiv.org/abs/2311.10057v3", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "DRESS: Instructing Large Vision-Language Models to Align and Interact\n  with Humans via Natural Language Feedback", "abstract": "We present DRESS, a large vision language model (LVLM) that innovatively\nexploits Natural Language feedback (NLF) from Large Language Models to enhance\nits alignment and interactions by addressing two key limitations in the\nstate-of-the-art LVLMs. First, prior LVLMs generally rely only on the\ninstruction finetuning stage to enhance alignment with human preferences.\nWithout incorporating extra feedback, they are still prone to generate\nunhelpful, hallucinated, or harmful responses. Second, while the visual\ninstruction tuning data is generally structured in a multi-turn dialogue\nformat, the connections and dependencies among consecutive conversational turns\nare weak. This reduces the capacity for effective multi-turn interactions. To\ntackle these, we propose a novel categorization of the NLF into two key types:\ncritique and refinement. The critique NLF identifies the strengths and\nweaknesses of the responses and is used to align the LVLMs with human\npreferences. The refinement NLF offers concrete suggestions for improvement and\nis adopted to improve the interaction ability of the LVLMs-- which focuses on\nLVLMs' ability to refine responses by incorporating feedback in multi-turn\ninteractions. To address the non-differentiable nature of NLF, we generalize\nconditional reinforcement learning for training. Our experimental results\ndemonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and\nharmless (21.03%) responses, and more effectively learn from feedback during\nmulti-turn interactions compared to SOTA LVMLs.", "published": "2023-11-16 18:37:29", "link": "http://arxiv.org/abs/2311.10081v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A Computationally Efficient Sparsified Online Newton Method", "abstract": "Second-order methods hold significant promise for enhancing the convergence\nof deep neural network training; however, their large memory and computational\ndemands have limited their practicality. Thus there is a need for scalable\nsecond-order methods that can efficiently train large models. In this paper, we\nintroduce the Sparsified Online Newton (SONew) method, a memory-efficient\nsecond-order algorithm that yields a sparsified yet effective preconditioner.\nThe algorithm emerges from a novel use of the LogDet matrix divergence measure;\nwe combine it with sparsity constraints to minimize regret in the online convex\noptimization framework. Empirically, we test our method on large scale\nbenchmarks of up to 1B parameters. We achieve up to 30% faster convergence,\n3.4% relative improvement in validation performance, and 80% relative\nimprovement in training loss, in comparison to memory efficient optimizers\nincluding first order methods. Powering the method is a surprising fact --\nimposing structured sparsity patterns, like tridiagonal and banded structure,\nrequires little to no overhead, making it as efficient and parallelizable as\nfirst-order methods. In wall-clock time, tridiagonal SONew is only about 3%\nslower per step than first-order methods but gives overall gains due to much\nfaster convergence. In contrast, one of the state-of-the-art (SOTA)\nmemory-intensive second-order methods, Shampoo, is unable to scale to large\nbenchmarks. Additionally, while Shampoo necessitates significant engineering\nefforts to scale to large benchmarks, SONew offers a more straightforward\nimplementation, increasing its practical appeal. SONew code is available at:\nhttps://github.com/devvrit/SONew", "published": "2023-11-16 18:44:22", "link": "http://arxiv.org/abs/2311.10085v1", "categories": ["cs.LG", "cs.CL", "math.OC"], "primary_category": "cs.LG"}
{"title": "A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal\n  Structures", "abstract": "The present paper introduces a novel object of study - a language fractal\nstructure. We hypothesize that a set of embeddings of all $n$-grams of a\nnatural language constitutes a representative sample of this fractal set. (We\nuse the term Hailonakea to refer to the sum total of all language fractal\nstructures, over all $n$). The paper estimates intrinsic (genuine) dimensions\nof language fractal structures for the Russian and English languages. To this\nend, we employ methods based on (1) topological data analysis and (2) a minimum\nspanning tree of a data graph for a cloud of points considered (Steele\ntheorem). For both languages, for all $n$, the intrinsic dimensions appear to\nbe non-integer values (typical for fractal sets), close to 9 for both of the\nRussian and English language.", "published": "2023-11-16 22:15:15", "link": "http://arxiv.org/abs/2311.10217v2", "categories": ["cs.CL", "cs.AI", "math.AT", "nlin.CD"], "primary_category": "cs.CL"}
{"title": "A BERT based Ensemble Approach for Sentiment Classification of Customer\n  Reviews and its Application to Nudge Marketing in e-Commerce", "abstract": "According to the literature, Product reviews are an important source of\ninformation for customers to support their buying decision. Product reviews\nimprove customer trust and loyalty. Reviews help customers in understanding\nwhat other customers think about a particular product and helps in driving\npurchase decisions. Therefore, for an e-commerce platform it is important to\nunderstand the sentiments in customer reviews to understand their products and\nservices, and it also allows them to potentially create positive consumer\ninteraction as well as long lasting relationships. Reviews also provide\ninnovative ways to market the products for an ecommerce company. One such\napproach is Nudge Marketing. Nudge marketing is a subtle way for an ecommerce\ncompany to help their customers make better decisions without hesitation.", "published": "2023-11-16 14:18:24", "link": "http://arxiv.org/abs/2311.10782v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ExFake: Towards an Explainable Fake News Detection Based on Content and\n  Social Context Information", "abstract": "ExFake is an explainable fake news detection system based on content and\ncontext-level information. It is concerned with the veracity analysis of online\nposts based on their content, social context (i.e., online users' credibility\nand historical behaviour), and data coming from trusted entities such as\nfact-checking websites and named entities. Unlike state-of-the-art systems, an\nExplainable AI (XAI) assistant is also adopted to help online social networks\n(OSN) users develop good reflexes when faced with any doubted information that\nspreads on social networks. The trustworthiness of OSN users is also addressed\nby assigning a credibility score to OSN users, as OSN users are one of the main\nculprits for spreading fake news. Experimental analysis on a real-world dataset\ndemonstrates that ExFake significantly outperforms other baseline methods for\nfake news detection.", "published": "2023-11-16 15:57:58", "link": "http://arxiv.org/abs/2311.10784v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Bergeron: Combating Adversarial Attacks through a Conscience-Based\n  Alignment Framework", "abstract": "Research into AI alignment has grown considerably since the recent\nintroduction of increasingly capable Large Language Models (LLMs).\nUnfortunately, modern methods of alignment still fail to fully prevent harmful\nresponses when models are deliberately attacked. Such vulnerabilities can lead\nto LLMs being manipulated into generating hazardous content: from instructions\nfor creating dangerous materials to inciting violence or endorsing unethical\nbehaviors. To help mitigate this issue, we introduce Bergeron: a framework\ndesigned to improve the robustness of LLMs against attacks without any\nadditional parameter fine-tuning. Bergeron is organized into two tiers; with a\nsecondary LLM acting as a guardian to the primary LLM. This framework better\nsafeguards the primary model against incoming attacks while monitoring its\noutput for any harmful content. Empirical analysis reviews that by using\nBergeron to complement models with existing alignment training, we can\nsignificantly improve the robustness and safety of multiple, commonly used\ncommercial and open-source LLMs. Specifically, we found that models integrated\nwith Bergeron are, on average, nearly seven times more resistant to attacks\ncompared to models without such support.", "published": "2023-11-16 07:31:18", "link": "http://arxiv.org/abs/2312.00029v3", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Sociodemographic Prompting is Not Yet an Effective Approach for\n  Simulating Subjective Judgments with LLMs", "abstract": "Human judgments are inherently subjective and are actively affected by\npersonal traits such as gender and ethnicity. While Large Language Models\n(LLMs) are widely used to simulate human responses across diverse contexts,\ntheir ability to account for demographic differences in subjective tasks\nremains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate\nnine popular LLMs on their ability to understand demographic differences in two\nsubjective judgment tasks: politeness and offensiveness. We find that in\nzero-shot settings, most models' predictions for both tasks align more closely\nwith labels from White participants than those from Asian or Black\nparticipants, while only a minor gender bias favoring women appears in the\npoliteness task. Furthermore, sociodemographic prompting does not consistently\nimprove and, in some cases, worsens LLMs' ability to perceive language from\nspecific sub-populations. These findings highlight potential demographic biases\nin LLMs when performing subjective judgment tasks and underscore the\nlimitations of sociodemographic prompting as a strategy to achieve pluralistic\nalignment. Code and data are available at:\nhttps://github.com/Jiaxin-Pei/LLM-as-Subjective-Judge.", "published": "2023-11-16 10:02:24", "link": "http://arxiv.org/abs/2311.09730v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "When \"A Helpful Assistant\" Is Not Really Helpful: Personas in System\n  Prompts Do Not Improve Performances of Large Language Models", "abstract": "Prompting serves as the major way humans interact with Large Language Models\n(LLM). Commercial AI systems commonly define the role of the LLM in system\nprompts. For example, ChatGPT uses ``You are a helpful assistant'' as part of\nits default system prompt. Despite current practices of adding personas to\nsystem prompts, it remains unclear how different personas affect a model's\nperformance on objective tasks. In this study, we present a systematic\nevaluation of personas in system prompts. We curate a list of 162 roles\ncovering 6 types of interpersonal relationships and 8 domains of expertise.\nThrough extensive analysis of 4 popular families of LLMs and 2,410 factual\nquestions, we demonstrate that adding personas in system prompts does not\nimprove model performance across a range of questions compared to the control\nsetting where no persona is added. Nevertheless, further analysis suggests that\nthe gender, type, and domain of the persona can all influence the resulting\nprediction accuracies. We further experimented with a list of persona search\nstrategies and found that, while aggregating results from the best persona for\neach question significantly improves prediction accuracy, automatically\nidentifying the best persona is challenging, with predictions often performing\nno better than random selection. Overall, our findings suggest that while\nadding a persona may lead to performance gains in certain settings, the effect\nof each persona can be largely random. Code and data are available at\nhttps://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.", "published": "2023-11-16 17:48:55", "link": "http://arxiv.org/abs/2311.10054v3", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving fairness for spoken language understanding in atypical speech\n  with Text-to-Speech", "abstract": "Spoken language understanding (SLU) systems often exhibit suboptimal\nperformance in processing atypical speech, typically caused by neurological\nconditions and motor impairments. Recent advancements in Text-to-Speech (TTS)\nsynthesis-based augmentation for more fair SLU have struggled to accurately\ncapture the unique vocal characteristics of atypical speakers, largely due to\ninsufficient data. To address this issue, we present a novel data augmentation\nmethod for atypical speakers by finetuning a TTS model, called Aty-TTS. Aty-TTS\nmodels speaker and atypical characteristics via knowledge transferring from a\nvoice conversion model. Then, we use the augmented data to train SLU models\nadapted to atypical speech. To train these data augmentation models and\nevaluate the resulting SLU systems, we have collected a new atypical speech\ndataset containing intent annotation. Both objective and subjective assessments\nvalidate that Aty-TTS is capable of generating high-quality atypical speech.\nFurthermore, it serves as an effective data augmentation strategy, contributing\nto more fair SLU systems that can better accommodate individuals with atypical\nspeech patterns.", "published": "2023-11-16 19:09:28", "link": "http://arxiv.org/abs/2311.10149v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "DINO-VITS: Data-Efficient Zero-Shot TTS with Self-Supervised Speaker\n  Verification Loss for Noise Robustness", "abstract": "We address zero-shot TTS systems' noise-robustness problem by proposing a\ndual-objective training for the speaker encoder using self-supervised DINO\nloss. This approach enhances the speaker encoder with the speech synthesis\nobjective, capturing a wider range of speech characteristics beneficial for\nvoice cloning. At the same time, the DINO objective improves speaker\nrepresentation learning, ensuring robustness to noise and speaker\ndiscriminability. Experiments demonstrate significant improvements in\nsubjective metrics under both clean and noisy conditions, outperforming\ntraditional speaker-encoderbased TTS systems. Additionally, we explore training\nzeroshot TTS on noisy, unlabeled data. Our two-stage training strategy,\nleveraging self-supervised speech models to distinguish between noisy and clean\nspeech, shows notable advances in similarity and naturalness, especially with\nnoisy training datasets, compared to the ASR-transcription-based approach.", "published": "2023-11-16 10:50:22", "link": "http://arxiv.org/abs/2311.09770v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "AQUATK: An Audio Quality Assessment Toolkit", "abstract": "Recent advancements in Neural Audio Synthesis (NAS) have outpaced the\ndevelopment of standardized evaluation methodologies and tools. To bridge this\ngap, we introduce AquaTk, an open-source Python library specifically designed\nto simplify and standardize the evaluation of NAS systems. AquaTk offers a\nrange of audio quality metrics, including a unique Python implementation of the\nbasic PEAQ algorithm, and operates in multiple modes to accommodate various\nuser needs.", "published": "2023-11-16 02:55:13", "link": "http://arxiv.org/abs/2311.10113v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Future Full-Ocean Deep SSPs Prediction based on Hierarchical Long\n  Short-Term Memory Neural Networks", "abstract": "The spatial-temporal distribution of underwater sound velocity affects the\npropagation mode of underwater acoustic signals. Therefore, rapid estimation\nand prediction of underwater sound velocity distribution is crucial for\nproviding underwater positioning, navigation and timing (PNT) services.\nCurrently, sound speed profile (SSP) inversion methods have a faster time\nresponse rate compared to direct measurement methods, however, most SSP\ninversion methods focus on constructing spatial dimensional sound velocity\nfields and are highly dependent on sonar observation data, thus high\nrequirements have been placed on observation data sources. To explore the\ndistribution pattern of sound velocity in the time dimension and achieve future\nSSP prediction without sonar observation data, we propose a hierarchical long\nshort-term memory (H-LSTM) neural network for SSP prediction. By our SSP\nprediction method, the sound speed distribution could be estimated without any\non-site data measurement process, so that the time efficiency could be greatly\nimproved. Through comparing with other state-of-the-art methods, H-LSTM has\nbetter accuracy performance on prediction of monthly average sound velocity\ndistribution, which is less than 1 m/s in different depth layers.", "published": "2023-11-16 03:26:47", "link": "http://arxiv.org/abs/2311.09537v1", "categories": ["cs.SD", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
{"title": "Multi-View Spectrogram Transformer for Respiratory Sound Classification", "abstract": "Deep neural networks have been applied to audio spectrograms for respiratory\nsound classification. Existing models often treat the spectrogram as a\nsynthetic image while overlooking its physical characteristics. In this paper,\na Multi-View Spectrogram Transformer (MVST) is proposed to embed different\nviews of time-frequency characteristics into the vision transformer.\nSpecifically, the proposed MVST splits the mel-spectrogram into different sized\npatches, representing the multi-view acoustic elements of a respiratory sound.\nThese patches and positional embeddings are then fed into transformer encoders\nto extract the attentional information among patches through a self-attention\nmechanism. Finally, a gated fusion scheme is designed to automatically weigh\nthe multi-view features to highlight the best one in a specific scenario.\nExperimental results on the ICBHI dataset demonstrate that the proposed MVST\nsignificantly outperforms state-of-the-art methods for classifying respiratory\nsounds.", "published": "2023-11-16 08:17:02", "link": "http://arxiv.org/abs/2311.09655v3", "categories": ["cs.SD", "cs.CV", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Formal Verification of Long Short-Term Memory based Audio Classifiers: A\n  Star based Approach", "abstract": "Formally verifying audio classification systems is essential to ensure\naccurate signal classification across real-world applications like\nsurveillance, automotive voice commands, and multimedia content management,\npreventing potential errors with serious consequences. Drawing from recent\nresearch, this study advances the utilization of star-set-based formal\nverification, extended through reachability analysis, tailored explicitly for\nLong Short-Term Memory architectures and their Convolutional variations within\nthe audio classification domain. By conceptualizing the classification process\nas a sequence of set operations, the star set-based reachability approach\nstreamlines the exploration of potential operational states attainable by the\nsystem. The paper serves as an encompassing case study, validating and\nverifying sequence audio classification analytics within real-world contexts.\nIt accentuates the necessity for robustness verification to ensure precise and\ndependable predictions, particularly in light of the impact of noise on the\naccuracy of output classifications.", "published": "2023-11-16 11:04:17", "link": "http://arxiv.org/abs/2311.12130v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
