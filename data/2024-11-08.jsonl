{"title": "Multi-asset and generalised Local Volatility. An efficient implementation", "abstract": "This article presents a generic hybrid numerical method to price a wide range\nof options on one or several assets, as well as assets with stochastic drift or\nvolatility. In particular for equity and interest rate hybrid with local\nvolatility.", "published": "2024-11-08 09:16:59", "link": "http://arxiv.org/abs/2411.05425v1", "categories": ["q-fin.CP", "q-fin.PR"], "primary_category": "q-fin.CP"}
{"title": "Optimal reinsurance and investment via stochastic projected gradient method based on Malliavin calculus", "abstract": "This paper proposes a new approach using the stochastic projected gradient\nmethod and Malliavin calculus for optimal reinsurance and investment\nstrategies. Unlike traditional methodologies, we aim to optimize static\ninvestment and reinsurance strategies by directly minimizing the ruin\nprobability. Furthermore, we provide a convergence analysis of the stochastic\nprojected gradient method for general constrained optimization problems whose\nobjective function has H\\\"older continuous gradient. Numerical experiments show\nthe effectiveness of our proposed method.", "published": "2024-11-08 09:12:02", "link": "http://arxiv.org/abs/2411.05417v1", "categories": ["q-fin.MF", "math.OC", "q-fin.CP"], "primary_category": "q-fin.MF"}
{"title": "Enforcing asymptotic behavior with DNNs for approximation and regression in finance", "abstract": "We propose a simple methodology to approximate functions with given\nasymptotic behavior by specifically constructed terms and an unconstrained deep\nneural network (DNN).\n  The methodology we describe extends to various asymptotic behaviors and\nmultiple dimensions and is easy to implement. In this work we demonstrate it\nfor linear asymptotic behavior in one-dimensional examples. We apply it to\nfunction approximation and regression problems where we measure approximation\nof only function values (``Vanilla Machine Learning''-VML) or also\napproximation of function and derivative values (``Differential Machine\nLearning''-DML) on several examples. We see that enforcing given asymptotic\nbehavior leads to better approximation and faster convergence.", "published": "2024-11-08 01:11:58", "link": "http://arxiv.org/abs/2411.05257v1", "categories": ["q-fin.CP", "cs.NA", "math.NA", "q-fin.PR"], "primary_category": "q-fin.CP"}
{"title": "Model-free portfolio allocation in continuous-time", "abstract": "We present a non-probabilistic, path-by-path framework for studying\npath-dependent (i.e., where weight is a functional of time and historical\ntime-series), long-only portfolio allocation in continuous-time based on [Chiu\n& Cont '23], where the fundamental concept of self-financing was introduced,\nindependent of any integration theory. In this article, we extend this concept\nto a portfolio allocation strategy and characterize it by a path-dependent\npartial differential equation. We derive the general explicit solution that\ndescribes the evolution of wealth in generic markets, including price paths\nthat may not evolve continuously or exhibit variation of any order. Explicit\nsolution examples are provided.\n  As an application of our continuous-time, path-dependent framework, we extend\nan aggregating algorithm of [Vovk '90] and the universal algorithm of [Cover\n'91] to continuous-time algorithms that combine multiple strategies into a\nsingle strategy. These continuous-time (meta) algorithms take multiple\nstrategies as input (which may themselves be generated by other algorithms) and\ntrack the wealth generated by the best individual strategy and the best convex\ncombination of strategies, with tracking error bounds in log wealth of order\nO(1) and O(ln t), respectively. This work extends Cover's theorem [Cover '91,\nThm 6.1] to a continuous-time, model-free setting.", "published": "2024-11-08 10:48:54", "link": "http://arxiv.org/abs/2411.05470v2", "categories": ["q-fin.MF"], "primary_category": "q-fin.MF"}
{"title": "Filling in Missing FX Implied Volatilities with Uncertainties: Improving VAE-Based Volatility Imputation", "abstract": "Missing data is a common problem in finance and often requires methods to\nfill in the gaps, or in other words, imputation. In this work, we focused on\nthe imputation of missing implied volatilities for FX options. Prior work has\nused variational autoencoders (VAEs), a neural network-based approach, to solve\nthis problem; however, using stronger classical baselines such as Heston with\njumps can significantly outperform their results. We show that simple\nmodifications to the architecture of the VAE lead to significant imputation\nperformance improvements (e.g., in low missingness regimes, nearly cutting the\nerror by half), removing the necessity of using $\\beta$-VAEs. Further, we\nmodify the VAE imputation algorithm in order to better handle the uncertainty\nin data, as well as to obtain accurate uncertainty estimates around imputed\nvalues.", "published": "2024-11-08 22:30:28", "link": "http://arxiv.org/abs/2411.05998v1", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "primary_category": "q-fin.ST"}
{"title": "Approaching multifractal complexity in decentralized cryptocurrency trading", "abstract": "Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.", "published": "2024-11-08 20:27:31", "link": "http://arxiv.org/abs/2411.05951v1", "categories": ["q-fin.ST", "cs.CE", "q-fin.TR", "stat.AP"], "primary_category": "q-fin.ST"}
{"title": "How Wash Traders Exploit Market Conditions in Cryptocurrency Markets", "abstract": "Wash trading, the practice of simultaneously placing buy and sell orders for\nthe same asset to inflate trading volume, has been prevalent in cryptocurrency\nmarkets. This paper investigates whether wash traders in Bitcoin act\ndeliberately to exploit market conditions and identifies the characteristics of\nsuch manipulative behavior. Using a unique dataset of 18 million transactions\nfrom Mt. Gox, once the largest Bitcoin exchange, I find that wash trading\nintensifies when legitimate trading volume is low and diminishes when it is\nhigh, indicating strategic timing to maximize impact in less liquid markets.\nThe activity also exhibits spillover effects across platforms and decreases\nwhen trading volumes in other asset classes like stocks or gold rise,\nsuggesting sensitivity to broader market dynamics. Additionally, wash traders\nexploit periods of heightened media attention and online rumors to amplify\ntheir influence, causing rapid but short-lived spikes in legitimate trading\nvolume. Using an exogenous demand shock associated with illicit online\nmarketplaces, I find that wash trading responds to contemporaneous events\naffecting Bitcoin demand. These results advance the understanding of\nmanipulative practices in digital currency markets and have significant\nimplications for regulators aiming to detect and prevent wash trading.", "published": "2024-11-08 17:32:01", "link": "http://arxiv.org/abs/2411.08720v1", "categories": ["q-fin.TR", "econ.GN", "q-fin.EC", "91G80", "J.4"], "primary_category": "q-fin.TR"}
{"title": "What talking you?: Translating Code-Mixed Messaging Texts to English", "abstract": "Translation of code-mixed texts to formal English allow a wider audience to\nunderstand these code-mixed languages, and facilitate downstream analysis\napplications such as sentiment analysis. In this work, we look at translating\nSinglish, which is colloquial Singaporean English, to formal standard English.\nSinglish is formed through the code-mixing of multiple Asian languages and\ndialects. We analysed the presence of other Asian languages and variants which\ncan facilitate translation. Our dataset is short message texts, written as\ninformal communication between Singlish speakers. We use a multi-step prompting\nscheme on five Large Language Models (LLMs) for language detection and\ntranslation. Our analysis show that LLMs do not perform well in this task, and\nwe describe the challenges involved in translation of code-mixed languages. We\nalso release our dataset in this link https://github.com/luoqichan/singlish.", "published": "2024-11-08 00:46:24", "link": "http://arxiv.org/abs/2411.05253v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SciDQA: A Deep Reading Comprehension Dataset over Scientific Papers", "abstract": "Scientific literature is typically dense, requiring significant background\nknowledge and deep comprehension for effective engagement. We introduce SciDQA,\na new dataset for reading comprehension that challenges LLMs for a deep\nunderstanding of scientific articles, consisting of 2,937 QA pairs. Unlike\nother scientific QA datasets, SciDQA sources questions from peer reviews by\ndomain experts and answers by paper authors, ensuring a thorough examination of\nthe literature. We enhance the dataset's quality through a process that\ncarefully filters out lower quality questions, decontextualizes the content,\ntracks the source document across different versions, and incorporates a\nbibliography for multi-document question-answering. Questions in SciDQA\nnecessitate reasoning across figures, tables, equations, appendices, and\nsupplementary materials, and require multi-document reasoning. We evaluate\nseveral open-source and proprietary LLMs across various configurations to\nexplore their capabilities in generating relevant and factual responses. Our\ncomprehensive evaluation, based on metrics for surface-level similarity and LLM\njudgements, highlights notable performance discrepancies. SciDQA represents a\nrigorously curated, naturally derived scientific QA dataset, designed to\nfacilitate research on complex scientific text understanding.", "published": "2024-11-08 05:28:22", "link": "http://arxiv.org/abs/2411.05338v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word reuse and combination support efficient communication of emerging\n  concepts", "abstract": "A key function of the lexicon is to express novel concepts as they emerge\nover time through a process known as lexicalization. The most common\nlexicalization strategies are the reuse and combination of existing words, but\nthey have typically been studied separately in the areas of word meaning\nextension and word formation. Here we offer an information-theoretic account of\nhow both strategies are constrained by a fundamental tradeoff between competing\ncommunicative pressures: word reuse tends to preserve the average length of\nword forms at the cost of less precision, while word combination tends to\nproduce more informative words at the expense of greater word length. We test\nour proposal against a large dataset of reuse items and compounds that appeared\nin English, French and Finnish over the past century. We find that these\nhistorically emerging items achieve higher levels of communicative efficiency\nthan hypothetical ways of constructing the lexicon, and both literal reuse\nitems and compounds tend to be more efficient than their non-literal\ncounterparts. These results suggest that reuse and combination are both\nconsistent with a unified account of lexicalization grounded in the theory of\nefficient communication.", "published": "2024-11-08 07:20:21", "link": "http://arxiv.org/abs/2411.05379v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Low-Resource Harmful Meme Detection with LMM Agents", "abstract": "The proliferation of Internet memes in the age of social media necessitates\neffective identification of harmful ones. Due to the dynamic nature of memes,\nexisting data-driven models may struggle in low-resource scenarios where only a\nfew labeled examples are available. In this paper, we propose an agency-driven\nframework for low-resource harmful meme detection, employing both outward and\ninward analysis with few-shot annotated samples. Inspired by the powerful\ncapacity of Large Multimodal Models (LMMs) on multimodal reasoning, we first\nretrieve relative memes with annotations to leverage label information as\nauxiliary signals for the LMM agent. Then, we elicit knowledge-revising\nbehavior within the LMM agent to derive well-generalized insights into meme\nharmfulness. By combining these strategies, our approach enables dialectical\nreasoning over intricate and implicit harm-indicative patterns. Extensive\nexperiments conducted on three meme datasets demonstrate that our proposed\napproach achieves superior performance than state-of-the-art methods on the\nlow-resource harmful meme detection task.", "published": "2024-11-08 07:43:15", "link": "http://arxiv.org/abs/2411.05383v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gap-Filling Prompting Enhances Code-Assisted Mathematical Reasoning", "abstract": "Despite the strong performance of large language models (LLMs) in tasks like\nmathematical reasoning, their practical use is limited by high computational\ndemands and proprietary restrictions. Chain-of-thought (CoT) and\nprogram-of-thought (PoT) fine-tuning are common methods to transfer LLM\nknowledge to small language models (SLMs). However, CoT often leads to\ncalculation errors in SLMs, while PoT has shown more promise. While most\nPoT-based approaches focus on direct problem-to-code conversion or extracting\nonly the key information from questions and then providing code solution for\nit, this work emphasizes filling the gaps in the question to clearly illustrate\nthe solution path, which can be challenging for an SLM to understand when such\ninformation is not explicitly provided. Therefore, this paper introduces\nGap-Filling Prompting (GFP), a novel two-step prompting strategy designed to\nenhance the problem-solving process for SLMs. The first step identifies these\ngaps and provides hints for filling them, while the second step adds the hints\nto the question to generate a final code solution. Experimental results on two\nbenchmark datasets demonstrate that GFP significantly improves the mathematical\nreasoning abilities of SLMs.", "published": "2024-11-08 08:52:59", "link": "http://arxiv.org/abs/2411.05407v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Supporting Automated Fact-checking across Topics: Similarity-driven\n  Gradual Topic Learning for Claim Detection", "abstract": "Selecting check-worthy claims for fact-checking is considered a crucial part\nof expediting the fact-checking process by filtering out and ranking the\ncheck-worthy claims for being validated among the impressive amount of claims\ncould be found online. The check-worthy claim detection task, however, becomes\nmore challenging when the model needs to deal with new topics that differ from\nthose seen earlier. In this study, we propose a domain-adaptation framework for\ncheck-worthy claims detection across topics for the Arabic language to adopt a\nnew topic, mimicking a real-life scenario of the daily emergence of events\nworldwide. We propose the Gradual Topic Learning (GTL) model, which builds an\nability to learning gradually and emphasizes the check-worthy claims for the\ntarget topic during several stages of the learning process. In addition, we\nintroduce the Similarity-driven Gradual Topic Learning (SGTL) model that\nsynthesizes gradual learning with a similarity-based strategy for the target\ntopic. Our experiments demonstrate the effectiveness of our proposed model,\nshowing an overall tendency for improving performance over the state-of-the-art\nbaseline across 11 out of the 14 topics under study.", "published": "2024-11-08 10:24:00", "link": "http://arxiv.org/abs/2411.05460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KyrgyzNLP: Challenges, Progress, and Future", "abstract": "Large language models (LLMs) have excelled in numerous benchmarks, advancing\nAI applications in both linguistic and non-linguistic tasks. However, this has\nprimarily benefited well-resourced languages, leaving less-resourced ones\n(LRLs) at a disadvantage. In this paper, we highlight the current state of the\nNLP field in the specific LRL: kyrgyz tili.\n  Human evaluation, including annotated datasets created by native speakers,\nremains an irreplaceable component of reliable NLP performance, especially for\nLRLs where automatic evaluations can fall short. In recent assessments of the\nresources for Turkic languages, Kyrgyz is labeled with the status 'Scraping\nBy', a severely under-resourced language spoken by millions. This is concerning\ngiven the growing importance of the language, not only in Kyrgyzstan but also\namong diaspora communities where it holds no official status.\n  We review prior efforts in the field, noting that many of the publicly\navailable resources have only recently been developed, with few exceptions\nbeyond dictionaries (the processed data used for the analysis is presented at\nhttps://kyrgyznlp.github.io/). While recent papers have made some headway, much\nmore remains to be done. Despite interest and support from both business and\ngovernment sectors in the Kyrgyz Republic, the situation for Kyrgyz language\nresources remains challenging. We stress the importance of community-driven\nefforts to build these resources, ensuring the future advancement\nsustainability. We then share our view of the most pressing challenges in\nKyrgyz NLP. Finally, we propose a roadmap for future development in terms of\nresearch topics and language resources.", "published": "2024-11-08 12:03:31", "link": "http://arxiv.org/abs/2411.05503v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LBPE: Long-token-first Tokenization to Improve Large Language Models", "abstract": "The prevalent use of Byte Pair Encoding (BPE) in Large Language Models (LLMs)\nfacilitates robust handling of subword units and avoids issues of\nout-of-vocabulary words. Despite its success, a critical challenge persists:\nlong tokens, rich in semantic information, have fewer occurrences in tokenized\ndatasets compared to short tokens, which can result in imbalanced learning\nissue across different tokens. To address that, we propose LBPE, which\nprioritizes long tokens during the encoding process. LBPE generates tokens\naccording to their reverse ranks of token length rather than their ranks in the\nvocabulary, granting longer tokens higher priority during the encoding process.\nConsequently, LBPE smooths the frequency differences between short and long\ntokens, and thus mitigates the learning imbalance. Extensive experiments across\ndiverse language modeling tasks demonstrate that LBPE consistently outperforms\nthe original BPE, well demonstrating its effectiveness.", "published": "2024-11-08 12:03:36", "link": "http://arxiv.org/abs/2411.05504v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Good is Your Wikipedia?", "abstract": "Wikipedia's perceived high quality and broad language coverage have\nestablished it as a fundamental resource in multilingual NLP. In the context of\nlow-resource languages, however, these quality assumptions are increasingly\nbeing scrutinised. This paper critically examines the data quality of Wikipedia\nin a non-English setting by subjecting it to various quality filtering\ntechniques, revealing widespread issues such as a high percentage of one-line\narticles and duplicate articles. We evaluate the downstream impact of quality\nfiltering on Wikipedia and find that data quality pruning is an effective means\nfor resource-efficient training without hurting performance, especially for\nlow-resource languages. Moreover, we advocate for a shift in perspective from\nseeking a general definition of data quality towards a more language- and\ntask-specific one. Ultimately, we aim for this study to serve as a guide to\nusing Wikipedia for pretraining in a multilingual setting.", "published": "2024-11-08 12:35:58", "link": "http://arxiv.org/abs/2411.05527v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Answerability of Queries in Retrieval-Augmented Code\n  Generation", "abstract": "Thanks to unprecedented language understanding and generation capabilities of\nlarge language model (LLM), Retrieval-augmented Code Generation (RaCG) has\nrecently been widely utilized among software developers. While this has\nincreased productivity, there are still frequent instances of incorrect codes\nbeing provided. In particular, there are cases where plausible yet incorrect\ncodes are generated for queries from users that cannot be answered with the\ngiven queries and API descriptions. This study proposes a task for evaluating\nanswerability, which assesses whether valid answers can be generated based on\nusers' queries and retrieved APIs in RaCG. Additionally, we build a benchmark\ndataset called Retrieval-augmented Code Generability Evaluation (RaCGEval) to\nevaluate the performance of models performing this task. Experimental results\nshow that this task remains at a very challenging level, with baseline models\nexhibiting a low performance of 46.7%. Furthermore, this study discusses\nmethods that could significantly improve performance.", "published": "2024-11-08 13:09:14", "link": "http://arxiv.org/abs/2411.05547v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating and Adapting Large Language Models to Represent Folktales in\n  Low-Resource Languages", "abstract": "Folktales are a rich resource of knowledge about the society and culture of a\ncivilisation. Digital folklore research aims to use automated techniques to\nbetter understand these folktales, and it relies on abstract representations of\nthe textual data. Although a number of large language models (LLMs) claim to be\nable to represent low-resource langauges such as Irish and Gaelic, we present\ntwo classification tasks to explore how useful these representations are, and\nthree adaptations to improve the performance of these models. We find that\nadapting the models to work with longer sequences, and continuing pre-training\non the domain of folktales improves classification performance, although these\nfindings are tempered by the impressive performance of a baseline SVM with\nnon-contextual features.", "published": "2024-11-08 14:26:56", "link": "http://arxiv.org/abs/2411.05593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Fake News on Social Media Towards Public Users of Different\n  Age Groups", "abstract": "This study examines how fake news affects social media users across a range\nof age groups and how machine learning (ML) and artificial intelligence (AI)\ncan help reduce the spread of false information. The paper evaluates various\nmachine learning models for their efficacy in identifying and categorizing fake\nnews and examines current trends in the spread of fake news, including deepfake\ntechnology. The study assesses four models using a Kaggle dataset: Random\nForest, Support Vector Machine (SVM), Neural Networks, and Logistic Regression.\nThe results show that SVM and neural networks perform better than other models,\nwith accuracies of 93.29% and 93.69%, respectively. The study also emphasises\nhow people in the elder age group diminished capacity for critical analysis of\nnews content makes them more susceptible to disinformation. Natural language\nprocessing (NLP) and deep learning approaches have the potential to improve the\naccuracy of false news detection. Biases in AI and ML models and difficulties\nin identifying information generated by AI continue to be major problems in\nspite of the developments. The study recommends that datasets be expanded to\nencompass a wider range of languages and that detection algorithms be\ncontinuously improved to keep up with the latest advancements in disinformation\ntactics. In order to combat fake news and promote an informed and resilient\nsociety, this study emphasizes the value of cooperative efforts between AI\nresearchers, social media platforms, and governments.", "published": "2024-11-08 15:32:20", "link": "http://arxiv.org/abs/2411.05638v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing Open-Source Large Language Models on Argumentation Mining\n  Subtasks", "abstract": "We explore the capability of four open-sourcelarge language models (LLMs) in\nargumentation mining (AM). We conduct experiments on three different corpora;\npersuasive essays(PE), argumentative microtexts (AMT) Part 1 and Part 2, based\non two argumentation mining sub-tasks: (i) argumentative discourse units\nclassifications (ADUC), and (ii) argumentative relation classification (ARC).\nThis work aims to assess the argumentation capability of open-source LLMs,\nincluding Mistral 7B, Mixtral8x7B, LlamA2 7B and LlamA3 8B in both, zero-shot\nand few-shot scenarios. Our analysis contributes to further assessing\ncomputational argumentation with open-source LLMs in future research efforts.", "published": "2024-11-08 15:34:08", "link": "http://arxiv.org/abs/2411.05639v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Model Capability in Vietnamese Fact-Checking\n  Data Generation", "abstract": "Large Language Models (LLMs), with gradually improving reading comprehension\nand reasoning capabilities, are being applied to a range of complex language\ntasks, including the automatic generation of language data for various\npurposes. However, research on applying LLMs for automatic data generation in\nlow-resource languages like Vietnamese is still underdeveloped and lacks\ncomprehensive evaluation. In this paper, we explore the use of LLMs for\nautomatic data generation for the Vietnamese fact-checking task, which faces\nsignificant data limitations. Specifically, we focus on fact-checking data\nwhere claims are synthesized from multiple evidence sentences to assess the\ninformation synthesis capabilities of LLMs. We develop an automatic data\nconstruction process using simple prompt techniques on LLMs and explore several\nmethods to improve the quality of the generated data. To evaluate the quality\nof the data generated by LLMs, we conduct both manual quality assessments and\nperformance evaluations using language models. Experimental results and manual\nevaluations illustrate that while the quality of the generated data has\nsignificantly improved through fine-tuning techniques, LLMs still cannot match\nthe data quality produced by humans.", "published": "2024-11-08 15:35:43", "link": "http://arxiv.org/abs/2411.05641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unmasking the Limits of Large Language Models: A Systematic Evaluation\n  of Masked Text Processing Ability through MskQA and MskCal", "abstract": "This paper sheds light on the limitations of Large Language Models (LLMs) by\nrigorously evaluating their ability to process masked text. We introduce two\nnovel tasks: MskQA, measuring reasoning on masked question-answering datasets\nlike RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic\nproblems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some\nresilience to masked text, their performance is highly contingent on masking\nrates and semantic cues. Specifically, \"solid masking,\" where semantic clues\nare entirely absent, leads to a significant performance drop compared to\n\"partial lifting,\" where some semantic information is retained, indicating\nLLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently\noutperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to\nhandle numerical reasoning with masked text. This underscores the crucial role\nof semantic cues in the reasoning process of LLMs. Our study illuminates the\ninterplay between background knowledge and reasoning ability in masked text\nprocessing, paving the way for a deeper understanding of LLM capabilities and\nlimitations, and highlighting the need for more robust evaluation methods to\naccurately assess their true comprehension abilities.", "published": "2024-11-08 16:07:47", "link": "http://arxiv.org/abs/2411.05665v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multi-hop Evidence Pursuit Meets the Web: Team Papelo at FEVER 2024", "abstract": "Separating disinformation from fact on the web has long challenged both the\nsearch and the reasoning powers of humans. We show that the reasoning power of\nlarge language models (LLMs) and the retrieval power of modern search engines\ncan be combined to automate this process and explainably verify claims. We\nintegrate LLMs and search under a multi-hop evidence pursuit strategy. This\nstrategy generates an initial question based on an input claim using a sequence\nto sequence model, searches and formulates an answer to the question, and\niteratively generates follow-up questions to pursue the evidence that is\nmissing using an LLM. We demonstrate our system on the FEVER 2024 (AVeriTeC)\nshared task. Compared to a strategy of generating all the questions at once,\nour method obtains .045 higher label accuracy and .155 higher AVeriTeC score\n(evaluating the adequacy of the evidence). Through ablations, we show the\nimportance of various design choices, such as the question generation method,\nmedium-sized context, reasoning with one document at a time, adding metadata,\nparaphrasing, reducing the problem to two classes, and reconsidering the final\nverdict. Our submitted system achieves .510 AVeriTeC score on the dev set and\n.477 AVeriTeC score on the test set.", "published": "2024-11-08 18:25:06", "link": "http://arxiv.org/abs/2411.05762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RefreshKV: Updating Small KV Cache During Long-form Generation", "abstract": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.", "published": "2024-11-08 18:57:07", "link": "http://arxiv.org/abs/2411.05787v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "One Small and One Large for Document-level Event Argument Extraction", "abstract": "Document-level Event Argument Extraction (EAE) faces two challenges due to\nincreased input length: 1) difficulty in distinguishing semantic boundaries\nbetween events, and 2) interference from redundant information. To address\nthese issues, we propose two methods. The first method introduces the Co and\nStructure Event Argument Extraction model (CsEAE) based on Small Language\nModels (SLMs). CsEAE includes a co-occurrences-aware module, which integrates\ninformation about all events present in the current input through context\nlabeling and co-occurrences event prompts extraction. Additionally, CsEAE\nincludes a structure-aware module that reduces interference from redundant\ninformation by establishing structural relationships between the sentence\ncontaining the trigger and other sentences in the document. The second method\nintroduces new prompts to transform the extraction task into a generative task\nsuitable for Large Language Models (LLMs), addressing gaps in EAE performance\nusing LLMs under Supervised Fine-Tuning (SFT) conditions. We also fine-tuned\nmultiple datasets to develop an LLM that performs better across most datasets.\nFinally, we applied insights from CsEAE to LLMs, achieving further performance\nimprovements. This suggests that reliable insights validated on SLMs are also\napplicable to LLMs. We tested our models on the Rams, WikiEvents, and MLEE\ndatasets. The CsEAE model achieved improvements of 2.1\\%, 2.3\\%, and 3.2\\% in\nthe Arg-C F1 metric compared to the baseline, PAIE~\\cite{PAIE}. For LLMs, we\ndemonstrated that their performance on document-level datasets is comparable to\nthat of SLMs~\\footnote{All code is available at\nhttps://github.com/simon-p-j-r/CsEAE}.", "published": "2024-11-08 14:44:01", "link": "http://arxiv.org/abs/2411.05895v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reducing Distraction in Long-Context Language Models by Focused Learning", "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their capacity to process long contexts. However, effectively\nutilizing this long context remains a challenge due to the issue of\ndistraction, where irrelevant information dominates lengthy contexts, causing\nLLMs to lose focus on the most relevant segments. To address this, we propose a\nnovel training method that enhances LLMs' ability to discern relevant\ninformation through a unique combination of retrieval-based data augmentation\nand contrastive learning. Specifically, during fine-tuning with long contexts,\nwe employ a retriever to extract the most relevant segments, serving as\naugmented inputs. We then introduce an auxiliary contrastive learning objective\nto explicitly ensure that outputs from the original context and the retrieved\nsub-context are closely aligned. Extensive experiments on long single-document\nand multi-document QA benchmarks demonstrate the effectiveness of our proposed\nmethod.", "published": "2024-11-08 19:27:42", "link": "http://arxiv.org/abs/2411.05928v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Empirical Impact of Data Sanitization on Language Models", "abstract": "Data sanitization in the context of language modeling involves identifying\nsensitive content, such as personally identifiable information (PII), and\nredacting them from a dataset corpus. It is a common practice used in natural\nlanguage processing (NLP) to maintain privacy. Nevertheless, the impact of data\nsanitization on the language understanding capability of a language model\nremains less studied. This paper empirically analyzes the effects of data\nsanitization across several benchmark language-modeling tasks including\ncomprehension question answering (Q&A), entailment, sentiment analysis, and\ntext classification. Our experiments cover a wide spectrum comprising\nfinetuning small-scale language models, to prompting large language models\n(LLMs), on both original and sanitized datasets, and comparing their\nperformance across the tasks. Interestingly, our results suggest that for some\ntasks such as sentiment analysis or entailment, the impact of redaction is\nquite low, typically around 1-5%, while for tasks such as comprehension Q&A\nthere is a big drop of >25% in performance observed in redacted queries as\ncompared to the original. For tasks that have a higher impact, we perform a\ndeeper dive to inspect the presence of task-critical entities. Finally, we\ninvestigate correlation between performance and number of redacted entities,\nand also suggest a strategy to repair an already redacted dataset by means of\ncontent-based subsampling. Additional details are available at\nhttps://sites.google.com/view/datasan.", "published": "2024-11-08 21:22:37", "link": "http://arxiv.org/abs/2411.05978v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-Grained Reward Optimization for Machine Translation using Error\n  Severity Mappings", "abstract": "Reinforcement learning (RL) has been proven to be an effective and robust\nmethod for training neural machine translation systems, especially when paired\nwith powerful reward models that accurately assess translation quality.\nHowever, most research has focused on RL methods that use sentence-level\nfeedback, which leads to inefficient learning signals due to the reward\nsparsity problem -- the model receives a single score for the entire sentence.\nTo address this, we introduce a novel approach that leverages fine-grained\ntoken-level reward mechanisms with RL methods. We use xCOMET, a\nstate-of-the-art quality estimation system as our token-level reward model.\nxCOMET provides detailed feedback by predicting fine-grained error spans and\ntheir severity given source-translation pairs. We conduct experiments on small\nand large translation datasets to compare the impact of sentence-level versus\nfine-grained reward signals on translation quality. Our results show that\ntraining with token-level rewards improves translation quality across language\npairs over baselines according to automatic and human evaluation. Furthermore,\ntoken-level reward optimization also improves training stability, evidenced by\na steady increase in mean rewards over training epochs.", "published": "2024-11-08 21:55:37", "link": "http://arxiv.org/abs/2411.05986v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GUIDEQ: Framework for Guided Questioning for progressive informational\n  collection and classification", "abstract": "Question Answering (QA) is an important part of tasks like text\nclassification through information gathering. These are finding increasing use\nin sectors like healthcare, customer support, legal services, etc., to collect\nand classify responses into actionable categories. LLMs, although can support\nQA systems, they face a significant challenge of insufficient or missing\ninformation for classification. Although LLMs excel in reasoning, the models\nrely on their parametric knowledge to answer. However, questioning the user\nrequires domain-specific information aiding to collect accurate information.\nOur work, GUIDEQ, presents a novel framework for asking guided questions to\nfurther progress a partial information. We leverage the explainability derived\nfrom the classifier model for along with LLMs for asking guided questions to\nfurther enhance the information. This further information helps in more\naccurate classification of a text. GUIDEQ derives the most significant\nkey-words representative of a label using occlusions. We develop GUIDEQ's\nprompting strategy for guided questions based on the top-3 classifier label\noutputs and the significant words, to seek specific and relevant information,\nand classify in a targeted manner. Through our experimental results, we\ndemonstrate that GUIDEQ outperforms other LLM-based baselines, yielding\nimproved F1-Score through the accurate collection of relevant further\ninformation. We perform various analytical studies and also report better\nquestion quality compared to our method.", "published": "2024-11-08 22:03:54", "link": "http://arxiv.org/abs/2411.05991v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Seeing Through the Fog: A Cost-Effectiveness Analysis of Hallucination\n  Detection Systems", "abstract": "This paper presents a comparative analysis of hallucination detection systems\nfor AI, focusing on automatic summarization and question answering tasks for\nLarge Language Models (LLMs). We evaluate different hallucination detection\nsystems using the diagnostic odds ratio (DOR) and cost-effectiveness metrics.\nOur results indicate that although advanced models can perform better they come\nat a much higher cost. We also demonstrate how an ideal hallucination detection\nsystem needs to maintain performance across different model sizes. Our findings\nhighlight the importance of choosing a detection system aligned with specific\napplication needs and resource constraints. Future research will explore hybrid\nsystems and automated identification of underperforming components to enhance\nAI reliability and efficiency in detecting and mitigating hallucinations.", "published": "2024-11-08 02:06:41", "link": "http://arxiv.org/abs/2411.05270v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "SpecHub: Provable Acceleration to Multi-Draft Speculative Decoding", "abstract": "Large Language Models (LLMs) have become essential in advancing natural\nlanguage processing (NLP) tasks, but their sequential token generation limits\ninference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising\nsolution by using a smaller draft model to generate multiple token sequences,\nwhich the target LLM verifies in parallel. However, current heuristic\napproaches, such as Recursive Rejection Sampling (RRS), suffer from low\nacceptance rates in subsequent drafts, limiting the advantages of using\nmultiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can\ntheoretically improve acceptance rates, but its computational cost is too high\nfor real-time use. We present SpecHub, a novel, efficient sampling-verification\nmethod for MDSD that improves acceptance rates with only linear computational\noverhead. By simplifying the OTM problem into a compact Linear Programming\nmodel, SpecHub significantly reduces computational complexity. It further\naccelerates sampling by leveraging a sparse joint distribution, focusing\ncomputation on high-probability token sequences. In extensive experiments,\nSpechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step\nthan RRS and RRS without replacement. We attach our code at\n\\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.", "published": "2024-11-08 02:47:07", "link": "http://arxiv.org/abs/2411.05289v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning Robustness of LLMs to Adversarial Typographical Errors", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning using Chain-of-Thought (CoT) prompting. However, CoT can be biased by\nusers' instruction. In this work, we study the reasoning robustness of LLMs to\ntypographical errors, which can naturally occur in users' queries. We design an\nAdversarial Typo Attack ($\\texttt{ATA}$) algorithm that iteratively samples\ntypos for words that are important to the query and selects the edit that is\nmost likely to succeed in attacking. It shows that LLMs are sensitive to\nminimal adversarial typographical changes. Notably, with 1 character edit,\nMistral-7B-Instruct's accuracy drops from 43.7% to 38.6% on GSM8K, while with 8\ncharacter edits the performance further drops to 19.2%. To extend our\nevaluation to larger and closed-source LLMs, we develop the $\\texttt{R$^2$ATA}$\nbenchmark, which assesses models' $\\underline{R}$easoning\n$\\underline{R}$obustness to $\\underline{\\texttt{ATA}}$. It includes adversarial\ntypographical questions derived from three widely used reasoning\ndatasets-GSM8K, BBH, and MMLU-by applying $\\texttt{ATA}$ to open-source LLMs.\n$\\texttt{R$^2$ATA}$ demonstrates remarkable transferability and causes notable\nperformance drops across multiple super large and closed-source LLMs.", "published": "2024-11-08 05:54:05", "link": "http://arxiv.org/abs/2411.05345v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for\n  Measuring the Capabilities of Spoken Language Models with 180 Tasks", "abstract": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized\nhuman-machine interactions by seamlessly integrating various forms of data.\nDeveloping a universal spoken language model that comprehends a wide range of\nnatural language instructions is critical for bridging communication gaps and\nfacilitating more intuitive interactions. However, the absence of a\ncomprehensive evaluation benchmark poses a significant challenge. We present\nDynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive\nevaluation of instruction-based universal speech models. Building upon the\nfirst generation, this second version incorporates 125 new tasks contributed\ncollaboratively by the global research community, expanding the benchmark to a\ntotal of 180 tasks, making it the largest benchmark for speech and audio\nevaluation. While the first generation of Dynamic-SUPERB was limited to\nclassification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation\ncapabilities by introducing a wide array of novel and diverse tasks, including\nregression and sequence generation, across speech, music, and environmental\naudio. Evaluation results indicate that none of the models performed well\nuniversally. SALMONN-13B excelled in English ASR, while WavLLM demonstrated\nhigh accuracy in emotion recognition, but current models still require further\ninnovations to handle a broader range of tasks. We will soon open-source all\ntask data and the evaluation pipeline.", "published": "2024-11-08 06:33:22", "link": "http://arxiv.org/abs/2411.05361v1", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Benchmarking Distributional Alignment of Large Language Models", "abstract": "Language models (LMs) are increasingly used as simulacra for people, yet\ntheir ability to match the distribution of views of a specific demographic\ngroup and be \\textit{distributionally aligned} remains uncertain. This notion\nof distributional alignment is complex, as there is significant variation in\nthe types of attributes that are simulated. Prior works have underexplored the\nrole of three critical variables -- the question domain, steering method, and\ndistribution expression method -- which motivates our contribution of a\nbenchmark explicitly addressing these dimensions. We construct a dataset\nexpanding beyond political values, create human baselines for this task, and\nevaluate the extent to which an LM can align with a particular group's opinion\ndistribution to inform design choices of such simulation systems. Our analysis\nreveals open problems regarding if, and how, LMs can be used to simulate\nhumans, and that LLMs can more accurately describe the opinion distribution\nthan simulate such distributions.", "published": "2024-11-08 08:41:17", "link": "http://arxiv.org/abs/2411.05403v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "An Early FIRST Reproduction and Improvements to Single-Token Decoding\n  for Fast Listwise Reranking", "abstract": "Recent advances have demonstrated that large language models (LLMs) excel as\nlistwise rerankers, but their high computational demands remain a barrier to\nwidespread adoption. Further, the traditional language modeling (LM) objective\nis not ideally suited for reranking tasks. FIRST is a novel approach that\naddresses these challenges by integrating a learning-to-rank objective and\nleveraging the logits of only the first generated token, thereby significantly\nreducing inference latency compared to traditional LLM rerankers. In this\nstudy, we extend the evaluation of FIRST to the TREC Deep Learning datasets\n(DL19-22), validating its robustness across diverse domains. We investigate the\ninfluence of different first-stage retrievers on FIRST rerankers, observing\ndiminishing returns and patterns consistent with traditional LLM rerankers.\nThrough applying the FIRST objective to a broader range of backbone models, we\nachieve effectiveness surpassing the original implementation. Our experiments\nconfirm that fast reranking with single-token logits does not compromise\nout-of-domain reranking quality. To better quantify the computational savings\nin the original study, we measure and compare latency to find a 21%-42% gain\nacross various models and benchmarks. Moreover, while LM training implicitly\nimproves zero-shot single-token reranking, our experiments also raise questions\nabout whether LM pre-training may hinder subsequent fine-tuning with the FIRST\nobjective. These findings pave the way for more efficient and effective\nlistwise reranking in future applications.", "published": "2024-11-08 12:08:17", "link": "http://arxiv.org/abs/2411.05508v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Asterisk*: Keep it Simple", "abstract": "This paper describes Asterisk, a compact GPT-based model for generating text\nembeddings. The model uses a minimalist architecture with two layers, two\nattention heads, and 256 embedding dimensions. By applying knowledge\ndistillation from larger pretrained models, we explore the trade-offs between\nmodel size and performance while minimizing computational and memory\nrequirements. The model is primarily evaluated and optimized for classification\ntasks, with experimental results showing its moderate performance in zero-shot\nclassification across various downstream applications. With additional\nconfiguration, the model performance can approach or even surpass that of\nlarger architectures on specific classification tasks.", "published": "2024-11-08 16:42:33", "link": "http://arxiv.org/abs/2411.05691v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Image2Text2Image: A Novel Framework for Label-Free Evaluation of\n  Image-to-Text Generation with Text-to-Image Diffusion Models", "abstract": "Evaluating the quality of automatically generated image descriptions is a\ncomplex task that requires metrics capturing various dimensions, such as\ngrammaticality, coverage, accuracy, and truthfulness. Although human evaluation\nprovides valuable insights, its cost and time-consuming nature pose\nlimitations. Existing automated metrics like BLEU, ROUGE, METEOR, and CIDEr\nattempt to fill this gap, but they often exhibit weak correlations with human\njudgment. To address this challenge, we propose a novel evaluation framework\ncalled Image2Text2Image, which leverages diffusion models, such as Stable\nDiffusion or DALL-E, for text-to-image generation. In the Image2Text2Image\nframework, an input image is first processed by a selected image captioning\nmodel, chosen for evaluation, to generate a textual description. Using this\ngenerated description, a diffusion model then creates a new image. By comparing\nfeatures extracted from the original and generated images, we measure their\nsimilarity using a designated similarity metric. A high similarity score\nsuggests that the model has produced a faithful textual description, while a\nlow score highlights discrepancies, revealing potential weaknesses in the\nmodel's performance. Notably, our framework does not rely on human-annotated\nreference captions, making it a valuable tool for assessing image captioning\nmodels. Extensive experiments and human evaluations validate the efficacy of\nour proposed Image2Text2Image evaluation framework. The code and dataset will\nbe published to support further research in the community.", "published": "2024-11-08 17:07:01", "link": "http://arxiv.org/abs/2411.05706v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "FinDVer: Explainable Claim Verification over Long and Hybrid-Content\n  Financial Documents", "abstract": "We introduce FinDVer, a comprehensive benchmark specifically designed to\nevaluate the explainable claim verification capabilities of LLMs in the context\nof understanding and analyzing long, hybrid-content financial documents.\nFinDVer contains 2,400 expert-annotated examples, divided into three subsets:\ninformation extraction, numerical reasoning, and knowledge-intensive reasoning,\neach addressing common scenarios encountered in real-world financial contexts.\nWe assess a broad spectrum of LLMs under long-context and RAG settings. Our\nresults show that even the current best-performing system, GPT-4o, still lags\nbehind human experts. We further provide in-depth analysis on long-context and\nRAG setting, Chain-of-Thought reasoning, and model reasoning errors, offering\ninsights to drive future advancements. We believe that FinDVer can serve as a\nvaluable benchmark for evaluating LLMs in claim verification over complex,\nexpert-domain documents.", "published": "2024-11-08 18:26:17", "link": "http://arxiv.org/abs/2411.05764v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fact or Fiction? Can LLMs be Reliable Annotators for Political Truths?", "abstract": "Political misinformation poses significant challenges to democratic\nprocesses, shaping public opinion and trust in media. Manual fact-checking\nmethods face issues of scalability and annotator bias, while machine learning\nmodels require large, costly labelled datasets. This study investigates the use\nof state-of-the-art large language models (LLMs) as reliable annotators for\ndetecting political factuality in news articles. Using open-source LLMs, we\ncreate a politically diverse dataset, labelled for bias through LLM-generated\nannotations. These annotations are validated by human experts and further\nevaluated by LLM-based judges to assess the accuracy and reliability of the\nannotations. Our approach offers a scalable and robust alternative to\ntraditional fact-checking, enhancing transparency and public trust in media.", "published": "2024-11-08 18:36:33", "link": "http://arxiv.org/abs/2411.05775v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLMs as Method Actors: A Model for Prompt Engineering and Architecture", "abstract": "We introduce \"Method Actors\" as a mental model for guiding LLM prompt\nengineering and prompt architecture. Under this mental model, LLMs should be\nthought of as actors; prompts as scripts and cues; and LLM responses as\nperformances. We apply this mental model to the task of improving LLM\nperformance at playing Connections, a New York Times word puzzle game that\nprior research identified as a challenging benchmark for evaluating LLM\nreasoning. Our experiments with GPT-4o show that a \"Method Actors\" approach can\nsignificantly improve LLM performance over both a vanilla and \"Chain of\nThoughts\" approach. A vanilla approach solves 27% of Connections puzzles in our\ndataset and a \"Chain of Thoughts\" approach solves 41% of puzzles, whereas our\nstrongest \"Method Actor\" approach solves 86% of puzzles. We also test OpenAI's\nnewest model designed specifically for complex reasoning tasks, o1-preview.\nWhen asked to solve a puzzle all at once, o1-preview solves 79% of Connections\npuzzles in our dataset, and when allowed to build puzzle solutions one guess at\na time over multiple API calls, o1-preview solves 100% of the puzzles.\nIncorporating a \"Method Actor\" prompt architecture increases the percentage of\npuzzles that o1-preview solves perfectly from 76% to 87%.", "published": "2024-11-08 18:45:06", "link": "http://arxiv.org/abs/2411.05778v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Using Language Models to Disambiguate Lexical Choices in Translation", "abstract": "In translation, a concept represented by a single word in a source language\ncan have multiple variations in a target language. The task of lexical\nselection requires using context to identify which variation is most\nappropriate for a source text. We work with native speakers of nine languages\nto create DTAiLS, a dataset of 1,377 sentence pairs that exhibit cross-lingual\nconcept variation when translating from English. We evaluate recent LLMs and\nneural machine translation systems on DTAiLS, with the best-performing model,\nGPT-4, achieving from 67 to 85% accuracy across languages. Finally, we use\nlanguage models to generate English rules describing target-language concept\nvariations. Providing weaker models with high-quality lexical rules improves\naccuracy substantially, in some cases reaching or outperforming GPT-4.", "published": "2024-11-08 18:48:57", "link": "http://arxiv.org/abs/2411.05781v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "When are 1.58 bits enough? A Bottom-up Exploration of BitNet\n  Quantization", "abstract": "Contemporary machine learning models, such as language models, are powerful,\nbut come with immense resource requirements both at training and inference\ntime. It has been shown that decoder-only language models can be trained to a\ncompetitive state with ternary weights (1.58 bits per weight), facilitating\nefficient inference. Here, we start our exploration with non-transformer model\narchitectures, investigating 1.58-bit training for multi-layer perceptrons and\ngraph neural networks. Then, we explore 1.58-bit training in other\ntransformer-based language models, namely encoder-only and encoder-decoder\nmodels. Our results show that in all of these settings, 1.58-bit training is on\npar with or sometimes even better than the standard 32/16-bit models.", "published": "2024-11-08 07:24:49", "link": "http://arxiv.org/abs/2411.05882v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Autoregressive Models in Vision: A Survey", "abstract": "Autoregressive modeling has been a huge success in the field of natural\nlanguage processing (NLP). Recently, autoregressive models have emerged as a\nsignificant area of focus in computer vision, where they excel in producing\nhigh-quality visual content. Autoregressive models in NLP typically operate on\nsubword tokens. However, the representation strategy in computer vision can\nvary in different levels, \\textit{i.e.}, pixel-level, token-level, or\nscale-level, reflecting the diverse and hierarchical nature of visual data\ncompared to the sequential structure of language. This survey comprehensively\nexamines the literature on autoregressive models applied to vision. To improve\nreadability for researchers from diverse research backgrounds, we start with\npreliminary sequence representation and modeling in vision. Next, we divide the\nfundamental frameworks of visual autoregressive models into three general\nsub-categories, including pixel-based, token-based, and scale-based models\nbased on the strategy of representation. We then explore the interconnections\nbetween autoregressive models and other generative models. Furthermore, we\npresent a multi-faceted categorization of autoregressive models in computer\nvision, including image generation, video generation, 3D generation, and\nmulti-modal generation. We also elaborate on their applications in diverse\ndomains, including emerging domains such as embodied AI and 3D medical AI, with\nabout 250 related references. Finally, we highlight the current challenges to\nautoregressive models in vision with suggestions about potential research\ndirections. We have also set up a Github repository to organize the papers\nincluded in this survey at:\n\\url{https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey}.", "published": "2024-11-08 17:15:12", "link": "http://arxiv.org/abs/2411.05902v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "The Dark Patterns of Personalized Persuasion in Large Language Models:\n  Exposing Persuasive Linguistic Features for Big Five Personality Traits in\n  LLMs Responses", "abstract": "This study explores how the Large Language Models (LLMs) adjust linguistic\nfeatures to create personalized persuasive outputs. While research showed that\nLLMs personalize outputs, a gap remains in understanding the linguistic\nfeatures of their persuasive capabilities. We identified 13 linguistic features\ncrucial for influencing personalities across different levels of the Big Five\nmodel of personality. We analyzed how prompts with personality trait\ninformation influenced the output of 19 LLMs across five model families. The\nfindings show that models use more anxiety-related words for neuroticism,\nincrease achievement-related words for conscientiousness, and employ fewer\ncognitive processes words for openness to experience. Some model families excel\nat adapting language for openness to experience, others for conscientiousness,\nwhile only one model adapts language for neuroticism. Our findings show how\nLLMs tailor responses based on personality cues in prompts, indicating their\npotential to create persuasive content affecting the mind and well-being of the\nrecipients.", "published": "2024-11-08 23:02:59", "link": "http://arxiv.org/abs/2411.06008v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-Document Financial Question Answering using LLMs", "abstract": "We propose two new methods for multi-document financial question answering.\nFirst, a method that uses semantic tagging, and then, queries the index to get\nthe context (RAG_SEM). And second, a Knowledge Graph (KG_RAG) based method that\nuses semantic tagging, and, retrieves knowledge graph triples from a graph\ndatabase, as context. KG_RAG uses knowledge graphs constructed using a small\nmodel that is fine-tuned using knowledge distillation using a large teacher\nmodel. The data consists of 18 10K reports of Apple, Microsoft, Alphabet,\nNVIDIA, Amazon and Tesla for the years 2021, 2022 and 2023. The list of\nquestions in the data consists of 111 complex questions including many esoteric\nquestions that are difficult to answer and the answers are not completely\nobvious. As evaluation metrics, we use overall scores as well as segmented\nscores for measurement including the faithfulness, relevance, correctness,\nsimilarity, an LLM based overall score and the rouge scores as well as a\nsimilarity of embeddings. We find that both methods outperform plain RAG\nsignificantly. KG_RAG outperforms RAG_SEM in four out of nine metrics.", "published": "2024-11-08 21:03:54", "link": "http://arxiv.org/abs/2411.07264v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Decoding Report Generators: A Cyclic Vision-Language Adapter for\n  Counterfactual Explanations", "abstract": "Despite significant advancements in report generation methods, a critical\nlimitation remains: the lack of interpretability in the generated text. This\npaper introduces an innovative approach to enhance the explainability of text\ngenerated by report generation models. Our method employs cyclic text\nmanipulation and visual comparison to identify and elucidate the features in\nthe original content that influence the generated text. By manipulating the\ngenerated reports and producing corresponding images, we create a comparative\nframework that highlights key attributes and their impact on the text\ngeneration process. This approach not only identifies the image features\naligned to the generated text but also improves transparency but also provides\ndeeper insights into the decision-making mechanisms of the report generation\nmodels. Our findings demonstrate the potential of this method to significantly\nenhance the interpretability and transparency of AI-generated reports.", "published": "2024-11-08 01:46:11", "link": "http://arxiv.org/abs/2411.05261v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Revisiting the Robustness of Watermarking to Paraphrasing Attacks", "abstract": "Amidst rising concerns about the internet being proliferated with content\ngenerated from language models (LMs), watermarking is seen as a principled way\nto certify whether text was generated from a model. Many recent watermarking\ntechniques slightly modify the output probabilities of LMs to embed a signal in\nthe generated output that can later be detected. Since early proposals for text\nwatermarking, questions about their robustness to paraphrasing have been\nprominently discussed. Lately, some techniques are deliberately designed and\nclaimed to be robust to paraphrasing. However, such watermarking schemes do not\nadequately account for the ease with which they can be reverse-engineered. We\nshow that with access to only a limited number of generations from a black-box\nwatermarked model, we can drastically increase the effectiveness of\nparaphrasing attacks to evade watermark detection, thereby rendering the\nwatermark ineffective.", "published": "2024-11-08 02:22:30", "link": "http://arxiv.org/abs/2411.05277v1", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Fox-1: Open Small Language Model for Cloud and Edge", "abstract": "We present Fox-1, a series of small language models (SLMs) consisting of\nFox-1-1.6B and Fox-1-1.6B-Instruct-v0.1. These models are pre-trained on 3\ntrillion tokens of web-scraped document data and fine-tuned with 5 billion\ntokens of instruction-following and multi-turn conversation data. Aiming to\nimprove the pre-training efficiency, Fox-1-1.6B model introduces a novel\n3-stage data curriculum across all the training data with 2K-8K sequence\nlength. In architecture design, Fox-1 features a deeper layer structure, an\nexpanded vocabulary, and utilizes Grouped Query Attention (GQA), offering a\nperformant and efficient architecture compared to other SLMs. Fox-1 achieves\nbetter or on-par performance in various benchmarks compared to StableLM-2-1.6B,\nGemma-2B, Qwen1.5-1.8B, and OpenELM1.1B, with competitive inference speed and\nthroughput. The model weights have been released under the Apache 2.0 license,\nwhere we aim to promote the democratization of LLMs and make them fully\naccessible to the whole open-source community.", "published": "2024-11-08 02:24:29", "link": "http://arxiv.org/abs/2411.05281v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving Multi-Domain Task-Oriented Dialogue System with Offline\n  Reinforcement Learning", "abstract": "Task-oriented dialogue (TOD) system is designed to accomplish user-defined\ntasks through dialogues. The TOD system has progressed towards end-to-end\nmodeling by leveraging pre-trained large language models. Fine-tuning the\npre-trained language models using only supervised learning leads to the\nexposure bias and token loss problem and it deviates the models from completing\nthe user's task. To address these issues, we propose a TOD system that\nleverages a unified pre-trained language model, GPT2, as a base model. It is\noptimized using supervised learning and reinforcement learning (RL). The issues\nin the TOD system are mitigated using a non-differentiable reward function. The\nreward is calculated using the weighted sum of the success rate and BLEU\nevaluation metrics. The success rate and BLEU metrics in reward calculation\nguide the language model for user task completion while ensuring a coherent and\nfluent response. Our model is acquired by fine-tuning a pre-trained model on\nthe dialogue-session level which comprises user utterance, belief state, system\nact, and system response. Experimental results on MultiWOZ2.1 demonstrate that\nour model increases the inform rate by 1.60% and the success rate by 3.17%\ncompared to the baseline.", "published": "2024-11-08 05:43:40", "link": "http://arxiv.org/abs/2411.05340v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Ev2R: Evaluating Evidence Retrieval in Automated Fact-Checking", "abstract": "Current automated fact-checking (AFC) approaches commonly evaluate evidence\neither implicitly via the predicted verdicts or by comparing retrieved evidence\nwith a predefined closed knowledge source, such as Wikipedia. However, these\nmethods suffer from limitations, resulting from their reliance on evaluation\nmetrics developed for different purposes and constraints imposed by closed\nknowledge sources. Recent advances in natural language generation (NLG)\nevaluation offer new possibilities for evidence assessment. In this work, we\nintroduce Ev2R, an evaluation framework for AFC that comprises three types of\napproaches for evidence evaluation: reference-based, proxy-reference, and\nreference-less. We evaluate their effectiveness through agreement with human\nratings and adversarial tests, and demonstrate that prompt-based scorers,\nparticularly those leveraging LLMs and reference evidence, outperform\ntraditional evaluation approaches.", "published": "2024-11-08 07:05:06", "link": "http://arxiv.org/abs/2411.05375v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VISTA: Visual Integrated System for Tailored Automation in Math Problem\n  Generation Using LLM", "abstract": "Generating accurate and consistent visual aids is a critical challenge in\nmathematics education, where visual representations like geometric shapes and\nfunctions play a pivotal role in enhancing student comprehension. This paper\nintroduces a novel multi-agent framework that leverages Large Language Models\n(LLMs) to automate the creation of complex mathematical visualizations\nalongside coherent problem text. Our approach not only simplifies the\ngeneration of precise visual aids but also aligns these aids with the problem's\ncore mathematical concepts, improving both problem creation and assessment. By\nintegrating multiple agents, each responsible for distinct tasks such as\nnumeric calculation, geometry validation, and visualization, our system\ndelivers mathematically accurate and contextually relevant problems with visual\naids. Evaluation across Geometry and Function problem types shows that our\nmethod significantly outperforms basic LLMs in terms of text coherence,\nconsistency, relevance and similarity, while maintaining the essential\ngeometrical and functional integrity of the original problems. Although some\nchallenges remain in ensuring consistent visual outputs, our framework\ndemonstrates the immense potential of LLMs in transforming the way educators\ngenerate and utilize visual aids in math education.", "published": "2024-11-08 09:15:56", "link": "http://arxiv.org/abs/2411.05423v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "WorkflowLLM: Enhancing Workflow Orchestration Capability of Large\n  Language Models", "abstract": "Recent advancements in large language models (LLMs) have driven a\nrevolutionary paradigm shift in process automation from Robotic Process\nAutomation to Agentic Process Automation by automating the workflow\norchestration procedure based on LLMs. However, existing LLMs (even the\nadvanced OpenAI GPT-4o) are confined to achieving satisfactory capability in\nworkflow orchestration. To address this limitation, we present WorkflowLLM, a\ndata-centric framework elaborately designed to enhance the capability of LLMs\nin workflow orchestration. It first constructs a large-scale fine-tuning\ndataset WorkflowBench with 106,763 samples, covering 1,503 APIs from 83\napplications across 28 categories. Specifically, the construction process can\nbe divided into three phases: (1) Data Collection: we collect real-world\nworkflow data from Apple Shortcuts and RoutineHub, transcribing them into\nPython-style code. We further equip them with generated hierarchical thought\nvia ChatGPT. (2) Query Expansion: we prompt ChatGPT to generate more task\nqueries to enrich the diversity and complexity of workflows. (3) Workflow\nGeneration: we leverage an annotator model trained on collected data to\ngenerate workflows for synthesized queries. Finally, we merge the synthetic\nsamples that pass quality confirmation with the collected samples to obtain the\nWorkflowBench. Based on WorkflowBench, we fine-tune Llama-3.1-8B to obtain\nWorkflowLlama. Our experiments show that WorkflowLlama demonstrates a strong\ncapacity to orchestrate complex workflows, while also achieving notable\ngeneralization performance on previously unseen APIs. Additionally,\nWorkflowBench exhibits robust zero-shot generalization capabilities on an\nout-of-distribution task planning dataset, T-Eval. Our data and code are\navailable at https://github.com/OpenBMB/WorkflowLLM.", "published": "2024-11-08 09:58:02", "link": "http://arxiv.org/abs/2411.05451v1", "categories": ["cs.SE", "cs.AI", "cs.CL"], "primary_category": "cs.SE"}
{"title": "EUREKHA: Enhancing User Representation for Key Hackers Identification in\n  Underground Forums", "abstract": "Underground forums serve as hubs for cybercriminal activities, offering a\nspace for anonymity and evasion of conventional online oversight. In these\nhidden communities, malicious actors collaborate to exchange illicit knowledge,\ntools, and tactics, driving a range of cyber threats from hacking techniques to\nthe sale of stolen data, malware, and zero-day exploits. Identifying the key\ninstigators (i.e., key hackers), behind these operations is essential but\nremains a complex challenge. This paper presents a novel method called EUREKHA\n(Enhancing User Representation for Key Hacker Identification in Underground\nForums), designed to identify these key hackers by modeling each user as a\ntextual sequence. This sequence is processed through a large language model\n(LLM) for domain-specific adaptation, with LLMs acting as feature extractors.\nThese extracted features are then fed into a Graph Neural Network (GNN) to\nmodel user structural relationships, significantly improving identification\naccuracy. Furthermore, we employ BERTopic (Bidirectional Encoder\nRepresentations from Transformers Topic Modeling) to extract personalized\ntopics from user-generated content, enabling multiple textual representations\nper user and optimizing the selection of the most representative sequence. Our\nstudy demonstrates that fine-tuned LLMs outperform state-of-the-art methods in\nidentifying key hackers. Additionally, when combined with GNNs, our model\nachieves significant improvements, resulting in approximately 6% and 10%\nincreases in accuracy and F1-score, respectively, over existing methods.\nEUREKHA was tested on the Hack-Forums dataset, and we provide open-source\naccess to our code.", "published": "2024-11-08 11:09:45", "link": "http://arxiv.org/abs/2411.05479v1", "categories": ["cs.CR", "cs.CL", "cs.SI"], "primary_category": "cs.CR"}
{"title": "Aioli: A Unified Optimization Framework for Language Model Data Mixing", "abstract": "Language model performance depends on identifying the optimal mixture of data\ngroups to train on (e.g., law, code, math). Prior work has proposed a diverse\nset of methods to efficiently learn mixture proportions, ranging from fitting\nregression models over training runs to dynamically updating proportions\nthroughout training. Surprisingly, we find that no existing method consistently\noutperforms a simple stratified sampling baseline in terms of average test\nperplexity per group. In this paper, we study the cause of this inconsistency\nby unifying existing methods into a standard optimization framework. We show\nthat all methods set proportions to minimize total loss, subject to a\nmethod-specific mixing law -- an assumption on how loss is a function of\nmixture proportions. We find that existing parameterizations of mixing laws can\nexpress the true loss-proportion relationship empirically, but the methods\nthemselves often set the mixing law parameters inaccurately, resulting in poor\nand inconsistent performance. Finally, we leverage the insights from our\nframework to derive a new online method named Aioli, which directly estimates\nthe mixing law parameters throughout training and uses them to dynamically\nadjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out\nof 6 datasets by an average of 0.28 test perplexity points, whereas existing\nmethods fail to consistently beat stratified sampling, doing up to 6.9 points\nworse. Moreover, in a practical setting where proportions are learned on\nshorter runs due to computational constraints, Aioli can dynamically adjust\nthese proportions over the full training run, consistently improving\nperformance over existing methods by up to 12.01 test perplexity points.", "published": "2024-11-08 17:50:24", "link": "http://arxiv.org/abs/2411.05735v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "FisherMask: Enhancing Neural Network Labeling Efficiency in Image\n  Classification Using Fisher Information", "abstract": "Deep learning (DL) models are popular across various domains due to their\nremarkable performance and efficiency. However, their effectiveness relies\nheavily on large amounts of labeled data, which are often time-consuming and\nlabor-intensive to generate manually. To overcome this challenge, it is\nessential to develop strategies that reduce reliance on extensive labeled data\nwhile preserving model performance. In this paper, we propose FisherMask, a\nFisher information-based active learning (AL) approach that identifies key\nnetwork parameters by masking them based on their Fisher information values.\nFisherMask enhances batch AL by using Fisher information to select the most\ncritical parameters, allowing the identification of the most impactful samples\nduring AL training. Moreover, Fisher information possesses favorable\nstatistical properties, offering valuable insights into model behavior and\nproviding a better understanding of the performance characteristics within the\nAL pipeline. Our extensive experiments demonstrate that FisherMask\nsignificantly outperforms state-of-the-art methods on diverse datasets,\nincluding CIFAR-10 and FashionMNIST, especially under imbalanced settings.\nThese improvements lead to substantial gains in labeling efficiency. Hence\nserving as an effective tool to measure the sensitivity of model parameters to\ndata samples. Our code is available on\n\\url{https://github.com/sgchr273/FisherMask}.", "published": "2024-11-08 18:10:46", "link": "http://arxiv.org/abs/2411.05752v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "End-to-End Navigation with Vision Language Models: Transforming Spatial\n  Reasoning into Question-Answering", "abstract": "We present VLMnav, an embodied framework to transform a Vision-Language Model\n(VLM) into an end-to-end navigation policy. In contrast to prior work, we do\nnot rely on a separation between perception, planning, and control; instead, we\nuse a VLM to directly select actions in one step. Surprisingly, we find that a\nVLM can be used as an end-to-end policy zero-shot, i.e., without any\nfine-tuning or exposure to navigation data. This makes our approach open-ended\nand generalizable to any downstream navigation task. We run an extensive study\nto evaluate the performance of our approach in comparison to baseline prompting\nmethods. In addition, we perform a design analysis to understand the most\nimpactful design decisions. Visual examples and code for our project can be\nfound at https://jirl-upenn.github.io/VLMnav/", "published": "2024-11-08 18:16:58", "link": "http://arxiv.org/abs/2411.05755v1", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Quantitative Assessment of Intersectional Empathetic Bias and\n  Understanding", "abstract": "A growing amount of literature critiques the current operationalizations of\nempathy based on loose definitions of the construct. Such definitions\nnegatively affect dataset quality, model robustness, and evaluation\nreliability. We propose an empathy evaluation framework that operationalizes\nempathy close to its psychological origins. The framework measures the variance\nin responses of LLMs to prompts using existing metrics for empathy and\nemotional valence. The variance is introduced through the controlled generation\nof the prompts by varying social biases affecting context understanding, thus\nimpacting empathetic understanding. The control over generation ensures high\ntheoretical validity of the constructs in the prompt dataset. Also, it makes\nhigh-quality translation, especially into languages that currently have\nlittle-to-no way of evaluating empathy or bias, such as the Slavonic family,\nmore manageable. Using chosen LLMs and various prompt types, we demonstrate the\nempathy evaluation with the framework, including multiple-choice answers and\nfree generation. The variance in our initial evaluation sample is small and we\nwere unable to measure convincing differences between the empathetic\nunderstanding in contexts given by different social groups. However, the\nresults are promising because the models showed significant alterations their\nreasoning chains needed to capture the relatively subtle changes in the\nprompts. This provides the basis for future research into the construction of\nthe evaluation sample and statistical methods for measuring the results.", "published": "2024-11-08 18:43:15", "link": "http://arxiv.org/abs/2411.05777v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles", "abstract": "Deaf and hard-of-hearing (DHH) students face significant barriers in\naccessing science, technology, engineering, and mathematics (STEM) education,\nnotably due to the scarcity of STEM resources in signed languages. To help\naddress this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia\narticles on STEM topics in English, interpreted into over 300 hours of American\nSign Language (ASL). ASL STEM Wiki is the first continuous signing dataset\nfocused on STEM, facilitating the development of AI resources for STEM\neducation in ASL. We identify several use cases of ASL STEM Wiki with\nhuman-centered applications. For example, because this dataset highlights the\nfrequent use of fingerspelling for technical concepts, which inhibits DHH\nstudents' ability to learn, we develop models to identify fingerspelled words\n-- which can later be used to query for appropriate ASL signs to suggest to\ninterpreters.", "published": "2024-11-08 18:50:37", "link": "http://arxiv.org/abs/2411.05783v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Generative Adapter: Contextualizing Language Models in Parameters with A\n  Single Forward Pass", "abstract": "Large language models (LMs) are typically adapted to improve performance on\nnew contexts (\\eg text prompts that define new tasks or domains) through\nfine-tuning or prompting. However, there is an accuracy compute tradeoff --\nfine-tuning incurs significant training cost and prompting increases inference\noverhead. We introduce $GenerativeAdapter$, an effective and efficient\nadaptation method that directly maps new contexts to low-rank LM adapters,\nthereby significantly reducing inference overhead with no need for finetuning.\nThe adapter generator is trained via self-supervised learning, and can be used\nto adapt a single frozen LM for any new task simply by mapping the associated\ntask or domain context to a new adapter. We apply $GenerativeAdapter$ to two\npretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the\nadapted models in three adaption scenarios: knowledge acquisition from\ndocuments, learning from demonstrations, and personalization for users. In\nStreamingQA, our approach is effective in injecting knowledge into the LM's\nparameters, achieving a 63.5% improvement in F1 score over the model with\nsupervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K\ntokens. In the MetaICL in-context learning evaluation, our method achieves an\naverage accuracy of $44.9$ across 26 tasks, outperforming the base model. On\nMSC, our method proves to be highly competitive in memorizing user information\nfrom conversations with a 4x reduction in computation and memory costs compared\nto prompting with full conversation history. Together, these results suggest\nthat $GenerativeAdapter$ should allow for general adaption to a wide range of\ndifferent contexts.", "published": "2024-11-08 00:42:47", "link": "http://arxiv.org/abs/2411.05877v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Identifying and Decomposing Compound Ingredients in Meal Plans Using\n  Large Language Models", "abstract": "This study explores the effectiveness of Large Language Models in meal\nplanning, focusing on their ability to identify and decompose compound\ningredients. We evaluated three models-GPT-4o, Llama-3 (70b), and Mixtral\n(8x7b)-to assess their proficiency in recognizing and breaking down complex\ningredient combinations. Preliminary results indicate that while Llama-3 (70b)\nand GPT-4o excels in accurate decomposition, all models encounter difficulties\nwith identifying essential elements like seasonings and oils. Despite strong\noverall performance, variations in accuracy and completeness were observed\nacross models. These findings underscore LLMs' potential to enhance\npersonalized nutrition but highlight the need for further refinement in\ningredient decomposition. Future research should address these limitations to\nimprove nutritional recommendations and health outcomes.", "published": "2024-11-08 12:38:10", "link": "http://arxiv.org/abs/2411.05892v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "SSSD: Simply-Scalable Speculative Decoding", "abstract": "Over the past year, Speculative Decoding has gained popularity as a technique\nfor accelerating Large Language Model inference. While several methods have\nbeen introduced, most struggle to deliver satisfactory performance at batch\nsizes typical for data centers ($\\geq 8$) and often involve significant\ndeployment complexities. In this work, we offer a theoretical explanation of\nhow Speculative Decoding can be effectively utilized with larger batch sizes.\nWe also introduce a method that integrates seamlessly into existing systems\nwithout additional training or the complexity of deploying a small LLM. In a\ncontinuous batching setting, we achieve a 4x increase in throughput without any\nlatency impact for short context generation, and a 1.7-2x improvement in both\nlatency and throughput for longer contexts.", "published": "2024-11-08 14:23:02", "link": "http://arxiv.org/abs/2411.05894v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Humans and Large Language Models in Clinical Decision Support: A Study\n  with Medical Calculators", "abstract": "Although large language models (LLMs) have been assessed for general medical\nknowledge using licensing exams, their ability to support clinical\ndecision-making, such as selecting medical calculators, remains uncertain. We\nassessed nine LLMs, including open-source, proprietary, and domain-specific\nmodels, with 1,009 multiple-choice question-answer pairs across 35 clinical\ncalculators and compared LLMs to humans on a subset of questions. While the\nhighest-performing LLM, OpenAI o1, provided an answer accuracy of 66.0% (CI:\n56.7-75.3%) on the subset of 100 questions, two human annotators nominally\noutperformed LLMs with an average answer accuracy of 79.5% (CI: 73.5-85.0%).\nUltimately, we evaluated medical trainees and LLMs in recommending medical\ncalculators across clinical scenarios like risk stratification and diagnosis.\nWith error analysis showing that the highest-performing LLMs continue to make\nmistakes in comprehension (49.3% of errors) and calculator knowledge (7.1% of\nerrors), our findings highlight that LLMs are not superior to humans in\ncalculator recommendation.", "published": "2024-11-08 15:50:19", "link": "http://arxiv.org/abs/2411.05897v2", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "BERTrend: Neural Topic Modeling for Emerging Trends Detection", "abstract": "Detecting and tracking emerging trends and weak signals in large, evolving\ntext corpora is vital for applications such as monitoring scientific\nliterature, managing brand reputation, surveilling critical infrastructure and\nmore generally to any kind of text-based event detection. Existing solutions\noften fail to capture the nuanced context or dynamically track evolving\npatterns over time. BERTrend, a novel method, addresses these limitations using\nneural topic modeling in an online setting. It introduces a new metric to\nquantify topic popularity over time by considering both the number of documents\nand update frequency. This metric classifies topics as noise, weak, or strong\nsignals, flagging emerging, rapidly growing topics for further investigation.\nExperimentation on two large real-world datasets demonstrates BERTrend's\nability to accurately detect and track meaningful weak signals while filtering\nout noise, offering a comprehensive solution for monitoring emerging trends in\nlarge-scale, evolving text corpora. The method can also be used for\nretrospective analysis of past events. In addition, the use of Large Language\nModels together with BERTrend offers efficient means for the interpretability\nof trends of events.", "published": "2024-11-08 19:31:19", "link": "http://arxiv.org/abs/2411.05930v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Quantifying artificial intelligence through algebraic generalization", "abstract": "The rapid development of modern artificial intelligence (AI) systems has\ncreated an urgent need for their scientific quantification. While their fluency\nacross a variety of domains is impressive, modern AI systems fall short on\ntests requiring symbolic processing and abstraction - a glaring limitation\ngiven the necessity for interpretable and reliable technology. Despite a surge\nof reasoning benchmarks emerging from the academic community, no comprehensive\nand theoretically-motivated framework exists to quantify reasoning (and more\ngenerally, symbolic ability) in AI systems. Here, we adopt a framework from\ncomputational complexity theory to explicitly quantify symbolic generalization:\nalgebraic circuit complexity. Many symbolic reasoning problems can be recast as\nalgebraic expressions. Thus, algebraic circuit complexity theory - the study of\nalgebraic expressions as circuit models (i.e., directed acyclic graphs) - is a\nnatural framework to study the complexity of symbolic computation. The tools of\nalgebraic circuit complexity enable the study of generalization by defining\nbenchmarks in terms of their complexity-theoretic properties (i.e., the\ndifficulty of a problem). Moreover, algebraic circuits are generic mathematical\nobjects; for a given algebraic circuit, an arbitrarily large number of samples\ncan be generated for a specific circuit, making it an optimal testbed for the\ndata-hungry machine learning algorithms that are used today. Here, we adopt\ntools from algebraic circuit complexity theory, apply it to formalize a science\nof symbolic generalization, and address key theoretical and empirical\nchallenges for its successful application to AI science and its impact on the\nbroader community.", "published": "2024-11-08 20:08:18", "link": "http://arxiv.org/abs/2411.05943v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "primary_category": "cs.AI"}
{"title": "Sentiment Analysis of Cyberbullying Data in Social Media", "abstract": "Social media has become an integral part of modern life, but it has also\nbrought with it the pervasive issue of cyberbullying a serious menace in\ntoday's digital age. Cyberbullying, a form of harassment that occurs on social\nnetworks, has escalated alongside the growth of these platforms. Sentiment\nanalysis holds significant potential not only for detecting bullying phrases\nbut also for identifying victims who are at high risk of harm, whether to\nthemselves or others. Our work focuses on leveraging deep learning and natural\nlanguage understanding techniques to detect traces of bullying in social media\nposts. We developed a Recurrent Neural Network with Long Short-Term Memory\n(LSTM) cells, using different embeddings. One approach utilizes BERT\nembeddings, while the other replaces the embeddings layer with the recently\nreleased embeddings API from OpenAI. We conducted a performance comparison\nbetween these two approaches to evaluate their effectiveness in sentiment\nanalysis of Formspring Cyberbullying data. Our Code is Available at\nhttps://github.com/ppujari/xcs224u", "published": "2024-11-08 20:41:04", "link": "http://arxiv.org/abs/2411.05958v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toward Transdisciplinary Approaches to Audio Deepfake Discernment", "abstract": "This perspective calls for scholars across disciplines to address the\nchallenge of audio deepfake detection and discernment through an\ninterdisciplinary lens across Artificial Intelligence methods and linguistics.\nWith an avalanche of tools for the generation of realistic-sounding fake speech\non one side, the detection of deepfakes is lagging on the other. Particularly\nhindering audio deepfake detection is the fact that current AI models lack a\nfull understanding of the inherent variability of language and the complexities\nand uniqueness of human speech. We see the promising potential in recent\ntransdisciplinary work that incorporates linguistic knowledge into AI\napproaches to provide pathways for expert-in-the-loop and to move beyond expert\nagnostic AI-based methods for more robust and comprehensive deepfake detection.", "published": "2024-11-08 20:59:25", "link": "http://arxiv.org/abs/2411.05969v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "FactLens: Benchmarking Fine-Grained Fact Verification", "abstract": "Large Language Models (LLMs) have shown impressive capability in language\ngeneration and understanding, but their tendency to hallucinate and produce\nfactually incorrect information remains a key limitation. To verify\nLLM-generated contents and claims from other sources, traditional verification\napproaches often rely on holistic models that assign a single factuality label\nto complex claims, potentially obscuring nuanced errors. In this paper, we\nadvocate for a shift toward fine-grained verification, where complex claims are\nbroken down into smaller sub-claims for individual verification, allowing for\nmore precise identification of inaccuracies, improved transparency, and reduced\nambiguity in evidence retrieval. However, generating sub-claims poses\nchallenges, such as maintaining context and ensuring semantic equivalence with\nrespect to the original claim. We introduce FactLens, a benchmark for\nevaluating fine-grained fact verification, with metrics and automated\nevaluators of sub-claim quality. The benchmark data is manually curated to\nensure high-quality ground truth. Our results show alignment between automated\nFactLens evaluators and human judgments, and we discuss the impact of sub-claim\ncharacteristics on the overall verification performance.", "published": "2024-11-08 21:26:57", "link": "http://arxiv.org/abs/2411.05980v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning the rules of peptide self-assembly through data mining with\n  large language models", "abstract": "Peptides are ubiquitous and important biologically derived molecules, that\nhave been found to self-assemble to form a wide array of structures. Extensive\nresearch has explored the impacts of both internal chemical composition and\nexternal environmental stimuli on the self-assembly behaviour of these systems.\nHowever, there is yet to be a systematic study that gathers this rich\nliterature data and collectively examines these experimental factors to provide\na global picture of the fundamental rules that govern protein self-assembly\nbehavior. In this work, we curate a peptide assembly database through a\ncombination of manual processing by human experts and literature mining\nfacilitated by a large language model. As a result, we collect more than 1,000\nexperimental data entries with information about peptide sequence, experimental\nconditions and corresponding self-assembly phases. Utilizing the collected\ndata, ML models are trained and evaluated, demonstrating excellent accuracy\n(>80\\%) and efficiency in peptide assembly phase classification. Moreover, we\nfine-tune our GPT model for peptide literature mining with the developed\ndataset, which exhibits markedly superior performance in extracting information\nfrom academic publications relative to the pre-trained model. We find that this\nworkflow can substantially improve efficiency when exploring potential\nself-assembling peptide candidates, through guiding experimental work, while\nalso deepening our understanding of the mechanisms governing peptide\nself-assembly. In doing so, novel structures can be accessed for a range of\napplications including sensing, catalysis and biomaterials.", "published": "2024-11-08 09:14:22", "link": "http://arxiv.org/abs/2411.05421v1", "categories": ["cond-mat.soft", "cond-mat.dis-nn", "cond-mat.mes-hall", "cs.AI", "cs.CL"], "primary_category": "cond-mat.soft"}
{"title": "Mitigating Hallucination with ZeroG: An Advanced Knowledge Management\n  Engine", "abstract": "The growth of digital documents presents significant challenges in efficient\nmanagement and knowledge extraction. Traditional methods often struggle with\ncomplex documents, leading to issues such as hallucinations and high latency in\nresponses from Large Language Models (LLMs). ZeroG, an innovative approach,\nsignificantly mitigates these challenges by leveraging knowledge distillation\nand prompt tuning to enhance model performance.\n  ZeroG utilizes a smaller model that replicates the behavior of a larger\nteacher model, ensuring contextually relevant and grounded responses, by\nemploying a black-box distillation approach, it creates a distilled dataset\nwithout relying on intermediate features, optimizing computational efficiency.\nThis method significantly enhances accuracy and reduces response times,\nproviding a balanced solution for modern document management.\n  Incorporating advanced techniques for document ingestion and metadata\nutilization, ZeroG improves the accuracy of question-and-answer systems. The\nintegration of graph databases and robust metadata management further\nstreamlines information retrieval, allowing for precise and context-aware\nresponses. By transforming how organizations interact with complex data, ZeroG\nenhances productivity and user experience, offering a scalable solution for the\ngrowing demands of digital document management.", "published": "2024-11-08 19:47:02", "link": "http://arxiv.org/abs/2411.05936v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.IR"}
{"title": "NeKo: Toward Post Recognition Generative Correction Large Language\n  Models with Task-Oriented Experts", "abstract": "Construction of a general-purpose post-recognition error corrector poses a\ncrucial question: how can we most effectively train a model on a large mixture\nof domain datasets? The answer would lie in learning dataset-specific features\nand digesting their knowledge in a single model. Previous methods achieve this\nby having separate correction language models, resulting in a significant\nincrease in parameters. In this work, we present Mixture-of-Experts as a\nsolution, highlighting that MoEs are much more than a scalability tool. We\npropose a Multi-Task Correction MoE, where we train the experts to become an\n``expert'' of speech-to-text, language-to-text and vision-to-text datasets by\nlearning to route each dataset's tokens to its mapped expert. Experiments on\nthe Open ASR Leaderboard show that we explore a new state-of-the-art\nperformance by achieving an average relative $5.0$% WER reduction and\nsubstantial improvements in BLEU scores for speech and translation tasks. On\nzero-shot evaluation, NeKo outperforms GPT-3.5 and Claude-Opus with $15.5$% to\n$27.6$% relative WER reduction in the Hyporadise benchmark. NeKo performs\ncompetitively on grammar and post-OCR correction as a multi-task model.", "published": "2024-11-08 20:11:24", "link": "http://arxiv.org/abs/2411.05945v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Game-theoretic LLM: Agent Workflow for Negotiation Games", "abstract": "This paper investigates the rationality of large language models (LLMs) in\nstrategic decision-making contexts, specifically within the framework of game\ntheory. We evaluate several state-of-the-art LLMs across a spectrum of\ncomplete-information and incomplete-information games. Our findings reveal that\nLLMs frequently deviate from rational strategies, particularly as the\ncomplexity of the game increases with larger payoff matrices or deeper\nsequential trees.\n  To address these limitations, we design multiple game-theoretic workflows\nthat guide the reasoning and decision-making processes of LLMs. These workflows\naim to enhance the models' ability to compute Nash Equilibria and make rational\nchoices, even under conditions of uncertainty and incomplete information.\nExperimental results demonstrate that the adoption of these workflows\nsignificantly improves the rationality and robustness of LLMs in game-theoretic\ntasks. Specifically, with the workflow, LLMs exhibit marked improvements in\nidentifying optimal strategies, achieving near-optimal allocations in\nnegotiation scenarios, and reducing susceptibility to exploitation during\nnegotiations. Furthermore, we explore the meta-strategic considerations of\nwhether it is rational for agents to adopt such workflows, recognizing that the\ndecision to use or forgo the workflow constitutes a game-theoretic issue in\nitself.\n  Our research contributes to a deeper understanding of LLMs' decision-making\ncapabilities in strategic contexts and provides insights into enhancing their\nrationality through structured workflows. The findings have implications for\nthe development of more robust and strategically sound AI agents capable of\nnavigating complex interactive environments. Code and data supporting this\nstudy are available at \\url{https://github.com/Wenyueh/game_theory}.", "published": "2024-11-08 22:02:22", "link": "http://arxiv.org/abs/2411.05990v2", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small\n  Language Model", "abstract": "We present a novel 4.5B parameter small language model that can handle\nmultiple input and output modalities, including text, images, videos, and\naudio. Despite its small size, the model achieves near state-of-the-art\nperformance on a variety of tasks, demonstrating the potential of multi-modal\nmodels to tackle complex real-world problems. Our approach leverages recent\nadvancements in language modeling and multi-task learning to create a versatile\nand high-performing model that can even be deployed for edge inference.\nExperimental results show the model's strong performance across multiple\nbenchmarks, paving the way for further progress in multi-modal artificial\nintelligence.", "published": "2024-11-08 17:15:17", "link": "http://arxiv.org/abs/2411.05903v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A Kalman Filter model for synchronization in musical ensembles", "abstract": "The synchronization of motor responses to rhythmic auditory cues is a\nfundamental biological phenomenon observed across various species. While the\nimportance of temporal alignment varies across different contexts, achieving\nprecise temporal synchronization is a prominent goal in musical performances.\nMusicians often incorporate expressive timing variations, which require precise\ncontrol over timing and synchronization, particularly in ensemble performance.\nThis is crucial because both deliberate expressive nuances and accidental\ntiming deviations can affect the overall timing of a performance. This\ndiscussion prompts the question of how musicians adjust their temporal dynamics\nto achieve synchronization within an ensemble. This paper introduces a novel\nfeedback correction model based on the Kalman Filter, aimed at improving the\nunderstanding of interpersonal timing in ensemble music performances. The\nproposed model performs similarly to other linear correction models in the\nliterature, with the advantage of low computational cost and good performance\neven in scenarios where the underlying tempo varies.", "published": "2024-11-08 21:03:34", "link": "http://arxiv.org/abs/2411.05971v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Classification of Adventitious Sounds Combining Cochleogram and Vision\n  Transformers", "abstract": "Early identification of respiratory irregularities is critical for improving\nlung health and reducing global mortality rates. The analysis of respiratory\nsounds plays a significant role in characterizing the respiratory system's\ncondition and identifying abnormalities. The main contribution of this study is\nto investigate the performance when the input data, represented by cochleogram,\nis used to feed the Vision Transformer architecture, since this input\nclassifier combination is the first time it has been applied to adventitious\nsound classification to our knowledge. Although ViT has shown promising results\nin audio classification tasks by applying self attention to spectrogram\npatches, we extend this approach by applying the cochleogram, which captures\nspecific spectro-temporal features of adventitious sounds. The proposed\nmethodology is evaluated on the ICBHI dataset. We compare the classification\nperformance of ViT with other state of the art CNN approaches using\nspectrogram, Mel frequency cepstral coefficients, constant Q transform, and\ncochleogram as input data. Our results confirm the superior classification\nperformance combining cochleogram and ViT, highlighting the potential of ViT\nfor reliable respiratory sound classification. This study contributes to the\nongoing efforts in developing automatic intelligent techniques with the aim to\nsignificantly augment the speed and effectiveness of respiratory disease\ndetection, thereby addressing a critical need in the medical field.", "published": "2024-11-08 20:37:37", "link": "http://arxiv.org/abs/2411.05955v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "On the Role of Noise in AudioVisual Integration: Evidence from\n  Artificial Neural Networks that Exhibit the McGurk Effect", "abstract": "Humans are able to fuse information from both auditory and visual modalities\nto help with understanding speech. This is frequently demonstrated through an\nphenomenon known as the McGurk Effect, during which a listener is presented\nwith incongruent auditory and visual speech that fuse together into the percept\nof an illusory intermediate phoneme. Building on a recent framework that\nproposes how to address developmental 'why' questions using artificial neural\nnetworks, we evaluated a set of recent artificial neural networks trained on\naudiovisual speech by testing them with audiovisually incongruent words\ndesigned to elicit the McGurk effect. We compared networks trained on clean\nspeech to those trained on noisy speech, and discovered that training with\nnoisy speech led to an increase in both visual responses and McGurk responses\nacross all models. Furthermore, we observed that systematically increasing the\nlevel of auditory noise during ANN training also increased the amount of\naudiovisual integration up to a point, but at extreme noise levels, this\nintegration failed to develop. These results suggest that excessive noise\nexposure during critical periods of audiovisual learning may negatively\ninfluence the development of audiovisual speech integration. This work also\ndemonstrates that the McGurk effect reliably emerges untrained from the\nbehaviour of both supervised and unsupervised networks. This supports the\nnotion that artificial neural networks might be useful models for certain\naspects of perception and cognition.", "published": "2024-11-08 17:16:27", "link": "http://arxiv.org/abs/2411.05715v1", "categories": ["cs.SD", "cs.MM", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An ambient denoising method based on multi-channel non-negative matrix\n  factorization for wheezing detection", "abstract": "In this paper, a parallel computing method is proposed to perform the\nbackground denoising and wheezing detection from a multi-channel recording\ncaptured during the auscultation process. The proposed system is based on a\nnon-negative matrix factorization (NMF) approach and a detection strategy.\nMoreover, the initialization of the proposed model is based on singular value\ndecomposition to avoid dependence on the initial values of the NMF parameters.\nAdditionally, novel update rules to simultaneously address the multichannel\ndenoising while preserving an orthogonal constraint to maximize source\nseparation have been designed. The proposed system has been evaluated for the\ntask of wheezing detection showing a significant improvement over\nstate-of-the-art algorithms when noisy sound sources are present. Moreover,\nparallel and high-performance techniques have been used to speedup the\nexecution of the proposed system, showing that it is possible to achieve fast\nexecution times, which enables its implementation in real-world scenarios.", "published": "2024-11-08 18:35:59", "link": "http://arxiv.org/abs/2411.05774v1", "categories": ["eess.AS", "cs.ET", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Tell What You Hear From What You See -- Video to Audio Generation\n  Through Text", "abstract": "The content of visual and audio scenes is multi-faceted such that a video can\nbe paired with various audio and vice-versa. Thereby, in video-to-audio\ngeneration task, it is imperative to introduce steering approaches for\ncontrolling the generated audio. While Video-to-Audio generation is a\nwell-established generative task, existing methods lack such controllability.\nIn this work, we propose VATT, a multi-modal generative framework that takes a\nvideo and an optional text prompt as input, and generates audio and optional\ntextual description of the audio. Such a framework has two advantages: i)\nVideo-to-Audio generation process can be refined and controlled via text which\ncomplements the context of visual information, and ii) The model can suggest\nwhat audio to generate for the video by generating audio captions. VATT\nconsists of two key modules: VATT Converter, a LLM that is fine-tuned for\ninstructions and includes a projection layer that maps video features to the\nLLM vector space; and VATT Audio, a transformer that generates audio tokens\nfrom visual frames and from optional text prompt using iterative parallel\ndecoding. The audio tokens are converted to a waveform by pretrained neural\ncodec. Experiments show that when VATT is compared to existing video-to-audio\ngeneration methods in objective metrics, it achieves competitive performance\nwhen the audio caption is not provided. When the audio caption is provided as a\nprompt, VATT achieves even more refined performance (lowest KLD score of 1.41).\nFurthermore, subjective studies show that VATT Audio has been chosen as\npreferred generated audio than audio generated by existing methods. VATT\nenables controllable video-to-audio generation through text as well as\nsuggesting text prompts for videos through audio captions, unlocking novel\napplications such as text-guided video-to-audio generation and video-to-audio\ncaptioning.", "published": "2024-11-08 16:29:07", "link": "http://arxiv.org/abs/2411.05679v3", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
