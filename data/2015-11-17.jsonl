{"title": "Learning to retrieve out-of-vocabulary words in speech recognition", "abstract": "Many Proper Names (PNs) are Out-Of-Vocabulary (OOV) words for speech\nrecognition systems used to process diachronic audio data. To help recovery of\nthe PNs missed by the system, relevant OOV PNs can be retrieved out of the many\nOOVs by exploiting semantic context of the spoken content. In this paper, we\npropose two neural network models targeted to retrieve OOV PNs relevant to an\naudio document: (a) Document level Continuous Bag of Words (D-CBOW), (b)\nDocument level Continuous Bag of Weighted Words (D-CBOW2). Both these models\ntake document words as input and learn with an objective to maximise the\nretrieval of co-occurring OOV PNs. With the D-CBOW2 model we propose a new\napproach in which the input embedding layer is augmented with a context anchor\nlayer. This layer learns to assign importance to input words and has the\nability to capture (task specific) key-words in a bag-of-word neural network\nmodel. With experiments on French broadcast news videos we show that these two\nmodels outperform the baseline methods based on raw embeddings from LDA,\nSkip-gram and Paragraph Vectors. Combining the D-CBOW and D-CBOW2 models gives\nfaster convergence during training.", "published": "2015-11-17 13:18:07", "link": "http://arxiv.org/abs/1511.05389v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Compositional Captioning: Describing Novel Object Categories\n  without Paired Training Data", "abstract": "While recent deep neural network models have achieved promising results on\nthe image captioning task, they rely largely on the availability of corpora\nwith paired image and sentence captions to describe objects in context. In this\nwork, we propose the Deep Compositional Captioner (DCC) to address the task of\ngenerating descriptions of novel objects which are not present in paired\nimage-sentence datasets. Our method achieves this by leveraging large object\nrecognition datasets and external text corpora and by transferring knowledge\nbetween semantically similar concepts. Current deep caption models can only\ndescribe objects contained in paired image-sentence corpora, despite the fact\nthat they are pre-trained with large object recognition datasets, namely\nImageNet. In contrast, our model can compose sentences that describe novel\nobjects and their interactions with other objects. We demonstrate our model's\nability to describe novel concepts by empirically evaluating its performance on\nMSCOCO and show qualitative results on ImageNet images of objects for which no\npaired image-caption data exist. Further, we extend our approach to generate\ndescriptions of objects in video clips. Our results show that DCC has distinct\nadvantages over existing image and video captioning approaches for generating\ndescriptions of new objects in context.", "published": "2015-11-17 06:44:48", "link": "http://arxiv.org/abs/1511.05284v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for\n  Visual Question Answering", "abstract": "We address the problem of Visual Question Answering (VQA), which requires\njoint image and language understanding to answer a question about a given\nphotograph. Recent approaches have applied deep image captioning methods based\non convolutional-recurrent networks to this problem, but have failed to model\nspatial inference. To remedy this, we propose a model we call the Spatial\nMemory Network and apply it to the VQA task. Memory networks are recurrent\nneural networks with an explicit attention mechanism that selects certain parts\nof the information stored in memory. Our Spatial Memory Network stores neuron\nactivations from different spatial regions of the image in its memory, and uses\nthe question to choose relevant regions for computing the answer, a process of\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\nattention architecture that aligns words with image patches in the first hop,\nand obtain improved results by adding a second attention hop which considers\nthe whole question to choose visual evidence based on the results of the first\nhop. To better understand the inference process learned by the network, we\ndesign synthetic questions that specifically require spatial inference and\nvisualize the attention weights. We evaluate our model on two published visual\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\nimage and question features to predict the answer [3].", "published": "2015-11-17 01:00:04", "link": "http://arxiv.org/abs/1511.05234v2", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Learning the Dimensionality of Word Embeddings", "abstract": "We describe a method for learning word embeddings with data-dependent\ndimensionality. Our Stochastic Dimensionality Skip-Gram (SD-SG) and Stochastic\nDimensionality Continuous Bag-of-Words (SD-CBOW) are nonparametric analogs of\nMikolov et al.'s (2013) well-known 'word2vec' models. Vector dimensionality is\nmade dynamic by employing techniques used by Cote & Larochelle (2016) to define\nan RBM with an infinite number of hidden units. We show qualitatively and\nquantitatively that SD-SG and SD-CBOW are competitive with their\nfixed-dimension counterparts while providing a distribution over embedding\ndimensionalities, which offers a window into how semantics distribute across\ndimensions.", "published": "2015-11-17 13:28:55", "link": "http://arxiv.org/abs/1511.05392v3", "categories": ["stat.ML", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Learning Articulated Motion Models from Visual and Lingual Signals", "abstract": "In order for robots to operate effectively in homes and workplaces, they must\nbe able to manipulate the articulated objects common within environments built\nfor and by humans. Previous work learns kinematic models that prescribe this\nmanipulation from visual demonstrations. Lingual signals, such as natural\nlanguage descriptions and instructions, offer a complementary means of\nconveying knowledge of such manipulation models and are suitable to a wide\nrange of interactions (e.g., remote manipulation). In this paper, we present a\nmultimodal learning framework that incorporates both visual and lingual\ninformation to estimate the structure and parameters that define kinematic\nmodels of articulated objects. The visual signal takes the form of an RGB-D\nimage stream that opportunistically captures object motion in an unprepared\nscene. Accompanying natural language descriptions of the motion constitute the\nlingual signal. We present a probabilistic language model that uses word\nembeddings to associate lingual verbs with their corresponding kinematic\nstructures. By exploiting the complementary nature of the visual and lingual\ninput, our method infers correct kinematic structures for various multiple-part\nobjects on which the previous state-of-the-art, visual-only system fails. We\nevaluate our multimodal learning framework on a dataset comprised of a variety\nof household objects, and demonstrate a 36% improvement in model accuracy over\nthe vision-only baseline.", "published": "2015-11-17 19:55:34", "link": "http://arxiv.org/abs/1511.05526v2", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
