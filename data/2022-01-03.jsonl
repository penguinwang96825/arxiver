{"title": "Learning with Latent Structures in Natural Language Processing: A Survey", "abstract": "While end-to-end learning with fully differentiable models has enabled\ntremendous success in natural language process (NLP) and machine learning,\nthere have been significant recent interests in learning with latent discrete\nstructures to incorporate better inductive biases for improved end-task\nperformance and better interpretability. This paradigm, however, is not\nstraightforwardly amenable to the mainstream gradient-based optimization\nmethods. This work surveys three main families of methods to learn such models:\nsurrogate gradients, continuous relaxation, and marginal likelihood\nmaximization via sampling. We conclude with a review of applications of these\nmethods and an inspection of the learned latent structure that they induce.", "published": "2022-01-03 06:16:17", "link": "http://arxiv.org/abs/2201.00490v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Which Student is Best? A Comprehensive Knowledge Distillation Exam for\n  Task-Specific BERT Models", "abstract": "We perform knowledge distillation (KD) benchmark from task-specific BERT-base\nteacher models to various student models: BiLSTM, CNN, BERT-Tiny, BERT-Mini,\nand BERT-Small. Our experiment involves 12 datasets grouped in two tasks: text\nclassification and sequence labeling in the Indonesian language. We also\ncompare various aspects of distillations including the usage of word embeddings\nand unlabeled data augmentation. Our experiments show that, despite the rising\npopularity of Transformer-based models, using BiLSTM and CNN student models\nprovide the best trade-off between performance and computational resource (CPU,\nRAM, and storage) compared to pruned BERT models. We further propose some quick\nwins on performing KD to produce small NLP models via efficient KD training\nmechanisms involving simple choices of loss functions, word embeddings, and\nunlabeled data preparation.", "published": "2022-01-03 10:07:13", "link": "http://arxiv.org/abs/2201.00558v1", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Toxicity Detection for Indic Multilingual Social Media Content", "abstract": "Toxic content is one of the most critical issues for social media platforms\ntoday. India alone had 518 million social media users in 2020. In order to\nprovide a good experience to content creators and their audience, it is crucial\nto flag toxic comments and the users who post that. But the big challenge is\nidentifying toxicity in low resource Indic languages because of the presence of\nmultiple representations of the same text. Moreover, the posts/comments on\nsocial media do not adhere to a particular format, grammar or sentence\nstructure; this makes the task of abuse detection even more challenging for\nmultilingual social media platforms. This paper describes the system proposed\nby team 'Moj Masti' using the data provided by ShareChat/Moj in \\emph{IIIT-D\nMultilingual Abusive Comment Identification} challenge. We focus on how we can\nleverage multilingual transformer based pre-trained and fine-tuned models to\napproach code-mixed/code-switched classification tasks. Our best performing\nsystem was an ensemble of XLM-RoBERTa and MuRIL which achieved a Mean F-1 score\nof 0.9 on the test data/leaderboard. We also observed an increase in the\nperformance by adding transliterated data. Furthermore, using weak metadata,\nensembling and some post-processing techniques boosted the performance of our\nsystem, thereby placing us 1st on the leaderboard.", "published": "2022-01-03 12:01:47", "link": "http://arxiv.org/abs/2201.00598v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Adversarial Benchmark for Fake News Detection Models", "abstract": "With the proliferation of online misinformation, fake news detection has\ngained importance in the artificial intelligence community. In this paper, we\npropose an adversarial benchmark that tests the ability of fake news detectors\nto reason about real-world facts. We formulate adversarial attacks that target\nthree aspects of \"understanding\": compositional semantics, lexical relations,\nand sensitivity to modifiers. We test our benchmark using BERT classifiers\nfine-tuned on the LIAR arXiv:arch-ive/1705648 and Kaggle Fake-News datasets,\nand show that both models fail to respond to changes in compositional and\nlexical meaning. Our results strengthen the need for such models to be used in\nconjunction with other fact checking methods.", "published": "2022-01-03 23:51:55", "link": "http://arxiv.org/abs/2201.00912v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "AI & Racial Equity: Understanding Sentiment Analysis Artificial\n  Intelligence, Data Security, and Systemic Theory in Criminal Justice Systems", "abstract": "Various forms of implications of artificial intelligence that either\nexacerbate or decrease racial systemic injustice have been explored in this\napplied research endeavor. Taking each thematic area of identifying, analyzing,\nand debating an systemic issue have been leveraged in investigating merits and\ndrawbacks of using algorithms to automate human decision making in racially\nsensitive environments. It has been asserted through the analysis of historical\nsystemic patterns, implicit biases, existing algorithmic risks, and legal\nimplications that natural language processing based AI, such as risk assessment\ntools, have racially disparate outcomes. It is concluded that more litigative\npolicies are needed to regulate and restrict how internal government\ninstitutions and corporations utilize algorithms, privacy and security risks,\nand auditing requirements in order to diverge from racially injustice outcomes\nand practices of the past.", "published": "2022-01-03 19:42:08", "link": "http://arxiv.org/abs/2201.00855v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Testing the Robustness of a BiLSTM-based Structural Story Classifier", "abstract": "The growing prevalence of counterfeit stories on the internet has fostered\nsignificant interest towards fast and scalable detection of fake news in the\nmachine learning community. While several machine learning techniques for this\npurpose have emerged, we observe that there is a need to evaluate the impact of\nnoise on these techniques' performance, where noise constitutes news articles\nbeing mistakenly labeled as fake (or real). This work takes a step in that\ndirection, where we examine the impact of noise on a state-of-the-art,\nstructural model based on BiLSTM (Bidirectional Long-Short Term Model) for fake\nnews detection, Hierarchical Discourse-level Structure for Fake News Detection\nby Karimi and Tang (Reference no. 9).", "published": "2022-01-03 00:27:05", "link": "http://arxiv.org/abs/2201.02733v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Actor-Critic Network for Q&A in an Adversarial Environment", "abstract": "Significant work has been placed in the Q&A NLP space to build models that\nare more robust to adversarial attacks. Two key areas of focus are in\ngenerating adversarial data for the purposes of training against these\nsituations or modifying existing architectures to build robustness within. This\npaper introduces an approach that joins these two ideas together to train a\ncritic model for use in an almost reinforcement learning framework. Using the\nAdversarial SQuAD \"Add One Sent\" dataset we show that there are some promising\nsigns for this method in protecting against Adversarial attacks.", "published": "2022-01-03 02:35:58", "link": "http://arxiv.org/abs/2201.00455v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Semi-supervised Stance Detection of Tweets Via Distant Network\n  Supervision", "abstract": "Detecting and labeling stance in social media text is strongly motivated by\nhate speech detection, poll prediction, engagement forecasting, and concerted\npropaganda detection. Today's best neural stance detectors need large volumes\nof training data, which is difficult to curate given the fast-changing\nlandscape of social media text and issues on which users opine. Homophily\nproperties over the social network provide strong signal of coarse-grained\nuser-level stance. But semi-supervised approaches for tweet-level stance\ndetection fail to properly leverage homophily. In light of this, We present\nSANDS, a new semi-supervised stance detector. SANDS starts from very few\nlabeled tweets. It builds multiple deep feature views of tweets. It also uses a\ndistant supervision signal from the social network to provide a surrogate loss\nsignal to the component learners. We prepare two new tweet datasets comprising\nover 236,000 politically tinted tweets from two demographics (US and India)\nposted by over 87,000 users, their follower-followee graph, and over 8,000\ntweets annotated by linguists. SANDS achieves a macro-F1 score of 0.55 (0.49)\non US (India)-based datasets, outperforming 17 baselines (including variants of\nSANDS) substantially, particularly for minority stance labels and noisy text.\nNumerous ablation experiments on SANDS disentangle the dynamics of textual and\nnetwork-propagated stance signals.", "published": "2022-01-03 13:04:54", "link": "http://arxiv.org/abs/2201.00614v2", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Sentiment Analysis and Sarcasm Detection of Indian General Election\n  Tweets", "abstract": "Social Media usage has increased to an all-time high level in today's digital\nworld. The majority of the population uses social media tools (like Twitter,\nFacebook, YouTube, etc.) to share their thoughts and experiences with the\ncommunity. Analysing the sentiments and opinions of the common public is very\nimportant for both the government and the business people. This is the reason\nbehind the activeness of many media agencies during the election time for\nperforming various kinds of opinion polls. In this paper, we have worked\ntowards analysing the sentiments of the people of India during the Lok Sabha\nelection of 2019 using the Twitter data of that duration. We have built an\nautomatic tweet analyser using the Transfer Learning technique to handle the\nunsupervised nature of this problem. We have used the Linear Support Vector\nClassifiers method in our Machine Learning model, also, the Term Frequency\nInverse Document Frequency (TF-IDF) methodology for handling the textual data\nof tweets. Further, we have increased the capability of the model to address\nthe sarcastic tweets posted by some of the users, which has not been yet\nconsidered by the researchers in this domain.", "published": "2022-01-03 17:30:00", "link": "http://arxiv.org/abs/2201.02127v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.IR"}
{"title": "Robust Natural Language Processing: Recent Advances, Challenges, and\n  Future Directions", "abstract": "Recent natural language processing (NLP) techniques have accomplished high\nperformance on benchmark datasets, primarily due to the significant improvement\nin the performance of deep learning. The advances in the research community\nhave led to great enhancements in state-of-the-art production systems for NLP\ntasks, such as virtual assistants, speech recognition, and sentiment analysis.\nHowever, such NLP systems still often fail when tested with adversarial\nattacks. The initial lack of robustness exposed troubling gaps in current\nmodels' language understanding capabilities, creating problems when NLP systems\nare deployed in real life. In this paper, we present a structured overview of\nNLP robustness research by summarizing the literature in a systemic way across\nvarious dimensions. We then take a deep-dive into the various dimensions of\nrobustness, across techniques, metrics, embeddings, and benchmarks. Finally, we\nargue that robustness should be multi-dimensional, provide insights into\ncurrent research, identify gaps in the literature to suggest directions worth\npursuing to address these gaps.", "published": "2022-01-03 17:17:11", "link": "http://arxiv.org/abs/2201.00768v1", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TFCN: Temporal-Frequential Convolutional Network for Single-Channel\n  Speech Enhancement", "abstract": "Deep learning based single-channel speech enhancement tries to train a neural\nnetwork model for the prediction of clean speech signal. There are a variety of\npopular network structures for single-channel speech enhancement, such as TCNN,\nUNet, WaveNet, etc. However, these structures usually contain millions of\nparameters, which is an obstacle for mobile applications. In this work, we\nproposed a light weight neural network for speech enhancement named TFCN. It is\na temporal-frequential convolutional network constructed of dilated\nconvolutions and depth-separable convolutions. We evaluate the performance of\nTFCN in terms of Short-Time Objective Intelligibility (STOI), perceptual\nevaluation of speech quality (PESQ) and a series of composite metrics named\nCsig, Cbak and Covl. Experimental results show that compared with TCN and\nseveral other state-of-the-art algorithms, the proposed structure achieves a\ncomparable performance with only 93,000 parameters. Further improvement can be\nachieved at the cost of more parameters, by introducing dense connections and\ndepth-separable convolutions with normal ones. Experiments also show that the\nproposed structure can work well both in causal and non-causal situations.", "published": "2022-01-03 05:17:13", "link": "http://arxiv.org/abs/2201.00480v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Signal-Aware Direction-of-Arrival Estimation Using Attention Mechanisms", "abstract": "The direction-of-arrival (DOA) of sound sources is an essential acoustic\nparameter used, e.g., for multi-channel speech enhancement or source tracking.\nComplex acoustic scenarios consisting of sources-of-interest, interfering\nsources, reverberation, and noise make the estimation of the DOAs corresponding\nto the sources-of-interest a challenging task. Recently proposed attention\nmechanisms allow DOA estimators to focus on the sources-of-interest and\ndisregard interference and noise, i.e., they are signal-aware. The attention is\ntypically obtained by a deep neural network (DNN) from a short-time Fourier\ntransform (STFT) based representation of a single microphone signal.\nSubsequently, attention has been applied as binary or ratio weighting to\nSTFT-based microphone signal representations to reduce the impact of frequency\nbins dominated by noise, interference, or reverberation. The impact of\nattention on DOA estimators and different training strategies for attention and\nDOA DNNs are not yet studied in depth. In this paper, we evaluate systems\nconsisting of different DNNs and signal processing-based methods for DOA\nestimation when attention is applied. Additionally, we propose training\nstrategies for attention-based DOA estimation optimized via a DOA objective,\ni.e., end-to-end. The evaluation of the proposed and the baseline systems is\nperformed using data generated with simulated and measured room impulse\nresponses under various acoustic conditions, like reverberation times, noise,\nand source array distances. Overall, DOA estimation using attention in\ncombination with signal-processing methods exhibits a far lower computational\ncomplexity than a fully DNN-based system; however, it yields comparable\nresults.", "published": "2022-01-03 07:30:00", "link": "http://arxiv.org/abs/2201.00503v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
