{"title": "Exploration on Generating Traditional Chinese Medicine Prescription from\n  Symptoms with an End-to-End method", "abstract": "Traditional Chinese Medicine (TCM) is an influential form of medical\ntreatment in China and surrounding areas. In this paper, we propose a TCM\nprescription generation task that aims to automatically generate a herbal\nmedicine prescription based on textual symptom descriptions.\nSequence-to-sequence (seq2seq) model has been successful in dealing with\nsequence generation tasks. We explore a potential end-to-end solution to the\nTCM prescription generation task using seq2seq models. However, experiments\nshow that directly applying seq2seq model leads to unfruitful results due to\nthe repetition problem. To solve the problem, we propose a novel decoder with\ncoverage mechanism and a novel soft loss function. The experimental results\ndemonstrate the effectiveness of the proposed approach. Judged by professors\nwho excel in TCM, the generated prescriptions are rated 7.3 out of 10. It shows\nthat the model can indeed help with the prescribing procedure in real life.", "published": "2018-01-27 04:01:44", "link": "http://arxiv.org/abs/1801.09030v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Word Vector with Prior Knowledge in Semantic Dictionary", "abstract": "Using low dimensional vector space to represent words has been very effective\nin many NLP tasks. However, it doesn't work well when faced with the problem of\nrare and unseen words. In this paper, we propose to leverage the knowledge in\nsemantic dictionary in combination with some morphological information to build\nan enhanced vector space. We get an improvement of 2.3% over the\nstate-of-the-art Heidel Time system in temporal expression recognition, and\nobtain a large gain in other name entity recognition (NER) tasks. The semantic\ndictionary Hownet alone also shows promising results in computing lexical\nsimilarity.", "published": "2018-01-27 04:13:52", "link": "http://arxiv.org/abs/1801.09031v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Sheaf Model of Contradictions and Disagreements. Preliminary Report\n  and Discussion", "abstract": "We introduce a new formal model -- based on the mathematical construct of\nsheaves -- for representing contradictory information in textual sources. This\nmodel has the advantage of letting us (a) identify the causes of the\ninconsistency; (b) measure how strong it is; (c) and do something about it,\ne.g. suggest ways to reconcile inconsistent advice. This model naturally\nrepresents the distinction between contradictions and disagreements. It is\nbased on the idea of representing natural language sentences as formulas with\nparameters sitting on lattices, creating partial orders based on predicates\nshared by theories, and building sheaves on these partial orders with products\nof lattices as stalks. Degrees of disagreement are measured by the existence of\nglobal and local sections.\n  Limitations of the sheaf approach and connections to recent work in natural\nlanguage processing, as well as the topics of contextuality in physics, data\nfusion, topological data analysis and epistemology are also discussed.", "published": "2018-01-27 05:13:55", "link": "http://arxiv.org/abs/1801.09036v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Convolution and Recursive Neural Networks for Sentiment\n  Analysis", "abstract": "This paper addresses the problem of sentence-level sentiment analysis. In\nrecent years, Convolution and Recursive Neural Networks have been proven to be\neffective network architecture for sentence-level sentiment analysis.\nNevertheless, each of them has their own potential drawbacks. For alleviating\ntheir weaknesses, we combined Convolution and Recursive Neural Networks into a\nnew network architecture. In addition, we employed transfer learning from a\nlarge document-level labeled sentiment dataset to improve the word embedding in\nour models. The resulting models outperform all recent Convolution and\nRecursive Neural Networks. Beyond that, our models achieve comparable\nperformance with state-of-the-art systems on Stanford Sentiment Treebank.", "published": "2018-01-27 08:33:22", "link": "http://arxiv.org/abs/1801.09053v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Additional Indexes for Fast Full-Text Search of Phrases That\n  Contain Frequently Used Words", "abstract": "Searches for phrases and word sets in large text arrays by means of\nadditional indexes are considered. Their use may reduce the query-processing\ntime by an order of magnitude in comparison with standard inverted files.", "published": "2018-01-27 12:02:03", "link": "http://arxiv.org/abs/1801.09079v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
