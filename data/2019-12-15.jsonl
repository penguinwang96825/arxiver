{"title": "Multilingual is not enough: BERT for Finnish", "abstract": "Deep learning-based language models pretrained on large unannotated text\ncorpora have been demonstrated to allow efficient transfer learning for natural\nlanguage processing, with recent approaches such as the transformer-based BERT\nmodel advancing the state of the art across a variety of tasks. While most work\non these models has focused on high-resource languages, in particular English,\na number of recent efforts have introduced multilingual models that can be\nfine-tuned to address tasks in a large number of different languages. However,\nwe still lack a thorough understanding of the capabilities of these models, in\nparticular for lower-resourced languages. In this paper, we focus on Finnish\nand thoroughly evaluate the multilingual BERT model on a range of tasks,\ncomparing it with a new Finnish BERT model trained from scratch. The new\nlanguage-specific model is shown to systematically and clearly outperform the\nmultilingual. While the multilingual model largely fails to reach the\nperformance of previously proposed methods, the custom Finnish BERT model\nestablishes new state-of-the-art results on all corpora for all reference\ntasks: part-of-speech tagging, named entity recognition, and dependency\nparsing. We release the model and all related resources created for this study\nwith open licenses at https://turkunlp.org/finbert .", "published": "2019-12-15 17:50:56", "link": "http://arxiv.org/abs/1912.07076v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Named Entity Recognition with Truecasing Pretraining", "abstract": "Although modern named entity recognition (NER) systems show impressive\nperformance on standard datasets, they perform poorly when presented with noisy\ndata. In particular, capitalization is a strong signal for entities in many\nlanguages, and even state of the art models overfit to this feature, with\ndrastically lower performance on uncapitalized text. In this work, we address\nthe problem of robustness of NER systems in data with noisy or uncertain\ncasing, using a pretraining objective that predicts casing in text, or a\ntruecaser, leveraging unlabeled data. The pretrained truecaser is combined with\na standard BiLSTM-CRF model for NER by appending output distributions to\ncharacter embeddings. In experiments over several datasets of varying domain\nand casing quality, we show that our new model improves performance in uncased\ntext, even adding value to uncased BERT embeddings. Our method achieves a new\nstate of the art on the WNUT17 shared task dataset.", "published": "2019-12-15 19:33:25", "link": "http://arxiv.org/abs/1912.07095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comparison of Architectures and Pretraining Methods for Contextualized\n  Multilingual Word Embeddings", "abstract": "The lack of annotated data in many languages is a well-known challenge within\nthe field of multilingual natural language processing (NLP). Therefore, many\nrecent studies focus on zero-shot transfer learning and joint training across\nlanguages to overcome data scarcity for low-resource languages. In this work we\n(i) perform a comprehensive comparison of state-ofthe-art multilingual word and\nsentence encoders on the tasks of named entity recognition (NER) and part of\nspeech (POS) tagging; and (ii) propose a new method for creating multilingual\ncontextualized word embeddings, compare it to multiple baselines and show that\nit performs at or above state-of-theart level in zero-shot transfer settings.\nFinally, we show that our method allows for better knowledge sharing across\nlanguages in a joint training setting.", "published": "2019-12-15 11:42:32", "link": "http://arxiv.org/abs/1912.10169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Indiscapes: Instance Segmentation Networks for Layout Parsing of\n  Historical Indic Manuscripts", "abstract": "Historical palm-leaf manuscript and early paper documents from Indian\nsubcontinent form an important part of the world's literary and cultural\nheritage. Despite their importance, large-scale annotated Indic manuscript\nimage datasets do not exist. To address this deficiency, we introduce\nIndiscapes, the first ever dataset with multi-regional layout annotations for\nhistorical Indic manuscripts. To address the challenge of large diversity in\nscripts and presence of dense, irregular layout elements (e.g. text lines,\npictures, multiple documents per image), we adapt a Fully Convolutional Deep\nNeural Network architecture for fully automatic, instance-level spatial layout\nparsing of manuscript images. We demonstrate the effectiveness of proposed\narchitecture on images from the Indiscapes dataset. For annotation flexibility\nand keeping the non-technical nature of domain experts in mind, we also\ncontribute a custom, web-based GUI annotation tool and a dashboard-style\nanalytics portal. Overall, our contributions set the stage for enabling\ndownstream applications such as OCR and word-spotting in historical Indic\nmanuscripts at scale.", "published": "2019-12-15 11:42:27", "link": "http://arxiv.org/abs/1912.07025v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Breaking Speech Recognizers to Imagine Lyrics", "abstract": "We introduce a new method for generating text, and in particular song lyrics,\nbased on the speech-like acoustic qualities of a given audio file. We repurpose\na vocal source separation algorithm and an acoustic model trained to recognize\nisolated speech, instead inputting instrumental music or environmental sounds.\nFeeding the \"mistakes\" of the vocal separator into the recognizer, we obtain a\ntranscription of words \\emph{imagined} to be spoken in the input audio. We\ndescribe the key components of our approach, present initial analysis, and\ndiscuss the potential of the method for machine-in-the-loop collaboration in\ncreative applications.", "published": "2019-12-15 05:34:45", "link": "http://arxiv.org/abs/1912.06979v1", "categories": ["cs.HC", "cs.CL", "cs.LG"], "primary_category": "cs.HC"}
{"title": "Computational Induction of Prosodic Structure", "abstract": "The present study has two goals relating to the grammar of prosody,\nunderstood as the rhythms and melodies of speech. First, an overview is\nprovided of the computable grammatical and phonetic approaches to prosody\nanalysis which use hypothetico-deductive methods and are based on learned\nhermeneutic intuitions about language. Second, a proposal is presented for an\ninductive grounding in the physical signal, in which prosodic structure is\ninferred using a language-independent method from the low-frequency spectrum of\nthe speech signal. The overview includes a discussion of computational aspects\nof standard generative and post-generative models, and suggestions for\nreformulating these to form inductive approaches. Also included is a discussion\nof linguistic phonetic approaches to analysis of annotations (pairs of speech\nunit labels with time-stamps) of recorded spoken utterances. The proposal\nintroduces the inductive approach of Rhythm Formant Theory (RFT) and the\nassociated Rhythm Formant Analysis (RFA) method are introduced, with the aim of\ncompleting a gap in the linguistic hypothetico-deductive cycle by grounding in\na language-independent inductive procedure of speech signal analysis. The\nvalidity of the method is demonstrated and applied to rhythm patterns in\nread-aloud Mandarin Chinese, finding differences from English which are related\nto lexical and grammatical differences between the languages, as well as\nindividual variation. The overall conclusions are (1) that normative\nlanguage-to-language phonological or phonetic comparisons of rhythm, for\nexample of Mandarin and English, are too simplistic, in view of diverse\nlanguage-internal factors due to genre and style differences as well as\nutterance dynamics, and (2) that language-independent empirical grounding of\nrhythm in the physical signal is called for.", "published": "2019-12-15 14:38:58", "link": "http://arxiv.org/abs/1912.07050v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Artificial mental phenomena: Psychophysics as a framework to detect\n  perception biases in AI models", "abstract": "Detecting biases in artificial intelligence has become difficult because of\nthe impenetrable nature of deep learning. The central difficulty is in relating\nunobservable phenomena deep inside models with observable, outside quantities\nthat we can measure from inputs and outputs. For example, can we detect\ngendered perceptions of occupations (e.g., female librarian, male electrician)\nusing questions to and answers from a word embedding-based system? Current\ntechniques for detecting biases are often customized for a task, dataset, or\nmethod, affecting their generalization. In this work, we draw from\nPsychophysics in Experimental Psychology---meant to relate quantities from the\nreal world (i.e., \"Physics\") into subjective measures in the mind (i.e.,\n\"Psyche\")---to propose an intellectually coherent and generalizable framework\nto detect biases in AI. Specifically, we adapt the two-alternative forced\nchoice task (2AFC) to estimate potential biases and the strength of those\nbiases in black-box models. We successfully reproduce previously-known biased\nperceptions in word embeddings and sentiment analysis predictions. We discuss\nhow concepts in experimental psychology can be naturally applied to\nunderstanding artificial mental phenomena, and how psychophysics can form a\nuseful methodological foundation to study fairness in AI.", "published": "2019-12-15 19:48:48", "link": "http://arxiv.org/abs/1912.10818v1", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Na\u00efveRole: Author-Contribution Extraction and Parsing from Biomedical\n  Manuscripts", "abstract": "Information about the contributions of individual authors to scientific\npublications is important for assessing authors' achievements. Some biomedical\npublications have a short section that describes authors' roles and\ncontributions. It is usually written in natural language and hence author\ncontributions cannot be trivially extracted in a machine-readable format. In\nthis paper, we present 1) A statistical analysis of roles in author\ncontributions sections, and 2) Na\\\"iveRole, a novel approach to extract\nstructured authors' roles from author contribution sections. For the first\npart, we used co-clustering techniques, as well as Open Information Extraction,\nto semi-automatically discover the popular roles within a corpus of 2,000\ncontributions sections from PubMed Central. The discovered roles were used to\nautomatically build a training set for Na\\\"iveRole, our role extractor\napproach, based on Na\\\"ive Bayes. Na\\\"iveRole extracts roles with a\nmicro-averaged precision of 0.68, recall of 0.48 and F1 of 0.57. It is, to the\nbest of our knowledge, the first attempt to automatically extract author roles\nfrom research papers. This paper is an extended version of a previous poster\npublished at JCDL 2018.", "published": "2019-12-15 14:37:06", "link": "http://arxiv.org/abs/1912.10170v1", "categories": ["cs.CL", "cs.DL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "BatVision: Learning to See 3D Spatial Layout with Two Ears", "abstract": "Many species have evolved advanced non-visual perception while artificial\nsystems fall behind. Radar and ultrasound complement camera-based vision but\nthey are often too costly and complex to set up for very limited information\ngain. In nature, sound is used effectively by bats, dolphins, whales, and\nhumans for navigation and communication. However, it is unclear how to best\nharness sound for machine perception. Inspired by bats' echolocation mechanism,\nwe design a low-cost BatVision system that is capable of seeing the 3D spatial\nlayout of space ahead by just listening with two ears. Our system emits short\nchirps from a speaker and records returning echoes through microphones in an\nartificial human pinnae pair. During training, we additionally use a stereo\ncamera to capture color images for calculating scene depths. We train a model\nto predict depth maps and even grayscale images from the sound alone. During\ntesting, our trained BatVision provides surprisingly good predictions of 2D\nvisual scenes from two 1D audio signals. Such a sound to vision system would\nbenefit robot navigation and machine vision, especially in low-light or\nno-light conditions. Our code and data are publicly available.", "published": "2019-12-15 09:33:04", "link": "http://arxiv.org/abs/1912.07011v3", "categories": ["cs.CV", "cs.RO", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Wykorzystanie sztucznej inteligencji do generowania tre\u015bci muzycznych", "abstract": "This thesis is presenting a method for generating short musical phrases using\na deep convolutional generative adversarial network (DCGAN). To train neural\nnetwork were used datasets of classical and jazz music MIDI recordings. Our\napproach introduces translating the MIDI data into graphical images in a piano\nroll format suitable for the network input size, using the RGB channels as\nadditional information carriers for improved performance. The network has\nlearned to generate images that are indistinguishable from the input data and,\nwhen translated back to MIDI and played back, include several musically\ninteresting rhythmic and harmonic structures. The results of the conducted\nexperiments are described and discussed, with conclusions for further work and\na short comparison with selected existing solutions.", "published": "2019-12-15 02:48:16", "link": "http://arxiv.org/abs/1912.10815v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
