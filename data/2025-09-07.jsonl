{"title": "Beamforming-LLM: What, Where and When Did I Miss?", "abstract": "We present Beamforming-LLM, a system that enables users to semantically\nrecall conversations they may have missed in multi-speaker environments. The\nsystem combines spatial audio capture using a microphone array with\nretrieval-augmented generation (RAG) to support natural language queries such\nas, \"What did I miss when I was following the conversation on dogs?\"\nDirectional audio streams are separated using beamforming, transcribed with\nWhisper, and embedded into a vector database using sentence encoders. Upon\nreceiving a user query, semantically relevant segments are retrieved,\ntemporally aligned with non-attended segments, and summarized using a\nlightweight large language model (GPT-4o-mini). The result is a user-friendly\ninterface that provides contrastive summaries, spatial context, and timestamped\naudio playback. This work lays the foundation for intelligent auditory memory\nsystems and has broad applications in assistive technology, meeting\nsummarization, and context-aware personal spatial computing.", "published": "2025-09-07 21:52:26", "link": "http://arxiv.org/abs/2509.06221v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "eess.AS"}
{"title": "MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment", "abstract": "This paper presents MSLEF, a multi-segment ensemble framework that employs\nLLM fine-tuning to enhance resume parsing in recruitment automation. It\nintegrates fine-tuned Large Language Models (LLMs) using weighted voting, with\neach model specializing in a specific resume segment to boost accuracy.\nBuilding on MLAR , MSLEF introduces a segment-aware architecture that leverages\nfield-specific weighting tailored to each resume part, effectively overcoming\nthe limitations of single-model systems by adapting to diverse formats and\nstructures. The framework incorporates Gemini-2.5-Flash LLM as a high-level\naggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4\n14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,\nBLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best\nsingle model by up to +7% in RS. Its segment-aware design enhances\ngeneralization across varied resume layouts, making it highly adaptable to\nreal-world hiring scenarios while ensuring precise and reliable candidate\nrepresentation.", "published": "2025-09-07 20:27:58", "link": "http://arxiv.org/abs/2509.06200v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation", "abstract": "This paper presents a novel approach to recruitment automation. Large\nLanguage Models (LLMs) were fine-tuned to improve accuracy and efficiency.\nBuilding upon our previous work on the Multilayer Large Language Model-Based\nRobotic Process Automation Applicant Tracking (MLAR) system . This work\nintroduces a novel methodology. Training fine-tuned LLMs specifically tuned for\nrecruitment tasks. The proposed framework addresses the limitations of generic\nLLMs by creating a synthetic dataset that uses a standardized JSON format. This\nhelps ensure consistency and scalability. In addition to the synthetic data\nset, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes\nwere parsed into the same structured JSON format and placed in the training\nset. This will help improve data diversity and realism. Through\nexperimentation, we demonstrate significant improvements in performance\nmetrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall\nsimilarity compared to base models and other state-of-the-art LLMs. In\nparticular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,\nindicating exceptional precision and recall in recruitment tasks. This study\nhighlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize\nrecruitment workflows by providing more accurate candidate-job matching.", "published": "2025-09-07 20:18:31", "link": "http://arxiv.org/abs/2509.06196v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Bias in Information Retrieval: The Nature of the Beast and Mitigation Methods", "abstract": "Language fairness in multilingual information retrieval (MLIR) systems is\ncrucial for ensuring equitable access to information across diverse languages.\nThis paper sheds light on the issue, based on the assumption that queries in\ndifferent languages, but with identical semantics, should yield equivalent\nranking lists when retrieving on the same multilingual documents. We evaluate\nthe degree of fairness using both traditional retrieval methods, and a DPR\nneural ranker based on mBERT and XLM-R. Additionally, we introduce `LaKDA', a\nnovel loss designed to mitigate language biases in neural MLIR approaches. Our\nanalysis exposes intrinsic language biases in current MLIR technologies, with\nnotable disparities across the retrieval methods, and the effectiveness of\nLaKDA in enhancing language fairness.", "published": "2025-09-07 20:10:49", "link": "http://arxiv.org/abs/2509.06195v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Understanding the Influence of Synthetic Data for Text Embedders", "abstract": "Recent progress in developing general purpose text embedders has been driven\nby training on ever-growing corpora of synthetic LLM-generated data.\nNonetheless, no publicly available synthetic dataset exists, posing a barrier\nto studying its role for generalization. To address this issue, we first\nreproduce and publicly release the synthetic data proposed by Wang et al.\n(Mistral-E5). Our synthetic data is high quality and leads to consistent\nimprovements in performance. Next, we critically examine where exactly\nsynthetic data improves model generalization. Our analysis reveals that\nbenefits from synthetic data are sparse and highly localized to individual\ndatasets. Moreover, we observe trade-offs between the performance on different\ncategories and data that benefits one task, degrades performance on another.\nOur findings highlight the limitations of current synthetic data approaches for\nbuilding general-purpose embedders and challenge the notion that training on\nsynthetic data leads to more robust embedding models across tasks.", "published": "2025-09-07 19:28:52", "link": "http://arxiv.org/abs/2509.06184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Long to Short: LLMs Excel at Trimming Own Reasoning Chains", "abstract": "O1/R1 style large reasoning models (LRMs) signal a substantial leap forward\nover conventional instruction-following LLMs. By applying test-time scaling to\ngenerate extended reasoning paths, they establish many SOTAs across a wide\nrange of complex reasoning tasks. However, recent studies show that LRMs are\nprone to suffer from overthinking -- the tendency to overcomplicate simple\nproblems, leading to excessive strategy switching and long, convoluted\nreasoning traces that hinder their interpretability. To mitigate this issue, we\nconduct a systematic investigation into the reasoning efficiency of a broad set\nof LRMs and uncover a common dilemma: the difficulty in balancing multiple\ngeneration objectives such as correctness and brevity. Based on this discovery,\nwe propose a test-time scaling method, EDIT (Efficient Dynamic Inference\nTrimming), which efficiently guides LRMs to identify the shortest correct\nreasoning paths at test time. EDIT employs constraint-guided generation while\njointly tracking length and answer distributions under varying constraints,\nallowing it to select responses that strike an optimal balance between\nconciseness and correctness. Extensive experiments across diverse models and\ndatasets show that EDIT substantially enhance the reasoning efficiency,\nproducing compact yet informative outputs that improve readability and user\nexperience.", "published": "2025-09-07 19:00:44", "link": "http://arxiv.org/abs/2509.06174v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Benchmarking Gender and Political Bias in Large Language Models", "abstract": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts.", "published": "2025-09-07 18:23:30", "link": "http://arxiv.org/abs/2509.06164v1", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reverse-Engineered Reasoning for Open-Ended Generation", "abstract": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.", "published": "2025-09-07 18:07:58", "link": "http://arxiv.org/abs/2509.06160v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models", "abstract": "Large language models (LLMs) are prone to catastrophic forgetting in\nsequential multi-task settings. Parameter regularization methods such as O-LoRA\nand N-LoRA alleviate task interference by enforcing low-rank subspace\northogonality, but they overlook the fact that conventional additive\nfine-tuning disrupts the intrinsic geometric structure of LLM parameters,\nlimiting performance. Our key insight is that the parameter space of LLMs\npossesses a geometric structure, which must be preserved in addition to\nenforcing orthogonality. Based on this, we propose Orthogonal Low-rank\nAdaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM\nfine-tuning: leveraging multiplicative updates to preserve parameter geometry\nwhile applying orthogonality constraints to task subspaces. Experiments\ndemonstrate that OLieRA achieves state-of-the-art results on the Standard CL\nbenchmark and remains among the top-performing methods in the Large Number of\nTasks setting.", "published": "2025-09-07 15:29:46", "link": "http://arxiv.org/abs/2509.06100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research", "abstract": "Chemical and materials research has traditionally relied heavily on knowledge\nnarrative, with progress often driven by language-based descriptions of\nprinciples, mechanisms, and experimental experiences, rather than tables,\nlimiting what conventional databases and ML can exploit. We present a\nlanguage-native database for boron nitride nanosheet (BNNS) polymer thermally\nconductive composites that captures lightly structured information from papers\nacross preparation, characterization, theory-computation, and mechanistic\nreasoning, with evidence-linked snippets. Records are organized in a\nheterogeneous database and queried via composite retrieval with semantics, key\nwords and value filters. The system can synthesizes literature into accurate,\nverifiable, and expert style guidance. This substrate enables high fidelity\nefficient Retrieval Augmented Generation (RAG) and tool augmented agents to\ninterleave retrieval with reasoning and deliver actionable SOP. The framework\nsupplies the language rich foundation required for LLM-driven materials\ndiscovery.", "published": "2025-09-07 15:15:55", "link": "http://arxiv.org/abs/2509.06093v1", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.AI", "cs.CL"], "primary_category": "cs.DB"}
{"title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge", "abstract": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.", "published": "2025-09-07 14:47:32", "link": "http://arxiv.org/abs/2509.06079v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis", "abstract": "Conversational Speech Synthesis (CSS) aims to generate speech with natural\nprosody by understanding the multimodal dialogue history (MDH). The latest work\npredicts the accurate prosody expression of the target utterance by modeling\nthe utterance-level interaction characteristics of MDH and the target\nutterance. However, MDH contains fine-grained semantic and prosody knowledge at\nthe word level. Existing methods overlook the fine-grained semantic and\nprosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a\nnovel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our\napproach constructs two specialized multimodal fine-grained dialogue\ninteraction graphs: a semantic interaction graph and a prosody interaction\ngraph. These two interaction graphs effectively encode interactions between\nword-level semantics, prosody, and their influence on subsequent utterances in\nMDH. The encoded interaction features are then leveraged to enhance synthesized\nspeech with natural conversational prosody. Experiments on the DailyTalk\ndataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of\nprosodic expressiveness. Code and speech samples are available at\nhttps://github.com/AI-S2-Lab/MFCIG-CSS.", "published": "2025-09-07 14:32:29", "link": "http://arxiv.org/abs/2509.06074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino", "abstract": "Large Language Models (LLMs) achieve remarkable performance across various\ntasks, but their tendency to produce hallucinations limits reliable adoption.\nBenchmarks such as TruthfulQA have been developed to measure truthfulness, yet\nthey are primarily available in English, leaving a gap in evaluating LLMs in\nlow-resource languages. To address this, we present KatotohananQA, a Filipino\ntranslation of the TruthfulQA benchmark. Seven free-tier proprietary models\nwere assessed using a binary-choice framework. Findings show a significant\nperformance gap between English and Filipino truthfulness, with newer OpenAI\nmodels (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.\nResults also reveal disparities across question characteristics, suggesting\nthat some question types, categories, and topics are less robust to\nmultilingual transfer which highlight the need for broader multilingual\nevaluation to ensure fairness and reliability in LLM usage.", "published": "2025-09-07 14:09:57", "link": "http://arxiv.org/abs/2509.06065v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching Vietnamese-English Speech Recognition", "abstract": "Code-switching (CS) presents a significant challenge for general Auto-Speech\nRecognition (ASR) systems. Existing methods often fail to capture the subtle\nphonological shifts inherent in CS scenarios. The challenge is particularly\ndifficult for language pairs like Vietnamese and English, where both distinct\nphonological features and the ambiguity arising from similar sound recognition\nare present. In this paper, we propose a novel architecture for\nVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC\nemploys a phoneme-centric approach, built upon an extended Vietnamese phoneme\nset as an intermediate representation to facilitate mixed-lingual modeling.\nExperimental results demonstrate that TSPC consistently outperforms existing\nbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a\nsignificantly lower word error rate of 20.8\\% with reduced training resources.\nFurthermore, the phonetic-based two-stage architecture enables phoneme\nadaptation and language conversion to enhance ASR performance in complex CS\nVietnamese-English ASR scenarios.", "published": "2025-09-07 09:19:03", "link": "http://arxiv.org/abs/2509.05983v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance", "abstract": "Vision-language models have demonstrated impressive capabilities in\ngenerating 2D images under various conditions; however the impressive\nperformance of these models in 2D is largely enabled by extensive, readily\navailable pretrained foundation models. Critically, comparable pretrained\nfoundation models do not exist for 3D, significantly limiting progress in this\ndomain. As a result, the potential of vision-language models to produce\nhigh-resolution 3D counterfactual medical images conditioned solely on natural\nlanguage descriptions remains completely unexplored. Addressing this gap would\nenable powerful clinical and research applications, such as personalized\ncounterfactual explanations, simulation of disease progression scenarios, and\nenhanced medical training by visualizing hypothetical medical conditions in\nrealistic detail. Our work takes a meaningful step toward addressing this\nchallenge by introducing a framework capable of generating high-resolution 3D\ncounterfactual medical images of synthesized patients guided by free-form\nlanguage prompts. We adapt state-of-the-art 3D diffusion models with\nenhancements from Simple Diffusion and incorporate augmented conditioning to\nimprove text alignment and image quality. To our knowledge, this represents the\nfirst demonstration of a language-guided native-3D diffusion model applied\nspecifically to neurological imaging data, where faithful three-dimensional\nmodeling is essential to represent the brain's three-dimensional structure.\nThrough results on two distinct neurological MRI datasets, our framework\nsuccessfully simulates varying counterfactual lesion loads in Multiple\nSclerosis (MS), and cognitive states in Alzheimer's disease, generating\nhigh-quality images while preserving subject fidelity in synthetically\ngenerated medical images. Our results lay the groundwork for prompt-driven\ndisease progression analysis within 3D medical imaging.", "published": "2025-09-07 08:52:18", "link": "http://arxiv.org/abs/2509.05978v1", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Accelerating Large Language Model Inference via Early-Exiting Algorithms", "abstract": "Large language models have achieved remarkable capabilities, but their\npractical deployment is hindered by significant computational costs. While\nadaptive computation methods like early-exiting promise to reduce these costs,\nthey introduce a fundamental conflict: the per-token dynamism intended to save\ncomputation often creates system-level bottlenecks that can paradoxically\nreduce throughput in batched inference. This dissertation resolves this\nconflict by co-designing adaptive algorithms and model architectures to strike\nan optimal balance between dynamism and efficiency. To this end, our work first\naddresses critical sources of overhead in conventional early-exiting by\nproposing an efficient parallel decoding mechanism. We then show that deep\nparameter sharing provides an architectural foundation that not only yields\ncompact, parameter-efficient models but also inherently mitigates the critical\nsynchronization issues affecting dynamic inference. Finally, this work presents\na unified framework where lightweight routers are pretrained to dynamically\nassign an optimal recursion depth for each token. This approach establishes a\nnew Pareto frontier between efficiency and performance by effectively\noptimizing for both adaptive computation and parameter efficiency within a\nsingle model.", "published": "2025-09-07 04:20:14", "link": "http://arxiv.org/abs/2509.05915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling", "abstract": "Recently, cross-attention-based contextual automatic speech recognition (ASR)\nmodels have made notable advancements in recognizing personalized biasing\nphrases. However, the effectiveness of cross-attention is affected by\nvariations in biasing information volume, especially when the length of the\nbiasing list increases significantly. We find that, regardless of the length of\nthe biasing list, only a limited amount of biasing information is most relevant\nto a specific ASR intermediate representation. Therefore, by identifying and\nintegrating the most relevant biasing information rather than the entire\nbiasing list, we can alleviate the effects of variations in biasing information\nvolume for contextual ASR. To this end, we propose a purified semantic\ncorrelation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and\ncalculate three semantic correlations between the ASR intermediate\nrepresentations and biasing information from coarse to fine: list-level,\nphrase-level, and token-level. Then, the three correlations are jointly modeled\nto produce their intersection, so that the most relevant biasing information\nacross various granularities is highlighted and integrated for contextual\nrecognition. In addition, to reduce the computational cost introduced by the\njoint modeling of three semantic correlations, we also propose a purification\nmechanism based on a grouped-and-competitive strategy to filter out irrelevant\nbiasing phrases. Compared with baselines, our PSC-Joint approach achieves\naverage relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%\non KeSpeech, across biasing lists of varying lengths.", "published": "2025-09-07 03:46:59", "link": "http://arxiv.org/abs/2509.05908v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues", "abstract": "As Large Language Models (LLMs) integrate into diverse workflows, they are\nincreasingly being considered \"collaborators\" with humans. If such AI\ncollaborators are to be reliable, their behavior over multiturn interactions\nmust be predictable, validated and verified before deployment. Common alignment\ntechniques are typically developed under simplified single-user settings and do\nnot account for the dynamics of long-horizon multiparty interactions. This\npaper examines how different alignment methods affect LLM agents' effectiveness\nas partners in multiturn, multiparty collaborations. We study this question\nthrough the lens of friction agents that intervene in group dialogues to\nencourage the collaborative group to slow down and reflect upon their reasoning\nfor deliberative decision-making. Using a roleplay methodology, we evaluate\ninterventions from differently-trained friction agents in collaborative task\nconversations. We propose a novel counterfactual evaluation framework that\nquantifies how friction interventions change the trajectory of group\ncollaboration and belief alignment. Our results show that a friction-aware\napproach significantly outperforms common alignment baselines in helping both\nconvergence to a common ground, or agreed-upon task-relevant propositions, and\ncorrectness of task outcomes.", "published": "2025-09-07 00:58:10", "link": "http://arxiv.org/abs/2509.05882v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries", "abstract": "Evaluating factual accuracy in Large Language Model (LLM)-generated clinical\ntext is a critical barrier to adoption, as expert review is unscalable for the\ncontinuous quality assurance these systems require. We address this challenge\nwith two complementary contributions. First, we introduce MedFactEval, a\nframework for scalable, fact-grounded evaluation where clinicians define\nhigh-salience key facts and an \"LLM Jury\"--a multi-LLM majority vote--assesses\ntheir inclusion in generated summaries. Second, we present MedAgentBrief, a\nmodel-agnostic, multi-step workflow designed to generate high-quality, factual\ndischarge summaries. To validate our evaluation framework, we established a\ngold-standard reference using a seven-physician majority vote on\nclinician-defined key facts from inpatient cases. The MedFactEval LLM Jury\nachieved almost perfect agreement with this panel (Cohen's kappa=81%), a\nperformance statistically non-inferior to that of a single human expert\n(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework\n(MedFactEval) and a high-performing generation workflow (MedAgentBrief),\noffering a comprehensive approach to advance the responsible deployment of\ngenerative AI in clinical workflows.", "published": "2025-09-07 00:41:47", "link": "http://arxiv.org/abs/2509.05878v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nautomated code generation but frequently produce code that fails formal\nverification, an essential requirement for hardware and safety-critical\ndomains. To overcome this fundamental limitation, we previously proposed\nPREFACE, a model-agnostic framework based on reinforcement learning (RL) that\niteratively repairs the prompts provided to frozen LLMs, systematically\nsteering them toward generating formally verifiable Dafny code without costly\nfine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis\nframework that embeds the previously proposed PREFACE flow to enable the\ngeneration of correctness-by-construction hardware directly from natural\nlanguage specifications. Proof2Silicon operates by: (1) leveraging PREFACE's\nverifier-driven RL agent to optimize prompt generation iteratively, ensuring\nDafny code correctness; (2) automatically translating verified Dafny programs\ninto synthesizable high-level C using Dafny's Python backend and PyLog; and (3)\nemploying Vivado HLS to produce RTL implementations. Evaluated rigorously on a\nchallenging 100-task benchmark, PREFACE's RL-guided prompt optimization\nconsistently improved Dafny verification success rates across diverse LLMs by\nup to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis\nsuccess rate of up to 72%, generating RTL designs through Vivado HLS synthesis\nflows. These results demonstrate a robust, scalable, and automated pipeline for\nLLM-driven, formally verified hardware synthesis, bridging natural-language\nspecification and silicon realization.", "published": "2025-09-07 23:04:15", "link": "http://arxiv.org/abs/2509.06239v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments", "abstract": "LLM-based agents have shown promise in various cooperative and strategic\nreasoning tasks, but their effectiveness in competitive multi-agent\nenvironments remains underexplored. To address this gap, we introduce\nPillagerBench, a novel framework for evaluating multi-agent systems in\nreal-time competitive team-vs-team scenarios in Minecraft. It provides an\nextensible API, multi-round testing, and rule-based built-in opponents for\nfair, reproducible comparisons. We also propose TactiCrafter, an LLM-based\nmulti-agent system that facilitates teamwork through human-readable tactics,\nlearns causal dependencies, and adapts to opponent strategies. Our evaluation\ndemonstrates that TactiCrafter outperforms baseline approaches and showcases\nadaptive learning through self-play. Additionally, we analyze its learning\nprocess and strategic evolution over multiple game episodes. To encourage\nfurther research, we have open-sourced PillagerBench, fostering advancements in\nmulti-agent AI for competitive environments.", "published": "2025-09-07 22:51:12", "link": "http://arxiv.org/abs/2509.06235v1", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.6; I.2.8"], "primary_category": "cs.AI"}
{"title": "Distillation of CNN Ensemble Results for Enhanced Long-Term Prediction of the ENSO Phenomenon", "abstract": "The accurate long-term forecasting of the El Nino Southern Oscillation (ENSO)\nis still one of the biggest challenges in climate science. While it is true\nthat short-to medium-range performance has been improved significantly using\nthe advances in deep learning, statistical dynamical hybrids, most operational\nsystems still use the simple mean of all ensemble members, implicitly assuming\nequal skill across members. In this study, we demonstrate, through a strictly\na-posteriori evaluation , for any large enough ensemble of ENSO forecasts,\nthere is a subset of members whose skill is substantially higher than that of\nthe ensemble mean. Using a state-of-the-art ENSO forecast system\ncross-validated against the 1986-2017 observed Nino3.4 index, we identify two\nTop-5 subsets one ranked on lowest Root Mean Square Error (RMSE) and another on\nhighest Pearson correlation. Generally across all leads, these outstanding\nmembers show higher correlation and lower RMSE, with the advantage rising\nenormously with lead time. Whereas at short leads (1 month) raises the mean\ncorrelation by about +0.02 (+1.7%) and lowers the RMSE by around 0.14 {\\deg}C\nor by 23.3% compared to the All-40 mean, at extreme leads (23 months) the\ncorrelation is raised by +0.43 (+172%) and RMSE by 0.18 {\\deg}C or by 22.5%\ndecrease. The enhancements are largest during crucial ENSO transition periods\nsuch as SON and DJF, when accurate amplitude and phase forecasting is of\ngreatest socio-economic benefit, and furthermore season-dependent e.g.,\nmid-year months such as JJA and MJJ have incredibly large RMSE reductions. This\nstudy provides a solid foundation for further investigations to identify\nreliable clues for detecting high-quality ensemble members, thereby enhancing\nforecasting skill.", "published": "2025-09-07 22:26:42", "link": "http://arxiv.org/abs/2509.06227v1", "categories": ["physics.ao-ph", "cs.AI", "cs.CE", "physics.app-ph"], "primary_category": "physics.ao-ph"}
{"title": "The Efficiency Frontier: Classical Shadows versus Quantum Footage", "abstract": "Interfacing quantum and classical processors is an important subroutine in\nfull-stack quantum algorithms. The so-called \"classical shadow\" method\nefficiently extracts essential classical information from quantum states,\nenabling the prediction of many properties of a quantum system from only a few\nmeasurements. However, for a small number of highly non-local observables, or\nwhen classical post-processing power is limited, the classical shadow method is\nnot always the most efficient choice. Here, we address this issue\nquantitatively by performing a full-stack resource analysis that compares\nclassical shadows with ``quantum footage,\" which refers to direct quantum\nmeasurement. Under certain assumptions, our analysis illustrates a boundary of\ndownload efficiency between classical shadows and quantum footage. For\nobservables expressed as linear combinations of Pauli matrices, the classical\nshadow method outperforms direct measurement when the number of observables is\nlarge and the Pauli weight is small. For observables in the form of large\nHermitian sparse matrices, the classical shadow method shows an advantage when\nthe number of observables, the sparsity of the matrix, and the number of qubits\nfall within a certain range. The key parameters influencing this behavior\ninclude the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight\n$w$, accuracy requirement $\\epsilon$, and failure tolerance $\\delta$. We also\ncompare the resource consumption of the two methods on different types of\nquantum computers and identify break-even points where the classical shadow\nmethod becomes more efficient, which vary depending on the hardware. This paper\nopens a new avenue for quantitatively designing optimal strategies for hybrid\nquantum-classical tomography and provides practical insights for selecting the\nmost suitable quantum measurement approach in real-world applications.", "published": "2025-09-07 21:46:56", "link": "http://arxiv.org/abs/2509.06218v1", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "quant-ph"}
{"title": "Agentic Software Engineering: Foundational Pillars and a Research Roadmap", "abstract": "Agentic Software Engineering (SE 3.0) represents a new era where intelligent\nagents are tasked not with simple code generation, but with achieving complex,\ngoal-oriented SE objectives. To harness these new capabilities while ensuring\ntrustworthiness, we must recognize a fundamental duality within the SE field in\nthe Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE\nfor Agents. This duality demands a radical reimagining of the foundational\npillars of SE (actors, processes, tools, and artifacts) which manifest\ndifferently across each modality. We propose two purpose-built workbenches to\nsupport this vision. The Agent Command Environment (ACE) serves as a command\ncenter where humans orchestrate and mentor agent teams, handling outputs such\nas Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The\nAgent Execution Environment (AEE) is a digital workspace where agents perform\ntasks while invoking human expertise when facing ambiguity or complex\ntrade-offs. This bi-directional partnership, which supports agent-initiated\nhuman callbacks and handovers, gives rise to new, structured engineering\nactivities (i.e., processes) that redefine human-AI collaboration, elevating\nthe practice from agentic coding to true agentic software engineering. This\npaper presents the Structured Agentic Software Engineering (SASE) vision,\noutlining several of the foundational pillars for the future of SE. The paper\nculminates in a research roadmap that identifies a few key challenges and\nopportunities while briefly discussing the resulting impact of this future on\nSE education. Our goal is not to offer a definitive solution, but to provide a\nconceptual scaffold with structured vocabulary to catalyze a community-wide\ndialogue, pushing the SE community to think beyond its classic, human-centric\ntenets toward a disciplined, scalable, and trustworthy agentic future.", "published": "2025-09-07 21:40:10", "link": "http://arxiv.org/abs/2509.06216v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning", "abstract": "We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)\nenvironment, a complex puzzle in which an agent must infer and execute hidden\nrules to clear a 6$\\times$6 board by placing game pieces into buckets. We\nexplore two state representation strategies, namely Feature-Centric (FC) and\nObject-Centric (OC), and employ a Transformer-based Advantage Actor-Critic\n(A2C) algorithm for training. The agent has access only to partial observations\nand must simultaneously infer the governing rule and learn the optimal policy\nthrough experience. We evaluate our models across multiple rule-based and\ntrial-list-based experimental setups, analyzing transfer effects and the impact\nof representation on learning efficiency.", "published": "2025-09-07 21:22:14", "link": "http://arxiv.org/abs/2509.06213v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control", "abstract": "Grasping of diverse objects in unstructured environments remains a\nsignificant challenge. Open-loop grasping methods, effective in controlled\nsettings, struggle in cluttered environments. Grasp prediction errors and\nobject pose changes during grasping are the main causes of failure. In\ncontrast, closed-loop methods address these challenges in simplified settings\n(e.g., single object on a table) on a limited set of objects, with no path to\ngeneralization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping\npolicy designed for robust and reactive grasping of novel objects in cluttered\nenvironments. Grasp-MPC incorporates a value function, trained on visual\nobservations from a large-scale synthetic dataset of 2 million grasp\ntrajectories that include successful and failed attempts. We deploy this\nlearned value function in an MPC framework in combination with other cost terms\nthat encourage collision avoidance and smooth execution. We evaluate Grasp-MPC\non FetchBench and real-world settings across diverse environments. Grasp-MPC\nimproves grasp success rates by up to 32.6% in simulation and 33.3% in\nreal-world noisy conditions, outperforming open-loop, diffusion policy,\ntransformer policy, and IQL approaches. Videos and more at\nhttp://grasp-mpc.github.io.", "published": "2025-09-07 20:28:21", "link": "http://arxiv.org/abs/2509.06201v1", "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "cs.RO"}
{"title": "AI Governance in Higher Education: A course design exploring regulatory, ethical and practical considerations", "abstract": "As artificial intelligence (AI) systems permeate critical sectors, the need\nfor professionals who can address ethical, legal and governance challenges has\nbecome urgent. Current AI ethics education remains fragmented, often siloed by\ndiscipline and disconnected from practice. This paper synthesizes literature\nand regulatory developments to propose a modular, interdisciplinary curriculum\nthat integrates technical foundations with ethics, law and policy. We highlight\nrecurring operational failures in AI - bias, misspecified objectives,\ngeneralization errors, misuse and governance breakdowns - and link them to\npedagogical strategies for teaching AI governance. Drawing on perspectives from\nthe EU, China and international frameworks, we outline a semester plan that\nemphasizes integrated ethics, stakeholder engagement and experiential learning.\nThe curriculum aims to prepare students to diagnose risks, navigate regulation\nand engage diverse stakeholders, fostering adaptive and ethically grounded\nprofessionals for responsible AI governance.", "published": "2025-09-07 19:09:12", "link": "http://arxiv.org/abs/2509.06176v1", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC", "68T01, 68T20, 91-08, 97U50, 97B10", "I.2.0; K.4.1; K.4.2; K.3.2"], "primary_category": "cs.CY"}
{"title": "Reasoning Language Model for Personalized Lung Cancer Screening", "abstract": "Accurate risk assessment in lung cancer screening is critical for enabling\nearly cancer detection and minimizing unnecessary invasive procedures. The Lung\nCT Screening Reporting and Data System (Lung-RADS) has been widely used as the\nstandard framework for patient management and follow-up. Nevertheless,\nLung-RADS faces trade-offs between sensitivity and specificity, as it\nstratifies risk solely based on lung nodule characteristics without\nincorporating various risk factors. Here we propose a reasoning language model\n(RLM) to integrate radiology findings with longitudinal medical records for\nindividualized lung cancer risk assessment. Through a systematic study\nincluding dataset construction and distillation, supervised fine-tuning,\nreinforcement learning, and comprehensive evaluation, our model makes\nsignificant improvements in risk prediction performance on datasets in the\nnational lung screening trial. Notably, RLM can decompose the risk evaluation\ntask into sub-components, analyze the contributions of diverse risk factors,\nand synthesize them into a final risk score computed using our data-driven\nsystem equation. Our approach improves both predictive accuracy and\nmonitorability through the chain of thought reasoning process, thereby\nfacilitating clinical translation into lung cancer screening.", "published": "2025-09-07 18:38:39", "link": "http://arxiv.org/abs/2509.06169v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning", "abstract": "Video Scene Graph Generation (VidSGG) aims to represent dynamic visual\ncontent by detecting objects and modeling their temporal interactions as\nstructured graphs. Prior studies typically target either coarse-grained\nbox-level or fine-grained panoptic pixel-level VidSGG, often requiring\ntask-specific architectures and multi-stage training pipelines. In this paper,\nwe present UNO (UNified Object-centric VidSGG), a single-stage, unified\nframework that jointly addresses both tasks within an end-to-end architecture.\nUNO is designed to minimize task-specific modifications and maximize parameter\nsharing, enabling generalization across different levels of visual granularity.\nThe core of UNO is an extended slot attention mechanism that decomposes visual\nfeatures into object and relation slots. To ensure robust temporal modeling, we\nintroduce object temporal consistency learning, which enforces consistent\nobject representations across frames without relying on explicit tracking\nmodules. Additionally, a dynamic triplet prediction module links relation slots\nto corresponding object pairs, capturing evolving interactions over time. We\nevaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results\ndemonstrate that UNO not only achieves competitive performance across both\ntasks but also offers improved efficiency through a unified, object-centric\ndesign.", "published": "2025-09-07 18:30:41", "link": "http://arxiv.org/abs/2509.06165v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Tracking daily paths in home contexts with RSSI fingerprinting based on UWB through deep learning models", "abstract": "The field of human activity recognition has evolved significantly, driven\nlargely by advancements in Internet of Things (IoT) device technology,\nparticularly in personal devices. This study investigates the use of\nultra-wideband (UWB) technology for tracking inhabitant paths in home\nenvironments using deep learning models. UWB technology estimates user\nlocations via time-of-flight and time-difference-of-arrival methods, which are\nsignificantly affected by the presence of walls and obstacles in real\nenvironments, reducing their precision. To address these challenges, we propose\na fingerprinting-based approach utilizing received signal strength indicator\n(RSSI) data collected from inhabitants in two flats (60 m2 and 100 m2) while\nperforming daily activities. We compare the performance of convolutional neural\nnetwork (CNN), long short-term memory (LSTM), and hybrid CNN+LSTM models, as\nwell as the use of Bluetooth technology. Additionally, we evaluate the impact\nof the type and duration of the temporal window (future, past, or a combination\nof both). Our results demonstrate a mean absolute error close to 50 cm,\nhighlighting the superiority of the hybrid model in providing accurate location\nestimates, thus facilitating its application in daily human activity\nrecognition in residential settings.", "published": "2025-09-07 18:08:05", "link": "http://arxiv.org/abs/2509.06161v1", "categories": ["cs.LG", "cs.AI", "I.2.0"], "primary_category": "cs.LG"}
{"title": "Exploring Light-Weight Object Recognition for Real-Time Document Detection", "abstract": "Object Recognition and Document Skew Estimation have come a long way in terms\nof performance and efficiency. New models follow one of two directions:\nimproving performance using larger models, and improving efficiency using\nsmaller models. However, real-time document detection and rectification is a\nniche that is largely unexplored by the literature, yet it remains a vital step\nfor automatic information retrieval from visual documents. In this work, we\nstrive towards an efficient document detection pipeline that is satisfactory in\nterms of Optical Character Recognition (OCR) retrieval and faster than other\navailable solutions. We adapt IWPOD-Net, a license plate detection network, and\ntrain it for detection on NBID, a synthetic ID card dataset. We experiment with\ndata augmentation and cross-dataset validation with MIDV (another synthetic ID\nand passport document dataset) to find the optimal scenario for the model.\nOther methods from both the Object Recognition and Skew Estimation\nstate-of-the-art are evaluated for comparison with our approach. We use each\nmethod to detect and rectify the document, which is then read by an OCR system.\nThe OCR output is then evaluated using a novel OCR quality metric based on the\nLevenshtein distance. Since the end goal is to improve automatic information\nretrieval, we use the overall OCR quality as a performance metric. We observe\nthat with a promising model, document rectification does not have to be perfect\nto attain state-of-the-art performance scores. We show that our model is\nsmaller and more efficient than current state-of-the-art solutions while\nretaining a competitive OCR quality metric. All code is available at\nhttps://github.com/BOVIFOCR/iwpod-doc-corners.git", "published": "2025-09-07 23:58:28", "link": "http://arxiv.org/abs/2509.06246v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation", "abstract": "Grounding object affordance is fundamental to robotic manipulation as it\nestablishes the critical link between perception and action among interacting\nobjects. However, prior works predominantly focus on predicting single-object\naffordance, overlooking the fact that most real-world interactions involve\nrelationships between pairs of objects. In this work, we address the challenge\nof object-to-object affordance grounding under limited data contraints.\nInspired by recent advances in few-shot learning with 2D vision foundation\nmodels, we propose a novel one-shot 3D object-to-object affordance learning\napproach for robotic manipulation. Semantic features from vision foundation\nmodels combined with point cloud representation for geometric understanding\nenable our one-shot learning pipeline to generalize effectively to novel\nobjects and categories. We further integrate our 3D affordance representation\nwith large language models (LLMs) for robotics manipulation, significantly\nenhancing LLMs' capability to comprehend and reason about object interactions\nwhen generating task-specific constraint functions. Our experiments on 3D\nobject-to-object affordance grounding and robotic manipulation demonstrate that\nour O$^3$Afford significantly outperforms existing baselines in terms of both\naccuracy and generalization capability.", "published": "2025-09-07 22:45:06", "link": "http://arxiv.org/abs/2509.06233v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models", "abstract": "Bone fractures present a major global health challenge, often resulting in\npain, reduced mobility, and productivity loss, particularly in low-resource\nsettings where access to expert radiology services is limited. Conventional\nimaging methods suffer from high costs, radiation exposure, and dependency on\nspecialized interpretation. To address this, we developed an AI-based solution\nfor automated fracture detection from X-ray images using a custom Convolutional\nNeural Network (CNN) and benchmarked it against transfer learning models\nincluding EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on\nthe publicly available FracAtlas dataset, comprising 4,083 anonymized\nmusculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94\nprecision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.\nAlthough transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)\nperformed poorly in this specific setup, these results should be interpreted in\nlight of class imbalance and data set limitations. This work highlights the\npromise of lightweight CNNs for detecting fractures in X-rays and underscores\nthe importance of fair benchmarking, diverse datasets, and external validation\nfor clinical translation", "published": "2025-09-07 22:30:25", "link": "http://arxiv.org/abs/2509.06228v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)", "abstract": "Recent 3D generative models, which are capable of generating full object\nshapes from just a few images, now open up new opportunities in robotics. In\nthis work, we show that 3D generative models can be used to augment a dataset\nfrom a single real-world demonstration, after which an omnidirectional policy\ncan be learned within this imagined dataset. We found that this enables a robot\nto perform a task when initialised from states very far from those observed\nduring the demonstration, including starting from the opposite side of the\nobject relative to the real-world demonstration, significantly reducing the\nnumber of demonstrations required for policy learning. Through several\nreal-world experiments across tasks such as grasping objects, opening a drawer,\nand placing trash into a bin, we study these omnidirectional policies by\ninvestigating the effect of various design choices on policy behaviour, and we\nshow superior performance to recent baselines which use alternative methods for\ndata augmentation.", "published": "2025-09-07 20:00:59", "link": "http://arxiv.org/abs/2509.06191v1", "categories": ["cs.RO", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes", "abstract": "The growing popularity of robotic minimally invasive surgeries has made deep\nlearning-based surgical training a key area of research. A thorough\nunderstanding of the surgical scene components is crucial, which semantic\nsegmentation models can help achieve. However, most existing work focuses on\nsurgical tools and overlooks anatomical objects. Additionally, current\nstate-of-the-art (SOTA) models struggle to balance capturing high-level\ncontextual features and low-level edge features. We propose a Feature-Adaptive\nSpatial Localization model (FASL-Seg), designed to capture features at multiple\nlevels of detail through two distinct processing streams, namely a Low-Level\nFeature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,\nfor varying feature resolutions - enabling precise segmentation of anatomy and\nsurgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark\ndatasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model\nachieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy\nsegmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU\nof 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,\nrespectively, outperforming SOTA overall performance, with comparable per-class\nSOTA results in both datasets and consistent performance in various classes for\nanatomy and instruments, demonstrating the effectiveness of distinct processing\nstreams for varying feature resolutions.", "published": "2025-09-07 17:59:09", "link": "http://arxiv.org/abs/2509.06159v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.4.6; I.4.8; J.3"], "primary_category": "eess.IV"}
{"title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts", "abstract": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.", "published": "2025-09-07 17:55:03", "link": "http://arxiv.org/abs/2509.06155v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving", "abstract": "The integration of AI with medical images enables the extraction of implicit\nimage-derived biomarkers for a precise health assessment. Recently, retinal\nage, a biomarker predicted from fundus images, is a proven predictor of\nsystemic disease risks, behavioral patterns, aging trajectory and even\nmortality. However, the capability to infer such sensitive biometric data\nraises significant privacy risks, where unauthorized use of fundus images could\nlead to bioinformation leakage, breaching individual privacy. In response, we\nformulate a new research problem of biometric privacy associated with medical\nimages and propose RetinaGuard, a novel privacy-enhancing framework that\nemploys a feature-level generative adversarial masking mechanism to obscure\nretinal age while preserving image visual quality and disease diagnostic\nutility. The framework further utilizes a novel multiple-to-one knowledge\ndistillation strategy incorporating a retinal foundation model and diverse\nsurrogate age encoders to enable a universal defense against black-box age\nprediction models. Comprehensive evaluations confirm that RetinaGuard\nsuccessfully obfuscates retinal age prediction with minimal impact on image\nquality and pathological feature representation. RetinaGuard is also flexible\nfor extension to other medical image derived biomarkers. RetinaGuard is also\nflexible for extension to other medical image biomarkers.", "published": "2025-09-07 17:16:42", "link": "http://arxiv.org/abs/2509.06142v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks", "abstract": "Multispectral and hyperspectral imagery are widely used in agriculture,\nenvironmental monitoring, and urban planning due to their complementary spatial\nand spectral characteristics. A fundamental trade-off persists: multispectral\nimagery offers high spatial but limited spectral resolution, while\nhyperspectral imagery provides rich spectra at lower spatial resolution. Prior\nhyperspectral generation approaches (e.g., pan-sharpening variants, matrix\nfactorization, CNNs) often struggle to jointly preserve spatial detail and\nspectral fidelity. In response, we propose SpecSwin3D, a transformer-based\nmodel that generates hyperspectral imagery from multispectral inputs while\npreserving both spatial and spectral quality. Specifically, SpecSwin3D takes\nfive multispectral bands as input and reconstructs 224 hyperspectral bands at\nthe same spatial resolution. In addition, we observe that reconstruction errors\ngrow for hyperspectral bands spectrally distant from the input bands. To\naddress this, we introduce a cascade training strategy that progressively\nexpands the spectral range to stabilize learning and improve fidelity.\nMoreover, we design an optimized band sequence that strategically repeats and\norders the five selected multispectral bands to better capture pairwise\nrelations within a 3D shifted-window transformer framework. Quantitatively, our\nmodel achieves a PSNR of 35.82 dB, SAM of 2.40{\\deg}, and SSIM of 0.96,\noutperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by\nmore than half. Beyond reconstruction, we further demonstrate the practical\nvalue of SpecSwin3D on two downstream tasks, including land use classification\nand burnt area segmentation.", "published": "2025-09-07 16:18:31", "link": "http://arxiv.org/abs/2509.06122v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "CARDIE: clustering algorithm on relevant descriptors for image enhancement", "abstract": "Automatic image clustering is a cornerstone of computer vision, yet its\napplication to image enhancement remains limited, primarily due to the\ndifficulty of defining clusters that are meaningful for this specific task. To\naddress this issue, we introduce CARDIE, an unsupervised algorithm that\nclusters images based on their color and luminosity content. In addition, we\nintroduce a method to quantify the impact of image enhancement algorithms on\nluminance distribution and local variance. Using this method, we demonstrate\nthat CARDIE produces clusters more relevant to image enhancement than those\nderived from semantic image attributes. Furthermore, we demonstrate that CARDIE\nclusters can be leveraged to resample image enhancement datasets, leading to\nimproved performance for tone mapping and denoising algorithms. To encourage\nadoption and ensure reproducibility, we publicly release CARDIE code on our\nGitHub.", "published": "2025-09-07 15:55:55", "link": "http://arxiv.org/abs/2509.06116v1", "categories": ["cs.CV", "I.4.8"], "primary_category": "cs.CV"}
{"title": "PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology", "abstract": "Accurate analysis of pathological images is essential for automated tumor\ndiagnosis but remains challenging due to high structural similarity and subtle\nmorphological variations in tissue images. Current vision-language (VL) models\noften struggle to capture the complex reasoning required for interpreting\nstructured pathological reports. To address these limitations, we propose\nPathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in\nhierarchical semantic understanding and compositional reasoning within the\npathology domain. Results of this benchmark reveal that existing VL models fail\nto effectively model intricate cross-modal relationships, hence limiting their\napplicability in clinical setting. To overcome this, we further introduce a\npathology-specific VL training scheme that generates enhanced and perturbed\nsamples for multimodal contrastive learning. Experimental evaluations\ndemonstrate that our approach achieves state-of-the-art performance on\nPathoHR-Bench and six additional pathology datasets, highlighting its\neffectiveness in fine-grained pathology representation.", "published": "2025-09-07 15:42:38", "link": "http://arxiv.org/abs/2509.06105v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation", "abstract": "Foundation models have become a promising paradigm for advancing medical\nimage analysis, particularly for segmentation tasks where downstream\napplications often emerge sequentially. Existing fine-tuning strategies,\nhowever, remain limited: parallel fine-tuning isolates tasks and fails to\nexploit shared knowledge, while multi-task fine-tuning requires simultaneous\naccess to all datasets and struggles with incremental task integration. To\naddress these challenges, we propose MedSeqFT, a sequential fine-tuning\nframework that progressively adapts pre-trained models to new tasks while\nrefining their representational capacity. MedSeqFT introduces two core\ncomponents: (1) Maximum Data Similarity (MDS) selection, which identifies\ndownstream samples most representative of the original pre-training\ndistribution to preserve general knowledge, and (2) Knowledge and\nGeneralization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge\ndistillation scheme that balances task-specific adaptation with the retention\nof pre-trained knowledge. Extensive experiments on two multi-task datasets\ncovering ten 3D segmentation tasks demonstrate that MedSeqFT consistently\noutperforms state-of-the-art fine-tuning strategies, yielding substantial\nperformance gains (e.g., an average Dice improvement of 3.0%). Furthermore,\nevaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT\nenhances transferability, particularly for tumor segmentation. Visual analyses\nof loss landscapes and parameter variations further highlight the robustness of\nMedSeqFT. These results establish sequential fine-tuning as an effective,\nknowledge-retentive paradigm for adapting foundation models to evolving\nclinical tasks. Code will be released.", "published": "2025-09-07 15:22:53", "link": "http://arxiv.org/abs/2509.06096v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization", "abstract": "In this work, we develop a novel technique for reconstructing images from\nprojection-based nano- and microtomography. Our contribution focuses on\nenhancing reconstruction quality, particularly for specimen composed of\nhomogeneous material phases connected by sharp edges. This is accomplished by\ntraining a neural network to identify edges within subpictures. The trained\nnetwork is then integrated into a mathematical optimization model, to reduce\nartifacts from previous reconstructions. To this end, the optimization approach\nfavors solutions according to the learned predictions, however may also\ndetermine alternative solutions if these are strongly supported by the raw\ndata. Hence, our technique successfully incorporates knowledge about the\nhomogeneity and presence of sharp edges in the sample and thereby eliminates\nblurriness. Our results on experimental datasets show significant enhancements\nin interface sharpness and material homogeneity compared to benchmark\nalgorithms. Thus, our technique produces high-quality reconstructions,\nshowcasing its potential for advancing tomographic imaging techniques.", "published": "2025-09-07 14:51:48", "link": "http://arxiv.org/abs/2509.06082v1", "categories": ["cs.CV", "cond-mat.mtrl-sci", "90C20, 94A08, 68U10"], "primary_category": "cs.CV"}
{"title": "Home-made Diffusion Model from Scratch to Hatch", "abstract": "We introduce Home-made Diffusion Model (HDM), an efficient yet powerful\ntext-to-image diffusion model optimized for training (and inferring) on\nconsumer-grade hardware. HDM achieves competitive 1024x1024 generation quality\nwhile maintaining a remarkably low training cost of $535-620 using four RTX5090\nGPUs, representing a significant reduction in computational requirements\ncompared to traditional approaches. Our key contributions include: (1)\nCross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer\n(XUT), that employs cross-attention for skip connections, providing superior\nfeature integration that leads to remarkable compositional consistency; (2) a\ncomprehensive training recipe that incorporates TREAD acceleration, a novel\nshifted square crop strategy for efficient arbitrary aspect-ratio training, and\nprogressive resolution scaling; and (3) an empirical demonstration that smaller\nmodels (343M parameters) with carefully crafted architectures can achieve\nhigh-quality results and emergent capabilities, such as intuitive camera\ncontrol. Our work provides an alternative paradigm of scaling, demonstrating a\nviable path toward democratizing high-quality text-to-image generation for\nindividual researchers and smaller organizations with limited computational\nresources.", "published": "2025-09-07 14:21:57", "link": "http://arxiv.org/abs/2509.06068v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities", "abstract": "Buoyancy-driven heat transfer in closed cavities serves as a canonical\ntestbed for thermal design High-fidelity CFD modelling yields accurate thermal\nfield solutions, yet its reliance on expert-crafted physics models, fine\nmeshes, and intensive computation limits rapid iteration. Recent developments\nin data-driven modeling, especially Graph Neural Networks (GNNs), offer new\nalternatives for learning thermal-fluid behavior directly from simulation data,\nparticularly on irregular mesh structures. However, conventional GNNs often\nstruggle to capture long-range dependencies in high-resolution graph\nstructures. To overcome this limitation, we propose a novel multi-stage GNN\narchitecture that leverages hierarchical pooling and unpooling operations to\nprogressively model global-to-local interactions across multiple spatial\nscales. We evaluate the proposed model on our newly developed CFD dataset\nsimulating natural convection within a rectangular cavities with varying aspect\nratios where the bottom wall is isothermal hot, the top wall is isothermal\ncold, and the two vertical walls are adiabatic. Experimental results\ndemonstrate that the proposed model achieves higher predictive accuracy,\nimproved training efficiency, and reduced long-term error accumulation compared\nto state-of-the-art (SOTA) GNN baselines. These findings underscore the\npotential of the proposed multi-stage GNN approach for modeling complex heat\ntransfer in mesh-based fluid dynamics simulations.", "published": "2025-09-07 13:05:39", "link": "http://arxiv.org/abs/2509.06041v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models", "abstract": "Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.", "published": "2025-09-07 12:53:06", "link": "http://arxiv.org/abs/2509.06040v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Degree Realization by Bipartite Cactus Graphs", "abstract": "The \\textsc{Degree Realization} problem with respect to a graph family\n$\\mathcal{F}$ is defined as follows. The input is a sequence $d$ of $n$\npositive integers, and the goal is to decide whether there exists a graph $G\n\\in \\mathcal{F}$ whose degrees correspond to $d$. The main challenges are to\nprovide a precise characterization of all the sequences that admit a\nrealization in $\\mathcal{F}$ and to design efficient algorithms that construct\none of the possible realizations, if one exists.\n  This paper studies the problem of realizing degree sequences by bipartite\ncactus graphs (where the input is given as a single sequence, without the\nbi-partition). A characterization of the sequences that have a cactus\nrealization is already known [28]. In this paper, we provide a systematic way\nto obtain such a characterization, accompanied by a realization algorithm. This\nallows us to derive a characterization for bipartite cactus graphs, and as a\nbyproduct, also for several other interesting sub-families of cactus graphs,\nincluding bridge-less cactus graphs and core cactus graphs, as well as for the\nbipartite sub-families of these families.", "published": "2025-09-07 20:05:50", "link": "http://arxiv.org/abs/2509.06194v1", "categories": ["cs.DM", "cs.DS", "05C07, 68R10"], "primary_category": "cs.DM"}
{"title": "Separable convex optimization over indegree polytopes", "abstract": "We study egalitarian (acyclic) orientations of undirected graphs under\nindegree-based objectives, such as minimizing the $\\varphi$-sum of indegrees\nfor a strictly convex function $\\varphi$, decreasing minimization (dec-min),\nand increasing maximization (inc-max). In the non-acyclic setting of Frank and\nMurota (2022), a single orientation simultaneously optimizes these three\nobjectives, however, restricting to acyclic orientations confines us to the\ncorners of the indegree polytope, where these fairness objectives do diverge.\nWe establish strong hardness results across a broad range of settings:\nminimizing the $\\varphi$-sum of indegrees is NP-hard for every discrete\nstrictly convex function $\\varphi$; dec-min and inc-max are NP-hard for every\nindegree bound $k \\geq 2$, as well as without a bound; and the complementary\ninc-min and dec-max problems are NP-hard even on $3$-regular graphs. On the\nalgorithmic side, we give a polynomial-time algorithm for minimizing the\nmaximum weighted indegree via a weighted smallest-last ordering. We also\nprovide an exact exponential-time algorithm for minimizing general separable\ndiscrete convex objectives over indegrees, and a polynomial-time algorithm for\nthe non-acyclic case. Finally, for maximizing the sum of the products of\nindegrees and outdegrees, we prove NP-hardness on graphs of maximum degree $4$,\ngive an algorithm for maximum degree $3$, and provide a $3$-approximation\nalgorithm. Our results delineate the algorithmic frontier of convex integral\noptimization over indegree (base-)polytopes, and highlight both theoretical\nconsequences and practical implications, notably for scheduling and\ndeadlock-free routing.", "published": "2025-09-07 19:16:16", "link": "http://arxiv.org/abs/2509.06182v1", "categories": ["math.CO", "cs.DM", "cs.DS", "math.OC"], "primary_category": "math.CO"}
{"title": "Modeling shopper interest broadness with entropy-driven dialogue policy in the context of arbitrarily large product catalogs", "abstract": "Conversational recommender systems promise rich interactions for e-commerce,\nbut balancing exploration (clarifying user needs) and exploitation (making\nrecommendations) remains challenging, especially when deploying large language\nmodels (LLMs) with vast product catalogs. We address this challenge by modeling\nthe breadth of user interest via the entropy of retrieval score distributions.\nOur method uses a neural retriever to fetch relevant items for a user query and\ncomputes the entropy of the re-ranked scores to dynamically route the dialogue\npolicy: low-entropy (specific) queries trigger direct recommendations, whereas\nhigh-entropy (ambiguous) queries prompt exploratory questions. This simple yet\neffective strategy allows an LLM-driven agent to remain aware of an arbitrarily\nlarge catalog in real-time without bloating its context window.", "published": "2025-09-07 19:30:09", "link": "http://arxiv.org/abs/2509.06185v1", "categories": ["cs.IR", "cs.LG"], "primary_category": "cs.IR"}
{"title": "DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers", "abstract": "We present DISTRIBUTEDANN, a distributed vector search service that makes it\npossible to search over a single 50 billion vector graph index spread across\nover a thousand machines that offers 26ms median query latency and processes\nover 100,000 queries per second. This is 6x more efficient than existing\npartitioning and routing strategies that route the vector query to a subset of\npartitions in a scale out vector search system. DISTRIBUTEDANN is built using\ntwo well-understood components: a distributed key-value store and an in-memory\nANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for\nserving the Bing search engine, and we share our experience from making this\ntransition.", "published": "2025-09-07 13:13:02", "link": "http://arxiv.org/abs/2509.06046v1", "categories": ["cs.DC", "cs.DS", "cs.IR", "E.1; H.3.3"], "primary_category": "cs.DC"}
{"title": "A Survey of Real-World Recommender Systems: Challenges, Constraints, and Industrial Perspectives", "abstract": "Recommender systems have generated tremendous value for both users and\nbusinesses, drawing significant attention from academia and industry alike.\nHowever, due to practical constraints, academic research remains largely\nconfined to offline dataset optimizations, lacking access to real user data and\nlarge-scale recommendation platforms. This limitation reduces practical\nrelevance, slows technological progress, and hampers a full understanding of\nthe key challenges in recommender systems. In this survey, we provide a\nsystematic review of industrial recommender systems and contrast them with\ntheir academic counterparts. We highlight key differences in data scale,\nreal-time requirements, and evaluation methodologies, and we summarize major\nreal-world recommendation scenarios along with their associated challenges. We\nthen examine how industry practitioners address these challenges in\nTransaction-Oriented Recommender Systems and Content-Oriented Recommender\nSystems, a new classification grounded in item characteristics and\nrecommendation objectives. Finally, we outline promising research directions,\nincluding the often-overlooked role of user decision-making, the integration of\neconomic and psychological theories, and concrete suggestions for advancing\nacademic research. Our goal is to enhance academia's understanding of practical\nrecommender systems, bridge the growing development gap, and foster stronger\ncollaboration between industry and academia.", "published": "2025-09-07 10:29:41", "link": "http://arxiv.org/abs/2509.06002v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Learning to Construct Knowledge through Sparse Reference Selection with Reinforcement Learning", "abstract": "The rapid expansion of scientific literature makes it increasingly difficult\nto acquire new knowledge, particularly in specialized domains where reasoning\nis complex, full-text access is restricted, and target references are sparse\namong a large set of candidates. We present a Deep Reinforcement Learning\nframework for sparse reference selection that emulates human knowledge\nconstruction, prioritizing which papers to read under limited time and cost.\nEvaluated on drug--gene relation discovery with access restricted to titles and\nabstracts, our approach demonstrates that both humans and machines can\nconstruct knowledge effectively from partial information.", "published": "2025-09-07 00:19:13", "link": "http://arxiv.org/abs/2509.05874v1", "categories": ["cs.LG", "cs.AI", "cs.IR", "I.2.6"], "primary_category": "cs.LG"}
{"title": "Moments of finitary factor maps between Bernoulli processes", "abstract": "The problem of what moments can exist for the coding radius of a finitary map\nbetween two i.i.d. processes, has been extensively studied in the case of\n$\\mathbb{Z}$-processes. Here we treat this problem for factor maps between\n$\\mathbb{Z}^{d}$-processes ($d>1$). By modeling the homomorphism with a map\nbetween spaces of finite sequences, we extend Harvey and Peres' result, showing\nthat for a finitary homomorphism between two i.i.d. processes of equal entropy,\nif the coding radius of the map has a finite $\\frac{d}{2}$-moment, then the two\nprocesses share the same informational variance. We use our modeling technique\nto prove a \"Schmidt-type theorem\" - that in case the above homomorphism has a\ncoding radius of exponential tails, then the two processes are essentially the\nsame. This result appears to be new even for the one-dimensional case,\naddressing a question of Angel and Spinka.", "published": "2025-09-07 11:26:05", "link": "http://arxiv.org/abs/2509.06018v1", "categories": ["math.PR", "cs.IT", "math.DS", "math.IT", "37A35, 37A50, 60G10"], "primary_category": "math.PR"}
{"title": "Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models", "abstract": "Recent deep learning-based methods for lossy image compression achieve\ncompetitive rate-distortion performance through extensive end-to-end training\nand advanced architectures. However, emerging applications increasingly\nprioritize semantic preservation over pixel-level reconstruction and demand\nrobust performance across diverse data distributions and downstream tasks.\nThese challenges call for advanced semantic compression paradigms. Motivated by\nthe zero-shot and representational capabilities of multimodal foundation\nmodels, we propose a novel semantic compression method based on the contrastive\nlanguage-image pretraining (CLIP) model. Rather than compressing images for\nreconstruction, we propose compressing the CLIP feature embeddings into minimal\nbits while preserving semantic information across different tasks. Experiments\nshow that our method maintains semantic integrity across benchmark datasets,\nachieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This\nis less than 5% of the bitrate required by mainstream image compression\napproaches for comparable performance. Remarkably, even under extreme\ncompression, the proposed approach exhibits zero-shot robustness across diverse\ndata distributions and downstream tasks.", "published": "2025-09-07 04:49:25", "link": "http://arxiv.org/abs/2509.05925v1", "categories": ["cs.CV", "cs.IT", "math.IT"], "primary_category": "cs.CV"}
{"title": "Ground state energies of multipartite $p$-spin models -- partially lifted RDT view", "abstract": "We consider ground state energies (GSE) of multipartite $p$-spin models.\nRelying on partially lifted random duality theory (pl RDT) concepts we\nintroduce an analytical mechanism that produces easy to compute lower and upper\nGSE bounds for \\emph{any} spin sets. We uncover that these bounds actually\nmatch in case of fully spherical sets thereby providing optimal GSE values for\nspherical multipartite pure $p$-spin models. Numerical evidence further\nsuggests that our upper and lower bounds may match even in the Ising scenarios.\nAs such developments are rather intriguing, we formulate several questions\nregarding the connection between our bounds matching generality on the one side\nand the spin sets structures on the other.", "published": "2025-09-07 04:23:31", "link": "http://arxiv.org/abs/2509.05916v1", "categories": ["math.PR", "cond-mat.dis-nn", "cs.IT", "math-ph", "math.IT", "math.MP"], "primary_category": "math.PR"}
{"title": "Study of Iterative Detection, Decoding and Channel Estimation for RIS-Aided MIMO Networks", "abstract": "This work proposes an iterative detection, decoding and channel estimation\nscheme for multiple-antenna systems assisted by multiple reflective intelligent\nsurfaces (RIS). A novel channel estimation technique that exploits low-density\nparity-check (LDPC) codes and iterative processing is developed to enhance\nestimation accuracy while reducing the number of required pilot symbols. The\nkey idea is to exploit encoded pilots to improve the iterative process,\nenabling the use of not only pilot bits but also parity bits from the coded\npacket to refine channel estimation. Simulations analyze a sub-6 GHz scenario\nwhere the channel propagation is not sparse and multiple RIS are deployed,\nconsidering both LOS and NLOS conditions. Numerical results show significant\nperformance gains for the proposed scheme and estimator over competing\napproaches.", "published": "2025-09-07 00:25:55", "link": "http://arxiv.org/abs/2509.05875v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services", "abstract": "The proliferation of Large Language Models (LLMs) has created a significant\nintegration challenge in the AI agent ecosystem, often called the \"$N \\times M$\nproblem,\" where N models require custom integrations for M tools. This\nfragmentation stifles innovation and creates substantial development overhead.\nWhile the Model Context Protocol (MCP) has emerged as a standard to resolve\nthis, its adoption is hindered by the manual effort required to convert the\nvast universe of existing software into MCP-compliant services. This is\nespecially true for the millions of open-source repositories on GitHub, the\nworld's largest collection of functional code. This paper introduces Code2MCP,\na highly automated, agentic framework designed to transform any GitHub\nrepository into a functional MCP service with minimal human intervention. Our\nsystem employs a multi-stage workflow that automates the entire process, from\ncode analysis and environment configuration to service generation and\ndeployment. A key innovation of our framework is an LLM-driven, closed-loop\n\"Run--Review--Fix\" cycle, which enables the system to autonomously debug and\nrepair the code it generates. Code2MCP produces not only deployable services\nbut also comprehensive technical documentation, acting as a catalyst to\naccelerate the MCP ecosystem by systematically unlocking the world's largest\nopen-source code repository and automating the critical last mile of tool\nintegration. The code is open-sourced at\nhttps://github.com/DEFENSE-SEU/MCP-Github-Agent.", "published": "2025-09-07 06:13:25", "link": "http://arxiv.org/abs/2509.05941v1", "categories": ["cs.SE", "cs.LG", "cs.MA"], "primary_category": "cs.SE"}
{"title": "Recursive vectorized computation of the Frobenius norm", "abstract": "Recursive algorithms for computing the Frobenius norm of a real array are\nproposed, based on hypot, a hypotenuse function. Comparing their relative\naccuracy bounds with those of the BLAS routine DNRM2 it is shown that the\nproposed algorithms could in many cases be significantly more accurate. The\nscalar recursive algorithms are vectorized with the Intel's vector instructions\nto achieve performance comparable to xNRM2, and are further parallelized with\nOpenCilk. Some scalar algorithms are unconditionally bitwise reproducible,\nwhile the reproducibility of the vector ones depends on the vector width.", "published": "2025-09-07 21:50:08", "link": "http://arxiv.org/abs/2509.06220v1", "categories": ["math.NA", "cs.NA", "65F35 (Primary) 65Y05, 65G50 (Secondary)"], "primary_category": "math.NA"}
{"title": "The role of the initial distribution in population survival within a bounded habitat", "abstract": "In this paper, we analyze the role of initial conditions in population\npersistence. Specifically, we consider the reaction-diffusion equation\n$u_t\\,=\\,D\\,(u^{\\nu-1}\\,u_x)_x\\,+\\,a\\,u^{\\mu}$, with $\\mu,\\nu>0$, accompanied\nby hostile boundary conditions and examine two families of one-parametric\ninitial distributions, including homogeneous distributions. The model was\npreviously studied by Colombo and Anteneodo (2018). They determined appropriate\nhabitat sizes $l$ for the survival of a population, whose individuals are\ninitially placed homogeneously within the full habitat domain with a total\ninitial population $n_0$. We show that the survival condition can be naturally\nformulated in terms of the parameter\n$Q:=\\frac{a}{D}l^{-\\mu+\\nu+2}n_0^{\\mu-\\nu}$. Indeed, there exists a critical\nvalue $Q_c$ determined by $\\mu$, $\\nu$ and the initial distribution parameter\nsuch that the survival condition can always be written as $Q\\geq Q_c$. Notably,\nfrom this point of view, one can derive a condition for $Q$ that holds\nuniversally for our model under conditional persistence ($\\mu\\geq\\nu$). It\napplies, in particular, to the case $\\mu=\\nu+2$, which was not addressed in the\npreviously mentioned work. Nevertheless, in this case $Q=\\frac{a}{D}n_0^2$,\ntherefore survival depends solely on the total population, not on the habitat\nsize. We apply a finite-difference scheme to estimate $Q_c$. Conversely, given\na population whose evolution is determined by $\\mu$, $\\nu$, $l$, $n_0$, and the\ngrowth and diffusion coefficients $a$ and $D$ (and consequently the value of\n$Q$) we use the numerical algorithm to estimate the initial distribution to\nensure population survival.", "published": "2025-09-07 19:13:14", "link": "http://arxiv.org/abs/2509.06179v1", "categories": ["math.AP", "cs.NA", "math.NA"], "primary_category": "math.AP"}
{"title": "High-order Magnus Expansion for Hamiltonian Simulation", "abstract": "Efficient simulation of quantum dynamics with time-dependent Hamiltonians is\nimportant not only for time-varying systems but also for time-independent\nHamiltonians in the interaction picture. Such simulations are more challenging\nthan their time-independent counterparts due to the complexity introduced by\ntime ordering. Existing algorithms that aim to capture commutator-based scaling\neither exhibit polynomial cost dependence on the Hamiltonian's time derivatives\nor are limited to low-order accuracy. In this work, we establish the general\ncommutator-scaling error bounds for the truncated Magnus expansion at arbitrary\norder, where only Hamiltonian terms appear in the nested commutators, with no\ntime derivatives involved. Building on this analysis, we design a high-order\nquantum algorithm with explicit circuit constructions. The algorithm achieves\ncost scaling with the commutator structure in the high-precision regime and\ndepends only logarithmically on the Hamiltonian's time variation, making it\nefficient for general time-dependent settings, including the interaction\npicture.", "published": "2025-09-07 13:44:55", "link": "http://arxiv.org/abs/2509.06054v1", "categories": ["quant-ph", "cs.NA", "math.NA"], "primary_category": "quant-ph"}
{"title": "Transformation from Bi-CG into Bi-CR using a residual smoothing-like scheme", "abstract": "Residual smoothing techniques, which produce a smooth convergence behavior of\nlinear iterative solvers, also form connections between different methods. For\nexample, minimal residual smoothing can transform the residuals of the\nconjugate gradient (CG) method into those of the conjugate residual (CR) method\nfor linear systems with symmetric matrices. In this study, we investigate\nwhether a similar relationship can be constructed between the Bi-CG and Bi-CR\nmethods for nonsymmetric linear systems. Numerical experiments regarding the\naforementioned connection demonstrate the validity of our insights.", "published": "2025-09-07 09:21:40", "link": "http://arxiv.org/abs/2509.05986v1", "categories": ["math.NA", "cs.NA", "65F10"], "primary_category": "math.NA"}
{"title": "High-order staggered Lagrangian hydrodynamics (I): framework of the discretization scheme", "abstract": "This paper presents a discretization framework for high-order staggered\nLagrangian hydrodynamics, bridging two well-established algorithms algorithms:\nhigh-order curvilinear finite element Lagrangian hydrodynamics [Dobrev et al.\n2012] and compatible hydrodynamics methods[Caramana et al.1998]. We emphasizes\nthe critical relationship between the degrees of freedom (DOFs) associated with\nthe density variable and the choice of numerical quadrature rules which is\nemployed in the mass conservation law. The precise quadrature rule will lead\nthe consistency issues arising from the density and internal energy variables\nbeing defined at different physical points. Our approach resolves this by\nunifying the quadrature rule for the specific discretization space, though this\ninevitably introduces hourglass distortion. Another key feature of the proposed\nframework is the strategic arrangement of DOFs for kinematic and thermodynamic\nvariables, which enhances computational efficiency and leads to diagonal mass\nmatrices in the momentum and energy equations. Finally, we present a smooth\nnumerical example that validate the accuracy of the proposed high-order\nLagrangian hydrodynamics framework.", "published": "2025-09-07 06:21:02", "link": "http://arxiv.org/abs/2509.05944v1", "categories": ["math.NA", "cs.NA", "49N45, 65N21"], "primary_category": "math.NA"}
{"title": "Deep Learning Option Pricing with Market Implied Volatility Surfaces", "abstract": "We present a deep learning framework for pricing options based on\nmarket-implied volatility surfaces. Using end-of-day S\\&P 500 index options\nquotes from 2018-2023, we construct arbitrage-free volatility surfaces and\ngenerate training data for American puts and arithmetic Asian options using\nQuantLib. To address the high dimensionality of volatility surfaces, we employ\na variational autoencoder (VAE) that compresses volatility surfaces across\nmaturities and strikes into a 10-dimensional latent representation. We feed\nthese latent variables, combined with option-specific inputs such as strike and\nmaturity, into a multilayer perceptron to predict option prices. Our model is\ntrained in stages: first to train the VAE for volatility surface compression\nand reconstruction, then options pricing mapping, and finally fine-tune the\nentire network end-to-end. The trained pricer achieves high accuracy across\nAmerican and Asian options, with prediction errors concentrated primarily near\nlong maturities and at-the-money strikes, where absolute bid-ask price\ndifferences are known to be large. Our method offers an efficient and scalable\napproach requiring only a single neural network forward pass and naturally\nimprove with additional data. By bridging volatility surface modeling and\noption pricing in a unified framework, it provides a fast and flexible\nalternative to traditional numerical approaches for exotic options.", "published": "2025-09-07 04:03:48", "link": "http://arxiv.org/abs/2509.05911v1", "categories": ["q-fin.CP"], "primary_category": "q-fin.CP"}
{"title": "Predicting Market Troughs: A Machine Learning Approach with Causal Interpretation", "abstract": "This paper provides robust, new evidence on the causal drivers of market\ntroughs. We demonstrate that conclusions about these triggers are critically\nsensitive to model specification, moving beyond restrictive linear models with\na flexible DML average partial effect causal machine learning framework. Our\nrobust estimates identify the volatility of options-implied risk appetite and\nmarket liquidity as key causal drivers, relationships misrepresented or\nobscured by simpler models. These findings provide high-frequency empirical\nsupport for intermediary asset pricing theories. This causal analysis is\nenabled by a high-performance nowcasting model that accurately identifies\ncapitulation events in real-time.", "published": "2025-09-07 04:38:40", "link": "http://arxiv.org/abs/2509.05922v1", "categories": ["q-fin.ST", "econ.EM", "stat.ML", "91G80, 62P05", "J.4"], "primary_category": "q-fin.ST"}
{"title": "Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators", "abstract": "Neural operators (NOs) approximate mappings between infinite-dimensional\nfunction spaces but require large datasets and struggle with scarce training\ndata. Many NO formulations don't explicitly encode causal, local-in-time\nstructure of physical evolution. While autoregressive models preserve causality\nby predicting next time-steps, they suffer from rapid error accumulation. We\nemploy Graph Neural Simulators (GNS) - a message-passing graph neural network\nframework - with explicit numerical time-stepping schemes to construct accurate\nforward models that learn PDE solutions by modeling instantaneous time\nderivatives. We evaluate our framework on three canonical PDE systems: (1) 2D\nBurgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2D\nAllen-Cahn equation. Rigorous evaluations demonstrate GNS significantly\nimproves data efficiency, achieving higher generalization accuracy with\nsubstantially fewer training trajectories compared to neural operator baselines\nlike DeepONet and FNO. GNS consistently achieves under 1% relative L2 errors\nwith only 30 training samples out of 1000 (3% of available data) across all\nthree PDE systems. It substantially reduces error accumulation over extended\ntemporal horizons: averaged across all cases, GNS reduces autoregressive error\nby 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce a\nPCA+KMeans trajectory selection strategy enhancing low-data performance.\nResults indicate combining graph-based local inductive biases with conventional\ntime integrators yields accurate, physically consistent, and scalable surrogate\nmodels for time-dependent PDEs.", "published": "2025-09-07 17:54:23", "link": "http://arxiv.org/abs/2509.06154v1", "categories": ["cs.LG", "stat.CO", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Additive Distributionally Robust Ranking and Selection", "abstract": "Ranking and selection (R&S) aims to identify the alternative with the best\nmean performance among $k$ simulated alternatives. The practical value of R&S\ndepends on accurate simulation input modeling, which often suffers from the\ncurse of input uncertainty due to limited data. Distributionally robust ranking\nand selection (DRR&S) addresses this challenge by modeling input uncertainty\nvia an ambiguity set of $m > 1$ plausible input distributions, resulting in\n$km$ scenarios in total. Recent DRR&S studies suggest a key structural insight:\nadditivity in budget allocation is essential for efficiency. However, existing\njustifications are heuristic, and fundamental properties such as consistency\nand the precise allocation pattern induced by additivity remain poorly\nunderstood. In this paper, we propose a simple additive allocation (AA)\nprocedure that aims to exclusively sample the $k + m - 1$ previously\nhypothesized critical scenarios. Leveraging boundary-crossing arguments, we\nestablish a lower bound on the probability of correct selection and\ncharacterize the procedure's budget allocation behavior. We then prove that AA\nis consistent and, surprisingly, achieves additivity in the strongest sense: as\nthe total budget increases, only $k + m - 1$ scenarios are sampled infinitely\noften. Notably, the worst-case scenarios of non-best alternatives may not be\namong them, challenging prior beliefs about their criticality. These results\noffer new and counterintuitive insights into the additive structure of DRR&S.\nTo improve practical performance while preserving this structure, we introduce\na general additive allocation (GAA) framework that flexibly incorporates\nsampling rules from traditional R&S procedures in a modular fashion. Numerical\nexperiments support our theoretical findings and demonstrate the competitive\nperformance of the proposed GAA procedures.", "published": "2025-09-07 17:36:29", "link": "http://arxiv.org/abs/2509.06147v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "If generative AI is the answer, what is the question?", "abstract": "Beginning with text and images, generative AI has expanded to audio, video,\ncomputer code, and molecules. Yet, if generative AI is the answer, what is the\nquestion? We explore the foundations of generation as a distinct machine\nlearning task with connections to prediction, compression, and decision-making.\nWe survey five major generative model families: autoregressive models,\nvariational autoencoders, normalizing flows, generative adversarial networks,\nand diffusion models. We then introduce a probabilistic framework that\nemphasizes the distinction between density estimation and generation. We review\na game-theoretic framework with a two-player adversary-learner setup to study\ngeneration. We discuss post-training modifications that prepare generative\nmodels for deployment. We end by highlighting some important topics in socially\nresponsible generation such as privacy, detection of AI-generated content, and\ncopyright and IP. We adopt a task-first framing of generation, focusing on what\ngeneration is as a machine learning problem, rather than only on how models\nimplement it.", "published": "2025-09-07 16:07:45", "link": "http://arxiv.org/abs/2509.06120v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Smoothed Online Optimization for Target Tracking: Robust and Learning-Augmented Algorithms", "abstract": "We introduce the Smoothed Online Optimization for Target Tracking (SOOTT)\nproblem, a new framework that integrates three key objectives in online\ndecision-making under uncertainty: (1) tracking cost for following a\ndynamically moving target, (2) adversarial perturbation cost for withstanding\nunpredictable disturbances, and (3) switching cost for penalizing abrupt\nchanges in decisions. This formulation captures real-world scenarios such as\nelastic and inelastic workload scheduling in AI clusters, where operators must\nbalance long-term service-level agreements (e.g., LLM training) against sudden\ndemand spikes (e.g., real-time inference). We first present BEST, a robust\nalgorithm with provable competitive guarantees for SOOTT. To enhance practical\nperformance, we introduce CoRT, a learning-augmented variant that incorporates\nuntrusted black-box predictions (e.g., from ML models) into its decision\nprocess. Our theoretical analysis shows that CoRT strictly improves over BEST\nwhen predictions are accurate, while maintaining robustness under arbitrary\nprediction errors. We validate our approach through a case study on workload\nscheduling, demonstrating that both algorithms effectively balance trajectory\ntracking, decision smoothness, and resilience to external disturbances.", "published": "2025-09-07 05:22:41", "link": "http://arxiv.org/abs/2509.05930v1", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights", "abstract": "Uncertainty Quantification (UQ) is essential in probabilistic machine\nlearning models, particularly for assessing the reliability of predictions. In\nthis paper, we present a systematic framework for estimating both epistemic and\naleatoric uncertainty in probabilistic models. We focus on Gaussian Process\nLatent Variable Models and employ scalable Random Fourier Features-based\nGaussian Processes to approximate predictive distributions efficiently. We\nderive a theoretical formulation for UQ, propose a Monte Carlo sampling-based\nestimation method, and conduct experiments to evaluate the impact of\nuncertainty estimation. Our results provide insights into the sources of\npredictive uncertainty and illustrate the effectiveness of our approach in\nquantifying the confidence in the predictions.", "published": "2025-09-07 00:38:33", "link": "http://arxiv.org/abs/2509.05877v1", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "DreamAudio: Customized Text-to-Audio Generation with Diffusion Models", "abstract": "With the development of large-scale diffusion-based and\nlanguage-modeling-based generative models, impressive progress has been\nachieved in text-to-audio generation. Despite producing high-quality outputs,\nexisting text-to-audio models mainly aim to generate semantically aligned sound\nand fall short on precisely controlling fine-grained acoustic characteristics\nof specific sounds. As a result, users that need specific sound content may\nfind it challenging to generate the desired audio clips. In this paper, we\npresent DreamAudio for customized text-to-audio generation (CTTA).\nSpecifically, we introduce a new framework that is designed to enable the model\nto identify auditory information from user-provided reference concepts for\naudio generation. Given a few reference audio samples containing personalized\naudio events, our system can generate new audio samples that include these\nspecific events. In addition, two types of datasets are developed for training\nand testing the customized systems. The experiments show that the proposed\nmodel, DreamAudio, generates audio samples that are highly consistent with the\ncustomized audio features and aligned well with the input text prompts.\nFurthermore, DreamAudio offers comparable performance in general text-to-audio\ntasks. We also provide a human-involved dataset containing audio events from\nreal-world CTTA cases as the benchmark for customized generation tasks.", "published": "2025-09-07 12:06:21", "link": "http://arxiv.org/abs/2509.06027v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Xi+: Uncertainty Supervision for Robust Speaker Embedding", "abstract": "There are various factors that can influence the performance of speaker\nrecognition systems, such as emotion, language and other speaker-related or\ncontext-related variations. Since individual speech frames do not contribute\nequally to the utterance-level representation, it is essential to estimate the\nimportance or reliability of each frame. The xi-vector model addresses this by\nassigning different weights to frames based on uncertainty estimation. However,\nits uncertainty estimation model is implicitly trained through classification\nloss alone and does not consider the temporal relationships between frames,\nwhich may lead to suboptimal supervision. In this paper, we propose an improved\narchitecture, xi+. Compared to xi-vector, xi+ incorporates a temporal attention\nmodule to capture frame-level uncertainty in a context-aware manner. In\naddition, we introduce a novel loss function, Stochastic Variance Loss, which\nexplicitly supervises the learning of uncertainty. Results demonstrate\nconsistent performance improvements of about 10\\% on the VoxCeleb1-O set and\n11\\% on the NIST SRE 2024 evaluation set.", "published": "2025-09-07 09:47:37", "link": "http://arxiv.org/abs/2509.05993v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Pinching Antenna System (PASS) Enhanced Covert Communications: Against Warden via Sensing", "abstract": "A sensing-aided covert communication network empowered by pinching antenna\nsystems (PASS) is proposed in this work. Unlike conventional fixed-position\nMIMO arrays, PASS dynamically reconfigures its pinching antennas (PAs) closer\nto the legitimate user, substantially enhancing covertness. To further secure\nthe adversary's channel state information (CSI), a sensing function is\nleveraged to track the malicious warden's movements. In particular, this paper\nfirst proposes an extended Kalman filter (EKF) based approach to fulfilling the\ntracking function. Building on this, a covert communication problem is\nformulated with a joint design of beamforming, artificial noise (AN) signals,\nand the position of PAs. Then, the beamforming and AN design subproblems are\nresolved jointly with a subspace approach, while the PA position optimization\nsubproblem is handled by a deep reinforcement learning (DRL) approach by\ntreating the evolution of the warden's mobility status as a temporally\ncorrected process. Numerical results are presented and demonstrate that: i) the\nEKF approach can accurately track the warden's CSI with low complexity, ii) the\neffectiveness of the proposed solution is verified by its outperformance over\nthe greedy and searching-based benchmarks, and iii) with new design degrees of\nfreedom (DoFs), the performance of PASS is superior to the conventional\nfully-digital MIMO systems.", "published": "2025-09-07 18:41:54", "link": "http://arxiv.org/abs/2509.06170v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Quantum Radar for ISAC: Sum-Rate Optimization", "abstract": "Integrated sensing and communication (ISAC) is emerging as a key enabler for\nspectrum-efficient and hardware-converged wireless networks. However, classical\nradar systems within ISAC architectures face fundamental limitations under low\nsignal power and high-noise conditions. This paper proposes a novel framework\nthat embeds quantum illumination radar into a base station to simultaneously\nsupport full-duplex classical communication and quantum-enhanced target\ndetection. The resulting integrated quantum sensing and classical communication\n(IQSCC) system is optimized via a sum-rate maximization formulation subject to\nradar sensing constraints. The non-convex joint optimization of transmit power\nand beamforming vectors is tackled using the successive convex approximation\ntechnique. Furthermore, we derive performance bounds for classical and quantum\nradar protocols under the statistical detection theory, highlighting the\nquantum advantage in low signal-to-interference-plus-noise ratio regimes.\nSimulation results demonstrate that the proposed IQSCC system achieves a higher\ncommunication throughput than the conventional ISAC baseline while satisfying\nthe sensing requirement.", "published": "2025-09-07 14:24:57", "link": "http://arxiv.org/abs/2509.06070v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "3D-Image Reconstruction using MIMO-SAR FMCW Radar", "abstract": "With the advancement of millimeter-wave radar technology, Synthetic Aperture\nRadar (SAR) imaging at millimeter-wave frequencies has gained significant\nattention in both academic research and industrial applications. However,\ntraditional SAR imaging algorithms primarily focus on extracting\ntwo-dimensional information from detected targets, which limits their potential\nfor 3D scene reconstruction. In this work, we demonstrated a fast time-domain\nreconstruction algorithm for achieving high-resolution 3D radar imaging at\nmillimeter-wave (mmWave) frequencies. This approach leverages a combination of\nvirtual Multiple Input Multiple Output (MIMO) Frequency Modulated Continuous\nWave (FMCW) radar with the precision of Synthetic Aperture Radar (SAR)\ntechnique, setting the stage for a new era of advanced radar imaging\napplications.", "published": "2025-09-07 08:47:45", "link": "http://arxiv.org/abs/2509.05977v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "DeepStream: Prototyping Deep Joint Source-Channel Coding for Real-Time Multimedia Transmissions", "abstract": "Deep learning-based joint source-channel coding (DeepJSCC) has emerged as a\npromising technique in 6G for enhancing the efficiency and reliability of data\ntransmission across diverse modalities, particularly in low signal-to-noise\nratio (SNR) environments. This advantage is realized by leveraging powerful\nneural networks to learn an optimal end-to-end mapping from the source data\ndirectly to the transmit symbol sequence, eliminating the need for separate\nsource coding, channel coding, and modulation. Although numerous efforts have\nbeen made towards efficient DeepJSCC, they have largely stayed at numerical\nsimulations that can be far from practice, leaving the real-world viability of\nDeepJSCC largely unverified. To this end, we prototype DeepStream upon\northogonal frequency division multiplexing (OFDM) technology to offer efficient\nand robust DeepJSCC for multimedia transmission. In conforming to OFDM, we\ndevelop both a feature-to-symbol mapping method and a cross-subcarrier\nprecoding method to improve the subcarrier independence and reduce\npeak-to-average power ratio. To reduce system complexity and enable flexibility\nin accommodating varying quality of service requirements, we further propose a\nprogressive coding strategy that adjusts the compression ratio based on latency\nwith minimal performance loss. We implement DeepStream for real-time image\ntransmission and video streaming using software-defined radio. Extensive\nevaluations verify that DeepStream outperforms both the standard scheme and the\ndirect deployment scheme. Particularly, at an SNR of 10 dB, DeepStream achieves\na PSNR of 35 dB for image transmission and an MS-SSIM of 20 dB for video\nstreaming, whereas the standard scheme fails to recover meaningful information.", "published": "2025-09-07 08:30:16", "link": "http://arxiv.org/abs/2509.05971v1", "categories": ["eess.SP", "cs.MM"], "primary_category": "eess.SP"}
{"title": "The Case for a DNANF 1Pb/s Trans-Atlantic Submarine Cable", "abstract": "The recent progress in low-loss hollow-core fibers allows to speculate on the\npossibility of building a transatlantic submarine cable that can achieve the\ngoal of 1 Pb/s per direction, leveraging bidirectional transmission, and at the\nsame time drastically increase span length, theoretically to 200km.", "published": "2025-09-07 07:54:00", "link": "http://arxiv.org/abs/2509.05959v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Active noise cancellation in ultra-low field MRI: distinct strategies for different channels", "abstract": "Ultra-low field magnetic resonance imaging(ULF-MRI) systems operating in open\nenvironments are highly susceptible to composite electromagnetic\ninterference(EMI). Different imaging channels respond non-uniformly to EMI\nowing to their distinct coupling characteristics. Here, we investigate\nchannel-specific interference pathways in a permanent-magnet-based low-field\nMRI system and show that saddle coils are intrinsically more vulnerable to\ntransverse EMI components than solenoidal coils. To mitigate these\nheterogeneous coupling effects, we propose a dual-stage suppression strategy\nthat combines front-end spatial-domain inverse field reconstruction with\nback-end channel-adaptive active noise cancellation. Experiments demonstrate\nthat this approach suppresses EMI by more than 80%, substantially improves\ninter-channel signal-to-noise ratio(SNR) consistency, and enhances the\nfused-image SNR by 24%. These findings elucidate the channel-dependent nature\nof EMI coupling and establish targeted mitigation strategies, providing both a\ntheoretical basis and practical guidance for noise suppression in future\narray-coil ULF-MRI systems.", "published": "2025-09-07 07:33:42", "link": "http://arxiv.org/abs/2509.05955v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Optimal Anchor Deployment and Topology Design for Large-Scale AUV Navigation", "abstract": "Seafloor acoustic anchors are an important component of AUV navigation,\nproviding absolute updates that correct inertial dead-reckoning. Unlike\nterrestrial positioning systems, the deployment of underwater anchor nodes is\nusually sparse due to the uneven distribution of underwater users, as well as\nthe high economic cost and difficult maintenance of underwater equipment. These\nanchor nodes lack satellite coverage and cannot form ubiquitous backhaul as\nterrestrial nodes do. In this paper, we investigate the optimal anchor\ndeployment topology to provide high-quality AUV navigation and positioning\nservices. We first analyze the possible deployment mode in large-scale\nunderwater navigation system, and formulate a topology optimization for\nunderwater anchor node deployment. Then, we derive a scaling law about the\ninfluence of anchors in each cluster on the navigation performance within a\ngiven area and demonstrate a service area coverage condition with a high\nprobability of reaching the destination. Finally, the optimization performance\nis evaluated through experimental results.", "published": "2025-09-07 02:59:44", "link": "http://arxiv.org/abs/2509.05903v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Toward a Metrology for Artificial Intelligence: Hidden-Rule Environments and Reinforcement Learning", "abstract": "We investigate reinforcement learning in the Game Of Hidden Rules (GOHR)\nenvironment, a complex puzzle in which an agent must infer and execute hidden\nrules to clear a 6$\\times$6 board by placing game pieces into buckets. We\nexplore two state representation strategies, namely Feature-Centric (FC) and\nObject-Centric (OC), and employ a Transformer-based Advantage Actor-Critic\n(A2C) algorithm for training. The agent has access only to partial observations\nand must simultaneously infer the governing rule and learn the optimal policy\nthrough experience. We evaluate our models across multiple rule-based and\ntrial-list-based experimental setups, analyzing transfer effects and the impact\nof representation on learning efficiency.", "published": "2025-09-07 21:22:14", "link": "http://arxiv.org/abs/2509.06213v2", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models", "abstract": "Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.", "published": "2025-09-07 12:53:06", "link": "http://arxiv.org/abs/2509.06040v2", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "High-order staggered Lagrangian hydrodynamics (I): framework of the discretization scheme", "abstract": "This paper presents a discretization framework for high-order staggered\nLagrangian hydrodynamics, bridging two well-established algorithms algorithms:\nhigh-order curvilinear finite element Lagrangian hydrodynamics [Dobrev et al.\n2012] and compatible hydrodynamics methods[Caramana et al.1998]. We emphasizes\nthe critical relationship between the degrees of freedom (DOFs) associated with\nthe density variable and the choice of numerical quadrature rules which is\nemployed in the mass conservation law. The precise quadrature rule will lead\nthe consistency issues arising from the density and internal energy variables\nbeing defined at different physical points. Our approach resolves this by\nunifying the quadrature rule for the specific discretization space, though this\ninevitably introduces hourglass distortion. Another key feature of the proposed\nframework is the strategic arrangement of DOFs for kinematic and thermodynamic\nvariables, which enhances computational efficiency and leads to diagonal mass\nmatrices in the momentum and energy equations. Finally, we present a smooth\nnumerical example that validate the accuracy of the proposed high-order\nLagrangian hydrodynamics framework.", "published": "2025-09-07 06:21:02", "link": "http://arxiv.org/abs/2509.05944v2", "categories": ["math.NA", "cs.NA", "49N45, 65N21"], "primary_category": "math.NA"}
{"title": "A Minimalist Bayesian Framework for Stochastic Optimization", "abstract": "The Bayesian paradigm offers principled tools for sequential decision-making\nunder uncertainty, but its reliance on a probabilistic model for all parameters\ncan hinder the incorporation of complex structural constraints. We introduce a\nminimalist Bayesian framework that places a prior only on the component of\ninterest, such as the location of the optimum. Nuisance parameters are\neliminated via profile likelihood, which naturally handles constraints. As a\ndirect instantiation, we develop a MINimalist Thompson Sampling (MINTS)\nalgorithm. Our framework accommodates structured problems, including\ncontinuum-armed Lipschitz bandits and dynamic pricing. It also provides a\nprobabilistic lens on classical convex optimization algorithms such as the\ncenter of gravity and ellipsoid methods. We further analyze MINTS for\nmulti-armed bandits and establish near-optimal regret guarantees.", "published": "2025-09-07 19:31:12", "link": "http://arxiv.org/abs/2509.07030v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Toric geometry of ReLU neural networks", "abstract": "Given a continuous finitely piecewise linear function $f:\\mathbb{R}^{n_0} \\to\n\\mathbb{R}$ and a fixed architecture $(n_0,\\ldots,n_k;1)$ of feedforward ReLU\nneural networks, the exact function realization problem is to determine when\nsome network with the given architecture realizes $f$. To develop a systematic\nway to answer these questions, we establish a connection between toric geometry\nand ReLU neural networks. This approach enables us to utilize numerous\nstructures and tools from algebraic geometry to study ReLU neural networks.\nStarting with an unbiased ReLU neural network with rational weights, we define\nthe ReLU fan, the ReLU toric variety, and the ReLU Cartier divisor associated\nwith the network. This work also reveals the connection between the tropical\ngeometry and the toric geometry of ReLU neural networks. As an application of\nthe toric geometry framework, we prove a necessary and sufficient criterion of\nfunctions realizable by unbiased shallow ReLU neural networks by computing\nintersection numbers of the ReLU Cartier divisor and torus-invariant curves.", "published": "2025-09-07 02:08:23", "link": "http://arxiv.org/abs/2509.05894v1", "categories": ["math.AG", "cs.LG", "stat.ML"], "primary_category": "math.AG"}
{"title": "The Efficiency Frontier: Classical Shadows versus Quantum Footage", "abstract": "Interfacing quantum and classical processors is an important subroutine in\nfull-stack quantum algorithms. The so-called \"classical shadow\" method\nefficiently extracts essential classical information from quantum states,\nenabling the prediction of many properties of a quantum system from only a few\nmeasurements. However, for a small number of highly non-local observables, or\nwhen classical post-processing power is limited, the classical shadow method is\nnot always the most efficient choice. Here, we address this issue\nquantitatively by performing a full-stack resource analysis that compares\nclassical shadows with \"quantum footage,\" which refers to direct quantum\nmeasurement. Under certain assumptions, our analysis illustrates a boundary of\ndownload efficiency between classical shadows and quantum footage. For\nobservables expressed as linear combinations of Pauli matrices, the classical\nshadow method outperforms direct measurement when the number of observables is\nlarge and the Pauli weight is small. For observables in the form of large\nHermitian sparse matrices, the classical shadow method shows an advantage when\nthe number of observables, the sparsity of the matrix, and the number of qubits\nfall within a certain range. The key parameters influencing this behavior\ninclude the number of qubits $n$, observables $M$, sparsity $k$, Pauli weight\n$w$, accuracy requirement $\\epsilon$, and failure tolerance $\\delta$. We also\ncompare the resource consumption of the two methods on different types of\nquantum computers and identify break-even points where the classical shadow\nmethod becomes more efficient, which vary depending on the hardware. This paper\nopens a new avenue for quantitatively designing optimal strategies for hybrid\nquantum-classical tomography and provides practical insights for selecting the\nmost suitable quantum measurement approach in real-world applications.", "published": "2025-09-07 21:46:56", "link": "http://arxiv.org/abs/2509.06218v2", "categories": ["quant-ph", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "quant-ph"}
{"title": "Uncertainty Quantification in Probabilistic Machine Learning Models: Theory, Methods, and Insights", "abstract": "Uncertainty Quantification (UQ) is essential in probabilistic machine\nlearning models, particularly for assessing the reliability of predictions. In\nthis paper, we present a systematic framework for estimating both epistemic and\naleatoric uncertainty in probabilistic models. We focus on Gaussian Process\nLatent Variable Models and employ scalable Random Fourier Features-based\nGaussian Processes to approximate predictive distributions efficiently. We\nderive a theoretical formulation for UQ, propose a Monte Carlo sampling-based\nestimation method, and conduct experiments to evaluate the impact of\nuncertainty estimation. Our results provide insights into the sources of\npredictive uncertainty and illustrate the effectiveness of our approach in\nquantifying the confidence in the predictions.", "published": "2025-09-07 00:38:33", "link": "http://arxiv.org/abs/2509.05877v2", "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Uncertainty Estimation using Variance-Gated Distributions", "abstract": "Evaluation of per-sample uncertainty quantification from neural networks is\nessential for decision-making involving high-risk applications. A common\napproach is to use the predictive distribution from Bayesian or approximation\nmodels and decompose the corresponding predictive uncertainty into epistemic\n(model-related) and aleatoric (data-related) components. However, additive\ndecomposition has recently been questioned. In this work, we propose an\nintuitive framework for uncertainty estimation and decomposition based on the\nsignal-to-noise ratio of class probability distributions across different model\npredictions. We introduce a variance-gated measure that scales predictions by a\nconfidence factor derived from ensembles. We use this measure to discuss the\nexistence of a collapse in the diversity of committee machines.", "published": "2025-09-07 16:19:21", "link": "http://arxiv.org/abs/2509.08846v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity", "abstract": "Recent years have seen many industrial implementations and much scholastic\nresearch, i.e., prototypes and theoretical frameworks, in Decentralized\nIdentity Management Systems (DIDMS). It is safe to say that Attestation-Based\nAttribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the\nsame level of attention in the literature as general Attribute-Based DIDMs\n(ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of\ndecentralization, i.e., DIDM, is to improve upon the security and\nprivacy-related issues of centralized Identity Management Systems (IDM) and\nAttribute-Based IDMs (ABIDM). And blockchain is the framework used for\ndecentralization in all these schemes. Many DIDMs - even ABDIDMs - have been\ndefined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin.\nHowever, despite the characteristics of Ripple that makes it appealing for an\nABIDM, there is a lack of research to develop an Identity Management System\n(IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM\non Ripple.", "published": "2025-09-07 14:12:25", "link": "http://arxiv.org/abs/2509.10545v1", "categories": ["cs.CR", "cs.IR", "E.1, H.3.3, E.3", "E.1; H.3.3; E.3"], "primary_category": "cs.CR"}
