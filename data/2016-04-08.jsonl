{"title": "Transfer Learning for Low-Resource Neural Machine Translation", "abstract": "The encoder-decoder framework for neural machine translation (NMT) has been\nshown effective in large data scenarios, but is much less effective for\nlow-resource languages. We present a transfer learning method that\nsignificantly improves Bleu scores across a range of low-resource languages.\nOur key idea is to first train a high-resource language pair (the parent\nmodel), then transfer some of the learned parameters to the low-resource pair\n(the child model) to initialize and constrain training. Using our transfer\nlearning method we improve baseline NMT models by an average of 5.6 Bleu on\nfour low-resource language pairs. Ensembling and unknown word replacement add\nanother 2 Bleu which brings the NMT performance on low-resource machine\ntranslation close to a strong syntax based machine translation (SBMT) system,\nexceeding its performance on one language pair. Additionally, using the\ntransfer learning model for re-scoring, we can improve the SBMT system by an\naverage of 1.3 Bleu, improving the state-of-the-art on low-resource machine\ntranslation.", "published": "2016-04-08 00:16:35", "link": "http://arxiv.org/abs/1604.02201v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
