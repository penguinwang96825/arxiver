{"title": "Benchmarking zero-shot and few-shot approaches for tokenization,\n  tagging, and dependency parsing of Tagalog text", "abstract": "The grammatical analysis of texts in any written language typically involves\na number of basic processing tasks, such as tokenization, morphological\ntagging, and dependency parsing. State-of-the-art systems can achieve high\naccuracy on these tasks for languages with large datasets, but yield poor\nresults for languages which have little to no annotated data. To address this\nissue for the Tagalog language, we investigate the use of alternative language\nresources for creating task-specific models in the absence of\ndependency-annotated Tagalog data. We also explore the use of word embeddings\nand data augmentation to improve performance when only a small amount of\nannotated Tagalog data is available. We show that these zero-shot and few-shot\napproaches yield substantial improvements on grammatical analysis of both\nin-domain and out-of-domain Tagalog text compared to state-of-the-art\nsupervised baselines.", "published": "2022-08-03 02:20:10", "link": "http://arxiv.org/abs/2208.01814v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effidit: Your AI Writing Assistant", "abstract": "In this technical report, we introduce Effidit (Efficient and Intelligent\nEditing), a digital writing assistant that facilitates users to write\nhigher-quality text more efficiently by using artificial intelligence (AI)\ntechnologies. Previous writing assistants typically provide the function of\nerror checking (to detect and correct spelling and grammatical errors) and\nlimited text-rewriting functionality. With the emergence of large-scale neural\nlanguage models, some systems support automatically completing a sentence or a\nparagraph. In Effidit, we significantly expand the capacities of a writing\nassistant by providing functions in five categories: text completion, error\nchecking, text polishing, keywords to sentences (K2S), and cloud input methods\n(cloud IME). In the text completion category, Effidit supports generation-based\nsentence completion, retrieval-based sentence completion, and phrase\ncompletion. In contrast, many other writing assistants so far only provide one\nor two of the three functions. For text polishing, we have three functions:\n(context-aware) phrase polishing, sentence paraphrasing, and sentence\nexpansion, whereas many other writing assistants often support one or two\nfunctions in this category. The main contents of this report include major\nmodules of Effidit, methods for implementing these modules, and evaluation\nresults of some key methods.", "published": "2022-08-03 02:24:45", "link": "http://arxiv.org/abs/2208.01815v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language", "abstract": "We present a new pre-trained language model (PLM) for Rabbinic Hebrew, termed\nBerel (BERT Embeddings for Rabbinic-Encoded Language). Whilst other PLMs exist\nfor processing Hebrew texts (e.g., HeBERT, AlephBert), they are all trained on\nmodern Hebrew texts, which diverges substantially from Rabbinic Hebrew in terms\nof its lexicographical, morphological, syntactic and orthographic norms. We\ndemonstrate the superiority of Berel on Rabbinic texts via a challenge set of\nHebrew homographs. We release the new model and homograph challenge set for\nunrestricted use.", "published": "2022-08-03 06:59:04", "link": "http://arxiv.org/abs/2208.01875v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Knowledge Transfer for Clinical Phenotyping", "abstract": "Clinical phenotyping enables the automatic extraction of clinical conditions\nfrom patient records, which can be beneficial to doctors and clinics worldwide.\nHowever, current state-of-the-art models are mostly applicable to clinical\nnotes written in English. We therefore investigate cross-lingual knowledge\ntransfer strategies to execute this task for clinics that do not use the\nEnglish language and have a small amount of in-domain data available. We\nevaluate these strategies for a Greek and a Spanish clinic leveraging clinical\nnotes from different clinical domains such as cardiology, oncology and the ICU.\nOur results reveal two strategies that outperform the state-of-the-art:\nTranslation-based methods in combination with domain-specific encoders and\ncross-lingual encoders plus adapters. We find that these strategies perform\nespecially well for classifying rare phenotypes and we advise on which method\nto prefer in which situation. Our results show that using multilingual data\noverall improves clinical phenotyping models and can compensate for data\nsparseness.", "published": "2022-08-03 08:33:21", "link": "http://arxiv.org/abs/2208.01912v1", "categories": ["cs.CL", "I.2.7; J.3"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Approaches for the Detection of Adverse Drug Reactions in\n  German from a Patient's Perspective", "abstract": "In this work, we present the first corpus for German Adverse Drug Reaction\n(ADR) detection in patient-generated content. The data consists of 4,169 binary\nannotated documents from a German patient forum, where users talk about health\nissues and get advice from medical doctors. As is common in social media data\nin this domain, the class labels of the corpus are very imbalanced. This and a\nhigh topic imbalance make it a very challenging dataset, since often, the same\nsymptom can have several causes and is not always related to a medication\nintake. We aim to encourage further multi-lingual efforts in the domain of ADR\ndetection and provide preliminary experiments for binary classification using\ndifferent methods of zero- and few-shot learning based on a multi-lingual\nmodel. When fine-tuning XLM-RoBERTa first on English patient forum data and\nthen on the new German data, we achieve an F1-score of 37.52 for the positive\nclass. We make the dataset and models publicly available for the community.", "published": "2022-08-03 12:52:01", "link": "http://arxiv.org/abs/2208.02031v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Large scale analysis of gender bias and sexism in song lyrics", "abstract": "We employ Natural Language Processing techniques to analyse 377808 English\nsong lyrics from the \"Two Million Song Database\" corpus, focusing on the\nexpression of sexism across five decades (1960-2010) and the measurement of\ngender biases. Using a sexism classifier, we identify sexist lyrics at a larger\nscale than previous studies using small samples of manually annotated popular\nsongs. Furthermore, we reveal gender biases by measuring associations in word\nembeddings learned on song lyrics. We find sexist content to increase across\ntime, especially from male artists and for popular songs appearing in Billboard\ncharts. Songs are also shown to contain different language biases depending on\nthe gender of the performer, with male solo artist songs containing more and\nstronger biases. This is the first large scale analysis of this type, giving\ninsights into language usage in such an influential part of popular culture.", "published": "2022-08-03 13:18:42", "link": "http://arxiv.org/abs/2208.02052v5", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Efficient Fine-Tuning of Compressed Language Models with Learners", "abstract": "Fine-tuning BERT-based models is resource-intensive in memory, computation,\nand time. While many prior works aim to improve inference efficiency via\ncompression techniques, e.g., pruning, these works do not explicitly address\nthe computational challenges of training to downstream tasks. We introduce\nLearner modules and priming, novel methods for fine-tuning that exploit the\noverparameterization of pre-trained language models to gain benefits in\nconvergence speed and resource utilization. Learner modules navigate the double\nbind of 1) training efficiently by fine-tuning a subset of parameters, and 2)\ntraining effectively by ensuring quick convergence and high metric scores. Our\nresults on DistilBERT demonstrate that learners perform on par with or surpass\nthe baselines. Learners train 7x fewer parameters than state-of-the-art methods\non GLUE. On CoLA, learners fine-tune 20% faster, and have significantly lower\nresource utilization.", "published": "2022-08-03 13:42:30", "link": "http://arxiv.org/abs/2208.02070v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SpanDrop: Simple and Effective Counterfactual Learning for Long\n  Sequences", "abstract": "Distilling supervision signal from a long sequence to make predictions is a\nchallenging task in machine learning, especially when not all elements in the\ninput sequence contribute equally to the desired output. In this paper, we\npropose SpanDrop, a simple and effective data augmentation technique that helps\nmodels identify the true supervision signal in a long sequence with very few\nexamples. By directly manipulating the input sequence, SpanDrop randomly\nablates parts of the sequence at a time and ask the model to perform the same\ntask to emulate counterfactual learning and achieve input attribution. Based on\ntheoretical analysis of its properties, we also propose a variant of SpanDrop\nbased on the beta-Bernoulli distribution, which yields diverse augmented\nsequences while providing a learning objective that is more consistent with the\noriginal dataset. We demonstrate the effectiveness of SpanDrop on a set of\ncarefully designed toy tasks, as well as various natural language processing\ntasks that require reasoning over long sequences to arrive at the correct\nanswer, and show that it helps models improve performance both when data is\nscarce and abundant.", "published": "2022-08-03 15:58:37", "link": "http://arxiv.org/abs/2208.02169v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "VQ-T: RNN Transducers using Vector-Quantized Prediction Network States", "abstract": "Beam search, which is the dominant ASR decoding algorithm for end-to-end\nmodels, generates tree-structured hypotheses. However, recent studies have\nshown that decoding with hypothesis merging can achieve a more efficient search\nwith comparable or better performance. But, the full context in recurrent\nnetworks is not compatible with hypothesis merging. We propose to use\nvector-quantized long short-term memory units (VQ-LSTM) in the prediction\nnetwork of RNN transducers. By training the discrete representation jointly\nwith the ASR network, hypotheses can be actively merged for lattice generation.\nOur experiments on the Switchboard corpus show that the proposed VQ RNN\ntransducers improve ASR performance over transducers with regular prediction\nnetworks while also producing denser lattices with a very low oracle word error\nrate (WER) for the same beam size. Additional language model rescoring\nexperiments also demonstrate the effectiveness of the proposed lattice\ngeneration scheme.", "published": "2022-08-03 02:45:52", "link": "http://arxiv.org/abs/2208.01818v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evaluating and improving social awareness of energy communities through\n  semantic network analysis of online news", "abstract": "The implementation of energy communities represents a cross-disciplinary\nphenomenon that has the potential to support the energy transition while\nfostering citizens' participation throughout the energy system and their\nexploitation of renewables. An important role is played by online information\nsources in engaging people in this process and increasing their awareness of\nassociated benefits. In this view, this work analyses online news data on\nenergy communities to understand people's awareness and the media importance of\nthis topic. We use the Semantic Brand Score (SBS) indicator as an innovative\nmeasure of semantic importance, combining social network analysis and text\nmining methods. Results show different importance trends for energy communities\nand other energy and society-related topics, also allowing the identification\nof their connections. Our approach gives evidence to information gaps and\npossible actions that could be taken to promote a low-carbon energy transition.", "published": "2022-08-03 07:43:31", "link": "http://arxiv.org/abs/2208.01892v1", "categories": ["cs.SI", "cs.CL", "physics.soc-ph", "I.2.7; H.0; J.4"], "primary_category": "cs.SI"}
{"title": "Masked Vision and Language Modeling for Multi-modal Representation\n  Learning", "abstract": "In this paper, we study how to use masked signal modeling in vision and\nlanguage (V+L) representation learning. Instead of developing masked language\nmodeling (MLM) and masked image modeling (MIM) independently, we propose to\nbuild joint masked vision and language modeling, where the masked signal of one\nmodality is reconstructed with the help from another modality. This is\nmotivated by the nature of image-text paired data that both of the image and\nthe text convey almost the same information but in different formats. The\nmasked signal reconstruction of one modality conditioned on another modality\ncan also implicitly learn cross-modal alignment between language tokens and\nimage patches. Our experiments on various V+L tasks show that the proposed\nmethod, along with common V+L alignment losses, achieves state-of-the-art\nperformance in the regime of millions of pre-training data. Also, we\noutperforms the other competitors by a significant margin in limited data\nscenarios.", "published": "2022-08-03 15:11:01", "link": "http://arxiv.org/abs/2208.02131v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model\n  for Financial Reports", "abstract": "We present KPI-BERT, a system which employs novel methods of named entity\nrecognition (NER) and relation extraction (RE) to extract and link key\nperformance indicators (KPIs), e.g. \"revenue\" or \"interest expenses\", of\ncompanies from real-world German financial documents. Specifically, we\nintroduce an end-to-end trainable architecture that is based on Bidirectional\nEncoder Representations from Transformers (BERT) combining a recurrent neural\nnetwork (RNN) with conditional label masking to sequentially tag entities\nbefore it classifies their relations. Our model also introduces a learnable\nRNN-based pooling mechanism and incorporates domain expert knowledge by\nexplicitly filtering impossible relations. We achieve a substantially higher\nprediction performance on a new practical dataset of German financial reports,\noutperforming several strong baselines including a competing state-of-the-art\nspan-based entity tagging approach.", "published": "2022-08-03 15:21:28", "link": "http://arxiv.org/abs/2208.02140v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Study of Modeling Rising Intonation in Cantonese Neural Speech\n  Synthesis", "abstract": "In human speech, the attitude of a speaker cannot be fully expressed only by\nthe textual content. It has to come along with the intonation. Declarative\nquestions are commonly used in daily Cantonese conversations, and they are\nusually uttered with rising intonation. Vanilla neural text-to-speech (TTS)\nsystems are not capable of synthesizing rising intonation for these sentences\ndue to the loss of semantic information. Though it has become more common to\ncomplement the systems with extra language models, their performance in\nmodeling rising intonation is not well studied. In this paper, we propose to\ncomplement the Cantonese TTS model with a BERT-based statement/question\nclassifier. We design different training strategies and compare their\nperformance. We conduct our experiments on a Cantonese corpus named CanTTS.\nEmpirical results show that the separate training approach obtains the best\ngeneralization performance and feasibility.", "published": "2022-08-03 16:21:08", "link": "http://arxiv.org/abs/2208.02189v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Word-Level Fine-Grained Story Visualization", "abstract": "Story visualization aims to generate a sequence of images to narrate each\nsentence in a multi-sentence story with a global consistency across dynamic\nscenes and characters. Current works still struggle with output images' quality\nand consistency, and rely on additional semantic information or auxiliary\ncaptioning networks. To address these challenges, we first introduce a new\nsentence representation, which incorporates word information from all story\nsentences to mitigate the inconsistency problem. Then, we propose a new\ndiscriminator with fusion features and further extend the spatial attention to\nimprove image quality and story consistency. Extensive experiments on different\ndatasets and human evaluation demonstrate the superior performance of our\napproach, compared to state-of-the-art methods, neither using segmentation\nmasks nor auxiliary captioning networks.", "published": "2022-08-03 21:01:47", "link": "http://arxiv.org/abs/2208.02341v3", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Adversarial Attacks on ASR Systems: An Overview", "abstract": "With the development of hardware and algorithms, ASR(Automatic Speech\nRecognition) systems evolve a lot. As The models get simpler, the difficulty of\ndevelopment and deployment become easier, ASR systems are getting closer to our\nlife. On the one hand, we often use APPs or APIs of ASR to generate subtitles\nand record meetings. On the other hand, smart speaker and self-driving car rely\non ASR systems to control AIoT devices. In past few years, there are a lot of\nworks on adversarial examples attacks against ASR systems. By adding a small\nperturbation to the waveforms, the recognition results make a big difference.\nIn this paper, we describe the development of ASR system, different assumptions\nof attacks, and how to evaluate these attacks. Next, we introduce the current\nworks on adversarial examples attacks from two attack assumptions: white-box\nattack and black-box attack. Different from other surveys, we pay more\nattention to which layer they perturb waveforms in ASR system, the relationship\nbetween these attacks, and their implementation methods. We focus on the effect\nof their works.", "published": "2022-08-03 06:46:42", "link": "http://arxiv.org/abs/2208.02250v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.CR", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Self-Supervised Speaker Verification Using Dynamic Loss-Gate and Label\n  Correction", "abstract": "For self-supervised speaker verification, the quality of pseudo labels\ndecides the upper bound of the system due to the massive unreliable labels. In\nthis work, we propose dynamic loss-gate and label correction (DLG-LC) to\nalleviate the performance degradation caused by unreliable estimated labels. In\nDLG, we adopt Gaussian Mixture Model (GMM) to dynamically model the loss\ndistribution and use the estimated GMM to distinguish the reliable and\nunreliable labels automatically. Besides, to better utilize the unreliable data\ninstead of dropping them directly, we correct the unreliable label with model\npredictions. Moreover, we apply the negative-pairs-free DINO framework in our\nexperiments for further improvement. Compared to the best-known speaker\nverification system with self-supervised learning, our proposed DLG-LC\nconverges faster and achieves 11.45%, 18.35% and 15.16% relative improvement on\nVox-O, Vox-E and Vox-H trials of Voxceleb1 evaluation dataset.", "published": "2022-08-03 09:10:13", "link": "http://arxiv.org/abs/2208.01928v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The SJTU System for Short-duration Speaker Verification Challenge 2021", "abstract": "This paper presents the SJTU system for both text-dependent and\ntext-independent tasks in short-duration speaker verification (SdSV) challenge\n2021. In this challenge, we explored different strong embedding extractors to\nextract robust speaker embedding. For text-independent task, language-dependent\nadaptive snorm is explored to improve the system performance under the\ncross-lingual verification condition. For text-dependent task, we mainly focus\non the in-domain fine-tuning strategies based on the model pre-trained on\nlarge-scale out-of-domain data. In order to improve the distinction between\ndifferent speakers uttering the same phrase, we proposed several novel\nphrase-aware fine-tuning strategies and phrase-aware neural PLDA. With such\nstrategies, the system performance is further improved. Finally, we fused the\nscores of different systems, and our fusion systems achieved 0.0473 in Task1\n(rank 3) and 0.0581 in Task2 (rank 8) on the primary evaluation metric.", "published": "2022-08-03 09:19:22", "link": "http://arxiv.org/abs/2208.01933v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Zero-Shot Style Transfer for Gesture Animation driven by Text and Speech\n  using Adversarial Disentanglement of Multimodal Style Encoding", "abstract": "Modeling virtual agents with behavior style is one factor for personalizing\nhuman agent interaction. We propose an efficient yet effective machine learning\napproach to synthesize gestures driven by prosodic features and text in the\nstyle of different speakers including those unseen during training. Our model\nperforms zero shot multimodal style transfer driven by multimodal data from the\nPATS database containing videos of various speakers. We view style as being\npervasive while speaking, it colors the communicative behaviors expressivity\nwhile speech content is carried by multimodal signals and text. This\ndisentanglement scheme of content and style allows us to directly infer the\nstyle embedding even of speaker whose data are not part of the training phase,\nwithout requiring any further training or fine tuning. The first goal of our\nmodel is to generate the gestures of a source speaker based on the content of\ntwo audio and text modalities. The second goal is to condition the source\nspeaker predicted gestures on the multimodal behavior style embedding of a\ntarget speaker. The third goal is to allow zero shot style transfer of speakers\nunseen during training without retraining the model. Our system consists of:\n(1) a speaker style encoder network that learns to generate a fixed dimensional\nspeaker embedding style from a target speaker multimodal data and (2) a\nsequence to sequence synthesis network that synthesizes gestures based on the\ncontent of the input modalities of a source speaker and conditioned on the\nspeaker style embedding. We evaluate that our model can synthesize gestures of\na source speaker and transfer the knowledge of target speaker style variability\nto the gesture generation task in a zero shot setup. We convert the 2D gestures\nto 3D poses and produce 3D animations. We conduct objective and subjective\nevaluations to validate our approach and compare it with a baseline.", "published": "2022-08-03 08:49:55", "link": "http://arxiv.org/abs/2208.01917v1", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Audio-visual scene classification via contrastive event-object alignment\n  and semantic-based fusion", "abstract": "Previous works on scene classification are mainly based on audio or visual\nsignals, while humans perceive the environmental scenes through multiple\nsenses. Recent studies on audio-visual scene classification separately\nfine-tune the largescale audio and image pre-trained models on the target\ndataset, then either fuse the intermediate representations of the audio model\nand the visual model, or fuse the coarse-grained decision of both models at the\nclip level. Such methods ignore the detailed audio events and visual objects in\naudio-visual scenes (AVS), while humans often identify different scenes through\naudio events and visual objects within and the congruence between them. To\nexploit the fine-grained information of audio events and visual objects in AVS,\nand coordinate the implicit relationship between audio events and visual\nobjects, this paper proposes a multibranch model equipped with contrastive\nevent-object alignment (CEOA) and semantic-based fusion (SF) for AVSC. CEOA\naims to align the learned embeddings of audio events and visual objects by\ncomparing the difference between audio-visual event-object pairs. Then, visual\nobjects associated with certain audio events and vice versa are accentuated by\ncross-attention and undergo SF for semantic-level fusion. Experiments show\nthat: 1) the proposed AVSC model equipped with CEOA and SF outperforms the\nresults of audio-only and visual-only models, i.e., the audio-visual results\nare better than the results from a single modality. 2) CEOA aligns the\nembeddings of audio events and related visual objects on a fine-grained level,\nand the SF effectively integrates both; 3) Compared with other large-scale\nintegrated systems, the proposed model shows competitive performance, even\nwithout using additional datasets and data augmentation tricks.", "published": "2022-08-03 14:11:46", "link": "http://arxiv.org/abs/2208.02086v1", "categories": ["cs.SD", "cs.MM", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "Estimating Visual Information From Audio Through Manifold Learning", "abstract": "We propose a new framework for extracting visual information about a scene\nonly using audio signals. Audio-based methods can overcome some of the\nlimitations of vision-based methods i.e., they do not require \"line-of-sight\",\nare robust to occlusions and changes in illumination, and can function as a\nbackup in case vision/lidar sensors fail. Therefore, audio-based methods can be\nuseful even for applications in which only visual information is of interest\nOur framework is based on Manifold Learning and consists of two steps. First,\nwe train a Vector-Quantized Variational Auto-Encoder to learn the data manifold\nof the particular visual modality we are interested in. Second, we train an\nAudio Transformation network to map multi-channel audio signals to the latent\nrepresentation of the corresponding visual sample. We show that our method is\nable to produce meaningful images from audio using a publicly available\naudio/visual dataset. In particular, we consider the prediction of the\nfollowing visual modalities from audio: depth and semantic segmentation. We\nhope the findings of our work can facilitate further research in visual\ninformation extraction from audio. Code is available at:\nhttps://github.com/ubc-vision/audio_manifold.", "published": "2022-08-03 20:47:11", "link": "http://arxiv.org/abs/2208.02337v2", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
