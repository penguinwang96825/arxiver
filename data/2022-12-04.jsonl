{"title": "KPT: Keyword-guided Pre-training for Grounded Dialog Generation", "abstract": "Incorporating external knowledge into the response generation process is\nessential to building more helpful and reliable dialog agents. However,\ncollecting knowledge-grounded conversations is often costly, calling for a\nbetter pre-trained model for grounded dialog generation that generalizes well\nw.r.t. different types of knowledge. In this work, we propose KPT\n(Keyword-guided Pre-Training), a novel self-supervised pre-training method for\ngrounded dialog generation without relying on extra knowledge annotation.\nSpecifically, we use a pre-trained language model to extract the most uncertain\ntokens in the dialog as keywords. With these keywords, we construct two kinds\nof knowledge and pre-train a knowledge-grounded response generation model,\naiming at handling two different scenarios: (1) the knowledge should be\nfaithfully grounded; (2) it can be selectively used. For the former, the\ngrounding knowledge consists of keywords extracted from the response. For the\nlatter, the grounding knowledge is additionally augmented with keywords\nextracted from other utterances in the same dialog. Since the knowledge is\nextracted from the dialog itself, KPT can be easily performed on a large volume\nand variety of dialogue data. We considered three data sources (open-domain,\ntask-oriented, conversational QA) with a total of 2.5M dialogues. We conduct\nextensive experiments on various few-shot knowledge-grounded generation tasks,\nincluding grounding on dialog acts, knowledge graphs, persona descriptions, and\nWikipedia passages. Our comprehensive experiments and analyses demonstrate that\nKPT consistently outperforms state-of-the-art methods on these tasks with\ndiverse grounding knowledge.", "published": "2022-12-04 04:05:01", "link": "http://arxiv.org/abs/2212.01739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MiLMo:Minority Multilingual Pre-trained Language Model", "abstract": "Pre-trained language models are trained on large-scale unsupervised data, and\nthey can fine-turn the model only on small-scale labeled datasets, and achieve\ngood results. Multilingual pre-trained language models can be trained on\nmultiple languages, and the model can understand multiple languages at the same\ntime. At present, the search on pre-trained models mainly focuses on rich\nresources, while there is relatively little research on low-resource languages\nsuch as minority languages, and the public multilingual pre-trained language\nmodel can not work well for minority languages. Therefore, this paper\nconstructs a multilingual pre-trained model named MiLMo that performs better on\nminority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and\nKorean. To solve the problem of scarcity of datasets on minority languages and\nverify the effectiveness of the MiLMo model, this paper constructs a minority\nmultilingual text classification dataset named MiTC, and trains a word2vec\nmodel for each language. By comparing the word2vec model and the pre-trained\nmodel in the text classification task, this paper provides an optimal scheme\nfor the downstream task research of minority languages. The final experimental\nresults show that the performance of the pre-trained model is better than that\nof the word2vec model, and it has achieved the best results in minority\nmultilingual text classification. The multilingual pre-trained model MiLMo,\nmultilingual word2vec model and multilingual text classification dataset MiTC\nare published on http://milmo.cmli-nlp.com/.", "published": "2022-12-04 09:28:17", "link": "http://arxiv.org/abs/2212.01779v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Constructing Highly Inductive Contexts for Dialogue Safety through\n  Controllable Reverse Generation", "abstract": "Large pretrained language models can easily produce toxic or biased content,\nwhich is prohibitive for practical use. In order to detect such toxic\ngenerations, existing methods rely on templates, real-world data extraction,\ncrowdsourcing workers, or automatic generation to construct adversarial\ncontexts that are likely to induce toxic generations. However, what type of\ncontext is more likely to induce unsafe responses is still under-explored. In\nthis paper, we identify that context toxicity and context category (e.g.,\n\\textit{profanity}, \\textit{insult}, \\textit{drugs}, etc.) are two important\nfactors to cause safety issues in response generation. Hence, we propose a\nmethod called \\emph{reverse generation} to construct adversarial contexts\nconditioned on a given response, with the flexibility to control category,\ntoxicity level, and inductivity of the generated contexts. Via reverse\ngeneration, we augment the existing BAD dataset and construct a new dataset\nBAD+ which contains more than 120K diverse and highly inductive contexts in 12\ncategories. We test three popular pretrained dialogue models (Blender,\nDialoGPT, and Plato2) and find that BAD+ can largely expose their safety\nproblems. Furthermore, we show that BAD+ can greatly enhance the safety of\ngeneration and reveal the key factors of safety improvement. Our code and\ndataset is available at \\url{https://github.com/thu-coai/Reverse_Generation}.", "published": "2022-12-04 12:23:41", "link": "http://arxiv.org/abs/2212.01810v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toward Efficient Language Model Pretraining and Downstream Adaptation\n  via Self-Evolution: A Case Study on SuperGLUE", "abstract": "This technical report briefly describes our JDExplore d-team's Vega v2\nsubmission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the\nwidely used general language understanding evaluation (GLUE) benchmark,\ncontaining eight difficult language understanding tasks, including question\nanswering, natural language inference, word sense disambiguation, coreference\nresolution, and reasoning. [Method] Instead of arbitrarily increasing the size\nof a pretrained language model (PLM), our aim is to 1) fully extract knowledge\nfrom the input pretraining data given a certain parameter budget, e.g., 6B, and\n2) effectively transfer this knowledge to downstream tasks. To achieve goal 1),\nwe propose self-evolution learning for PLMs to wisely predict the informative\ntokens that should be masked, and supervise the masked language modeling (MLM)\nprocess with rectified smooth labels. For goal 2), we leverage the prompt\ntransfer technique to improve the low-resource tasks by transferring the\nknowledge from the foundation model and related downstream tasks to the target\ntask. [Results] According to our submission record (Oct. 2022), with our\noptimized pretraining and fine-tuning strategies, our 6B Vega method achieved\nnew state-of-the-art performance on 4/8 tasks, sitting atop the SuperGLUE\nleaderboard on Oct. 8, 2022, with an average score of 91.3.", "published": "2022-12-04 15:36:18", "link": "http://arxiv.org/abs/2212.01853v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Democratizing Neural Machine Translation with OPUS-MT", "abstract": "This paper presents the OPUS ecosystem with a focus on the development of\nopen machine translation models and tools, and their integration into end-user\napplications, development platforms and professional workflows. We discuss our\non-going mission of increasing language coverage and translation quality, and\nalso describe on-going work on the development of modular translation models\nand speed-optimized compact solutions for real-time translation on regular\ndesktops and small devices.", "published": "2022-12-04 22:16:27", "link": "http://arxiv.org/abs/2212.01936v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grounded Keys-to-Text Generation: Towards Factual Open-Ended Generation", "abstract": "Large pre-trained language models have recently enabled open-ended generation\nframeworks (e.g., prompt-to-text NLG) to tackle a variety of tasks going beyond\nthe traditional data-to-text generation. While this framework is more general,\nit is under-specified and often leads to a lack of controllability restricting\ntheir real-world usage. We propose a new grounded keys-to-text generation task:\nthe task is to generate a factual description about an entity given a set of\nguiding keys, and grounding passages. To address this task, we introduce a new\ndataset, called EntDeGen. Inspired by recent QA-based evaluation measures, we\npropose an automatic metric, MAFE, for factual correctness of generated\ndescriptions. Our EntDescriptor model is equipped with strong rankers to fetch\nhelpful passages and generate entity descriptions. Experimental result shows a\ngood correlation (60.14) between our proposed metric and human judgments of\nfactuality. Our rankers significantly improved the factual correctness of\ngenerated descriptions (15.95% and 34.51% relative gains in recall and\nprecision). Finally, our ablation study highlights the benefit of combining\nkeys and groundings.", "published": "2022-12-04 23:59:41", "link": "http://arxiv.org/abs/2212.01956v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An LSTM model for Twitter Sentiment Analysis", "abstract": "Sentiment analysis on social media such as Twitter provides organizations and\nindividuals an effective way to monitor public emotions towards them and their\ncompetitors. As a result, sentiment analysis has become an important and\nchallenging task. In this work, we have collected seven publicly available and\nmanually annotated twitter sentiment datasets. We create a new training and\ntesting dataset from the collected datasets. We develop an LSTM model to\nclassify sentiment of a tweet and evaluate the model with the new dataset.", "published": "2022-12-04 10:42:46", "link": "http://arxiv.org/abs/2212.01791v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Pair-Based Joint Encoding with Relational Graph Convolutional Networks\n  for Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction (ECPE) aims to extract emotion clauses and\ncorresponding cause clauses, which have recently received growing attention.\nPrevious methods sequentially encode features with a specified order. They\nfirst encode the emotion and cause features for clause extraction and then\ncombine them for pair extraction. This lead to an imbalance in inter-task\nfeature interaction where features extracted later have no direct contact with\nthe former. To address this issue, we propose a novel Pair-Based Joint Encoding\n(PBJE) network, which generates pairs and clauses features simultaneously in a\njoint feature encoding manner to model the causal relationship in clauses. PBJE\ncan balance the information flow among emotion clauses, cause clauses and\npairs. From a multi-relational perspective, we construct a heterogeneous\nundirected graph and apply the Relational Graph Convolutional Network (RGCN) to\ncapture the various relationship between clauses and the relationship between\npairs and clauses. Experimental results show that PBJE achieves\nstate-of-the-art performance on the Chinese benchmark corpus.", "published": "2022-12-04 15:24:14", "link": "http://arxiv.org/abs/2212.01844v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Applying Multilingual Models to Question Answering (QA)", "abstract": "We study the performance of monolingual and multilingual language models on\nthe task of question-answering (QA) on three diverse languages: English,\nFinnish and Japanese. We develop models for the tasks of (1) determining if a\nquestion is answerable given the context and (2) identifying the answer texts\nwithin the context using IOB tagging. Furthermore, we attempt to evaluate the\neffectiveness of a pre-trained multilingual encoder (Multilingual BERT) on\ncross-language zero-shot learning for both the answerability and IOB sequence\nclassifiers.", "published": "2022-12-04 21:58:33", "link": "http://arxiv.org/abs/2212.01933v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Automaton-Based Representations of Task Knowledge from Generative\n  Language Models", "abstract": "Automaton-based representations of task knowledge play an important role in\ncontrol and planning for sequential decision-making problems. However,\nobtaining the high-level task knowledge required to build such automata is\noften difficult. Meanwhile, large-scale generative language models (GLMs) can\nautomatically generate relevant task knowledge. However, the textual outputs\nfrom GLMs cannot be formally verified or used for sequential decision-making.\nWe propose a novel algorithm named GLM2FSA, which constructs a finite state\nautomaton (FSA) encoding high-level task knowledge from a brief\nnatural-language description of the task goal. GLM2FSA first sends queries to a\nGLM to extract task knowledge in textual form, and then it builds an FSA to\nrepresent this text-based knowledge. The proposed algorithm thus fills the gap\nbetween natural-language task descriptions and automaton-based representations,\nand the constructed FSA can be formally verified against user-defined\nspecifications. We accordingly propose a method to iteratively refine the\nqueries to the GLM based on the outcomes, e.g., counter-examples, from\nverification. We demonstrate GLM2FSA's ability to build and refine\nautomaton-based representations of everyday tasks (e.g., crossing a road), and\nalso of tasks that require highly-specialized knowledge (e.g., executing secure\nmulti-party computation).", "published": "2022-12-04 22:34:16", "link": "http://arxiv.org/abs/2212.01944v5", "categories": ["cs.FL", "cs.CL"], "primary_category": "cs.FL"}
{"title": "Utilizing Background Knowledge for Robust Reasoning over Traffic\n  Situations", "abstract": "Understanding novel situations in the traffic domain requires an intricate\ncombination of domain-specific and causal commonsense knowledge. Prior work has\nprovided sufficient perception-based modalities for traffic monitoring, in this\npaper, we focus on a complementary research aspect of Intelligent\nTransportation: traffic understanding. We scope our study to text-based methods\nand datasets given the abundant commonsense knowledge that can be extracted\nusing language models from large corpus and knowledge graphs. We adopt three\nknowledge-driven approaches for zero-shot QA over traffic situations, based on\nprior natural language inference methods, commonsense models with knowledge\ngraph self-supervision, and dense retriever-based models. We constructed two\ntext-based multiple-choice question answering sets: BDD-QA for evaluating\ncausal reasoning in the traffic domain and HDT-QA for measuring the possession\nof domain knowledge akin to human driving license tests. Among the methods,\nUnified-QA reaches the best performance on the BDD-QA dataset with the\nadaptation of multiple formats of question answers. Language models trained\nwith inference information and commonsense knowledge are also good at\npredicting the cause and effect in the traffic domain but perform badly at\nanswering human-driving QA sets. For such sets, DPR+Unified-QA performs the\nbest due to its efficient knowledge extraction.", "published": "2022-12-04 09:17:24", "link": "http://arxiv.org/abs/2212.07798v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DFEE: Interactive DataFlow Execution and Evaluation Kit", "abstract": "DataFlow has been emerging as a new paradigm for building task-oriented\nchatbots due to its expressive semantic representations of the dialogue tasks.\nDespite the availability of a large dataset SMCalFlow and a simplified syntax,\nthe development and evaluation of DataFlow-based chatbots remain challenging\ndue to the system complexity and the lack of downstream toolchains. In this\ndemonstration, we present DFEE, an interactive DataFlow Execution and\nEvaluation toolkit that supports execution, visualization and benchmarking of\nsemantic parsers given dialogue input and backend database. We demonstrate the\nsystem via a complex dialog task: event scheduling that involves temporal\nreasoning. It also supports diagnosing the parsing results via a friendly\ninterface that allows developers to examine dynamic DataFlow and the\ncorresponding execution results. To illustrate how to benchmark SoTA models, we\npropose a novel benchmark that covers more sophisticated event scheduling\nscenarios and a new metric on task success evaluation. The codes of DFEE have\nbeen released on https://github.com/amazonscience/dataflow-evaluation-toolkit.", "published": "2022-12-04 23:44:34", "link": "http://arxiv.org/abs/2212.08099v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Languages You Know Influence Those You Learn: Impact of Language\n  Characteristics on Multi-Lingual Text-to-Text Transfer", "abstract": "Multi-lingual language models (LM), such as mBERT, XLM-R, mT5, mBART, have\nbeen remarkably successful in enabling natural language tasks in low-resource\nlanguages through cross-lingual transfer from high-resource ones. In this work,\nwe try to better understand how such models, specifically mT5, transfer *any*\nlinguistic and semantic knowledge across languages, even though no explicit\ncross-lingual signals are provided during pre-training. Rather, only\nunannotated texts from each language are presented to the model separately and\nindependently of one another, and the model appears to implicitly learn\ncross-lingual connections. This raises several questions that motivate our\nstudy, such as: Are the cross-lingual connections between every language pair\nequally strong? What properties of source and target language impact the\nstrength of cross-lingual transfer? Can we quantify the impact of those\nproperties on the cross-lingual transfer?\n  In our investigation, we analyze a pre-trained mT5 to discover the attributes\nof cross-lingual connections learned by the model. Through a statistical\ninterpretation framework over 90 language pairs across three tasks, we show\nthat transfer performance can be modeled by a few linguistic and data-derived\nfeatures. These observations enable us to interpret cross-lingual understanding\nof the mT5 model. Through these observations, one can favorably choose the best\nsource language for a task, and can anticipate its training data demands. A key\nfinding of this work is that similarity of syntax, morphology and phonology are\ngood predictors of cross-lingual transfer, significantly more than just the\nlexical similarity of languages. For a given language, we are able to predict\nzero-shot performance, that increases on a logarithmic scale with the number of\nfew-shot target language data points.", "published": "2022-12-04 07:22:21", "link": "http://arxiv.org/abs/2212.01757v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Improving End-to-end Speech Translation by Leveraging Auxiliary Speech\n  and Text Data", "abstract": "We present a method for introducing a text encoder into pre-trained\nend-to-end speech translation systems. It enhances the ability of adapting one\nmodality (i.e., source-language speech) to another (i.e., source-language\ntext). Thus, the speech translation model can learn from both unlabeled and\nlabeled data, especially when the source-language text data is abundant. Beyond\nthis, we present a denoising method to build a robust text encoder that can\ndeal with both normal and noisy text data. Our system sets new\nstate-of-the-arts on the MuST-C En-De, En-Fr, and LibriSpeech En-Fr tasks.", "published": "2022-12-04 09:27:56", "link": "http://arxiv.org/abs/2212.01778v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Understanding How Model Size Affects Few-shot Instruction Prompting", "abstract": "Large Language Models are affected by the phenomena of memorizing and\nforgetting their training data. But how do these vary by model size? We work\ntowards this question by investigating how the model size affects the model's\nability to discriminate a word's meaning in a given context. We introduce a\ndataset called DeltaWords, which evaluates a model's ability to follow\ninstructions to select a sentence which replaces the target word with its\nantonym. We show a weak inverse scaling trend, where task accuracy degrades as\nmodel size increase, under extremely few-shot prompting regimes. We show that\nincreasing the number of examples tend to disproportionately benefit larger\nmodels than smaller models.", "published": "2022-12-04 19:59:52", "link": "http://arxiv.org/abs/2212.01907v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Cross-lingual Similarity of Multilingual Representations Revisited", "abstract": "Related works used indexes like CKA and variants of CCA to measure the\nsimilarity of cross-lingual representations in multilingual language models. In\nthis paper, we argue that assumptions of CKA/CCA align poorly with one of the\nmotivating goals of cross-lingual learning analysis, i.e., explaining zero-shot\ncross-lingual transfer. We highlight what valuable aspects of cross-lingual\nsimilarity these indexes fail to capture and provide a motivating case study\n\\textit{demonstrating the problem empirically}. Then, we introduce\n\\textit{Average Neuron-Wise Correlation (ANC)} as a straightforward alternative\nthat is exempt from the difficulties of CKA/CCA and is good specifically in a\ncross-lingual context. Finally, we use ANC to construct evidence that the\npreviously introduced ``first align, then predict'' pattern takes place not\nonly in masked language models (MLMs) but also in multilingual models with\n\\textit{causal language modeling} objectives (CLMs). Moreover, we show that the\npattern extends to the \\textit{scaled versions} of the MLMs and CLMs (up to 85x\noriginal mBERT).\\footnote{Our code is publicly available at\n\\url{https://github.com/TartuNLP/xsim}}", "published": "2022-12-04 21:02:07", "link": "http://arxiv.org/abs/2212.01924v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Persona-Based Conversational AI: State of the Art and Challenges", "abstract": "Conversational AI has become an increasingly prominent and practical\napplication of machine learning. However, existing conversational AI techniques\nstill suffer from various limitations. One such limitation is a lack of\nwell-developed methods for incorporating auxiliary information that could help\na model understand conversational context better. In this paper, we explore how\npersona-based information could help improve the quality of response generation\nin conversations. First, we provide a literature review focusing on the current\nstate-of-the-art methods that utilize persona information. We evaluate two\nstrong baseline methods, the Ranking Profile Memory Network and the\nPoly-Encoder, on the NeurIPS ConvAI2 benchmark dataset. Our analysis elucidates\nthe importance of incorporating persona information into conversational\nsystems. Additionally, our study highlights several limitations with current\nstate-of-the-art methods and outlines challenges and future research directions\nfor advancing personalized conversational AI technology.", "published": "2022-12-04 18:16:57", "link": "http://arxiv.org/abs/2212.03699v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generative Models for Improved Naturalness, Intelligibility, and Voicing\n  of Whispered Speech", "abstract": "This work adapts two recent architectures of generative models and evaluates\ntheir effectiveness for the conversion of whispered speech to normal speech. We\nincorporate the normal target speech into the training criterion of\nvector-quantized variational autoencoders (VQ-VAEs) and MelGANs, thereby\nconditioning the systems to recover voiced speech from whispered inputs.\nObjective and subjective quality measures indicate that both VQ-VAEs and\nMelGANs can be modified to perform the conversion task. We find that the\nproposed approaches significantly improve the Mel cepstral distortion (MCD)\nmetric by at least 25% relative to a DiscoGAN baseline. Subjective listening\ntests suggest that the MelGAN-based system significantly improves naturalness,\nintelligibility, and voicing compared to the whispered input speech. A novel\nevaluation measure based on differences between latent speech representations\nalso indicates that our MelGAN-based approach yields improvements relative to\nthe baseline.", "published": "2022-12-04 09:06:18", "link": "http://arxiv.org/abs/2212.01775v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Tragic Talkers: A Shakespearean Sound- and Light-Field Dataset for\n  Audio-Visual Machine Learning Research", "abstract": "3D audio-visual production aims to deliver immersive and interactive\nexperiences to the consumer. Yet, faithfully reproducing real-world 3D scenes\nremains a challenging task. This is partly due to the lack of available\ndatasets enabling audio-visual research in this direction. In most of the\nexisting multi-view datasets, the accompanying audio is neglected. Similarly,\ndatasets for spatial audio research primarily offer unimodal content, and when\nvisual data is included, the quality is far from meeting the standard\nproduction needs. We present \"Tragic Talkers\", an audio-visual dataset\nconsisting of excerpts from the \"Romeo and Juliet\" drama captured with\nmicrophone arrays and multiple co-located cameras for light-field video. Tragic\nTalkers provides ideal content for object-based media (OBM) production. It is\ndesigned to cover various conventional talking scenarios, such as monologues,\ntwo-people conversations, and interactions with considerable movement and\nocclusion, yielding 30 sequences captured from a total of 22 different points\nof view and two 16-element microphone arrays. Additionally, we provide voice\nactivity labels, 2D face bounding boxes for each camera view, 2D pose detection\nkeypoints, 3D tracking data of the mouth of the actors, and dialogue\ntranscriptions. We believe the community will benefit from this dataset as it\ncan assist multidisciplinary research. Possible uses of the dataset are\ndiscussed.", "published": "2022-12-04 18:48:44", "link": "http://arxiv.org/abs/2212.01892v1", "categories": ["eess.AS", "cs.MM", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech MOS multi-task learning and rater bias correction", "abstract": "Perceptual speech quality is an important performance metric for\nteleconferencing applications. The mean opinion score (MOS) is standardized for\nthe perceptual evaluation of speech quality and is obtained by asking listeners\nto rate the quality of a speech sample. Recently, there has been increasing\nresearch interest in developing models for estimating MOS blindly. Here we\npropose a multi-task framework to include additional labels and data in\ntraining to improve the performance of a blind MOS estimation model.\nExperimental results indicate that the proposed model can be trained to jointly\nestimate MOS, reverberation time (T60), and clarity (C50) by combining two\ndisjoint data sets in training, one containing only MOS labels and the other\ncontaining only T60 and C50 labels. Furthermore, we use a semi-supervised\nframework to combine two MOS data sets in training, one containing only MOS\nlabels (per ITU-T Recommendation P.808), and the other containing separate\nscores for speech signal, background noise, and overall quality (per ITU-T\nRecommendation P.835). Finally, we present preliminary results for addressing\nindividual rater bias in the MOS labels.", "published": "2022-12-04 20:06:27", "link": "http://arxiv.org/abs/2212.01911v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Melody transcription via generative pre-training", "abstract": "Despite the central role that melody plays in music perception, it remains an\nopen challenge in music information retrieval to reliably detect the notes of\nthe melody present in an arbitrary music recording. A key challenge in melody\ntranscription is building methods which can handle broad audio containing any\nnumber of instrument ensembles and musical styles - existing strategies work\nwell for some melody instruments or styles but not all. To confront this\nchallenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a\ngenerative model of broad music audio, thereby improving performance on melody\ntranscription by $20$% relative to conventional spectrogram features. Another\nobstacle in melody transcription is a lack of training data - we derive a new\ndataset containing $50$ hours of melody transcriptions from crowdsourced\nannotations of broad music. The combination of generative pre-training and a\nnew dataset for this task results in $77$% stronger performance on melody\ntranscription relative to the strongest available baseline. By pairing our new\nmelody transcription approach with solutions for beat detection, key\nestimation, and chord recognition, we build Sheet Sage, a system capable of\ntranscribing human-readable lead sheets directly from music audio.\n  Audio examples can be found at https://chrisdonahue.com/sheetsage and code at\nhttps://github.com/chrisdonahue/sheetsage .", "published": "2022-12-04 18:09:23", "link": "http://arxiv.org/abs/2212.01884v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
