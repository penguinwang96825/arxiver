{"title": "Fast calculation of entropy with Zhang's estimator", "abstract": "Entropy is a fundamental property of a repertoire. Here, we present an\nefficient algorithm to estimate the entropy of types with the help of Zhang's\nestimator. The algorithm takes advantage of the fact that the number of\ndifferent frequencies in a text is in general much smaller than the number of\ntypes. We justify the convenience of the algorithm by means of an analysis of\nthe statistical properties of texts from more than 1000 languages. Our work\nopens up various possibilities for future research.", "published": "2017-07-26 05:11:58", "link": "http://arxiv.org/abs/1707.08290v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can string kernels pass the test of time in Native Language\n  Identification?", "abstract": "We describe a machine learning approach for the 2017 shared task on Native\nLanguage Identification (NLI). The proposed approach combines several kernels\nusing multiple kernel learning. While most of our kernels are based on\ncharacter p-grams (also known as n-grams) extracted from essays or speech\ntranscripts, we also use a kernel based on i-vectors, a low-dimensional\nrepresentation of audio recordings, provided by the shared task organizers. For\nthe learning stage, we choose Kernel Discriminant Analysis (KDA) over Kernel\nRidge Regression (KRR), because the former classifier obtains better results\nthan the latter one on the development set. In our previous work, we have used\na similar machine learning approach to achieve state-of-the-art NLI results.\nThe goal of this paper is to demonstrate that our shallow and simple approach\nbased on string kernels (with minor improvements) can pass the test of time and\nreach state-of-the-art performance in the 2017 NLI shared task, despite the\nrecent advances in natural language processing. We participated in all three\ntracks, in which the competitors were allowed to use only the essays (essay\ntrack), only the speech transcripts (speech track), or both (fusion track).\nUsing only the data provided by the organizers for training our models, we have\nreached a macro F1 score of 86.95% in the closed essay track, a macro F1 score\nof 87.55% in the closed speech track, and a macro F1 score of 93.19% in the\nclosed fusion track. With these scores, our team (UnibucKernel) ranked in the\nfirst group of teams in all three tracks, while attaining the best scores in\nthe speech and the fusion tracks.", "published": "2017-07-26 10:03:40", "link": "http://arxiv.org/abs/1707.08349v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SPEECH-COCO: 600k Visually Grounded Spoken Captions Aligned to MSCOCO\n  Data Set", "abstract": "This paper presents an augmentation of MSCOCO dataset where speech is added\nto image and text. Speech captions are generated using text-to-speech (TTS)\nsynthesis resulting in 616,767 spoken captions (more than 600h) paired with\nimages. Disfluencies and speed perturbation are added to the signal in order to\nsound more natural. Each speech signal (WAV) is paired with a JSON file\ncontaining exact timecode for each word/syllable/phoneme in the spoken caption.\nSuch a corpus could be used for Language and Vision (LaVi) tasks including\nspeech input or output instead of text. Investigating multimodal learning\nschemes for unsupervised speech pattern discovery is also possible with this\ncorpus, as demonstrated by a preliminary study conducted on a subset of the\ncorpus (10h, 10k spoken captions). The dataset is available on Zenodo:\nhttps://zenodo.org/record/4282267", "published": "2017-07-26 13:40:21", "link": "http://arxiv.org/abs/1707.08435v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Men Are from Mars, Women Are from Venus: Evaluation and Modelling of\n  Verbal Associations", "abstract": "We present a quantitative analysis of human word association pairs and study\nthe types of relations presented in the associations. We put our main focus on\nthe correlation between response types and respondent characteristics such as\noccupation and gender by contrasting syntagmatic and paradigmatic associations.\nFinally, we propose a personalised distributed word association model and show\nthe importance of incorporating demographic factors into the models commonly\nused in natural language processing.", "published": "2017-07-26 14:20:12", "link": "http://arxiv.org/abs/1707.08458v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradient-based Inference for Networks with Output Constraints", "abstract": "Practitioners apply neural networks to increasingly complex problems in\nnatural language processing, such as syntactic parsing and semantic role\nlabeling that have rich output structures. Many such structured-prediction\nproblems require deterministic constraints on the output values; for example,\nin sequence-to-sequence syntactic parsing, we require that the sequential\noutputs encode valid trees. While hidden units might capture such properties,\nthe network is not always able to learn such constraints from the training data\nalone, and practitioners must then resort to post-processing. In this paper, we\npresent an inference method for neural networks that enforces deterministic\nconstraints on outputs without performing rule-based post-processing or\nexpensive discrete search. Instead, in the spirit of gradient-based training,\nwe enforce constraints with gradient-based inference (GBI): for each input at\ntest-time, we nudge continuous model weights until the network's unconstrained\ninference procedure generates an output that satisfies the constraints. We\nstudy the efficacy of GBI on three tasks with hard constraints: semantic role\nlabeling, syntactic parsing, and sequence transduction. In each case, the\nalgorithm not only satisfies constraints but improves accuracy, even when the\nunderlying network is state-of-the-art.", "published": "2017-07-26 19:00:10", "link": "http://arxiv.org/abs/1707.08608v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Temporal dynamics of semantic relations in word embeddings: an\n  application to predicting armed conflict participants", "abstract": "This paper deals with using word embedding models to trace the temporal\ndynamics of semantic relations between pairs of words. The set-up is similar to\nthe well-known analogies task, but expanded with a time dimension. To this end,\nwe apply incremental updating of the models with new training texts, including\nincremental vocabulary expansion, coupled with learned transformation matrices\nthat let us map between members of the relation. The proposed approach is\nevaluated on the task of predicting insurgent armed groups based on\ngeographical locations. The gold standard data for the time span 1994--2010 is\nextracted from the UCDP Armed Conflicts dataset. The results show that the\nmethod is feasible and outperforms the baselines, but also that important work\nstill remains to be done.", "published": "2017-07-26 23:02:11", "link": "http://arxiv.org/abs/1707.08660v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strawman: an Ensemble of Deep Bag-of-Ngrams for Sentiment Analysis", "abstract": "This paper describes a builder entry, named \"strawman\", to the sentence-level\nsentiment analysis task of the \"Build It, Break It\" shared task of the First\nWorkshop on Building Linguistically Generalizable NLP Systems. The goal of a\nbuilder is to provide an automated sentiment analyzer that would serve as a\ntarget for breakers whose goal is to find pairs of minimally-differing\nsentences that break the analyzer.", "published": "2017-07-26 17:30:57", "link": "http://arxiv.org/abs/1707.08939v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ASDA : Analyseur Syntaxique du Dialecte Alg{\u00e9}rien dans un but\n  d'analyse s{\u00e9}mantique", "abstract": "Opinion mining and sentiment analysis in social media is a research issue\nhaving a great interest in the scientific community. However, before begin this\nanalysis, we are faced with a set of problems. In particular, the problem of\nthe richness of languages and dialects within these media. To address this\nproblem, we propose in this paper an approach of construction and\nimplementation of Syntactic analyzer named ASDA. This tool represents a parser\nfor the Algerian dialect that label the terms of a given corpus. Thus, we\nconstruct a labeling table containing for each term its stem, different\nprefixes and suffixes, allowing us to determine the different grammatical parts\na sort of POS tagging. This labeling will serve us later in the semantic\nprocessing of the Algerian dialect, like the automatic translation of this\ndialect or sentiment analysis", "published": "2017-07-26 07:48:46", "link": "http://arxiv.org/abs/1707.08998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Implicit Entity Linking in Tweets", "abstract": "Over the years, Twitter has become one of the largest communication platforms\nproviding key data to various applications such as brand monitoring, trend\ndetection, among others. Entity linking is one of the major tasks in natural\nlanguage understanding from tweets and it associates entity mentions in text to\ncorresponding entries in knowledge bases in order to provide unambiguous\ninterpretation and additional con- text. State-of-the-art techniques have\nfocused on linking explicitly mentioned entities in tweets with reasonable\nsuccess. However, we argue that in addition to explicit mentions i.e. The movie\nGravity was more ex- pensive than the mars orbiter mission entities (movie\nGravity) can also be mentioned implicitly i.e. This new space movie is crazy.\nyou must watch it!. This paper introduces the problem of implicit entity\nlinking in tweets. We propose an approach that models the entities by\nexploiting their factual and contextual knowledge. We demonstrate how to use\nthese models to perform implicit entity linking on a ground truth dataset with\n397 tweets from two domains, namely, Movie and Book. Specifically, we show: 1)\nthe importance of linking implicit entities and its value addition to the\nstandard entity linking task, and 2) the importance of exploiting contextual\nknowledge associated with an entity for linking their implicit mentions. We\nalso make the ground truth dataset publicly available to foster the research in\nthis new research area.", "published": "2017-07-26 14:36:58", "link": "http://arxiv.org/abs/1707.08470v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Self-organized Hierarchical Softmax", "abstract": "We propose a new self-organizing hierarchical softmax formulation for\nneural-network-based language models over large vocabularies. Instead of using\na predefined hierarchical structure, our approach is capable of learning word\nclusters with clear syntactical and semantic meaning during the language model\ntraining process. We provide experiments on standard benchmarks for language\nmodeling and sentence compression tasks. We find that this approach is as fast\nas other efficient softmax approximations, while achieving comparable or even\nbetter performance relative to similar full softmax models.", "published": "2017-07-26 18:01:32", "link": "http://arxiv.org/abs/1707.08588v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Tale of Two DRAGGNs: A Hybrid Approach for Interpreting\n  Action-Oriented and Goal-Oriented Instructions", "abstract": "Robots operating alongside humans in diverse, stochastic environments must be\nable to accurately interpret natural language commands. These instructions\noften fall into one of two categories: those that specify a goal condition or\ntarget state, and those that specify explicit actions, or how to perform a\ngiven task. Recent approaches have used reward functions as a semantic\nrepresentation of goal-based commands, which allows for the use of a\nstate-of-the-art planner to find a policy for the given task. However, these\nreward functions cannot be directly used to represent action-oriented commands.\nWe introduce a new hybrid approach, the Deep Recurrent Action-Goal Grounding\nNetwork (DRAGGN), for task grounding and execution that handles natural\nlanguage from either category as input, and generalizes to unseen environments.\nOur robot-simulation results demonstrate that a system successfully\ninterpreting both goal-oriented and action-oriented task specifications brings\nus closer to robust natural language understanding for human-robot interaction.", "published": "2017-07-26 23:57:29", "link": "http://arxiv.org/abs/1707.08668v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Guiding Reinforcement Learning Exploration Using Natural Language", "abstract": "In this work we present a technique to use natural language to help\nreinforcement learning generalize to unseen environments. This technique uses\nneural machine translation, specifically the use of encoder-decoder networks,\nto learn associations between natural language behavior descriptions and\nstate-action information. We then use this learned model to guide agent\nexploration using a modified version of policy shaping to make it more\neffective at learning in unseen environments. We evaluate this technique using\nthe popular arcade game, Frogger, under ideal and non-ideal conditions. This\nevaluation shows that our modified policy shaping algorithm improves over a\nQ-learning agent as well as a baseline version of policy shaping.", "published": "2017-07-26 19:23:54", "link": "http://arxiv.org/abs/1707.08616v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.AI"}
{"title": "Probabilistic Graphical Models for Credibility Analysis in Evolving\n  Online Communities", "abstract": "One of the major hurdles preventing the full exploitation of information from\nonline communities is the widespread concern regarding the quality and\ncredibility of user-contributed content. Prior works in this domain operate on\na static snapshot of the community, making strong assumptions about the\nstructure of the data (e.g., relational tables), or consider only shallow\nfeatures for text classification.\n  To address the above limitations, we propose probabilistic graphical models\nthat can leverage the joint interplay between multiple factors in online\ncommunities --- like user interactions, community dynamics, and textual content\n--- to automatically assess the credibility of user-contributed online content,\nand the expertise of users and their evolution with user-interpretable\nexplanation. To this end, we devise new models based on Conditional Random\nFields for different settings like incorporating partial expert knowledge for\nsemi-supervised learning, and handling discrete labels as well as numeric\nratings for fine-grained analysis. This enables applications such as extracting\nreliable side-effects of drugs from user-contributed posts in healthforums, and\nidentifying credible content in news communities.\n  Online communities are dynamic, as users join and leave, adapt to evolving\ntrends, and mature over time. To capture this dynamics, we propose generative\nmodels based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian\nMotion to trace the continuous evolution of user expertise and their language\nmodel over time. This allows us to identify expert users and credible content\njointly over time, improving state-of-the-art recommender systems by explicitly\nconsidering the maturity of users. This also enables applications such as\nidentifying helpful product reviews, and detecting fake and anomalous reviews\nwith limited information.", "published": "2017-07-26 07:41:27", "link": "http://arxiv.org/abs/1707.08309v1", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.SI"}
{"title": "Video Highlight Prediction Using Audience Chat Reactions", "abstract": "Sports channel video portals offer an exciting domain for research on\nmultimodal, multilingual analysis. We present methods addressing the problem of\nautomatic video highlight prediction based on joint visual features and textual\nanalysis of the real-world audience discourse with complex slang, in both\nEnglish and traditional Chinese. We present a novel dataset based on League of\nLegends championships recorded from North American and Taiwanese Twitch.tv\nchannels (will be released for further research), and demonstrate strong\nresults on these using multimodal, character-level CNN-RNN model architectures.", "published": "2017-07-26 17:44:38", "link": "http://arxiv.org/abs/1707.08559v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
