{"title": "OpinionDigest: A Simple Framework for Opinion Summarization", "abstract": "We present OpinionDigest, an abstractive opinion summarization framework,\nwhich does not rely on gold-standard summaries for training. The framework uses\nan Aspect-based Sentiment Analysis model to extract opinion phrases from\nreviews, and trains a Transformer model to reconstruct the original reviews\nfrom these extractions. At summarization time, we merge extractions from\nmultiple reviews and select the most popular ones. The selected opinions are\nused as input to the trained Transformer model, which verbalizes them into an\nopinion summary. OpinionDigest can also generate customized summaries, tailored\nto specific user needs, by filtering the selected opinions according to their\naspect and/or sentiment. Automatic evaluation on Yelp data shows that our\nframework outperforms competitive baselines. Human studies on two corpora\nverify that OpinionDigest produces informative summaries and shows promising\ncustomization capabilities.", "published": "2020-05-05 01:22:29", "link": "http://arxiv.org/abs/2005.01901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Syntactic Preordering for Controlled Paraphrase Generation", "abstract": "Paraphrasing natural language sentences is a multifaceted process: it might\ninvolve replacing individual words or short phrases, local rearrangement of\ncontent, or high-level restructuring like topicalization or passivization. Past\napproaches struggle to cover this space of paraphrase possibilities in an\ninterpretable manner. Our work, inspired by pre-ordering literature in machine\ntranslation, uses syntactic transformations to softly \"reorder'' the source\nsentence and guide our neural paraphrasing model. First, given an input\nsentence, we derive a set of feasible syntactic rearrangements using an\nencoder-decoder model. This model operates over a partially lexical, partially\nsyntactic view of the sentence and can reorder big chunks. Next, we use each\nproposed rearrangement to produce a sequence of position embeddings, which\nencourages our final encoder-decoder paraphrase model to attend to the source\nwords in a particular order. Our evaluation, both automatic and human, shows\nthat the proposed system retains the quality of the baseline approaches while\ngiving a substantial increase in the diversity of the generated paraphrases", "published": "2020-05-05 09:02:25", "link": "http://arxiv.org/abs/2005.02013v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Establishing Baselines for Text Classification in Low-Resource Languages", "abstract": "While transformer-based finetuning techniques have proven effective in tasks\nthat involve low-resource, low-data environments, a lack of properly\nestablished baselines and benchmark datasets make it hard to compare different\napproaches that are aimed at tackling the low-resource setting. In this work,\nwe provide three contributions. First, we introduce two previously unreleased\ndatasets as benchmark datasets for text classification and low-resource\nmultilabel text classification for the low-resource language Filipino. Second,\nwe pretrain better BERT and DistilBERT models for use within the Filipino\nsetting. Third, we introduce a simple degradation test that benchmarks a\nmodel's resistance to performance degradation as the number of training samples\nare reduced. We analyze our pretrained model's degradation speeds and look\ntowards the use of this method for comparing models aimed at operating within\nthe low-resource setting. We release all our models and datasets for the\nresearch community to use.", "published": "2020-05-05 11:17:07", "link": "http://arxiv.org/abs/2005.02068v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artemis: A Novel Annotation Methodology for Indicative Single Document\n  Summarization", "abstract": "We describe Artemis (Annotation methodology for Rich, Tractable, Extractive,\nMulti-domain, Indicative Summarization), a novel hierarchical annotation\nprocess that produces indicative summaries for documents from multiple domains.\nCurrent summarization evaluation datasets are single-domain and focused on a\nfew domains for which naturally occurring summaries can be easily found, such\nas news and scientific articles. These are not sufficient for training and\nevaluation of summarization models for use in document management and\ninformation retrieval systems, which need to deal with documents from multiple\ndomains. Compared to other annotation methods such as Relative Utility and\nPyramid, Artemis is more tractable because judges don't need to look at all the\nsentences in a document when making an importance judgment for one of the\nsentences, while providing similarly rich sentence importance annotations. We\ndescribe the annotation process in detail and compare it with other similar\nevaluation systems. We also present analysis and experimental results over a\nsample set of 532 annotated documents.", "published": "2020-05-05 13:38:02", "link": "http://arxiv.org/abs/2005.02146v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Dialog Management: Recent Advances and Challenges", "abstract": "Dialog management (DM) is a crucial component in a task-oriented dialog\nsystem. Given the dialog history, DM predicts the dialog state and decides the\nnext action that the dialog agent should take. Recently, dialog policy learning\nhas been widely formulated as a Reinforcement Learning (RL) problem, and more\nworks focus on the applicability of DM. In this paper, we survey recent\nadvances and challenges within three critical topics for DM: (1) improving\nmodel scalability to facilitate dialog system modeling in new scenarios, (2)\ndealing with the data scarcity problem for dialog policy learning, and (3)\nenhancing the training efficiency to achieve better task-completion performance\n. We believe that this survey can shed a light on future research in dialog\nmanagement.", "published": "2020-05-05 14:31:24", "link": "http://arxiv.org/abs/2005.02233v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Creating a Multimodal Dataset of Images and Text to Study Abusive\n  Language", "abstract": "In order to study online hate speech, the availability of datasets containing\nthe linguistic phenomena of interest are of crucial importance. However, when\nit comes to specific target groups, for example teenagers, collecting such data\nmay be problematic due to issues with consent and privacy restrictions.\nFurthermore, while text-only datasets of this kind have been widely used,\nlimitations set by image-based social media platforms like Instagram make it\ndifficult for researchers to experiment with multimodal hate speech data. We\ntherefore developed CREENDER, an annotation tool that has been used in school\nclasses to create a multimodal dataset of images and abusive comments, which we\nmake freely available under Apache 2.0 license. The corpus, with Italian\ncomments, has been analysed from different perspectives, to investigate whether\nthe subject of the images plays a role in triggering a comment. We find that\nusers judge the same images in different ways, although the presence of a\nperson in the picture increases the probability to get an offensive comment.", "published": "2020-05-05 14:31:47", "link": "http://arxiv.org/abs/2005.02235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Code-switching patterns can be an effective route to improve performance\n  of downstream NLP applications: A case study of humour, sarcasm and hate\n  speech detection", "abstract": "In this paper we demonstrate how code-switching patterns can be utilised to\nimprove various downstream NLP applications. In particular, we encode different\nswitching features to improve humour, sarcasm and hate speech detection tasks.\nWe believe that this simple linguistic observation can also be potentially\nhelpful in improving other similar NLP applications.", "published": "2020-05-05 15:48:34", "link": "http://arxiv.org/abs/2005.02295v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural CRF Model for Sentence Alignment in Text Simplification", "abstract": "The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation.", "published": "2020-05-05 16:47:51", "link": "http://arxiv.org/abs/2005.02324v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Digraph of Senegal s local languages: issues, challenges and prospects\n  of their transliteration", "abstract": "The local languages in Senegal, like those of West African countries in\ngeneral, are written based on two alphabets: supplemented Arabic alphabet\n(called Ajami) and Latin alphabet. Each writing has its own applications. Ajami\nwriting is generally used by people educated in Koranic schools for\ncommunication, business, literature (religious texts, poetry, etc.),\ntraditional religious medicine, etc. Writing with Latin characters is used for\nlocalization of ICT (Web, dictionaries, Windows and Google tools translated in\nWolof, etc.), the translation of legal texts (commercial code and constitution\ntranslated in Wolof) and religious ones (Quran and Bible in Wolof), book\nedition, etc. To facilitate both populations general access to knowledge, it is\nuseful to set up transliteration tools between these two scriptures. This work\nfalls within the framework of the implementation of project for a collaborative\nonline dictionary Wolof (Nguer E. M., Khoule M, Thiam M. N., Mbaye B. T.,\nThiare O., Cisse M. T., Mangeot M. 2014), which will involve people using Ajami\nwriting. Our goal will consist, on the one hand in raising the issues related\nto the transliteration and the challenges that this will raise, and on the\nother one, presenting the perspectives.", "published": "2020-05-05 16:48:37", "link": "http://arxiv.org/abs/2005.02325v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's Easier to Translate out of English than into it: Measuring Neural\n  Translation Difficulty by Cross-Mutual Information", "abstract": "The performance of neural machine translation systems is commonly evaluated\nin terms of BLEU. However, due to its reliance on target language properties\nand generation, the BLEU metric does not allow an assessment of which\ntranslation directions are more difficult to model. In this paper, we propose\ncross-mutual information (XMI): an asymmetric information-theoretic metric of\nmachine translation difficulty that exploits the probabilistic nature of most\nneural machine translation models. XMI allows us to better evaluate the\ndifficulty of translating text into the target language while controlling for\nthe difficulty of the target-side generation component independent of the\ntranslation task. We then present the first systematic and controlled study of\ncross-lingual translation difficulties using modern neural translation systems.\nCode for replicating our experiments is available online at\nhttps://github.com/e-bug/nmt-difficulty.", "published": "2020-05-05 17:38:48", "link": "http://arxiv.org/abs/2005.02354v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient strategies for hierarchical text classification: External\n  knowledge and auxiliary tasks", "abstract": "In hierarchical text classification, we perform a sequence of inference steps\nto predict the category of a document from top to bottom of a given class\ntaxonomy. Most of the studies have focused on developing novels neural network\narchitectures to deal with the hierarchical structure, but we prefer to look\nfor efficient ways to strengthen a baseline model. We first define the task as\na sequence-to-sequence problem. Afterwards, we propose an auxiliary synthetic\ntask of bottom-up-classification. Then, from external dictionaries, we retrieve\ntextual definitions for the classes of all the hierarchy's layers, and map them\ninto the word vector space. We use the class-definition embeddings as an\nadditional input to condition the prediction of the next layer and in an\nadapted beam search. Whereas the modified search did not provide large gains,\nthe combination of the auxiliary task and the additional input of\nclass-definitions significantly enhance the classification accuracy. With our\nefficient approaches, we outperform previous studies, using a drastically\nreduced number of parameters, in two well-known English datasets.", "published": "2020-05-05 20:22:18", "link": "http://arxiv.org/abs/2005.02473v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phonetic and Visual Priors for Decipherment of Informal Romanization", "abstract": "Informal romanization is an idiosyncratic process used by humans in informal\ndigital communication to encode non-Latin script languages into Latin character\nsets found on common keyboards. Character substitution choices differ between\nusers but have been shown to be governed by the same main principles observed\nacross a variety of languages---namely, character pairs are often associated\nthrough phonetic or visual similarity. We propose a noisy-channel WFST cascade\nmodel for deciphering the original non-Latin script from observed romanized\ntext in an unsupervised fashion. We train our model directly on romanized data\nfrom two languages: Egyptian Arabic and Russian. We demonstrate that adding\ninductive bias through phonetic and visual priors on character mappings\nsubstantially improves the model's performance on both languages, yielding\nresults much closer to the supervised skyline. Finally, we introduce a new\ndataset of romanized Russian, collected from a Russian social network website\nand partially annotated for our experiments.", "published": "2020-05-05 21:57:27", "link": "http://arxiv.org/abs/2005.02517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Cascade Transformer: an Application for Efficient Answer Sentence\n  Selection", "abstract": "Large transformer-based language models have been shown to be very effective\nin many classification tasks. However, their computational complexity prevents\ntheir use in applications requiring the classification of a large set of\ncandidates. While previous works have investigated approaches to reduce model\nsize, relatively little attention has been paid to techniques to improve batch\nthroughput during inference. In this paper, we introduce the Cascade\nTransformer, a simple yet effective technique to adapt transformer-based models\ninto a cascade of rankers. Each ranker is used to prune a subset of candidates\nin a batch, thus dramatically increasing throughput at inference time. Partial\nencodings from the transformer model are shared among rerankers, providing\nfurther speed-up. When compared to a state-of-the-art transformer model, our\napproach reduces computation by 37% with almost no impact on accuracy, as\nmeasured on two English Question Answering datasets.", "published": "2020-05-05 23:32:01", "link": "http://arxiv.org/abs/2005.02534v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Speak to your Parser: Interactive Text-to-SQL with Natural Language\n  Feedback", "abstract": "We study the task of semantic parse correction with natural language\nfeedback. Given a natural language utterance, most semantic parsing systems\npose the problem as one-shot translation where the utterance is mapped to a\ncorresponding logical form. In this paper, we investigate a more interactive\nscenario where humans can further interact with the system by providing\nfree-form natural language feedback to correct the system when it generates an\ninaccurate interpretation of an initial utterance. We focus on natural language\nto SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL\ninterpretations and the corresponding natural language feedback. We compare\nvarious reference models for the correction task and show that incorporating\nsuch a rich form of feedback can significantly improve the overall semantic\nparsing accuracy while retaining the flexibility of natural language\ninteraction. While we estimated human correction accuracy is 81.5%, our best\nmodel achieves only 25.1%, which leaves a large gap for improvement in future\nresearch. SPLASH is publicly available at https://aka.ms/Splash_dataset.", "published": "2020-05-05 23:58:09", "link": "http://arxiv.org/abs/2005.02539v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Digraphie des langues ouest africaines : Latin2Ajami : un algorithme de\n  translitteration automatique", "abstract": "The national languages of Senegal, like those of West Africa country in\ngeneral, are written with two alphabets : the Latin alphabet that draws its\nstrength from official decreesm and the completed Arabic script (Ajami),\nwidespread and well integrated, that has little institutional support. This\ndigraph created two worlds ignoring each other. Indeed, Ajami writing is\ngenerally used daily by populations from Koranic schools, while writing with\nthe Latin alphabet is used by people from the public school. To solve this\nproblem, it is useful to establish transliteration tools between these two\nscriptures. Preliminary work (Nguer, Bao-Diop, Fall, khoule, 2015) was\nperformed to locate the problems, challenges and prospects. This present work,\nmaking it subsequently fell into this. Its objective is the study and creation\nof a transliteration algorithm from latin towards Ajami.", "published": "2020-05-05 16:52:46", "link": "http://arxiv.org/abs/2005.02827v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-grained Financial Opinion Mining: A Survey and Research Agenda", "abstract": "Opinion mining is a prevalent research issue in many domains. In the\nfinancial domain, however, it is still in the early stages. Most of the\nresearches on this topic only focus on the coarse-grained market sentiment\nanalysis, i.e., 2-way classification for bullish/bearish. Thanks to the recent\nfinancial technology (FinTech) development, some interdisciplinary researchers\nstart to involve in the in-depth analysis of investors' opinions. In this\nposition paper, we first define the financial opinions from both coarse-grained\nand fine-grained points of views, and then provide an overview on the issues\nalready tackled. In addition to listing research issues of the existing topics,\nwe further propose a road map of fine-grained financial opinion mining for\nfuture researches, and point out several challenges yet to explore. Moreover,\nwe provide possible directions to deal with the proposed research issues.", "published": "2020-05-05 01:07:07", "link": "http://arxiv.org/abs/2005.01897v3", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Probabilistic Assumptions Matter: Improved Models for\n  Distantly-Supervised Document-Level Question Answering", "abstract": "We address the problem of extractive question answering using document-level\ndistant super-vision, pairing questions and relevant documents with answer\nstrings. We compare previously used probability space and distant super-vision\nassumptions (assumptions on the correspondence between the weak answer string\nlabels and possible answer mention spans). We show that these assumptions\ninteract, and that different configurations provide complementary benefits. We\ndemonstrate that a multi-objective model can efficiently combine the advantages\nof multiple assumptions and out-perform the best individual formulation. Our\napproach outperforms previous state-of-the-art models by 4.3 points in F1 on\nTriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.", "published": "2020-05-05 01:08:36", "link": "http://arxiv.org/abs/2005.01898v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dynamically Adjusting Transformer Batch Size by Monitoring Gradient\n  Direction Change", "abstract": "The choice of hyper-parameters affects the performance of neural models.\nWhile much previous research (Sutskever et al., 2013; Duchi et al., 2011;\nKingma and Ba, 2015) focuses on accelerating convergence and reducing the\neffects of the learning rate, comparatively few papers concentrate on the\neffect of batch size. In this paper, we analyze how increasing batch size\naffects gradient direction, and propose to evaluate the stability of gradients\nwith their angle change. Based on our observations, the angle change of\ngradient direction first tends to stabilize (i.e. gradually decrease) while\naccumulating mini-batches, and then starts to fluctuate. We propose to\nautomatically and dynamically determine batch sizes by accumulating gradients\nof mini-batches and performing an optimization step at just the time when the\ndirection of gradients starts to fluctuate. To improve the efficiency of our\napproach for large models, we propose a sampling approach to select gradients\nof parameters sensitive to the batch size. Our approach dynamically determines\nproper and efficient batch sizes during training. In our experiments on the WMT\n14 English to German and English to French tasks, our approach improves the\nTransformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.", "published": "2020-05-05 08:47:34", "link": "http://arxiv.org/abs/2005.02008v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Exploring Contextual Word-level Style Relevance for Unsupervised Style\n  Transfer", "abstract": "Unsupervised style transfer aims to change the style of an input sentence\nwhile preserving its original content without using parallel training data. In\ncurrent dominant approaches, owing to the lack of fine-grained control on the\ninfluence from the target style,they are unable to yield desirable output\nsentences. In this paper, we propose a novel attentional sequence-to-sequence\n(Seq2seq) model that dynamically exploits the relevance of each output word to\nthe target style for unsupervised style transfer. Specifically, we first\npretrain a style classifier, where the relevance of each input word to the\noriginal style can be quantified via layer-wise relevance propagation. In a\ndenoising auto-encoding manner, we train an attentional Seq2seq model to\nreconstruct input sentences and repredict word-level previously-quantified\nstyle relevance simultaneously. In this way, this model is endowed with the\nability to automatically predict the style relevance of each output word. Then,\nwe equip the decoder of this model with a neural style component to exploit the\npredicted wordlevel style relevance for better style transfer. Particularly, we\nfine-tune this model using a carefully-designed objective function involving\nstyle transfer, style relevance consistency, content preservation and fluency\nmodeling loss terms. Experimental results show that our proposed model achieves\nstate-of-the-art performance in terms of both transfer accuracy and content\npreservation.", "published": "2020-05-05 10:24:28", "link": "http://arxiv.org/abs/2005.02049v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Self-organizing Pattern in Multilayer Network for Words and Syllables", "abstract": "One of the ultimate goals for linguists is to find universal properties in\nhuman languages. Although words are generally considered as representing\narbitrary mapping between linguistic forms and meanings, we propose a new\nuniversal law that highlights the equally important role of syllables, which is\ncomplementary to Zipf's. By plotting rank-rank frequency distribution of word\nand syllable for English and Chinese corpora, visible lines appear and can be\nfit to a master curve. We discover the multi-layer network for words and\nsyllables based on this analysis exhibits the feature of self-organization\nwhich relies heavily on the inclusion of syllables and their connections.\nAnalytic form for the scaling structure is derived and used to quantify how\nInternet slang becomes fashionable, which demonstrates its usefulness as a new\ntool to evolutionary linguistics.", "published": "2020-05-05 12:01:47", "link": "http://arxiv.org/abs/2005.02087v1", "categories": ["cs.CL", "nlin.AO"], "primary_category": "cs.CL"}
{"title": "SLEDGE: A Simple Yet Effective Baseline for COVID-19 Scientific\n  Knowledge Search", "abstract": "With worldwide concerns surrounding the Severe Acute Respiratory Syndrome\nCoronavirus 2 (SARS-CoV-2), there is a rapidly growing body of literature on\nthe virus. Clinicians, researchers, and policy-makers need a way to effectively\nsearch these articles. In this work, we present a search system called SLEDGE,\nwhich utilizes SciBERT to effectively re-rank articles. We train the model on a\ngeneral-domain answer ranking dataset, and transfer the relevance signals to\nSARS-CoV-2 for evaluation. We observe SLEDGE's effectiveness as a strong\nbaseline on the TREC-COVID challenge (topping the learderboard with an nDCG@10\nof 0.6844). Insights provided by a detailed analysis provide some potential\nfuture directions to explore, including the importance of filtering by date and\nthe potential of neural methods that rely more heavily on count signals. We\nrelease the code to facilitate future work on this critical task at\nhttps://github.com/Georgetown-IR-Lab/covid-neural-ir", "published": "2020-05-05 17:51:27", "link": "http://arxiv.org/abs/2005.02365v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on\n  10,000+ Abstracts in the COVID-19 Open Research Dataset", "abstract": "This paper introduces CODA-19, a human-annotated dataset that codes the\nBackground, Purpose, Method, Finding/Contribution, and Other sections of 10,966\nEnglish abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by\n248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved\nlabeling quality comparable to that of experts. Each abstract was annotated by\nnine different workers, and the final labels were acquired by majority vote.\nThe inter-annotator agreement (Cohen's kappa) between the crowd and the\nbiomedical expert (0.741) is comparable to inter-expert agreement (0.788).\nCODA-19's labels have an accuracy of 82.2% when compared to the biomedical\nexpert's labels, while the accuracy between experts was 85.0%. Reliable human\nannotations help scientists access and integrate the rapidly accelerating\ncoronavirus literature, and also serve as the battery of AI/NLP research, but\nobtaining expert annotations can be slow. We demonstrated that a non-expert\ncrowd can be rapidly employed at scale to join the fight against COVID-19.", "published": "2020-05-05 17:51:42", "link": "http://arxiv.org/abs/2005.02367v5", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Automated Personalized Feedback Improves Learning Gains in an\n  Intelligent Tutoring System", "abstract": "We investigate how automated, data-driven, personalized feedback in a\nlarge-scale intelligent tutoring system (ITS) improves student learning\noutcomes. We propose a machine learning approach to generate personalized\nfeedback, which takes individual needs of students into account. We utilize\nstate-of-the-art machine learning and natural language processing techniques to\nprovide the students with personalized hints, Wikipedia-based explanations, and\nmathematical hints. Our model is used in Korbit, a large-scale dialogue-based\nITS with thousands of students launched in 2019, and we demonstrate that the\npersonalized feedback leads to considerable improvement in student learning\noutcomes and in the subjective evaluation of the feedback.", "published": "2020-05-05 18:30:08", "link": "http://arxiv.org/abs/2005.02431v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Russian Natural Language Generation: Creation of a Language Modelling\n  Dataset and Evaluation with Modern Neural Architectures", "abstract": "Generating coherent, grammatically correct, and meaningful text is very\nchallenging, however, it is crucial to many modern NLP systems. So far,\nresearch has mostly focused on English language, for other languages both\nstandardized datasets, as well as experiments with state-of-the-art models, are\nrare. In this work, we i) provide a novel reference dataset for Russian\nlanguage modeling, ii) experiment with popular modern methods for text\ngeneration, namely variational autoencoders, and generative adversarial\nnetworks, which we trained on the new dataset. We evaluate the generated text\nregarding metrics such as perplexity, grammatical correctness and lexical\ndiversity.", "published": "2020-05-05 20:20:25", "link": "http://arxiv.org/abs/2005.02470v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering\n  Models", "abstract": "Retrieval question answering (ReQA) is the task of retrieving a\nsentence-level answer to a question from an open corpus (Ahmad et\nal.,2019).This paper presents MultiReQA, anew multi-domain ReQA evaluation\nsuite com-posed of eight retrieval QA tasks drawn from publicly available QA\ndatasets. We provide the first systematic retrieval based evaluation over these\ndatasets using two supervised neural models, based on fine-tuning BERT\nandUSE-QA models respectively, as well as a surprisingly strong information\nretrieval baseline,BM25. Five of these tasks contain both train-ing and test\ndata, while three contain test data only. Performance on the five tasks with\ntrain-ing data shows that while a general model covering all domains is\nachievable, the best performance is often obtained by training exclusively on\nin-domain data.", "published": "2020-05-05 21:30:16", "link": "http://arxiv.org/abs/2005.02507v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ExpBERT: Representation Engineering with Natural Language Explanations", "abstract": "Suppose we want to specify the inductive bias that married couples typically\ngo on honeymoons for the task of extracting pairs of spouses from text. In this\npaper, we allow model developers to specify these types of inductive biases as\nnatural language explanations. We use BERT fine-tuned on MultiNLI to\n``interpret'' these explanations with respect to the input sentence, producing\nexplanation-guided representations of the input. Across three relation\nextraction tasks, our method, ExpBERT, matches a BERT baseline but with 3--20x\nless labeled data and improves on the baseline by 3--10 F1 points with the same\namount of labeled data.", "published": "2020-05-05 03:40:23", "link": "http://arxiv.org/abs/2005.01932v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "End-to-end Whispered Speech Recognition with Frequency-weighted\n  Approaches and Pseudo Whisper Pre-training", "abstract": "Whispering is an important mode of human speech, but no end-to-end\nrecognition results for it were reported yet, probably due to the scarcity of\navailable whispered speech data. In this paper, we present several approaches\nfor end-to-end (E2E) recognition of whispered speech considering the special\ncharacteristics of whispered speech and the scarcity of data. This includes a\nfrequency-weighted SpecAugment policy and a frequency-divided CNN feature\nextractor for better capturing the high-frequency structures of whispered\nspeech, and a layer-wise transfer learning approach to pre-train a model with\nnormal or normal-to-whispered converted speech then fine-tune it with whispered\nspeech to bridge the gap between whispered and normal speech. We achieve an\noverall relative reduction of 19.8% in PER and 44.4% in CER on a relatively\nsmall whispered TIMIT corpus. The results indicate as long as we have a good\nE2E model pre-trained on normal or pseudo-whispered speech, a relatively small\nset of whispered speech may suffice to obtain a reasonably good E2E whispered\nspeech recognizer.", "published": "2020-05-05 07:08:53", "link": "http://arxiv.org/abs/2005.01972v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term\n  Importance Estimation and Neural Query Rewriting", "abstract": "Conversational search plays a vital role in conversational information\nseeking. As queries in information seeking dialogues are ambiguous for\ntraditional ad-hoc information retrieval (IR) systems due to the coreference\nand omission resolution problems inherent in natural language dialogue,\nresolving these ambiguities is crucial. In this paper, we tackle conversational\npassage retrieval (ConvPR), an important component of conversational search, by\naddressing query ambiguities with query reformulation integrated into a\nmulti-stage ad-hoc IR system. Specifically, we propose two conversational query\nreformulation (CQR) methods: (1) term importance estimation and (2) neural\nquery rewriting. For the former, we expand conversational queries using\nimportant terms extracted from the conversational context with frequency-based\nsignals. For the latter, we reformulate conversational queries into natural,\nstandalone, human-understandable queries with a pretrained sequence-tosequence\nmodel. Detailed analyses of the two CQR methods are provided quantitatively and\nqualitatively, explaining their advantages, disadvantages, and distinct\nbehaviors. Moreover, to leverage the strengths of both CQR methods, we propose\ncombining their output with reciprocal rank fusion, yielding state-of-the-art\nretrieval effectiveness, 30% improvement in terms of NDCG@3 compared to the\nbest submission of TREC CAsT 2019.", "published": "2020-05-05 14:30:20", "link": "http://arxiv.org/abs/2005.02230v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Contextualizing Hate Speech Classifiers with Post-hoc Explanation", "abstract": "Hate speech classifiers trained on imbalanced datasets struggle to determine\nif group identifiers like \"gay\" or \"black\" are used in offensive or prejudiced\nways. Such biases manifest in false positives when these identifiers are\npresent, due to models' inability to learn the contexts which constitute a\nhateful usage of identifiers. We extract SOC post-hoc explanations from\nfine-tuned BERT classifiers to efficiently detect bias towards identity terms.\nThen, we propose a novel regularization technique based on these explanations\nthat encourages models to learn from the context of group identifiers in\naddition to the identifiers themselves. Our approach improved over baselines in\nlimiting false positives on out-of-domain data while maintaining or improving\nin-domain performance. Project page:\nhttps://inklab.usc.edu/contextualize-hate-speech/.", "published": "2020-05-05 18:56:40", "link": "http://arxiv.org/abs/2005.02439v3", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Cross-media Structured Common Space for Multimedia Event Extraction", "abstract": "We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to\nextract events and their arguments from multimedia documents. We develop the\nfirst benchmark and collect a dataset of 245 multimedia news articles with\nextensively annotated events and arguments. We propose a novel method, Weakly\nAligned Structured Embedding (WASE), that encodes structured representations of\nsemantic information from textual and visual data into a common embedding\nspace. The structures are aligned across modalities by employing a weakly\nsupervised training strategy, which enables exploiting available resources\nwithout explicit cross-media annotation. Compared to uni-modal state-of-the-art\nmethods, our approach achieves 4.0% and 9.8% absolute F-score gains on text\nevent argument role labeling and visual event extraction. Compared to\nstate-of-the-art multimedia unstructured representations, we achieve 8.3% and\n5.0% absolute F-score gains on multimedia event extraction and argument role\nlabeling, respectively. By utilizing images, we extract 21.4% more event\nmentions than traditional text-only methods.", "published": "2020-05-05 20:21:53", "link": "http://arxiv.org/abs/2005.02472v1", "categories": ["cs.MM", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.MM"}
{"title": "ESG2Risk: A Deep Learning Framework from ESG News to Stock Volatility\n  Prediction", "abstract": "Incorporating environmental, social, and governance (ESG) considerations into\nsystematic investments has drawn numerous attention recently. In this paper, we\nfocus on the ESG events in financial news flow and exploring the predictive\npower of ESG related financial news on stock volatility. In particular, we\ndevelop a pipeline of ESG news extraction, news representations, and Bayesian\ninference of deep learning models. Experimental evaluation on real data and\ndifferent markets demonstrates the superior predicting performance as well as\nthe relation of high volatility prediction to stocks with potential high risk\nand low return. It also shows the prospect of the proposed pipeline as a\nflexible predicting framework for various textual data and target variables.", "published": "2020-05-05 23:01:36", "link": "http://arxiv.org/abs/2005.02527v1", "categories": ["q-fin.CP", "cs.CL", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "Smart To-Do : Automatic Generation of To-Do Items from Emails", "abstract": "Intelligent features in email service applications aim to increase\nproductivity by helping people organize their folders, compose their emails and\nrespond to pending tasks. In this work, we explore a new application,\nSmart-To-Do, that helps users with task management over emails. We introduce a\nnew task and dataset for automatically generating To-Do items from emails where\nthe sender has promised to perform an action. We design a two-stage process\nleveraging recent advances in neural text generation and sequence-to-sequence\nlearning, obtaining BLEU and ROUGE scores of 0:23 and 0:63 for this task. To\nthe best of our knowledge, this is the first work to address the problem of\ncomposing To-Do items from emails.", "published": "2020-05-05 02:21:40", "link": "http://arxiv.org/abs/2005.06282v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ImpactCite: An XLNet-based method for Citation Impact Analysis", "abstract": "Citations play a vital role in understanding the impact of scientific\nliterature. Generally, citations are analyzed quantitatively whereas\nqualitative analysis of citations can reveal deeper insights into the impact of\na scientific artifact in the community. Therefore, citation impact analysis\n(which includes sentiment and intent classification) enables us to quantify the\nquality of the citations which can eventually assist us in the estimation of\nranking and impact. The contribution of this paper is two-fold. First, we\nbenchmark the well-known language models like BERT and ALBERT along with\nseveral popular networks for both tasks of sentiment and intent classification.\nSecond, we provide ImpactCite, which is XLNet-based method for citation impact\nanalysis. All evaluations are performed on a set of publicly available citation\nanalysis datasets. Evaluation results reveal that ImpactCite achieves a new\nstate-of-the-art performance for both citation intent and sentiment\nclassification by outperforming the existing approaches by 3.44% and 1.33% in\nF1-score. Therefore, we emphasize ImpactCite (XLNet-based solution) for both\ntasks to better understand the impact of a citation. Additional efforts have\nbeen performed to come up with CSC-Clean corpus, which is a clean and reliable\ndataset for citation sentiment classification.", "published": "2020-05-05 08:31:54", "link": "http://arxiv.org/abs/2005.06611v1", "categories": ["cs.CL", "cs.DL", "cs.SI"], "primary_category": "cs.CL"}
