{"title": "The 2015 Sheffield System for Transcription of Multi-Genre Broadcast\n  Media", "abstract": "We describe the University of Sheffield system for participation in the 2015\nMulti-Genre Broadcast (MGB) challenge task of transcribing multi-genre\nbroadcast shows. Transcription was one of four tasks proposed in the MGB\nchallenge, with the aim of advancing the state of the art of automatic speech\nrecognition, speaker diarisation and automatic alignment of subtitles for\nbroadcast media. Four topics are investigated in this work: Data selection\ntechniques for training with unreliable data, automatic speech segmentation of\nbroadcast media shows, acoustic modelling and adaptation in highly variable\nenvironments, and language modelling of multi-genre shows. The final system\noperates in multiple passes, using an initial unadapted decoding stage to\nrefine segmentation, followed by three adapted passes: a hybrid DNN pass with\ninput features normalised by speaker-based cepstral normalisation, another\nhybrid stage with input features normalised by speaker feature-MLLR\ntransformations, and finally a bottleneck-based tandem stage with noise and\nspeaker factorisation. The combination of these three system outputs provides a\nfinal error rate of 27.5% on the official development set, consisting of 47\nmulti-genre shows.", "published": "2015-12-21 14:31:31", "link": "http://arxiv.org/abs/1512.06643v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Backward and Forward Language Modeling for Constrained Sentence\n  Generation", "abstract": "Recent language models, especially those based on recurrent neural networks\n(RNNs), make it possible to generate natural language from a learned\nprobability. Language generation has wide applications including machine\ntranslation, summarization, question answering, conversation systems, etc.\nExisting methods typically learn a joint probability of words conditioned on\nadditional information, which is (either statically or dynamically) fed to\nRNN's hidden layer. In many applications, we are likely to impose hard\nconstraints on the generated texts, i.e., a particular word must appear in the\nsentence. Unfortunately, existing approaches could not solve this problem. In\nthis paper, we propose a novel backward and forward language model. Provided a\nspecific word, we use RNNs to generate previous words and future words, either\nsimultaneously or asynchronously, resulting in two model variants. In this way,\nthe given word could appear at any position in the sentence. Experimental\nresults show that the generated texts are comparable to sequential LMs in\nquality.", "published": "2015-12-21 13:07:31", "link": "http://arxiv.org/abs/1512.06612v2", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
