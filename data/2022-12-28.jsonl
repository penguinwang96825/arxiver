{"title": "Leveraging World Knowledge in Implicit Hate Speech Detection", "abstract": "While much attention has been paid to identifying explicit hate speech,\nimplicit hateful expressions that are disguised in coded or indirect language\nare pervasive and remain a major challenge for existing hate speech detection\nsystems. This paper presents the first attempt to apply Entity Linking (EL)\ntechniques to both explicit and implicit hate speech detection, where we show\nthat such real world knowledge about entity mentions in a text does help models\nbetter detect hate speech, and the benefit of adding it into the model is more\npronounced when explicit entity triggers (e.g., rally, KKK) are present. We\nalso discuss cases where real world knowledge does not add value to hate speech\ndetection, which provides more insights into understanding and modeling the\nsubtleties of hate speech.", "published": "2022-12-28 21:23:55", "link": "http://arxiv.org/abs/2212.14100v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TempCLR: Temporal Alignment Representation with Contrastive Learning", "abstract": "Video representation learning has been successful in video-text pre-training\nfor zero-shot transfer, where each sentence is trained to be close to the\npaired video clips in a common feature space. For long videos, given a\nparagraph of description where the sentences describe different segments of the\nvideo, by matching all sentence-clip pairs, the paragraph and the full video\nare aligned implicitly. However, such unit-level comparison may ignore global\ntemporal context, which inevitably limits the generalization ability. In this\npaper, we propose a contrastive learning framework TempCLR to compare the full\nvideo and the paragraph explicitly. As the video/paragraph is formulated as a\nsequence of clips/sentences, under the constraint of their temporal order, we\nuse dynamic time warping to compute the minimum cumulative cost over\nsentence-clip pairs as the sequence-level distance. To explore the temporal\ndynamics, we break the consistency of temporal succession by shuffling video\nclips w.r.t. temporal granularity. Then, we obtain the representations for\nclips/sentences, which perceive the temporal information and thus facilitate\nthe sequence alignment. In addition to pre-training on the video and paragraph,\nour approach can also generalize on the matching between video instances. We\nevaluate our approach on video retrieval, action step localization, and\nfew-shot action recognition, and achieve consistent performance gain over all\nthree tasks. Detailed ablation studies are provided to justify the approach\ndesign.", "published": "2022-12-28 08:10:31", "link": "http://arxiv.org/abs/2212.13738v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Automatic Recognition and Classification of Future Work Sentences from\n  Academic Articles in a Specific Domain", "abstract": "Future work sentences (FWS) are the particular sentences in academic papers\nthat contain the author's description of their proposed follow-up research\ndirection. This paper presents methods to automatically extract FWS from\nacademic papers and classify them according to the different future directions\nembodied in the paper's content. FWS recognition methods will enable subsequent\nresearchers to locate future work sentences more accurately and quickly and\nreduce the time and cost of acquiring the corpus. The current work on automatic\nidentification of future work sentences is relatively small, and the existing\nresearch cannot accurately identify FWS from academic papers, and thus cannot\nconduct data mining on a large scale. Furthermore, there are many aspects to\nthe content of future work, and the subdivision of the content is conducive to\nthe analysis of specific development directions. In this paper, Nature Language\nProcessing (NLP) is used as a case study, and FWS are extracted from academic\npapers and classified into different types. We manually build an annotated\ncorpus with six different types of FWS. Then, automatic recognition and\nclassification of FWS are implemented using machine learning models, and the\nperformance of these models is compared based on the evaluation metrics. The\nresults show that the Bernoulli Bayesian model has the best performance in the\nautomatic recognition task, with the Macro F1 reaching 90.73%, and the SCIBERT\nmodel has the best performance in the automatic classification task, with the\nweighted average F1 reaching 72.63%. Finally, we extract keywords from FWS and\ngain a deep understanding of the key content described in FWS, and we also\ndemonstrate that content determination in FWS will be reflected in the\nsubsequent research work by measuring the similarity between future work\nsentences and the abstracts.", "published": "2022-12-28 15:26:04", "link": "http://arxiv.org/abs/2212.13860v1", "categories": ["cs.CL", "cs.DL"], "primary_category": "cs.CL"}
{"title": "Demonstrate-Search-Predict: Composing retrieval and language models for\n  knowledge-intensive NLP", "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach\nfor addressing knowledge-intensive tasks using frozen language models (LM) and\nretrieval models (RM). Existing work has combined these in simple\n\"retrieve-then-read\" pipelines in which the RM retrieves passages that are\ninserted into the LM prompt. To begin to fully realize the potential of frozen\nLMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that\nrelies on passing natural language texts in sophisticated pipelines between an\nLM and an RM. DSP can express high-level programs that bootstrap pipeline-aware\ndemonstrations, search for relevant passages, and generate grounded\npredictions, systematically breaking down problems into small transformations\nthat the LM and RM can handle more reliably. We have written novel DSP programs\nfor answering questions in open-domain, multi-hop, and conversational settings,\nestablishing in early evaluations new state-of-the-art in-context learning\nresults and delivering 37-120%, 8-39%, and 80-290% relative gains against the\nvanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a\ncontemporaneous self-ask pipeline, respectively. We release DSP at\nhttps://github.com/stanfordnlp/dsp", "published": "2022-12-28 18:52:44", "link": "http://arxiv.org/abs/2212.14024v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Cramming: Training a Language Model on a Single GPU in One Day", "abstract": "Recent trends in language modeling have focused on increasing performance\nthrough scaling, and have resulted in an environment where training language\nmodels is out of reach for most researchers and practitioners. While most in\nthe community are asking how to push the limits of extreme computation, we ask\nthe opposite question: How far can we get with a single GPU in just one day?\n  We investigate the downstream performance achievable with a transformer-based\nlanguage model trained completely from scratch with masked language modeling\nfor a single day on a single consumer GPU. Aside from re-analyzing nearly all\ncomponents of the pretraining pipeline for this scenario and providing a\nmodified pipeline with performance close to BERT, we investigate why scaling\ndown is hard, and which modifications actually improve performance in this\nscenario. We provide evidence that even in this constrained setting,\nperformance closely follows scaling laws observed in large-compute settings.\nThrough the lens of scaling laws, we categorize a range of recent improvements\nto training and architecture and discuss their merit and practical\napplicability (or lack thereof) for the limited compute setting.", "published": "2022-12-28 18:59:28", "link": "http://arxiv.org/abs/2212.14034v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models", "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.", "published": "2022-12-28 17:56:03", "link": "http://arxiv.org/abs/2212.14052v3", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Data Augmentation using Transformers and Similarity Measures for\n  Improving Arabic Text Classification", "abstract": "The performance of learning models heavily relies on the availability and\nadequacy of training data. To address the dataset adequacy issue, researchers\nhave extensively explored data augmentation (DA) as a promising approach. DA\ngenerates new data instances through transformations applied to the available\ndata, thereby increasing dataset size and variability. This approach has\nenhanced model performance and accuracy, particularly in addressing class\nimbalance problems in classification tasks. However, few studies have explored\nDA for the Arabic language, relying on traditional approaches such as\nparaphrasing or noising-based techniques. In this paper, we propose a new\nArabic DA method that employs the recent powerful modeling technique, namely\nthe AraGPT-2, for the augmentation process. The generated sentences are\nevaluated in terms of context, semantics, diversity, and novelty using the\nEuclidean, cosine, Jaccard, and BLEU distances. Finally, the AraBERT\ntransformer is used on sentiment classification tasks to evaluate the\nclassification performance of the augmented Arabic dataset. The experiments\nwere conducted on four sentiment Arabic datasets: AraSarcasm, ASTD, ATT, and\nMOVIE. The selected datasets vary in size, label number, and unbalanced\nclasses. The results show that the proposed methodology enhanced the Arabic\nsentiment text classification on all datasets with an increase in F1 score by\n4% in AraSarcasm, 6% in ASTD, 9% in ATT, and 13% in MOVIE.", "published": "2022-12-28 16:38:43", "link": "http://arxiv.org/abs/2212.13939v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Choosing the Number of Topics in LDA Models -- A Monte Carlo Comparison\n  of Selection Criteria", "abstract": "Selecting the number of topics in LDA models is considered to be a difficult\ntask, for which alternative approaches have been proposed. The performance of\nthe recently developed singular Bayesian information criterion (sBIC) is\nevaluated and compared to the performance of alternative model selection\ncriteria. The sBIC is a generalization of the standard BIC that can be\nimplemented to singular statistical models. The comparison is based on Monte\nCarlo simulations and carried out for several alternative settings, varying\nwith respect to the number of topics, the number of documents and the size of\ndocuments in the corpora. Performance is measured using different criteria\nwhich take into account the correct number of topics, but also whether the\nrelevant topics from the DGPs are identified. Practical recommendations for LDA\nmodel selection in applications are derived.", "published": "2022-12-28 19:37:04", "link": "http://arxiv.org/abs/2212.14074v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Towards automating Codenames spymasters with deep reinforcement learning", "abstract": "Although most reinforcement learning research has centered on competitive\ngames, little work has been done on applying it to co-operative multiplayer\ngames or text-based games. Codenames is a board game that involves both\nasymmetric co-operation and natural language processing, which makes it an\nexcellent candidate for advancing RL research. To my knowledge, this work is\nthe first to formulate Codenames as a Markov Decision Process and apply some\nwell-known reinforcement learning algorithms such as SAC, PPO, and A2C to the\nenvironment. Although none of the above algorithms converge for the Codenames\nenvironment, neither do they converge for a simplified environment called\nClickPixel, except when the board size is small.", "published": "2022-12-28 21:45:59", "link": "http://arxiv.org/abs/2212.14104v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving a sequence-to-sequence nlp model using a reinforcement\n  learning policy algorithm", "abstract": "Nowadays, the current neural network models of dialogue generation(chatbots)\nshow great promise for generating answers for chatty agents. But they are\nshort-sighted in that they predict utterances one at a time while disregarding\ntheir impact on future outcomes. Modelling a dialogue's future direction is\ncritical for generating coherent, interesting dialogues, a need that has led\ntraditional NLP dialogue models that rely on reinforcement learning. In this\narticle, we explain how to combine these objectives by using deep reinforcement\nlearning to predict future rewards in chatbot dialogue. The model simulates\nconversations between two virtual agents, with policy gradient methods used to\nreward sequences that exhibit three useful conversational characteristics: the\nflow of informality, coherence, and simplicity of response (related to\nforward-looking function). We assess our model based on its diversity, length,\nand complexity with regard to humans. In dialogue simulation, evaluations\ndemonstrated that the proposed model generates more interactive responses and\nencourages a more sustained successful conversation. This work commemorates a\npreliminary step toward developing a neural conversational model based on the\nlong-term success of dialogues.", "published": "2022-12-28 22:46:57", "link": "http://arxiv.org/abs/2212.14117v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Customizing Knowledge Graph Embedding to Improve Clinical Study\n  Recommendation", "abstract": "Inferring knowledge from clinical trials using knowledge graph embedding is\nan emerging area. However, customizing graph embeddings for different use cases\nremains a significant challenge. We propose custom2vec, an algorithmic\nframework to customize graph embeddings by incorporating user preferences in\ntraining the embeddings. It captures user preferences by adding custom nodes\nand links derived from manually vetted results of a separate information\nretrieval method. We propose a joint learning objective to preserve the\noriginal network structure while incorporating the user's custom annotations.\nWe hypothesize that the custom training improves user-expected predictions, for\nexample, in link prediction tasks. We demonstrate the effectiveness of\ncustom2vec for clinical trials related to non-small cell lung cancer (NSCLC)\nwith two customization scenarios: recommending immuno-oncology trials\nevaluating PD-1 inhibitors and exploring similar trials that compare new\ntherapies with a standard of care. The results show that custom2vec training\nachieves better performance than the conventional training methods. Our\napproach is a novel way to customize knowledge graph embeddings and enable more\naccurate recommendations and predictions.", "published": "2022-12-28 21:41:25", "link": "http://arxiv.org/abs/2212.14102v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY", "q-bio.QM"], "primary_category": "cs.LG"}
{"title": "Distributed Active Noise Control System Based on a Block Diffusion FxLMS\n  Algorithm with Bidirectional Communication", "abstract": "Recently, distributed active noise control systems based on diffusion\nadaptation have attracted significant research interest due to their balance\nbetween computational complexity and stability compared to conventional\ncentralized and decentralized adaptation schemes. However, the existing\ndiffusion FxLMS algorithm employs node-specific adaptation and\nneighborhood-wide combination, and assumes that the control filters of neighbor\nnodes are similar to each other. This assumption is not true in practical\napplications, and it leads to inferior performance to the centralized\ncontroller approach. In contrast, this paper proposes a Block Diffusion FxLMS\nalgorithm with bidirectional communication, which uses neighborhood-wide\nadaptation and node-specific combination to update the control filters.\nSimulation results validate that the proposed algorithm converges to the\nsolution of the centralized controller with reduced computational burden.", "published": "2022-12-28 10:39:16", "link": "http://arxiv.org/abs/2212.13777v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Singing Voice Synthesis Based on a Musical Note Position-Aware Attention\n  Mechanism", "abstract": "This paper proposes a novel sequence-to-sequence (seq2seq) model with a\nmusical note position-aware attention mechanism for singing voice synthesis\n(SVS). A seq2seq modeling approach that can simultaneously perform acoustic and\ntemporal modeling is attractive. However, due to the difficulty of the temporal\nmodeling of singing voices, many recent SVS systems with an\nencoder-decoder-based model still rely on explicitly on duration information\ngenerated by additional modules. Although some studies perform simultaneous\nmodeling using seq2seq models with an attention mechanism, they have\ninsufficient robustness against temporal modeling. The proposed attention\nmechanism is designed to estimate the attention weights by considering the\nrhythm given by the musical score. Furthermore, several techniques are also\nintroduced to improve the modeling performance of the singing voice.\nExperimental results indicated that the proposed model is effective in terms of\nboth naturalness and robustness of timing.", "published": "2022-12-28 05:24:23", "link": "http://arxiv.org/abs/2212.13703v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
