{"title": "Learning How to Translate North Korean through South Korean", "abstract": "South and North Korea both use the Korean language. However, Korean NLP\nresearch has focused on South Korean only, and existing NLP systems of the\nKorean language, such as neural machine translation (NMT) models, cannot\nproperly handle North Korean inputs. Training a model using North Korean data\nis the most straightforward approach to solving this problem, but there is\ninsufficient data to train NMT models. In this study, we create data for North\nKorean NMT models using a comparable corpus. First, we manually create\nevaluation data for automatic alignment and machine translation. Then, we\ninvestigate automatic alignment methods suitable for North Korean. Finally, we\nverify that a model trained by North Korean bilingual data without human\nannotation can significantly boost North Korean translation accuracy compared\nto existing South Korean models in zero-shot settings.", "published": "2022-01-27 01:21:29", "link": "http://arxiv.org/abs/2201.11258v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Highly Generalizable Models for Multilingual Hate Speech Detection", "abstract": "Hate speech detection has become an important research topic within the past\ndecade. More private corporations are needing to regulate user generated\ncontent on different platforms across the globe. In this paper, we introduce a\nstudy of multilingual hate speech classification. We compile a dataset of 11\nlanguages and resolve different taxonomies by analyzing the combined data with\nbinary labels: hate speech or not hate speech. Defining hate speech in a single\nway across different languages and datasets may erase cultural nuances to the\ndefinition, therefore, we utilize language agnostic embeddings provided by\nLASER and MUSE in order to develop models that can use a generalized definition\nof hate speech across datasets. Furthermore, we evaluate prior state of the art\nmethodologies for hate speech detection under our expanded dataset. We conduct\nthree types of experiments for a binary hate speech classification task:\nMultilingual-Train Monolingual-Test, MonolingualTrain Monolingual-Test and\nLanguage-Family-Train Monolingual Test scenarios to see if performance\nincreases for each language due to learning more from other language data.", "published": "2022-01-27 03:09:38", "link": "http://arxiv.org/abs/2201.11294v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Higher-Order Semantic Dependency Parser", "abstract": "Higher-order features bring significant accuracy gains in semantic dependency\nparsing. However, modeling higher-order features with exact inference is\nNP-hard. Graph neural networks (GNNs) have been demonstrated to be an effective\ntool for solving NP-hard problems with approximate inference in many graph\nlearning tasks. Inspired by the success of GNNs, we investigate building a\nhigher-order semantic dependency parser by applying GNNs. Instead of explicitly\nextracting higher-order features from intermediate parsing graphs, GNNs\naggregate higher-order information concisely by stacking multiple GNN layers.\nExperimental results show that our model outperforms the previous\nstate-of-the-art parser on the SemEval 2015 Task 18 English datasets.", "published": "2022-01-27 04:15:46", "link": "http://arxiv.org/abs/2201.11312v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pan More Gold from the Sand: Refining Open-domain Dialogue Training with\n  Noisy Self-Retrieval Generation", "abstract": "Real human conversation data are complicated, heterogeneous, and noisy, from\nwhich building open-domain dialogue systems remains a challenging task. In\nfact, such dialogue data still contains a wealth of information and knowledge,\nhowever, they are not fully explored. In this paper, we show existing\nopen-domain dialogue generation methods that memorize context-response paired\ndata with autoregressive or encode-decode language models underutilize the\ntraining data. Different from current approaches, using external knowledge, we\nexplore a retrieval-generation training framework that can take advantage of\nthe heterogeneous and noisy training data by considering them as \"evidence\". In\nparticular, we use BERTScore for retrieval, which gives better qualities of the\nevidence and generation. Experiments over publicly available datasets\ndemonstrate that our method can help models generate better responses, even\nsuch training data are usually impressed as low-quality data. Such performance\ngain is comparable with those improved by enlarging the training set, even\nbetter. We also found that the model performance has a positive correlation\nwith the relevance of the retrieved evidence. Moreover, our method performed\nwell on zero-shot experiments, which indicates that our method can be more\nrobust to real-world data.", "published": "2022-01-27 08:02:59", "link": "http://arxiv.org/abs/2201.11367v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Systematic Investigation of Strategies Tailored for Low-Resource\n  Settings for Low-Resource Dependency Parsing", "abstract": "In this work, we focus on low-resource dependency parsing for multiple\nlanguages. Several strategies are tailored to enhance performance in\nlow-resource scenarios. While these are well-known to the community, it is not\ntrivial to select the best-performing combination of these strategies for a\nlow-resource language that we are interested in, and not much attention has\nbeen given to measuring the efficacy of these strategies. We experiment with 5\nlow-resource strategies for our ensembled approach on 7 Universal Dependency\n(UD) low-resource languages. Our exhaustive experimentation on these languages\nsupports the effective improvements for languages not covered in pretrained\nmodels. We show a successful application of the ensembled system on a truly\nlow-resource language Sanskrit. The code and data are available at:\nhttps://github.com/Jivnesh/SanDP", "published": "2022-01-27 08:24:53", "link": "http://arxiv.org/abs/2201.11374v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prabhupadavani: A Code-mixed Speech Translation Data for 25 Languages", "abstract": "Nowadays, the interest in code-mixing has become ubiquitous in Natural\nLanguage Processing (NLP); however, not much attention has been given to\naddress this phenomenon for Speech Translation (ST) task. This can be solely\nattributed to the lack of code-mixed ST task labelled data. Thus, we introduce\nPrabhupadavani, which is a multilingual code-mixed ST dataset for 25 languages.\nIt is multi-domain, covers ten language families, containing 94 hours of speech\nby 130+ speakers, manually aligned with corresponding text in the target\nlanguage. The Prabhupadavani is about Vedic culture and heritage from Indic\nliterature, where code-switching in the case of quotation from literature is\nimportant in the context of humanities teaching. To the best of our knowledge,\nPrabhupadvani is the first multi-lingual code-mixed ST dataset available in the\nST literature. This data also can be used for a code-mixed machine translation\ntask. All the dataset can be accessed at https://github.com/frozentoad9/CMST.", "published": "2022-01-27 09:24:36", "link": "http://arxiv.org/abs/2201.11391v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Yes-Yes-Yes: Proactive Data Collection for ACL Rolling Review and Beyond", "abstract": "The shift towards publicly available text sources has enabled language\nprocessing at unprecedented scale, yet leaves under-serviced the domains where\npublic and openly licensed data is scarce. Proactively collecting text data for\nresearch is a viable strategy to address this scarcity, but lacks systematic\nmethodology taking into account the many ethical, legal and\nconfidentiality-related aspects of data collection. Our work presents a case\nstudy on proactive data collection in peer review -- a challenging and\nunder-resourced NLP domain. We outline ethical and legal desiderata for\nproactive data collection and introduce \"Yes-Yes-Yes\", the first donation-based\npeer reviewing data collection workflow that meets these requirements. We\nreport on the implementation of Yes-Yes-Yes at ACL Rolling Review and\nempirically study the implications of proactive data collection for the dataset\nsize and the biases induced by the donation behavior on the peer reviewing\nplatform.", "published": "2022-01-27 11:02:43", "link": "http://arxiv.org/abs/2201.11443v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus", "abstract": "Semantic code search is the task of retrieving relevant code snippet given a\nnatural language query. Different from typical information retrieval tasks,\ncode search requires to bridge the semantic gap between the programming\nlanguage and natural language, for better describing intrinsic concepts and\nsemantics. Recently, deep neural network for code search has been a hot\nresearch topic. Typical methods for neural code search first represent the code\nsnippet and query text as separate embeddings, and then use vector distance\n(e.g. dot-product or cosine) to calculate the semantic similarity between them.\nThere exist many different ways for aggregating the variable length of code or\nquery tokens into a learnable embedding, including bi-encoder, cross-encoder,\nand poly-encoder. The goal of the query encoder and code encoder is to produce\nembeddings that are close with each other for a related pair of query and the\ncorresponding desired code snippet, in which the choice and design of encoder\nis very significant.\n  In this paper, we propose a novel deep semantic model which makes use of the\nutilities of not only the multi-modal sources, but also feature extractors such\nas self-attention, the aggregated vectors, combination of the intermediate\nrepresentations. We apply the proposed model to tackle the CodeSearchNet\nchallenge about semantic code search. We align cross-lingual embedding for\nmulti-modality learning with large batches and hard example mining, and combine\ndifferent learned representations for better enhancing the representation\nlearning. Our model is trained on CodeSearchNet corpus and evaluated on the\nheld-out data, the final model achieves 0.384 NDCG and won the first place in\nthis benchmark. Models and code are available at\nhttps://github.com/overwindows/SemanticCodeSearch.git.", "published": "2022-01-27 04:15:59", "link": "http://arxiv.org/abs/2201.11313v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Grad2Task: Improved Few-shot Text Classification Using Gradients for\n  Task Representation", "abstract": "Large pretrained language models (LMs) like BERT have improved performance in\nmany disparate natural language processing (NLP) tasks. However, fine tuning\nsuch models requires a large number of training examples for each target task.\nSimultaneously, many realistic NLP problems are \"few shot\", without a\nsufficiently large training set. In this work, we propose a novel conditional\nneural process-based approach for few-shot text classification that learns to\ntransfer from other diverse tasks with rich annotation. Our key idea is to\nrepresent each task using gradient information from a base model and to train\nan adaptation network that modulates a text classifier conditioned on the task\nrepresentation. While previous task-aware few-shot learners represent tasks by\ninput encoding, our novel task representation is more powerful, as the gradient\ncaptures input-output relationships of a task. Experimental results show that\nour approach outperforms traditional fine-tuning, sequential transfer learning,\nand state-of-the-art meta learning approaches on a collection of diverse\nfew-shot tasks. We further conducted analysis and ablations to justify our\ndesign choices.", "published": "2022-01-27 15:29:30", "link": "http://arxiv.org/abs/2201.11576v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and\n  Languages", "abstract": "Reliable evaluation benchmarks designed for replicability and\ncomprehensiveness have driven progress in machine learning. Due to the lack of\na multilingual benchmark, however, vision-and-language research has mostly\nfocused on English language tasks. To fill this gap, we introduce the\nImage-Grounded Language Understanding Evaluation benchmark. IGLUE brings\ntogether - by both aggregating pre-existing datasets and creating new ones -\nvisual question answering, cross-modal retrieval, grounded reasoning, and\ngrounded entailment tasks across 20 diverse languages. Our benchmark enables\nthe evaluation of multilingual multimodal models for transfer learning, not\nonly in a zero-shot setting, but also in newly defined few-shot learning\nsetups. Based on the evaluation of the available state-of-the-art models, we\nfind that translate-test transfer is superior to zero-shot transfer and that\nfew-shot learning is hard to harness for many tasks. Moreover, downstream\nperformance is partially explained by the amount of available unlabelled\ntextual data for pretraining, and only weakly by the typological distance of\ntarget-source languages. We hope to encourage future research efforts in this\narea by releasing the benchmark to the community.", "published": "2022-01-27 18:53:22", "link": "http://arxiv.org/abs/2201.11732v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Going Extreme: Comparative Analysis of Hate Speech in Parler and Gab", "abstract": "Social platforms such as Gab and Parler, branded as `free-speech' networks,\nhave seen a significant growth of their user base in recent years. This\npopularity is mainly attributed to the stricter moderation enforced by\nmainstream platforms such as Twitter, Facebook, and Reddit. In this work we\nprovide the first large scale analysis of hate-speech on Parler.\n  We experiment with an array of algorithms for hate-speech detection,\ndemonstrating limitations of transfer learning in that domain, given the\nillusive and ever changing nature of the ways hate-speech is delivered. In\norder to improve classification accuracy we annotated 10K Parler posts, which\nwe use to fine-tune a BERT classifier. Classification of individual posts is\nthen leveraged for the classification of millions of users via label\npropagation over the social network. Classifying users by their propensity to\ndisseminate hate, we find that hate mongers make 16.1\\% of Parler active users,\nand that they have distinct characteristics comparing to other user groups. We\nfind that hate mongers are more active, more central and express distinct\nlevels of sentiment and convey a distinct array of emotions like anger and\nsadness. We further complement our analysis by comparing the trends discovered\nin Parler and those found in Gab.\n  To the best of our knowledge, this is among the first works to analyze hate\nspeech in Parler in a quantitative manner and on the user level, and the first\nannotated dataset to be made available to the community.", "published": "2022-01-27 19:29:17", "link": "http://arxiv.org/abs/2201.11770v1", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
{"title": "Clinical-Longformer and Clinical-BigBird: Transformers for long clinical\n  sequences", "abstract": "Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nour source code available at\n[https://github.com/luoyuanlab/Clinical-Longformer] the pre-trained models\navailable for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].", "published": "2022-01-27 22:51:58", "link": "http://arxiv.org/abs/2201.11838v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Ontology-enhanced Prompt-tuning for Few-shot Learning", "abstract": "Few-shot Learning (FSL) is aimed to make predictions based on a limited\nnumber of samples. Structured data such as knowledge graphs and ontology\nlibraries has been leveraged to benefit the few-shot setting in various tasks.\nHowever, the priors adopted by the existing methods suffer from challenging\nknowledge missing, knowledge noise, and knowledge heterogeneity, which hinder\nthe performance for few-shot learning. In this study, we explore knowledge\ninjection for FSL with pre-trained language models and propose\nontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the\nontology transformation based on the external knowledge graph to address the\nknowledge missing issue, which fulfills and converts structure knowledge to\ntext. We further introduce span-sensitive knowledge injection via a visible\nmatrix to select informative knowledge to handle the knowledge noise issue. To\nbridge the gap between knowledge and text, we propose a collective training\nalgorithm to optimize representations jointly. We evaluate our proposed\nOntoPrompt in three tasks, including relation extraction, event extraction, and\nknowledge graph completion, with eight datasets. Experimental results\ndemonstrate that our approach can obtain better few-shot performance than\nbaselines.", "published": "2022-01-27 05:41:36", "link": "http://arxiv.org/abs/2201.11332v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reasoning Like Program Executors", "abstract": "Reasoning over natural language is a long-standing goal for the research\ncommunity. However, studies have shown that existing language models are\ninadequate in reasoning. To address the issue, we present POET, a novel\nreasoning pre-training paradigm. Through pre-training language models with\nprograms and their execution results, POET empowers language models to harvest\nthe reasoning knowledge possessed by program executors via a data-driven\napproach. POET is conceptually simple and can be instantiated by different\nkinds of program executors. In this paper, we showcase two simple instances\nPOET-Math and POET-Logic, in addition to a complex instance, POET-SQL.\nExperimental results on six benchmarks demonstrate that POET can significantly\nboost model performance in natural language reasoning, such as numerical\nreasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on\nreasoning-enhancement pre-training, and we hope our analysis would shed light\non the future research of reasoning like program executors.", "published": "2022-01-27 12:28:24", "link": "http://arxiv.org/abs/2201.11473v2", "categories": ["cs.CL", "cs.AI", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Human Interpretation of Saliency-based Explanation Over Text", "abstract": "While a lot of research in explainable AI focuses on producing effective\nexplanations, less work is devoted to the question of how people understand and\ninterpret the explanation. In this work, we focus on this question through a\nstudy of saliency-based explanations over textual data. Feature-attribution\nexplanations of text models aim to communicate which parts of the input text\nwere more influential than others towards the model decision. Many current\nexplanation methods, such as gradient-based or Shapley value-based methods,\nprovide measures of importance which are well-understood mathematically. But\nhow does a person receiving the explanation (the explainee) comprehend it? And\ndoes their understanding match what the explanation attempted to communicate?\nWe empirically investigate the effect of various factors of the input, the\nfeature-attribution explanation, and visualization procedure, on laypeople's\ninterpretation of the explanation. We query crowdworkers for their\ninterpretation on tasks in English and German, and fit a GAMM model to their\nresponses considering the factors of interest. We find that people often\nmis-interpret the explanations: superficial and unrelated factors, such as word\nlength, influence the explainees' importance assignment despite the explanation\ncommunicating importance directly. We then show that some of this distortion\ncan be attenuated: we propose a method to adjust saliencies based on model\nestimates of over- and under-perception, and explore bar charts as an\nalternative to heatmap saliency visualization. We find that both approaches can\nattenuate the distorting effect of specific factors, leading to\nbetter-calibrated understanding of the explanation.", "published": "2022-01-27 15:20:32", "link": "http://arxiv.org/abs/2201.11569v2", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthesizing Dysarthric Speech Using Multi-talker TTS for Dysarthric\n  Speech Recognition", "abstract": "Dysarthria is a motor speech disorder often characterized by reduced speech\nintelligibility through slow, uncoordinated control of speech production\nmuscles. Automatic Speech recognition (ASR) systems may help dysarthric talkers\ncommunicate more effectively. To have robust dysarthria-specific ASR,\nsufficient training speech is required, which is not readily available. Recent\nadvances in Text-To-Speech (TTS) synthesis multi-speaker end-to-end TTS systems\nsuggest the possibility of using synthesis for data augmentation. In this\npaper, we aim to improve multi-speaker end-to-end TTS systems to synthesize\ndysarthric speech for improved training of a dysarthria-specific DNN-HMM ASR.\nIn the synthesized speech, we add dysarthria severity level and pause insertion\nmechanisms to other control parameters such as pitch, energy, and duration.\nResults show that a DNN-HMM model trained on additional synthetic dysarthric\nspeech achieves WER improvement of 12.2% compared to the baseline, the addition\nof the severity level and pause insertion controls decrease WER by 6.5%,\nshowing the effectiveness of adding these parameters. Audio samples are\navailable at", "published": "2022-01-27 15:22:09", "link": "http://arxiv.org/abs/2201.11571v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Recursive Decoding: A Situated Cognition Approach to Compositional\n  Generation in Grounded Language Understanding", "abstract": "Compositional generalization is a troubling blind spot for neural language\nmodels. Recent efforts have presented techniques for improving a model's\nability to encode novel combinations of known inputs, but less work has focused\non generating novel combinations of known outputs. Here we focus on this latter\n\"decode-side\" form of generalization in the context of gSCAN, a synthetic\nbenchmark for compositional generalization in grounded language understanding.\nWe present Recursive Decoding (RD), a novel procedure for training and using\nseq2seq models, targeted towards decode-side generalization. Rather than\ngenerating an entire output sequence in one pass, models are trained to predict\none token at a time. Inputs (i.e., the external gSCAN environment) are then\nincrementally updated based on predicted tokens, and re-encoded for the next\ndecoder time step. RD thus decomposes a complex, out-of-distribution sequence\ngeneration task into a series of incremental predictions that each resemble\nwhat the model has already seen during training. RD yields dramatic improvement\non two previously neglected generalization tasks in gSCAN. We provide analyses\nto elucidate these gains over failure of a baseline, and then discuss\nimplications for generalization in naturalistic grounded language\nunderstanding, and seq2seq more generally.", "published": "2022-01-27 19:13:42", "link": "http://arxiv.org/abs/2201.11766v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Visual Transfer Learning using Knowledge Graphs", "abstract": "Recent approaches of computer vision utilize deep learning methods as they\nperform quite well if training and testing domains follow the same underlying\ndata distribution. However, it has been shown that minor variations in the\nimages that occur when using these methods in the real world can lead to\nunpredictable errors. Transfer learning is the area of machine learning that\ntries to prevent these errors. Especially, approaches that augment image data\nusing auxiliary knowledge encoded in language embeddings or knowledge graphs\n(KGs) have achieved promising results in recent years. This survey focuses on\nvisual transfer learning approaches using KGs. KGs can represent auxiliary\nknowledge either in an underlying graph-structured schema or in a vector-based\nknowledge graph embedding. Intending to enable the reader to solve visual\ntransfer learning problems with the help of specific KG-DL configurations we\nstart with a description of relevant modeling structures of a KG of various\nexpressions, such as directed labeled graphs, hypergraphs, and hyper-relational\ngraphs. We explain the notion of feature extractor, while specifically\nreferring to visual and semantic features. We provide a broad overview of\nknowledge graph embedding methods and describe several joint training\nobjectives suitable to combine them with high dimensional visual embeddings.\nThe main section introduces four different categories on how a KG can be\ncombined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge\nGraph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as\na Peer. To help researchers find evaluation benchmarks, we provide an overview\nof generic KGs and a set of image processing datasets and benchmarks including\nvarious types of auxiliary knowledge. Last, we summarize related surveys and\ngive an outlook about challenges and open issues for future research.", "published": "2022-01-27 20:19:55", "link": "http://arxiv.org/abs/2201.11794v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Sentiment-Aware Automatic Speech Recognition pre-training for enhanced\n  Speech Emotion Recognition", "abstract": "We propose a novel multi-task pre-training method for Speech Emotion\nRecognition (SER). We pre-train SER model simultaneously on Automatic Speech\nRecognition (ASR) and sentiment classification tasks to make the acoustic ASR\nmodel more ``emotion aware''. We generate targets for the sentiment\nclassification using text-to-sentiment model trained on publicly available\ndata. Finally, we fine-tune the acoustic ASR on emotion annotated speech data.\nWe evaluated the proposed approach on the MSP-Podcast dataset, where we\nachieved the best reported concordance correlation coefficient (CCC) of 0.41\nfor valence prediction.", "published": "2022-01-27 22:20:28", "link": "http://arxiv.org/abs/2201.11826v1", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "primary_category": "cs.CL"}
{"title": "TableQuery: Querying tabular data with natural language", "abstract": "This paper presents TableQuery, a novel tool for querying tabular data using\ndeep learning models pre-trained to answer questions on free text. Existing\ndeep learning methods for question answering on tabular data have various\nlimitations, such as having to feed the entire table as input into a neural\nnetwork model, making them unsuitable for most real-world applications. Since\nreal-world data might contain millions of rows, it may not entirely fit into\nthe memory. Moreover, data could be stored in live databases, which are updated\nin real-time, and it is impractical to serialize an entire database to a neural\nnetwork-friendly format each time it is updated. In TableQuery, we use deep\nlearning models pre-trained for question answering on free text to convert\nnatural language queries to structured queries, which can be run against a\ndatabase or a spreadsheet. This method eliminates the need for fitting the\nentire data into memory as well as serializing databases. Furthermore, deep\nlearning models pre-trained for question answering on free text are readily\navailable on platforms such as HuggingFace Model Hub (7). TableQuery does not\nrequire re-training; when a newly trained model for question answering with\nbetter performance is available, it can replace the existing model in\nTableQuery.", "published": "2022-01-27 17:26:25", "link": "http://arxiv.org/abs/2202.00454v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Deep Recurrent Learning for Heart Sounds Segmentation based on\n  Instantaneous Frequency Features", "abstract": "In this work, a novel stack of well-known technologies is presented to\ndetermine an automatic method to segment the heart sounds in a phonocardiogram\n(PCG). We will show a deep recurrent neural network (DRNN) capable of\nsegmenting a PCG into its main components and a very specific way of extracting\ninstantaneous frequency that will play an important role in the training and\ntesting of the proposed model. More specifically, it involves a Long Short-Term\nMemory (LSTM) neural network accompanied by the Fourier Synchrosqueezed\nTransform (FSST) used to extract instantaneous time-frequency features from a\nPCG. The present approach was tested on heart sound signals longer than 5\nseconds and shorter than 35 seconds from freely-available databases. This\napproach proved that, with a relatively small architecture, a small set of\ndata, and the right features, this method achieved an almost state-of-the-art\nperformance, showing an average sensitivity of 89.5%, an average positive\npredictive value of 89.3\\% and an average accuracy of 91.3%.", "published": "2022-01-27 04:40:09", "link": "http://arxiv.org/abs/2201.11320v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "The MSXF TTS System for ICASSP 2022 ADD Challenge", "abstract": "This paper presents our MSXF TTS system for Task 3.1 of the Audio Deep\nSynthesis Detection (ADD) Challenge 2022. We use an end to end text to speech\nsystem, and add a constraint loss to the system when training stage. The end to\nend TTS system is VITS, and the pre-training self-supervised model is wav2vec\n2.0. And we also explore the influence of the speech speed and volume in\nspoofing. The faster speech means the less the silence part in audio, the\neasier to fool the detector. We also find the smaller the volume, the better\nspoofing ability, though we normalize volume for submission. Our team is\nidentified as C2, and we got the fourth place in the challenge.", "published": "2022-01-27 09:30:52", "link": "http://arxiv.org/abs/2201.11400v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Exploring Graph Representation of Chorales", "abstract": "This work explores areas overlapping music, graph theory, and machine\nlearning. An embedding representation of a node, in a weighted undirected graph\n$\\mathcal{G}$, is a representation that captures the meaning of nodes in an\nembedding space. In this work, 383 Bach chorales were compiled and represented\nas a graph. Two application cases were investigated in this paper (i) learning\nnode embedding representation using \\emph{Continuous Bag of Words (CBOW),\nskip-gram}, and \\emph{node2vec} algorithms, and (ii) learning node labels from\nneighboring nodes based on a collective classification approach. The results of\nthis exploratory study ascertains many salient features of the graph-based\nrepresentation approach applicable to music applications.", "published": "2022-01-27 09:46:10", "link": "http://arxiv.org/abs/2201.11745v1", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
