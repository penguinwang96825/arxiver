{"title": "A Shared Attention Mechanism for Interpretation of Neural Automatic\n  Post-Editing Systems", "abstract": "Automatic post-editing (APE) systems aim to correct the systematic errors\nmade by machine translators. In this paper, we propose a neural APE system that\nencodes the source (src) and machine translated (mt) sentences with two\nseparate encoders, but leverages a shared attention mechanism to better\nunderstand how the two inputs contribute to the generation of the post-edited\n(pe) sentences. Our empirical observations have showed that when the mt is\nincorrect, the attention shifts weight toward tokens in the src sentence to\nproperly edit the incorrect translation. The model has been trained and\nevaluated on the official data from the WMT16 and WMT17 APE IT domain\nEnglish-German shared tasks. Additionally, we have used the extra 500K\nartificial data provided by the shared task. Our system has been able to\nreproduce the accuracies of systems trained with the same data, while at the\nsame time providing better interpretability.", "published": "2018-07-01 00:31:27", "link": "http://arxiv.org/abs/1807.00248v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Approach to Encoding Context for Spoken Language\n  Understanding", "abstract": "In task-oriented dialogue systems, spoken language understanding, or SLU,\nrefers to the task of parsing natural language user utterances into semantic\nframes. Making use of context from prior dialogue history holds the key to more\neffective SLU. State of the art approaches to SLU use memory networks to encode\ncontext by processing multiple utterances from the dialogue at each turn,\nresulting in significant trade-offs between accuracy and computational\nefficiency. On the other hand, downstream components like the dialogue state\ntracker (DST) already keep track of the dialogue state, which can serve as a\nsummary of the dialogue history. In this work, we propose an efficient approach\nto encoding context from prior utterances for SLU. More specifically, our\narchitecture includes a separate recurrent neural network (RNN) based encoding\nmodule that accumulates dialogue context to guide the frame parsing sub-tasks\nand can be shared between SLU and DST. In our experiments, we demonstrate the\neffectiveness of our approach on dialogues from two domains.", "published": "2018-07-01 04:11:18", "link": "http://arxiv.org/abs/1807.00267v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lost in Translation: Analysis of Information Loss During Machine\n  Translation Between Polysynthetic and Fusional Languages", "abstract": "Machine translation from polysynthetic to fusional languages is a challenging\ntask, which gets further complicated by the limited amount of parallel text\navailable. Thus, translation performance is far from the state of the art for\nhigh-resource and more intensively studied language pairs. To shed light on the\nphenomena which hamper automatic translation to and from polysynthetic\nlanguages, we study translations from three low-resource, polysynthetic\nlanguages (Nahuatl, Wixarika and Yorem Nokki) into Spanish and vice versa.\nDoing so, we find that in a morpheme-to-morpheme alignment an important amount\nof information contained in polysynthetic morphemes has no Spanish counterpart,\nand its translation is often omitted. We further conduct a qualitative analysis\nand, thus, identify morpheme types that are commonly hard to align or ignored\nin the translation process.", "published": "2018-07-01 07:17:36", "link": "http://arxiv.org/abs/1807.00286v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling, comprehending and summarizing textual content by graphs", "abstract": "Automatic Text Summarization strategies have been successfully employed to\ndigest text collections and extract its essential content. Usually, summaries\nare generated using textual corpora that belongs to the same domain area where\nthe summary will be used. Nonetheless, there are special cases where it is not\nfound enough textual sources, and one possible alternative is to generate a\nsummary from a different domain. One manner to summarize texts consists of\nusing a graph model. This model allows giving more importance to words\ncorresponding to the main concepts from the target domain found in the\nsummarized text. This gives the reader an overview of the main text concepts as\nwell as their relationships. However, this kind of summarization presents a\nsignificant number of repeated terms when compared to human-generated\nsummaries. In this paper, we present an approach to produce graph-model\nextractive summaries of texts, meeting the target domain exigences and treating\nthe terms repetition problem. To evaluate the proposition, we performed a\nseries of experiments showing that the proposed approach statistically improves\nthe performance of a model based on Graph Centrality, achieving better\ncoverage, accuracy, and recall.", "published": "2018-07-01 09:42:10", "link": "http://arxiv.org/abs/1807.00303v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
