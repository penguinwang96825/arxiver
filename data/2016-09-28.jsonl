{"title": "Character Sequence Models for ColorfulWords", "abstract": "We present a neural network architecture to predict a point in color space\nfrom the sequence of characters in the color's name. Using large scale\ncolor--name pairs obtained from an online color design forum, we evaluate our\nmodel on a \"color Turing test\" and find that, given a name, the colors\npredicted by our model are preferred by annotators to color names created by\nhumans. Our datasets and demo system are available online at colorlab.us.", "published": "2016-09-28 05:41:18", "link": "http://arxiv.org/abs/1609.08777v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effective Combination of Language and Vision Through Model Composition\n  and the R-CCA Method", "abstract": "We address the problem of integrating textual and visual information in\nvector space models for word meaning representation. We first present the\nResidual CCA (R-CCA) method, that complements the standard CCA method by\nrepresenting, for each modality, the difference between the original signal and\nthe signal projected to the shared, max correlation, space. We then show that\nconstructing visual and textual representations and then post-processing them\nthrough composition of common modeling motifs such as PCA, CCA, R-CCA and\nlinear interpolation (a.k.a sequential modeling) yields high quality models. On\nfive standard semantic benchmarks our sequential models outperform recent\nmultimodal representation learning alternatives, including ones that rely on\njoint representation learning. For two of these benchmarks our R-CCA method is\npart of the Best configuration our algorithm yields.", "published": "2016-09-28 08:11:28", "link": "http://arxiv.org/abs/1609.08810v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Equation Parsing: Mapping Sentences to Grounded Equations", "abstract": "Identifying mathematical relations expressed in text is essential to\nunderstanding a broad range of natural language text from election reports, to\nfinancial news, to sport commentaries to mathematical word problems. This paper\nfocuses on identifying and understanding mathematical relations described\nwithin a single sentence. We introduce the problem of Equation Parsing -- given\na sentence, identify noun phrases which represent variables, and generate the\nmathematical equation expressing the relation described in the sentence. We\nintroduce the notion of projective equation parsing and provide an efficient\nalgorithm to parse text to projective equations. Our system makes use of a high\nprecision lexicon of mathematical expressions and a pipeline of structured\npredictors, and generates correct equations in $70\\%$ of the cases. In $60\\%$\nof the time, it also identifies the correct noun phrase $\\rightarrow$ variables\nmapping, significantly outperforming baselines. We also release a new annotated\ndataset for task evaluation.", "published": "2016-09-28 08:54:05", "link": "http://arxiv.org/abs/1609.08824v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Byte-based Language Identification with Deep Convolutional Networks", "abstract": "We report on our system for the shared task on discriminating between similar\nlanguages (DSL 2016). The system uses only byte representations in a deep\nresidual network (ResNet). The system, named ResIdent, is trained only on the\ndata released with the task (closed training). We obtain 84.88% accuracy on\nsubtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A\nlarge difference in accuracy on development data can be observed with\nrelatively minor changes in our network's architecture and hyperparameters. We\ntherefore expect fine-tuning of these parameters to yield higher accuracies.", "published": "2016-09-28 16:51:56", "link": "http://arxiv.org/abs/1609.09004v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Psychologically Motivated Text Mining", "abstract": "Natural language processing techniques are increasingly applied to identify\nsocial trends and predict behavior based on large text collections. Existing\nmethods typically rely on surface lexical and syntactic information. Yet,\nresearch in psychology shows that patterns of human conceptualisation, such as\nmetaphorical framing, are reliable predictors of human expectations and\ndecisions. In this paper, we present a method to learn patterns of metaphorical\nframing from large text collections, using statistical techniques. We apply the\nmethod to data in three different languages and evaluate the identified\npatterns, demonstrating their psychological validity.", "published": "2016-09-28 17:58:23", "link": "http://arxiv.org/abs/1609.09019v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using Natural Language Processing and Qualitative Analysis to Intervene\n  in Gang Violence: A Collaboration Between Social Work Researchers and Data\n  Scientists", "abstract": "The U.S. has the highest rate of firearm-related deaths when compared to\nother industrialized countries. Violence particularly affects low-income, urban\nneighborhoods in cities like Chicago, which saw a 40% increase in firearm\nviolence from 2014 to 2015 to more than 3,000 shooting victims. While recent\nstudies have found that urban, gang-involved individuals curate a unique and\ncomplex communication style within and between social media platforms,\norganizations focused on reducing gang violence are struggling to keep up with\nthe growing complexity of social media platforms and the sheer volume of data\nthey present. In this paper, describe the Digital Urban Violence Analysis\nApproach (DUVVA), a collaborative qualitative analysis method used in a\ncollaboration between data scientists and social work researchers to develop a\nsuite of systems for decoding the high- stress language of urban, gang-involved\nyouth. Our approach leverages principles of grounded theory when analyzing\napproximately 800 tweets posted by Chicago gang members and participation of\nyouth from Chicago neighborhoods to create a language resource for natural\nlanguage processing (NLP) methods. In uncovering the unique language and\ncommunication style, we developed automated tools with the potential to detect\naggressive language on social media and aid individuals and groups in\nperforming violence prevention and interruption.", "published": "2016-09-28 05:44:10", "link": "http://arxiv.org/abs/1609.08779v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Unsupervised Neural Hidden Markov Models", "abstract": "In this work, we present the first results for neuralizing an Unsupervised\nHidden Markov Model. We evaluate our approach on tag in- duction. Our approach\noutperforms existing generative models and is competitive with the\nstate-of-the-art though with a simpler model easily extended to include\nadditional context.", "published": "2016-09-28 16:55:52", "link": "http://arxiv.org/abs/1609.09007v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Stance Classification in Rumours as a Sequential Task Exploiting the\n  Tree Structure of Social Media Conversations", "abstract": "Rumour stance classification, the task that determines if each tweet in a\ncollection discussing a rumour is supporting, denying, questioning or simply\ncommenting on the rumour, has been attracting substantial interest. Here we\nintroduce a novel approach that makes use of the sequence of transitions\nobserved in tree-structured conversation threads in Twitter. The conversation\nthreads are formed by harvesting users' replies to one another, which results\nin a nested tree-like structure. Previous work addressing the stance\nclassification task has treated each tweet as a separate unit. Here we analyse\ntweets by virtue of their position in a sequence and test two sequential\nclassifiers, Linear-Chain CRF and Tree CRF, each of which makes different\nassumptions about the conversational structure. We experiment with eight\nTwitter datasets, collected during breaking news, and show that exploiting the\nsequential structure of Twitter conversations achieves significant improvements\nover the non-sequential methods. Our work is the first to model Twitter\nconversations as a tree structure in this manner, introducing a novel way of\ntackling NLP tasks on Twitter conversations.", "published": "2016-09-28 18:24:12", "link": "http://arxiv.org/abs/1609.09028v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Memory Visualization for Gated Recurrent Neural Networks in Speech\n  Recognition", "abstract": "Recurrent neural networks (RNNs) have shown clear superiority in sequence\nmodeling, particularly the ones with gated units, such as long short-term\nmemory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties\nbehind the remarkable performance remain unclear in many applications, e.g.,\nautomatic speech recognition (ASR). This paper employs visualization techniques\nto study the behavior of LSTM and GRU when performing speech recognition tasks.\nOur experiments show some interesting patterns in the gated memory, and some of\nthem have inspired simple yet effective modifications on the network structure.\nWe report two of such modifications: (1) lazy cell update in LSTM, and (2)\nshortcut connections for residual learning. Both modifications lead to more\ncomprehensible and powerful networks.", "published": "2016-09-28 06:26:16", "link": "http://arxiv.org/abs/1609.08789v3", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Hierarchical Memory Networks for Answer Selection on Unknown Words", "abstract": "Recently, end-to-end memory networks have shown promising results on Question\nAnswering task, which encode the past facts into an explicit memory and perform\nreasoning ability by making multiple computational steps on the memory.\nHowever, memory networks conduct the reasoning on sentence-level memory to\noutput coarse semantic vectors and do not further take any attention mechanism\nto focus on words, which may lead to the model lose some detail information,\nespecially when the answers are rare or unknown words. In this paper, we\npropose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the\npast facts into sentence-level memory and word-level memory respectively. Then,\n(k)-max pooling is exploited following reasoning module on the sentence-level\nmemory to sample the (k) most relevant sentences to a question and feed these\nsentences into attention mechanism on the word-level memory to focus the words\nin the selected sentences. Finally, the prediction is jointly learned over the\noutputs of the sentence-level reasoning module and the word-level attention\nmechanism. The experimental results demonstrate that our approach successfully\nconducts answer selection on unknown words and achieves a better performance\nthan memory networks.", "published": "2016-09-28 10:03:05", "link": "http://arxiv.org/abs/1609.08843v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
