{"title": "Sequence-to-sequence Pre-training with Data Augmentation for Sentence\n  Rewriting", "abstract": "We study sequence-to-sequence (seq2seq) pre-training with data augmentation\nfor sentence rewriting. Instead of training a seq2seq model with gold training\ndata and augmented data simultaneously, we separate them to train in different\nphases: pre-training with the augmented data and fine-tuning with the gold\ndata. We also introduce multiple data augmentation methods to help model\npre-training for sentence rewriting. We evaluate our approach in two typical\nwell-defined sentence rewriting tasks: Grammatical Error Correction (GEC) and\nFormality Style Transfer (FST). Experiments demonstrate our approach can better\nutilize augmented data without hurting the model's trust in gold data and\nfurther improve the model's performance with our proposed data augmentation\nmethods.\n  Our approach substantially advances the state-of-the-art results in\nwell-recognized sentence rewriting benchmarks over both GEC and FST.\nSpecifically, it pushes the CoNLL-2014 benchmark's $F_{0.5}$ score and JFLEG\nTest GLEU score to 62.61 and 63.54 in the restricted training setting, 66.77\nand 65.22 respectively in the unrestricted setting, and advances GYAFC\nbenchmark's BLEU to 74.24 (2.23 absolute improvement) in E&M domain and 77.97\n(2.64 absolute improvement) in F&R domain.", "published": "2019-09-13 02:20:25", "link": "http://arxiv.org/abs/1909.06002v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging 2-hop Distant Supervision from Table Entity Pairs for\n  Relation Extraction", "abstract": "Distant supervision (DS) has been widely used to automatically construct\n(noisy) labeled data for relation extraction (RE). Given two entities, distant\nsupervision exploits sentences that directly mention them for predicting their\nsemantic relation. We refer to this strategy as 1-hop DS, which unfortunately\nmay not work well for long-tail entities with few supporting sentences. In this\npaper, we introduce a new strategy named 2-hop DS to enhance distantly\nsupervised RE, based on the observation that there exist a large number of\nrelational tables on the Web which contain entity pairs that share common\nrelations. We refer to such entity pairs as anchors for each other, and collect\nall sentences that mention the anchor entity pairs of a given target entity\npair to help relation prediction. We develop a new neural RE method REDS2 in\nthe multi-instance learning paradigm, which adopts a hierarchical model\nstructure to fuse information respectively from 1-hop DS and 2-hop DS.\nExtensive experimental results on a benchmark dataset show that REDS2 can\nconsistently outperform various baselines across different settings by a\nsubstantial margin.", "published": "2019-09-13 02:43:36", "link": "http://arxiv.org/abs/1909.06007v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Taxonomical hierarchy of canonicalized relations from multiple Knowledge\n  Bases", "abstract": "This work addresses two important questions pertinent to Relation Extraction\n(RE). First, what are all possible relations that could exist between any two\ngiven entity types? Second, how do we define an unambiguous taxonomical (is-a)\nhierarchy among the identified relations? To address the first question, we use\nthree resources Wikipedia Infobox, Wikidata, and DBpedia. This study focuses on\nrelations between person, organization and location entity types. We exploit\nWikidata and DBpedia in a data-driven manner, and Wikipedia Infobox templates\nmanually to generate lists of relations. Further, to address the second\nquestion, we canonicalize, filter, and combine the identified relations from\nthe three resources to construct a taxonomical hierarchy. This hierarchy\ncontains 623 canonical relations with highest contribution from Wikipedia\nInfobox followed by DBpedia and Wikidata. The generated relation list subsumes\nan average of 85% of relations from RE datasets when entity types are\nrestricted.", "published": "2019-09-13 14:20:30", "link": "http://arxiv.org/abs/1909.06249v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameterized Convolutional Neural Networks for Aspect Level Sentiment\n  Classification", "abstract": "We introduce a novel parameterized convolutional neural network for aspect\nlevel sentiment classification. Using parameterized filters and parameterized\ngates, we incorporate aspect information into convolutional neural networks\n(CNN). Experiments demonstrate that our parameterized filters and parameterized\ngates effectively capture the aspect-specific features, and our CNN-based\nmodels achieve excellent results on SemEval 2014 datasets.", "published": "2019-09-13 14:59:51", "link": "http://arxiv.org/abs/1909.06276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Bias Mitigation by Modelling Biases in Corpora", "abstract": "Several recent studies have shown that strong natural language understanding\n(NLU) models are prone to relying on unwanted dataset biases without learning\nthe underlying task, resulting in models that fail to generalize to\nout-of-domain datasets and are likely to perform poorly in real-world\nscenarios. We propose two learning strategies to train neural models, which are\nmore robust to such biases and transfer better to out-of-domain datasets. The\nbiases are specified in terms of one or more bias-only models, which learn to\nleverage the dataset biases. During training, the bias-only models' predictions\nare used to adjust the loss of the base model to reduce its reliance on biases\nby down-weighting the biased examples and focusing the training on the hard\nexamples. We experiment on large-scale natural language inference and fact\nverification benchmarks, evaluating on out-of-domain datasets that are\nspecifically designed to assess the robustness of models against known biases\nin the training data. Results show that our debiasing methods greatly improve\nrobustness in all settings and better transfer to other textual entailment\ndatasets. Our code and data are publicly available in\n\\url{https://github.com/rabeehk/robust-nli}.", "published": "2019-09-13 16:41:13", "link": "http://arxiv.org/abs/1909.06321v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Ensemble Learning for News Stance Detection", "abstract": "Stance detection in fake news is an important component in news veracity\nassessment because this process helps fact-checking by understanding stance to\na central claim from different information sources. The Fake News Challenge\nStage 1 (FNC-1) held in 2017 was setup for this purpose, which involves\nestimating the stance of a news article body relative to a given headline. This\nthesis starts from the error analysis for the three top-performing systems in\nFNC-1. Based on the analysis, a simple but tough-to-beat Multilayer Perceptron\nsystem is chosen as the baseline. Afterwards, three approaches are explored to\nimprove baseline.The first approach explores the possibility of improving the\nprediction accuracy by adding extra keywords features when training a model,\nwhere keywords are converted to an indicator vector and then concatenated to\nthe baseline features. A list of keywords is manually selected based on the\nerror analysis, which may best reflect some characteristics of fake news titles\nand bodies. To make this selection process automatically, three algorithms are\ncreated based on Mutual Information (MI) theory: keywords generator based on MI\nstance class, MI customised class, and Pointwise MI algorithm. The second\napproach is based on word embedding, where word2vec model is introduced and two\ndocument similarities calculation algorithms are implemented: wor2vec cosine\nsimilarity and WMD distance. The third approach is ensemble learning. Different\nmodels are configured together with two continuous outputs combining\nalgorithms. The 10-fold cross validation reveals that the ensemble of three\nneural network models trained from simple bag-of-words features gives the best\nperformance. It is therefore selected to compete in FNC-1. After\nhyperparameters fine tuning, the selected deep ensemble model beats the FNC-1\nwinner team by a remarkable 34.25 marks under FNC-1's evaluation metric.", "published": "2019-09-13 13:39:59", "link": "http://arxiv.org/abs/1909.12233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Machine Translation with 4-Bit Precision and Beyond", "abstract": "Neural Machine Translation (NMT) is resource intensive. We design a\nquantization procedure to compress NMT models better for devices with limited\nhardware capability. Because most neural network parameters are near zero, we\nemploy logarithmic quantization in lieu of fixed-point quantization. However,\nwe find bias terms are less amenable to log quantization but note they comprise\na tiny fraction of the model, so we leave them uncompressed. We also propose to\nuse an error-feedback mechanism during retraining, to preserve the compressed\nmodel as a stale gradient. We empirically show that NMT models based on\nTransformer or RNN architecture can be compressed up to 4-bit precision without\nany noticeable quality degradation. Models can be compressed up to binary\nprecision, albeit with lower quality. The RNN architecture seems to be more\nrobust to quantization, compared to the Transformer.", "published": "2019-09-13 08:55:08", "link": "http://arxiv.org/abs/1909.06091v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A General Framework for Implicit and Explicit Debiasing of\n  Distributional Word Vector Spaces", "abstract": "Distributional word vectors have recently been shown to encode many of the\nhuman biases, most notably gender and racial biases, and models for attenuating\nsuch biases have consequently been proposed. However, existing models and\nstudies (1) operate on under-specified and mutually differing bias definitions,\n(2) are tailored for a particular bias (e.g., gender bias) and (3) have been\nevaluated inconsistently and non-rigorously. In this work, we introduce a\ngeneral framework for debiasing word embeddings. We operationalize the\ndefinition of a bias by discerning two types of bias specification: explicit\nand implicit. We then propose three debiasing models that operate on explicit\nor implicit bias specifications and that can be composed towards more robust\ndebiasing. Finally, we devise a full-fledged evaluation framework in which we\ncouple existing bias metrics with newly proposed ones. Experimental findings\nacross three embedding methods suggest that the proposed debiasing models are\nrobust and widely applicable: they often completely remove the bias both\nimplicitly and explicitly without degradation of semantic information encoded\nin any of the input distributional spaces. Moreover, we successfully transfer\ndebiasing models, by means of cross-lingual embedding spaces, and remove or\nattenuate biases in distributional word vector spaces of languages that lack\nreadily available bias specifications.", "published": "2019-09-13 08:57:14", "link": "http://arxiv.org/abs/1909.06092v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Automated Quest Generation in Text-Adventure Games", "abstract": "Interactive fictions, or text-adventures, are games in which a player\ninteracts with a world entirely through textual descriptions and text actions.\nText-adventure games are typically structured as puzzles or quests wherein the\nplayer must execute certain actions in a certain order to succeed. In this\npaper, we consider the problem of procedurally generating a quest, defined as a\nseries of actions required to progress towards a goal, in a text-adventure\ngame. Quest generation in text environments is challenging because they must be\nsemantically coherent. We present and evaluate two quest generation techniques:\n(1) a Markov model, and (2) a neural generative model. We specifically look at\ngenerating quests about cooking and train our models on recipe data. We\nevaluate our techniques with human participant studies looking at perceived\ncreativity and coherence.", "published": "2019-09-13 15:10:06", "link": "http://arxiv.org/abs/1909.06283v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Gated Self-attention Memory Network for Answer Selection", "abstract": "Answer selection is an important research problem, with applications in many\nareas. Previous deep learning based approaches for the task mainly adopt the\nCompare-Aggregate architecture that performs word-level comparison followed by\naggregation. In this work, we take a departure from the popular\nCompare-Aggregate architecture, and instead, propose a new gated self-attention\nmemory network for the task. Combined with a simple transfer learning technique\nfrom a large-scale online corpus, our model outperforms previous methods by a\nlarge margin, achieving new state-of-the-art results on two standard answer\nselection datasets: TrecQA and WikiQA.", "published": "2019-09-13 17:56:58", "link": "http://arxiv.org/abs/1909.09696v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Say What I Want: Towards the Dark Side of Neural Dialogue Models", "abstract": "Neural dialogue models have been widely adopted in various chatbot\napplications because of their good performance in simulating and generalizing\nhuman conversations. However, there exists a dark side of these models -- due\nto the vulnerability of neural networks, a neural dialogue model can be\nmanipulated by users to say what they want, which brings in concerns about the\nsecurity of practical chatbot services. In this work, we investigate whether we\ncan craft inputs that lead a well-trained black-box neural dialogue model to\ngenerate targeted outputs. We formulate this as a reinforcement learning (RL)\nproblem and train a Reverse Dialogue Generator which efficiently finds such\ninputs for targeted outputs. Experiments conducted on a representative neural\ndialogue model show that our proposed model is able to discover such desired\ninputs in a considerable portion of cases. Overall, our work reveals this\nweakness of neural dialogue models and may prompt further researches of\ndeveloping corresponding solutions to avoid it.", "published": "2019-09-13 05:50:50", "link": "http://arxiv.org/abs/1909.06044v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Neural Correction Model for Open-Domain Named Entity Recognition", "abstract": "Named Entity Recognition (NER) plays an important role in a wide range of\nnatural language processing tasks, such as relation extraction, question\nanswering, etc. However, previous studies on NER are limited to particular\ngenres, using small manually-annotated or large but low-quality datasets.\nMeanwhile, previous datasets for open-domain NER, built using distant\nsupervision, suffer from low precision, recall and ratio of annotated tokens\n(RAT). In this work, to address the low precision and recall problems, we first\nutilize DBpedia as the source of distant supervision to annotate abstracts from\nWikipedia and design a neural correction model trained with a human-annotated\nNER dataset, DocRED, to correct the false entity labels. In this way, we build\na large and high-quality dataset called AnchorNER and then train various models\nwith it. To address the low RAT problem of previous datasets, we introduce a\nmulti-task learning method to exploit the context information. We evaluate our\nmethods on five NER datasets and our experimental results show that models\ntrained with AnchorNER and our multi-task learning method obtain\nstate-of-the-art performances in the open-domain setting.", "published": "2019-09-13 06:44:30", "link": "http://arxiv.org/abs/1909.06058v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PubMedQA: A Dataset for Biomedical Research Question Answering", "abstract": "We introduce PubMedQA, a novel biomedical question answering (QA) dataset\ncollected from PubMed abstracts. The task of PubMedQA is to answer research\nquestions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial\nfibrillation after coronary artery bypass grafting?) using the corresponding\nabstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k\nartificially generated QA instances. Each PubMedQA instance is composed of (1)\na question which is either an existing research article title or derived from\none, (2) a context which is the corresponding abstract without its conclusion,\n(3) a long answer, which is the conclusion of the abstract and, presumably,\nanswers the research question, and (4) a yes/no/maybe answer which summarizes\nthe conclusion. PubMedQA is the first QA dataset where reasoning over\nbiomedical research texts, especially their quantitative contents, is required\nto answer the questions. Our best performing model, multi-phase fine-tuning of\nBioBERT with long answer bag-of-word statistics as additional supervision,\nachieves 68.1% accuracy, compared to single human performance of 78.0% accuracy\nand majority-baseline of 55.2% accuracy, leaving much room for improvement.\nPubMedQA is publicly available at https://pubmedqa.github.io.", "published": "2019-09-13 11:18:20", "link": "http://arxiv.org/abs/1909.06146v1", "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "Neural Architectures for Fine-Grained Propaganda Detection in News", "abstract": "This paper describes our system (MIC-CIS) details and results of\nparticipation in the fine-grained propaganda detection shared task 2019. To\naddress the tasks of sentence (SLC) and fragment level (FLC) propaganda\ndetection, we explore different neural architectures (e.g., CNN, LSTM-CRF and\nBERT) and extract linguistic (e.g., part-of-speech, named entity, readability,\nsentiment, emotion, etc.), layout and topical features. Specifically, we have\ndesigned multi-granularity and multi-tasking neural architectures to jointly\nperform both the sentence and fragment level propaganda detection.\nAdditionally, we investigate different ensemble schemes such as\nmajority-voting, relax-voting, etc. to boost overall system performance.\nCompared to the other participating systems, our submissions are ranked 3rd and\n4th in FLC and SLC tasks, respectively.", "published": "2019-09-13 12:11:47", "link": "http://arxiv.org/abs/1909.06162v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Neural Approach to Irony Generation", "abstract": "Ironies can not only express stronger emotions but also show a sense of\nhumor. With the development of social media, ironies are widely used in public.\nAlthough many prior research studies have been conducted in irony detection,\nfew studies focus on irony generation. The main challenges for irony generation\nare the lack of large-scale irony dataset and difficulties in modeling the\nironic pattern. In this work, we first systematically define irony generation\nbased on style transfer task. To address the lack of data, we make use of\ntwitter and build a large-scale dataset. We also design a combination of\nrewards for reinforcement learning to control the generation of ironic\nsentences. Experimental results demonstrate the effectiveness of our model in\nterms of irony accuracy, sentiment preservation, and content preservation.", "published": "2019-09-13 13:05:27", "link": "http://arxiv.org/abs/1909.06200v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "End-to-End Neural Speaker Diarization with Self-attention", "abstract": "Speaker diarization has been mainly developed based on the clustering of\nspeaker embeddings. However, the clustering-based approach has two major\nproblems; i.e., (i) it is not optimized to minimize diarization errors\ndirectly, and (ii) it cannot handle speaker overlaps correctly. To solve these\nproblems, the End-to-End Neural Diarization (EEND), in which a bidirectional\nlong short-term memory (BLSTM) network directly outputs speaker diarization\nresults given a multi-talker recording, was recently proposed. In this study,\nwe enhance EEND by introducing self-attention blocks instead of BLSTM blocks.\nIn contrast to BLSTM, which is conditioned only on its previous and next hidden\nstates, self-attention is directly conditioned on all the other frames, making\nit much suitable for dealing with the speaker diarization problem. We evaluated\nour proposed method on simulated mixtures, real telephone calls, and real\ndialogue recordings. The experimental results revealed that the self-attention\nwas the key to achieving good performance and that our proposed method\nperformed significantly better than the conventional BLSTM-based method. Our\nmethod was even better than that of the state-of-the-art x-vector\nclustering-based method. Finally, by visualizing the latent representation, we\nshow that the self-attention can capture global speaker characteristics in\naddition to local speech activity dynamics. Our source code is available online\nat https://github.com/hitachi-speech/EEND.", "published": "2019-09-13 14:18:18", "link": "http://arxiv.org/abs/1909.06247v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Scene Graph Parsing by Attention Graph", "abstract": "Scene graph representations, which form a graph of visual object nodes\ntogether with their attributes and relations, have proved useful across a\nvariety of vision and language applications. Recent work in the area has used\nNatural Language Processing dependency tree methods to automatically build\nscene graphs.\n  In this work, we present an 'Attention Graph' mechanism that can be trained\nend-to-end, and produces a scene graph structure that can be lifted directly\nfrom the top layer of a standard Transformer model.\n  The scene graphs generated by our model achieve an F-score similarity of\n52.21% to ground-truth graphs on the evaluation set using the SPICE metric,\nsurpassing the best previous approaches by 2.5%.", "published": "2019-09-13 14:54:37", "link": "http://arxiv.org/abs/1909.06273v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Comparative Study on Transformer vs RNN in Speech Applications", "abstract": "Sequence-to-sequence models have been widely used in end-to-end speech\nprocessing, for example, automatic speech recognition (ASR), speech translation\n(ST), and text-to-speech (TTS). This paper focuses on an emergent\nsequence-to-sequence model called Transformer, which achieves state-of-the-art\nperformance in neural machine translation and other natural language processing\napplications. We undertook intensive studies in which we experimentally\ncompared and analyzed Transformer and conventional recurrent neural networks\n(RNN) in a total of 15 ASR, one multilingual ASR, one ST, and two TTS\nbenchmarks. Our experiments revealed various training tips and significant\nperformance benefits obtained with Transformer for each task including the\nsurprising superiority of Transformer in 13/15 ASR benchmarks in comparison\nwith RNN. We are preparing to release Kaldi-style reproducible recipes using\nopen source and publicly available datasets for all the ASR, ST, and TTS tasks\nfor the community to succeed our exciting outcomes.", "published": "2019-09-13 16:27:08", "link": "http://arxiv.org/abs/1909.06317v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Probing the Information Encoded in X-vectors", "abstract": "Deep neural network based speaker embeddings, such as x-vectors, have been\nshown to perform well in text-independent speaker recognition/verification\ntasks. In this paper, we use simple classifiers to investigate the contents\nencoded by x-vector embeddings. We probe these embeddings for information\nrelated to the speaker, channel, transcription (sentence, words, phones), and\nmeta information about the utterance (duration and augmentation type), and\ncompare these with the information encoded by i-vectors across a varying number\nof dimensions. We also study the effect of data augmentation during extractor\ntraining on the information captured by x-vectors. Experiments on the RedDots\ndata set show that x-vectors capture spoken content and channel-related\ninformation, while performing well on speaker verification tasks.", "published": "2019-09-13 17:56:13", "link": "http://arxiv.org/abs/1909.06351v2", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Addressing Semantic Drift in Question Generation for Semi-Supervised\n  Question Answering", "abstract": "Text-based Question Generation (QG) aims at generating natural and relevant\nquestions that can be answered by a given answer in some context. Existing QG\nmodels suffer from a \"semantic drift\" problem, i.e., the semantics of the\nmodel-generated question drifts away from the given context and answer. In this\npaper, we first propose two semantics-enhanced rewards obtained from downstream\nquestion paraphrasing and question answering tasks to regularize the QG model\nto generate semantically valid questions. Second, since the traditional\nevaluation metrics (e.g., BLEU) often fall short in evaluating the quality of\ngenerated questions, we propose a QA-based evaluation method which measures the\nQG model's ability to mimic human annotators in generating QA training data.\nExperiments show that our method achieves the new state-of-the-art performance\nw.r.t. traditional metrics, and also performs best on our QA-based evaluation\nmetrics. Further, we investigate how to use our QG model to augment QA datasets\nand enable semi-supervised QA. We propose two ways to generate synthetic QA\npairs: generate new questions from existing articles or collect QA pairs from\nnew articles. We also propose two empirically effective strategies, a data\nfilter and mixing mini-batch training, to properly use the QG-generated data\nfor QA. Experiments show that our method improves over both BiDAF and BERT QA\nbaselines, even without introducing new articles.", "published": "2019-09-13 17:59:03", "link": "http://arxiv.org/abs/1909.06356v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Household Task Knowledge from WikiHow Descriptions", "abstract": "Commonsense procedural knowledge is important for AI agents and robots that\noperate in a human environment. While previous attempts at constructing\nprocedural knowledge are mostly rule- and template-based, recent advances in\ndeep learning provide the possibility of acquiring such knowledge directly from\nnatural language sources. As a first step in this direction, we propose a model\nto learn embeddings for tasks, as well as the individual steps that need to be\ntaken to solve them, based on WikiHow articles. We learn these embeddings such\nthat they are predictive of both step relevance and step ordering. We also\nexperiment with the use of integer programming for inferring consistent global\nstep orderings from noisy pairwise predictions.", "published": "2019-09-13 19:16:53", "link": "http://arxiv.org/abs/1909.06414v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adaptive Scheduling for Multi-Task Learning", "abstract": "To train neural machine translation models simultaneously on multiple tasks\n(languages), it is common to sample each task uniformly or in proportion to\ndataset sizes. As these methods offer little control over performance\ntrade-offs, we explore different task scheduling approaches. We first consider\nexisting non-adaptive techniques, then move on to adaptive schedules that\nover-sample tasks with poorer results compared to their respective baseline. As\nexplicit schedules can be inefficient, especially if one task is highly\nover-sampled, we also consider implicit schedules, learning to scale learning\nrates or gradients of individual tasks instead. These techniques allow training\nmultilingual models that perform better for low-resource language pairs (tasks\nwith small amount of data), while minimizing negative effects on high-resource\ntasks.", "published": "2019-09-13 20:23:40", "link": "http://arxiv.org/abs/1909.06434v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "SANVis: Visual Analytics for Understanding Self-Attention Networks", "abstract": "Attention networks, a deep neural network architecture inspired by humans'\nattention mechanism, have seen significant success in image captioning, machine\ntranslation, and many other applications. Recently, they have been further\nevolved into an advanced approach called multi-head self-attention networks,\nwhich can encode a set of input vectors, e.g., word vectors in a sentence, into\nanother set of vectors. Such encoding aims at simultaneously capturing diverse\nsyntactic and semantic features within a set, each of which corresponds to a\nparticular attention head, forming altogether multi-head attention. Meanwhile,\nthe increased model complexity prevents users from easily understanding and\nmanipulating the inner workings of models. To tackle the challenges, we present\na visual analytics system called SANVis, which helps users understand the\nbehaviors and the characteristics of multi-head self-attention networks. Using\na state-of-the-art self-attention model called Transformer, we demonstrate\nusage scenarios of SANVis in machine translation tasks. Our system is available\nat http://short.sanvis.org", "published": "2019-09-13 05:59:40", "link": "http://arxiv.org/abs/1909.09595v1", "categories": ["cs.CL", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Spoken Speech Enhancement using EEG", "abstract": "In this paper we demonstrate spoken speech enhancement using\nelectroencephalography (EEG) signals using a generative adversarial network\n(GAN) based model, gated recurrent unit (GRU) regression based model, temporal\nconvolutional network (TCN) regression model and finally using a mixed TCN GRU\nregression model.\n  We compare our EEG based speech enhancement results with traditional log\nminimum mean-square error (MMSE) speech enhancement algorithm and our proposed\nmethods demonstrate significant improvement in speech enhancement quality\ncompared to the traditional method. Our overall results demonstrate that EEG\nfeatures can be used to clean speech recorded in presence of background noise.\nTo the best of our knowledge this is the first time a spoken speech enhancement\nis demonstrated using EEG features recorded in parallel with spoken speech.", "published": "2019-09-13 21:44:08", "link": "http://arxiv.org/abs/1909.09132v8", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
