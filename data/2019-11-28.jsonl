{"title": "Improving Neural Relation Extraction with Positive and Unlabeled\n  Learning", "abstract": "We present a novel approach to improve the performance of distant supervision\nrelation extraction with Positive and Unlabeled (PU) Learning. This approach\nfirst applies reinforcement learning to decide whether a sentence is positive\nto a given relation, and then positive and unlabeled bags are constructed. In\ncontrast to most previous studies, which mainly use selected positive instances\nonly, we make full use of unlabeled instances and propose two new\nrepresentations for positive and unlabeled bags. These two representations are\nthen combined in an appropriate way to make bag-level prediction. Experimental\nresults on a widely used real-world dataset demonstrate that this new approach\nindeed achieves significant and consistent improvements as compared to several\ncompetitive baselines.", "published": "2019-11-28 06:53:14", "link": "http://arxiv.org/abs/1911.12556v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion\n  Analysis", "abstract": "In this paper, we propose a two-layered multi-task attention based neural\nnetwork that performs sentiment analysis through emotion analysis. The proposed\napproach is based on Bidirectional Long Short-Term Memory and uses\nDistributional Thesaurus as a source of external knowledge to improve the\nsentiment and emotion prediction. The proposed system has two levels of\nattention to hierarchically build a meaningful representation. We evaluate our\nsystem on the benchmark dataset of SemEval 2016 Task 6 and also compare it with\nthe state-of-the-art systems on Stance Sentiment Emotion Corpus. Experimental\nresults show that the proposed system improves the performance of sentiment\nanalysis by 3.2 F-score points on SemEval 2016 Task 6 dataset. Our network also\nboosts the performance of emotion analysis by 5 F-score points on Stance\nSentiment Emotion Corpus.", "published": "2019-11-28 07:43:04", "link": "http://arxiv.org/abs/1911.12569v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word Embedding based New Corpus for Low-resourced Language: Sindhi", "abstract": "Representing words and phrases into dense vectors of real numbers which\nencode semantic and syntactic properties is a vital constituent in natural\nlanguage processing (NLP). The success of neural network (NN) models in NLP\nlargely rely on such dense word representations learned on the large unlabeled\ncorpus. Sindhi is one of the rich morphological language, spoken by large\npopulation in Pakistan and India lacks corpora which plays an essential role of\na test-bed for generating word embeddings and developing language independent\nNLP systems. In this paper, a large corpus of more than 61 million words is\ndeveloped for low-resourced Sindhi language for training neural word\nembeddings. The corpus is acquired from multiple web-resources using\nweb-scrappy. Due to the unavailability of open source preprocessing tools for\nSindhi, the prepossessing of such large corpus becomes a challenging problem\nspecially cleaning of noisy data extracted from web resources. Therefore, a\npreprocessing pipeline is employed for the filtration of noisy text.\nAfterwards, the cleaned vocabulary is utilized for training Sindhi word\nembeddings with state-of-the-art GloVe, Skip-Gram (SG), and Continuous Bag of\nWords (CBoW) word2vec algorithms. The intrinsic evaluation approach of cosine\nsimilarity matrix and WordSim-353 are employed for the evaluation of generated\nSindhi word embeddings. Moreover, we compare the proposed word embeddings with\nrecently revealed Sindhi fastText (SdfastText) word representations. Our\nintrinsic evaluation results demonstrate the high quality of our generated\nSindhi word embeddings using SG, CBoW, and GloVe as compare to SdfastText word\nrepresentations.", "published": "2019-11-28 08:11:44", "link": "http://arxiv.org/abs/1911.12579v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Fine-Grained Sentiment Dataset for Norwegian", "abstract": "We introduce NoReC_fine, a dataset for fine-grained sentiment analysis in\nNorwegian, annotated with respect to polar expressions, targets and holders of\nopinion. The underlying texts are taken from a corpus of professionally\nauthored reviews from multiple news-sources and across a wide variety of\ndomains, including literature, games, music, products, movies and more. We here\npresent a detailed description of this annotation effort. We provide an\noverview of the developed annotation guidelines, illustrated with examples, and\npresent an analysis of inter-annotator agreement. We also report the first\nexperimental results on the dataset, intended as a preliminary benchmark for\nfurther experiments.", "published": "2019-11-28 14:09:50", "link": "http://arxiv.org/abs/1911.12722v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multimodal Machine Translation through Visuals and Speech", "abstract": "Multimodal machine translation involves drawing information from more than\none modality, based on the assumption that the additional modalities will\ncontain useful alternative views of the input data. The most prominent tasks in\nthis area are spoken language translation, image-guided translation, and\nvideo-guided translation, which exploit audio and visual modalities,\nrespectively. These tasks are distinguished from their monolingual counterparts\nof speech recognition, image captioning, and video captioning by the\nrequirement of models to generate outputs in a different language. This survey\nreviews the major data resources for these tasks, the evaluation campaigns\nconcentrated around them, the state of the art in end-to-end and pipeline\napproaches, and also the challenges in performance evaluation. The paper\nconcludes with a discussion of directions for future research in these areas:\nthe need for more expansive and challenging datasets, for targeted evaluations\nof model performance, and for multimodality in both the input and output space.", "published": "2019-11-28 17:18:41", "link": "http://arxiv.org/abs/1911.12798v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings\n  and Grammatical Errors", "abstract": "The lack of large-scale datasets has been a major hindrance to the\ndevelopment of NLP tasks such as spelling correction and grammatical error\ncorrection (GEC). As a complementary new resource for these tasks, we present\nthe GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and\ngrammatical errors along with their corrections harvested from GitHub, a large\nand popular platform for hosting and sharing git repositories. The dataset,\nwhich we have made publicly available, contains more than 350k edits and 65M\ncharacters in more than 15 languages, making it the largest dataset of\nmisspellings to date. We also describe our process for filtering true typo\nedits based on learned classifiers on a small annotated subset, and demonstrate\nthat typo edits can be identified with F1 ~ 0.9 using a very simple classifier\nwith only three features. The detailed analyses of the dataset show that\nexisting spelling correctors merely achieve an F-measure of approx. 0.5,\nsuggesting that the dataset serves as a new, rich source of spelling errors\nthat complement existing datasets.", "published": "2019-11-28 22:57:45", "link": "http://arxiv.org/abs/1911.12893v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Can We Know What Language Models Know?", "abstract": "Recent work has presented intriguing results examining the knowledge\ncontained in language models (LM) by having the LM fill in the blanks of\nprompts such as \"Obama is a _ by profession\". These prompts are usually\nmanually created, and quite possibly sub-optimal; another prompt such as \"Obama\nworked as a _\" may result in more accurately predicting the correct profession.\nBecause of this, given an inappropriate prompt, we might fail to retrieve facts\nthat the LM does know, and thus any given prompt only provides a lower bound\nestimate of the knowledge contained in an LM. In this paper, we attempt to more\naccurately estimate the knowledge contained in LMs by automatically discovering\nbetter prompts to use in this querying process. Specifically, we propose\nmining-based and paraphrasing-based methods to automatically generate\nhigh-quality and diverse prompts, as well as ensemble methods to combine\nanswers from different prompts. Extensive experiments on the LAMA benchmark for\nextracting relational knowledge from LMs demonstrate that our methods can\nimprove accuracy from 31.1% to 39.6%, providing a tighter lower bound on what\nLMs know. We have released the code and the resulting LM Prompt And Query\nArchive (LPAQA) at https://github.com/jzbjyb/LPAQA.", "published": "2019-11-28 05:55:42", "link": "http://arxiv.org/abs/1911.12543v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "DiscoTK: Using Discourse Structure for Machine Translation Evaluation", "abstract": "We present novel automatic metrics for machine translation evaluation that\nuse discourse structure and convolution kernels to compare the discourse tree\nof an automatic translation with that of the human reference. We experiment\nwith five transformations and augmentations of a base discourse tree\nrepresentation based on the rhetorical structure theory, and we combine the\nkernel scores for each of them into a single score. Finally, we add other\nmetrics from the ASIYA MT evaluation toolkit, and we tune the weights of the\ncombination on actual human judgments. Experiments on the WMT12 and WMT13\nmetrics shared task datasets show correlation with human judgments that\noutperforms what the best systems that participated in these years achieved,\nboth at the segment and at the system level.", "published": "2019-11-28 06:05:12", "link": "http://arxiv.org/abs/1911.12547v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "KPTimes: A Large-Scale Dataset for Keyphrase Generation on News\n  Documents", "abstract": "Keyphrase generation is the task of predicting a set of lexical units that\nconveys the main content of a source text. Existing datasets for keyphrase\ngeneration are only readily available for the scholarly domain and include\nnon-expert annotations. In this paper we present KPTimes, a large-scale dataset\nof news texts paired with editor-curated keyphrases. Exploring the dataset, we\nshow how editors tag documents, and how their annotations differ from those\nfound in existing datasets. We also train and evaluate state-of-the-art neural\nkeyphrase generation models on KPTimes to gain insights on how well they\nperform on the news domain. The dataset is available online at\nhttps://github.com/ygorg/KPTimes .", "published": "2019-11-28 07:12:30", "link": "http://arxiv.org/abs/1911.12559v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Inducing Relational Knowledge from BERT", "abstract": "One of the most remarkable properties of word embeddings is the fact that\nthey capture certain types of semantic and syntactic relationships. Recently,\npre-trained language models such as BERT have achieved groundbreaking results\nacross a wide range of Natural Language Processing tasks. However, it is\nunclear to what extent such models capture relational knowledge beyond what is\nalready captured by standard word embeddings. To explore this question, we\npropose a methodology for distilling relational knowledge from a pre-trained\nlanguage model. Starting from a few seed instances of a given relation, we\nfirst use a large text corpus to find sentences that are likely to express this\nrelation. We then use a subset of these extracted sentences as templates.\nFinally, we fine-tune a language model to predict whether a given word pair is\nlikely to be an instance of some relation, when given an instantiated template\nfor that relation as input.", "published": "2019-11-28 15:38:53", "link": "http://arxiv.org/abs/1911.12753v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Metre as a stylometric feature in Latin hexameter poetry", "abstract": "This paper demonstrates that metre is a privileged indicator of authorial\nstyle in classical Latin hexameter poetry. Using only metrical features,\npairwise classification experiments are performed between 5 first-century\nauthors (10 comparisons) using four different machine-learning models. The\nresults showed a two-label classification accuracy of at least 95% with samples\nas small as ten lines and no greater than eighty lines (up to around 500\nwords). These sample sizes are an order of magnitude smaller than those\ntypically recommended for BOW ('bag of words') or n-gram approaches, and the\nreported accuracy is outstanding. Additionally, this paper explores the\npotential for novelty (forgery) detection, or 'one-class classification'. An\nanalysis of the disputed Aldine Additamentum (Sil. Ital. Puni. 8:144-225)\nconcludes (p=0.0013) that the metrical style differs significantly from that of\nthe rest of the poem.", "published": "2019-11-28 01:35:51", "link": "http://arxiv.org/abs/1911.12478v2", "categories": ["cs.CL", "cs.LG", "stat.AP"], "primary_category": "cs.CL"}
{"title": "Minimum Bayes Risk Training of RNN-Transducer for End-to-End Speech\n  Recognition", "abstract": "In this work, we propose minimum Bayes risk (MBR) training of RNN-Transducer\n(RNN-T) for end-to-end speech recognition. Specifically, initialized with a\nRNN-T trained model, MBR training is conducted via minimizing the expected edit\ndistance between the reference label sequence and on-the-fly generated N-best\nhypothesis. We also introduce a heuristic to incorporate an external neural\nnetwork language model (NNLM) in RNN-T beam search decoding and explore MBR\ntraining with the external NNLM. Experimental results demonstrate an MBR\ntrained model outperforms a RNN-T trained model substantially and further\nimprovements can be achieved if trained with an external NNLM. Our best MBR\ntrained system achieves absolute character error rate (CER) reductions of 1.2%\nand 0.5% on read and spontaneous Mandarin speech respectively over a strong\nconvolution and transformer based RNN-T baseline trained on ~21,000 hours of\nspeech.", "published": "2019-11-28 02:17:56", "link": "http://arxiv.org/abs/1911.12487v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Language-Independent Sentiment Analysis Using Subjectivity and\n  Positional Information", "abstract": "We describe a novel language-independent approach to the task of determining\nthe polarity, positive or negative, of the author's opinion on a specific topic\nin natural language text. In particular, weights are assigned to attributes,\nindividual words or word bi-grams, based on their position and on their\nlikelihood of being subjective. The subjectivity of each attribute is estimated\nin a two-step process, where first the probability of being subjective is\ncalculated for each sentence containing the attribute, and then these\nprobabilities are used to alter the attribute's weights for polarity\nclassification. The evaluation results on a standard dataset of movie reviews\nshows 89.85% classification accuracy, which rivals the best previously\npublished results for this dataset for systems that use no additional\nlinguistic information nor external resources.", "published": "2019-11-28 05:55:44", "link": "http://arxiv.org/abs/1911.12544v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "RETRO: Relation Retrofitting For In-Database Machine Learning on Textual\n  Data", "abstract": "There are massive amounts of textual data residing in databases, valuable for\nmany machine learning (ML) tasks. Since ML techniques depend on numerical input\nrepresentations, word embeddings are increasingly utilized to convert symbolic\nrepresentations such as text into meaningful numbers. However, a naive\none-to-one mapping of each word in a database to a word embedding vector is not\nsufficient and would lead to poor accuracies in ML tasks. Thus, we argue to\nadditionally incorporate the information given by the database schema into the\nembedding, e.g. which words appear in the same column or are related to each\nother. In this paper, we propose RETRO (RElational reTROfitting), a novel\napproach to learn numerical representations of text values in databases,\ncapturing the best of both worlds, the rich information encoded by word\nembeddings and the relational information encoded by database tables. We\nformulate relation retrofitting as a learning problem and present an efficient\nalgorithm solving it. We investigate the impact of various hyperparameters on\nthe learning problem and derive good settings for all of them. Our evaluation\nshows that the proposed embeddings are ready-to-use for many ML tasks such as\nclassification and regression and even outperform state-of-the-art techniques\nin integration tasks such as null value imputation and link prediction.", "published": "2019-11-28 12:37:26", "link": "http://arxiv.org/abs/1911.12674v2", "categories": ["cs.DB", "cs.CL", "cs.LG", "H.2.8, H.3.3, I.2.7", "H.2.8; H.3.3; I.2.7"], "primary_category": "cs.DB"}
{"title": "Sentiment Analysis On Indian Indigenous Languages: A Review On\n  Multilingual Opinion Mining", "abstract": "An increase in the use of smartphones has laid to the use of the internet and\nsocial media platforms. The most commonly used social media platforms are\nTwitter, Facebook, WhatsApp and Instagram. People are sharing their personal\nexperiences, reviews, feedbacks on the web. The information which is available\non the web is unstructured and enormous. Hence, there is a huge scope of\nresearch on understanding the sentiment of the data available on the web.\nSentiment Analysis (SA) can be carried out on the reviews, feedbacks,\ndiscussions available on the web. There has been extensive research carried out\non SA in the English language, but data on the web also contains different\nother languages which should be analyzed. This paper aims to analyze, review\nand discuss the approaches, algorithms, challenges faced by the researchers\nwhile carrying out the SA on Indigenous languages.", "published": "2019-11-28 20:00:40", "link": "http://arxiv.org/abs/1911.12848v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Proq: Projection-based Runtime Assertions for Debugging on a Quantum\n  Computer", "abstract": "In this paper, we propose Proq, a runtime assertion scheme for testing and\ndebugging quantum programs on a quantum computer. The predicates in Proq are\nrepresented by projections (or equivalently, closed subspaces of the state\nspace), following Birkhoff-von Neumann quantum logic. The satisfaction of a\nprojection by a quantum state can be directly checked upon a small number of\nprojective measurements rather than a large number of repeated executions. On\nthe theory side, we rigorously prove that checking projection-based assertions\ncan help locate bugs or statistically assure that the semantic function of the\ntested program is close to what we expect, for both exact and approximate\nquantum programs. On the practice side, we consider hardware constraints and\nintroduce several techniques to transform the assertions, making them directly\nexecutable on the measurement-restricted quantum computers. We also propose to\nachieve simplified assertion implementation using local projection technique\nwith soundness guaranteed. We compare Proq with existing quantum program\nassertions and demonstrate the effectiveness and efficiency of Proq by its\napplications to assert two ingenious quantum algorithms, the\nHarrow-Hassidim-Lloyd algorithm and Shor's algorithm.", "published": "2019-11-28 20:24:11", "link": "http://arxiv.org/abs/1911.12855v2", "categories": ["cs.PL", "cs.CL", "cs.ET", "quant-ph"], "primary_category": "cs.PL"}
{"title": "Using VAEs and Normalizing Flows for One-shot Text-To-Speech Synthesis\n  of Expressive Speech", "abstract": "We propose a Text-to-Speech method to create an unseen expressive style using\none utterance of expressive speech of around one second. Specifically, we\nenhance the disentanglement capabilities of a state-of-the-art\nsequence-to-sequence based system with a Variational AutoEncoder (VAE) and a\nHouseholder Flow. The proposed system provides a 22% KL-divergence reduction\nwhile jointly improving perceptual metrics over state-of-the-art. At synthesis\ntime we use one example of expressive style as a reference input to the encoder\nfor generating any text in the desired style. Perceptual MUSHRA evaluations\nshow that we can create a voice with a 9% relative naturalness improvement over\nstandard Neural Text-to-Speech, while also improving the perceived emotional\nintensity (59 compared to the 55 of neutral speech).", "published": "2019-11-28 15:57:14", "link": "http://arxiv.org/abs/1911.12760v2", "categories": ["cs.LG", "cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Performance Comparison of UCA and UCCA based Real-time Sound Source\n  Localization Systems using Circular Harmonics SRP Method", "abstract": "Many sound source localization (SSL) algorithms based on circular microphone\narray (CMA), including uniform circular array (UCA) and uniform concentric\ncircular array (UCCA), have been well developed and verified via computer\nsimulations and offline processing. On the other hand, beamforming in the\nharmonic domain has been shown to be a very efficient tool for broadband sound\nsource localization due to its frequency-invariant properties. In this paper,\ndesign, implementation, and performance of a real-time sound source\nlocalization system are discussed. Specifically, we analyze the effect of\nparameter settings and compare the performance between UCA and UCCA through\nreal-time experiments in a real room. The proposed method shows significant\nimprovement by using UCCA instead of UCA.", "published": "2019-11-28 09:49:36", "link": "http://arxiv.org/abs/1911.12616v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Augmentation Methods on Monophonic Audio for Instrument Classification\n  in Polyphonic Music", "abstract": "Instrument classification is one of the fields in Music Information Retrieval\n(MIR) that has attracted a lot of research interest. However, the majority of\nthat is dealing with monophonic music, while efforts on polyphonic material\nmainly focus on predominant instrument recognition. In this paper, we propose\nan approach for instrument classification in polyphonic music from purely\nmonophonic data, that involves performing data augmentation by mixing different\naudio segments. A variety of data augmentation techniques focusing on different\nsonic aspects, such as overlaying audio segments of the same genre, as well as\npitch and tempo-based synchronization, are explored. We utilize Convolutional\nNeural Networks for the classification task, comparing shallow to deep network\narchitectures. We further investigate the usage of a combination of the above\nclassifiers, each trained on a single augmented dataset. An ensemble of\nVGG-like classifiers, trained on non-augmented, pitch-synchronized,\ntempo-synchronized and genre-similar excerpts, respectively, yields the best\nresults, achieving slightly above 80% in terms of label ranking average\nprecision (LRAP) in the IRMAS test set.ruments in over 2300 testing tracks.", "published": "2019-11-28 03:12:22", "link": "http://arxiv.org/abs/1911.12505v2", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Unsupervised Neural Mask Estimator For Generalized Eigen-Value\n  Beamforming Based ASR", "abstract": "The state-of-art methods for acoustic beamforming in multi-channel ASR are\nbased on a neural mask estimator that predicts the presence of speech and\nnoise. These models are trained using a paired corpus of clean and noisy\nrecordings (teacher model). In this paper, we attempt to move away from the\nrequirements of having supervised clean recordings for training the mask\nestimator. The models based on signal enhancement and beamforming using\nmulti-channel linear prediction serve as the required mask estimate. In this\nway, the model training can also be carried out on real recordings of noisy\nspeech rather than simulated ones alone done in a typical teacher model.\nSeveral experiments performed on noisy and reverberant environments in the\nCHiME-3 corpus as well as the REVERB challenge corpus highlight the\neffectiveness of the proposed approach. The ASR results for the proposed\napproach provide performances that are significantly better than a teacher\nmodel trained on an out-of-domain dataset and on par with the oracle mask\nestimators trained on the in-domain dataset.", "published": "2019-11-28 09:53:45", "link": "http://arxiv.org/abs/1911.12617v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Machine learning for music genre: multifaceted review and\n  experimentation with audioset", "abstract": "Music genre classification is one of the sub-disciplines of music information\nretrieval (MIR) with growing popularity among researchers, mainly due to the\nalready open challenges. Although research has been prolific in terms of number\nof published works, the topic still suffers from a problem in its foundations:\nthere is no clear and formal definition of what genre is. Music categorizations\nare vague and unclear, suffering from human subjectivity and lack of agreement.\nIn its first part, this paper offers a survey trying to cover the many\ndifferent aspects of the matter. Its main goal is give the reader an overview\nof the history and the current state-of-the-art, exploring techniques and\ndatasets used to the date, as well as identifying current challenges, such as\nthis ambiguity of genre definitions or the introduction of human-centric\napproaches. The paper pays special attention to new trends in machine learning\napplied to the music annotation problem. Finally, we also include a music genre\nclassification experiment that compares different machine learning models using\nAudioset.", "published": "2019-11-28 09:57:28", "link": "http://arxiv.org/abs/1911.12618v1", "categories": ["cs.SD", "cs.IR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASR is all you need: cross-modal distillation for lip reading", "abstract": "The goal of this work is to train strong models for visual speech recognition\nwithout requiring human annotated ground truth data. We achieve this by\ndistilling from an Automatic Speech Recognition (ASR) model that has been\ntrained on a large-scale audio-only corpus. We use a cross-modal distillation\nmethod that combines Connectionist Temporal Classification (CTC) with a\nframe-wise cross-entropy loss. Our contributions are fourfold: (i) we show that\nground truth transcriptions are not necessary to train a lip reading system;\n(ii) we show how arbitrary amounts of unlabelled video data can be leveraged to\nimprove performance; (iii) we demonstrate that distillation significantly\nspeeds up training; and, (iv) we obtain state-of-the-art results on the\nchallenging LRS2 and LRS3 datasets for training only on publicly available\ndata.", "published": "2019-11-28 15:15:27", "link": "http://arxiv.org/abs/1911.12747v2", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Three Orthogonal Dimensions for Psychoacoustic Sonification", "abstract": "Objective: Three perceptually orthogonal auditory dimensions for\nmultidimensional and multivariate data sonification are identified and\nexperimentally validated. Background: Psychoacoustic investigations have shown\nthat orthogonal acoustical parameters may interfere perceptually. The\nliterature hardly offers any solutions to this problem, and previous auditory\ndisplay approaches have failed to implement auditory dimensions that are\nperceived orthogonally by a user. In this study we demonstrate how a location\nin three-dimensional space can be sonified unambiguously by the implementation\nof perceptually orthogonal psychoacoustic attributes in monophonic playback.\nMethod: Perceptually orthogonal auditory attributes are identified from\nliterature research and experience in music and psychoacoustic research. We\ncarried out an experiment with 21 participants who identified sonified\nlocations in two-dimensional space. Results: With just 5 minutes of explanation\nand exploration, naive users can interpret our multidimensional sonification\nwith high accuracy. Conclusion: We identified a set of perceptually orthogonal\nauditory dimensions suitable for three-dimensional data sonification.\nApplication: Three-dimensional data sonification promises blind navigation,\ne.g. for unmanned vehicles, and reliable real-time monitoring of multivariate\ndata, e.g., in the patient care sector.", "published": "2019-11-28 13:19:50", "link": "http://arxiv.org/abs/1912.00766v1", "categories": ["cs.SD", "eess.AS", "q-bio.NC", "H.5.5, H.5.2, H.1.2, G.1.8", "H.5.5; H.5.2; H.1.2; G.1.8"], "primary_category": "cs.SD"}
