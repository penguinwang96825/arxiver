{"title": "Zero-Resource Knowledge-Grounded Dialogue Generation", "abstract": "While neural conversation models have shown great potentials towards\ngenerating informative and engaging responses via introducing external\nknowledge, learning such a model often requires knowledge-grounded dialogues\nthat are difficult to obtain. To overcome the data challenge and reduce the\ncost of building a knowledge-grounded dialogue system, we explore the problem\nunder a zero-resource setting by assuming no context-knowledge-response triples\nare needed for training. To this end, we propose representing the knowledge\nthat bridges a context and a response and the way that the knowledge is\nexpressed as latent variables, and devise a variational approach that can\neffectively estimate a generation model from a dialogue corpus and a knowledge\ncorpus that are independent with each other. Evaluation results on three\nbenchmarks of knowledge-grounded dialogue generation indicate that our model\ncan achieve comparable performance with state-of-the-art methods that rely on\nknowledge-grounded dialogues for training, and exhibits a good generalization\nability over different topics and different datasets.", "published": "2020-08-29 05:48:32", "link": "http://arxiv.org/abs/2008.12918v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Computation of Expectations under Spanning Tree Distributions", "abstract": "We give a general framework for inference in spanning tree models. We propose\nunified algorithms for the important cases of first-order expectations and\nsecond-order expectations in edge-factored, non-projective spanning-tree\nmodels. Our algorithms exploit a fundamental connection between gradients and\nexpectations, which allows us to derive efficient algorithms. These algorithms\nare easy to implement with or without automatic differentiation software. We\nmotivate the development of our framework with several \\emph{cautionary tales}\nof previous research, which has developed numerous inefficient algorithms for\ncomputing expectations and their gradients. We demonstrate how our framework\nefficiently computes several quantities with known algorithms, including the\nexpected attachment score, entropy, and generalized expectation criteria. As a\nbonus, we give algorithms for quantities that are missing in the literature,\nincluding the KL divergence. In all cases, our approach matches the efficiency\nof existing algorithms and, in several cases, reduces the runtime complexity by\na factor of the sentence length. We validate the implementation of our\nframework through runtime experiments. We find our algorithms are up to 15 and\n9 times faster than previous algorithms for computing the Shannon entropy and\nthe gradient of the generalized expectation objective, respectively.", "published": "2020-08-29 14:58:26", "link": "http://arxiv.org/abs/2008.12988v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data augmentation using prosody and false starts to recognize non-native\n  children's speech", "abstract": "This paper describes AaltoASR's speech recognition system for the INTERSPEECH\n2020 shared task on Automatic Speech Recognition (ASR) for non-native\nchildren's speech. The task is to recognize non-native speech from children of\nvarious age groups given a limited amount of speech. Moreover, the speech being\nspontaneous has false starts transcribed as partial words, which in the test\ntranscriptions leads to unseen partial words. To cope with these two\nchallenges, we investigate a data augmentation-based approach. Firstly, we\napply the prosody-based data augmentation to supplement the audio data.\nSecondly, we simulate false starts by introducing partial-word noise in the\nlanguage modeling corpora creating new words. Acoustic models trained on\nprosody-based augmented data outperform the models using the baseline recipe or\nthe SpecAugment-based augmentation. The partial-word noise also helps to\nimprove the baseline language model. Our ASR system, a combination of these\nschemes, is placed third in the evaluation period and achieves the word error\nrate of 18.71%. Post-evaluation period, we observe that increasing the amounts\nof prosody-based augmented data leads to better performance. Furthermore,\nremoving low-confidence-score words from hypotheses can lead to further gains.\nThese two improvements lower the ASR error rate to 17.99%.", "published": "2020-08-29 05:32:32", "link": "http://arxiv.org/abs/2008.12914v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "SocCogCom at SemEval-2020 Task 11: Characterizing and Detecting\n  Propaganda using Sentence-Level Emotional Salience Features", "abstract": "This paper describes a system developed for detecting propaganda techniques\nfrom news articles. We focus on examining how emotional salience features\nextracted from a news segment can help to characterize and predict the presence\nof propaganda techniques. Correlation analyses surfaced interesting patterns\nthat, for instance, the \"loaded language\" and \"slogan\" techniques are\nnegatively associated with valence and joy intensity but are positively\nassociated with anger, fear and sadness intensity. In contrast, \"flag waving\"\nand \"appeal to fear-prejudice\" have the exact opposite pattern. Through\npredictive experiments, results further indicate that whereas BERT-only\nfeatures obtained F1-score of 0.548, emotion intensity features and BERT hybrid\nfeatures were able to obtain F1-score of 0.570, when a simple feedforward\nnetwork was used as the classifier in both settings. On gold test data, our\nsystem obtained micro-averaged F1-score of 0.558 on overall detection efficacy\nover fourteen propaganda techniques. It performed relatively well in detecting\n\"loaded language\" (F1 = 0.772), \"name calling and labeling\" (F1 = 0.673),\n\"doubt\" (F1 = 0.604) and \"flag waving\" (F1 = 0.543).", "published": "2020-08-29 16:55:29", "link": "http://arxiv.org/abs/2008.13012v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Source-Aware Neural Speech Coding for Noisy Speech Compression", "abstract": "This paper introduces a novel neural network-based speech coding system that\ncan process noisy speech effectively. The proposed source-aware neural audio\ncoding (SANAC) system harmonizes a deep autoencoder-based source separation\nmodel and a neural coding system so that it can explicitly perform source\nseparation and coding in the latent space. An added benefit of this system is\nthat the codec can allocate a different amount of bits to the underlying\nsources so that the more important source sounds better in the decoded signal.\nWe target a new use case where the user on the receiver side cares about the\nquality of the non-speech components in speech communication, while the speech\nsource still carries the most crucial information. Both objective and\nsubjective evaluation tests show that SANAC can recover the original noisy\nspeech better than the baseline neural audio coding system, which is with no\nsource-aware coding mechanism, and two conventional codecs.", "published": "2020-08-29 01:22:12", "link": "http://arxiv.org/abs/2008.12889v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
