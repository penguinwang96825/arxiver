{"title": "Multilingual Modal Sense Classification using a Convolutional Neural\n  Network", "abstract": "Modal sense classification (MSC) is a special WSD task that depends on the\nmeaning of the proposition in the modal's scope. We explore a CNN architecture\nfor classifying modal sense in English and German. We show that CNNs are\nsuperior to manually designed feature-based classifiers and a standard NN\nclassifier. We analyze the feature maps learned by the CNN and identify known\nand previously unattested linguistic features. We benchmark the CNN on a\nstandard WSD task, where it compares favorably to models using\nsense-disambiguated target vectors.", "published": "2016-08-18 11:41:45", "link": "http://arxiv.org/abs/1608.05243v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DNN-based Speech Synthesis for Indian Languages from ASCII text", "abstract": "Text-to-Speech synthesis in Indian languages has a seen lot of progress over\nthe decade partly due to the annual Blizzard challenges. These systems assume\nthe text to be written in Devanagari or Dravidian scripts which are nearly\nphonemic orthography scripts. However, the most common form of computer\ninteraction among Indians is ASCII written transliterated text. Such text is\ngenerally noisy with many variations in spelling for the same word. In this\npaper we evaluate three approaches to synthesize speech from such noisy ASCII\ntext: a naive Uni-Grapheme approach, a Multi-Grapheme approach, and a\nsupervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the\nASCII text to a phonetic script, and then learn a Deep Neural Network to\nsynthesize speech from that. We train and test our models on Blizzard Challenge\ndatasets that were transliterated to ASCII using crowdsourcing. Our experiments\non Hindi, Tamil and Telugu demonstrate that our models generate speech of\ncompetetive quality from ASCII text compared to the speech synthesized from the\nnative scripts. All the accompanying transliterated datasets are released for\npublic access.", "published": "2016-08-18 18:58:39", "link": "http://arxiv.org/abs/1608.05374v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from\n  Sentence Alignments", "abstract": "While cross-lingual word embeddings have been studied extensively in recent\nyears, the qualitative differences between the different algorithms remain\nvague. We observe that whether or not an algorithm uses a particular feature\nset (sentence IDs) accounts for a significant performance gap among these\nalgorithms. This feature set is also used by traditional alignment algorithms,\nsuch as IBM Model-1, which demonstrate similar performance to state-of-the-art\nembedding algorithms on a variety of benchmarks. Overall, we observe that\ndifferent algorithmic approaches for utilizing the sentence ID feature space\nresult in similar performance. This paper draws both empirical and theoretical\nparallels between the embedding and alignment literature, and suggests that\nadding additional sources of information, which go beyond the traditional\nsignal of bilingual sentence-aligned corpora, may substantially improve\ncross-lingual word embeddings, and that future baselines should at least take\nsuch features into account.", "published": "2016-08-18 20:27:46", "link": "http://arxiv.org/abs/1608.05426v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
