{"title": "What Does BERT Look At? An Analysis of BERT's Attention", "abstract": "Large pre-trained neural networks such as BERT have had great recent success\nin NLP, motivating a growing body of research investigating what aspects of\nlanguage they are able to learn from unlabeled data. Most recent analysis has\nfocused on model outputs (e.g., language model surprisal) or internal vector\nrepresentations (e.g., probing classifiers). Complementary to these works, we\npropose methods for analyzing the attention mechanisms of pre-trained models\nand apply them to BERT. BERT's attention heads exhibit patterns such as\nattending to delimiter tokens, specific positional offsets, or broadly\nattending over the whole sentence, with heads in the same layer often\nexhibiting similar behaviors. We further show that certain attention heads\ncorrespond well to linguistic notions of syntax and coreference. For example,\nwe find heads that attend to the direct objects of verbs, determiners of nouns,\nobjects of prepositions, and coreferent mentions with remarkably high accuracy.\nLastly, we propose an attention-based probing classifier and use it to further\ndemonstrate that substantial syntactic information is captured in BERT's\nattention.", "published": "2019-06-11 01:31:41", "link": "http://arxiv.org/abs/1906.04341v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Document-grounded Matching Network for Response Selection in\n  Retrieval-based Chatbots", "abstract": "We present a document-grounded matching network (DGMN) for response selection\nthat can power a knowledge-aware retrieval-based chatbot system. The challenges\nof building such a model lie in how to ground conversation contexts with\nbackground documents and how to recognize important information in the\ndocuments for matching. To overcome the challenges, DGMN fuses information in a\ndocument and a context into representations of each other, and dynamically\ndetermines if grounding is necessary and importance of different parts of the\ndocument and the context through hierarchical interaction with a response at\nthe matching step. Empirical studies on two public data sets indicate that DGMN\ncan significantly improve upon state-of-the-art methods and at the same time\nenjoys good interpretability.", "published": "2019-06-11 03:00:34", "link": "http://arxiv.org/abs/1906.04362v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DoubleTransfer at MEDIQA 2019: Multi-Source Transfer Learning for\n  Natural Language Understanding in the Medical Domain", "abstract": "This paper describes our competing system to enter the MEDIQA-2019\ncompetition. We use a multi-source transfer learning approach to transfer the\nknowledge from MT-DNN and SciBERT to natural language understanding tasks in\nthe medical domain. For transfer learning fine-tuning, we use multi-task\nlearning on NLI, RQE and QA tasks on general and medical domains to improve\nperformance. The proposed methods are proved effective for natural language\nunderstanding in the medical domain, and we rank the first place on the QA\ntask.", "published": "2019-06-11 04:07:37", "link": "http://arxiv.org/abs/1906.04382v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning a Matching Model with Co-teaching for Multi-turn Response\n  Selection in Retrieval-based Dialogue Systems", "abstract": "We study learning of a matching model for response selection in\nretrieval-based dialogue systems. The problem is equally important with\ndesigning the architecture of a model, but is less explored in existing\nliterature. To learn a robust matching model from noisy training data, we\npropose a general co-teaching framework with three specific teaching strategies\nthat cover both teaching with loss functions and teaching with data curriculum.\nUnder the framework, we simultaneously learn two matching models with\nindependent training sets. In each iteration, one model transfers the knowledge\nlearned from its training set to the other model, and at the same time receives\nthe guide from the other model on how to overcome noise in training. Through\nbeing both a teacher and a student, the two models learn from each other and\nget improved together. Evaluation results on two public data sets indicate that\nthe proposed learning approach can generally and significantly improve the\nperformance of existing matching models.", "published": "2019-06-11 06:55:04", "link": "http://arxiv.org/abs/1906.04413v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Learning for Contextualized Extractive Summarization", "abstract": "Existing models for extractive summarization are usually trained from scratch\nwith a cross-entropy loss, which does not explicitly capture the global context\nat the document level. In this paper, we aim to improve this task by\nintroducing three auxiliary pre-training tasks that learn to capture the\ndocument-level context in a self-supervised fashion. Experiments on the\nwidely-used CNN/DM dataset validate the effectiveness of the proposed auxiliary\ntasks. Furthermore, we show that after pre-training, a clean model with simple\nbuilding blocks is able to outperform previous state-of-the-art that are\ncarefully designed.", "published": "2019-06-11 09:53:17", "link": "http://arxiv.org/abs/1906.04466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in\n  Languages with Rich Morphology", "abstract": "Gender stereotypes are manifest in most of the world's languages and are\nconsequently propagated or amplified by NLP systems. Although research has\nfocused on mitigating gender stereotypes in English, the approaches that are\ncommonly employed produce ungrammatical sentences in morphologically rich\nlanguages. We present a novel approach for converting between\nmasculine-inflected and feminine-inflected sentences in such languages. For\nSpanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level\nof tags and accuracies of 90% and 87% at the level of forms. By evaluating our\napproach using four different languages, we show that, on average, it reduces\ngender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.", "published": "2019-06-11 13:22:24", "link": "http://arxiv.org/abs/1906.04571v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading\n  Comprehension", "abstract": "This paper considers the reading comprehension task in which multiple\ndocuments are given as input. Prior work has shown that a pipeline of\nretriever, reader, and reranker can improve the overall performance. However,\nthe pipeline system is inefficient since the input is re-encoded within each\nmodule, and is unable to leverage upstream components to help downstream\ntraining. In this work, we present RE$^3$QA, a unified question answering model\nthat combines context retrieving, reading comprehension, and answer reranking\nto predict the final answer. Unlike previous pipelined approaches, RE$^3$QA\nshares contextualized text representation across different components, and is\ncarefully designed to use high-quality upstream outputs (e.g., retrieved\ncontext or candidate answers) for directly supervising downstream modules\n(e.g., the reader or the reranker). As a result, the whole network can be\ntrained end-to-end to avoid the context inconsistency problem. Experiments show\nthat our model outperforms the pipelined baseline and achieves state-of-the-art\nresults on two versions of TriviaQA and two variants of SQuAD.", "published": "2019-06-11 14:18:43", "link": "http://arxiv.org/abs/1906.04618v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Journal Name Extraction from Japanese Scientific News Articles", "abstract": "In Japanese scientific news articles, although the research results are\ndescribed clearly, the article's sources tend to be uncited. This makes it\ndifficult for readers to know the details of the research. In this paper, we\naddress the task of extracting journal names from Japanese scientific news\narticles. We hypothesize that a journal name is likely to occur in a specific\ncontext. To support the hypothesis, we construct a character-based method and\nextract journal names using this method. This method only uses the left and\nright context features of journal names. The results of the journal name\nextractions suggest that the distribution hypothesis plays an important role in\nidentifying the journal names.", "published": "2019-06-11 15:35:43", "link": "http://arxiv.org/abs/1906.04655v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Summaries with Topic Templates and Structured Convolutional\n  Decoders", "abstract": "Existing neural generation approaches create multi-sentence text as a single\nsequence. In this paper we propose a structured convolutional decoder that is\nguided by the content structure of target summaries. We compare our model with\nexisting sequential decoders on three data sets representing different domains.\nAutomatic and human evaluation demonstrate that our summaries have better\ncontent coverage.", "published": "2019-06-11 16:39:11", "link": "http://arxiv.org/abs/1906.04687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HEAD-QA: A Healthcare Dataset for Complex Reasoning", "abstract": "We present HEAD-QA, a multi-choice question answering testbed to encourage\nresearch on complex reasoning. The questions come from exams to access a\nspecialized position in the Spanish healthcare system, and are challenging even\nfor highly specialized humans. We then consider monolingual (Spanish) and\ncross-lingual (to English) experiments with information retrieval and neural\ntechniques. We show that: (i) HEAD-QA challenges current methods, and (ii) the\nresults lag well behind human performance, demonstrating its usefulness as a\nbenchmark for future work.", "published": "2019-06-11 17:06:49", "link": "http://arxiv.org/abs/1906.04701v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Kind of Language Is Hard to Language-Model?", "abstract": "How language-agnostic are current state-of-the-art NLP tools? Are there some\ntypes of language that are easier to model with current methods? In prior work\n(Cotterell et al., 2018) we attempted to address this question for language\nmodeling, and observed that recurrent neural network language models do not\nperform equally well over all the high-resource European languages found in the\nEuroparl corpus. We speculated that inflectional morphology may be the primary\nculprit for the discrepancy. In this paper, we extend these earlier experiments\nto cover 69 languages from 13 language families using a multilingual Bible\ncorpus. Methodologically, we introduce a new paired-sample multiplicative\nmixed-effects model to obtain language difficulty coefficients from\nat-least-pairwise parallel corpora. In other words, the model is aware of\ninter-sentence variation and can handle missing data. Exploiting this model, we\nshow that \"translationese\" is not any easier to model than natively written\nlanguage in a fair comparison. Trying to answer the question of what features\ndifficult languages have in common, we try and fail to reproduce our earlier\n(Cotterell et al., 2018) observation about morphological complexity and instead\nreveal far simpler statistics of the data that seem to drive complexity in a\nmuch larger sample.", "published": "2019-06-11 17:56:08", "link": "http://arxiv.org/abs/1906.04726v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Discovery of Gendered Language through Latent-Variable\n  Modeling", "abstract": "Studying the ways in which language is gendered has long been an area of\ninterest in sociolinguistics. Studies have explored, for example, the speech of\nmale and female characters in film and the language used to describe male and\nfemale politicians. In this paper, we aim not to merely study this phenomenon\nqualitatively, but instead to quantify the degree to which the language used to\ndescribe men and women is different and, moreover, different in a positive or\nnegative way. To that end, we introduce a generative latent-variable model that\njointly represents adjective (or verb) choice, with its sentiment, given the\nnatural gender of a head (or dependent) noun. We find that there are\nsignificant differences between descriptions of male and female nouns and that\nthese differences align with common gender stereotypes: Positive adjectives\nused to describe women are more often related to their bodies than adjectives\nused to describe men.", "published": "2019-06-11 18:18:29", "link": "http://arxiv.org/abs/1906.04760v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PerspectroScope: A Window to the World of Diverse Perspectives", "abstract": "This work presents PerspectroScope, a web-based system which lets users query\na discussion-worthy natural language claim, and extract and visualize various\nperspectives in support or against the claim, along with evidence supporting\neach perspective. The system thus lets users explore various perspectives that\ncould touch upon aspects of the issue at hand.The system is built as a\ncombination of retrieval engines and learned textual-entailment-like\nclassifiers built using a few recent developments in natural language\nunderstanding. To make the system more adaptive, expand its coverage, and\nimprove its decisions over time, our platform employs various mechanisms to get\ncorrections from the users.\n  PerspectroScope is available at github.com/CogComp/perspectroscope.", "published": "2019-06-11 18:18:39", "link": "http://arxiv.org/abs/1906.04761v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Systematic Comparison of English Noun Compound Representations", "abstract": "Building meaningful representations of noun compounds is not trivial since\nmany of them scarcely appear in the corpus. To that end, composition functions\napproximate the distributional representation of a noun compound by combining\nits constituent distributional vectors. In the more general case, phrase\nembeddings have been trained by minimizing the distance between the vectors\nrepresenting paraphrases. We compare various types of noun compound\nrepresentations, including distributional, compositional, and paraphrase-based\nrepresentations, through a series of tasks and analyses, and with an extensive\nnumber of underlying word embeddings. We find that indeed, in most cases,\ncomposition functions produce higher quality representations than\ndistributional ones, and they improve with computational power. No single\nfunction performs best in all scenarios, suggesting that a joint training\nobjective may produce improved representations.", "published": "2019-06-11 19:03:09", "link": "http://arxiv.org/abs/1906.04772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unmasking Bias in News", "abstract": "We present experiments on detecting hyperpartisanship in news using a\n'masking' method that allows us to assess the role of style vs. content for the\ntask at hand. Our results corroborate previous research on this task in that\ntopic related features yield better results than stylistic ones. We\nadditionally show that competitive results can be achieved by simply including\nhigher-length n-grams, which suggests the need to develop more challenging\ndatasets and tasks that address implicit and more subtle forms of bias.", "published": "2019-06-11 21:37:51", "link": "http://arxiv.org/abs/1906.04836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cued@wmt19:ewc&lms", "abstract": "Two techniques provide the fabric of the Cambridge University Engineering\nDepartment's (CUED) entry to the WMT19 evaluation campaign: elastic weight\nconsolidation (EWC) and different forms of language modelling (LMs). We report\nsubstantial gains by fine-tuning very strong baselines on former WMT test sets\nusing a combination of checkpoint averaging and EWC. A sentence-level\nTransformer LM and a document-level LM based on a modified Transformer\narchitecture yield further gains. As in previous years, we also extract\n$n$-gram probabilities from SMT lattices which can be seen as a\nsource-conditioned $n$-gram LM.", "published": "2019-06-11 17:49:57", "link": "http://arxiv.org/abs/1906.05447v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine\n  Decoding", "abstract": "Generating long and informative review text is a challenging natural language\ngeneration task. Previous work focuses on word-level generation, neglecting the\nimportance of topical and syntactic characteristics from natural languages. In\nthis paper, we propose a novel review generation model by characterizing an\nelaborately designed aspect-aware coarse-to-fine generation process. First, we\nmodel the aspect transitions to capture the overall content flow. Then, to\ngenerate a sentence, an aspect-aware sketch will be predicted using an\naspect-aware decoder. Finally, another decoder fills in the semantic slots by\ngenerating corresponding words. Our approach is able to jointly utilize aspect\nsemantics, syntactic sketch, and context information. Extensive experiments\nresults have demonstrated the effectiveness of the proposed model.", "published": "2019-06-11 06:59:56", "link": "http://arxiv.org/abs/1906.05667v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Translating Translationese: A Two-Step Approach to Unsupervised Machine\n  Translation", "abstract": "Given a rough, word-by-word gloss of a source language sentence, target\nlanguage natives can uncover the latent, fully-fluent rendering of the\ntranslation. In this work we explore this intuition by breaking translation\ninto a two step process: generating a rough gloss by means of a dictionary and\nthen `translating' the resulting pseudo-translation, or `Translationese' into a\nfully fluent translation. We build our Translationese decoder once from a\nmish-mash of parallel data that has the target language in common and then can\nbuild dictionaries on demand using unsupervised techniques, resulting in\nrapidly generated unsupervised neural MT systems for many source languages. We\napply this process to 14 test languages, obtaining better or comparable\ntranslation results on high-resource languages than previously published\nunsupervised MT studies, and obtaining good quality results for low-resource\nlanguages that have never been used in an unsupervised MT scenario.", "published": "2019-06-11 17:56:29", "link": "http://arxiv.org/abs/1906.05683v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Federated Learning for Emoji Prediction in a Mobile Keyboard", "abstract": "We show that a word-level recurrent neural network can predict emoji from\ntext typed on a mobile keyboard. We demonstrate the usefulness of transfer\nlearning for predicting emoji by pretraining the model using a language\nmodeling task. We also propose mechanisms to trigger emoji and tune the\ndiversity of candidates. The model is trained using a distributed on-device\nlearning framework called federated learning. The federated model is shown to\nachieve better performance than a server-trained model. This work demonstrates\nthe feasibility of using federated learning to train production-quality models\nfor natural language understanding tasks while keeping users' data on their\ndevices.", "published": "2019-06-11 00:40:33", "link": "http://arxiv.org/abs/1906.04329v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Parallel Scheduled Sampling", "abstract": "Auto-regressive models are widely used in sequence generation problems. The\noutput sequence is typically generated in a predetermined order, one discrete\nunit (pixel or word or character) at a time. The models are trained by\nteacher-forcing where ground-truth history is fed to the model as input, which\nat test time is replaced by the model prediction. Scheduled Sampling aims to\nmitigate this discrepancy between train and test time by randomly replacing\nsome discrete units in the history with the model's prediction. While\nteacher-forced training works well with ML accelerators as the computation can\nbe parallelized across time, Scheduled Sampling involves undesirable sequential\nprocessing. In this paper, we introduce a simple technique to parallelize\nScheduled Sampling across time. Experimentally, we find the proposed technique\nleads to equivalent or better performance on image generation, summarization,\ndialog generation, and translation compared to teacher-forced training. In\ndialog response generation task, Parallel Scheduled Sampling achieves 1.6 BLEU\nscore (11.5%) improvement over teacher-forcing while in image generation it\nachieves 20% and 13.8% improvement in Frechet Inception Distance (FID) and\nInception Score (IS) respectively. Further, we discuss the effects of different\nhyper-parameters associated with Scheduled Sampling on the model performance.", "published": "2019-06-11 00:43:38", "link": "http://arxiv.org/abs/1906.04331v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lightweight and Efficient Neural Natural Language Processing with\n  Quaternion Networks", "abstract": "Many state-of-the-art neural models for NLP are heavily parameterized and\nthus memory inefficient. This paper proposes a series of lightweight and memory\nefficient neural architectures for a potpourri of natural language processing\n(NLP) tasks. To this end, our models exploit computation using Quaternion\nalgebra and hypercomplex spaces, enabling not only expressive inter-component\ninteractions but also significantly ($75\\%$) reduced parameter size due to\nlesser degrees of freedom in the Hamilton product. We propose Quaternion\nvariants of models, giving rise to new architectures such as the Quaternion\nattention Model and Quaternion Transformer. Extensive experiments on a battery\nof NLP tasks demonstrates the utility of proposed Quaternion-inspired models,\nenabling up to $75\\%$ reduction in parameter size without significant loss in\nperformance.", "published": "2019-06-11 04:56:17", "link": "http://arxiv.org/abs/1906.04393v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning of Minimalist Numeral Grammars", "abstract": "Speech-controlled user interfaces facilitate the operation of devices and\nhousehold functions to laymen. State-of-the-art language technology scans the\nacoustically analyzed speech signal for relevant keywords that are subsequently\ninserted into semantic slots to interpret the user's intent. In order to\ndevelop proper cognitive information and communication technologies, simple\nslot-filling should be replaced by utterance meaning transducers (UMT) that are\nbased on semantic parsers and a \\emph{mental lexicon}, comprising syntactic,\nphonetic and semantic features of the language under consideration. This\nlexicon must be acquired by a cognitive agent during interaction with its\nusers. We outline a reinforcement learning algorithm for the acquisition of the\nsyntactic morphology and arithmetic semantics of English numerals, based on\nminimalist grammar (MG), a recent computational implementation of generative\nlinguistics. Number words are presented to the agent by a teacher in form of\nutterance meaning pairs (UMP) where the meanings are encoded as arithmetic\nterms from a suitable term algebra. Since MG encodes universal linguistic\ncompetence through inference rules, thereby separating innate linguistic\nknowledge from the contingently acquired lexicon, our approach unifies\ngenerative grammar and reinforcement learning, hence potentially resolving the\nstill pending Chomsky-Skinner controversy.", "published": "2019-06-11 08:54:23", "link": "http://arxiv.org/abs/1906.04447v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Relationship-Embedded Representation Learning for Grounding Referring\n  Expressions", "abstract": "Grounding referring expressions in images aims to locate the object instance\nin an image described by a referring expression. It involves a joint\nunderstanding of natural language and image content, and is essential for a\nrange of visual tasks related to human-computer interaction. As a\nlanguage-to-vision matching task, the core of this problem is to not only\nextract all the necessary information (i.e., objects and the relationships\namong them) in both the image and referring expression, but also make full use\nof context information to align cross-modal semantic concepts in the extracted\ninformation. Unfortunately, existing work on grounding referring expressions\nfails to accurately extract multi-order relationships from the referring\nexpression and associate them with the objects and their related contexts in\nthe image. In this paper, we propose a Cross-Modal Relationship Extractor\n(CMRE) to adaptively highlight objects and relationships (spatial and semantic\nrelations) related to the given expression with a cross-modal attention\nmechanism, and represent the extracted information as a language-guided visual\nrelation graph. In addition, we propose a Gated Graph Convolutional Network\n(GGCN) to compute multimodal semantic contexts by fusing information from\ndifferent modes and propagating multimodal information in the structured\nrelation graph. Experimental results on three common benchmark datasets show\nthat our Cross-Modal Relationship Inference Network, which consists of CMRE and\nGGCN, significantly surpasses all existing state-of-the-art methods. Code is\navailable at https://github.com/sibeiyang/sgmn/tree/master/lib/cmrin_models", "published": "2019-06-11 09:47:26", "link": "http://arxiv.org/abs/1906.04464v3", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Modeling Sentiment Dependencies with Graph Convolutional Networks for\n  Aspect-level Sentiment Classification", "abstract": "Aspect-level sentiment classification aims to distinguish the sentiment\npolarities over one or more aspect terms in a sentence. Existing approaches\nmostly model different aspects in one sentence independently, which ignore the\nsentiment dependencies between different aspects. However, we find such\ndependency information between different aspects can bring additional valuable\ninformation. In this paper, we propose a novel aspect-level sentiment\nclassification model based on graph convolutional networks (GCN) which can\neffectively capture the sentiment dependencies between multi-aspects in one\nsentence. Our model firstly introduces bidirectional attention mechanism with\nposition encoding to model aspect-specific representations between each aspect\nand its context words, then employs GCN over the attention mechanism to capture\nthe sentiment dependencies between different aspects in one sentence. We\nevaluate the proposed approach on the SemEval 2014 datasets. Experiments show\nthat our model outperforms the state-of-the-art methods. We also conduct\nexperiments to evaluate the effectiveness of GCN module, which indicates that\nthe dependencies between different aspects is highly helpful in aspect-level\nsentiment classification.", "published": "2019-06-11 11:26:25", "link": "http://arxiv.org/abs/1906.04501v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Inter-sentence Relation Extraction with Document-level Graph\n  Convolutional Neural Network", "abstract": "Inter-sentence relation extraction deals with a number of complex semantic\nrelationships in documents, which require local, non-local, syntactic and\nsemantic dependencies. Existing methods do not fully exploit such dependencies.\nWe present a novel inter-sentence relation extraction model that builds a\nlabelled edge graph convolutional neural network model on a document-level\ngraph. The graph is constructed using various inter- and intra-sentence\ndependencies to capture local and non-local dependency information. In order to\npredict the relation of an entity pair, we utilise multi-instance learning with\nbi-affine pairwise scoring. Experimental results show that our model achieves\ncomparable performance to the state-of-the-art neural models on two\nbiochemistry datasets. Our analysis shows that all the types in the graph are\neffective for inter-sentence relation extraction.", "published": "2019-06-11 16:30:27", "link": "http://arxiv.org/abs/1906.04684v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Using Structured Representation and Data: A Hybrid Model for Negation\n  and Sentiment in Customer Service Conversations", "abstract": "Twitter customer service interactions have recently emerged as an effective\nplatform to respond and engage with customers. In this work, we explore the\nrole of negation in customer service interactions, particularly applied to\nsentiment analysis. We define rules to identify true negation cues and scope\nmore suited to conversational data than existing general review data. Using\nsemantic knowledge and syntactic structure from constituency parse trees, we\npropose an algorithm for scope detection that performs comparable to state of\nthe art BiLSTM. We further investigate the results of negation scope detection\nfor the sentiment prediction task on customer service conversation data using\nboth a traditional SVM and a Neural Network. We propose an antonym dictionary\nbased method for negation applied to a CNN-LSTM combination model for sentiment\nanalysis. Experimental results show that the antonym-based method outperforms\nthe previous lexicon-based and neural network methods.", "published": "2019-06-11 17:15:32", "link": "http://arxiv.org/abs/1906.04706v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Labeling, Cutting, Grouping: an Efficient Text Line Segmentation Method\n  for Medieval Manuscripts", "abstract": "This paper introduces a new way for text-line extraction by integrating\ndeep-learning based pre-classification and state-of-the-art segmentation\nmethods. Text-line extraction in complex handwritten documents poses a\nsignificant challenge, even to the most modern computer vision algorithms.\nHistorical manuscripts are a particularly hard class of documents as they\npresent several forms of noise, such as degradation, bleed-through, interlinear\nglosses, and elaborated scripts. In this work, we propose a novel method which\nuses semantic segmentation at pixel level as intermediate task, followed by a\ntext-line extraction step. We measured the performance of our method on a\nrecent dataset of challenging medieval manuscripts and surpassed\nstate-of-the-art results by reducing the error by 80.7%. Furthermore, we\ndemonstrate the effectiveness of our approach on various other datasets written\nin different scripts. Hence, our contribution is two-fold. First, we\ndemonstrate that semantic pixel segmentation can be used as strong denoising\npre-processing step before performing text line extraction. Second, we\nintroduce a novel, simple and robust algorithm that leverages the high-quality\nsemantic segmentation to achieve a text-line extraction performance of 99.42%\nline IU on a challenging dataset.", "published": "2019-06-11 11:06:43", "link": "http://arxiv.org/abs/1906.11894v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "From Fully Supervised to Zero Shot Settings for Twitter Hashtag\n  Recommendation", "abstract": "We propose a comprehensive end-to-end pipeline for Twitter hashtags\nrecommendation system including data collection, supervised training setting\nand zero shot training setting. In the supervised training setting, we have\nproposed and compared the performance of various deep learning architectures,\nnamely Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and\nTransformer Network. However, it is not feasible to collect data for all\npossible hashtag labels and train a classifier model on them. To overcome this\nlimitation, we propose a Zero Shot Learning (ZSL) paradigm for predicting\nunseen hashtag labels by learning the relationship between the semantic space\nof tweets and the embedding space of hashtag labels. We evaluated various\nstate-of-the-art ZSL methods like Convex combination of Semantic Embedding\n(ConSE), Embarrassingly Simple Zero-Shot Learning (ESZSL) and Deep Embedding\nModel for Zero-Shot Learning (DEM-ZSL) for the hashtag recommendation task. We\ndemonstrate the effectiveness and scalability of ZSL methods for the\nrecommendation of unseen hashtags. To the best of our knowledge, this is the\nfirst quantitative evaluation of ZSL methods to date for unseen hashtags\nrecommendations from tweet text.", "published": "2019-06-11 17:38:28", "link": "http://arxiv.org/abs/1906.04914v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Calibration, Entropy Rates, and Memory in Language Models", "abstract": "Building accurate language models that capture meaningful long-term\ndependencies is a core challenge in natural language processing. Towards this\nend, we present a calibration-based approach to measure long-term discrepancies\nbetween a generative sequence model and the true distribution, and use these\ndiscrepancies to improve the model. Empirically, we show that state-of-the-art\nlanguage models, including LSTMs and Transformers, are \\emph{miscalibrated}:\nthe entropy rates of their generations drift dramatically upward over time. We\nthen provide provable methods to mitigate this phenomenon. Furthermore, we show\nhow this calibration-based approach can also be used to measure the amount of\nmemory that language models use for prediction.", "published": "2019-06-11 17:00:49", "link": "http://arxiv.org/abs/1906.05664v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "A Focus on Neural Machine Translation for African Languages", "abstract": "African languages are numerous, complex and low-resourced. The datasets\nrequired for machine translation are difficult to discover, and existing\nresearch is hard to reproduce. Minimal attention has been given to machine\ntranslation for African languages so there is scant research regarding the\nproblems that arise when using machine translation techniques. To begin\naddressing these problems, we trained models to translate English to five of\nthe official South African languages (Afrikaans, isiZulu, Northern Sotho,\nSetswana, Xitsonga), making use of modern neural machine translation\ntechniques. The results obtained show the promise of using neural machine\ntranslation techniques for African languages. By providing reproducible\npublicly-available data, code and results, this research aims to provide a\nstarting point for other researchers in African machine translation to compare\nto and build upon.", "published": "2019-06-11 15:38:34", "link": "http://arxiv.org/abs/1906.05685v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Deep Learning based Emotion Recognition System Using Speech Features and\n  Transcriptions", "abstract": "This paper proposes a speech emotion recognition method based on speech\nfeatures and speech transcriptions (text). Speech features such as Spectrogram\nand Mel-frequency Cepstral Coefficients (MFCC) help retain emotion-related\nlow-level characteristics in speech whereas text helps capture semantic\nmeaning, both of which help in different aspects of emotion detection. We\nexperimented with several Deep Neural Network (DNN) architectures, which take\nin different combinations of speech features and text as inputs. The proposed\nnetwork architectures achieve higher accuracies when compared to\nstate-of-the-art methods on a benchmark dataset. The combined MFCC-Text\nConvolutional Neural Network (CNN) model proved to be the most accurate in\nrecognizing emotions in IEMOCAP data.", "published": "2019-06-11 17:35:02", "link": "http://arxiv.org/abs/1906.05681v1", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Focal Loss based Residual Convolutional Neural Network for Speech\n  Emotion Recognition", "abstract": "This paper proposes a Residual Convolutional Neural Network (ResNet) based on\nspeech features and trained under Focal Loss to recognize emotion in speech.\nSpeech features such as Spectrogram and Mel-frequency Cepstral Coefficients\n(MFCCs) have shown the ability to characterize emotion better than just plain\ntext. Further Focal Loss, first used in One-Stage Object Detectors, has shown\nthe ability to focus the training process more towards hard-examples and\ndown-weight the loss assigned to well-classified examples, thus preventing the\nmodel from being overwhelmed by easily classifiable examples.", "published": "2019-06-11 17:31:23", "link": "http://arxiv.org/abs/1906.05682v1", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
