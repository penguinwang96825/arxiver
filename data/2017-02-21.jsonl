{"title": "Learning to generate one-sentence biographies from Wikidata", "abstract": "We investigate the generation of one-sentence Wikipedia biographies from\nfacts derived from Wikidata slot-value pairs. We train a recurrent neural\nnetwork sequence-to-sequence model with attention to select facts and generate\ntextual summaries. Our model incorporates a novel secondary objective that\nhelps ensure it generates sentences that contain the input facts. The model\nachieves a BLEU score of 41, improving significantly upon the vanilla\nsequence-to-sequence model and scoring roughly twice that of a simple template\nbaseline. Human preference evaluation suggests the model is nearly as good as\nthe Wikipedia reference. Manual analysis explores content selection, suggesting\nthe model can trade the ability to infer knowledge against the risk of\nhallucinating incorrect information.", "published": "2017-02-21 01:30:59", "link": "http://arxiv.org/abs/1702.06235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning Based Argument Component Detection", "abstract": "Argument component detection (ACD) is an important sub-task in argumentation\nmining. ACD aims at detecting and classifying different argument components in\nnatural language texts. Historical annotations (HAs) are important features the\nhuman annotators consider when they manually perform the ACD task. However, HAs\nare largely ignored by existing automatic ACD techniques. Reinforcement\nlearning (RL) has proven to be an effective method for using HAs in some\nnatural language processing tasks. In this work, we propose a RL-based ACD\ntechnique, and evaluate its performance on two well-annotated corpora. Results\nsuggest that, in terms of classification accuracy, HAs-augmented RL outperforms\nplain RL by at most 17.85%, and outperforms the state-of-the-art supervised\nlearning algorithm by at most 11.94%.", "published": "2017-02-21 02:18:38", "link": "http://arxiv.org/abs/1702.06239v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid Dialog State Tracker with ASR Features", "abstract": "This paper presents a hybrid dialog state tracker enhanced by trainable\nSpoken Language Understanding (SLU) for slot-filling dialog systems. Our\narchitecture is inspired by previously proposed neural-network-based\nbelief-tracking systems. In addition, we extended some parts of our modular\narchitecture with differentiable rules to allow end-to-end training. We\nhypothesize that these rules allow our tracker to generalize better than pure\nmachine-learning based systems. For evaluation, we used the Dialog State\nTracking Challenge (DSTC) 2 dataset - a popular belief tracking testbed with\ndialogs from restaurant information system. To our knowledge, our hybrid\ntracker sets a new state-of-the-art result in three out of four categories\nwithin the DSTC2.", "published": "2017-02-21 11:34:14", "link": "http://arxiv.org/abs/1702.06336v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multitask Learning with CTC and Segmental CRF for Speech Recognition", "abstract": "Segmental conditional random fields (SCRFs) and connectionist temporal\nclassification (CTC) are two sequence labeling methods used for end-to-end\ntraining of speech recognition models. Both models define a transcription\nprobability by marginalizing decisions about latent segmentation alternatives\nto derive a sequence probability: the former uses a globally normalized joint\nmodel of segment labels and durations, and the latter classifies each frame as\neither an output symbol or a \"continuation\" of the previous label. In this\npaper, we train a recognition model by optimizing an interpolation between the\nSCRF and CTC losses, where the same recurrent neural network (RNN) encoder is\nused for feature extraction for both outputs. We find that this multitask\nobjective improves recognition accuracy when decoding with either the SCRF or\nCTC models. Additionally, we show that CTC can also be used to pretrain the RNN\nencoder, which improves the convergence rate when learning the joint model.", "published": "2017-02-21 13:39:35", "link": "http://arxiv.org/abs/1702.06378v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Multi-Step Reasoning for Question Answering on Semi-Structured\n  Tables", "abstract": "Advances in natural language processing tasks have gained momentum in recent\nyears due to the increasingly popular neural network methods. In this paper, we\nexplore deep learning techniques for answering multi-step reasoning questions\nthat operate on semi-structured tables. Challenges here arise from the level of\nlogical compositionality expressed by questions, as well as the domain\nopenness. Our approach is weakly supervised, trained on question-answer-table\ntriples without requiring intermediate strong supervision. It performs two\nphases: first, machine understandable logical forms (programs) are generated\nfrom natural language questions following the work of [Pasupat and Liang,\n2015]. Second, paraphrases of logical forms and questions are embedded in a\njointly learned vector space using word and character convolutional neural\nnetworks. A neural scoring function is further used to rank and retrieve the\nmost probable logical form (interpretation) of a question. Our best single\nmodel achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best\nensemble of our models pushes the state-of-the-art score on this task to 38.7%,\nthus slightly surpassing both the engineered feature scoring baseline, as well\nas the Neural Programmer model of [Neelakantan et al., 2016].", "published": "2017-02-21 21:24:26", "link": "http://arxiv.org/abs/1702.06589v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Complexity of CCG Parsing", "abstract": "We study the parsing complexity of Combinatory Categorial Grammar (CCG) in\nthe formalism of Vijay-Shanker and Weir (1994). As our main result, we prove\nthat any parsing algorithm for this formalism will take in the worst case\nexponential time when the size of the grammar, and not only the length of the\ninput sentence, is included in the analysis. This sets the formalism of\nVijay-Shanker and Weir (1994) apart from weakly equivalent formalisms such as\nTree-Adjoining Grammar (TAG), for which parsing can be performed in time\npolynomial in the combined size of grammar and input sentence. Our results\ncontribute to a refined understanding of the class of mildly context-sensitive\ngrammars, and inform the search for new, mildly context-sensitive versions of\nCCG.", "published": "2017-02-21 21:36:51", "link": "http://arxiv.org/abs/1702.06594v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syst\u00e8mes du LIA \u00e0 DEFT'13", "abstract": "The 2013 D\\'efi de Fouille de Textes (DEFT) campaign is interested in two\ntypes of language analysis tasks, the document classification and the\ninformation extraction in the specialized domain of cuisine recipes. We present\nthe systems that the LIA has used in DEFT 2013. Our systems show interesting\nresults, even though the complexity of the proposed tasks.", "published": "2017-02-21 17:14:56", "link": "http://arxiv.org/abs/1702.06478v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Algorithmes de classification et d'optimisation: participation du\n  LIA/ADOC \u00e1 DEFT'14", "abstract": "This year, the DEFT campaign (D\\'efi Fouilles de Textes) incorporates a task\nwhich aims at identifying the session in which articles of previous TALN\nconferences were presented. We describe the three statistical systems developed\nat LIA/ADOC for this task. A fusion of these systems enables us to obtain\ninteresting results (micro-precision score of 0.76 measured on the test corpus)", "published": "2017-02-21 18:24:52", "link": "http://arxiv.org/abs/1702.06510v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Efficient Social Network Multilingual Classification using Character,\n  POS n-grams and Dynamic Normalization", "abstract": "In this paper we describe a dynamic normalization process applied to social\nnetwork multilingual documents (Facebook and Twitter) to improve the\nperformance of the Author profiling task for short texts. After the\nnormalization process, $n$-grams of characters and n-grams of POS tags are\nobtained to extract all the possible stylistic information encoded in the\ndocuments (emoticons, character flooding, capital letters, references to other\nusers, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of\nperformance.", "published": "2017-02-21 16:26:54", "link": "http://arxiv.org/abs/1702.06467v1", "categories": ["cs.IR", "cs.CL", "cs.SI"], "primary_category": "cs.IR"}
