{"title": "Learning to Jointly Translate and Predict Dropped Pronouns with a Shared\n  Reconstruction Mechanism", "abstract": "Pronouns are frequently omitted in pro-drop languages, such as Chinese,\ngenerally leading to significant challenges with respect to the production of\ncomplete translations. Recently, Wang et al. (2018) proposed a novel\nreconstruction-based approach to alleviating dropped pronoun (DP) translation\nproblems for neural machine translation models. In this work, we improve the\noriginal model from two perspectives. First, we employ a shared reconstructor\nto better exploit encoder and decoder representations. Second, we jointly learn\nto translate and predict DPs in an end-to-end manner, to avoid the errors\npropagated from an external DP prediction model. Experimental results show that\nour approach significantly improves both translation performance and DP\nprediction accuracy.", "published": "2018-10-15 06:30:04", "link": "http://arxiv.org/abs/1810.06195v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UMONS Submission for WMT18 Multimodal Translation Task", "abstract": "This paper describes the UMONS solution for the Multimodal Machine\nTranslation Task presented at the third conference on machine translation\n(WMT18). We explore a novel architecture, called deepGRU, based on recent\nfindings in the related task of Neural Image Captioning (NIC). The models\npresented in the following sections lead to the best METEOR translation score\nfor both constrained (English, image) -> German and (English, image) -> French\nsub-tasks.", "published": "2018-10-15 09:05:21", "link": "http://arxiv.org/abs/1810.06233v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bringing back simplicity and lightliness into neural image captioning", "abstract": "Neural Image Captioning (NIC) or neural caption generation has attracted a\nlot of attention over the last few years. Describing an image with a natural\nlanguage has been an emerging challenge in both fields of computer vision and\nlanguage processing. Therefore a lot of research has focused on driving this\ntask forward with new creative ideas. So far, the goal has been to maximize\nscores on automated metric and to do so, one has to come up with a plurality of\nnew modules and techniques. Once these add up, the models become complex and\nresource-hungry. In this paper, we take a small step backwards in order to\nstudy an architecture with interesting trade-off between performance and\ncomputational complexity. To do so, we tackle every component of a neural\ncaptioning model and propose one or more solution that lightens the model\noverall. Our ideas are inspired by two related tasks: Multimodal and Monomodal\nNeural Machine Translation.", "published": "2018-10-15 09:42:55", "link": "http://arxiv.org/abs/1810.06245v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "(Self-Attentive) Autoencoder-based Universal Language Representation for\n  Machine Translation", "abstract": "Universal language representation is the holy grail in machine translation\n(MT). Thanks to the new neural MT approach, it seems that there are good\nperspectives towards this goal. In this paper, we propose a new architecture\nbased on combining variational autoencoders with encoder-decoders and\nintroducing an interlingual loss as an additional training objective. By adding\nand forcing this interlingual loss, we are able to train multiple encoders and\ndecoders for each language, sharing a common universal representation. Since\nthe final objective of this universal representation is producing close results\nfor similar input sentences (in any language), we propose to evaluate it by\nencoding the same sentence in two different languages, decoding both latent\nrepresentations into the same language and comparing both outputs. Preliminary\nresults on the WMT 2017 Turkish/English task shows that the proposed\narchitecture is capable of learning a universal language representation and\nsimultaneously training both translation directions with state-of-the-art\nresults.", "published": "2018-10-15 13:52:35", "link": "http://arxiv.org/abs/1810.06351v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Adaptation Layers for Cross-domain Named Entity Recognition", "abstract": "Recent research efforts have shown that neural architectures can be effective\nin conventional information extraction tasks such as named entity recognition,\nyielding state-of-the-art results on standard newswire datasets. However,\ndespite significant resources required for training such models, the\nperformance of a model trained on one domain typically degrades dramatically\nwhen applied to a different domain, yet extracting entities from new emerging\ndomains such as social media can be of significant interest. In this paper, we\nempirically investigate effective methods for conveniently adapting an\nexisting, well-trained neural NER model for a new domain. Unlike existing\napproaches, we propose lightweight yet effective methods for performing domain\nadaptation for neural models. Specifically, we introduce adaptation layers on\ntop of existing neural architectures, where no re-training using the source\ndomain data is required. We conduct extensive empirical studies and show that\nour approach significantly outperforms state-of-the-art methods.", "published": "2018-10-15 14:24:36", "link": "http://arxiv.org/abs/1810.06368v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Poincar\u00e9 GloVe: Hyperbolic Word Embeddings", "abstract": "Words are not created equal. In fact, they form an aristocratic graph with a\nlatent hierarchical structure that the next generation of unsupervised learned\nword embeddings should reveal. In this paper, justified by the notion of\ndelta-hyperbolicity or tree-likeliness of a space, we propose to embed words in\na Cartesian product of hyperbolic spaces which we theoretically connect to the\nGaussian word embeddings and their Fisher geometry. This connection allows us\nto introduce a novel principled hypernymy score for word embeddings. Moreover,\nwe adapt the well-known Glove algorithm to learn unsupervised word embeddings\nin this type of Riemannian manifolds. We further explain how to solve the\nanalogy task using the Riemannian parallel transport that generalizes vector\narithmetics to this new type of geometry. Empirically, based on extensive\nexperiments, we prove that our embeddings, trained unsupervised, are the first\nto simultaneously outperform strong and popular baselines on the tasks of\nsimilarity, analogy and hypernymy detection. In particular, for word hypernymy,\nwe obtain new state-of-the-art on fully unsupervised WBLESS classification\naccuracy.", "published": "2018-10-15 17:54:36", "link": "http://arxiv.org/abs/1810.06546v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diacritization of Maghrebi Arabic Sub-Dialects", "abstract": "Diacritization process attempt to restore the short vowels in Arabic written\ntext; which typically are omitted. This process is essential for applications\nsuch as Text-to-Speech (TTS). While diacritization of Modern Standard Arabic\n(MSA) still holds the lion share, research on dialectal Arabic (DA)\ndiacritization is very limited. In this paper, we present our contribution and\nresults on the automatic diacritization of two sub-dialects of Maghrebi Arabic,\nnamely Tunisian and Moroccan, using a character-level deep neural network\narchitecture that stacks two bi-LSTM layers over a CRF output layer. The model\nachieves word error rate of 2.7% and 3.6% for Moroccan and Tunisian\nrespectively and is capable of implicitly identifying the sub-dialect of the\ninput.", "published": "2018-10-15 19:08:03", "link": "http://arxiv.org/abs/1810.06619v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robust Neural Machine Translation with Joint Textual and Phonetic\n  Embedding", "abstract": "Neural machine translation (NMT) is notoriously sensitive to noises, but\nnoises are almost inevitable in practice. One special kind of noise is the\nhomophone noise, where words are replaced by other words with similar\npronunciations. We propose to improve the robustness of NMT to homophone noises\nby 1) jointly embedding both textual and phonetic information of source\nsentences, and 2) augmenting the training dataset with homophone noises.\nInterestingly, to achieve better translation quality and more robustness, we\nfound that most (though not all) weights should be put on the phonetic rather\nthan textual information. Experiments show that our method not only\nsignificantly improves the robustness of NMT to homophone noises, but also\nsurprisingly improves the translation quality on some clean test sets.", "published": "2018-10-15 22:23:28", "link": "http://arxiv.org/abs/1810.06729v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Marrying Universal Dependencies and Universal Morphology", "abstract": "The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects\neach present schemata for annotating the morphosyntactic details of language.\nEach project also provides corpora of annotated text in many languages - UD at\nthe token level and UniMorph at the type level. As each corpus is built by\ndifferent annotators, language-specific decisions hinder the goal of universal\nschemata. With compatibility of tags, each project's annotations could be used\nto validate the other's. Additionally, the availability of both type- and\ntoken-level resources would be a boon to tasks such as parsing and homograph\ndisambiguation. To ease this interoperability, we present a deterministic\nmapping from Universal Dependencies v2 features into the UniMorph schema. We\nvalidate our approach by lookup in the UniMorph corpora and find a\nmacro-average of 64.13% recall. We also note incompatibilities due to paucity\nof data on either side. Finally, we present a critical evaluation of the\nfoundations, strengths, and weaknesses of the two annotation projects.", "published": "2018-10-15 23:00:13", "link": "http://arxiv.org/abs/1810.06743v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Super Characters: A Conversion from Sentiment Classification to Image\n  Classification", "abstract": "We propose a method named Super Characters for sentiment classification. This\nmethod converts the sentiment classification problem into image classification\nproblem by projecting texts into images and then applying CNN models for\nclassification. Text features are extracted automatically from the generated\nSuper Characters images, hence there is no need of any explicit step of\nembedding the words or characters into numerical vector representations.\nExperimental results on large social media corpus show that the Super\nCharacters method consistently outperforms other methods for sentiment\nclassification and topic classification tasks on ten large social media\ndatasets of millions of contents in four different languages, including\nChinese, Japanese, Korean and English.", "published": "2018-10-15 22:49:48", "link": "http://arxiv.org/abs/1810.07653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can Euroscepticism Contribute to a European Public Sphere? The\n  Europeanization of Media Discourses about Euroscepticism across Six Countries", "abstract": "This study compares the media discourses about Euroscepticism in 2014 across\nsix countries (United Kingdom, Ireland, France, Spain, Sweden, and Denmark). We\nassess the extent to which the mass media's reporting of Euroscepticism\nindicates the Europeanization of public spheres. Using a mixed-methods approach\ncombining LDA topic modeling and qualitative coding, we find that approximately\n70 per cent of print articles mentioning \"Euroscepticism\" or \"Eurosceptic\" are\nframed in a non-domestic (i.e. European) context. In five of the six cases\nstudied, articles exhibiting a European context are strikingly similar in\ncontent, with the British case as the exception. However, coverage of British\nEuroscepticism drives Europeanization in other Member States. Bivariate\nlogistic regressions further reveal three macro-level structural variables that\nsignificantly correlate with a Europeanized media discourse: newspaper type\n(tabloid or broadsheet), presence of a strong Eurosceptic party, and\nrelationship to the EU budget (net contributor or receiver of EU funds).", "published": "2018-10-15 23:06:03", "link": "http://arxiv.org/abs/1810.06745v1", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "The Trajectory of Voice Onset Time with Vocal Aging", "abstract": "Vocal aging, a universal process of human aging, can largely affect one's\nlanguage use, possibly including some subtle acoustic features of one's\nutterances like Voice Onset Time. To figure out the time effects, Queen\nElizabeth's Christmas speeches are documented and analyzed in the long-term\ntrend. We build statistical models of time dependence in Voice Onset Time,\ncontrolling a wide range of other fixed factors, to present annual variations\nand the simulated trajectory. It is revealed that the variation range of Voice\nOnset Time has been narrowing over fifty years with a slight reduction in the\nmean value, which, possibly, is an effect of diminishing exertion, resulting\nfrom subdued muscle contraction, transcending other non-linguistic factors in\nforming Voice Onset Time patterns over a long time.", "published": "2018-10-15 08:30:27", "link": "http://arxiv.org/abs/1810.07030v1", "categories": ["cs.SD", "cs.CL"], "primary_category": "cs.SD"}
{"title": "Improving Topic Models with Latent Feature Word Representations", "abstract": "Probabilistic topic models are widely used to discover latent topics in\ndocument collections, while latent feature vector representations of words have\nbeen used to obtain high performance in many NLP tasks. In this paper, we\nextend two different Dirichlet multinomial topic models by incorporating latent\nfeature vector representations of words trained on very large corpora to\nimprove the word-topic mapping learnt on a smaller corpus. Experimental results\nshow that by using information from the external corpora, our new models\nproduce significant improvements on topic coherence, document clustering and\ndocument classification tasks, especially on datasets with few or short\ndocuments.", "published": "2018-10-15 12:34:05", "link": "http://arxiv.org/abs/1810.06306v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structured Content Preservation for Unsupervised Text Style Transfer", "abstract": "Text style transfer aims to modify the style of a sentence while keeping its\ncontent unchanged. Recent style transfer systems often fail to faithfully\npreserve the content after changing the style. This paper proposes a structured\ncontent preserving model that leverages linguistic information in the\nstructured fine-grained supervisions to better preserve the style-independent\ncontent during style transfer. In particular, we achieve the goal by devising\nrich model objectives based on both the sentence's lexical information and a\nlanguage model that conditions on content. The resulting model therefore is\nencouraged to retain the semantic meaning of the target sentences. We perform\nextensive experiments that compare our model to other existing approaches in\nthe tasks of sentiment and political slant transfer. Our model achieves\nsignificant improvement in terms of both content preservation and style\ntransfer in automatic and human evaluation.", "published": "2018-10-15 17:19:18", "link": "http://arxiv.org/abs/1810.06526v2", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Stop Illegal Comments: A Multi-Task Deep Learning Approach", "abstract": "Deep learning methods are often difficult to apply in the legal domain due to\nthe large amount of labeled data required by deep learning methods. A recent\nnew trend in the deep learning community is the application of multi-task\nmodels that enable single deep neural networks to perform more than one task at\nthe same time, for example classification and translation tasks. These powerful\nnovel models are capable of transferring knowledge among different tasks or\ntraining sets and therefore could open up the legal domain for many deep\nlearning applications. In this paper, we investigate the transfer learning\ncapabilities of such a multi-task model on a classification task on the\npublicly available Kaggle toxic comment dataset for classifying illegal\ncomments and we can report promising results.", "published": "2018-10-15 20:22:44", "link": "http://arxiv.org/abs/1810.06665v1", "categories": ["cs.IR", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.IR"}
{"title": "Deep Transfer Reinforcement Learning for Text Summarization", "abstract": "Deep neural networks are data hungry models and thus face difficulties when\nattempting to train on small text datasets. Transfer learning is a potential\nsolution but their effectiveness in the text domain is not as explored as in\nareas such as image analysis. In this paper, we study the problem of transfer\nlearning for text summarization and discuss why existing state-of-the-art\nmodels fail to generalize well on other (unseen) datasets. We propose a\nreinforcement learning framework based on a self-critic policy gradient\napproach which achieves good generalization and state-of-the-art results on a\nvariety of datasets. Through an extensive set of experiments, we also show the\nability of our proposed framework to fine-tune the text summarization model\nusing only a few training samples. To the best of our knowledge, this is the\nfirst work that studies transfer learning in text summarization and provides a\ngeneric solution that works well on unseen data.", "published": "2018-10-15 20:26:44", "link": "http://arxiv.org/abs/1810.06667v2", "categories": ["cs.LG", "cs.CL", "stat.ML", "I.2.6; I.2.7; I.2.10"], "primary_category": "cs.LG"}
{"title": "Named-Entity Linking Using Deep Learning For Legal Documents: A Transfer\n  Learning Approach", "abstract": "In the legal domain it is important to differentiate between words in\ngeneral, and afterwards to link the occurrences of the same entities. The topic\nto solve these challenges is called Named-Entity Linking (NEL). Current\nsupervised neural networks designed for NEL use publicly available datasets for\ntraining and testing. However, this paper focuses especially on the aspect of\napplying transfer learning approach using networks trained for NEL to legal\ndocuments. Experiments show consistent improvement in the legal datasets that\nwere created from the European Union law in the scope of this research. Using\ntransfer learning approach, we reached F1-score of 98.90\\% and 98.01\\% on the\nlegal small and large test dataset.", "published": "2018-10-15 20:38:00", "link": "http://arxiv.org/abs/1810.06673v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Trellis Networks for Sequence Modeling", "abstract": "We present trellis networks, a new architecture for sequence modeling. On the\none hand, a trellis network is a temporal convolutional network with special\nstructure, characterized by weight tying across depth and direct injection of\nthe input into deep layers. On the other hand, we show that truncated recurrent\nnetworks are equivalent to trellis networks with special sparsity structure in\ntheir weight matrices. Thus trellis networks with general weight matrices\ngeneralize truncated recurrent networks. We leverage these connections to\ndesign high-performing trellis networks that absorb structural and algorithmic\nelements from both recurrent and convolutional models. Experiments demonstrate\nthat trellis networks outperform the current state of the art methods on a\nvariety of challenging benchmarks, including word-level language modeling and\ncharacter-level language modeling tasks, and stress tests designed to evaluate\nlong-term memory retention. The code is available at\nhttps://github.com/locuslab/trellisnet .", "published": "2018-10-15 20:50:05", "link": "http://arxiv.org/abs/1810.06682v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Polyphonic Sound Event Detection by using Capsule Neural Networks", "abstract": "Artificial sound event detection (SED) has the aim to mimic the human ability\nto perceive and understand what is happening in the surroundings. Nowadays,\nDeep Learning offers valuable techniques for this goal such as Convolutional\nNeural Networks (CNNs). The Capsule Neural Network (CapsNet) architecture has\nbeen recently introduced in the image processing field with the intent to\novercome some of the known limitations of CNNs, specifically regarding the\nscarce robustness to affine transformations (i.e., perspective, size,\norientation) and the detection of overlapped images. This motivated the authors\nto employ CapsNets to deal with the polyphonic-SED task, in which multiple\nsound events occur simultaneously. Specifically, we propose to exploit the\ncapsule units to represent a set of distinctive properties for each individual\nsound event. Capsule units are connected through a so-called \"dynamic routing\"\nthat encourages learning part-whole relationships and improves the detection\nperformance in a polyphonic context. This paper reports extensive evaluations\ncarried out on three publicly available datasets, showing how the CapsNet-based\nalgorithm not only outperforms standard CNNs but also allows to achieve the\nbest results with respect to the state of the art algorithms.", "published": "2018-10-15 12:56:45", "link": "http://arxiv.org/abs/1810.06325v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Modeling of nonlinear audio effects with end-to-end deep neural networks", "abstract": "In the context of music production, distortion effects are mainly used for\naesthetic reasons and are usually applied to electric musical instruments. Most\nexisting methods for nonlinear modeling are often either simplified or\noptimized to a very specific circuit. In this work, we investigate deep\nlearning architectures for audio processing and we aim to find a general\npurpose end-to-end deep neural network to perform modeling of nonlinear audio\neffects. We show the network modeling various nonlinearities and we discuss the\ngeneralization capabilities among different instruments.", "published": "2018-10-15 18:30:11", "link": "http://arxiv.org/abs/1810.06603v2", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
