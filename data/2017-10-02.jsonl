{"title": "Visual Reasoning with Natural Language", "abstract": "Natural language provides a widely accessible and expressive interface for\nrobotic agents. To understand language in complex environments, agents must\nreason about the full range of language inputs and their correspondence to the\nworld. Such reasoning over language and vision is an open problem that is\nreceiving increasing attention. While existing data sets focus on visual\ndiversity, they do not display the full range of natural language expressions,\nsuch as counting, set reasoning, and comparisons.\n  We propose a simple task for natural language visual reasoning, where images\nare paired with descriptive statements. The task is to predict if a statement\nis true for the given scene. This abstract describes our existing synthetic\nimages corpus and our current work on collecting real vision data.", "published": "2017-10-02 01:52:05", "link": "http://arxiv.org/abs/1710.00453v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Crowd-Annotated Spanish Corpus for Humor Analysis", "abstract": "Computational Humor involves several tasks, such as humor recognition, humor\ngeneration, and humor scoring, for which it is useful to have human-curated\ndata. In this work we present a corpus of 27,000 tweets written in Spanish and\ncrowd-annotated by their humor value and funniness score, with about four\nannotations per tweet, tagged by 1,300 people over the Internet. It is equally\ndivided between tweets coming from humorous and non-humorous accounts. The\ninter-annotator agreement Krippendorff's alpha value is 0.5710. The dataset is\navailable for general use and can serve as a basis for humor detection and as a\nfirst step to tackle subjectivity.", "published": "2017-10-02 04:16:36", "link": "http://arxiv.org/abs/1710.00477v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attentive Convolution: Equipping CNNs with RNN-style Attention\n  Mechanisms", "abstract": "In NLP, convolutional neural networks (CNNs) have benefited less than\nrecurrent neural networks (RNNs) from attention mechanisms. We hypothesize that\nthis is because the attention in CNNs has been mainly implemented as attentive\npooling (i.e., it is applied to pooling) rather than as attentive convolution\n(i.e., it is integrated into convolution). Convolution is the differentiator of\nCNNs in that it can powerfully model the higher-level representation of a word\nby taking into account its local fixed-size context in the input text t^x. In\nthis work, we propose an attentive convolution network, ATTCONV. It extends the\ncontext scope of the convolution operation, deriving higher-level features for\na word not only from local context, but also information extracted from\nnonlocal context by the attention mechanism commonly used in RNNs. This\nnonlocal context can come (i) from parts of the input text t^x that are distant\nor (ii) from extra (i.e., external) contexts t^y. Experiments on sentence\nmodeling with zero-context (sentiment analysis), single-context (textual\nentailment) and multiple-context (claim verification) demonstrate the\neffectiveness of ATTCONV in sentence representation learning with the\nincorporation of context. In particular, attentive convolution outperforms\nattentive pooling and is a strong competitor to popular attentive RNNs.", "published": "2017-10-02 07:58:19", "link": "http://arxiv.org/abs/1710.00519v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Chatbots from Forum Data: Model Selection Using Question\n  Answering Metrics", "abstract": "We propose to use question answering (QA) data from Web forums to train\nchatbots from scratch, i.e., without dialog training data. First, we extract\npairs of question and answer sentences from the typically much longer texts of\nquestions and answers in a forum. We then use these shorter texts to train\nseq2seq models in a more efficient way. We further improve the parameter\noptimization using a new model selection strategy based on QA measures.\nFinally, we propose to use extrinsic evaluation with respect to a QA task as an\nautomatic evaluation method for chatbots. The evaluation shows that the model\nachieves a MAP of 63.5% on the extrinsic task. Moreover, it can answer\ncorrectly 49.5% of the questions when they are similar to questions asked in\nthe forum, and 47.3% of the questions when they are more conversational in\nstyle.", "published": "2017-10-02 14:34:25", "link": "http://arxiv.org/abs/1710.00689v1", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Compiling and Processing Historical and Contemporary Portuguese Corpora", "abstract": "This technical report describes the framework used for processing three large\nPortuguese corpora. Two corpora contain texts from newspapers, one published in\nBrazil and the other published in Portugal. The third corpus is Colonia, a\nhistorical Portuguese collection containing texts written between the 16th and\nthe early 20th century. The report presents pre-processing methods,\nsegmentation, and annotation of the corpora as well as indexing and querying\nmethods. Finally, it presents published research papers using the corpora.", "published": "2017-10-02 17:18:37", "link": "http://arxiv.org/abs/1710.00803v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Distributional Inclusion Vector Embedding for Unsupervised Hypernymy\n  Detection", "abstract": "Modeling hypernymy, such as poodle is-a dog, is an important generalization\naid to many NLP tasks, such as entailment, coreference, relation extraction,\nand question answering. Supervised learning from labeled hypernym sources, such\nas WordNet, limits the coverage of these models, which can be addressed by\nlearning hypernyms from unlabeled text. Existing unsupervised methods either do\nnot scale to large vocabularies or yield unacceptably poor accuracy. This paper\nintroduces distributional inclusion vector embedding (DIVE), a\nsimple-to-implement unsupervised method of hypernym discovery via per-word\nnon-negative vector embeddings which preserve the inclusion property of word\ncontexts in a low-dimensional and interpretable space. In experimental\nevaluations more comprehensive than any previous literature of which we are\naware-evaluating on 11 datasets using multiple existing as well as newly\nproposed scoring functions-we find that our method provides up to double the\nprecision of previous unsupervised embeddings, and the highest average\nperformance, using a much more compact word representation, and yielding many\nnew state-of-the-art results.", "published": "2017-10-02 19:33:46", "link": "http://arxiv.org/abs/1710.00880v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Minimal Dependency Translation: a Framework for Computer-Assisted\n  Translation for Under-Resourced Languages", "abstract": "This paper introduces Minimal Dependency Translation (MDT), an ongoing\nproject to develop a rule-based framework for the creation of rudimentary\nbilingual lexicon-grammars for machine translation and computer-assisted\ntranslation into and out of under-resourced languages as well as initial steps\ntowards an implementation of MDT for English-to-Amharic translation. The basic\nunits in MDT, called groups, are headed multi-item sequences. In addition to\nwordforms, groups may contain lexemes, syntactic-semantic categories, and\ngrammatical features. Each group is associated with one or more translations,\neach of which is a group in a target language. During translation, constraint\nsatisfaction is used to select a set of source-language groups for the input\nsentence and to sequence the words in the associated target-language groups.", "published": "2017-10-02 21:56:16", "link": "http://arxiv.org/abs/1710.00923v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Identifying Nominals with No Head Match Co-references Using Deep\n  Learning", "abstract": "Identifying nominals with no head match is a long-standing challenge in\ncoreference resolution with current systems performing significantly worse than\nhumans. In this paper we present a new neural network architecture which\noutperforms the current state-of-the-art system on the English portion of the\nCoNLL 2012 Shared Task. This is done by using a logistic regression on features\nproduced by two submodels, one of which is has the architecture proposed in\n[CM16a] while the other combines domain specific embeddings of the antecedent\nand the mention. We also propose some simple additional features which seem to\nimprove performance for all models substantially, increasing F1 by almost 4% on\nbasic logistic regression and other complex models.", "published": "2017-10-02 23:02:17", "link": "http://arxiv.org/abs/1710.00936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Perception of Readers and Writers in Emoji use", "abstract": "Previous research has traditionally analyzed emoji sentiment from the point\nof view of the reader of the content not the author. Here, we analyze emoji\nsentiment from the point of view of the author and present a emoji sentiment\nbenchmark that was built from an employee happiness dataset where emoji happen\nto be annotated with daily happiness of the author of the comment. The data\nspans over 3 years, and 4k employees of 56 companies based in Barcelona. We\ncompare sentiment of writers to readers. Results indicate that, there is an 82%\nagreement in how emoji sentiment is perceived by readers and writers. Finally,\nwe report that when authors use emoji they report higher levels of happiness.\nEmoji use was not found to be correlated with differences in author moodiness.", "published": "2017-10-02 20:07:18", "link": "http://arxiv.org/abs/1710.00888v2", "categories": ["cs.IR", "cs.CL", "cs.HC"], "primary_category": "cs.IR"}
