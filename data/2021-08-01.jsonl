{"title": "Learning to Look Inside: Augmenting Token-Based Encoders with\n  Character-Level Information", "abstract": "Commonly-used transformer language models depend on a tokenization schema\nwhich sets an unchangeable subword vocabulary prior to pre-training, destined\nto be applied to all downstream tasks regardless of domain shift, novel word\nformations, or other sources of vocabulary mismatch. Recent work has shown that\n\"token-free\" models can be trained directly on characters or bytes, but\ntraining these models from scratch requires substantial computational\nresources, and this implies discarding the many domain-specific models that\nwere trained on tokens. In this paper, we present XRayEmb, a method for\nretrofitting existing token-based models with character-level information.\nXRayEmb is composed of a character-level \"encoder\" that computes vector\nrepresentations of character sequences, and a generative component that decodes\nfrom the internal representation to a character sequence. We show that\nincorporating XRayEmb's learned vectors into sequences of pre-trained token\nembeddings helps performance on both autoregressive and masked pre-trained\ntransformer architectures and on both sequence-level and sequence tagging\ntasks, particularly on non-standard English text.", "published": "2021-08-01 08:09:26", "link": "http://arxiv.org/abs/2108.00391v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-based Aspect Reasoning for Knowledge Base Question Answering\n  on Clinical Notes", "abstract": "Question Answering (QA) in clinical notes has gained a lot of attention in\nthe past few years. Existing machine reading comprehension approaches in\nclinical domain can only handle questions about a single block of clinical\ntexts and fail to retrieve information about multiple patients and their\nclinical notes. To handle more complex questions, we aim at creating knowledge\nbase from clinical notes to link different patients and clinical notes, and\nperforming knowledge base question answering (KBQA). Based on the expert\nannotations available in the n2c2 dataset, we first created the ClinicalKBQA\ndataset that includes around 9K QA pairs and covers questions about seven\nmedical topics using more than 300 question templates. Then, we investigated an\nattention-based aspect reasoning (AAR) method for KBQA and analyzed the impact\nof different aspects of answers (e.g., entity, type, path, and context) for\nprediction. The AAR method achieves better performance due to the well-designed\nencoder and attention mechanism. From our experiments, we find that both\naspects, type and path, enable the model to identify answers satisfying the\ngeneral conditions and produce lower precision and higher recall. On the other\nhand, the aspects, entity and context, limit the answers by node-specific\ninformation and lead to higher precision and lower recall.", "published": "2021-08-01 17:58:46", "link": "http://arxiv.org/abs/2108.00513v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Geolocation differences of language use in urban areas", "abstract": "The explosion in the availability of natural language data in the era of\nsocial media has given rise to a host of applications such as sentiment\nanalysis and opinion mining. Simultaneously, the growing availability of\nprecise geolocation information is enabling visualization of global phenomena\nsuch as environmental changes and disease propagation. Opportunities for\ntracking spatial variations in language use, however, have largely been\noverlooked, especially on small spatial scales. Here we explore the use of\nTwitter data with precise geolocation information to resolve spatial variations\nin language use on an urban scale down to single city blocks. We identify\nseveral categories of language tokens likely to show distinctive patterns of\nuse and develop quantitative methods to visualize the spatial distributions\nassociated with these patterns. Our analysis concentrates on comparison of\ncontrasting pairs of Tweet distributions from the same category, each defined\nby a set of tokens. Our work shows that analysis of small-scale variations can\nprovide unique information on correlations between language use and social\ncontext which are highly valuable to a wide range of fields from linguistic\nscience and commercial advertising to social services.", "published": "2021-08-01 19:55:45", "link": "http://arxiv.org/abs/2108.00533v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation\n  for Multi-turn Dialogue", "abstract": "An intelligent dialogue system in a multi-turn setting should not only\ngenerate the responses which are of good quality, but it should also generate\nthe responses which can lead to long-term success of the dialogue. Although,\nthe current approaches improved the response quality, but they over-look the\ntraining signals present in the dialogue data. We can leverage these signals to\ngenerate the weakly supervised training data for learning dialog policy and\nreward estimator, and make the policy take actions (generates responses) which\ncan foresee the future direction for a successful (rewarding) conversation. We\nsimulate the dialogue between an agent and a user (modelled similar to an agent\nwith supervised learning objective) to interact with each other. The agent uses\ndynamic blocking to generate ranked diverse responses and\nexploration-exploitation to select among the Top-K responses. Each simulated\nstate-action pair is evaluated (works as a weak annotation) with three quality\nmodules: Semantic Relevant, Semantic Coherence and Consistent Flow. Empirical\nstudies with two benchmarks indicate that our model can significantly\nout-perform the response quality and lead to a successful conversation on both\nautomatic evaluation and human judgement.", "published": "2021-08-01 08:00:45", "link": "http://arxiv.org/abs/2108.01487v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Social Meaning Detection with Pragmatic Masking and Surrogate\n  Fine-Tuning", "abstract": "Masked language models (MLMs) are pre-trained with a denoising objective that\nis in a mismatch with the objective of downstream fine-tuning. We propose\npragmatic masking and surrogate fine-tuning as two complementing strategies\nthat exploit social cues to drive pre-trained representations toward a broad\nset of concepts useful for a wide class of social meaning tasks. We test our\nmodels on $15$ different Twitter datasets for social meaning detection. Our\nmethods achieve $2.34\\%$ $F_1$ over a competitive baseline, while outperforming\ndomain-specific language models pre-trained on large datasets. Our methods also\nexcel in few-shot learning: with only $5\\%$ of training data (severely\nfew-shot), our methods enable an impressive $68.54\\%$ average $F_1$. The\nmethods are also language agnostic, as we show in a zero-shot setting involving\nsix datasets from three different languages.", "published": "2021-08-01 03:32:21", "link": "http://arxiv.org/abs/2108.00356v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformer-Encoder-GRU (T-E-GRU) for Chinese Sentiment Analysis on\n  Chinese Comment Text", "abstract": "Chinese sentiment analysis (CSA) has always been one of the challenges in\nnatural language processing due to its complexity and uncertainty. Transformer\nhas succeeded in capturing semantic features, but it uses position encoding to\ncapture sequence features, which has great shortcomings compared with the\nrecurrent model. In this paper, we propose T-E-GRU for Chinese sentiment\nanalysis, which combine transformer encoder and GRU. We conducted experiments\non three Chinese comment datasets. In view of the confusion of punctuation\nmarks in Chinese comment texts, we selectively retain some punctuation marks\nwith sentence segmentation ability. The experimental results show that T-E-GRU\noutperforms classic recurrent model and recurrent model with attention.", "published": "2021-08-01 08:42:26", "link": "http://arxiv.org/abs/2108.00400v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Enhancing Content Preservation in Text Style Transfer Using Reverse\n  Attention and Conditional Layer Normalization", "abstract": "Text style transfer aims to alter the style (e.g., sentiment) of a sentence\nwhile preserving its content. A common approach is to map a given sentence to\ncontent representation that is free of style, and the content representation is\nfed to a decoder with a target style. Previous methods in filtering style\ncompletely remove tokens with style at the token level, which incurs the loss\nof content information. In this paper, we propose to enhance content\npreservation by implicitly removing the style information of each token with\nreverse attention, and thereby retain the content. Furthermore, we fuse content\ninformation when building the target style representation, making it dynamic\nwith respect to the content. Our method creates not only style-independent\ncontent representation, but also content-dependent style representation in\ntransferring style. Empirical results show that our method outperforms the\nstate-of-the-art baselines by a large margin in terms of content preservation.\nIn addition, it is also competitive in terms of style transfer accuracy and\nfluency.", "published": "2021-08-01 12:54:46", "link": "http://arxiv.org/abs/2108.00449v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "DECAF: Deep Extreme Classification with Label Features", "abstract": "Extreme multi-label classification (XML) involves tagging a data point with\nits most relevant subset of labels from an extremely large label set, with\nseveral applications such as product-to-product recommendation with millions of\nproducts. Although leading XML algorithms scale to millions of labels, they\nlargely ignore label meta-data such as textual descriptions of the labels. On\nthe other hand, classical techniques that can utilize label metadata via\nrepresentation learning using deep networks struggle in extreme settings. This\npaper develops the DECAF algorithm that addresses these challenges by learning\nmodels enriched by label metadata that jointly learn model parameters and\nfeature representations using deep networks and offer accurate classification\nat the scale of millions of labels. DECAF makes specific contributions to model\narchitecture design, initialization, and training, enabling it to offer up to\n2-6% more accurate prediction than leading extreme classifiers on publicly\navailable benchmark product-to-product recommendation datasets, such as\nLF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster\nat inference than leading deep extreme classifiers, which makes it suitable for\nreal-time applications that require predictions within a few milliseconds. The\ncode for DECAF is available at the following URL\nhttps://github.com/Extreme-classification/DECAF.", "published": "2021-08-01 05:36:05", "link": "http://arxiv.org/abs/2108.00368v1", "categories": ["cs.CL", "cs.IR", "cs.LG", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "Realised Volatility Forecasting: Machine Learning via Financial Word\n  Embedding", "abstract": "This study develops a financial word embedding using 15 years of business\nnews. Our results show that this specialised language model produces more\naccurate results than general word embeddings, based on a financial benchmark\nwe established. As an application, we incorporate this word embedding into a\nsimple machine learning model to enhance the HAR model for forecasting realised\nvolatility. This approach statistically and economically outperforms\nestablished econometric models. Using an explainable AI method, we also\nidentify key phrases in business news that contribute significantly to\nvolatility, offering insights into language patterns tied to market dynamics.", "published": "2021-08-01 15:43:57", "link": "http://arxiv.org/abs/2108.00480v4", "categories": ["q-fin.CP", "cs.CL", "cs.LG"], "primary_category": "q-fin.CP"}
{"title": "You too Brutus! Trapping Hateful Users in Social Media: Challenges,\n  Solutions & Insights", "abstract": "Hate speech is regarded as one of the crucial issues plaguing the online\nsocial media. The current literature on hate speech detection leverages\nprimarily the textual content to find hateful posts and subsequently identify\nhateful users. However, this methodology disregards the social connections\nbetween users. In this paper, we run a detailed exploration of the problem\nspace and investigate an array of models ranging from purely textual to graph\nbased to finally semi-supervised techniques using Graph Neural Networks (GNN)\nthat utilize both textual and graph-based features. We run exhaustive\nexperiments on two datasets -- Gab, which is loosely moderated and Twitter,\nwhich is strictly moderated. Overall the AGNN model achieves 0.791 macro\nF1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset\nusing only 5% of the labeled instances, considerably outperforming all the\nother models including the fully supervised ones. We perform detailed error\nanalysis on the best performing text and graph based models and observe that\nhateful users have unique network neighborhood signatures and the AGNN model\nbenefits by paying attention to these signatures. This property, as we observe,\nalso allows the model to generalize well across platforms in a zero-shot\nsetting. Lastly, we utilize the best performing GNN model to analyze the\nevolution of hateful users and their targets over time in Gab.", "published": "2021-08-01 19:13:58", "link": "http://arxiv.org/abs/2108.00524v1", "categories": ["cs.SI", "cs.CL", "cs.LG"], "primary_category": "cs.SI"}
{"title": "A Survey on Audio Synthesis and Audio-Visual Multimodal Processing", "abstract": "With the development of deep learning and artificial intelligence, audio\nsynthesis has a pivotal role in the area of machine learning and shows strong\napplicability in the industry. Meanwhile, significant efforts have been\ndedicated by researchers to handle multimodal tasks at present such as\naudio-visual multimodal processing. In this paper, we conduct a survey on audio\nsynthesis and audio-visual multimodal processing, which helps understand\ncurrent research and future trends. This review focuses on text to speech(TTS),\nmusic generation and some tasks that combine visual and acoustic information.\nThe corresponding technical methods are comprehensively classified and\nintroduced, and their future development trends are prospected. This survey can\nprovide some guidance for researchers who are interested in the areas like\naudio synthesis and audio-visual multimodal processing.", "published": "2021-08-01 12:35:16", "link": "http://arxiv.org/abs/2108.00443v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "SurpriseNet: Melody Harmonization Conditioning on User-controlled\n  Surprise Contours", "abstract": "The surprisingness of a song is an essential and seemingly subjective factor\nin determining whether the listener likes it. With the help of information\ntheory, it can be described as the transition probability of a music sequence\nmodeled as a Markov chain. In this study, we introduce the concept of deriving\nentropy variations over time, so that the surprise contour of each chord\nsequence can be extracted. Based on this, we propose a user-controllable\nframework that uses a conditional variational autoencoder (CVAE) to harmonize\nthe melody based on the given chord surprise indication. Through explicit\nconditions, the model can randomly generate various and harmonic chord\nprogressions for a melody, and the Spearman's correlation and p-value\nsignificance show that the resulting chord progressions match the given\nsurprise contour quite well. The vanilla CVAE model was evaluated in a basic\nmelody harmonization task (no surprise control) in terms of six objective\nmetrics. The results of experiments on the Hooktheory Lead Sheet Dataset show\nthat our model achieves performance comparable to the state-of-the-art melody\nharmonization model.", "published": "2021-08-01 07:00:59", "link": "http://arxiv.org/abs/2108.00378v2", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End to End Bangla Speech Synthesis", "abstract": "Text-to-Speech (TTS) system is a system where speech is synthesized from a\ngiven text following any particular approach. Concatenative synthesis, Hidden\nMarkov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with\nmultiple building blocks, etc. are the main approaches for implementing a TTS\nsystem. Here, we are presenting our deep learning-based end-to-end Bangla\nspeech synthesis system. It has been implemented with minimal human annotation\nusing only 3 major components (Encoder, Decoder, Post-processing net including\nwaveform synthesis). It does not require any frontend preprocessor and\nGrapheme-to-Phoneme (G2P) converter. Our model has been trained with\nphonetically balanced 20 hours of single speaker speech data. It has obtained a\n3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a\n0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,\n4.5] as objective evaluation. It is outperforming all existing non-commercial\nstate-of-the-art Bangla TTS systems based on naturalness.", "published": "2021-08-01 17:16:03", "link": "http://arxiv.org/abs/2108.00500v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Is Disentanglement enough? On Latent Representations for Controllable\n  Music Generation", "abstract": "Improving controllability or the ability to manipulate one or more attributes\nof the generated data has become a topic of interest in the context of deep\ngenerative models of music. Recent attempts in this direction have relied on\nlearning disentangled representations from data such that the underlying\nfactors of variation are well separated. In this paper, we focus on the\nrelationship between disentanglement and controllability by conducting a\nsystematic study using different supervised disentanglement learning algorithms\nbased on the Variational Auto-Encoder (VAE) architecture. Our experiments show\nthat a high degree of disentanglement can be achieved by using different forms\nof supervision to train a strong discriminative encoder. However, in the\nabsence of a strong generative decoder, disentanglement does not necessarily\nimply controllability. The structure of the latent space with respect to the\nVAE-decoder plays an important role in boosting the ability of a generative\nmodel to manipulate different attributes. To this end, we also propose methods\nand metrics to help evaluate the quality of a latent space with respect to the\nafforded degree of controllability.", "published": "2021-08-01 18:37:43", "link": "http://arxiv.org/abs/2108.01450v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
