{"title": "Does the Geometry of Word Embeddings Help Document Classification? A\n  Case Study on Persistent Homology Based Representations", "abstract": "We investigate the pertinence of methods from algebraic topology for text\ndata analysis. These methods enable the development of\nmathematically-principled isometric-invariant mappings from a set of vectors to\na document embedding, which is stable with respect to the geometry of the\ndocument in the selected metric space. In this work, we evaluate the utility of\nthese topology-based document representations in traditional NLP tasks,\nspecifically document clustering and sentiment classification. We find that the\nembeddings do not benefit text analysis. In fact, performance is worse than\nsimple techniques like $\\textit{tf-idf}$, indicating that the geometry of the\ndocument does not provide enough variability for classification on the basis of\ntopic or sentiment in the chosen datasets.", "published": "2017-05-31 00:43:04", "link": "http://arxiv.org/abs/1705.10900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysis of the Effect of Dependency Information on Predicate-Argument\n  Structure Analysis and Zero Anaphora Resolution", "abstract": "This paper investigates and analyzes the effect of dependency information on\npredicate-argument structure analysis (PASA) and zero anaphora resolution (ZAR)\nfor Japanese, and shows that a straightforward approach of PASA and ZAR works\neffectively even if dependency information was not available. We constructed an\nanalyzer that directly predicts relationships of predicates and arguments with\ntheir semantic roles from a POS-tagged corpus. The features of the system are\ndesigned to compensate for the absence of syntactic information by using\nfeatures used in dependency parsing as a reference. We also constructed\nanalyzers that use the oracle dependency and the real dependency parsing\nresults, and compared with the system that does not use any syntactic\ninformation to verify that the improvement provided by dependencies is not\ncrucial.", "published": "2017-05-31 07:32:29", "link": "http://arxiv.org/abs/1705.10962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning When to Attend for Neural Machine Translation", "abstract": "In the past few years, attention mechanisms have become an indispensable\ncomponent of end-to-end neural machine translation models. However, previous\nattention models always refer to some source words when predicting a target\nword, which contradicts with the fact that some target words have no\ncorresponding source words. Motivated by this observation, we propose a novel\nattention model that has the capability of determining when a decoder should\nattend to source words and when it should not. Experimental results on NIST\nChinese-English translation tasks show that the new model achieves an\nimprovement of 0.8 BLEU score over a state-of-the-art baseline.", "published": "2017-05-31 16:05:49", "link": "http://arxiv.org/abs/1705.11160v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are distributional representations ready for the real world? Evaluating\n  word vectors for grounded perceptual meaning", "abstract": "Distributional word representation methods exploit word co-occurrences to\nbuild compact vector encodings of words. While these representations enjoy\nwidespread use in modern natural language processing, it is unclear whether\nthey accurately encode all necessary facets of conceptual meaning. In this\npaper, we evaluate how well these representations can predict perceptual and\nconceptual features of concrete concepts, drawing on two semantic norm datasets\nsourced from human participants. We find that several standard word\nrepresentations fail to encode many salient perceptual features of concepts,\nand show that these deficits correlate with word-word similarity prediction\nerrors. Our analyses provide motivation for grounded and embodied language\nlearning approaches, which may help to remedy these deficits.", "published": "2017-05-31 16:31:54", "link": "http://arxiv.org/abs/1705.11168v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adversarial Ranking for Language Generation", "abstract": "Generative adversarial networks (GANs) have great successes on synthesizing\ndata. However, the existing GANs restrict the discriminator to be a binary\nclassifier, and thus limit their learning capacity for tasks that need to\nsynthesize output with rich structures such as natural language descriptions.\nIn this paper, we propose a novel generative adversarial network, RankGAN, for\ngenerating high-quality language descriptions. Rather than training the\ndiscriminator to learn and assign absolute binary predicate for individual data\nsample, the proposed RankGAN is able to analyze and rank a collection of\nhuman-written and machine-written sentences by giving a reference group. By\nviewing a set of data samples collectively and evaluating their quality through\nrelative ranking scores, the discriminator is able to make better assessment\nwhich in turn helps to learn a better generator. The proposed RankGAN is\noptimized through the policy gradient technique. Experimental results on\nmultiple public datasets clearly demonstrate the effectiveness of the proposed\napproach.", "published": "2017-05-31 09:21:04", "link": "http://arxiv.org/abs/1705.11001v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Adversarial Generation of Natural Language", "abstract": "Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.", "published": "2017-05-31 03:06:39", "link": "http://arxiv.org/abs/1705.10929v1", "categories": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Controllable Invariance through Adversarial Feature Learning", "abstract": "Learning meaningful representations that maintain the content necessary for a\nparticular task while filtering away detrimental variations is a problem of\ngreat interest in machine learning. In this paper, we tackle the problem of\nlearning representations invariant to a specific factor or trait of data. The\nrepresentation learning process is formulated as an adversarial minimax game.\nWe analyze the optimal equilibrium of such a game and find that it amounts to\nmaximizing the uncertainty of inferring the detrimental factor given the\nrepresentation while maximizing the certainty of making task-specific\npredictions. On three benchmark tasks, namely fair and bias-free\nclassification, language-independent generation, and lighting-independent image\nclassification, we show that the proposed framework induces an invariant\nrepresentation, and leads to better generalization evidenced by the improved\nperformance.", "published": "2017-05-31 14:57:33", "link": "http://arxiv.org/abs/1705.11122v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Emergence of Language with Multi-agent Games: Learning to Communicate\n  with Sequences of Symbols", "abstract": "Learning to communicate through interaction, rather than relying on explicit\nsupervision, is often considered a prerequisite for developing a general AI. We\nstudy a setting where two agents engage in playing a referential game and, from\nscratch, develop a communication protocol necessary to succeed in this game.\nUnlike previous work, we require that messages they exchange, both at train and\ntest time, are in the form of a language (i.e. sequences of discrete symbols).\nWe compare a reinforcement learning approach and one using a differentiable\nrelaxation (straight-through Gumbel-softmax estimator) and observe that the\nlatter is much faster to converge and it results in more effective protocols.\nInterestingly, we also observe that the protocol we induce by optimizing the\ncommunication success exhibits a degree of compositionality and variability\n(i.e. the same information can be phrased in different ways), both properties\ncharacteristic of natural languages. As the ultimate goal is to ensure that\ncommunication is accomplished in natural language, we also perform experiments\nwhere we inject prior information about natural language into our model and\nstudy properties of the resulting protocol.", "published": "2017-05-31 17:47:55", "link": "http://arxiv.org/abs/1705.11192v2", "categories": ["cs.LG", "cs.CL", "cs.CV", "cs.MA"], "primary_category": "cs.LG"}
