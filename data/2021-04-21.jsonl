{"title": "Discriminative Self-training for Punctuation Prediction", "abstract": "Punctuation prediction for automatic speech recognition (ASR) output\ntranscripts plays a crucial role for improving the readability of the ASR\ntranscripts and for improving the performance of downstream natural language\nprocessing applications. However, achieving good performance on punctuation\nprediction often requires large amounts of labeled speech transcripts, which is\nexpensive and laborious. In this paper, we propose a Discriminative\nSelf-Training approach with weighted loss and discriminative label smoothing to\nexploit unlabeled speech transcripts. Experimental results on the English\nIWSLT2011 benchmark test set and an internal Chinese spoken language dataset\ndemonstrate that the proposed approach achieves significant improvement on\npunctuation prediction accuracy over strong baselines including BERT, RoBERTa,\nand ELECTRA models. The proposed Discriminative Self-Training approach\noutperforms the vanilla self-training approach. We establish a new\nstate-of-the-art (SOTA) on the IWSLT2011 test set, outperforming the current\nSOTA model by 1.3% absolute gain on F$_1$.", "published": "2021-04-21 03:32:47", "link": "http://arxiv.org/abs/2104.10339v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Biomedical Pretrained Language Models with Knowledge", "abstract": "Pretrained language models have shown success in many natural language\nprocessing tasks. Many works explore incorporating knowledge into language\nmodels. In the biomedical domain, experts have taken decades of effort on\nbuilding large-scale knowledge bases. For example, the Unified Medical Language\nSystem (UMLS) contains millions of entities with their synonyms and defines\nhundreds of relations among entities. Leveraging this knowledge can benefit a\nvariety of downstream tasks such as named entity recognition and relation\nextraction. To this end, we propose KeBioLM, a biomedical pretrained language\nmodel that explicitly leverages knowledge from the UMLS knowledge bases.\nSpecifically, we extract entities from PubMed abstracts and link them to UMLS.\nWe then train a knowledge-aware language model that firstly applies a text-only\nencoding layer to learn entity representation and applies a text-entity fusion\nencoding to aggregate entity representation. Besides, we add two training\nobjectives as entity detection and entity linking. Experiments on the named\nentity recognition and relation extraction from the BLURB benchmark demonstrate\nthe effectiveness of our approach. Further analysis on a collected probing\ndataset shows that our model has better ability to model medical knowledge.", "published": "2021-04-21 03:57:26", "link": "http://arxiv.org/abs/2104.10344v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pre-training for Spoken Language Understanding with Joint Textual and\n  Phonetic Representation Learning", "abstract": "In the traditional cascading architecture for spoken language understanding\n(SLU), it has been observed that automatic speech recognition errors could be\ndetrimental to the performance of natural language understanding. End-to-end\n(E2E) SLU models have been proposed to directly map speech input to desired\nsemantic frame with a single model, hence mitigating ASR error propagation.\nRecently, pre-training technologies have been explored for these E2E models. In\nthis paper, we propose a novel joint textual-phonetic pre-training approach for\nlearning spoken language representations, aiming at exploring the full\npotentials of phonetic information to improve SLU robustness to ASR errors. We\nexplore phoneme labels as high-level speech features, and design and compare\npre-training tasks based on conditional masked language model objectives and\ninter-sentence relation objectives. We also investigate the efficacy of\ncombining textual and phonetic information during fine-tuning. Experimental\nresults on spoken language understanding benchmarks, Fluent Speech Commands and\nSNIPS, show that the proposed approach significantly outperforms strong\nbaseline models and improves robustness of spoken language understanding to ASR\nerrors.", "published": "2021-04-21 05:19:13", "link": "http://arxiv.org/abs/2104.10357v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Speech Translation via Cross-modal Progressive Training", "abstract": "End-to-end speech translation models have become a new trend in research due\nto their potential of reducing error propagation. However, these models still\nsuffer from the challenge of data scarcity. How to effectively use unlabeled or\nother parallel corpora from machine translation is promising but still an open\nproblem. In this paper, we propose Cross Speech-Text Network (XSTNet), an\nend-to-end model for speech-to-text translation. XSTNet takes both speech and\ntext as input and outputs both transcription and translation text. The model\nbenefits from its three key design aspects: a self-supervised pre-trained\nsub-network as the audio encoder, a multi-task training objective to exploit\nadditional parallel bilingual text, and a progressive training procedure. We\nevaluate the performance of XSTNet and baselines on the MuST-C En-X and\nLibriSpeech En-Fr datasets. In particular, XSTNet achieves state-of-the-art\nresults on all language directions with an average BLEU of 28.8, outperforming\nthe previous best method by 3.2 BLEU. Code, models, cases, and more detailed\nanalysis are available at https://github.com/ReneeYe/XSTNet.", "published": "2021-04-21 06:44:31", "link": "http://arxiv.org/abs/2104.10380v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On User Interfaces for Large-Scale Document-Level Human Evaluation of\n  Machine Translation Outputs", "abstract": "Recent studies emphasize the need of document context in human evaluation of\nmachine translations, but little research has been done on the impact of user\ninterfaces on annotator productivity and the reliability of assessments. In\nthis work, we compare human assessment data from the last two WMT evaluation\ncampaigns collected via two different methods for document-level evaluation.\nOur analysis shows that a document-centric approach to evaluation where the\nannotator is presented with the entire document context on a screen leads to\nhigher quality segment and document level assessments. It improves the\ncorrelation between segment and document scores and increases inter-annotator\nagreement for document scores but is considerably more time consuming for\nannotators.", "published": "2021-04-21 08:40:18", "link": "http://arxiv.org/abs/2104.10408v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-end Biomedical Entity Linking with Span-based Dictionary Matching", "abstract": "Disease name recognition and normalization, which is generally called\nbiomedical entity linking, is a fundamental process in biomedical text mining.\nRecently, neural joint learning of both tasks has been proposed to utilize the\nmutual benefits. While this approach achieves high performance, disease\nconcepts that do not appear in the training dataset cannot be accurately\npredicted. This study introduces a novel end-to-end approach that combines span\nrepresentations with dictionary-matching features to address this problem. Our\nmodel handles unseen concepts by referring to a dictionary while maintaining\nthe performance of neural network-based models, in an end-to-end fashion.\nExperiments using two major datasets demonstrate that our model achieved\ncompetitive results with strong baselines, especially for unseen concepts\nduring training.", "published": "2021-04-21 12:24:12", "link": "http://arxiv.org/abs/2104.10493v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disfluency Detection with Unlabeled Data and Small BERT Models", "abstract": "Disfluency detection models now approach high accuracy on English text.\nHowever, little exploration has been done in improving the size and inference\ntime of the model. At the same time, automatic speech recognition (ASR) models\nare moving from server-side inference to local, on-device inference. Supporting\nmodels in the transcription pipeline (like disfluency detection) must follow\nsuit. In this work we concentrate on the disfluency detection task, focusing on\nsmall, fast, on-device models based on the BERT architecture. We demonstrate it\nis possible to train disfluency detection models as small as 1.3 MiB, while\nretaining high performance. We build on previous work that showed the benefit\nof data augmentation approaches such as self-training. Then, we evaluate the\neffect of domain mismatch between conversational and written text on model\nperformance. We find that domain adaptation and data augmentation strategies\nhave a more pronounced effect on these smaller models, as compared to\nconventional BERT models.", "published": "2021-04-21 21:24:32", "link": "http://arxiv.org/abs/2104.10769v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Extracting Adverse Drug Events from Clinical Notes", "abstract": "Adverse drug events (ADEs) are unexpected incidents caused by the\nadministration of a drug or medication. To identify and extract these events,\nwe require information about not just the drug itself but attributes describing\nthe drug (e.g., strength, dosage), the reason why the drug was initially\nprescribed, and any adverse reaction to the drug. This paper explores the\nrelationship between a drug and its associated attributes using relation\nextraction techniques. We explore three approaches: a rule-based approach, a\ndeep learning-based approach, and a contextualized language model-based\napproach. We evaluate our system on the n2c2-2018 ADE extraction dataset. Our\nexperimental results demonstrate that the contextualized language model-based\napproach outperformed other models overall and obtain the state-of-the-art\nperformance in ADE extraction with a Precision of 0.93, Recall of 0.96, and an\n$F_1$ score of 0.94; however, for certain relation types, the rule-based\napproach obtained a higher Precision and Recall than either learning approach.", "published": "2021-04-21 23:10:20", "link": "http://arxiv.org/abs/2104.10791v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Diverse and Specific Clarification Question Generation with Keywords", "abstract": "Product descriptions on e-commerce websites often suffer from missing\nimportant aspects. Clarification question generation (CQGen) can be a promising\napproach to help alleviate the problem. Unlike traditional QGen assuming the\nexistence of answers in the context and generating questions accordingly, CQGen\nmimics user behaviors of asking for unstated information. The generated CQs can\nserve as a sanity check or proofreading to help e-commerce merchant to identify\npotential missing information before advertising their product, and improve\nconsumer experience consequently. Due to the variety of possible user\nbackgrounds and use cases, the information need can be quite diverse but also\nspecific to a detailed topic, while previous works assume generating one CQ per\ncontext and the results tend to be generic. We thus propose the task of Diverse\nCQGen and also tackle the challenge of specificity. We propose a new model\nnamed KPCNet, which generates CQs with Keyword Prediction and Conditioning, to\ndeal with the tasks. Automatic and human evaluation on 2 datasets (Home &\nKitchen, Office) showed that KPCNet can generate more specific questions and\npromote better group-level diversity than several competing baselines.", "published": "2021-04-21 02:29:33", "link": "http://arxiv.org/abs/2104.10317v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor\n  Based on Multi-Task Adversarial Training", "abstract": "This paper describes MagicPai's system for SemEval 2021 Task 7, HaHackathon:\nDetecting and Rating Humor and Offense. This task aims to detect whether the\ntext is humorous and how humorous it is. There are four subtasks in the\ncompetition. In this paper, we mainly present our solution, a multi-task\nlearning model based on adversarial examples, for task 1a and 1b. More\nspecifically, we first vectorize the cleaned dataset and add the perturbation\nto obtain more robust embedding representations. We then correct the loss via\nthe confidence level. Finally, we perform interactive joint learning on\nmultiple tasks to capture the relationship between whether the text is humorous\nand how humorous it is. The final result shows the effectiveness of our system.", "published": "2021-04-21 03:23:02", "link": "http://arxiv.org/abs/2104.10336v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement\n  Verification and Evidence Finding with Tables", "abstract": "Question answering from semi-structured tables can be seen as a semantic\nparsing task and is significant and practical for pushing the boundary of\nnatural language understanding. Existing research mainly focuses on\nunderstanding contents from unstructured evidence, e.g., news, natural language\nsentences, and documents. The task of verification from structured evidence,\nsuch as tables, charts, and databases, is still less explored. This paper\ndescribes sattiy team's system in SemEval-2021 task 9: Statement Verification\nand Evidence Finding with Tables (SEM-TAB-FACT). This competition aims to\nverify statements and to find evidence from tables for scientific articles and\nto promote the proper interpretation of the surrounding article. In this paper,\nwe exploited ensemble models of pre-trained language models over tables, TaPas\nand TaBERT, for Task A and adjust the result based on some rules extracted for\nTask B. Finally, in the leaderboard, we attain the F1 scores of 0.8496 and\n0.7732 in Task A for the 2-way and 3-way evaluation, respectively, and the F1\nscore of 0.4856 in Task B.", "published": "2021-04-21 06:11:49", "link": "http://arxiv.org/abs/2104.10366v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context\n  Disambiguation", "abstract": "This paper presents the PALI team's winning system for SemEval-2021 Task 2:\nMultilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune\nXLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to\ndetermine whether the target word in the two contexts contains the same meaning\nor not. In the implementation, we first specifically design an input tag to\nemphasize the target word in the contexts. Second, we construct a new vector on\nthe fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected\nnetwork to output the probability of whether the target word in the context has\nthe same meaning or not. The new vector is attained by concatenating the\nembedding of the [CLS] token and the embeddings of the target word in the\ncontexts. In training, we explore several tricks, such as the Ranger optimizer,\ndata augmentation, and adversarial training, to improve the model prediction.\nConsequently, we attain first place in all four cross-lingual tasks.", "published": "2021-04-21 06:24:49", "link": "http://arxiv.org/abs/2104.10375v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Should we Stop Training More Monolingual Models, and Simply Use Machine\n  Translation Instead?", "abstract": "Most work in NLP makes the assumption that it is desirable to develop\nsolutions in the native language in question. There is consequently a strong\ntrend towards building native language models even for low-resource languages.\nThis paper questions this development, and explores the idea of simply\ntranslating the data into English, thereby enabling the use of pretrained, and\nlarge-scale, English language models. We demonstrate empirically that a large\nEnglish language model coupled with modern machine translation outperforms\nnative language models in most Scandinavian languages. The exception to this is\nFinnish, which we assume is due to inferior translation quality. Our results\nsuggest that machine translation is a mature technology, which raises a serious\ncounter-argument for training native language models for low-resource\nlanguages. This paper therefore strives to make a provocative but important\npoint. As English language models are improving at an unprecedented pace, which\nin turn improves machine translation, it is from an empirical and environmental\nstand-point more effective to translate data from low-resource languages into\nEnglish, than to build language models for such languages.", "published": "2021-04-21 10:21:24", "link": "http://arxiv.org/abs/2104.10441v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "How Will Your Tweet Be Received? Predicting the Sentiment Polarity of\n  Tweet Replies", "abstract": "Twitter sentiment analysis, which often focuses on predicting the polarity of\ntweets, has attracted increasing attention over the last years, in particular\nwith the rise of deep learning (DL). In this paper, we propose a new task:\npredicting the predominant sentiment among (first-order) replies to a given\ntweet. Therefore, we created RETWEET, a large dataset of tweets and replies\nmanually annotated with sentiment labels. As a strong baseline, we propose a\ntwo-stage DL-based method: first, we create automatically labeled training data\nby applying a standard sentiment classifier to tweet replies and aggregating\nits predictions for each original tweet; our rationale is that individual\nerrors made by the classifier are likely to cancel out in the aggregation step.\nSecond, we use the automatically labeled data for supervised training of a\nneural network to predict reply sentiment from the original tweets. The\nresulting classifier is evaluated on the new RETWEET dataset, showing promising\nresults, especially considering that it has been trained without any manually\nlabeled data. Both the dataset and the baseline implementation are publicly\navailable.", "published": "2021-04-21 13:08:45", "link": "http://arxiv.org/abs/2104.10513v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Improving BERT Pretraining with Syntactic Supervision", "abstract": "Bidirectional masked Transformers have become the core theme in the current\nNLP landscape. Despite their impressive benchmarks, a recurring theme in recent\nresearch has been to question such models' capacity for syntactic\ngeneralization. In this work, we seek to address this question by adding a\nsupervised, token-level supertagging objective to standard unsupervised\npretraining, enabling the explicit incorporation of syntactic biases into the\nnetwork's training dynamics. Our approach is straightforward to implement,\ninduces a marginal computational overhead and is general enough to adapt to a\nvariety of settings. We apply our methodology on Lassy Large, an automatically\nannotated corpus of written Dutch. Our experiments suggest that our\nsyntax-aware model performs on par with established baselines, despite Lassy\nLarge being one order of magnitude smaller than commonly used corpora.", "published": "2021-04-21 13:15:58", "link": "http://arxiv.org/abs/2104.10516v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Fine-grained Fact-Article Correspondence in Legal Cases", "abstract": "Automatically recommending relevant law articles to a given legal case has\nattracted much attention as it can greatly release human labor from searching\nover the large database of laws. However, current researches only support\ncoarse-grained recommendation where all relevant articles are predicted as a\nwhole without explaining which specific fact each article is relevant with.\nSince one case can be formed of many supporting facts, traversing over them to\nverify the correctness of recommendation results can be time-consuming. We\nbelieve that learning fine-grained correspondence between each single fact and\nlaw articles is crucial for an accurate and trustworthy AI system. With this\nmotivation, we perform a pioneering study and create a corpus with manually\nannotated fact-article correspondences. We treat the learning as a text\nmatching task and propose a multi-level matching network to address it. To help\nthe model better digest the content of law articles, we parse articles in form\nof premise-conclusion pairs with random forest. Experiments show that the\nparsed form yielded better performance and the resulting model surpassed other\npopular text matching baselines. Furthermore, we compare with previous\nresearches and find that establishing the fine-grained fact-article\ncorrespondences can improve the recommendation accuracy by a large margin. Our\nbest system reaches an F1 score of 96.3%, making it of great potential for\npractical use. It can also significantly boost the downstream task of legal\ndecision prediction, increasing the F1 score by up to 12.7%.", "published": "2021-04-21 19:06:58", "link": "http://arxiv.org/abs/2104.10726v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Sensitivity as a Complexity Measure for Sequence Classification Tasks", "abstract": "We introduce a theoretical framework for understanding and predicting the\ncomplexity of sequence classification tasks, using a novel extension of the\ntheory of Boolean function sensitivity. The sensitivity of a function, given a\ndistribution over input sequences, quantifies the number of disjoint subsets of\nthe input sequence that can each be individually changed to change the output.\nWe argue that standard sequence classification methods are biased towards\nlearning low-sensitivity functions, so that tasks requiring high sensitivity\nare more difficult. To that end, we show analytically that simple lexical\nclassifiers can only express functions of bounded sensitivity, and we show\nempirically that low-sensitivity functions are easier to learn for LSTMs. We\nthen estimate sensitivity on 15 NLP tasks, finding that sensitivity is higher\non challenging tasks collected in GLUE than on simple text classification\ntasks, and that sensitivity predicts the performance both of simple lexical\nclassifiers and of vanilla BiLSTMs without pretrained contextualized\nembeddings. Within a task, sensitivity predicts which inputs are hard for such\nsimple models. Our results suggest that the success of massively pretrained\ncontextual representations stems in part because they provide representations\nfrom which information can be extracted by low-sensitivity decoders.", "published": "2021-04-21 03:56:59", "link": "http://arxiv.org/abs/2104.10343v1", "categories": ["cs.CL", "cs.CC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Document Representations for Large-Scale Zero-Shot Learning", "abstract": "Zero-shot learning aims to recognize unseen objects using their semantic\nrepresentations. Most existing works use visual attributes labeled by humans,\nnot suitable for large-scale applications. In this paper, we revisit the use of\ndocuments as semantic representations. We argue that documents like Wikipedia\npages contain rich visual information, which however can easily be buried by\nthe vast amount of non-visual sentences. To address this issue, we propose a\nsemi-automatic mechanism for visual sentence extraction that leverages the\ndocument section headers and the clustering structure of visual sentences. The\nextracted visual sentences, after a novel weighting scheme to distinguish\nsimilar classes, essentially form semantic representations like visual\nattributes but need much less human effort. On the ImageNet dataset with over\n10,000 unseen classes, our representations lead to a 64% relative improvement\nagainst the commonly used ones.", "published": "2021-04-21 05:17:55", "link": "http://arxiv.org/abs/2104.10355v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Link Prediction on N-ary Relational Data Based on Relatedness Evaluation", "abstract": "With the overwhelming popularity of Knowledge Graphs (KGs), researchers have\npoured attention to link prediction to fill in missing facts for a long time.\nHowever, they mainly focus on link prediction on binary relational data, where\nfacts are usually represented as triples in the form of (head entity, relation,\ntail entity). In practice, n-ary relational facts are also ubiquitous. When\nencountering such facts, existing studies usually decompose them into triples\nby introducing a multitude of auxiliary virtual entities and additional\ntriples. These conversions result in the complexity of carrying out link\nprediction on n-ary relational data. It has even proven that they may cause\nloss of structure information. To overcome these problems, in this paper, we\nrepresent each n-ary relational fact as a set of its role and role-value pairs.\nWe then propose a method called NaLP to conduct link prediction on n-ary\nrelational data, which explicitly models the relatedness of all the role and\nrole-value pairs in an n-ary relational fact. We further extend NaLP by\nintroducing type constraints of roles and role-values without any external\ntype-specific supervision, and proposing a more reasonable negative sampling\nmechanism. Experimental results validate the effectiveness and merits of the\nproposed methods.", "published": "2021-04-21 09:06:54", "link": "http://arxiv.org/abs/2104.10424v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Text Summarization of Czech News Articles Using Named Entities", "abstract": "The foundation for the research of summarization in the Czech language was\nlaid by the work of Straka et al. (2018). They published the SumeCzech, a large\nCzech news-based summarization dataset, and proposed several baseline\napproaches. However, it is clear from the achieved results that there is a\nlarge space for improvement. In our work, we focus on the impact of named\nentities on the summarization of Czech news articles. First, we annotate\nSumeCzech with named entities. We propose a new metric ROUGE_NE that measures\nthe overlap of named entities between the true and generated summaries, and we\nshow that it is still challenging for summarization systems to reach a high\nscore in it. We propose an extractive summarization approach Named Entity\nDensity that selects a sentence with the highest ratio between a number of\nentities and the length of the sentence as the summary of the article. The\nexperiments show that the proposed approach reached results close to the solid\nbaseline in the domain of news articles selecting the first sentence. Moreover,\nwe demonstrate that the selected sentence reflects the style of reports\nconcisely identifying to whom, when, where, and what happened. We propose that\nsuch a summary is beneficial in combination with the first sentence of an\narticle in voice applications presenting news articles. We propose two\nabstractive summarization approaches based on Seq2Seq architecture. The first\napproach uses the tokens of the article. The second approach has access to the\nnamed entity annotations. The experiments show that both approaches exceed\nstate-of-the-art results previously reported by Straka et al. (2018), with the\nlatter achieving slightly better results on SumeCzech's out-of-domain testing\nset.", "published": "2021-04-21 10:48:14", "link": "http://arxiv.org/abs/2104.10454v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Sampling-Based Training Criteria for Neural Language Modeling", "abstract": "As the vocabulary size of modern word-based language models becomes ever\nlarger, many sampling-based training criteria are proposed and investigated.\nThe essence of these sampling methods is that the softmax-related traversal\nover the entire vocabulary can be simplified, giving speedups compared to the\nbaseline. A problem we notice about the current landscape of such sampling\nmethods is the lack of a systematic comparison and some myths about preferring\none over another. In this work, we consider Monte Carlo sampling, importance\nsampling, a novel method we call compensated partial summation, and noise\ncontrastive estimation. Linking back to the three traditional criteria, namely\nmean squared error, binary cross-entropy, and cross-entropy, we derive the\ntheoretical solutions to the training problems. Contrary to some common belief,\nwe show that all these sampling methods can perform equally well, as long as we\ncorrect for the intended class posterior probabilities. Experimental results in\nlanguage modeling and automatic speech recognition on Switchboard and\nLibriSpeech support our claim, with all sampling-based methods showing similar\nperplexities and word error rates while giving the expected speedups.", "published": "2021-04-21 12:55:52", "link": "http://arxiv.org/abs/2104.10507v2", "categories": ["cs.CL", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Accented Speech Recognition: A Survey", "abstract": "Automatic Speech Recognition (ASR) systems generalize poorly on accented\nspeech. The phonetic and linguistic variability of accents present hard\nchallenges for ASR systems today in both data collection and modeling\nstrategies. The resulting bias in ASR performance across accents comes at a\ncost to both users and providers of ASR.\n  We present a survey of current promising approaches to accented speech\nrecognition and highlight the key challenges in the space. Approaches mostly\nfocus on single model generalization and accent feature engineering. Among the\nchallenges, lack of a standard benchmark makes research and comparison\nespecially difficult.", "published": "2021-04-21 20:21:06", "link": "http://arxiv.org/abs/2104.10747v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Clustering Introductory Computer Science Exercises Using Topic Modeling\n  Methods", "abstract": "Manually determining concepts present in a group of questions is a\nchallenging and time-consuming process. However, the process is an essential\nstep while modeling a virtual learning environment since a mapping between\nconcepts and questions using mastery level assessment and recommendation\nengines are required. We investigated unsupervised semantic models (known as\ntopic modeling techniques) to assist computer science teachers in this task and\npropose a method to transform Computer Science 1 teacher-provided code\nsolutions into representative text documents, including the code structure\ninformation. By applying non-negative matrix factorization and latent Dirichlet\nallocation techniques, we extract the underlying relationship between questions\nand validate the results using an external dataset. We consider the\ninterpretability of the learned concepts using 14 university professors' data,\nand the results confirm six semantically coherent clusters using the current\ndataset. Moreover, the six topics comprise the main concepts present in the\ntest dataset, achieving 0.75 in the normalized pointwise mutual information\nmetric. The metric correlates with human ratings, making the proposed method\nuseful and providing semantics for large amounts of unannotated code.", "published": "2021-04-21 20:23:53", "link": "http://arxiv.org/abs/2104.10748v1", "categories": ["cs.LG", "cs.CL", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Adapting Long Context NLM for ASR Rescoring in Conversational Agents", "abstract": "Neural Language Models (NLM), when trained and evaluated with context\nspanning multiple utterances, have been shown to consistently outperform both\nconventional n-gram language models and NLMs that use limited context. In this\npaper, we investigate various techniques to incorporate turn based context\nhistory into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent\nbased NLMs, we explore context carry over mechanism and feature based\naugmentation, where we incorporate other forms of contextual information such\nas bot response and system dialogue acts as classified by a Natural Language\nUnderstanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem\nwith contextual NLM, we propose the use of attention layer over lexical\nmetadata to improve feature based augmentation. Additionally, we adapt our\ncontextual NLM towards user provided on-the-fly speech patterns by leveraging\nencodings from a large pre-trained masked language model and performing fusion\nwith a Transformer-XL based NLM. We test our proposed models using N-best\nrescoring of ASR hypotheses of task-oriented dialogues and also evaluate on\ndownstream NLU tasks such as intent classification and slot labeling. The best\nperforming model shows a relative WER between 1.6% and 9.1% and a slot labeling\nF1 score improvement of 4% over non-contextual baselines.", "published": "2021-04-21 00:15:21", "link": "http://arxiv.org/abs/2104.11070v2", "categories": ["cs.CL", "cs.LG", "cs.SD"], "primary_category": "cs.CL"}
{"title": "Room adaptive conditioning method for sound event classification in\n  reverberant environments", "abstract": "Ensuring performance robustness for a variety of situations that can occur in\nreal-world environments is one of the challenging tasks in sound event\nclassification. One of the unpredictable and detrimental factors in\nperformance, especially in indoor environments, is reverberation. To alleviate\nthis problem, we propose a conditioning method that provides room impulse\nresponse (RIR) information to help the network become less sensitive to\nenvironmental information and focus on classifying the desired sound.\nExperimental results show that the proposed method successfully reduced\nperformance degradation caused by the reverberation of the room. In particular,\nour proposed method works even with similar RIR that can be inferred from the\nroom type rather than the exact one, which has the advantage of potentially\nbeing used in real-world applications.", "published": "2021-04-21 09:42:05", "link": "http://arxiv.org/abs/2104.10431v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Scene-aware Far-field Automatic Speech Recognition", "abstract": "We propose a novel method for generating scene-aware training data for\nfar-field automatic speech recognition. We use a deep learning-based estimator\nto non-intrusively compute the sub-band reverberation time of an environment\nfrom its speech samples. We model the acoustic characteristics of a scene with\nits reverberation time and represent it using a multivariate Gaussian\ndistribution. We use this distribution to select acoustic impulse responses\nfrom a large real-world dataset for augmenting speech data. The speech\nrecognition system trained on our scene-aware data consistently outperforms the\nsystem trained using many more random acoustic impulse responses on the REVERB\nand the AMI far-field benchmarks. In practice, we obtain 2.64% absolute\nimprovement in word error rate compared with using training data of the same\nsize with uniformly distributed reverberation times.", "published": "2021-04-21 20:58:30", "link": "http://arxiv.org/abs/2104.10757v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Label-Synchronous Speech-to-Text Alignment for ASR Using Forward and\n  Backward Transformers", "abstract": "This paper proposes a novel label-synchronous speech-to-text alignment\ntechnique for automatic speech recognition (ASR). The speech-to-text alignment\nis a problem of splitting long audio recordings with un-aligned transcripts\ninto utterance-wise pairs of speech and text. Unlike conventional methods based\non frame-synchronous prediction, the proposed method re-defines the\nspeech-to-text alignment as a label-synchronous text mapping problem. This\nenables an accurate alignment benefiting from the strong inference ability of\nthe state-of-the-art attention-based encoder-decoder models, which cannot be\napplied to the conventional methods. Two different Transformer models named\nforward Transformer and backward Transformer are respectively used for\nestimating an initial and final tokens of a given speech segment based on\nend-of-sentence prediction with teacher-forcing. Experiments using the corpus\nof spontaneous Japanese (CSJ) demonstrate that the proposed method provides an\naccurate utterance-wise alignment, that matches the manually annotated\nalignment with as few as 0.2% errors. It is also confirmed that a\nTransformer-based hybrid CTC/Attention ASR model using the aligned speech and\ntext pairs as an additional training data reduces character error rates\nrelatively up to 59.0%, which is significantly better than 39.0% reduction by a\nconventional alignment method based on connectionist temporal classification\nmodel.", "published": "2021-04-21 03:05:12", "link": "http://arxiv.org/abs/2104.10328v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices", "abstract": "This work focuses on the analysis that whether 3D face models can be learned\nfrom only the speech inputs of speakers. Previous works for cross-modal face\nsynthesis study image generation from voices. However, image synthesis includes\nvariations such as hairstyles, backgrounds, and facial textures, that are\narguably irrelevant to voice or without direct studies to show correlations. We\ninstead investigate the ability to reconstruct 3D faces to concentrate on only\ngeometry, which is more physiologically grounded. We propose both the\nsupervised learning and unsupervised learning frameworks. Especially we\ndemonstrate how unsupervised learning is possible in the absence of a direct\nvoice-to-3D-face dataset under limited availability of 3D face scans when the\nmodel is equipped with knowledge distillation. To evaluate the performance, we\nalso propose several metrics to measure the geometric fitness of two 3D faces\nbased on points, lines, and regions. We find that 3D face shapes can be\nreconstructed from voices. Experimental results suggest that 3D faces can be\nreconstructed from voices, and our method can improve the performance over the\nbaseline. The best performance gains (15% - 20%) on ear-to-ear distance ratio\nmetric (ER) coincides with the intuition that one can roughly envision whether\na speaker's face is overall wider or thinner only from a person's voice. See\nour project page for codes and data.", "published": "2021-04-21 01:14:50", "link": "http://arxiv.org/abs/2104.10299v1", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.GR"}
