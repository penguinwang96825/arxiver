{"title": "A neural joint model for Vietnamese word segmentation, POS tagging and\n  dependency parsing", "abstract": "We propose the first multi-task learning model for joint Vietnamese word\nsegmentation, part-of-speech (POS) tagging and dependency parsing. In\nparticular, our model extends the BIST graph-based dependency parser\n(Kiperwasser and Goldberg, 2016) with BiLSTM-CRF-based neural layers (Huang et\nal., 2015) for word segmentation and POS tagging. On Vietnamese benchmark\ndatasets, experimental results show that our joint model obtains\nstate-of-the-art or competitive performances.", "published": "2018-12-30 03:03:28", "link": "http://arxiv.org/abs/1812.11459v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Variational Self-attention Model for Sentence Representation", "abstract": "This paper proposes a variational self-attention model (VSAM) that employs\nvariational inference to derive self-attention. We model the self-attention\nvector as random variables by imposing a probabilistic distribution. The\nself-attention mechanism summarizes source information as an attention vector\nby weighted sum, where the weights are a learned probabilistic distribution.\nCompared with conventional deterministic counterpart, the stochastic units\nincorporated by VSAM allow multi-modal attention distributions. Furthermore, by\nmarginalizing over the latent variables, VSAM is more robust against\noverfitting. Experiments on the stance detection task demonstrate the\nsuperiority of our method.", "published": "2018-12-30 15:36:11", "link": "http://arxiv.org/abs/1812.11559v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Sentiment Classification of Customer Reviews about Automobiles in Roman\n  Urdu", "abstract": "Text mining is a broad field having sentiment mining as its important\nconstituent in which we try to deduce the behavior of people towards a specific\nitem, merchandise, politics, sports, social media comments, review sites etc.\nOut of many issues in sentiment mining, analysis and classification, one major\nissue is that the reviews and comments can be in different languages like\nEnglish, Arabic, Urdu etc. Handling each language according to its rules is a\ndifficult task. A lot of research work has been done in English Language for\nsentiment analysis and classification but limited sentiment analysis work is\nbeing carried out on other regional languages like Arabic, Urdu and Hindi. In\nthis paper, Waikato Environment for Knowledge Analysis (WEKA) is used as a\nplatform to execute different classification models for text classification of\nRoman Urdu text. Reviews dataset has been scrapped from different automobiles\nsites. These extracted Roman Urdu reviews, containing 1000 positive and 1000\nnegative reviews, are then saved in WEKA attribute-relation file format (arff)\nas labeled examples. Training is done on 80% of this data and rest of it is\nused for testing purpose which is done using different models and results are\nanalyzed in each case. The results show that Multinomial Naive Bayes\noutperformed Bagging, Deep Neural Network, Decision Tree, Random Forest,\nAdaBoost, k-NN and SVM Classifiers in terms of more accuracy, precision, recall\nand F-measure.", "published": "2018-12-30 18:50:35", "link": "http://arxiv.org/abs/1812.11587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Selectively Transfer: Reinforced Transfer Learning for Deep\n  Text Matching", "abstract": "Deep text matching approaches have been widely studied for many applications\nincluding question answering and information retrieval systems. To deal with a\ndomain that has insufficient labeled data, these approaches can be used in a\nTransfer Learning (TL) setting to leverage labeled data from a resource-rich\nsource domain. To achieve better performance, source domain data selection is\nessential in this process to prevent the \"negative transfer\" problem. However,\nthe emerging deep transfer models do not fit well with most existing data\nselection methods, because the data selection policy and the transfer learning\nmodel are not jointly trained, leading to sub-optimal training efficiency.\n  In this paper, we propose a novel reinforced data selector to select\nhigh-quality source domain data to help the TL model. Specifically, the data\nselector \"acts\" on the source domain data to find a subset for optimization of\nthe TL model, and the performance of the TL model can provide \"rewards\" in turn\nto update the selector. We build the reinforced data selector based on the\nactor-critic framework and integrate it to a DNN based transfer learning model,\nresulting in a Reinforced Transfer Learning (RTL) method. We perform a thorough\nexperimental evaluation on two major tasks for text matching, namely,\nparaphrase identification and natural language inference. Experimental results\nshow the proposed RTL can significantly improve the performance of the TL\nmodel. We further investigate different settings of states, rewards, and policy\noptimization methods to examine the robustness of our method. Last, we conduct\na case study on the selected data and find our method is able to select source\ndomain data whose Wasserstein distance is close to the target domain data. This\nis reasonable and intuitive as such source domain data can provide more\ntransferability power to the model.", "published": "2018-12-30 15:39:57", "link": "http://arxiv.org/abs/1812.11561v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
