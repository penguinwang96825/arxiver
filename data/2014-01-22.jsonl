{"title": "A new keyphrases extraction method based on suffix tree data structure\n  for arabic documents clustering", "abstract": "Document Clustering is a branch of a larger area of scientific study known as\ndata mining .which is an unsupervised classification using to find a structure\nin a collection of unlabeled data. The useful information in the documents can\nbe accompanied by a large amount of noise words when using Full Text\nRepresentation, and therefore will affect negatively the result of the\nclustering process. So it is with great need to eliminate the noise words and\nkeeping just the useful information in order to enhance the quality of the\nclustering results. This problem occurs with different degree for any language\nsuch as English, European, Hindi, Chinese, and Arabic Language. To overcome\nthis problem, in this paper, we propose a new and efficient Keyphrases\nextraction method based on the Suffix Tree data structure (KpST), the extracted\nKeyphrases are then used in the clustering process instead of Full Text\nRepresentation. The proposed method for Keyphrases extraction is language\nindependent and therefore it may be applied to any language. In this\ninvestigation, we are interested to deal with the Arabic language which is one\nof the most complex languages. To evaluate our method, we conduct an\nexperimental study on Arabic Documents using the most popular Clustering\napproach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with\nseven linkage techniques and a variety of distance functions and similarity\nmeasures to perform Arabic Document Clustering task. The obtained results show\nthat our method for extracting Keyphrases increases the quality of the\nclustering results. We propose also to study the effect of using the stemming\nfor the testing dataset to cluster it with the same documents clustering\ntechniques and similarity/distance measures.", "published": "2014-01-22 12:36:38", "link": "http://arxiv.org/abs/1401.5644v1", "categories": ["cs.CL", "cs.IR", "H.2.3"], "primary_category": "cs.CL"}
{"title": "Parsimonious Topic Models with Salient Word Discovery", "abstract": "We propose a parsimonious topic model for text corpora. In related models\nsuch as Latent Dirichlet Allocation (LDA), all words are modeled\ntopic-specifically, even though many words occur with similar frequencies\nacross different topics. Our modeling determines salient words for each topic,\nwhich have topic-specific probabilities, with the rest explained by a universal\nshared model. Further, in LDA all topics are in principle present in every\ndocument. By contrast our model gives sparse topic representation, determining\nthe (small) subset of relevant topics for each document. We derive a Bayesian\nInformation Criterion (BIC), balancing model complexity and goodness of fit.\nHere, interestingly, we identify an effective sample size and corresponding\npenalty specific to each parameter type in our model. We minimize BIC to\njointly determine our entire model -- the topic-specific words,\ndocument-specific topics, all model parameter values, {\\it and} the total\nnumber of topics -- in a wholly unsupervised fashion. Results on three text\ncorpora and an image dataset show that our model achieves higher test set\nlikelihood and better agreement with ground-truth class labels, compared to LDA\nand to a model designed to incorporate sparsity.", "published": "2014-01-22 21:47:48", "link": "http://arxiv.org/abs/1401.6169v2", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML", "I.7.0; I.5.3; G.3; I.5.2"], "primary_category": "cs.LG"}
