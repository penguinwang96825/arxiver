{"title": "I3rab: A New Arabic Dependency Treebank Based on Arabic Grammatical\n  Theory", "abstract": "Treebanks are valuable linguistic resources that include the syntactic\nstructure of a language sentence in addition to POS-tags and morphological\nfeatures. They are mainly utilized in modeling statistical parsers. Although\nthe statistical natural language parser has recently become more accurate for\nlanguages such as English, those for the Arabic language still have low\naccuracy. The purpose of this paper is to construct a new Arabic dependency\ntreebank based on the traditional Arabic grammatical theory and the\ncharacteristics of the Arabic language, to investigate their effects on the\naccuracy of statistical parsers. The proposed Arabic dependency treebank,\ncalled I3rab, contrasts with existing Arabic dependency treebanks in two main\nconcepts. The first concept is the approach of determining the main word of the\nsentence, and the second concept is the representation of the joined and covert\npronouns. To evaluate I3rab, we compared its performance against a subset of\nPrague Arabic Dependency Treebank that shares a comparable level of details.\nThe conducted experiments show that the percentage improvement reached up to\n7.5% in UAS and 18.8% in LAS.", "published": "2020-07-11 13:34:44", "link": "http://arxiv.org/abs/2007.05772v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Feature Selection on Noisy Twitter Short Text Messages for Language\n  Identification", "abstract": "The task of written language identification involves typically the detection\nof the languages present in a sample of text. Moreover, a sequence of text may\nnot belong to a single inherent language but also may be mixture of text\nwritten in multiple languages. This kind of text is generated in large volumes\nfrom social media platforms due to its flexible and user friendly environment.\nSuch text contains very large number of features which are essential for\ndevelopment of statistical, probabilistic as well as other kinds of language\nmodels. The large number of features have rich as well as irrelevant and\nredundant features which have diverse effect over the performance of the\nlearning model. Therefore, feature selection methods are significant in\nchoosing feature that are most relevant for an efficient model. In this\narticle, we basically consider the Hindi-English language identification task\nas Hindi and English are often two most widely spoken languages of India. We\napply different feature selection algorithms across various learning algorithms\nin order to analyze the effect of the algorithm as well as the number of\nfeatures on the performance of the task. The methodology focuses on the word\nlevel language identification using a novel dataset of 6903 tweets extracted\nfrom Twitter. Various n-gram profiles are examined with different feature\nselection algorithms over many classifiers. Finally, an exhaustive comparative\nanalysis is put forward with respect to the overall experiments conducted for\nthe task.", "published": "2020-07-11 09:22:01", "link": "http://arxiv.org/abs/2007.05727v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Machine Learning Speaking my Language? A Critical Look at the\n  NLP-Pipeline Across 8 Human Languages", "abstract": "Natural Language Processing (NLP) is increasingly used as a key ingredient in\ncritical decision-making systems such as resume parsers used in sorting a list\nof job candidates. NLP systems often ingest large corpora of human text,\nattempting to learn from past human behavior and decisions in order to produce\nsystems that will make recommendations about our future world. Over 7000 human\nlanguages are being spoken today and the typical NLP pipeline underrepresents\nspeakers of most of them while amplifying the voices of speakers of other\nlanguages. In this paper, a team including speakers of 8 languages - English,\nChinese, Urdu, Farsi, Arabic, French, Spanish, and Wolof - takes a critical\nlook at the typical NLP pipeline and how even when a language is technically\nsupported, substantial caveats remain to prevent full participation. Despite\nhuge and admirable investments in multilingual support in many tools and\nresources, we are still making NLP-guided decisions that systematically and\ndramatically underrepresent the voices of much of the world.", "published": "2020-07-11 22:56:37", "link": "http://arxiv.org/abs/2007.05872v1", "categories": ["cs.CL", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Investigation of Sentiment Controllable Chatbot", "abstract": "Conventional seq2seq chatbot models attempt only to find sentences with the\nhighest probabilities conditioned on the input sequences, without considering\nthe sentiment of the output sentences. In this paper, we investigate four\nmodels to scale or adjust the sentiment of the chatbot response: a\npersona-based model, reinforcement learning, a plug and play model, and\nCycleGAN, all based on the seq2seq model. We also develop machine-evaluated\nmetrics to estimate whether the responses are reasonable given the input. These\nmetrics, together with human evaluation, are used to analyze the performance of\nthe four models in terms of different aspects; reinforcement learning and\nCycleGAN are shown to be very attractive.", "published": "2020-07-11 16:04:30", "link": "http://arxiv.org/abs/2007.07196v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Deep or Simple Models for Semantic Tagging? It Depends on your Data\n  [Experiments]", "abstract": "Semantic tagging, which has extensive applications in text mining, predicts\nwhether a given piece of text conveys the meaning of a given semantic tag. The\nproblem of semantic tagging is largely solved with supervised learning and\ntoday, deep learning models are widely perceived to be better for semantic\ntagging. However, there is no comprehensive study supporting the popular\nbelief. Practitioners often have to train different types of models for each\nsemantic tagging task to identify the best model. This process is both\nexpensive and inefficient.\n  We embark on a systematic study to investigate the following question: Are\ndeep models the best performing model for all semantic tagging tasks? To answer\nthis question, we compare deep models against \"simple models\" over datasets\nwith varying characteristics. Specifically, we select three prevalent deep\nmodels (i.e. CNN, LSTM, and BERT) and two simple models (i.e. LR and SVM), and\ncompare their performance on the semantic tagging task over 21 datasets.\nResults show that the size, the label ratio, and the label cleanliness of a\ndataset significantly impact the quality of semantic tagging. Simple models\nachieve similar tagging quality to deep models on large datasets, but the\nruntime of simple models is much shorter. Moreover, simple models can achieve\nbetter tagging quality than deep models when targeting datasets show worse\nlabel cleanliness and/or more severe imbalance. Based on these findings, our\nstudy can systematically guide practitioners in selecting the right learning\nmodel for their semantic tagging task.", "published": "2020-07-11 00:05:50", "link": "http://arxiv.org/abs/2007.05651v2", "categories": ["cs.CL", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BERT Learns (and Teaches) Chemistry", "abstract": "Modern computational organic chemistry is becoming increasingly data-driven.\nThere remain a large number of important unsolved problems in this area such as\nproduct prediction given reactants, drug discovery, and metric-optimized\nmolecule synthesis, but efforts to solve these problems using machine learning\nhave also increased in recent years. In this work, we propose the use of\nattention to study functional groups and other property-impacting molecular\nsubstructures from a data-driven perspective, using a transformer-based model\n(BERT) on datasets of string representations of molecules and analyzing the\nbehavior of its attention heads. We then apply the representations of\nfunctional groups and atoms learned by the model to tackle problems of\ntoxicity, solubility, drug-likeness, and synthesis accessibility on smaller\ndatasets using the learned representations as features for graph convolution\nand attention models on the graph structure of molecules, as well as\nfine-tuning of BERT. Finally, we propose the use of attention visualization as\na helpful tool for chemistry practitioners and students to quickly identify\nimportant substructures in various chemical properties.", "published": "2020-07-11 00:23:07", "link": "http://arxiv.org/abs/2007.16012v1", "categories": ["q-bio.BM", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "q-bio.BM"}
{"title": "Quasi-Periodic WaveNet: An Autoregressive Raw Waveform Generative Model\n  with Pitch-dependent Dilated Convolution Neural Network", "abstract": "In this paper, a pitch-adaptive waveform generative model named\nQuasi-Periodic WaveNet (QPNet) is proposed to improve the limited pitch\ncontrollability of vanilla WaveNet (WN) using pitch-dependent dilated\nconvolution neural networks (PDCNNs). Specifically, as a probabilistic\nautoregressive generation model with stacked dilated convolution layers, WN\nachieves high-fidelity audio waveform generation. However, the pure-data-driven\nnature and the lack of prior knowledge of audio signals degrade the pitch\ncontrollability of WN. For instance, it is difficult for WN to precisely\ngenerate the periodic components of audio signals when the given auxiliary\nfundamental frequency ($F_{0}$) features are outside the $F_{0}$ range observed\nin the training data. To address this problem, QPNet with two novel designs is\nproposed. First, the PDCNN component is applied to dynamically change the\nnetwork architecture of WN according to the given auxiliary $F_{0}$ features.\nSecond, a cascaded network structure is utilized to simultaneously model the\nlong- and short-term dependencies of quasi-periodic signals such as speech. The\nperformances of single-tone sinusoid and speech generations are evaluated. The\nexperimental results show the effectiveness of the PDCNNs for unseen auxiliary\n$F_{0}$ features and the effectiveness of the cascaded structure for speech\ngeneration.", "published": "2020-07-11 02:23:08", "link": "http://arxiv.org/abs/2007.05663v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Fast Griffin Lim based Waveform Generation Strategy for Text-to-Speech\n  Synthesis", "abstract": "The performance of text-to-speech (TTS) systems heavily depends on\nspectrogram to waveform generation, also known as the speech reconstruction\nphase. The time required for the same is known as synthesis delay. In this\npaper, an approach to reduce speech synthesis delay has been proposed. It aims\nto enhance the TTS systems for real-time applications such as digital\nassistants, mobile phones, embedded devices, etc. The proposed approach applies\nFast Griffin Lim Algorithm (FGLA) instead Griffin Lim algorithm (GLA) as\nvocoder in the speech synthesis phase. GLA and FGLA are both iterative, but the\nconvergence rate of FGLA is faster than GLA. The proposed approach is tested on\nLJSpeech, Blizzard and Tatoeba datasets and the results for FGLA are compared\nagainst GLA and neural Generative Adversarial Network (GAN) based vocoder. The\nperformance is evaluated based on synthesis delay and speech quality. A 36.58%\nreduction in speech synthesis delay has been observed. The quality of the\noutput speech has improved, which is advocated by higher Mean opinion scores\n(MOS) and faster convergence with FGLA as opposed to GLA.", "published": "2020-07-11 13:10:09", "link": "http://arxiv.org/abs/2007.05764v1", "categories": ["eess.AS", "cs.MM"], "primary_category": "eess.AS"}
{"title": "Do We Need Sound for Sound Source Localization?", "abstract": "During the performance of sound source localization which uses both visual\nand aural information, it presently remains unclear how much either image or\nsound modalities contribute to the result, i.e. do we need both image and sound\nfor sound source localization? To address this question, we develop an\nunsupervised learning system that solves sound source localization by\ndecomposing this task into two steps: (i) \"potential sound source\nlocalization\", a step that localizes possible sound sources using only visual\ninformation (ii) \"object selection\", a step that identifies which objects are\nactually sounding using aural information. Our overall system achieves\nstate-of-the-art performance in sound source localization, and more\nimportantly, we find that despite the constraint on available information, the\nresults of (i) achieve similar performance. From this observation and further\nexperiments, we show that visual information is dominant in \"sound\" source\nlocalization when evaluated with the currently adopted benchmark dataset.\nMoreover, we show that the majority of sound-producing objects within the\nsamples in this dataset can be inherently identified using only visual\ninformation, and thus that the dataset is inadequate to evaluate a system's\ncapability to leverage aural information. As an alternative, we present an\nevaluation protocol that enforces both visual and aural information to be\nleveraged, and verify this property through several experiments.", "published": "2020-07-11 08:57:58", "link": "http://arxiv.org/abs/2007.05722v1", "categories": ["cs.CV", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
{"title": "Transformer-XL Based Music Generation with Multiple Sequences of\n  Time-valued Notes", "abstract": "Current state-of-the-art AI based classical music creation algorithms such as\nMusic Transformer are trained by employing single sequence of notes with\ntime-shifts. The major drawback of absolute time interval expression is the\ndifficulty of similarity computing of notes that share the same note value yet\ndifferent tempos, in one or among MIDI files. In addition, the usage of single\nsequence restricts the model to separately and effectively learn music\ninformation such as harmony and rhythm. In this paper, we propose a framework\nwith two novel methods to respectively track these two shortages, one is the\nconstruction of time-valued note sequences that liberate note values from\ntempos and the other is the separated usage of four sequences, namely, former\nnote on to current note on, note on to note off, pitch, and velocity, for\njointly training of four Transformer-XL networks. Through training on a 23-hour\npiano MIDI dataset, our framework generates significantly better and hour-level\nlonger music than three state-of-the-art baselines, namely Music Transformer,\nDeepJ, and single sequence-based Transformer-XL, evaluated automatically and\nmanually.", "published": "2020-07-11 20:16:24", "link": "http://arxiv.org/abs/2007.07244v1", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Look and Listen: A Multi-modality Late Fusion Approach to Scene\n  Classification for Autonomous Machines", "abstract": "The novelty of this study consists in a multi-modality approach to scene\nclassification, where image and audio complement each other in a process of\ndeep late fusion. The approach is demonstrated on a difficult classification\nproblem, consisting of two synchronised and balanced datasets of 16,000 data\nobjects, encompassing 4.4 hours of video of 8 environments with varying degrees\nof similarity. We first extract video frames and accompanying audio at one\nsecond intervals. The image and the audio datasets are first classified\nindependently, using a fine-tuned VGG16 and an evolutionary optimised deep\nneural network, with accuracies of 89.27% and 93.72%, respectively. This is\nfollowed by late fusion of the two neural networks to enable a higher order\nfunction, leading to accuracy of 96.81% in this multi-modality classifier with\nsynchronised video frames and audio clips. The tertiary neural network\nimplemented for late fusion outperforms classical state-of-the-art classifiers\nby around 3% when the two primary networks are considered as feature\ngenerators. We show that situations where a single-modality may be confused by\nanomalous data points are now corrected through an emerging higher order\nintegration. Prominent examples include a water feature in a city misclassified\nas a river by the audio classifier alone and a densely crowded street\nmisclassified as a forest by the image classifier alone. Both are examples\nwhich are correctly classified by our multi-modality approach.", "published": "2020-07-11 16:47:05", "link": "http://arxiv.org/abs/2007.10175v1", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
