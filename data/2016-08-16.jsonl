{"title": "Fast, Small and Exact: Infinite-order Language Modelling with Compressed\n  Suffix Trees", "abstract": "Efficient methods for storing and querying are critical for scaling\nhigh-order n-gram language models to large corpora. We propose a language model\nbased on compressed suffix trees, a representation that is highly compact and\ncan be easily held in memory, while supporting queries needed in computing\nlanguage model probabilities on-the-fly. We present several optimisations which\nimprove query runtimes up to 2500x, despite only incurring a modest increase in\nconstruction time and memory usage. For large corpora and high Markov orders,\nour method is highly competitive with the state-of-the-art KenLM package. It\nimposes much lower memory requirements, often by orders of magnitude, and has\nruntimes that are either similar (for training) or comparable (for querying).", "published": "2016-08-16 02:33:21", "link": "http://arxiv.org/abs/1608.04465v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Authorship clustering using multi-headed recurrent neural networks", "abstract": "A recurrent neural network that has been trained to separately model the\nlanguage of several documents by unknown authors is used to measure similarity\nbetween the documents. It is able to find clues of common authorship even when\nthe documents are very short and about disparate topics. While it is easy to\nmake statistically significant predictions regarding authorship, it is\ndifficult to group documents into definite clusters with high accuracy.", "published": "2016-08-16 05:04:46", "link": "http://arxiv.org/abs/1608.04485v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural versus Phrase-Based Machine Translation Quality: a Case Study", "abstract": "Within the field of Statistical Machine Translation (SMT), the neural\napproach (NMT) has recently emerged as the first technology able to challenge\nthe long-standing dominance of phrase-based approaches (PBMT). In particular,\nat the IWSLT 2015 evaluation campaign, NMT outperformed well established\nstate-of-the-art PBMT systems on English-German, a language pair known to be\nparticularly hard because of morphology and syntactic differences. To\nunderstand in what respects NMT provides better translation quality than PBMT,\nwe perform a detailed analysis of neural versus phrase-based SMT outputs,\nleveraging high quality post-edits performed by professional translators on the\nIWSLT data. For the first time, our analysis provides useful insights on what\nlinguistic phenomena are best modeled by neural models -- such as the\nreordering of verbs -- while pointing out other aspects that remain to be\nimproved.", "published": "2016-08-16 15:04:18", "link": "http://arxiv.org/abs/1608.04631v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Efficient Character-Level Neural Machine Translation", "abstract": "Neural machine translation aims at building a single large neural network\nthat can be trained to maximize translation performance. The encoder-decoder\narchitecture with an attention mechanism achieves a translation performance\ncomparable to the existing state-of-the-art phrase-based systems on the task of\nEnglish-to-French translation. However, the use of large vocabulary becomes the\nbottleneck in both training and improving the performance. In this paper, we\npropose an efficient architecture to train a deep character-level neural\nmachine translation by introducing a decimator and an interpolator. The\ndecimator is used to sample the source sequence before encoding while the\ninterpolator is used to resample after decoding. Such a deep model has two\nmajor advantages. It avoids the large vocabulary issue radically; at the same\ntime, it is much faster and more memory-efficient in training than conventional\ncharacter-based models. More interestingly, our model is able to translate the\nmisspelled word like human beings.", "published": "2016-08-16 07:44:02", "link": "http://arxiv.org/abs/1608.04738v2", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Learning Latent Local Conversation Modes for Predicting Community\n  Endorsement in Online Discussions", "abstract": "Many social media platforms offer a mechanism for readers to react to\ncomments, both positively and negatively, which in aggregate can be thought of\nas community endorsement. This paper addresses the problem of predicting\ncommunity endorsement in online discussions, leveraging both the participant\nresponse structure and the text of the comment. The different types of features\nare integrated in a neural network that uses a novel architecture to learn\nlatent modes of discussion structure that perform as well as deep neural\nnetworks but are more interpretable. In addition, the latent modes can be used\nto weight text features thereby improving prediction accuracy.", "published": "2016-08-16 23:37:43", "link": "http://arxiv.org/abs/1608.04808v2", "categories": ["cs.SI", "cs.CL"], "primary_category": "cs.SI"}
