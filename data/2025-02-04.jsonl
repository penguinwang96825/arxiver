{"title": "When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks", "abstract": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect.", "published": "2025-02-04 10:23:11", "link": "http://arxiv.org/abs/2502.02199v1", "categories": ["cs.CL", "cs.CE", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Liquidity provision of utility indifference type in decentralized exchanges", "abstract": "We present a mathematical formulation of liquidity provision in decentralized\nexchanges. We focus on constant function market makers of utility indifference\ntype, which include constant product market makers with concentrated liquidity\nas a special case. First, we examine no-arbitrage conditions for a liquidity\npool and compute an optimal arbitrage strategy when there is an external liquid\nmarket. Second, we show that liquidity provision suffers from impermanent loss\nunless a transaction fee is levied under the general framework with\nconcentrated liquidity. Third, we establish the well-definedness of\narbitrage-free reserve processes of a liquidity pool in continuous-time and\nshow that there is no loss-versus-rebalancing under a nonzero fee if the\nexternal market price is continuous. We then argue that liquidity provision by\nmultiple liquidity providers can be understood as liquidity provision by a\nrepresentative liquidity provider, meaning that the analysis boils down to that\nfor a single liquidity provider. Last, but not least, we give an answer to the\nfundamental question in which sense the very construction of constant function\nmarket makers with concentrated liquidity in the popular platform Uniswap v3 is\noptimal.", "published": "2025-02-04 02:06:28", "link": "http://arxiv.org/abs/2502.01931v1", "categories": ["q-fin.TR", "q-fin.MF", "91G15"], "primary_category": "q-fin.TR"}
{"title": "FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data", "abstract": "Large language models (LLMs) excel at generating human-like responses but\noften struggle with interactive tasks that require access to real-time\ninformation. This limitation poses challenges in finance, where models must\naccess up-to-date information, such as recent news or price movements, to\nsupport decision-making. To address this, we introduce Financial Agent, a\nknowledge-grounding approach for LLMs to handle financial queries using\nreal-time text and tabular data. Our contributions are threefold: First, we\ndevelop a Financial Context Dataset of over 50,000 financial queries paired\nwith the required context. Second, we train FinBloom 7B, a custom 7 billion\nparameter LLM, on 14 million financial news articles from Reuters and Deutsche\nPresse-Agentur, alongside 12 million Securities and Exchange Commission (SEC)\nfilings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to\nserve as a Financial Agent. This agent generates relevant financial context,\nenabling efficient real-time data retrieval to answer user queries. By reducing\nlatency and eliminating the need for users to manually provide accurate data,\nour approach significantly enhances the capability of LLMs to handle dynamic\nfinancial tasks. Our proposed approach makes real-time financial decisions,\nalgorithmic trading and other related tasks streamlined, and is valuable in\ncontexts with high-velocity data flows.", "published": "2025-02-04 06:51:34", "link": "http://arxiv.org/abs/2502.18471v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "cs.IR"}
{"title": "FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL Contest 2024", "abstract": "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.", "published": "2025-02-04 04:11:09", "link": "http://arxiv.org/abs/2502.01992v1", "categories": ["q-fin.TR", "cs.LG"], "primary_category": "q-fin.TR"}
{"title": "Conceptual Metaphor Theory as a Prompting Paradigm for Large Language\n  Models", "abstract": "We introduce Conceptual Metaphor Theory (CMT) as a framework for enhancing\nlarge language models (LLMs) through cognitive prompting in complex reasoning\ntasks. CMT leverages metaphorical mappings to structure abstract reasoning,\nimproving models' ability to process and explain intricate concepts. By\nincorporating CMT-based prompts, we guide LLMs toward more structured and\nhuman-like reasoning patterns. To evaluate this approach, we compare four\nnative models (Llama3.2, Phi3, Gemma2, and Mistral) against their CMT-augmented\ncounterparts on benchmark tasks spanning domain-specific reasoning, creative\ninsight, and metaphor interpretation. Responses were automatically evaluated\nusing the Llama3.3 70B model. Experimental results indicate that CMT prompting\nsignificantly enhances reasoning accuracy, clarity, and metaphorical coherence,\noutperforming baseline models across all evaluated tasks.", "published": "2025-02-04 00:26:39", "link": "http://arxiv.org/abs/2502.01901v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gradient-Regularized Latent Space Modulation in Large Language Models\n  for Structured Contextual Synthesis", "abstract": "Generating structured textual content requires mechanisms that enforce\ncoherence, stability, and adherence to predefined constraints while maintaining\nsemantic fidelity. Conventional approaches often rely on rule-based heuristics\nor fine-tuning strategies that lack flexibility and generalizability across\ndiverse tasks. The incorporation of Gradient-Regularized Latent Space\nModulation (GRLSM) introduces a novel paradigm for guiding text generation\nthrough the application of structured constraints within the latent space. The\nintegration of gradient-based regularization mitigates abrupt variations in\nlatent representations, ensuring a smoother encoding process that enhances\nstructural consistency and logical progression within generated sequences.\nComparative evaluations demonstrate that latent space modulation leads to a\nreduction in perplexity, increased coherence scores, and improved structural\nalignment across multiple domains. Stability assessments further indicate that\nthe imposition of spectral norm constraints facilitates more controlled\nvariations in generated text, preserving semantic consistency under input\nperturbations. Empirical results confirm that structured latent space\nconstraints not only refine the organization of generated outputs but also\nenhance interpretability through more predictable and reliable synthesis\npatterns. Performance metrics illustrate that the GRLSM framework substantially\nreduces structural inconsistencies while preserving the generative flexibility\ninherent in neural models.", "published": "2025-02-04 03:43:52", "link": "http://arxiv.org/abs/2502.01979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Wavelet-based Positional Representation for Long Context", "abstract": "In the realm of large-scale language models, a significant challenge arises\nwhen extrapolating sequences beyond the maximum allowable length. This is\nbecause the model's position embedding mechanisms are limited to positions\nencountered during training, thus preventing effective representation of\npositions in longer sequences. We analyzed conventional position encoding\nmethods for long contexts and found the following characteristics. (1) When the\nrepresentation dimension is regarded as the time axis, Rotary Position\nEmbedding (RoPE) can be interpreted as a restricted wavelet transform using\nHaar-like wavelets. However, because it uses only a fixed scale parameter, it\ndoes not fully exploit the advantages of wavelet transforms, which capture the\nfine movements of non-stationary signals using multiple scales (window sizes).\nThis limitation could explain why RoPE performs poorly in extrapolation. (2)\nPrevious research as well as our own analysis indicates that Attention with\nLinear Biases (ALiBi) functions similarly to windowed attention, using windows\nof varying sizes. However, it has limitations in capturing deep dependencies\nbecause it restricts the receptive field of the model. From these insights, we\npropose a new position representation method that captures multiple scales\n(i.e., window sizes) by leveraging wavelet transforms without limiting the\nmodel's attention field. Experimental results show that this new method\nimproves the performance of the model in both short and long contexts. In\nparticular, our method allows extrapolation of position information without\nlimiting the model's attention field.", "published": "2025-02-04 04:44:53", "link": "http://arxiv.org/abs/2502.02004v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Contextual Memory Reweaving in Large Language Models Using Layered\n  Latent State Reconstruction", "abstract": "Memory retention challenges in deep neural architectures have ongoing\nlimitations in the ability to process and recall extended contextual\ninformation. Token dependencies degrade as sequence length increases, leading\nto a decline in coherence and factual consistency across longer outputs. A\nstructured approach is introduced to mitigate this issue through the reweaving\nof latent states captured at different processing layers, reinforcing token\nrepresentations over extended sequences. The proposed Contextual Memory\nReweaving framework incorporates a Layered Latent State Reconstruction\nmechanism to systematically integrate past contextual embeddings without\nintroducing external memory modules. Experimental results demonstrate\nimprovements in recall accuracy across a range of sequence lengths, with\nnotable gains in the retention of rarely occurring tokens and numerical\nreasoning consistency. Further analysis of computational efficiency indicates\nthat the additional processing overhead remains within acceptable thresholds,\nenabling scalability across different model sizes. Evaluations in long-form\ntext generation and ambiguous query resolution highlight the capacity of memory\nreweaving to enhance continuity and reduce inconsistencies over extended\noutputs. Attention weight distributions reveal more structured allocation\npatterns, suggesting that reweaved latent states contribute to improved\ncontextual awareness. The findings establish a framework for refining memory\nretention mechanisms in language models, addressing long-standing challenges in\nhandling complex, multi-step reasoning tasks.", "published": "2025-02-04 06:25:20", "link": "http://arxiv.org/abs/2502.02046v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AmaSQuAD: A Benchmark for Amharic Extractive Question Answering", "abstract": "This research presents a novel framework for translating extractive\nquestion-answering datasets into low-resource languages, as demonstrated by the\ncreation of the AmaSQuAD dataset, a translation of SQuAD 2.0 into Amharic. The\nmethodology addresses challenges related to misalignment between translated\nquestions and answers, as well as the presence of multiple answer instances in\nthe translated context. For this purpose, we used cosine similarity utilizing\nembeddings from a fine-tuned BERT-based model for Amharic and Longest Common\nSubsequence (LCS). Additionally, we fine-tune the XLM-R model on the AmaSQuAD\nsynthetic dataset for Amharic Question-Answering. The results show an\nimprovement in baseline performance, with the fine-tuned model achieving an\nincrease in the F1 score from 36.55% to 44.41% and 50.01% to 57.5% on the\nAmaSQuAD development dataset. Moreover, the model demonstrates improvement on\nthe human-curated AmQA dataset, increasing the F1 score from 67.80% to 68.80%\nand the exact match score from 52.50% to 52.66%.The AmaSQuAD dataset is\npublicly available Datasets", "published": "2025-02-04 06:27:39", "link": "http://arxiv.org/abs/2502.02047v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rethinking stance detection: A theoretically-informed research agenda\n  for user-level inference using language models", "abstract": "Stance detection has emerged as a popular task in natural language processing\nresearch, enabled largely by the abundance of target-specific social media\ndata. While there has been considerable research on the development of stance\ndetection models, datasets, and application, we highlight important gaps\npertaining to (i) a lack of theoretical conceptualization of stance, and (ii)\nthe treatment of stance at an individual- or user-level, as opposed to\nmessage-level. In this paper, we first review the interdisciplinary origins of\nstance as an individual-level construct to highlight relevant attributes (e.g.,\npsychological features) that might be useful to incorporate in stance detection\nmodels. Further, we argue that recent pre-trained and large language models\n(LLMs) might offer a way to flexibly infer such user-level attributes and/or\nincorporate them in modelling stance. To better illustrate this, we briefly\nreview and synthesize the emerging corpus of studies on using LLMs for\ninferring stance, and specifically on incorporating user attributes in such\ntasks. We conclude by proposing a four-point agenda for pursuing stance\ndetection research that is theoretically informed, inclusive, and practically\nimpactful.", "published": "2025-02-04 07:52:20", "link": "http://arxiv.org/abs/2502.02074v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "LongDPO: Unlock Better Long-form Generation Abilities for LLMs via\n  Critique-augmented Stepwise Information", "abstract": "Long-form generation is crucial for academic writing papers and repo-level\ncode generation. Despite this, current models, including GPT-4o, still exhibit\nunsatisfactory performance. Existing methods that utilize preference learning\nwith outcome supervision often fail to provide detailed feedback for extended\ncontexts. This shortcoming can lead to content that does not fully satisfy\nquery requirements, resulting in issues like length deviations, and diminished\nquality. In this paper, we propose enhancing long-form generation by\nincorporating process supervision. We employ Monte Carlo Tree Search to gather\nstepwise preference pairs, utilizing a global memory pool to maintain\nconsistency. To address the issue of suboptimal candidate selection, we\nintegrate external critiques to refine and improve the quality of the\npreference pairs. Finally, we apply step-level DPO using the collected stepwise\npreference pairs. Experimental results show that our method improves length and\nquality on long-form generation benchmarks, with almost lossless performance on\ngeneral benchmarks across various model backbones.", "published": "2025-02-04 08:25:17", "link": "http://arxiv.org/abs/2502.02095v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evalita-LLM: Benchmarking Large Language Models on Italian", "abstract": "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language\nModels (LLMs) on Italian tasks. The distinguishing and innovative features of\nEvalita-LLM are the following: (i) all tasks are native Italian, avoiding\nissues of translating from Italian and potential cultural biases; (ii) in\naddition to well established multiple-choice tasks, the benchmark includes\ngenerative tasks, enabling more natural interaction with LLMs; (iii) all tasks\nare evaluated against multiple prompts, this way mitigating the model\nsensitivity to specific prompts and allowing a fairer and objective evaluation.\nWe propose an iterative methodology, where candidate tasks and candidate\nprompts are validated against a set of LLMs used for development. We report\nexperimental results from the benchmark's development phase, and provide\nperformance statistics for several state-of-the-art LLMs.", "published": "2025-02-04 12:58:19", "link": "http://arxiv.org/abs/2502.02289v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking", "abstract": "Multimodal large language models (MLLMs) exhibit impressive capabilities but\nstill face challenges in complex visual reasoning. While recent efforts attempt\nto enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking\nthrough explicit search structures or teacher-guided distillation, they often\nstruggle to balance performance and efficiency. A critical limitation is their\nheavy reliance on extensive data and search spaces, resulting in low-efficiency\nimplicit insight extraction and data utilization. To address this, we propose\nAStar, an Automated Structured thinking paradigm for multimodal reasoning via\nMonte Carlo Tree Search (MCTS). AStar automatically derives high-level\ncognitive reasoning patterns from limited data using MCTS-powered hierarchical\nstructures. Building on these explicit patterns, we design a unified reasoning\nframework that seamlessly integrates models' internal reasoning capabilities\nand external reasoning guidelines, enabling efficient inference with minimal\ntree iterations. This novel paradigm strikes a compelling balance between\nperformance and efficiency. Extensive experiments demonstrate AStar's\neffectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse\nbenchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining\nsubstantial data and computational efficiency.", "published": "2025-02-04 14:18:29", "link": "http://arxiv.org/abs/2502.02339v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Premise-Augmented Reasoning Chains Improve Error Identification in Math\n  reasoning with LLMs", "abstract": "Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large\nlanguage models (LLMs) by enabling detailed step-by-step solutions. However,\ndue to the verbosity of LLMs, the resulting reasoning chains can be long,\nmaking it harder to verify the reasoning steps and trace issues resulting from\ndependencies between the steps that may be farther away in the sequence of\nsteps. Importantly, mathematical reasoning allows each step to be derived from\na small set of premises, which are a subset of the preceding steps in the\nreasoning chain. In this paper, we present a framework that identifies the\npremises for each step, to improve the evaluation of reasoning. We restructure\nconventional linear reasoning chains into Premise Augmented Reasoning Chains\n(PARC) by introducing premise links, resulting in a directed acyclic graph\nwhere the nodes are the steps and the edges are the premise links. Through\nexperiments with a PARC-based dataset that we built, namely PERL (Premises and\nERrors identification in LLMs), we demonstrate that LLMs can reliably identify\npremises within complex reasoning chains. In particular, even open-source LLMs\nachieve 90% recall in premise identification. We also show that PARC helps to\nidentify errors in reasoning chains more reliably. The accuracy of error\nidentification improves by 6% to 16% absolute when step-by-step verification is\ncarried out in PARC under the premises. Our findings highlight the utility of\npremise-centric representations in addressing complex problem-solving tasks and\nopen new avenues for improving the reliability of LLM-based reasoning\nevaluations.", "published": "2025-02-04 14:44:58", "link": "http://arxiv.org/abs/2502.02362v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAIR: Improving Safety Alignment with Introspective Reasoning", "abstract": "Ensuring the safety and harmlessness of Large Language Models (LLMs) has\nbecome equally critical as their performance in applications. However, existing\nsafety alignment methods typically suffer from safety-performance trade-offs\nand the susceptibility to jailbreak attacks, primarily due to their reliance on\ndirect refusals for malicious queries. In this paper, we propose STAIR, a novel\nframework that integrates SafeTy Alignment with Itrospective Reasoning. We\nenable LLMs to identify safety risks through step-by-step analysis by\nself-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR\nfirst equips the model with a structured reasoning capability and then advances\nsafety alignment via iterative preference optimization on step-level reasoning\ndata generated using our newly proposed Safety-Informed Monte Carlo Tree Search\n(SI-MCTS). We further train a process reward model on this data to guide\ntest-time searches for improved responses. Extensive experiments show that\nSTAIR effectively mitigates harmful outputs while better preserving\nhelpfulness, compared to instinctive alignment strategies. With test-time\nscaling, STAIR achieves a safety performance comparable to Claude-3.5 against\npopular jailbreak attacks. Relevant resources in this work are available at\nhttps://github.com/thu-ml/STAIR.", "published": "2025-02-04 15:02:55", "link": "http://arxiv.org/abs/2502.02384v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Multilingual Machine Translation with Open Large Language Models at\n  Practical Scale: An Empirical Study", "abstract": "Large language models (LLMs) have shown continuously improving multilingual\ncapabilities, and even small-scale open-source models have demonstrated rapid\nperformance enhancement. In this paper, we systematically explore the abilities\nof open LLMs with less than ten billion parameters to handle multilingual\nmachine translation (MT) tasks. We conduct comprehensive evaluations on six\npopular LLMs and find that models like Gemma2-9B exhibit impressive\nmultilingual translation capabilities. We then introduce the Parallel-First\nMonolingual-Second (PFMS) data mixing strategy in the continual pretraining\nstage to further enhance the MT performance and present GemmaX2-28, a 9B model\nachieving top-tier multilingual translation performance across 28 languages.\nSpecifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA)\nmodels such as TowerInstruct and XALMA and achieves competitive performance\nwith Google Translate and GPT-4-turbo.", "published": "2025-02-04 16:57:03", "link": "http://arxiv.org/abs/2502.02481v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Adaptive Self-improvement LLM Agentic System for ML Library Development", "abstract": "ML libraries, often written in architecture-specific programming languages\n(ASPLs) that target domain-specific architectures, are key to efficient ML\nsystems. However, writing these high-performance ML libraries is challenging\nbecause it requires expert knowledge of ML algorithms and the ASPL. Large\nlanguage models (LLMs), on the other hand, have shown general coding\ncapabilities. However, challenges remain when using LLMs for generating ML\nlibraries using ASPLs because 1) this task is complicated even for experienced\nhuman programmers and 2) there are limited code examples because of the\nesoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning\nwith limited data in order to complete this task. To address these challenges,\nwe introduce an adaptive self-improvement agentic system. In order to evaluate\nthe effectiveness of our system, we construct a benchmark of a typical ML\nlibrary and generate ASPL code with both open and closed-source LLMs on this\nbenchmark. Our results show improvements of up to $3.9\\times$ over a baseline\nsingle LLM.", "published": "2025-02-04 17:57:17", "link": "http://arxiv.org/abs/2502.02534v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A comparison of translation performance between DeepL and Supertext", "abstract": "As strong machine translation (MT) systems are increasingly based on large\nlanguage models (LLMs), reliable quality benchmarking requires methods that\ncapture their ability to leverage extended context. This study compares two\ncommercial MT systems -- DeepL and Supertext -- by assessing their performance\non unsegmented texts. We evaluate translation quality across four language\ndirections with professional translators assessing segments with full\ndocument-level context. While segment-level assessments indicate no strong\npreference between the systems in most cases, document-level analysis reveals a\npreference for Supertext in three out of four language directions, suggesting\nsuperior consistency across longer texts. We advocate for more\ncontext-sensitive evaluation methodologies to ensure that MT quality\nassessments reflect real-world usability. We release all evaluation data and\nscripts for further analysis and reproduction at\nhttps://github.com/supertext/evaluation_deepl_supertext.", "published": "2025-02-04 18:53:42", "link": "http://arxiv.org/abs/2502.02577v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spatio-temporal transformer to support automatic sign language\n  translation", "abstract": "Sign Language Translation (SLT) systems support hearing-impaired people\ncommunication by finding equivalences between signed and spoken languages. This\ntask is however challenging due to multiple sign variations, complexity in\nlanguage and inherent richness of expressions. Computational approaches have\nevidenced capabilities to support SLT. Nonetheless, these approaches remain\nlimited to cover gestures variability and support long sequence translations.\nThis paper introduces a Transformer-based architecture that encodes\nspatio-temporal motion gestures, preserving both local and long-range spatial\ninformation through the use of multiple convolutional and attention mechanisms.\nThe proposed approach was validated on the Colombian Sign Language Translation\nDataset (CoL-SLTD) outperforming baseline approaches, and achieving a BLEU4 of\n46.84%. Additionally, the proposed approach was validated on the\nRWTH-PHOENIX-Weather-2014T (PHOENIX14T), achieving a BLEU4 score of 30.77%,\ndemonstrating its robustness and effectiveness in handling real-world\nvariations", "published": "2025-02-04 18:59:19", "link": "http://arxiv.org/abs/2502.02587v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "How Inclusively do LMs Perceive Social and Moral Norms?", "abstract": "This paper discusses and contains offensive content. Language models (LMs)\nare used in decision-making systems and as interactive assistants. However, how\nwell do these models making judgements align with the diversity of human\nvalues, particularly regarding social and moral norms? In this work, we\ninvestigate how inclusively LMs perceive norms across demographic groups (e.g.,\ngender, age, and income). We prompt 11 LMs on rules-of-thumb (RoTs) and compare\ntheir outputs with the existing responses of 100 human annotators. We introduce\nthe Absolute Distance Alignment Metric (ADA-Met) to quantify alignment on\nordinal questions. We find notable disparities in LM responses, with younger,\nhigher-income groups showing closer alignment, raising concerns about the\nrepresentation of marginalized perspectives. Our findings highlight the\nimportance of further efforts to make LMs more inclusive of diverse human\nvalues. The code and prompts are available on GitHub under the CC BY-NC 4.0\nlicense.", "published": "2025-02-04 20:24:17", "link": "http://arxiv.org/abs/2502.02696v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-Lingual Transfer for Low-Resource Natural Language Processing", "abstract": "Natural Language Processing (NLP) has seen remarkable advances in recent\nyears, particularly with the emergence of Large Language Models that have\nachieved unprecedented performance across many tasks. However, these\ndevelopments have mainly benefited a small number of high-resource languages\nsuch as English. The majority of languages still face significant challenges\ndue to the scarcity of training data and computational resources. To address\nthis issue, this thesis focuses on cross-lingual transfer learning, a research\narea aimed at leveraging data and models from high-resource languages to\nimprove NLP performance for low-resource languages. Specifically, we focus on\nSequence Labeling tasks such as Named Entity Recognition, Opinion Target\nExtraction, and Argument Mining.\n  The research is structured around three main objectives: (1) advancing\ndata-based cross-lingual transfer learning methods through improved translation\nand annotation projection techniques, (2) developing enhanced model-based\ntransfer learning approaches utilizing state-of-the-art multilingual models,\nand (3) applying these methods to real-world problems while creating\nopen-source resources that facilitate future research in low-resource NLP.\n  More specifically, this thesis presents a new method to improve data-based\ntransfer with T-Projection, a state-of-the-art annotation projection method\nthat leverages text-to-text multilingual models and machine translation\nsystems. T-Projection significantly outperforms previous annotation projection\nmethods by a wide margin. For model-based transfer, we introduce a constrained\ndecoding algorithm that enhances cross-lingual Sequence Labeling in zero-shot\nsettings using text-to-text models. Finally, we develop Medical mT5, the first\nmultilingual text-to-text medical model, demonstrating the practical impact of\nour research on real-world applications.", "published": "2025-02-04 21:17:46", "link": "http://arxiv.org/abs/2502.02722v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language\n  Model", "abstract": "While large language models have facilitated breakthroughs in many\napplications of artificial intelligence, their inherent largeness makes them\ncomputationally expensive and challenging to deploy in resource-constrained\nsettings. In this paper, we document the development of SmolLM2, a\nstate-of-the-art \"small\" (1.7 billion parameter) language model (LM). To attain\nstrong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a\nmulti-stage training process that mixes web text with specialized math, code,\nand instruction-following data. We additionally introduce new specialized\ndatasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing\ndatasets to be problematically small or low-quality. To inform our design\ndecisions, we perform both small-scale ablations as well as a manual refinement\nprocess that updates the dataset mixing rates at each stage based on the\nperformance at the previous stage. Ultimately, we demonstrate that SmolLM2\noutperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To\nfacilitate future research on LM development as well as applications of small\nLMs, we release both SmolLM2 as well as all of the datasets we prepared in the\ncourse of this project.", "published": "2025-02-04 21:43:16", "link": "http://arxiv.org/abs/2502.02737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TRUTH DECAY: Quantifying Multi-Turn Sycophancy in Language Models", "abstract": "Rapid improvements in large language models have unveiled a critical\nchallenge in human-AI interaction: sycophancy. In this context, sycophancy\nrefers to the tendency of models to excessively agree with or flatter users,\noften at the expense of factual accuracy. While previous studies have primarily\nanalyzed this behavior in single-turn interactions, its persistence and\nevolution in multi-step conversations remain largely unexplored. We introduce\nTRUTH DECAY, a benchmark specifically designed to evaluate sycophancy in\nextended dialogues, where language models must navigate iterative user\nfeedback, challenges, and persuasion. We prompt models to elicit four types of\nsycophantic biases. We then propose and test sycophancy reduction strategies,\nevaluating their effectiveness beyond single-step interactions.", "published": "2025-02-04 06:56:32", "link": "http://arxiv.org/abs/2503.11656v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automating Mathematical Proof Generation Using Large Language Model\n  Agents and Knowledge Graphs", "abstract": "Large Language Models have demonstrated remarkable capabilities in natural\nlanguage processing tasks, including mathematical problem-solving that requires\nmulti-step logical reasoning. However, challenges persist in automating the\nidentification of key mathematical concepts, understanding their\ninterrelations, and formalizing proofs within a rigorous framework. We present\na novel framework that leverages knowledge graphs to augment LLMs to construct\nand formalize mathematical proofs. Our results demonstrate significant\nperformance improvements across multiple datasets, with using knowledge graphs,\nachieving up to a 34% success rate on the MUSTARDSAUCE dataset on o1-mini and\nconsistently outperforming baseline approaches by 2-11% across different\nmodels. We show how this approach bridges the gap between natural language\nunderstanding and formal logic proof systems and achieve elevated results for\nfoundation models over baseline.", "published": "2025-02-04 07:17:34", "link": "http://arxiv.org/abs/2503.11657v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fairness through Difference Awareness: Measuring Desired Group\n  Discrimination in LLMs", "abstract": "Algorithmic fairness has conventionally adopted a perspective of racial\ncolor-blindness (i.e., difference unaware treatment). We contend that in a\nrange of important settings, group difference awareness matters. For example,\ndifferentiating between groups may be necessary in legal contexts (e.g., the\nU.S. compulsory draft applies to men but not women) and harm assessments (e.g.,\ncalling a girl a terrorist may be less harmful than calling a Muslim person\none). In our work we first introduce an important distinction between\ndescriptive (fact-based), normative (value-based), and correlation\n(association-based) benchmarks. This distinction is significant because each\ncategory requires distinct interpretation and mitigation tailored to its\nspecific characteristics. Then, we present a benchmark suite composed of eight\ndifferent scenarios for a total of 16k questions that enables us to assess\ndifference awareness. Finally, we show results across ten models that\ndemonstrate difference awareness is a distinct dimension of fairness where\nexisting bias mitigation strategies may backfire.", "published": "2025-02-04 01:56:28", "link": "http://arxiv.org/abs/2502.01926v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?", "abstract": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.", "published": "2025-02-04 02:23:06", "link": "http://arxiv.org/abs/2502.01941v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Boundary-Driven Table-Filling with Cross-Granularity Contrastive\n  Learning for Aspect Sentiment Triplet Extraction", "abstract": "The Aspect Sentiment Triplet Extraction (ASTE) task aims to extract aspect\nterms, opinion terms, and their corresponding sentiment polarity from a given\nsentence. It remains one of the most prominent subtasks in fine-grained\nsentiment analysis. Most existing approaches frame triplet extraction as a 2D\ntable-filling process in an end-to-end manner, focusing primarily on word-level\ninteractions while often overlooking sentence-level representations. This\nlimitation hampers the model's ability to capture global contextual\ninformation, particularly when dealing with multi-word aspect and opinion terms\nin complex sentences. To address these issues, we propose boundary-driven\ntable-filling with cross-granularity contrastive learning (BTF-CCL) to enhance\nthe semantic consistency between sentence-level representations and word-level\nrepresentations. By constructing positive and negative sample pairs, the model\nis forced to learn the associations at both the sentence level and the word\nlevel. Additionally, a multi-scale, multi-granularity convolutional method is\nproposed to capture rich semantic information better. Our approach can capture\nsentence-level contextual information more effectively while maintaining\nsensitivity to local details. Experimental results show that the proposed\nmethod achieves state-of-the-art performance on public benchmarks according to\nthe F1 score.", "published": "2025-02-04 02:23:45", "link": "http://arxiv.org/abs/2502.01942v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Token Cleaning: Fine-Grained Data Selection for LLM Supervised\n  Fine-Tuning", "abstract": "Recent studies show that in supervised fine-tuning (SFT) of large language\nmodels (LLMs), data quality matters more than quantity. While most data\ncleaning methods concentrate on filtering entire samples, the quality of\nindividual tokens within a sample can vary significantly. After pre-training,\neven in high-quality samples, patterns or phrases that are not task-related can\nbe redundant or uninformative. Continuing to fine-tune on these patterns may\noffer limited benefit and even degrade downstream task performance. In this\npaper, we investigate token quality from a noisy-label perspective and propose\na generic token cleaning pipeline for SFT tasks. Our method filters out\nuninformative tokens while preserving those carrying key task-specific\ninformation. Specifically, we first evaluate token quality by examining the\ninfluence of model updates on each token, then apply a threshold-based\nseparation. The token influence can be measured in a single pass with a fixed\nreference model or iteratively with self-evolving reference models. The\nbenefits and limitations of both methods are analyzed theoretically by error\nupper bounds. Extensive experiments show that our framework consistently\nimproves performance across multiple downstream tasks.", "published": "2025-02-04 03:26:58", "link": "http://arxiv.org/abs/2502.01968v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reasoning Bias of Next Token Prediction Training", "abstract": "Since the inception of Large Language Models (LLMs), the quest to efficiently\ntrain them for superior reasoning capabilities has been a pivotal challenge.\nThe dominant training paradigm for LLMs is based on next token prediction\n(NTP). Alternative methodologies, called Critical Token Prediction (CTP),\nfocused exclusively on specific critical tokens (such as the answer in Q\\&A\ndataset), aiming to reduce the overfitting of extraneous information and noise.\nContrary to initial assumptions, our research reveals that despite NTP's\nexposure to noise during training, it surpasses CTP in reasoning ability. We\nattribute this counterintuitive outcome to the regularizing influence of noise\non the training dynamics. Our empirical analysis shows that NTP-trained models\nexhibit enhanced generalization and robustness across various benchmark\nreasoning datasets, demonstrating greater resilience to perturbations and\nachieving flatter loss minima. These findings illuminate that NTP is\ninstrumental in fostering reasoning abilities during pretraining, whereas CTP\nis more effective for finetuning, thereby enriching our comprehension of\noptimal training strategies in LLM development.", "published": "2025-02-04 04:46:41", "link": "http://arxiv.org/abs/2502.02007v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fine-tuning Language Models for Recipe Generation: A Comparative\n  Analysis and Benchmark Study", "abstract": "This research presents an exploration and study of the recipe generation task\nby fine-tuning various very small language models, with a focus on developing\nrobust evaluation metrics and comparing across different language models the\nopen-ended task of recipe generation. This study presents extensive experiments\nwith multiple model architectures, ranging from T5-small (Raffel et al., 2023)\nand SmolLM-135M(Allal et al., 2024) to Phi-2 (Research, 2023), implementing\nboth traditional NLP metrics and custom domain-specific evaluation metrics. Our\nnovel evaluation framework incorporates recipe-specific metrics for assessing\ncontent quality and introduces approaches to allergen substitution. The results\nindicate that, while larger models generally perform better on standard\nmetrics, the relationship between model size and recipe quality is more nuanced\nwhen considering domain-specific metrics. SmolLM-360M and SmolLM-1.7B\ndemonstrate comparable performance despite their size difference before and\nafter fine-tuning, while fine-tuning Phi-2 shows notable limitations in recipe\ngeneration despite its larger parameter count. The comprehensive evaluation\nframework and allergen substitution systems provide valuable insights for\nfuture work in recipe generation and broader NLG tasks that require domain\nexpertise and safety considerations.", "published": "2025-02-04 05:25:01", "link": "http://arxiv.org/abs/2502.02028v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Topic Modeling in Marathi", "abstract": "While topic modeling in English has become a prevalent and well-explored\narea, venturing into topic modeling for Indic languages remains relatively\nrare. The limited availability of resources, diverse linguistic structures, and\nunique challenges posed by Indic languages contribute to the scarcity of\nresearch and applications in this domain. Despite the growing interest in\nnatural language processing and machine learning, there exists a noticeable gap\nin the comprehensive exploration of topic modeling methodologies tailored\nspecifically for languages such as Hindi, Marathi, Tamil, and others. In this\npaper, we examine several topic modeling approaches applied to the Marathi\nlanguage. Specifically, we compare various BERT and non-BERT approaches,\nincluding multilingual and monolingual BERT models, using topic coherence and\ntopic diversity as evaluation metrics. Our analysis provides insights into the\nperformance of these approaches for Marathi language topic modeling. The key\nfinding of the paper is that BERTopic, when combined with BERT models trained\non Indic languages, outperforms LDA in terms of topic modeling performance.", "published": "2025-02-04 08:32:08", "link": "http://arxiv.org/abs/2502.02100v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Attribute Extraction from News Web Pages", "abstract": "This paper addresses the challenge of automatically extracting attributes\nfrom news article web pages across multiple languages. Recent neural network\nmodels have shown high efficacy in extracting information from semi-structured\nweb pages. However, these models are predominantly applied to domains like\ne-commerce and are pre-trained using English data, complicating their\napplication to web pages in other languages. We prepared a multilingual dataset\ncomprising 3,172 marked-up news web pages across six languages (English,\nGerman, Russian, Chinese, Korean, and Arabic) from 161 websites. The dataset is\npublicly available on GitHub. We fine-tuned the pre-trained state-of-the-art\nmodel, MarkupLM, to extract news attributes from these pages and evaluated the\nimpact of translating pages into English on extraction quality. Additionally,\nwe pre-trained another state-of-the-art model, DOM-LM, on multilingual data and\nfine-tuned it on our dataset. We compared both fine-tuned models to existing\nopen-source news data extraction tools, achieving superior extraction metrics.", "published": "2025-02-04 09:43:40", "link": "http://arxiv.org/abs/2502.02167v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Mass-Editing Memory with Attention in Transformers: A cross-lingual\n  exploration of knowledge", "abstract": "Recent research has explored methods for updating and modifying factual\nknowledge in large language models, often focusing on specific multi-layer\nperceptron blocks. This study expands on this work by examining the\neffectiveness of existing knowledge editing methods across languages and\ndelving into the role of attention mechanisms in this process. Drawing from the\ninsights gained, we propose Mass-Editing Memory with Attention in Transformers\n(MEMAT), a method that achieves significant improvements in all metrics while\nrequiring minimal parameter modifications. MEMAT delivers a remarkable 10%\nincrease in magnitude metrics, benefits languages not included in the training\ndata and also demonstrates a high degree of portability. Our code and data are\nat https://github.com/dtamayo-nlp/MEMAT.", "published": "2025-02-04 09:47:55", "link": "http://arxiv.org/abs/2502.02173v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Conversation AI Dialog for Medicare powered by Finetuning and Retrieval\n  Augmented Generation", "abstract": "Large language models (LLMs) have shown impressive capabilities in natural\nlanguage processing tasks, including dialogue generation. This research aims to\nconduct a novel comparative analysis of two prominent techniques, fine-tuning\nwith LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG)\nframework, in the context of doctor-patient chat conversations with multiple\ndatasets of mixed medical domains. The analysis involves three state-of-the-art\nmodels: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient\ndialogues, we comprehensively evaluate the performance of models, assessing key\nmetrics such as language quality (perplexity, BLEU score), factual accuracy\n(fact-checking against medical knowledge bases), adherence to medical\nguidelines, and overall human judgments (coherence, empathy, safety). The\nfindings provide insights into the strengths and limitations of each approach,\nshedding light on their suitability for healthcare applications. Furthermore,\nthe research investigates the robustness of the models in handling diverse\npatient queries, ranging from general health inquiries to specific medical\nconditions. The impact of domain-specific knowledge integration is also\nexplored, highlighting the potential for enhancing LLM performance through\ntargeted data augmentation and retrieval strategies.", "published": "2025-02-04 11:50:40", "link": "http://arxiv.org/abs/2502.02249v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VaiBot: Shuttle Between the Instructions and Parameters of Large\n  Language Models", "abstract": "How to interact with LLMs through \\emph{instructions} has been widely studied\nby researchers. However, previous studies have treated the emergence of\ninstructions and the training of LLMs on task data as separate processes,\noverlooking the inherent unity between the two. This paper proposes a neural\nnetwork framework, VaiBot, that integrates VAE and VIB, designed to uniformly\nmodel, learn, and infer both deduction and induction tasks under LLMs. Through\nexperiments, we demonstrate that VaiBot performs on par with existing baseline\nmethods in terms of deductive capabilities while significantly surpassing them\nin inductive capabilities. We also find that VaiBot can scale up using general\ninstruction-following data and exhibits excellent one-shot induction abilities.\nWe finally synergistically integrate the deductive and inductive processes of\nVaiBot. Through T-SNE dimensionality reduction, we observe that its\ninductive-deductive process significantly improves the distribution of training\nparameters, enabling it to outperform baseline methods in inductive reasoning\ntasks. The code and data for this paper can be found at\nhttps://anonymous.4open.science/r/VaiBot-021F.", "published": "2025-02-04 13:36:54", "link": "http://arxiv.org/abs/2502.02315v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ReSpark: Leveraging Previous Data Reports as References to Generate New\n  Reports with LLMs", "abstract": "Creating data reports is time-consuming, as it requires iterative exploration\nand understanding of data, followed by summarizing the insights. While large\nlanguage models (LLMs) are powerful tools for data processing and text\ngeneration, they often struggle to produce complete data reports that fully\nmeet user expectations. One significant challenge is effectively communicating\nthe entire analysis logic to LLMs. Moreover, determining a comprehensive\nanalysis logic can be mentally taxing for users. To address these challenges,\nwe propose ReSpark, an LLM-based method that leverages existing data reports as\nreferences for creating new ones. Given a data table, ReSpark searches for\nsimilar-topic reports, parses them into interdependent segments corresponding\nto analytical objectives, and executes them with new data. It identifies\ninconsistencies and customizes the objectives, data transformations, and\ntextual descriptions. ReSpark allows users to review real-time outputs, insert\nnew objectives, and modify report content. Its effectiveness was evaluated\nthrough comparative and user studies.", "published": "2025-02-04 14:00:32", "link": "http://arxiv.org/abs/2502.02329v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning", "abstract": "Research on LLM technologies is rapidly emerging, with most of them employing\na 'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. To validate the\neffectiveness of our framework, we conducted extensive experiments across a\nrange of generative and reasoning tasks. These experiments demonstrated that\nour framework outperforms conventional inference processes on accuracy,\ncoherence, and diversity. The framework's ability to iteratively expand its\nsearch space while retaining contextually relevant information results.", "published": "2025-02-04 15:10:33", "link": "http://arxiv.org/abs/2502.02390v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named\n  Entity Recognition in a Multilingual Framework", "abstract": "We introduce FewTopNER, a novel framework that integrates few-shot named\nentity recognition (NER) with topic-aware contextual modeling to address the\nchallenges of cross-lingual and low-resource scenarios. FewTopNER leverages a\nshared multilingual encoder based on XLM-RoBERTa, augmented with\nlanguage-specific calibration mechanisms, to generate robust contextual\nembeddings. The architecture comprises a prototype-based entity recognition\nbranch, employing BiLSTM and Conditional Random Fields for sequence labeling,\nand a topic modeling branch that extracts document-level semantic features\nthrough hybrid probabilistic and neural methods. A cross-task bridge\nfacilitates dynamic bidirectional attention and feature fusion between entity\nand topic representations, thereby enhancing entity disambiguation by\nincorporating global semantic context. Empirical evaluations on multilingual\nbenchmarks across English, French, Spanish, German, and Italian demonstrate\nthat FewTopNER significantly outperforms existing state-of-the-art few-shot NER\nmodels. In particular, the framework achieves improvements of 2.5-4.0\npercentage points in F1 score and exhibits enhanced topic coherence, as\nmeasured by normalized pointwise mutual information. Ablation studies further\nconfirm the critical contributions of the shared encoder and cross-task\nintegration mechanisms to the overall performance. These results underscore the\nefficacy of incorporating topic-aware context into few-shot NER and highlight\nthe potential of FewTopNER for robust cross-lingual applications in\nlow-resource settings.", "published": "2025-02-04 15:13:40", "link": "http://arxiv.org/abs/2502.02391v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Activation-Informed Merging of Large Language Models", "abstract": "Model merging, a method that combines the parameters and embeddings of\nmultiple fine-tuned large language models (LLMs), offers a promising approach\nto enhance model performance across various tasks while maintaining\ncomputational efficiency. This paper introduces Activation-Informed Merging\n(AIM), a technique that integrates the information from the activation space of\nLLMs into the merging process to improve performance and robustness. AIM is\ndesigned as a flexible, complementary solution that is applicable to any\nexisting merging method. It aims to preserve critical weights from the base\nmodel, drawing on principles from continual learning~(CL) and model\ncompression. Utilizing a task-agnostic calibration set, AIM selectively\nprioritizes essential weights during merging. We empirically demonstrate that\nAIM significantly enhances the performance of merged models across multiple\nbenchmarks. Our findings suggest that considering the activation-space\ninformation can provide substantial advancements in the model merging\nstrategies for LLMs with up to 40\\% increase in benchmark performance.", "published": "2025-02-04 15:42:03", "link": "http://arxiv.org/abs/2502.02421v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Generative Psycho-Lexical Approach for Constructing Value Systems in\n  Large Language Models", "abstract": "Values are core drivers of individual and collective perception, cognition,\nand behavior. Value systems, such as Schwartz's Theory of Basic Human Values,\ndelineate the hierarchy and interplay among these values, enabling\ncross-disciplinary investigations into decision-making and societal dynamics.\nRecently, the rise of Large Language Models (LLMs) has raised concerns\nregarding their elusive intrinsic values. Despite growing efforts in\nevaluating, understanding, and aligning LLM values, a psychologically grounded\nLLM value system remains underexplored. This study addresses the gap by\nintroducing the Generative Psycho-Lexical Approach (GPLA), a scalable,\nadaptable, and theoretically informed method for constructing value systems.\nLeveraging GPLA, we propose a psychologically grounded five-factor value system\ntailored for LLMs. For systematic validation, we present three benchmarking\ntasks that integrate psychological principles with cutting-edge AI priorities.\nOur results reveal that the proposed value system meets standard psychological\ncriteria, better captures LLM values, improves LLM safety prediction, and\nenhances LLM alignment, when compared to the canonical Schwartz's values.", "published": "2025-02-04 16:10:55", "link": "http://arxiv.org/abs/2502.02444v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in\n  Non-English Discourse with a Chinese Case Study", "abstract": "This study explores computational approaches for measuring moral foundations\n(MFs) in non-English corpora. Since most resources are developed primarily for\nEnglish, cross-linguistic applications of moral foundation theory remain\nlimited. Using Chinese as a case study, this paper evaluates the effectiveness\nof applying English resources to machine translated text, local language\nlexicons, multilingual language models, and large language models (LLMs) in\nmeasuring MFs in non-English texts. The results indicate that machine\ntranslation and local lexicon approaches are insufficient for complex moral\nassessments, frequently resulting in a substantial loss of cultural\ninformation. In contrast, multilingual models and LLMs demonstrate reliable\ncross-language performance with transfer learning, with LLMs excelling in terms\nof data efficiency. Importantly, this study also underscores the need for\nhuman-in-the-loop validation of automated MF assessment, as the most advanced\nmodels may overlook cultural nuances in cross-language measurements. The\nfindings highlight the potential of LLMs for cross-language MF measurements and\nother complex multilingual deductive coding tasks.", "published": "2025-02-04 16:17:01", "link": "http://arxiv.org/abs/2502.02451v2", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "SAISA: Towards Multimodal Large Language Models with Both Training and\n  Inference Efficiency", "abstract": "Multimodal Large Language Models (MLLMs) mainly fall into two architectures,\neach involving a trade-off between training and inference efficiency: embedding\nspace alignment (e.g., LLaVA-1.5) is inefficient during inference, while\ncross-attention space alignment (e.g., Flamingo) is inefficient in training. In\nthis paper, we compare these two architectures and identify the key factors for\nbuilding efficient MLLMs. A primary difference between them lies in how\nattention is applied to visual tokens, particularly in their interactions with\neach other. To investigate whether attention among visual tokens is necessary,\nwe propose a new self-attention mechanism, NAAViT (\\textbf{N}o\n\\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which\neliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that\nattention among visual tokens is highly redundant. Based on these insights, we\nintroduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace\n\\textbf{A}lignment), a novel architecture that enhance both training and\ninference efficiency. SAISA directly aligns visual features with the input\nspaces of NAAViT self-attention blocks, reducing computational overhead in both\nself-attention blocks and feed-forward networks (FFNs). Using the same\nconfiguration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training\nbudget by 26\\%, while achieving superior performance in terms of accuracy.\nComprehensive ablation studies further validate the effectiveness of SAISA\nacross various LLMs and visual encoders. The code and model will be publicly\navailable at https://github.com/icip-cas/SAISA.", "published": "2025-02-04 16:28:53", "link": "http://arxiv.org/abs/2502.02458v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and\n  Retrieval-Augmented Generation", "abstract": "Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical\ncomponents of modern applications in information retrieval, question answering,\nor knowledge-based text generation. However, existing solutions are often\nfragmented, lacking a unified framework that easily integrates these essential\nprocesses. The absence of a standardized implementation, coupled with the\ncomplexity of retrieval and re-ranking workflows, makes it challenging for\nresearchers to compare and evaluate different approaches in a consistent\nenvironment. While existing toolkits such as Rerankers and RankLLM provide\ngeneral-purpose reranking pipelines, they often lack the flexibility required\nfor fine-grained experimentation and benchmarking. In response to these\nchallenges, we introduce Rankify, a powerful and modular open-source toolkit\ndesigned to unify retrieval, re-ranking, and RAG within a cohesive framework.\nRankify supports a wide range of retrieval techniques, including dense and\nsparse retrievers, while incorporating state-of-the-art re-ranking models to\nenhance retrieval quality. Additionally, Rankify includes a collection of\npre-retrieved datasets to facilitate benchmarking, available at Huggingface\n(https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To\nencourage adoption and ease of integration, we provide comprehensive\ndocumentation (http://rankify.readthedocs.io/), an open-source implementation\non GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for\neasy installation (https://pypi.org/project/rankify/). As a unified and\nlightweight framework, Rankify allows researchers and practitioners to advance\nretrieval and re-ranking methodologies while ensuring consistency, scalability,\nand ease of use.", "published": "2025-02-04 16:33:25", "link": "http://arxiv.org/abs/2502.02464v3", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Analyzing Similarity Metrics for Data Selection for Language Model\n  Pretraining", "abstract": "Similarity between training examples is used to curate pretraining datasets\nfor language models by many methods -- for diversification and to select\nexamples similar to high-quality data. However, similarity is typically\nmeasured with off-the-shelf embedding models that are generic or trained for\ntasks such as retrieval. This paper introduces a framework to analyze the\nsuitability of embedding models specifically for data curation in the language\nmodel pretraining setting. We quantify the correlation between similarity in\nthe embedding space to similarity in pretraining loss between different\ntraining examples, and how diversifying in the embedding space affects\npretraining quality. We analyze a variety of embedding models in our framework,\nwith experiments using the Pile dataset for pretraining a 1.7B parameter\ndecoder-only language model. We find that the embedding models we consider are\nall useful for pretraining data curation. Moreover, a simple approach of\naveraging per-token embeddings proves to be surprisingly competitive with more\nsophisticated embedding models -- likely because the latter are not designed\nspecifically for pretraining data curation. Indeed, we believe our analysis and\nevaluation framework can serve as a foundation for the design of embedding\nmodels that specifically reason about similarity in pretraining datasets.", "published": "2025-02-04 17:09:44", "link": "http://arxiv.org/abs/2502.02494v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM\n  Reasoning via Autoregressive Search", "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models will be fully open-sourced.", "published": "2025-02-04 17:26:58", "link": "http://arxiv.org/abs/2502.02508v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are Language Models Up to Sequential Optimization Problems? From\n  Evaluation to a Hegelian-Inspired Enhancement", "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nnumerous fields, presenting an opportunity to revolutionize optimization\nproblem-solving, a crucial, ubiquitous, and complex domain. This paper explores\nthe proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We\nintroduce WorldGen, a dynamic framework for generating unseen SOPs with\ncontrollable complexities, to evaluate LLM performance. Our initial\nobservations reveal that while LLMs perform well on simple SOPs, their\nperformance significantly degrades with increased complexity. Motivated by\nthis, we revisit philosophical hypotheses on reasoning to enhance LLM\nperformance. Inspired by the influential framework of Hegelian Dialectics, we\npropose ACE, demonstrating how the performance of LLMs in SOP contexts can be\nsignificantly improved without any retraining or further fine-tuning.", "published": "2025-02-04 18:47:31", "link": "http://arxiv.org/abs/2502.02573v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention\n  Logit Interpolation (GALI)", "abstract": "Transformer-based Large Language Models (LLMs) struggle to process inputs\nexceeding their training context window, with performance degrading due to\npositional out-of-distribution (O.O.D.) that disrupt attention computations.\nExisting solutions, fine-tuning and training-free methods, are limited by\ncomputational inefficiency, attention logit outliers or loss of local\npositional information. To address this, we propose Greedy Attention Logit\nInterpolation (GALI), a training-free length extrapolation method that\nmaximizes the utilization of pretrained positional intervals while avoiding\nattention logit outliers through attention logit interpolation. The result\ndemonstrates that GALI consistently outperforms state-of-the-art training-free\nmethods. Our findings reveal that LLMs interpret positional intervals unevenly\nwithin their training context window, suggesting that extrapolating within a\nsmaller positional interval range yields superior results-even for\nshort-context tasks. GALI represents a significant step toward resolving the\npositional O.O.D. challenge, enabling more reliable long-text understanding in\nLLMs. Our implementation of GALI, along with the experiments from our paper, is\nopen-sourced at https://github.com/AcademyCityL/GALI.", "published": "2025-02-04 19:01:24", "link": "http://arxiv.org/abs/2502.02659v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Transformers Boost the Performance of Decision Trees on Tabular Data\n  across Sample Sizes", "abstract": "Large language models (LLMs) perform remarkably well on tabular datasets in\nzero- and few-shot settings, since they can extract meaning from natural\nlanguage column headers that describe features and labels. Similarly, TabPFN, a\nrecent non-LLM transformer pretrained on numerous tables for in-context\nlearning, has demonstrated excellent performance for dataset sizes up to a\nthousand samples. In contrast, gradient-boosted decision trees (GBDTs) are\ntypically trained from scratch on each dataset without benefiting from\npretraining data and must learn the relationships between columns from their\nentries alone since they lack natural language understanding. LLMs and TabPFN\nexcel on small tabular datasets where a strong prior is essential, yet they are\nnot competitive with GBDTs on medium or large datasets, since their context\nlengths are limited. In this paper, we propose a simple and lightweight\napproach for fusing large language models and TabPFN with gradient-boosted\ndecision trees, which allows scalable GBDTs to benefit from the natural\nlanguage capabilities and pretraining of transformers. We name our fusion\nmethods LLM-Boost and PFN-Boost, respectively. While matching or surpassing the\nperformance of the transformer at sufficiently small dataset sizes and GBDTs at\nsufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone\ncomponents on a wide range of dataset sizes in between. We demonstrate\nstate-of-the-art performance against numerous baselines and ensembling\nalgorithms. We find that PFN-Boost achieves the best average performance among\nall methods we test for all but very small dataset sizes. We release our code\nat http://github.com/MayukaJ/LLM-Boost .", "published": "2025-02-04 19:30:41", "link": "http://arxiv.org/abs/2502.02672v2", "categories": ["cs.CL", "cs.LG", "I.2.m; I.2.6; I.2.7"], "primary_category": "cs.CL"}
{"title": "A Unified Understanding and Evaluation of Steering Methods", "abstract": "Steering methods provide a practical approach to controlling large language\nmodels by applying steering vectors to intermediate activations, guiding\noutputs toward desired behaviors while avoiding retraining. Despite their\ngrowing importance, the field lacks a unified understanding and consistent\nevaluation across tasks and datasets, hindering progress. This paper introduces\na unified framework for analyzing and evaluating steering methods, formalizing\ntheir core principles and offering theoretical insights into their\neffectiveness. Through comprehensive empirical evaluations on multiple-choice\nand open-ended text generation tasks, we validate these insights, identifying\nkey factors that influence performance and demonstrating the superiority of\ncertain methods. Our work bridges theoretical and practical perspectives,\noffering actionable guidance for advancing the design, optimization, and\ndeployment of steering methods in LLMs.", "published": "2025-02-04 20:55:24", "link": "http://arxiv.org/abs/2502.02716v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning", "abstract": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.", "published": "2025-02-04 23:26:10", "link": "http://arxiv.org/abs/2502.02770v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Multi-Lingual Cyber Threat Detection in Tweets/X Using ML, DL, and LLM:\n  A Comparative Analysis", "abstract": "Cyber threat detection has become an important area of focus in today's\ndigital age due to the growing spread of fake information and harmful content\non social media platforms such as Twitter (now 'X'). These cyber threats, often\ndisguised within tweets, pose significant risks to individuals, communities,\nand even nations, emphasizing the need for effective detection systems. While\nprevious research has explored tweet-based threats, much of the work is limited\nto specific languages, domains, or locations, or relies on single-model\napproaches, reducing their applicability to diverse real-world scenarios. To\naddress these gaps, our study focuses on multi-lingual tweet cyber threat\ndetection using a variety of advanced models. The research was conducted in\nthree stages: (1) We collected and labeled tweet datasets in four languages\nEnglish, Chinese, Russian, and Arabic employing both manual and polarity-based\nlabeling methods to ensure high-quality annotations. (2) Each dataset was\nanalyzed individually using machine learning (ML) and deep learning (DL) models\nto assess their performance on distinct languages. (3) Finally, we combined all\nfour datasets into a single multi-lingual dataset and applied DL and large\nlanguage model (LLM) architectures to evaluate their efficacy in identifying\ncyber threats across various languages. Our results show that among machine\nlearning models, Random Forest (RF) attained the highest performance; however,\nthe Bi-LSTM architecture consistently surpassed other DL and LLM architectures\nacross all datasets. These findings underline the effectiveness of Bi-LSTM in\nmultilingual cyber threat detection. The code for this paper can be found at\nthis link: https://github.com/Mmurrad/Tweet-Data-Classification.git.", "published": "2025-02-04 03:46:24", "link": "http://arxiv.org/abs/2502.04346v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SCALM: Detecting Bad Practices in Smart Contracts Through LLMs", "abstract": "As the Ethereum platform continues to mature and gain widespread usage, it is\ncrucial to maintain high standards of smart contract writing practices. While\nbad practices in smart contracts may not directly lead to security issues, they\ndo elevate the risk of encountering problems. Therefore, to understand and\navoid these bad practices, this paper introduces the first systematic study of\nbad practices in smart contracts, delving into over 35 specific issues.\nSpecifically, we propose a large language models (LLMs)-based framework, SCALM.\nIt combines Step-Back Prompting and Retrieval-Augmented Generation (RAG) to\nidentify and address various bad practices effectively. Our extensive\nexperiments using multiple LLMs and datasets have shown that SCALM outperforms\nexisting tools in detecting bad practices in smart contracts.", "published": "2025-02-04 15:15:13", "link": "http://arxiv.org/abs/2502.04347v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Prompt-based Depth Pruning of Large Language Models", "abstract": "Depth pruning aims to reduce the inference cost of a large language model\nwithout any hardware-specific complications, by simply removing several less\nimportant transformer blocks. However, our empirical findings suggest that the\nimportance of a transformer block may be highly task-dependent -- a block that\nis crucial for a task can be removed without degrading the accuracy on another\ntask. Based on this observation, we develop a dynamic depth pruning algorithm,\ncoined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which\nblocks to omit from the model based on the input prompt. PuDDing operates by\ntraining a lightweight router to predict the best omission set among a set of\noptions, where this option set has also been constructed in a data-driven\nmanner. Empirical results on commonsense reasoning benchmarks demonstrate that\nPuDDing effectively accelerates the inference language models, and achieves\nbetter on-task performance than static depth pruning baselines.", "published": "2025-02-04 15:16:17", "link": "http://arxiv.org/abs/2502.04348v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Dynamic benchmarking framework for LLM-based conversational data capture", "abstract": "The rapid evolution of large language models (LLMs) has transformed\nconversational agents, enabling complex human-machine interactions. However,\nevaluation frameworks often focus on single tasks, failing to capture the\ndynamic nature of multi-turn dialogues. This paper introduces a dynamic\nbenchmarking framework to assess LLM-based conversational agents through\ninteractions with synthetic users. The framework integrates generative agent\nsimulation to evaluate performance on key dimensions: information extraction,\ncontext awareness, and adaptive engagement. By simulating various aspects of\nuser behavior, our work provides a scalable, automated, and flexible\nbenchmarking approach. Experimental evaluation - within a loan application use\ncase - demonstrates the framework's effectiveness under one-shot and few-shot\nextraction conditions. Results show that adaptive strategies improve data\nextraction accuracy, especially when handling ambiguous responses. Future work\nwill extend its applicability to broader domains and incorporate additional\nmetrics (e.g., conversational coherence, user engagement). This study\ncontributes a structured, scalable approach to evaluating LLM-based\nconversational agents, facilitating real-world deployment.", "published": "2025-02-04 15:47:47", "link": "http://arxiv.org/abs/2502.04349v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NER4all or Context is All You Need: Using LLMs for low-effort,\n  high-performance NER on historical texts. A humanities informed approach", "abstract": "Named entity recognition (NER) is a core task for historical research in\nautomatically establishing all references to people, places, events and the\nlike. Yet, do to the high linguistic and genre diversity of sources, only\nlimited canonisation of spellings, the level of required historical domain\nknowledge, and the scarcity of annotated training data, established approaches\nto natural language processing (NLP) have been both extremely expensive and\nyielded only unsatisfactory results in terms of recall and precision. Our paper\nintroduces a new approach. We demonstrate how readily-available,\nstate-of-the-art LLMs significantly outperform two leading NLP frameworks,\nspaCy and flair, for NER in historical documents by seven to twentytwo percent\nhigher F1-Scores. Our ablation study shows how providing historical context to\nthe task and a bit of persona modelling that turns focus away from a purely\nlinguistic approach are core to a successful prompting strategy. We also\ndemonstrate that, contrary to our expectations, providing increasing numbers of\nexamples in few-shot approaches does not improve recall or precision below a\nthreshold of 16-shot. In consequence, our approach democratises access to NER\nfor all historians by removing the barrier of scripting languages and\ncomputational skills required for established NLP tools and instead leveraging\nnatural language prompts and consumer-grade tools and frontends.", "published": "2025-02-04 16:54:23", "link": "http://arxiv.org/abs/2502.04351v1", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Investigating the Robustness of Deductive Reasoning with Large Language\n  Models", "abstract": "Large Language Models (LLMs) have been shown to achieve impressive results\nfor many reasoning-based Natural Language Processing (NLP) tasks, suggesting a\ndegree of deductive reasoning capability. However, it remains unclear to which\nextent LLMs, in both informal and autoformalisation methods, are robust on\nlogical deduction tasks. Moreover, while many LLM-based deduction methods have\nbeen proposed, there is a lack of a systematic study that analyses the impact\nof their design components. Addressing these two challenges, we propose the\nfirst study of the robustness of LLM-based deductive reasoning methods. We\ndevise a framework with two families of perturbations: adversarial noise and\ncounterfactual statements, which jointly generate seven perturbed datasets. We\norganize the landscape of LLM reasoners according to their reasoning format,\nformalisation syntax, and feedback for error recovery. The results show that\nadversarial noise affects autoformalisation, while counterfactual statements\ninfluence all approaches. Detailed feedback does not improve overall accuracy\ndespite reducing syntax errors, pointing to the challenge of LLM-based methods\nto self-correct effectively.", "published": "2025-02-04 17:16:51", "link": "http://arxiv.org/abs/2502.04352v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LLM-ProS: Analyzing Large Language Models' Performance in Competitive\n  Problem Solving", "abstract": "The rapid advancement of large language models has opened new avenues for\nautomating complex problem-solving tasks such as algorithmic coding and\ncompetitive programming. This paper introduces a novel evaluation technique,\nLLM-ProS, to assess the performance of state-of-the-art LLMs on International\nCollegiate Programming Contest (ICPC) problems. Using a curated dataset of 166\nWorld Finals problems from 2011 to 2024, we benchmark the models' reasoning,\naccuracy, and efficiency. We evaluate the five models-GPT-4o, Mistral Large,\nLlama-3.1-405B, and the o1 family, consisting of o1-mini and o1-preview, across\ncritical metrics like correctness, resource utilization, and response\ncalibration. Our results reveal significant differences in the models'\nabilities to generalize, adapt, and solve novel problems. We also investigated\nthe impact of training methodologies, dataset contamination, and\nchain-of-thought reasoning on model performance. The findings provide new\ninsights into optimizing LLMs for algorithmic tasks, highlighting both\nstrengths and limitations of current models.", "published": "2025-02-04 18:55:14", "link": "http://arxiv.org/abs/2502.04355v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Position: Stop Acting Like Language Model Agents Are Normal Agents", "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of\nautonomously navigating interactions with humans and tools. Their design and\ndeployment tends to presume they are normal agents capable of sustaining\ncoherent goals, adapting across contexts and acting with a measure of\nintentionality. These assumptions are critical to prospective use cases in\nindustrial, social and governmental settings. But LMAs are not normal agents.\nThey inherit the structural problems of the large language models (LLMs) around\nwhich they are built: hallucinations, jailbreaking, misalignment and\nunpredictability. In this Position paper we argue LMAs should not be treated as\nnormal agents, because doing so leads to problems that undermine their utility\nand trustworthiness. We enumerate pathologies of agency intrinsic to LMAs.\nDespite scaffolding such as external memory and tools, they remain\nontologically stateless, stochastic, semantically sensitive, and linguistically\nintermediated. These pathologies destabilise the ontological properties of LMAs\nincluding identifiability, continuity, persistence and and consistency,\nproblematising their claim to agency. In response, we argue LMA ontological\nproperties should be measured before, during and after deployment so that the\nnegative effects of pathologies can be mitigated.", "published": "2025-02-04 08:14:18", "link": "http://arxiv.org/abs/2502.10420v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Causal Interpretations in Observational Studies: The Role of\n  Sociocultural Backgrounds and Team Dynamics", "abstract": "The prevalence of drawing causal conclusions from observational studies has\nraised concerns about potential exaggeration in science communication. While\nsome believe causal language should only apply to randomized controlled trials,\nothers argue that rigorous methods can justify causal claims in observational\nstudies. Ideally, causal language should align with the strength of the\nevidence. However, through the analysis of over 80,000 observational study\nabstracts using computational linguistic and regression methods, we found that\ncausal language is more frequently used by less experienced authors, smaller\nresearch teams, male last authors, and authors from countries with higher\nuncertainty avoidance indices. These findings suggest that the use of causal\nlanguage may be influenced by external factors such as the sociocultural\nbackgrounds of authors and the dynamics of research collaboration. This newly\nidentified link deepens our understanding of how such factors help shape\nscientific conclusions in causal inference and science communication.", "published": "2025-02-04 02:00:10", "link": "http://arxiv.org/abs/2502.12159v1", "categories": ["physics.soc-ph", "cs.CL"], "primary_category": "physics.soc-ph"}
{"title": "PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation,\n  Negative Demonstration, and Adaptive Sampling", "abstract": "Many-shot jailbreaking circumvents the safety alignment of large language\nmodels by exploiting their ability to process long input sequences. To achieve\nthis, the malicious target prompt is prefixed with hundreds of fabricated\nconversational turns between the user and the model. These fabricated exchanges\nare randomly sampled from a pool of malicious questions and responses, making\nit appear as though the model has already complied with harmful instructions.\nIn this paper, we present PANDAS: a hybrid technique that improves many-shot\njailbreaking by modifying these fabricated dialogues with positive\naffirmations, negative demonstrations, and an optimized adaptive sampling\nmethod tailored to the target prompt's topic. Extensive experiments on AdvBench\nand HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS\nsignificantly outperforms baseline methods in long-context scenarios. Through\nan attention analysis, we provide insights on how long-context vulnerabilities\nare exploited and show how PANDAS further improves upon many-shot jailbreaking.", "published": "2025-02-04 01:51:31", "link": "http://arxiv.org/abs/2502.01925v1", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CITER: Collaborative Inference for Efficient Large Language Model\n  Decoding with Token-Level Routing", "abstract": "Large language models have achieved remarkable success in various tasks but\nsuffer from high computational costs during inference, limiting their\ndeployment in resource-constrained applications. To address this issue, we\npropose a novel Collaborative Inference with Token-lEvel Routing (CITER)\nframework that enables efficient collaboration between small and large language\nmodels (SLMs \\& LLMs) through a token-level routing strategy. Specifically,\nCITER routes non-critical tokens to an SLM for efficiency and routes critical\ntokens to an LLM for generalization quality. We formulate router training as a\npolicy optimization, where the router receives rewards based on both the\nquality of predictions and the inference costs of generation. This allows the\nrouter to learn to predict token-level routing scores and make routing\ndecisions based on both the current token and the future impact of its\ndecisions. To further accelerate the reward evaluation process, we introduce a\nshortcut which significantly reduces the costs of the reward estimation and\nimproving the practicality of our approach. Extensive experiments on five\nbenchmark datasets demonstrate that CITER reduces the inference costs while\npreserving high-quality generation, offering a promising solution for real-time\nand resource-constrained applications. Our data and code are available at\nhttps://github.com/aiming-lab/CITER.", "published": "2025-02-04 03:36:44", "link": "http://arxiv.org/abs/2502.01976v4", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Layer by Layer: Uncovering Hidden Representations in Language Models", "abstract": "From extracting features to generating text, the outputs of large language\nmodels (LLMs) typically rely on their final layers, following the conventional\nwisdom that earlier layers capture only low-level cues. However, our analysis\nshows that intermediate layers can encode even richer representations, often\nimproving performance on a wide range of downstream tasks. To explain and\nquantify these hidden-layer properties, we propose a unified framework of\nrepresentation quality metrics based on information theory, geometry, and\ninvariance to input perturbations. Our framework highlights how each model\nlayer balances information compression and signal preservation, revealing why\nmid-depth embeddings can exceed the last layer's performance. Through extensive\nexperiments on 32 text-embedding tasks and comparisons across model\narchitectures (transformers, state-space models) and domains (language,\nvision), we demonstrate that intermediate layers consistently provide stronger\nfeatures. These findings challenge the standard focus on final-layer embeddings\nand open new directions for model analysis and optimization, including\nstrategic use of mid-layer representations for more robust and accurate AI\nsystems.", "published": "2025-02-04 05:03:42", "link": "http://arxiv.org/abs/2502.02013v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer\n  Inference", "abstract": "Residual transformations enhance the representational depth and expressive\npower of large language models (LLMs). However, applying static residual\ntransformations across all tokens in auto-regressive generation leads to a\nsuboptimal trade-off between inference efficiency and generation fidelity.\nExisting methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth\naddress this by modulating the residual transformation based on token-level\ncomplexity. Nevertheless, these approaches predominantly consider the distance\ntraversed by tokens through the model layers, neglecting the underlying\nvelocity of residual evolution. We introduce Mixture of Multi-rate Residuals\n(M2R2), a framework that dynamically modulates residual velocity to improve\nearly alignment, enhancing inference efficiency. Evaluations on reasoning\noriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2\nsurpasses state-of-the-art distance-based strategies, balancing generation\nquality and speedup. In self-speculative decoding setup, M2R2 achieves up to\n2.8x speedups on MT-Bench, outperforming methods like 2-model speculative\ndecoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE)\narchitectures, integrating early residual alignment with ahead-of-time expert\nloading into high-bandwidth memory (HBM) accelerates decoding, reduces\nexpert-switching bottlenecks, and achieves a 2.9x speedup, making it highly\neffective in resource-constrained environments.", "published": "2025-02-04 06:13:52", "link": "http://arxiv.org/abs/2502.02040v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficient Domain Adaptation of Multimodal Embeddings using Constrastive\n  Learning", "abstract": "Recent advancements in machine learning (ML), natural language processing\n(NLP), and foundational models have shown promise for real-life applications in\ncritical, albeit compute-constrainted fields like healthcare.\n  In such areas, combining foundational models with supervised ML offers\npotential for automating tasks like diagnosis and treatment planning, but the\nlimited availability of onsite computational resources pose significant\nchallenges before applying these technologies effectively: Current approaches\neither yield subpar results when using pretrained models without task-specific\nadaptation, or require substantial computational resources for fine-tuning,\nwhich is often a barrier to entry in such environments.\n  This renders them inaccessible in applications where performance and quality\nstandards are high, but computational resources are scarce.\n  To bridge the gap between best-in-class performance and accessibility, we\npropose a novel method for adapting foundational, multimodal embeddings to\ndownstream tasks, without the need of expensive fine-tuning processes.\n  Our method leverages frozen embeddings from Large Language Models (LLMs) and\nVision Models, and uses contrastive learning to train a small, task-specific\nnonlinear projection that can be used in the downstream task, without having to\nfine-tune the original foundational models.\n  We show that this efficient procedure leads to significant performance\nimprovements across various downstream tasks, and perhaps more importantly with\nminimal computational overhead, offering a practical solution for the use of\nadvanced, foundational ML models in resource-constrained settings.", "published": "2025-02-04 06:30:12", "link": "http://arxiv.org/abs/2502.02048v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Anticipate & Act : Integrating LLMs and Classical Planning for Efficient\n  Task Execution in Household Environments", "abstract": "Assistive agents performing household tasks such as making the bed or cooking\nbreakfast often compute and execute actions that accomplish one task at a time.\nHowever, efficiency can be improved by anticipating upcoming tasks and\ncomputing an action sequence that jointly achieves these tasks.\nState-of-the-art methods for task anticipation use data-driven deep networks\nand Large Language Models (LLMs), but they do so at the level of high-level\ntasks and/or require many training examples. Our framework leverages the\ngeneric knowledge of LLMs through a small number of prompts to perform\nhigh-level task anticipation, using the anticipated tasks as goals in a\nclassical planning system to compute a sequence of finer-granularity actions\nthat jointly achieve these goals. We ground and evaluate our framework's\nabilities in realistic scenarios in the VirtualHome environment and demonstrate\na 31% reduction in execution time compared with a system that does not consider\nupcoming tasks.", "published": "2025-02-04 07:31:55", "link": "http://arxiv.org/abs/2502.02066v1", "categories": ["cs.RO", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement", "abstract": "An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/", "published": "2025-02-04 07:32:39", "link": "http://arxiv.org/abs/2502.02067v2", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Robust and Secure Code Watermarking for Large Language Models via\n  ML/Crypto Codesign", "abstract": "This paper introduces RoSeMary, the first-of-its-kind ML/Crypto codesign\nwatermarking framework that regulates LLM-generated code to avoid intellectual\nproperty rights violations and inappropriate misuse in software development.\nHigh-quality watermarks adhering to the detectability-fidelity-robustness\ntri-objective are limited due to codes' low-entropy nature. Watermark\nverification, however, often needs to reveal the signature and requires\nre-encoding new ones for code reuse, which potentially compromising the\nsystem's usability. To overcome these challenges, RoSeMary obtains high-quality\nwatermarks by training the watermark insertion and extraction modules\nend-to-end to ensure (i) unaltered watermarked code functionality and (ii)\nenhanced detectability and robustness leveraging pre-trained CodeT5 as the\ninsertion backbone to enlarge the code syntactic and variable rename\ntransformation search space. In the deployment, RoSeMary uses zero-knowledge\nproofs for secure verification without revealing the underlying signatures.\nExtensive evaluations demonstrated RoSeMary achieves high detection accuracy\nwhile preserving the code functionality. RoSeMary is also robust against\nattacks and provides efficient secure watermark verification.", "published": "2025-02-04 07:35:28", "link": "http://arxiv.org/abs/2502.02068v2", "categories": ["cs.CR", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for\n  Detection of Bias, Discrimination and Stereotyping", "abstract": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development.", "published": "2025-02-04 07:44:20", "link": "http://arxiv.org/abs/2502.02072v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Risk-Aware Driving Scenario Analysis with Large Language Models", "abstract": "Large Language Models (LLMs) can capture nuanced contextual relationships,\nreasoning, and complex problem-solving. By leveraging their ability to process\nand interpret large-scale information, LLMs have shown potential to address\ndomain-specific challenges, including those in autonomous driving systems. This\npaper proposes a novel framework that leverages LLMs for risk-aware analysis of\ngenerated driving scenarios. We hypothesize that LLMs can effectively evaluate\nwhether driving scenarios generated by autonomous driving testing simulators\nare safety-critical. To validate this hypothesis, we conducted an empirical\nevaluation to assess the effectiveness of LLMs in performing this task. This\nframework will also provide feedback to generate the new safety-critical\nscenario by using adversarial method to modify existing non-critical scenarios\nand test their effectiveness in validating motion planning algorithms. Code and\nscenarios are available at:\nhttps://github.com/yuangao-tum/Riskaware-Scenario-analyse", "published": "2025-02-04 09:19:13", "link": "http://arxiv.org/abs/2502.02145v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Vulnerability Mitigation for Safety-Aligned Language Models via\n  Debiasing", "abstract": "Safety alignment is an essential research topic for real-world AI\napplications. Despite the multifaceted nature of safety and trustworthiness in\nAI, current safety alignment methods often focus on a comprehensive notion of\nsafety. By carefully assessing models from the existing safety-alignment\nmethods, we found that, while they generally improved overall safety\nperformance, they failed to ensure safety in specific categories. Our study\nfirst identified the difficulty of eliminating such vulnerabilities without\nsacrificing the model's helpfulness. We observed that, while smaller KL penalty\nparameters, increased training iterations, and dataset cleansing can enhance\nsafety, they do not necessarily improve the trade-off between safety and\nhelpfulness. We discovered that safety alignment could even induce undesired\neffects and result in a model that prefers generating negative tokens leading\nto rejective responses, regardless of the input context. To address this, we\nintroduced a learning-free method, Token-level Safety-Debiased Inference\n(TSDI), to estimate and correct this bias during the generation process using\nrandomly constructed prompts. Our experiments demonstrated that our method\ncould enhance the model's helpfulness while maintaining safety, thus improving\nthe trade-off Pareto-front.", "published": "2025-02-04 09:31:54", "link": "http://arxiv.org/abs/2502.02153v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Can You Move These Over There? An LLM-based VR Mover for Supporting\n  Object Manipulation", "abstract": "In our daily lives, we can naturally convey instructions for the spatial\nmanipulation of objects using words and gestures. Transposing this form of\ninteraction into virtual reality (VR) object manipulation can be beneficial. We\npropose VR Mover, an LLM-empowered solution that can understand and interpret\nthe user's vocal instruction to support object manipulation. By simply pointing\nand speaking, the LLM can manipulate objects without structured input. Our user\nstudy demonstrates that VR Mover enhances user usability, overall experience\nand performance on multi-object manipulation, while also reducing workload and\narm fatigue. Users prefer the proposed natural interface for broad movements\nand may complementarily switch to gizmos or virtual hands for finer\nadjustments. These findings are believed to contribute to design implications\nfor future LLM-based object manipulation interfaces, highlighting the potential\nfor more intuitive and efficient user interactions in VR environments.", "published": "2025-02-04 10:27:40", "link": "http://arxiv.org/abs/2502.02201v1", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.ET"], "primary_category": "cs.HC"}
{"title": "Avoiding spurious sharpness minimization broadens applicability of SAM", "abstract": "Curvature regularization techniques like Sharpness Aware Minimization (SAM)\nhave shown great promise in improving generalization on vision tasks. However,\nwe find that SAM performs poorly in domains like natural language processing\n(NLP), often degrading performance -- even with twice the compute budget. We\ninvestigate the discrepancy across domains and find that in the NLP setting,\nSAM is dominated by regularization of the logit statistics -- instead of\nimproving the geometry of the function itself. We use this observation to\ndevelop an alternative algorithm we call Functional-SAM, which regularizes\ncurvature only through modification of the statistics of the overall function\nimplemented by the neural network, and avoids spurious minimization through\nlogit manipulation. Furthermore, we argue that preconditioning the SAM\nperturbation also prevents spurious minimization, and when combined with\nFunctional-SAM, it gives further improvements. Our proposed algorithms show\nimproved performance over AdamW and SAM baselines when trained for an equal\nnumber of steps, in both fixed-length and Chinchilla-style training settings,\nat various model scales (including billion-parameter scale). On the whole, our\nwork highlights the importance of more precise characterizations of sharpness\nin broadening the applicability of curvature regularization to large language\nmodels (LLMs).", "published": "2025-02-04 15:25:47", "link": "http://arxiv.org/abs/2502.02407v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies", "abstract": "Large language models, employed as multiple agents that interact and\ncollaborate with each other, have excelled at solving complex tasks. The agents\nare programmed with prompts that declare their functionality, along with the\ntopologies that orchestrate interactions across agents. Designing prompts and\ntopologies for multi-agent systems (MAS) is inherently complex. To automate the\nentire design process, we first conduct an in-depth analysis of the design\nspace aiming to understand the factors behind building effective MAS. We reveal\nthat prompts together with topologies play critical roles in enabling more\neffective MAS design. Based on the insights, we propose Multi-Agent System\nSearch (MASS), a MAS optimization framework that efficiently exploits the\ncomplex MAS design space by interleaving its optimization stages, from local to\nglobal, from prompts to topologies, over three stages: 1) block-level (local)\nprompt optimization; 2) workflow topology optimization; 3) workflow-level\n(global) prompt optimization, where each stage is conditioned on the\niteratively optimized prompts/topologies from former stages. We show that\nMASS-optimized multi-agent systems outperform a spectrum of existing\nalternatives by a substantial margin. Based on the MASS-found systems, we\nfinally propose design principles behind building effective multi-agent\nsystems.", "published": "2025-02-04 17:56:44", "link": "http://arxiv.org/abs/2502.02533v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "primary_category": "cs.LG"}
{"title": "ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization", "abstract": "The optimal bit-width for achieving the best trade-off between quantized\nmodel size and accuracy has been a subject of ongoing debate. While some\nadvocate for 4-bit quantization, others propose that 1.58-bit offers superior\nresults. However, the lack of a cohesive framework for different bits has left\nsuch conclusions relatively tenuous. We present ParetoQ, the first unified\nframework that facilitates rigorous comparisons across 1-bit, 1.58-bit, 2-bit,\n3-bit, and 4-bit quantization settings. Our findings reveal a notable learning\ntransition between 2 and 3 bits: For 3-bits and above, the fine-tuned models\nstay close to their original pre-trained distributions, whereas for learning\n2-bit networks or below, the representations change drastically. By optimizing\ntraining schemes and refining quantization functions, ParetoQ surpasses all\nprevious methods tailored to specific bit widths. Remarkably, our ParetoQ\nternary 600M-parameter model even outperforms the previous SoTA ternary\n3B-parameter model in accuracy, using only one-fifth of the parameters.\nExtensive experimentation shows that ternary, 2-bit, and 3-bit quantization\nmaintains comparable performance in the size-accuracy trade-off and generally\nexceeds 4-bit and binary quantization. Considering hardware constraints, 2-bit\nquantization offers promising potential for memory reduction and speedup.", "published": "2025-02-04 18:59:26", "link": "http://arxiv.org/abs/2502.02631v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "On Teacher Hacking in Language Model Distillation", "abstract": "Post-training of language models (LMs) increasingly relies on the following\ntwo stages: (i) knowledge distillation, where the LM is trained to imitate a\nlarger teacher LM, and (ii) reinforcement learning from human feedback (RLHF),\nwhere the LM is aligned by optimizing a reward model. In the second RLHF stage,\na well-known challenge is reward hacking, where the LM over-optimizes the\nreward model. Such phenomenon is in line with Goodhart's law and can lead to\ndegraded performance on the true objective. In this paper, we investigate\nwhether a similar phenomenon, that we call teacher hacking, can occur during\nknowledge distillation. This could arise because the teacher LM is itself an\nimperfect approximation of the true distribution. To study this, we propose a\ncontrolled experimental setup involving: (i) an oracle LM representing the\nground-truth distribution, (ii) a teacher LM distilled from the oracle, and\n(iii) a student LM distilled from the teacher. Our experiments reveal the\nfollowing insights. When using a fixed offline dataset for distillation,\nteacher hacking occurs; moreover, we can detect it by observing when the\noptimization process deviates from polynomial convergence laws. In contrast,\nemploying online data generation techniques effectively mitigates teacher\nhacking. More precisely, we identify data diversity as the key factor in\npreventing hacking. Overall, our findings provide a deeper understanding of the\nbenefits and limitations of distillation for building robust and efficient LMs.", "published": "2025-02-04 19:26:28", "link": "http://arxiv.org/abs/2502.02671v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Streaming Speaker Change Detection and Gender Classification for\n  Transducer-Based Multi-Talker Speech Translation", "abstract": "Streaming multi-talker speech translation is a task that involves not only\ngenerating accurate and fluent translations with low latency but also\nrecognizing when a speaker change occurs and what the speaker's gender is.\nSpeaker change information can be used to create audio prompts for a zero-shot\ntext-to-speech system, and gender can help to select speaker profiles in a\nconventional text-to-speech model. We propose to tackle streaming speaker\nchange detection and gender classification by incorporating speaker embeddings\ninto a transducer-based streaming end-to-end speech translation model. Our\nexperiments demonstrate that the proposed methods can achieve high accuracy for\nboth speaker change detection and gender classification.", "published": "2025-02-04 19:50:15", "link": "http://arxiv.org/abs/2502.02683v1", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Peri-LN: Revisiting Layer Normalization in the Transformer Architecture", "abstract": "Designing Transformer architectures with the optimal layer normalization (LN)\nstrategy that ensures large-scale training stability and expedite convergence\nhas remained elusive, even in this era of large language models (LLMs). To this\nend, we present a comprehensive analytical foundation for understanding how\ndifferent LN strategies influence training dynamics in large-scale Transformer\ntraining. Until recently, Pre-LN and Post-LN have long dominated standard\npractices despite their limitations in large-scale training. However, several\nopen-source large-scale models have recently begun silently adopting a third\nstrategy without much explanation. This strategy places layer normalization\n(LN) peripherally around sublayers, a design we term Peri-LN. While Peri-LN has\ndemonstrated promising empirical performance, its precise mechanisms and\nbenefits remain almost unexplored. Our in-depth analysis shows that Peri-LN\nstrikes an ideal balance in variance growth -- unlike Pre-LN and Post-LN, which\nare prone to vanishing gradients and ``massive activations.'' To validate our\ntheoretical insight, we conduct large-scale experiments on Transformers up to\n3.2B parameters, showing that Peri-LN consistently achieves more balanced\nvariance growth, steadier gradient flow, and convergence stability. Our results\nsuggest that Peri-LN warrants broader consideration for large-scale Transformer\narchitectures, providing renewed insights into the optimal placement and\napplication of LN.", "published": "2025-02-04 21:29:47", "link": "http://arxiv.org/abs/2502.02732v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "JingFang: A Traditional Chinese Medicine Large Language Model of\n  Expert-Level Medical Diagnosis and Syndrome Differentiation-Based Treatment", "abstract": "Traditional Chinese medicine (TCM) plays a vital role in health protection\nand disease treatment, but its practical application requires extensive medical\nknowledge and clinical experience. Existing TCM Large Language Models (LLMs)\nexhibit critical limitations of uncomprehensive medical consultation and\ndiagnoses, and inaccurate syndrome differentiation-based treatment. To address\nthese issues, this study establishes JingFang (JF): a novel TCM Large Language\nModel that demonstrates the expert-level capability of medical diagnosis and\nsyndrome differentiation-based treatment. We innovate a Multi-agent Dynamic\nCollaborative Chain-of-Thought Mechanism (MDCCTM) for medical consultation,\nenabling JF with effective and accurate diagnostic ability. In addition, a\nSyndrome Agent and a Dual-Stage Retrieval Scheme (DSRS) are developed to\nsignificantly enhance the capacity of JF for disease treatment based on\nsyndrome differentiation. JingFang not only facilitates the application of LLMs\nbut also promotes the effective practice of TCM in human health protection and\ndisease treatment.", "published": "2025-02-04 01:45:42", "link": "http://arxiv.org/abs/2502.04345v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "CognArtive: Large Language Models for Automating Art Analysis and\n  Decoding Aesthetic Elements", "abstract": "Art, as a universal language, can be interpreted in diverse ways, with\nartworks embodying profound meanings and nuances. The advent of Large Language\nModels (LLMs) and the availability of Multimodal Large Language Models (MLLMs)\nraise the question of how these transformative models can be used to assess and\ninterpret the artistic elements of artworks. While research has been conducted\nin this domain, to the best of our knowledge, a deep and detailed understanding\nof the technical and expressive features of artworks using LLMs has not been\nexplored. In this study, we investigate the automation of a formal art analysis\nframework to analyze a high-throughput number of artworks rapidly and examine\nhow their patterns evolve over time. We explore how LLMs can decode artistic\nexpressions, visual elements, composition, and techniques, revealing emerging\npatterns that develop across periods. Finally, we discuss the strengths and\nlimitations of LLMs in this context, emphasizing their ability to process vast\nquantities of art-related data and generate insightful interpretations. Due to\nthe exhaustive and granular nature of the results, we have developed\ninteractive data visualizations, available online\nhttps://cognartive.github.io/, to enhance understanding and accessibility.", "published": "2025-02-04 18:08:23", "link": "http://arxiv.org/abs/2502.04353v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Reviving The Classics: Active Reward Modeling in Large Language Model\n  Alignment", "abstract": "Building neural reward models from human preferences is a pivotal component\nin reinforcement learning from human feedback (RLHF) and large language model\nalignment research. Given the scarcity and high cost of human annotation, how\nto select the most informative pairs to annotate is an essential yet\nchallenging open problem. In this work, we highlight the insight that an ideal\ncomparison dataset for reward modeling should balance exploration of the\nrepresentation space and make informative comparisons between pairs with\nmoderate reward differences. Technically, challenges arise in quantifying the\ntwo objectives and efficiently prioritizing the comparisons to be annotated. To\naddress this, we propose the Fisher information-based selection strategies,\nadapt theories from the classical experimental design literature, and apply\nthem to the final linear layer of the deep neural network-based reward modeling\ntasks. Empirically, our method demonstrates remarkable performance, high\ncomputational efficiency, and stability compared to other selection methods\nfrom deep learning and classical statistical literature across multiple\nopen-source LLMs and datasets. Further ablation studies reveal that\nincorporating cross-prompt comparisons in active reward modeling significantly\nenhances labeling efficiency, shedding light on the potential for improved\nannotation strategies in RLHF.", "published": "2025-02-04 18:47:11", "link": "http://arxiv.org/abs/2502.04354v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Open Foundation Models in Healthcare: Challenges, Paradoxes, and\n  Opportunities with GenAI Driven Personalized Prescription", "abstract": "In response to the success of proprietary Large Language Models (LLMs) such\nas OpenAI's GPT-4, there is a growing interest in developing open,\nnon-proprietary LLMs and AI foundation models (AIFMs) for transparent use in\nacademic, scientific, and non-commercial applications. Despite their inability\nto match the refined functionalities of their proprietary counterparts, open\nmodels hold immense potential to revolutionize healthcare applications. In this\npaper, we examine the prospects of open-source LLMs and AIFMs for developing\nhealthcare applications and make two key contributions. Firstly, we present a\ncomprehensive survey of the current state-of-the-art open-source healthcare\nLLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their\nutility across various healthcare tasks. Secondly, to evaluate the\ngeneral-purpose applications of open LLMs in healthcare, we present a case\nstudy on personalized prescriptions. This task is particularly significant due\nto its critical role in delivering tailored, patient-specific medications that\ncan greatly improve treatment outcomes. In addition, we compare the performance\nof open-source models with proprietary models in settings with and without\nRetrieval-Augmented Generation (RAG). Our findings suggest that, although less\nrefined, open LLMs can achieve performance comparable to proprietary models\nwhen paired with grounding techniques such as RAG. Furthermore, to highlight\nthe clinical significance of LLMs-empowered personalized prescriptions, we\nperform subjective assessment through an expert clinician. We also elaborate on\nethical considerations and potential risks associated with the misuse of\npowerful LLMs and AIFMs, highlighting the need for a cautious and responsible\nimplementation in healthcare.", "published": "2025-02-04 19:16:56", "link": "http://arxiv.org/abs/2502.04356v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reusing Embeddings: Reproducible Reward Model Research in Large Language\n  Model Alignment without GPUs", "abstract": "Large Language Models (LLMs) have made substantial strides in structured\ntasks through Reinforcement Learning (RL), demonstrating proficiency in\nmathematical reasoning and code generation. However, applying RL in broader\ndomains like chatbots and content generation -- through the process known as\nReinforcement Learning from Human Feedback (RLHF) -- presents unique\nchallenges. Reward models in RLHF are critical, acting as proxies that evaluate\nthe alignment of LLM outputs with human intent. Despite advancements, the\ndevelopment of reward models is hindered by challenges such as computational\nheavy training, costly evaluation, and therefore poor reproducibility. We\nadvocate for using embedding-based input in reward model research as an\naccelerated solution to those challenges. By leveraging embeddings for reward\nmodeling, we can enhance reproducibility, reduce computational demands on\nhardware, improve training stability, and significantly reduce training and\nevaluation costs, hence facilitating fair and efficient comparisons in this\nactive research area. We then show a case study of reproducing existing reward\nmodel ensemble research using embedding-based reward models. We discussed\nfuture avenues for research, aiming to contribute to safer and more effective\nLLM deployments.", "published": "2025-02-04 19:37:35", "link": "http://arxiv.org/abs/2502.04357v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring Spatial Language Grounding Through Referring Expressions", "abstract": "Spatial Reasoning is an important component of human cognition and is an area\nin which the latest Vision-language models (VLMs) show signs of difficulty. The\ncurrent analysis works use image captioning tasks and visual question\nanswering. In this work, we propose using the Referring Expression\nComprehension task instead as a platform for the evaluation of spatial\nreasoning by VLMs. This platform provides the opportunity for a deeper analysis\nof spatial comprehension and grounding abilities when there is 1) ambiguity in\nobject detection, 2) complex spatial expressions with a longer sentence\nstructure and multiple spatial relations, and 3) expressions with negation\n('not'). In our analysis, we use task-specific architectures as well as large\nVLMs and highlight their strengths and weaknesses in dealing with these\nspecific situations. While all these models face challenges with the task at\nhand, the relative behaviors depend on the underlying models and the specific\ncategories of spatial semantics (topological, directional, proximal, etc.). Our\nresults highlight these challenges and behaviors and provide insight into\nresearch gaps and future directions.", "published": "2025-02-04 22:58:15", "link": "http://arxiv.org/abs/2502.04359v1", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Neurons Speak in Ranges: Breaking Free from Discrete Neuronal\n  Attribution", "abstract": "Interpreting and controlling the internal mechanisms of large language models\n(LLMs) is crucial for improving their trustworthiness and utility. Recent\nefforts have primarily focused on identifying and manipulating neurons by\nestablishing discrete mappings between neurons and semantic concepts. However,\nsuch mappings struggle to handle the inherent polysemanticity in LLMs, where\nindividual neurons encode multiple, distinct concepts. This makes precise\ncontrol challenging and complicates downstream interventions. Through an\nin-depth analysis of both encoder and decoder-based LLMs across multiple text\nclassification datasets, we uncover that while individual neurons encode\nmultiple concepts, their activation magnitudes vary across concepts in\ndistinct, Gaussian-like patterns. Building on this insight, we introduce\nNeuronLens, a novel range-based interpretation and manipulation framework that\nprovides a finer view of neuron activation distributions to localize concept\nattribution within a neuron. Extensive empirical evaluations demonstrate that\nNeuronLens significantly reduces unintended interference, while maintaining\nprecise control for manipulation of targeted concepts, outperforming existing\nmethods.", "published": "2025-02-04 03:33:55", "link": "http://arxiv.org/abs/2502.06809v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Aligning Human and Machine Attention for Enhanced Supervised Learning", "abstract": "Attention, or prioritization of certain information items over others, is a\ncritical element of any learning process, for both humans and machines. Given\nthat humans continue to outperform machines in certain learning tasks, it seems\nplausible that machine performance could be enriched by aligning machine\nattention with human attention mechanisms -- yet research on this topic is\nsparse and has achieved only limited success. This paper proposes a new\napproach to address this gap, called Human-Machine Attention Learning (HuMAL).\nThis approach involves reliance on data annotated by humans to reflect their\nself-perceived attention during specific tasks. We evaluate several alternative\nstrategies for integrating such human attention data into machine learning (ML)\nalgorithms, using a sentiment analysis task (review data from Yelp) and a\npersonality-type classification task (data from myPersonality). The\nbest-performing HuMAL strategy significantly enhances the task performance of\nfine-tuned transformer models (BERT, as well as GPT-2 and XLNET), and the\nbenefit is particularly pronounced under challenging conditions of imbalanced\nor sparse labeled data. This research contributes to a deeper understanding of\nstrategies for integrating human attention into ML models and highlights the\npotential of leveraging human cognition to augment ML in real-world\napplications.", "published": "2025-02-04 20:44:38", "link": "http://arxiv.org/abs/2502.06811v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study\n  on Vaccination Debate on Social Media", "abstract": "Nowadays, social media is pivotal in shaping public discourse, especially on\npolarizing issues like vaccination, where diverse moral perspectives influence\nindividual opinions. In NLP, data scarcity and complexity of psycholinguistic\ntasks, such as identifying morality frames, make relying solely on human\nannotators costly, time-consuming, and prone to inconsistency due to cognitive\nload. To address these issues, we leverage large language models (LLMs), which\nare adept at adapting new tasks through few-shot learning, utilizing a handful\nof in-context examples coupled with explanations that connect examples to task\nprinciples. Our research explores LLMs' potential to assist human annotators in\nidentifying morality frames within vaccination debates on social media. We\nemploy a two-step process: generating concepts and explanations with LLMs,\nfollowed by human evaluation using a \"think-aloud\" tool. Our study shows that\nintegrating LLMs into the annotation process enhances accuracy, reduces task\ndifficulty, lowers cognitive load, suggesting a promising avenue for human-AI\ncollaboration in complex psycholinguistic tasks.", "published": "2025-02-04 04:10:23", "link": "http://arxiv.org/abs/2502.01991v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and\n  Maliseet", "abstract": "We present lightweight flow matching multilingual text-to-speech (TTS)\nsystems for Ojibwe, Mi'kmaq, and Maliseet, three Indigenous languages in North\nAmerica. Our results show that training a multilingual TTS model on three\ntypologically similar languages can improve the performance over monolingual\nmodels, especially when data are scarce. Attention-free architectures are\nhighly competitive with self-attention architecture with higher memory\nefficiency. Our research not only advances technical development for the\nrevitalization of low-resource languages but also highlights the cultural gap\nin human evaluation protocols, calling for a more community-centered approach\nto human evaluation.", "published": "2025-02-04 20:36:55", "link": "http://arxiv.org/abs/2502.02703v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance", "abstract": "Existing methods fail to effectively steer Large Language Models (LLMs)\nbetween textual reasoning and code generation, leaving symbolic computing\ncapabilities underutilized. We introduce CodeSteer, an effective method for\nguiding LLM code/text generation. We construct a comprehensive benchmark\nSymBench comprising 37 symbolic tasks with adjustable complexity and also\nsynthesize datasets of 12k multi-round guidance/generation trajectories and\n5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly\ndesigned multi-round supervised fine-tuning (SFT) and direct preference\noptimization (DPO). The resulting model, CodeSteerLLM, augmented with the\nproposed symbolic and self-answer checkers, effectively guides the code/text\ngeneration of larger models. Augmenting GPT-4o with CodeSteer raises its\naverage performance score from 53.3 to 86.4, even outperforming the existing\nbest LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all\n37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates\nsuperior generalizability, providing an average 41.8 performance boost on\nClaude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic\ncomputing to maintain strong performance on highly complex tasks. Models,\nDatasets, and Codes are available at\nhttps://github.com/yongchao98/CodeSteer-v1.0.", "published": "2025-02-04 15:53:59", "link": "http://arxiv.org/abs/2502.04350v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SC", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM\n  Primitives", "abstract": "Decomposing hard problems into subproblems often makes them easier and more\nefficient to solve. With large language models (LLMs) crossing critical\nreliability thresholds for a growing slate of capabilities, there is an\nincreasing effort to decompose systems into sets of LLM-based agents, each of\nwhom can be delegated sub-tasks. However, this decomposition (even when\nautomated) is often intuitive, e.g., based on how a human might assign roles to\nmembers of a human team. How close are these role decompositions to optimal?\nThis position paper argues that asymptotic analysis with LLM primitives is\nneeded to reason about the efficiency of such decomposed systems, and that\ninsights from such analysis will unlock opportunities for scaling them. By\ntreating the LLM forward pass as the atomic unit of computational cost, one can\nseparate out the (often opaque) inner workings of a particular LLM from the\ninherent efficiency of how a set of LLMs are orchestrated to solve hard\nproblems. In other words, if we want to scale the deployment of LLMs to the\nlimit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM\nprimitives should be used to reason about and develop more powerful\ndecompositions of large problems into LLM agents.", "published": "2025-02-04 20:47:43", "link": "http://arxiv.org/abs/2502.04358v1", "categories": ["cs.CL", "cs.AI", "cs.CC", "cs.LG", "cs.NE"], "primary_category": "cs.CL"}
{"title": "Self-Supervised Convolutional Audio Models are Flexible Acoustic Feature\n  Learners: A Domain Specificity and Transfer-Learning Study", "abstract": "Self-supervised learning (SSL) algorithms have emerged as powerful tools that\ncan leverage large quantities of unlabeled audio data to pre-train robust\nrepresentations that support strong performance on diverse downstream tasks. Up\nto now these have mostly been developed separately for speech and non-speech\napplications. Here, we explored the domain specificity of a convolutional\nmodel's pre-training data relative to different downstream speech and\nnon-speech tasks using a self-supervised pre-training approach (BYOL-A). We\nfound that these pre-trained models (regardless of whether they were\npre-trained on speech data, non-speech data or both) enabled good performance\non nearly all downstream tasks, beating or nearly matching the performance of\npopular domain-specific models. Only small domain-specificity advantages were\nobserved between the different pre-training datasets. The popular\ndomain-specific models used as baselines performed very well in their target\ndomains, but generally faltered outside of them. Together, these results\ndemonstrate that SSL methods can be a powerful way to learn flexible\nrepresentations for domain specific data without labels. These models can be a\npowerful resource for later transfer learning, fine-tuning or data exploration\napplications when the downstream data are similar, but also perhaps when there\nmay be a domain mismatch.", "published": "2025-02-04 14:50:12", "link": "http://arxiv.org/abs/2502.02366v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with\n  Complex Spectrum Modeling", "abstract": "Neural audio codecs have been widely adopted in audio-generative tasks\nbecause their compact and discrete representations are suitable for both\nlarge-language-model-style and regression-based generative models. However,\nmost neural codecs struggle to model out-of-domain audio, resulting in error\npropagations to downstream generative tasks. In this paper, we first argue that\ninformation loss from codec compression degrades out-of-domain robustness.\nThen, we propose full-band 48~kHz ComplexDec with complex spectral input and\noutput to ease the information loss while adopting the same 24~kbps bitrate as\nthe baseline AuidoDec and ScoreDec. Objective and subjective evaluations\ndemonstrate the out-of-domain robustness of ComplexDec trained using only the\n30-hour VCTK corpus.", "published": "2025-02-04 05:16:15", "link": "http://arxiv.org/abs/2502.02019v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Investigation of perceptual music similarity focusing on each\n  instrumental part", "abstract": "This paper presents an investigation of perceptual similarity between music\ntracks focusing on each individual instrumental part based on a large-scale\nlistening test towards developing an instrumental-part-based music retrieval.\nIn the listening test, 586 subjects evaluate the perceptual similarity of the\naudio tracks through an ABX test. We use the music tracks and their stems in\nthe test set of the slakh2100 dataset. The perceptual similarity is evaluated\nbased on four perspectives: timbre, rhythm, melody, and overall. We have\nanalyzed the results of the listening test and have found that 1) perceptual\nmusic similarity varies depending on which instrumental part is focused on\nwithin each track; 2) rhythm and melody tend to have a larger impact on the\nperceptual music similarity than timbre except for the melody of drums; and 3)\nthe previously proposed music similarity features tend to capture the\nperceptual similarity on timbre mainly.", "published": "2025-02-04 09:11:05", "link": "http://arxiv.org/abs/2502.02138v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound Judgment: Properties of Consequential Sounds Affecting\n  Human-Perception of Robots", "abstract": "Positive human-perception of robots is critical to achieving sustained use of\nrobots in shared environments. One key factor affecting human-perception of\nrobots are their sounds, especially the consequential sounds which robots (as\nmachines) must produce as they operate. This paper explores qualitative\nresponses from 182 participants to gain insight into human-perception of robot\nconsequential sounds. Participants viewed videos of different robots performing\ntheir typical movements, and responded to an online survey regarding their\nperceptions of robots and the sounds they produce. Topic analysis was used to\nidentify common properties of robot consequential sounds that participants\nexpressed liking, disliking, wanting or wanting to avoid being produced by\nrobots. Alongside expected reports of disliking high pitched and loud sounds,\nmany participants preferred informative and audible sounds (over no sound) to\nprovide predictability of purpose and trajectory of the robot. Rhythmic sounds\nwere preferred over acute or continuous sounds, and many participants wanted\nmore natural sounds (such as wind or cat purrs) in-place of machine-like noise.\nThe results presented in this paper support future research on methods to\nimprove consequential sounds produced by robots by highlighting features of\nsounds that cause negative perceptions, and providing insights into sound\nprofile changes for improvement of human-perception of robots, thus enhancing\nhuman robot interaction.", "published": "2025-02-04 06:33:43", "link": "http://arxiv.org/abs/2502.02051v1", "categories": ["cs.RO", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.RO"}
{"title": "Pruning-aware Loss Functions for STOI-Optimized Pruned Recurrent\n  Autoencoders for the Compression of the Stimulation Patterns of Cochlear\n  Implants at Zero Delay", "abstract": "Cochlear implants (CIs) are surgically implanted hearing devices, which allow\nto restore a sense of hearing in people suffering from profound hearing loss.\nWireless streaming of audio from external devices to CI signal processors has\nbecome common place. Specialized compression based on the stimulation patterns\nof a CI by deep recurrent autoencoders can decrease the power consumption in\nsuch a wireless streaming application through bit-rate reduction at zero\nlatency.\n  While previous research achieved considerable bit-rate reductions, model\nsizes were ignored, which can be of crucial importance in hearing-aids due to\ntheir limited computational resources. This work investigates maximizing\nobjective speech intelligibility of the coded stimulation patterns of deep\nrecurrent autoencoders while minimizing model size. For this purpose, a\npruning-aware loss is proposed, which captures the impact of pruning during\ntraining. This training with a pruning-aware loss is compared to conventional\nmagnitude-informed pruning and is found to yield considerable improvements in\nobjective intelligibility, especially at higher pruning rates. After\nfine-tuning, little to no degradation of objective intelligibility is observed\nup to a pruning rate of about 55\\,\\%. The proposed pruning-aware loss yields\nsubstantial gains in objective speech intelligibility scores after pruning\ncompared to the magnitude-informed baseline for pruning rates above 45\\,\\%.", "published": "2025-02-04 15:44:15", "link": "http://arxiv.org/abs/2502.02424v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Dementia Classification Using Acoustic Speech and Feature Selection", "abstract": "Dementia is a general term for a group of syndromes that affect cognitive\nfunctions such as memory, thinking, reasoning, and the ability to perform daily\ntasks. The number of dementia patients is increasing as the population ages,\nand it is estimated that over 10 million people develop dementia each year.\nDementia progresses gradually, and the sooner a patient receives help and\nsupport, the better their chances of maintaining their functional abilities.\nFor this reason, early diagnosis of dementia is important. In recent years,\nmachine learning models based on naturally spoken language have been developed\nfor the early diagnosis of dementia. These methods have proven to be\nuser-friendly, cost-effective, scalable, and capable of providing extremely\nfast diagnoses. This study utilizes the well-known ADReSS challenge dataset for\nclassifying healthy controls and Alzheimer's patients. The dataset contains\nspeech recordings from a picture description task featuring a kitchen scene,\ncollected from both healthy controls and dementia patients. Unlike most\nstudies, this research does not segment the audio recordings into active speech\nsegments; instead, acoustic features are extracted from entire recordings. The\nstudy employs Ridge linear regression, Extreme Minimal Learning Machine, and\nLinear Support Vector Machine machine learning models to compute feature\nimportance scores based on model outputs. The Ridge model performed best in\nLeave-One-Subject-Out cross-validation, achieving a classification accuracy of\n87.8%. The EMLM model, proved to be effective in both cross-validation and the\nclassification of a separate test dataset, with accuracies of 85.3% and 79.2%,\nrespectively. The study's results rank among the top compared to other studies\nusing the same dataset and acoustic feature extraction for dementia diagnosis.", "published": "2025-02-04 14:50:19", "link": "http://arxiv.org/abs/2502.03484v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
