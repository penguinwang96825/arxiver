{"title": "When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks", "abstract": "Large language models (LLMs) have shown remarkable success in language\nmodelling due to scaling laws found in model size and the hidden dimension of\nthe model's text representation. Yet, we demonstrate that compressed\nrepresentations of text can yield better performance in LLM-based regression\ntasks. In this paper, we compare the relative performance of embedding\ncompression in three different signal-to-noise contexts: financial return\nprediction, writing quality assessment and review scoring. Our results show\nthat compressing embeddings, in a minimally supervised manner using an\nautoencoder's hidden representation, can mitigate overfitting and improve\nperformance on noisy tasks, such as financial return prediction; but that\ncompression reduces performance on tasks that have high causal dependencies\nbetween the input and target data. Our results suggest that the success of\ninterpretable compressed representations such as sentiment may be due to a\nregularising effect.", "published": "2025-02-04 10:23:11", "link": "http://arxiv.org/abs/2502.02199v1", "categories": ["cs.CL", "cs.CE", "cs.LG", "q-fin.CP"], "primary_category": "cs.CL"}
{"title": "Liquidity provision of utility indifference type in decentralized exchanges", "abstract": "We present a mathematical formulation of liquidity provision in decentralized\nexchanges. We focus on constant function market makers of utility indifference\ntype, which include constant product market makers with concentrated liquidity\nas a special case. First, we examine no-arbitrage conditions for a liquidity\npool and compute an optimal arbitrage strategy when there is an external liquid\nmarket. Second, we show that liquidity provision suffers from impermanent loss\nunless a transaction fee is levied under the general framework with\nconcentrated liquidity. Third, we establish the well-definedness of\narbitrage-free reserve processes of a liquidity pool in continuous-time and\nshow that there is no loss-versus-rebalancing under a nonzero fee if the\nexternal market price is continuous. We then argue that liquidity provision by\nmultiple liquidity providers can be understood as liquidity provision by a\nrepresentative liquidity provider, meaning that the analysis boils down to that\nfor a single liquidity provider. Last, but not least, we give an answer to the\nfundamental question in which sense the very construction of constant function\nmarket makers with concentrated liquidity in the popular platform Uniswap v3 is\noptimal.", "published": "2025-02-04 02:06:28", "link": "http://arxiv.org/abs/2502.01931v1", "categories": ["q-fin.TR", "q-fin.MF", "91G15"], "primary_category": "q-fin.TR"}
{"title": "FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data", "abstract": "Large language models (LLMs) excel at generating human-like responses but\noften struggle with interactive tasks that require access to real-time\ninformation. This limitation poses challenges in finance, where models must\naccess up-to-date information, such as recent news or price movements, to\nsupport decision-making. To address this, we introduce Financial Agent, a\nknowledge-grounding approach for LLMs to handle financial queries using\nreal-time text and tabular data. Our contributions are threefold: First, we\ndevelop a Financial Context Dataset of over 50,000 financial queries paired\nwith the required context. Second, we train FinBloom 7B, a custom 7 billion\nparameter LLM, on 14 million financial news articles from Reuters and Deutsche\nPresse-Agentur, alongside 12 million Securities and Exchange Commission (SEC)\nfilings. Third, we fine-tune FinBloom 7B using the Financial Context Dataset to\nserve as a Financial Agent. This agent generates relevant financial context,\nenabling efficient real-time data retrieval to answer user queries. By reducing\nlatency and eliminating the need for users to manually provide accurate data,\nour approach significantly enhances the capability of LLMs to handle dynamic\nfinancial tasks. Our proposed approach makes real-time financial decisions,\nalgorithmic trading and other related tasks streamlined, and is valuable in\ncontexts with high-velocity data flows.", "published": "2025-02-04 06:51:34", "link": "http://arxiv.org/abs/2502.18471v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "q-fin.ST"], "primary_category": "cs.IR"}
{"title": "FinRLlama: A Solution to LLM-Engineered Signals Challenge at FinRL Contest 2024", "abstract": "In response to Task II of the FinRL Challenge at ACM ICAIF 2024, this study\nproposes a novel prompt framework for fine-tuning large language models (LLM)\nwith Reinforcement Learning from Market Feedback (RLMF). Our framework\nincorporates market-specific features and short-term price dynamics to generate\nmore precise trading signals. Traditional LLMs, while competent in sentiment\nanalysis, lack contextual alignment for financial market applications. To\nbridge this gap, we fine-tune the LLaMA-3.2-3B-Instruct model using a custom\nRLMF prompt design that integrates historical market data and reward-based\nfeedback. Our evaluation shows that this RLMF-tuned framework outperforms\nbaseline methods in signal consistency and achieving tighter trading outcomes;\nawarded as winner of Task II. You can find the code for this project on GitHub.", "published": "2025-02-04 04:11:09", "link": "http://arxiv.org/abs/2502.01992v1", "categories": ["q-fin.TR", "cs.LG"], "primary_category": "q-fin.TR"}
