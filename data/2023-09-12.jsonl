{"title": "Do PLMs Know and Understand Ontological Knowledge?", "abstract": "Ontological knowledge, which comprises classes and properties and their\nrelationships, is integral to world knowledge. It is significant to explore\nwhether Pretrained Language Models (PLMs) know and understand such knowledge.\nHowever, existing PLM-probing studies focus mainly on factual knowledge,\nlacking a systematic probing of ontological knowledge. In this paper, we focus\non probing whether PLMs store ontological knowledge and have a semantic\nunderstanding of the knowledge rather than rote memorization of the surface\nform. To probe whether PLMs know ontological knowledge, we investigate how well\nPLMs memorize: (1) types of entities; (2) hierarchical relationships among\nclasses and properties, e.g., Person is a subclass of Animal and Member of\nSports Team is a subproperty of Member of ; (3) domain and range constraints of\nproperties, e.g., the subject of Member of Sports Team should be a Person and\nthe object should be a Sports Team. To further probe whether PLMs truly\nunderstand ontological knowledge beyond memorization, we comprehensively study\nwhether they can reliably perform logical reasoning with given knowledge\naccording to ontological entailment rules. Our probing results show that PLMs\ncan memorize certain ontological knowledge and utilize implicit knowledge in\nreasoning. However, both the memorizing and reasoning performances are less\nthan perfect, indicating incomplete knowledge and understanding.", "published": "2023-09-12 03:20:50", "link": "http://arxiv.org/abs/2309.05936v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Balanced and Explainable Social Media Analysis for Public Health with\n  Large Language Models", "abstract": "As social media becomes increasingly popular, more and more public health\nactivities emerge, which is worth noting for pandemic monitoring and government\ndecision-making. Current techniques for public health analysis involve popular\nmodels such as BERT and large language models (LLMs). Although recent progress\nin LLMs has shown a strong ability to comprehend knowledge by being fine-tuned\non specific domain datasets, the costs of training an in-domain LLM for every\nspecific public health task are especially expensive. Furthermore, such kinds\nof in-domain datasets from social media are generally highly imbalanced, which\nwill hinder the efficiency of LLMs tuning. To tackle these challenges, the data\nimbalance issue can be overcome by sophisticated data augmentation methods for\nsocial media datasets. In addition, the ability of the LLMs can be effectively\nutilised by prompting the model properly. In light of the above discussion, in\nthis paper, a novel ALEX framework is proposed for social media analysis on\npublic health. Specifically, an augmentation pipeline is developed to resolve\nthe data imbalance issue. Furthermore, an LLMs explanation mechanism is\nproposed by prompting an LLM with the predicted results from BERT models.\nExtensive experiments conducted on three tasks at the Social Media Mining for\nHealth 2023 (SMM4H) competition with the first ranking in two tasks demonstrate\nthe superior performance of the proposed ALEX method. Our code has been\nreleased in https://github.com/YanJiangJerry/ALEX.", "published": "2023-09-12 04:15:34", "link": "http://arxiv.org/abs/2309.05951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Content Reduction, Surprisal and Information Density Estimation for Long\n  Documents", "abstract": "Many computational linguistic methods have been proposed to study the\ninformation content of languages. We consider two interesting research\nquestions: 1) how is information distributed over long documents, and 2) how\ndoes content reduction, such as token selection and text summarization, affect\nthe information density in long documents. We present four criteria for\ninformation density estimation for long documents, including surprisal,\nentropy, uniform information density, and lexical density. Among those\ncriteria, the first three adopt the measures from information theory. We\npropose an attention-based word selection method for clinical notes and study\nmachine summarization for multiple-domain documents. Our findings reveal the\nsystematic difference in information density of long text in various domains.\nEmpirical results on automated medical coding from long clinical notes show the\neffectiveness of the attention-based word selection method.", "published": "2023-09-12 07:08:22", "link": "http://arxiv.org/abs/2309.06009v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation\n  Suite for Large Language Models", "abstract": "The rapid development of Large Language Models (LLMs) and the emergence of\nnovel abilities with scale have necessitated the construction of holistic,\ndiverse and challenging benchmarks such as HELM and BIG-bench. However, at the\nmoment, most of these benchmarks focus only on performance in English and\nevaluations that include Southeast Asian (SEA) languages are few in number. We\ntherefore propose BHASA, a holistic linguistic and cultural evaluation suite\nfor LLMs in SEA languages. It comprises three components: (1) a NLP benchmark\ncovering eight tasks across Natural Language Understanding (NLU), Generation\n(NLG) and Reasoning (NLR) tasks, (2) LINDSEA, a linguistic diagnostic toolkit\nthat spans the gamut of linguistic phenomena including syntax, semantics and\npragmatics, and (3) a cultural diagnostics dataset that probes for both\ncultural representation and sensitivity. For this preliminary effort, we\nimplement the NLP benchmark only for Indonesian, Vietnamese, Thai and Tamil,\nand we only include Indonesian and Tamil for LINDSEA and the cultural\ndiagnostics dataset. As GPT-4 is purportedly one of the best-performing\nmultilingual LLMs at the moment, we use it as a yardstick to gauge the\ncapabilities of LLMs in the context of SEA languages. Our initial experiments\non GPT-4 with BHASA find it lacking in various aspects of linguistic\ncapabilities, cultural representation and sensitivity in the targeted SEA\nlanguages. BHASA is a work in progress and will continue to be improved and\nexpanded in the future. The repository for this paper can be found at:\nhttps://github.com/aisingapore/BHASA", "published": "2023-09-12 09:31:25", "link": "http://arxiv.org/abs/2309.06085v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of GUA-SPA at IberLEF 2023: Guarani-Spanish Code Switching\n  Analysis", "abstract": "We present the first shared task for detecting and analyzing code-switching\nin Guarani and Spanish, GUA-SPA at IberLEF 2023. The challenge consisted of\nthree tasks: identifying the language of a token, NER, and a novel task of\nclassifying the way a Spanish span is used in the code-switched context. We\nannotated a corpus of 1500 texts extracted from news articles and tweets,\naround 25 thousand tokens, with the information for the tasks. Three teams took\npart in the evaluation phase, obtaining in general good results for Task 1, and\nmore mixed results for Tasks 2 and 3.", "published": "2023-09-12 12:18:18", "link": "http://arxiv.org/abs/2309.06163v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Re-Reading Improves Reasoning in Large Language Models", "abstract": "To enhance the reasoning capabilities of off-the-shelf Large Language Models\n(LLMs), we introduce a simple, yet general and effective prompting method, Re2,\ni.e., \\textbf{Re}-\\textbf{Re}ading the question as input. Unlike most\nthought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim\nto elicit the reasoning process in the output, Re2 shifts the focus to the\ninput by processing questions twice, thereby enhancing the understanding\nprocess. Consequently, Re2 demonstrates strong generality and compatibility\nwith most thought-eliciting prompting methods, including CoT. Crucially, Re2\nfacilitates a \"bidirectional\" encoding in unidirectional decoder-only LLMs\nbecause the first pass could provide global information for the second pass. We\nbegin with a preliminary empirical study as the foundation of Re2, illustrating\nits potential to enable \"bidirectional\" attention mechanisms. We then evaluate\nRe2 on extensive reasoning benchmarks across 14 datasets, spanning 112\nexperiments, to validate its effectiveness and generality. Our findings\nindicate that, with the exception of a few scenarios on vanilla ChatGPT, Re2\nconsistently enhances the reasoning performance of LLMs through a simple\nre-reading strategy. Further analyses reveal Re2's adaptability, showing how it\ncan be effectively integrated with different LLMs, thought-eliciting prompting,\nand ensemble strategies. Our code is available at\n\\url{https://github.com/Tebmer/Rereading-LLM-Reasoning/}", "published": "2023-09-12 14:36:23", "link": "http://arxiv.org/abs/2309.06275v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning to Predict Concept Ordering for Common Sense Generation", "abstract": "Prior work has shown that the ordering in which concepts are shown to a\ncommonsense generator plays an important role, affecting the quality of the\ngenerated sentence. However, it remains a challenge to determine the optimal\nordering of a given set of concepts such that a natural sentence covering all\nthe concepts could be generated from a pretrained generator. To understand the\nrelationship between the ordering of the input concepts and the quality of the\ngenerated sentences, we conduct a systematic study considering multiple\nlanguage models (LMs) and concept ordering strategies. We find that BART-large\nmodel consistently outperforms all other LMs considered in this study when\nfine-tuned using the ordering of concepts as they appear in CommonGen training\ndata as measured using multiple evaluation metrics. Moreover, the larger\nGPT3-based large language models (LLMs) variants do not necessarily outperform\nmuch smaller LMs on this task, even when fine-tuned on task-specific training\ndata. Interestingly, human annotators significantly reorder input concept sets\nwhen manually writing sentences covering those concepts, and this ordering\nprovides the best sentence generations independently of the LM used for the\ngeneration, outperforming a probabilistic concept ordering baseline", "published": "2023-09-12 16:27:18", "link": "http://arxiv.org/abs/2309.06363v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cited Text Spans for Citation Text Generation", "abstract": "An automatic citation generation system aims to concisely and accurately\ndescribe the relationship between two scientific articles. To do so, such a\nsystem must ground its outputs to the content of the cited paper to avoid\nnon-factual hallucinations. Due to the length of scientific documents, existing\nabstractive approaches have conditioned only on cited paper abstracts. We\ndemonstrate empirically that the abstract is not always the most appropriate\ninput for citation generation and that models trained in this way learn to\nhallucinate. We propose to condition instead on the cited text span (CTS) as an\nalternative to the abstract. Because manual CTS annotation is extremely time-\nand labor-intensive, we experiment with distant labeling of candidate CTS\nsentences, achieving sufficiently strong performance to substitute for\nexpensive human annotations in model training, and we propose a\nhuman-in-the-loop, keyword-based CTS retrieval approach that makes generating\ncitation texts grounded in the full text of cited papers both promising and\npractical.", "published": "2023-09-12 16:28:36", "link": "http://arxiv.org/abs/2309.06365v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Widely Interpretable Semantic Representation: Frameless Meaning\n  Representation for Broader Applicability", "abstract": "This paper presents a novel semantic representation, WISeR, that overcomes\nchallenges for Abstract Meaning Representation (AMR). Despite its strengths,\nAMR is not easily applied to languages or domains without predefined semantic\nframes, and its use of numbered arguments results in semantic role labels,\nwhich are not directly interpretable and are semantically overloaded for\nparsers. We examine the numbered arguments of predicates in AMR and convert\nthem to thematic roles that do not require reference to semantic frames. We\ncreate a new corpus of 1K English dialogue sentences annotated in both WISeR\nand AMR. WISeR shows stronger inter-annotator agreement for beginner and\nexperienced annotators, with beginners becoming proficient in WISeR annotation\nmore quickly. Finally, we train a state-of-the-art parser on the AMR 3.0 corpus\nand a WISeR corpus converted from AMR 3.0. The parser is evaluated on these\ncorpora and our dialogue corpus. The WISeR model exhibits higher accuracy than\nits AMR counterpart across the board, demonstrating that WISeR is easier for\nparsers to learn.", "published": "2023-09-12 17:44:40", "link": "http://arxiv.org/abs/2309.06460v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Leveraging Large Language Models for Automated Dialogue Analysis", "abstract": "Developing high-performing dialogue systems benefits from the automatic\nidentification of undesirable behaviors in system responses. However, detecting\nsuch behaviors remains challenging, as it draws on a breadth of general\nknowledge and understanding of conversational practices. Although recent\nresearch has focused on building specialized classifiers for detecting specific\ndialogue behaviors, the behavior coverage is still incomplete and there is a\nlack of testing on real-world human-bot interactions. This paper investigates\nthe ability of a state-of-the-art large language model (LLM), ChatGPT-3.5, to\nperform dialogue behavior detection for nine categories in real human-bot\ndialogues. We aim to assess whether ChatGPT can match specialized models and\napproximate human performance, thereby reducing the cost of behavior detection\ntasks. Our findings reveal that neither specialized models nor ChatGPT have yet\nachieved satisfactory results for this task, falling short of human\nperformance. Nevertheless, ChatGPT shows promising potential and often\noutperforms specialized detection models. We conclude with an in-depth\nexamination of the prevalent shortcomings of ChatGPT, offering guidance for\nfuture research to enhance LLM capabilities.", "published": "2023-09-12 18:03:55", "link": "http://arxiv.org/abs/2309.06490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Overview of Memotion 3: Sentiment and Emotion Analysis of Codemixed\n  Hinglish Memes", "abstract": "Analyzing memes on the internet has emerged as a crucial endeavor due to the\nimpact this multi-modal form of content wields in shaping online discourse.\nMemes have become a powerful tool for expressing emotions and sentiments,\npossibly even spreading hate and misinformation, through humor and sarcasm. In\nthis paper, we present the overview of the Memotion 3 shared task, as part of\nthe DeFactify 2 workshop at AAAI-23. The task released an annotated dataset of\nHindi-English code-mixed memes based on their Sentiment (Task A), Emotion (Task\nB), and Emotion intensity (Task C). Each of these is defined as an individual\ntask and the participants are ranked separately for each task. Over 50 teams\nregistered for the shared task and 5 made final submissions to the test set of\nthe Memotion 3 dataset. CLIP, BERT modifications, ViT etc. were the most\npopular models among the participants along with approaches such as\nStudent-Teacher model, Fusion, and Ensembling. The best final F1 score for Task\nA is 34.41, Task B is 79.77 and Task C is 59.82.", "published": "2023-09-12 18:47:29", "link": "http://arxiv.org/abs/2309.06517v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Encoders Lack Knowledge: Leveraging Generative LLMs for\n  Domain-Specific Semantic Textual Similarity", "abstract": "Amidst the sharp rise in the evaluation of large language models (LLMs) on\nvarious tasks, we find that semantic textual similarity (STS) has been\nunder-explored. In this study, we show that STS can be cast as a text\ngeneration problem while maintaining strong performance on multiple STS\nbenchmarks. Additionally, we show generative LLMs significantly outperform\nexisting encoder-based STS models when characterizing the semantic similarity\nbetween two texts with complex semantic relationships dependent on world\nknowledge. We validate this claim by evaluating both generative LLMs and\nexisting encoder-based STS models on three newly collected STS challenge sets\nwhich require world knowledge in the domains of Health, Politics, and Sports.\nAll newly collected data is sourced from social media content posted after May\n2023 to ensure the performance of closed-source models like ChatGPT cannot be\ncredited to memorization. Our results show that, on average, generative LLMs\noutperform the best encoder-only baselines by an average of 22.3% on STS tasks\nrequiring world knowledge. Our results suggest generative language models with\nSTS-specific prompting strategies achieve state-of-the-art performance in\ncomplex, domain-specific STS tasks.", "published": "2023-09-12 19:32:45", "link": "http://arxiv.org/abs/2309.06541v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance of ChatGPT-3.5 and GPT-4 on the United States Medical\n  Licensing Examination With and Without Distractions", "abstract": "As Large Language Models (LLMs) are predictive models building their response\nbased on the words in the prompts, there is a risk that small talk and\nirrelevant information may alter the response and the suggestion given.\nTherefore, this study aims to investigate the impact of medical data mixed with\nsmall talk on the accuracy of medical advice provided by ChatGPT. USMLE step 3\nquestions were used as a model for relevant medical data. We use both multiple\nchoice and open ended questions. We gathered small talk sentences from human\nparticipants using the Mechanical Turk platform. Both sets of USLME questions\nwere arranged in a pattern where each sentence from the original questions was\nfollowed by a small talk sentence. ChatGPT 3.5 and 4 were asked to answer both\nsets of questions with and without the small talk sentences. A board-certified\nphysician analyzed the answers by ChatGPT and compared them to the formal\ncorrect answer. The analysis results demonstrate that the ability of\nChatGPT-3.5 to answer correctly was impaired when small talk was added to\nmedical data for multiple-choice questions (72.1\\% vs. 68.9\\%) and open\nquestions (61.5\\% vs. 44.3\\%; p=0.01), respectively. In contrast, small talk\nphrases did not impair ChatGPT-4 ability in both types of questions (83.6\\% and\n66.2\\%, respectively). According to these results, ChatGPT-4 seems more\naccurate than the earlier 3.5 version, and it appears that small talk does not\nimpair its capability to provide medical recommendations. Our results are an\nimportant first step in understanding the potential and limitations of\nutilizing ChatGPT and other LLMs for physician-patient interactions, which\ninclude casual conversations.", "published": "2023-09-12 05:54:45", "link": "http://arxiv.org/abs/2309.08625v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Robustness of Neural Inverse Text Normalization via\n  Data-Augmentation, Semi-Supervised Learning, and Post-Aligning Method", "abstract": "Inverse text normalization (ITN) is crucial for converting spoken-form into\nwritten-form, especially in the context of automatic speech recognition (ASR).\nWhile most downstream tasks of ASR rely on written-form, ASR systems often\noutput spoken-form, highlighting the necessity for robust ITN in product-level\nASR-based applications. Although neural ITN methods have shown promise, they\nstill encounter performance challenges, particularly when dealing with\nASR-generated spoken text. These challenges arise from the out-of-domain\nproblem between training data and ASR-generated text. To address this, we\npropose a direct training approach that utilizes ASR-generated written or\nspoken text, with pairs augmented through ASR linguistic context emulation and\na semi-supervised learning method enhanced by a large language model,\nrespectively. Additionally, we introduce a post-aligning method to manage\nunpredictable errors, thereby enhancing the reliability of ITN. Our experiments\nshow that our proposed methods remarkably improved ITN performance in various\nASR scenarios.", "published": "2023-09-12 06:05:57", "link": "http://arxiv.org/abs/2309.08626v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stochastic LLMs do not Understand Language: Towards Symbolic,\n  Explainable and Ontologically Based LLMs", "abstract": "In our opinion the exuberance surrounding the relative success of data-driven\nlarge language models (LLMs) is slightly misguided and for several reasons (i)\nLLMs cannot be relied upon for factual information since for LLMs all ingested\ntext (factual or non-factual) was created equal; (ii) due to their subsymbolic\nna-ture, whatever 'knowledge' these models acquire about language will always\nbe buried in billions of microfeatures (weights), none of which is meaningful\non its own; and (iii) LLMs will often fail to make the correct inferences in\nseveral linguistic contexts (e.g., nominal compounds, copredication, quantifier\nscope ambi-guities, intensional contexts. Since we believe the relative success\nof data-driven large language models (LLMs) is not a reflection on the symbolic\nvs. subsymbol-ic debate but a reflection on applying the successful strategy of\na bottom-up reverse engineering of language at scale, we suggest in this paper\napplying the effective bottom-up strategy in a symbolic setting resulting in\nsymbolic, explainable, and ontologically grounded language models.", "published": "2023-09-12 02:14:05", "link": "http://arxiv.org/abs/2309.05918v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Answering Subjective Induction Questions on Products by Summarizing\n  Multi-sources Multi-viewpoints Knowledge", "abstract": "This paper proposes a new task in the field of Answering Subjective Induction\nQuestion on Products (SUBJPQA). The answer to this kind of question is\nnon-unique, but can be interpreted from many perspectives. For example, the\nanswer to 'whether the phone is heavy' has a variety of different viewpoints. A\nsatisfied answer should be able to summarize these subjective opinions from\nmultiple sources and provide objective knowledge, such as the weight of a\nphone. That is quite different from the traditional QA task, in which the\nanswer to a factoid question is unique and can be found from a single data\nsource. To address this new task, we propose a three-steps method. We first\nretrieve all answer-related clues from multiple knowledge sources on facts and\nopinions. The implicit commonsense facts are also collected to supplement the\nnecessary but missing contexts. We then capture their relevance with the\nquestions by interactive attention. Next, we design a reinforcement-based\nsummarizer to aggregate all these knowledgeable clues. Based on a\ntemplate-controlled decoder, we can output a comprehensive and\nmulti-perspective answer. Due to the lack of a relevant evaluated benchmark set\nfor the new task, we construct a large-scale dataset, named SupQA, consisting\nof 48,352 samples across 15 product domains. Evaluation results show the\neffectiveness of our approach.", "published": "2023-09-12 03:27:08", "link": "http://arxiv.org/abs/2309.05938v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Circuit Breaking: Removing Model Behaviors with Targeted Ablation", "abstract": "Language models often exhibit behaviors that improve performance on a\npre-training objective but harm performance on downstream tasks. We propose a\nnovel approach to removing undesirable behaviors by ablating a small number of\ncausal pathways between model components, with the intention of disabling the\ncomputational circuit responsible for the bad behavior. Given a small dataset\nof inputs where the model behaves poorly, we learn to ablate a small number of\nimportant causal pathways. In the setting of reducing GPT-2 toxic language\ngeneration, we find ablating just 12 of the 11.6K causal edges mitigates toxic\ngeneration with minimal degradation of performance on other inputs.", "published": "2023-09-12 05:51:56", "link": "http://arxiv.org/abs/2309.05973v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAP-Gen: Retrieval-Augmented Patch Generation with CodeT5 for Automatic\n  Program Repair", "abstract": "Automatic program repair (APR) is crucial to reduce manual debugging efforts\nfor developers and improve software reliability. While conventional\nsearch-based techniques typically rely on heuristic rules or a redundancy\nassumption to mine fix patterns, recent years have witnessed the surge of deep\nlearning (DL) based approaches to automate the program repair process in a\ndata-driven manner. However, their performance is often limited by a fixed set\nof parameters to model the highly complex search space of APR. To ease such\nburden on the parametric models, in this work, we propose a novel\nRetrieval-Augmented Patch Generation framework (RAP-Gen) by explicitly\nleveraging relevant fix patterns retrieved from a codebase of previous bug-fix\npairs. Specifically, we build a hybrid patch retriever to account for both\nlexical and semantic matching based on the raw source code in a\nlanguage-agnostic manner, which does not rely on any code-specific features. In\naddition, we adapt a code-aware language model CodeT5 as our foundation model\nto facilitate both patch retrieval and generation tasks in a unified manner. We\nadopt a stage-wise approach where the patch retriever first retrieves a\nrelevant external bug-fix pair to augment the buggy input for the CodeT5 patch\ngenerator, which synthesizes a ranked list of repair patch candidates. Notably,\nRAP-Gen is a generic APR framework that can flexibly integrate different patch\nretrievers and generators to repair various types of bugs. We thoroughly\nevaluate RAP-Gen on three benchmarks in two programming languages, including\nthe TFix benchmark in JavaScript, and Code Refinement and Defects4J benchmarks\nin Java, where the bug localization information may or may not be provided.\nExperimental results show that RAP-Gen significantly outperforms previous\nstate-of-the-art approaches on all benchmarks, e.g., repairing 15 more bugs on\n818 Defects4J bugs.", "published": "2023-09-12 08:52:56", "link": "http://arxiv.org/abs/2309.06057v1", "categories": ["cs.SE", "cs.CL"], "primary_category": "cs.SE"}
{"title": "Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms:\n  Exploring Tuning Strategies", "abstract": "The cross-lingual transfer is a promising technique to solve tasks in\nless-resourced languages. In this empirical study, we compare two fine-tuning\napproaches combined with zero-shot and full-shot learning approaches for large\nlanguage models in a cross-lingual setting. As fine-tuning strategies, we\ncompare parameter-efficient adapter methods with fine-tuning of all parameters.\nAs cross-lingual transfer strategies, we compare the intermediate-training\n(\\textit{IT}) that uses each language sequentially and cross-lingual validation\n(\\textit{CLV}) that uses a target language already in the validation phase of\nfine-tuning. We assess the success of transfer and the extent of catastrophic\nforgetting in a source language due to cross-lingual transfer, i.e., how much\npreviously acquired knowledge is lost when we learn new information in a\ndifferent language. The results on two different classification problems, hate\nspeech detection and product reviews, each containing datasets in several\nlanguages, show that the \\textit{IT} cross-lingual strategy outperforms\n\\textit{CLV} for the target language. Our findings indicate that, in the\nmajority of cases, the \\textit{CLV} strategy demonstrates superior retention of\nknowledge in the base language (English) compared to the \\textit{IT} strategy,\nwhen evaluating catastrophic forgetting in multiple cross-lingual transfers.", "published": "2023-09-12 09:37:08", "link": "http://arxiv.org/abs/2309.06089v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Visual Taxonomy Expansion", "abstract": "Taxonomy expansion task is essential in organizing the ever-increasing volume\nof new concepts into existing taxonomies. Most existing methods focus\nexclusively on using textual semantics, leading to an inability to generalize\nto unseen terms and the \"Prototypical Hypernym Problem.\" In this paper, we\npropose Visual Taxonomy Expansion (VTE), introducing visual features into the\ntaxonomy expansion task. We propose a textual hypernymy learning task and a\nvisual prototype learning task to cluster textual and visual semantics. In\naddition to the tasks on respective modalities, we introduce a hyper-proto\nconstraint that integrates textual and visual semantics to produce fine-grained\nvisual semantics. Our method is evaluated on two datasets, where we obtain\ncompelling results. Specifically, on the Chinese taxonomy dataset, our method\nsignificantly improves accuracy by 8.75 %. Additionally, our approach performs\nbetter than ChatGPT on the Chinese taxonomy dataset.", "published": "2023-09-12 10:17:28", "link": "http://arxiv.org/abs/2309.06105v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Characterizing Latent Perspectives of Media Houses Towards Public\n  Figures", "abstract": "Media houses reporting on public figures, often come with their own biases\nstemming from their respective worldviews. A characterization of these\nunderlying patterns helps us in better understanding and interpreting news\nstories. For this, we need diverse or subjective summarizations, which may not\nbe amenable for classifying into predefined class labels. This work proposes a\nzero-shot approach for non-extractive or generative characterizations of person\nentities from a corpus using GPT-2. We use well-articulated articles from\nseveral well-known news media houses as a corpus to build a sound argument for\nthis approach. First, we fine-tune a GPT-2 pre-trained language model with a\ncorpus where specific person entities are characterized. Second, we further\nfine-tune this with demonstrations of person entity characterizations, created\nfrom a corpus of programmatically constructed characterizations. This twice\nfine-tuned model is primed with manual prompts consisting of entity names that\nwere not previously encountered in the second fine-tuning, to generate a simple\nsentence about the entity. The results were encouraging, when compared against\nactual characterizations from the corpus.", "published": "2023-09-12 10:27:39", "link": "http://arxiv.org/abs/2309.06112v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning\n  Strategies are not Better than Random Selection", "abstract": "Search methods based on Pretrained Language Models (PLM) have demonstrated\ngreat effectiveness gains compared to statistical and early neural ranking\nmodels. However, fine-tuning PLM-based rankers requires a great amount of\nannotated training data. Annotating data involves a large manual effort and\nthus is expensive, especially in domain specific tasks. In this paper we\ninvestigate fine-tuning PLM-based rankers under limited training data and\nbudget. We investigate two scenarios: fine-tuning a ranker from scratch, and\ndomain adaptation starting with a ranker already fine-tuned on general data,\nand continuing fine-tuning on a target dataset. We observe a great variability\nin effectiveness when fine-tuning on different randomly selected subsets of\ntraining data. This suggests that it is possible to achieve effectiveness gains\nby actively selecting a subset of the training data that has the most positive\neffect on the rankers. This way, it would be possible to fine-tune effective\nPLM rankers at a reduced annotation budget. To investigate this, we adapt\nexisting Active Learning (AL) strategies to the task of fine-tuning PLM rankers\nand investigate their effectiveness, also considering annotation and\ncomputational costs. Our extensive analysis shows that AL strategies do not\nsignificantly outperform random selection of training subsets in terms of\neffectiveness. We further find that gains provided by AL strategies come at the\nexpense of more assessments (thus higher annotation costs) and AL strategies\nunderperform random selection when comparing effectiveness given a fixed\nannotation cost. Our results highlight that ``optimal'' subsets of training\ndata that provide high effectiveness at low annotation cost do exist, but\ncurrent mainstream AL strategies applied to PLM rankers are not capable of\nidentifying them.", "published": "2023-09-12 11:17:42", "link": "http://arxiv.org/abs/2309.06131v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Measuring vagueness and subjectivity in texts: from symbolic to neural\n  VAGO", "abstract": "We present a hybrid approach to the automated measurement of vagueness and\nsubjectivity in texts. We first introduce the expert system VAGO, we illustrate\nit on a small benchmark of fact vs. opinion sentences, and then test it on the\nlarger French press corpus FreSaDa to confirm the higher prevalence of\nsubjective markers in satirical vs. regular texts. We then build a neural clone\nof VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores\nobtained on FreSaDa. Using explainability tools (LIME), we show the interest of\nthis neural version for the enrichment of the lexicons of the symbolic version,\nand for the production of versions in other languages.", "published": "2023-09-12 11:18:29", "link": "http://arxiv.org/abs/2309.06132v2", "categories": ["cs.CL", "cs.AI", "68T07, 68T50"], "primary_category": "cs.CL"}
{"title": "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by\n  Finding Problematic Prompts", "abstract": "Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown\nremarkable ability in high-quality content generation, and become one of the\nrepresentatives for the recent wave of transformative AI. Nevertheless, such\nadvance comes with an intensifying concern about the misuse of this generative\ntechnology, especially for producing copyrighted or NSFW (i.e. not safe for\nwork) images. Although efforts have been made to filter inappropriate\nimages/prompts or remove undesirable concepts/styles via model fine-tuning, the\nreliability of these safety mechanisms against diversified problematic prompts\nremains largely unexplored. In this work, we propose Prompting4Debugging (P4D)\nas a debugging and red-teaming tool that automatically finds problematic\nprompts for diffusion models to test the reliability of a deployed safety\nmechanism. We demonstrate the efficacy of our P4D tool in uncovering new\nvulnerabilities of SD models with safety mechanisms. Particularly, our result\nshows that around half of prompts in existing safe prompting benchmarks which\nwere originally considered \"safe\" can actually be manipulated to bypass many\ndeployed safety mechanisms, including concept removal, negative prompt, and\nsafety guidance. Our findings suggest that, without comprehensive testing, the\nevaluations on limited safe prompting benchmarks can lead to a false sense of\nsafety for text-to-image models.", "published": "2023-09-12 11:19:36", "link": "http://arxiv.org/abs/2309.06135v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "AKEM: Aligning Knowledge Base to Queries with Ensemble Model for Entity\n  Recognition and Linking", "abstract": "This paper presents a novel approach to address the Entity Recognition and\nLinking Challenge at NLPCC 2015. The task involves extracting named entity\nmentions from short search queries and linking them to entities within a\nreference Chinese knowledge base. To tackle this problem, we first expand the\nexisting knowledge base and utilize external knowledge to identify candidate\nentities, thereby improving the recall rate. Next, we extract features from the\ncandidate entities and utilize Support Vector Regression and Multiple Additive\nRegression Tree as scoring functions to filter the results. Additionally, we\napply rules to further refine the results and enhance precision. Our method is\ncomputationally efficient and achieves an F1 score of 0.535.", "published": "2023-09-12 12:37:37", "link": "http://arxiv.org/abs/2309.06175v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Glancing Future for Simultaneous Machine Translation", "abstract": "Simultaneous machine translation (SiMT) outputs translation while reading the\nsource sentence. Unlike conventional sequence-to-sequence (seq2seq) training,\nexisting SiMT methods adopt the prefix-to-prefix (prefix2prefix) training,\nwhere the model predicts target tokens based on partial source tokens. However,\nthe prefix2prefix training diminishes the ability of the model to capture\nglobal information and introduces forced predictions due to the absence of\nessential source information. Consequently, it is crucial to bridge the gap\nbetween the prefix2prefix training and seq2seq training to enhance the\ntranslation capability of the SiMT model. In this paper, we propose a novel\nmethod that glances future in curriculum learning to achieve the transition\nfrom the seq2seq training to prefix2prefix training. Specifically, we gradually\nreduce the available source information from the whole sentence to the prefix\ncorresponding to that latency. Our method is applicable to a wide range of SiMT\nmethods and experiments demonstrate that our method outperforms strong\nbaselines.", "published": "2023-09-12 12:46:20", "link": "http://arxiv.org/abs/2309.06179v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The first step is the hardest: Pitfalls of Representing and Tokenizing\n  Temporal Data for Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization\nacross diverse tasks, leading individuals to increasingly use them as personal\nassistants and universal computing engines. Nevertheless, a notable obstacle\nemerges when feeding numerical/temporal data into these models, such as data\nsourced from wearables or electronic health records. LLMs employ tokenizers in\ntheir input that break down text into smaller units. However, tokenizers are\nnot designed to represent numerical values and might struggle to understand\nrepetitive patterns and context, treating consecutive values as separate tokens\nand disregarding their temporal relationships. Here, we discuss recent works\nthat employ LLMs for human-centric tasks such as in mobile health sensing and\npresent a case study showing that popular LLMs tokenize temporal data\nincorrectly. To address that, we highlight potential solutions such as prompt\ntuning with lightweight embedding layers as well as multimodal adapters, that\ncan help bridge this \"modality gap\". While the capability of language models to\ngeneralize to other modalities with minimal or no finetuning is exciting, this\npaper underscores the fact that their outputs cannot be meaningful if they\nstumble over input nuances.", "published": "2023-09-12 13:51:29", "link": "http://arxiv.org/abs/2309.06236v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Narrowing the Gap between Supervised and Unsupervised Sentence\n  Representation Learning with Large Language Model", "abstract": "Sentence Representation Learning (SRL) is a fundamental task in Natural\nLanguage Processing (NLP), with the Contrastive Learning of Sentence Embeddings\n(CSE) being the mainstream technique due to its superior performance. An\nintriguing phenomenon in CSE is the significant performance gap between\nsupervised and unsupervised methods, with their only difference lying in the\ntraining data. Previous works attribute this performance gap to differences in\ntwo representation properties (alignment and uniformity). However, since\nalignment and uniformity only measure the results, they fail to answer \"What\naspects of the training data contribute to the performance gap?\" and \"How can\nthe performance gap be narrowed?\", In this paper, we conduct empirical\nexperiments to answer these \"What\" and \"How\" questions. We first answer the\n\"What\" question by thoroughly comparing the behavior of supervised and\nunsupervised CSE during their respective training processes. From the\ncomparison, we identify the similarity pattern as a key factor to the\nperformance gap, and introduce a metric, called Relative Fitting Difficulty\n(RFD), to measure the complexity of the similarity pattern. Then, based on the\ninsights gained from the \"What\" question, we tackle the \"How\" question by\nincreasing the pattern complexity of the training data. We achieve this by\nleveraging the In-Context Learning (ICL) capability of the Large Language Model\n(LLM) to generate data that simulates complex patterns. By utilizing the\nhierarchical patterns in the LLM-generated data, we effectively narrow the gap\nbetween supervised and unsupervised CSE. We release our codes and appendix at\nhttps://github.com/BDBC-KG-NLP/NGCSE.", "published": "2023-09-12 08:16:58", "link": "http://arxiv.org/abs/2309.06453v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Minimum Bayes' Risk Decoding for System Combination of Grammatical Error\n  Correction Systems", "abstract": "For sequence-to-sequence tasks it is challenging to combine individual system\noutputs. Further, there is also often a mismatch between the decoding criterion\nand the one used for assessment. Minimum Bayes' Risk (MBR) decoding can be used\nto combine system outputs in a manner that encourages better alignment with the\nfinal assessment criterion. This paper examines MBR decoding for Grammatical\nError Correction (GEC) systems, where performance is usually evaluated in terms\nof edits and an associated F-score. Hence, we propose a novel MBR loss function\ndirectly linked to this form of criterion. Furthermore, an approach to expand\nthe possible set of candidate sentences is described. This builds on a current\nmax-voting combination scheme, as well as individual edit-level selection.\nExperiments on three popular GEC datasets and with state-of-the-art GEC systems\ndemonstrate the efficacy of the proposed MBR approach. Additionally, the paper\nhighlights how varying reward metrics within the MBR decoding framework can\nprovide control over precision, recall, and the F-score in combined GEC\nsystems.", "published": "2023-09-12 18:51:10", "link": "http://arxiv.org/abs/2309.06520v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Do Generative Large Language Models need billions of parameters?", "abstract": "This paper presents novel systems and methodologies for the development of\nefficient large language models (LLMs). It explores the trade-offs between\nmodel size, performance, and computational resources, with the aim of\nmaximizing the efficiency of these AI systems. The research explores novel\nmethods that allow different parts of the model to share parameters, reducing\nthe total number of unique parameters required. This approach ensures that the\nmodel remains compact without sacrificing its ability to learn and represent\ncomplex language structures. This study provides valuable insights and tools\nfor creating more efficient and effective LLMs, contributing to a more\nsustainable and accessible future for AI language modeling.", "published": "2023-09-12 20:25:22", "link": "http://arxiv.org/abs/2309.06589v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How We Define Harm Impacts Data Annotations: Explaining How Annotators\n  Distinguish Hateful, Offensive, and Toxic Comments", "abstract": "Computational social science research has made advances in machine learning\nand natural language processing that support content moderators in detecting\nharmful content. These advances often rely on training datasets annotated by\ncrowdworkers for harmful content. In designing instructions for annotation\ntasks to generate training data for these algorithms, researchers often treat\nthe harm concepts that we train algorithms to detect - 'hateful', 'offensive',\n'toxic', 'racist', 'sexist', etc. - as interchangeable. In this work, we\nstudied whether the way that researchers define 'harm' affects annotation\noutcomes. Using Venn diagrams, information gain comparisons, and content\nanalyses, we reveal that annotators do not use the concepts 'hateful',\n'offensive', and 'toxic' interchangeably. We identify that features of harm\ndefinitions and annotators' individual characteristics explain much of how\nannotators use these terms differently. Our results offer empirical evidence\ndiscouraging the common practice of using harm concepts interchangeably in\ncontent moderation research. Instead, researchers should make specific choices\nabout which harm concepts to analyze based on their research goals. Recognizing\nthat researchers are often resource constrained, we also encourage researchers\nto provide information to bound their findings when their concepts of interest\ndiffer from concepts that off-the-shelf harmful content detection algorithms\nidentify. Finally, we encourage algorithm providers to ensure their instruments\ncan adapt to contextually-specific content detection goals (e.g., soliciting\ninstrument users' feedback).", "published": "2023-09-12 19:23:40", "link": "http://arxiv.org/abs/2309.15827v1", "categories": ["cs.CL", "cs.CY", "K.4.1"], "primary_category": "cs.CL"}
{"title": "SAGE: Structured Attribute Value Generation for Billion-Scale Product\n  Catalogs", "abstract": "We introduce SAGE; a Generative LLM for inferring attribute values for\nproducts across world-wide e-Commerce catalogs. We introduce a novel\nformulation of the attribute-value prediction problem as a Seq2Seq\nsummarization task, across languages, product types and target attributes. Our\nnovel modeling approach lifts the restriction of predicting attribute values\nwithin a pre-specified set of choices, as well as, the requirement that the\nsought attribute values need to be explicitly mentioned in the text. SAGE can\ninfer attribute values even when such values are mentioned implicitly using\nperiphrastic language, or not-at-all-as is the case for common-sense defaults.\nAdditionally, SAGE is capable of predicting whether an attribute is\ninapplicable for the product at hand, or non-obtainable from the available\ninformation. SAGE is the first method able to tackle all aspects of the\nattribute-value-prediction task as they arise in practical settings in\ne-Commerce catalogs. A comprehensive set of experiments demonstrates the\neffectiveness of the proposed approach, as well as, its superiority against\nstate-of-the-art competing alternatives. Moreover, our experiments highlight\nSAGE's ability to tackle the task of predicting attribute values in zero-shot\nsetting; thereby, opening up opportunities for significantly reducing the\noverall number of labeled examples required for training.", "published": "2023-09-12 02:24:16", "link": "http://arxiv.org/abs/2309.05920v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "A Survey of Hallucination in Large Foundation Models", "abstract": "Hallucination in a foundation model (FM) refers to the generation of content\nthat strays from factual reality or includes fabricated information. This\nsurvey paper provides an extensive overview of recent efforts that aim to\nidentify, elucidate, and tackle the problem of hallucination, with a particular\nfocus on ``Large'' Foundation Models (LFMs). The paper classifies various types\nof hallucination phenomena that are specific to LFMs and establishes evaluation\ncriteria for assessing the extent of hallucination. It also examines existing\nstrategies for mitigating hallucination in LFMs and discusses potential\ndirections for future research in this area. Essentially, the paper offers a\ncomprehensive examination of the challenges and solutions related to\nhallucination in LFMs.", "published": "2023-09-12 02:34:06", "link": "http://arxiv.org/abs/2309.05922v1", "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Language Models as Black-Box Optimizers for Vision-Language Models", "abstract": "Vision-language models (VLMs) pre-trained on web-scale datasets have\ndemonstrated remarkable capabilities on downstream tasks when fine-tuned with\nminimal data. However, many VLMs rely on proprietary data and are not\nopen-source, which restricts the use of white-box approaches for fine-tuning.\nAs such, we aim to develop a black-box approach to optimize VLMs through\nnatural language prompts, thereby avoiding the need to access model parameters,\nfeature embeddings, or even output logits. We propose employing chat-based LLMs\nto search for the best text prompt for VLMs. Specifically, we adopt an\nautomatic hill-climbing procedure that converges to an effective prompt by\nevaluating the performance of current prompts and asking LLMs to refine them\nbased on textual feedback, all within a conversational process without\nhuman-in-the-loop. In a challenging 1-shot image classification setup, our\nsimple approach surpasses the white-box continuous prompting method (CoOp) by\nan average of 1.5% across 11 datasets including ImageNet. Our approach also\noutperforms both human-engineered and LLM-generated prompts. We highlight the\nadvantage of conversational feedback that incorporates both positive and\nnegative prompts, suggesting that LLMs can utilize the implicit gradient\ndirection in textual feedback for a more efficient search. In addition, we find\nthat the text prompts generated through our strategy are not only more\ninterpretable but also transfer well across different VLM architectures in a\nblack-box manner. Lastly, we apply our framework to optimize the\nstate-of-the-art black-box VLM (DALL-E 3) for text-to-image generation, prompt\ninversion, and personalization.", "published": "2023-09-12 04:03:41", "link": "http://arxiv.org/abs/2309.05950v5", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "primary_category": "cs.CL"}
{"title": "The Moral Machine Experiment on Large Language Models", "abstract": "As large language models (LLMs) become more deeply integrated into various\nsectors, understanding how they make moral judgments has become crucial,\nparticularly in the realm of autonomous driving. This study utilized the Moral\nMachine framework to investigate the ethical decision-making tendencies of\nprominent LLMs, including GPT-3.5, GPT-4, PaLM 2, and Llama 2, comparing their\nresponses to human preferences. While LLMs' and humans' preferences such as\nprioritizing humans over pets and favoring saving more lives are broadly\naligned, PaLM 2 and Llama 2, especially, evidence distinct deviations.\nAdditionally, despite the qualitative similarities between the LLM and human\npreferences, there are significant quantitative disparities, suggesting that\nLLMs might lean toward more uncompromising decisions, compared to the milder\ninclinations of humans. These insights elucidate the ethical frameworks of LLMs\nand their potential implications for autonomous driving.", "published": "2023-09-12 04:49:39", "link": "http://arxiv.org/abs/2309.05958v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering\n  Trends across Diverse Platforms", "abstract": "Community Question Answering (CQA) platforms steadily gain popularity as they\nprovide users with fast responses to their queries. The swiftness of these\nresponses is contingent on a mixture of query-specific and user-related\nelements. This paper scrutinizes these contributing factors within the context\nof six highly popular CQA platforms, identified through their standout\nanswering speed. Our investigation reveals a correlation between the time taken\nto yield the first response to a question and several variables: the metadata,\nthe formulation of the questions, and the level of interaction among users.\nAdditionally, by employing conventional machine learning models to analyze\nthese metadata and patterns of user interaction, we endeavor to predict which\nqueries will receive their initial responses promptly.", "published": "2023-09-12 05:03:28", "link": "http://arxiv.org/abs/2309.05961v5", "categories": ["cs.SI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.SI"}
{"title": "Breaking through the learning plateaus of in-context learning in\n  Transformer", "abstract": "In-context learning, i.e., learning from context examples, is an impressive\nability of Transformer. Training Transformers to possess this in-context\nlearning skill is computationally intensive due to the occurrence of learning\nplateaus, which are periods within the training process where there is minimal\nor no enhancement in the model's in-context learning capability. To study the\nmechanism behind the learning plateaus, we conceptually seperate a component\nwithin the model's internal representation that is exclusively affected by the\nmodel's weights. We call this the \"weights component\", and the remainder is\nidentified as the \"context component\". By conducting meticulous and controlled\nexperiments on synthetic tasks, we note that the persistence of learning\nplateaus correlates with compromised functionality of the weights component.\nRecognizing the impaired performance of the weights component as a fundamental\nbehavior drives learning plateaus, we have developed three strategies to\nexpedite the learning of Transformers. The effectiveness of these strategies is\nfurther confirmed in natural language processing tasks. In conclusion, our\nresearch demonstrates the feasibility of cultivating a powerful in-context\nlearning ability within AI systems in an eco-friendly manner.", "published": "2023-09-12 08:45:25", "link": "http://arxiv.org/abs/2309.06054v3", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Improving and Evaluating the Detection of Fragmentation in News\n  Recommendations with the Clustering of News Story Chains", "abstract": "News recommender systems play an increasingly influential role in shaping\ninformation access within democratic societies. However, tailoring\nrecommendations to users' specific interests can result in the divergence of\ninformation streams. Fragmented access to information poses challenges to the\nintegrity of the public sphere, thereby influencing democracy and public\ndiscourse. The Fragmentation metric quantifies the degree of fragmentation of\ninformation streams in news recommendations. Accurate measurement of this\nmetric requires the application of Natural Language Processing (NLP) to\nidentify distinct news events, stories, or timelines. This paper presents an\nextensive investigation of various approaches for quantifying Fragmentation in\nnews recommendations. These approaches are evaluated both intrinsically, by\nmeasuring performance on news story clustering, and extrinsically, by assessing\nthe Fragmentation scores of different simulated news recommender scenarios. Our\nfindings demonstrate that agglomerative hierarchical clustering coupled with\nSentenceBERT text representation is substantially better at detecting\nFragmentation than earlier implementations. Additionally, the analysis of\nsimulated scenarios yields valuable insights and recommendations for\nstakeholders concerning the measurement and interpretation of Fragmentation.", "published": "2023-09-12 13:01:20", "link": "http://arxiv.org/abs/2309.06192v2", "categories": ["cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Human Action Co-occurrence in Lifestyle Vlogs using Graph Link\n  Prediction", "abstract": "We introduce the task of automatic human action co-occurrence identification,\ni.e., determine whether two human actions can co-occur in the same interval of\ntime. We create and make publicly available the ACE (Action Co-occurrencE)\ndataset, consisting of a large graph of ~12k co-occurring pairs of visual\nactions and their corresponding video clips. We describe graph link prediction\nmodels that leverage visual and textual information to automatically infer if\ntwo actions are co-occurring. We show that graphs are particularly well suited\nto capture relations between human actions, and the learned graph\nrepresentations are effective for our task and capture novel and relevant\ninformation across different data domains. The ACE dataset and the code\nintroduced in this paper are publicly available at\nhttps://github.com/MichiganNLP/vlog_action_co-occurrence.", "published": "2023-09-12 13:38:44", "link": "http://arxiv.org/abs/2309.06219v3", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.IR"], "primary_category": "cs.CV"}
{"title": "Leveraging Large Language Models and Weak Supervision for Social Media\n  data annotation: an evaluation using COVID-19 self-reported vaccination\n  tweets", "abstract": "The COVID-19 pandemic has presented significant challenges to the healthcare\nindustry and society as a whole. With the rapid development of COVID-19\nvaccines, social media platforms have become a popular medium for discussions\non vaccine-related topics. Identifying vaccine-related tweets and analyzing\nthem can provide valuable insights for public health research-ers and\npolicymakers. However, manual annotation of a large number of tweets is\ntime-consuming and expensive. In this study, we evaluate the usage of Large\nLanguage Models, in this case GPT-4 (March 23 version), and weak supervision,\nto identify COVID-19 vaccine-related tweets, with the purpose of comparing\nperformance against human annotators. We leveraged a manu-ally curated\ngold-standard dataset and used GPT-4 to provide labels without any additional\nfine-tuning or instructing, in a single-shot mode (no additional prompting).", "published": "2023-09-12 18:18:23", "link": "http://arxiv.org/abs/2309.06503v1", "categories": ["cs.CL", "cs.LG", "cs.SI"], "primary_category": "cs.CL"}
{"title": "RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of\n  Language Models", "abstract": "Recent advancements in language models (LMs) have gained substantial\nattentions on their capability to generate human-like responses. Though\nexhibiting a promising future for various applications such as conversation AI,\nthese LMs face deployment challenges on various devices due to their extreme\ncomputational cost and unpredictable inference latency. Such varied inference\nlatency, identified as a consequence of uncertainty intrinsic to the nature of\nlanguage, can lead to computational inefficiency and degrade the overall\nperformance of LMs, especially under high-traffic workloads. Unfortunately, the\nbandwidth of these uncertainty sources is extensive, complicating the\nprediction of latency and the effects emanating from such uncertainties. To\nunderstand and mitigate the impact of uncertainty on real-time\nresponse-demanding systems, we take the first step to comprehend, quantify and\noptimize these uncertainty-induced latency performance variations in LMs.\nSpecifically, we present RT-LM, an uncertainty-aware resource management\necosystem for real-time inference of LMs. RT-LM innovatively quantifies how\nspecific input uncertainties, adversely affect latency, often leading to an\nincreased output length. Exploiting these insights, we devise a lightweight yet\neffective method to dynamically correlate input text uncertainties with output\nlength at runtime. Utilizing this quantification as a latency heuristic, we\nintegrate the uncertainty information into a system-level scheduler which\nexplores several uncertainty-induced optimization opportunities, including\nuncertainty-aware prioritization, dynamic consolidation, and strategic CPU\noffloading. Quantitative experiments across five state-of-the-art LMs on two\nhardware platforms demonstrates that RT-LM can significantly reduce the average\nresponse time and improve throughput while incurring a rather small runtime\noverhead.", "published": "2023-09-12 22:22:10", "link": "http://arxiv.org/abs/2309.06619v1", "categories": ["cs.LG", "cs.CL", "cs.SY", "eess.SY"], "primary_category": "cs.LG"}
{"title": "Exploring Large Language Models for Ontology Alignment", "abstract": "This work investigates the applicability of recent generative Large Language\nModels (LLMs), such as the GPT series and Flan-T5, to ontology alignment for\nidentifying concept equivalence mappings across ontologies. To test the\nzero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging\nsubsets from two equivalence matching datasets of the OAEI Bio-ML track, taking\ninto account concept labels and structural contexts. Preliminary findings\nsuggest that LLMs have the potential to outperform existing ontology alignment\nsystems like BERTMap, given careful framework and prompt design.", "published": "2023-09-12 17:01:02", "link": "http://arxiv.org/abs/2309.07172v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech\n  Recognition for Children VS. Adults", "abstract": "Recent advancements in Automatic Speech Recognition (ASR) systems,\nexemplified by Whisper, have demonstrated the potential of these systems to\napproach human-level performance given sufficient data. However, this progress\ndoesn't readily extend to ASR for children due to the limited availability of\nsuitable child-specific databases and the distinct characteristics of\nchildren's speech. A recent study investigated leveraging the My Science Tutor\n(MyST) children's speech corpus to enhance Whisper's performance in recognizing\nchildren's speech. They were able to demonstrate some improvement on a limited\ntestset. This paper builds on these findings by enhancing the utility of the\nMyST dataset through more efficient data preprocessing. We reduce the Word\nError Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and\nfrom 13.23% to 8.61% with Whisper-Medium and show that this improvement can be\ngeneralized to unseen datasets. We also highlight important challenges towards\nimproving children's ASR performance. The results showcase the viable and\nefficient integration of Whisper for effective children's speech recognition.", "published": "2023-09-12 06:58:18", "link": "http://arxiv.org/abs/2309.07927v3", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Evaluating Dynamic Topic Models", "abstract": "There is a lack of quantitative measures to evaluate the progression of\ntopics through time in dynamic topic models (DTMs). Filling this gap, we\npropose a novel evaluation measure for DTMs that analyzes the changes in the\nquality of each topic over time. Additionally, we propose an extension\ncombining topic quality with the model's temporal consistency. We demonstrate\nthe utility of the proposed measure by applying it to synthetic data and data\nfrom existing DTMs. We also conducted a human evaluation, which indicates that\nthe proposed measure correlates well with human judgment. Our findings may help\nin identifying changing topics, evaluating different DTMs, and guiding future\nresearch in this area.", "published": "2023-09-12 13:30:25", "link": "http://arxiv.org/abs/2309.08627v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Recovering from Privacy-Preserving Masking with Large Language Models", "abstract": "Model adaptation is crucial to handle the discrepancy between proxy training\ndata and actual users data received. To effectively perform adaptation, textual\ndata of users is typically stored on servers or their local devices, where\ndownstream natural language processing (NLP) models can be directly trained\nusing such in-domain data. However, this might raise privacy and security\nconcerns due to the extra risks of exposing user information to adversaries.\nReplacing identifying information in textual data with a generic marker has\nbeen recently explored. In this work, we leverage large language models (LLMs)\nto suggest substitutes of masked tokens and have their effectiveness evaluated\non downstream language modeling tasks. Specifically, we propose multiple\npre-trained and fine-tuned LLM-based approaches and perform empirical studies\non various datasets for the comparison of these methods. Experimental results\nshow that models trained on the obfuscation corpora are able to achieve\ncomparable performance with the ones trained on the original data without\nprivacy-preserving token masking.", "published": "2023-09-12 16:39:41", "link": "http://arxiv.org/abs/2309.08628v3", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Fingerprint Attack: Client De-Anonymization in Federated Learning", "abstract": "Federated Learning allows collaborative training without data sharing in\nsettings where participants do not trust the central server and one another.\nPrivacy can be further improved by ensuring that communication between the\nparticipants and the server is anonymized through a shuffle; decoupling the\nparticipant identity from their data. This paper seeks to examine whether such\na defense is adequate to guarantee anonymity, by proposing a novel\nfingerprinting attack over gradients sent by the participants to the server. We\nshow that clustering of gradients can easily break the anonymization in an\nempirical study of learning federated language models on two language corpora.\nWe then show that training with differential privacy can provide a practical\ndefense against our fingerprint attack.", "published": "2023-09-12 11:10:30", "link": "http://arxiv.org/abs/2310.05960v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "AstroLLaMA: Towards Specialized Foundation Models in Astronomy", "abstract": "Large language models excel in many human-language tasks but often falter in\nhighly specialized domains like scholarly astronomy. To bridge this gap, we\nintroduce AstroLLaMA, a 7-billion-parameter model fine-tuned from LLaMA-2 using\nover 300,000 astronomy abstracts from arXiv. Optimized for traditional causal\nlanguage modeling, AstroLLaMA achieves a 30% lower perplexity than Llama-2,\nshowing marked domain adaptation. Our model generates more insightful and\nscientifically relevant text completions and embedding extraction than\nstate-of-the-arts foundation models despite having significantly fewer\nparameters. AstroLLaMA serves as a robust, domain-specific model with broad\nfine-tuning potential. Its public release aims to spur astronomy-focused\nresearch, including automatic paper summarization and conversational agent\ndevelopment.", "published": "2023-09-12 11:02:27", "link": "http://arxiv.org/abs/2309.06126v1", "categories": ["astro-ph.IM", "astro-ph.CO", "astro-ph.GA", "astro-ph.HE", "cs.CL", "cs.LG"], "primary_category": "astro-ph.IM"}
{"title": "Can large-scale vocoded spoofed data improve speech spoofing\n  countermeasure with a self-supervised front end?", "abstract": "A speech spoofing countermeasure (CM) that discriminates between unseen\nspoofed and bona fide data requires diverse training data. While many datasets\nuse spoofed data generated by speech synthesis systems, it was recently found\nthat data vocoded by neural vocoders were also effective as the spoofed\ntraining data. Since many neural vocoders are fast in building and generation,\nthis study used multiple neural vocoders and created more than 9,000 hours of\nvocoded data on the basis of the VoxCeleb2 corpus. This study investigates how\nthis large-scale vocoded data can improve spoofing countermeasures that use\ndata-hungry self-supervised learning (SSL) models. Experiments demonstrated\nthat the overall CM performance on multiple test sets improved when using\nfeatures extracted by an SSL model continually trained on the vocoded data.\nFurther improvement was observed when using a new SSL distilled from the two\nSSLs before and after the continual training. The CM with the distilled SSL\noutperformed the previous best model on challenging unseen test sets, including\nthe ASVspoof 2019 logical access, WaveFake, and In-the-Wild.", "published": "2023-09-12 07:25:08", "link": "http://arxiv.org/abs/2309.06014v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "iPhonMatchNet: Zero-Shot User-Defined Keyword Spotting Using Implicit\n  Acoustic Echo Cancellation", "abstract": "In response to the increasing interest in human--machine communication across\nvarious domains, this paper introduces a novel approach called iPhonMatchNet,\nwhich addresses the challenge of barge-in scenarios, wherein user speech\noverlaps with device playback audio, thereby creating a self-referencing\nproblem. The proposed model leverages implicit acoustic echo cancellation\n(iAEC) techniques to increase the efficiency of user-defined keyword spotting\nmodels, achieving a remarkable 95% reduction in mean absolute error with a\nminimal increase in model size (0.13%) compared to the baseline model,\nPhonMatchNet. We also present an efficient model structure and demonstrate its\ncapability to learn iAEC functionality without requiring a clean signal. The\nfindings of our study indicate that the proposed model achieves competitive\nperformance in real-world deployment conditions of smart devices.", "published": "2023-09-12 10:03:31", "link": "http://arxiv.org/abs/2309.06096v3", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "SynVox2: Towards a privacy-friendly VoxCeleb2 dataset", "abstract": "The success of deep learning in speaker recognition relies heavily on the use\nof large datasets. However, the data-hungry nature of deep learning methods has\nalready being questioned on account the ethical, privacy, and legal concerns\nthat arise when using large-scale datasets of natural speech collected from\nreal human speakers. For example, the widely-used VoxCeleb2 dataset for speaker\nrecognition is no longer accessible from the official website. To mitigate\nthese concerns, this work presents an initiative to generate a privacy-friendly\nsynthetic VoxCeleb2 dataset that ensures the quality of the generated speech in\nterms of privacy, utility, and fairness. We also discuss the challenges of\nusing synthetic data for the downstream task of speaker verification.", "published": "2023-09-12 11:28:07", "link": "http://arxiv.org/abs/2309.06141v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "ASPED: An Audio Dataset for Detecting Pedestrians", "abstract": "We introduce the new audio analysis task of pedestrian detection and present\na new large-scale dataset for this task. While the preliminary results prove\nthe viability of using audio approaches for pedestrian detection, they also\nshow that this challenging task cannot be easily solved with standard\napproaches.", "published": "2023-09-12 19:10:45", "link": "http://arxiv.org/abs/2309.06531v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CleanUNet 2: A Hybrid Speech Denoising Model on Waveform and Spectrogram", "abstract": "In this work, we present CleanUNet 2, a speech denoising model that combines\nthe advantages of waveform denoiser and spectrogram denoiser and achieves the\nbest of both worlds. CleanUNet 2 uses a two-stage framework inspired by popular\nspeech synthesis methods that consist of a waveform model and a spectrogram\nmodel. Specifically, CleanUNet 2 builds upon CleanUNet, the state-of-the-art\nwaveform denoiser, and further boosts its performance by taking predicted\nspectrograms from a spectrogram denoiser as the input. We demonstrate that\nCleanUNet 2 outperforms previous methods in terms of various objective and\nsubjective evaluations.", "published": "2023-09-12 05:55:41", "link": "http://arxiv.org/abs/2309.05975v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Assessing the Generalization Gap of Learning-Based Speech Enhancement\n  Systems in Noisy and Reverberant Environments", "abstract": "The acoustic variability of noisy and reverberant speech mixtures is\ninfluenced by multiple factors, such as the spectro-temporal characteristics of\nthe target speaker and the interfering noise, the signal-to-noise ratio (SNR)\nand the room characteristics. This large variability poses a major challenge\nfor learning-based speech enhancement systems, since a mismatch between the\ntraining and testing conditions can substantially reduce the performance of the\nsystem. Generalization to unseen conditions is typically assessed by testing\nthe system with a new speech, noise or binaural room impulse response (BRIR)\ndatabase different from the one used during training. However, the difficulty\nof the speech enhancement task can change across databases, which can\nsubstantially influence the results. The present study introduces a\ngeneralization assessment framework that uses a reference model trained on the\ntest condition, such that it can be used as a proxy for the difficulty of the\ntest condition. This allows to disentangle the effect of the change in task\ndifficulty from the effect of dealing with new data, and thus to define a new\nmeasure of generalization performance termed the generalization gap. The\nprocedure is repeated in a cross-validation fashion by cycling through multiple\nspeech, noise, and BRIR databases to accurately estimate the generalization\ngap. The proposed framework is applied to evaluate the generalization potential\nof a feedforward neural network (FFNN), Conv-TasNet, DCCRN and MANNER. We find\nthat for all models, the performance degrades the most in speech mismatches,\nwhile good noise and room generalization can be achieved by training on\nmultiple databases. Moreover, while recent models show higher performance in\nmatched conditions, their performance substantially decreases in mismatched\nconditions and can become inferior to that of the FFNN-based system.", "published": "2023-09-12 12:51:12", "link": "http://arxiv.org/abs/2309.06183v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
