{"title": "Benchmarking Knowledge-Enhanced Commonsense Question Answering via\n  Knowledge-to-Text Transformation", "abstract": "A fundamental ability of humans is to utilize commonsense knowledge in\nlanguage understanding and question answering. In recent years, many\nknowledge-enhanced Commonsense Question Answering (CQA) approaches have been\nproposed. However, it remains unclear: (1) How far can we get by exploiting\nexternal knowledge for CQA? (2) How much potential of knowledge has been\nexploited in current CQA models? (3) Which are the most promising directions\nfor future CQA? To answer these questions, we benchmark knowledge-enhanced CQA\nby conducting extensive experiments on multiple standard CQA datasets using a\nsimple and effective knowledge-to-text transformation framework. Experiments\nshow that: (1) Our knowledge-to-text framework is effective and achieves\nstate-of-the-art performance on CommonsenseQA dataset, providing a simple and\nstrong knowledge-enhanced baseline for CQA; (2) The potential of knowledge is\nstill far from being fully exploited in CQA -- there is a significant\nperformance gap from current models to our models with golden knowledge; and\n(3) Context-sensitive knowledge selection, heterogeneous knowledge\nexploitation, and commonsense-rich language models are promising CQA\ndirections.", "published": "2021-01-04 04:29:03", "link": "http://arxiv.org/abs/2101.00760v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Portuguese Semantic Role Labeling with Transformers and\n  Transfer Learning", "abstract": "The Natural Language Processing task of determining \"Who did what to whom\" is\ncalled Semantic Role Labeling. For English, recent methods based on Transformer\nmodels have allowed for major improvements in this task over the previous state\nof the art. However, for low resource languages, like Portuguese, currently\navailable semantic role labeling models are hindered by scarce training data.\nIn this paper, we explore a model architecture with only a pre-trained\nTransformer-based model, a linear layer, softmax and Viterbi decoding. We\nsubstantially improve the state-of-the-art performance in Portuguese by over 15\nF1. Additionally, we improve semantic role labeling results in Portuguese\ncorpora by exploiting cross-lingual transfer learning using multilingual\npre-trained models, and transfer learning from dependency parsing in\nPortuguese, evaluating the various proposed approaches empirically.", "published": "2021-01-04 19:56:01", "link": "http://arxiv.org/abs/2101.01213v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Reddit Entity Linking Dataset", "abstract": "We introduce and make publicly available an entity linking dataset from\nReddit that contains 17,316 linked entities, each annotated by three human\nannotators and then grouped into Gold, Silver, and Bronze to indicate\ninter-annotator agreement. We analyze the different errors and disagreements\nmade by annotators and suggest three types of corrections to the raw data.\nFinally, we tested existing entity linking models that are trained and tuned on\ntext from non-social media datasets. We find that, although these existing\nentity linking models perform very well on their original datasets, they\nperform poorly on this social media dataset. We also show that the majority of\nthese errors can be attributed to poor performance on the mention detection\nsubtask. These results indicate the need for better entity linking models that\ncan be applied to the enormous amount of social media text.", "published": "2021-01-04 20:34:04", "link": "http://arxiv.org/abs/2101.01228v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Coreference Resolution: Are the eliminated spans totally worthless?", "abstract": "Various neural-based methods have been proposed so far for joint mention\ndetection and coreference resolution. However, existing works on coreference\nresolution are mainly dependent on filtered mention representation, while other\nspans are largely neglected. In this paper, we aim at increasing the\nutilization rate of data and investigating whether those eliminated spans are\ntotally useless, or to what extent they can improve the performance of\ncoreference resolution. To achieve this, we propose a mention representation\nrefining strategy where spans highly related to mentions are well leveraged\nusing a pointer network for representation enhancing. Notably, we utilize an\nadditional loss term in this work to encourage the diversity between entity\nclusters. Experimental results on the document-level CoNLL-2012 Shared Task\nEnglish dataset show that eliminated spans are indeed much effective and our\napproach can achieve competitive results when compared with previous\nstate-of-the-art in coreference resolution.", "published": "2021-01-04 02:02:49", "link": "http://arxiv.org/abs/2101.00737v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis", "abstract": "Aspect based sentiment analysis (ABSA) involves three fundamental subtasks:\naspect term extraction, opinion term extraction, and aspect-level sentiment\nclassification. Early works only focused on solving one of these subtasks\nindividually. Some recent work focused on solving a combination of two\nsubtasks, e.g., extracting aspect terms along with sentiment polarities or\nextracting the aspect and opinion terms pair-wisely. More recently, the triple\nextraction task has been proposed, i.e., extracting the (aspect term, opinion\nterm, sentiment polarity) triples from a sentence. However, previous approaches\nfail to solve all subtasks in a unified end-to-end framework. In this paper, we\npropose a complete solution for ABSA. We construct two machine reading\ncomprehension (MRC) problems and solve all subtasks by joint training two\nBERT-MRC models with parameters sharing. We conduct experiments on these\nsubtasks, and results on several benchmark datasets demonstrate the\neffectiveness of our proposed framework, which significantly outperforms\nexisting state-of-the-art methods.", "published": "2021-01-04 07:47:53", "link": "http://arxiv.org/abs/2101.00816v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How to Train Your Agent to Read and Write", "abstract": "Reading and writing research papers is one of the most privileged abilities\nthat a qualified researcher should master. However, it is difficult for new\nresearchers (\\eg{students}) to fully {grasp} this ability. It would be\nfascinating if we could train an intelligent agent to help people read and\nsummarize papers, and perhaps even discover and exploit the potential knowledge\nclues to write novel papers. Although there have been existing works focusing\non summarizing (\\emph{i.e.}, reading) the knowledge in a given text or\ngenerating (\\emph{i.e.}, writing) a text based on the given knowledge, the\nability of simultaneously reading and writing is still under development.\nTypically, this requires an agent to fully understand the knowledge from the\ngiven text materials and generate correct and fluent novel paragraphs, which is\nvery challenging in practice. In this paper, we propose a Deep ReAder-Writer\n(DRAW) network, which consists of a \\textit{Reader} that can extract knowledge\ngraphs (KGs) from input paragraphs and discover potential knowledge, a\ngraph-to-text \\textit{Writer} that generates a novel paragraph, and a\n\\textit{Reviewer} that reviews the generated paragraph from three different\naspects. Extensive experiments show that our DRAW network outperforms\nconsidered baselines and several state-of-the-art methods on AGENDA and\nM-AGENDA datasets. Our code and supplementary are released at\nhttps://github.com/menggehe/DRAW.", "published": "2021-01-04 12:22:04", "link": "http://arxiv.org/abs/2101.00916v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CRSLab: An Open-Source Toolkit for Building Conversational Recommender\n  System", "abstract": "In recent years, conversational recommender system (CRS) has received much\nattention in the research community. However, existing studies on CRS vary in\nscenarios, goals and techniques, lacking unified, standardized implementation\nor comparison. To tackle this challenge, we propose an open-source CRS toolkit\nCRSLab, which provides a unified and extensible framework with highly-decoupled\nmodules to develop CRSs. Based on this framework, we collect 6 commonly-used\nhuman-annotated CRS datasets and implement 18 models that include recent\ntechniques such as graph neural network and pre-training models. Besides, our\ntoolkit provides a series of automatic evaluation protocols and a human-machine\ninteraction interface to test and compare different CRS methods. The project\nand documents are released at https://github.com/RUCAIBox/CRSLab.", "published": "2021-01-04 13:10:31", "link": "http://arxiv.org/abs/2101.00939v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Improving reference mining in patents with BERT", "abstract": "In this paper we address the challenge of extracting scientific references\nfrom patents. We approach the problem as a sequence labelling task and\ninvestigate the merits of BERT models to the extraction of these long\nsequences. References in patents to scientific literature are relevant to study\nthe connection between science and industry. Most prior work only uses the\nfront-page citations for this analysis, which are provided in the metadata of\npatent archives. In this paper we build on prior work using Conditional Random\nFields (CRF) and Flair for reference extraction. We improve the quality of the\ntraining data and train three BERT-based models on the labelled data (BERT,\nbioBERT, sciBERT). We find that the improved training data leads to a large\nimprovement in the quality of the trained models. In addition, the BERT models\nbeat CRF and Flair, with recall scores around 97% obtained with cross\nvalidation. With the best model we label a large collection of 33 thousand\npatents, extract the citations, and match them to publications in the Web of\nScience database. We extract 50% more references than with the old training\ndata and methods: 735 thousand references in total. With these\npatent-publication links, follow-up research will further analyze which types\nof scientific work lead to inventions.", "published": "2021-01-04 15:56:21", "link": "http://arxiv.org/abs/2101.01039v3", "categories": ["cs.IR", "cs.CL", "H.3.1; I.2.7"], "primary_category": "cs.IR"}
{"title": "Outline to Story: Fine-grained Controllable Story Generation from\n  Cascaded Events", "abstract": "Large-scale pretrained language models have shown thrilling generation\ncapabilities, especially when they generate consistent long text in thousands\nof words with ease. However, users of these models can only control the prefix\nof sentences or certain global aspects of generated text. It is challenging to\nsimultaneously achieve fine-grained controllability and preserve the\nstate-of-the-art unconditional text generation capability. In this paper, we\nfirst propose a new task named \"Outline to Story\" (O2S) as a test bed for\nfine-grained controllable generation of long text, which generates a\nmulti-paragraph story from cascaded events, i.e. a sequence of outline events\nthat guide subsequent paragraph generation. We then create dedicate datasets\nfor future benchmarks, built by state-of-the-art keyword extraction techniques.\nFinally, we propose an extremely simple yet strong baseline method for the O2S\ntask, which fine tunes pre-trained language models on augmented sequences of\noutline-story pairs with simple language modeling objective. Our method does\nnot introduce any new parameters or perform any architecture modification,\nexcept several special tokens as delimiters to build augmented sequences.\nExtensive experiments on various datasets demonstrate state-of-the-art\nconditional story generation performance with our model, achieving better\nfine-grained controllability and user flexibility. Our paper is among the first\nones by our knowledge to propose a model and to create datasets for the task of\n\"outline to story\". Our work also instantiates research interest of\nfine-grained controllable generation of open-domain long text, where\ncontrolling inputs are represented by short text.", "published": "2021-01-04 08:16:21", "link": "http://arxiv.org/abs/2101.00822v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Transformer-based Conditional Variational Autoencoder for Controllable\n  Story Generation", "abstract": "We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.", "published": "2021-01-04 08:31:11", "link": "http://arxiv.org/abs/2101.00828v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Generalized Spatio-Temporal RNN Beamformer for Target Speech Separation", "abstract": "Although the conventional mask-based minimum variance distortionless response\n(MVDR) could reduce the non-linear distortion, the residual noise level of the\nMVDR separated speech is still high. In this paper, we propose a\nspatio-temporal recurrent neural network based beamformer (RNN-BF) for target\nspeech separation. This new beamforming framework directly learns the\nbeamforming weights from the estimated speech and noise spatial covariance\nmatrices. Leveraging on the temporal modeling capability of RNNs, the RNN-BF\ncould automatically accumulate the statistics of the speech and noise\ncovariance matrices to learn the frame-level beamforming weights in a recursive\nway. An RNN-based generalized eigenvalue (RNN-GEV) beamformer and a more\ngeneralized RNN beamformer (GRNN-BF) are proposed. We further improve the\nRNN-GEV and the GRNN-BF by using layer normalization to replace the commonly\nused mask normalization on the covariance matrices. The proposed GRNN-BF\nobtains better performance against prior arts in terms of speech quality\n(PESQ), speech-to-noise ratio (SNR) and word error rate (WER).", "published": "2021-01-04 23:31:41", "link": "http://arxiv.org/abs/2101.01280v5", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A novel policy for pre-trained Deep Reinforcement Learning for Speech\n  Emotion Recognition", "abstract": "Reinforcement Learning (RL) is a semi-supervised learning paradigm which an\nagent learns by interacting with an environment. Deep learning in combination\nwith RL provides an efficient method to learn how to interact with the\nenvironment is called Deep Reinforcement Learning (deep RL). Deep RL has gained\ntremendous success in gaming - such as AlphaGo, but its potential have rarely\nbeing explored for challenging tasks like Speech Emotion Recognition (SER). The\ndeep RL being used for SER can potentially improve the performance of an\nautomated call centre agent by dynamically learning emotional-aware response to\ncustomer queries. While the policy employed by the RL agent plays a major role\nin action selection, there is no current RL policy tailored for SER. In\naddition, extended learning period is a general challenge for deep RL which can\nimpact the speed of learning for SER. Therefore, in this paper, we introduce a\nnovel policy - \"Zeta policy\" which is tailored for SER and apply Pre-training\nin deep RL to achieve faster learning rate. Pre-training with cross dataset was\nalso studied to discover the feasibility of pre-training the RL Agent with a\nsimilar dataset in a scenario of where no real environmental data is not\navailable. IEMOCAP and SAVEE datasets were used for the evaluation with the\nproblem being to recognize four emotions happy, sad, angry and neutral in the\nutterances provided. Experimental results show that the proposed \"Zeta policy\"\nperforms better than existing policies. The results also support that\npre-training can reduce the training time upon reducing the warm-up period and\nis robust to cross-corpus scenario.", "published": "2021-01-04 02:13:26", "link": "http://arxiv.org/abs/2101.00738v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
