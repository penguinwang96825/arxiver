{"title": "Workflow Discovery from Dialogues in the Low Data Regime", "abstract": "Text-based dialogues are now widely used to solve real-world problems. In\ncases where solution strategies are already known, they can sometimes be\ncodified into workflows and used to guide humans or artificial agents through\nthe task of helping clients. We introduce a new problem formulation that we\ncall Workflow Discovery (WD) in which we are interested in the situation where\na formal workflow may not yet exist. Still, we wish to discover the set of\nactions that have been taken to resolve a particular problem. We also examine a\nsequence-to-sequence (Seq2Seq) approach for this novel task. We present\nexperiments where we extract workflows from dialogues in the Action-Based\nConversations Dataset (ABCD). Since the ABCD dialogues follow known workflows\nto guide agents, we can evaluate our ability to extract such workflows using\nground truth sequences of actions. We propose and evaluate an approach that\nconditions models on the set of possible actions, and we show that using this\nstrategy, we can improve WD performance. Our conditioning approach also\nimproves zero-shot and few-shot WD performance when transferring learned models\nto unseen domains within and across datasets. Further, on ABCD a modified\nvariant of our Seq2Seq method achieves state-of-the-art performance on related\nbut different problems of Action State Tracking (AST) and Cascading Dialogue\nSuccess (CDS) across many evaluation metrics.", "published": "2022-05-24 01:12:03", "link": "http://arxiv.org/abs/2205.11690v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Survey on Neural Open Information Extraction: Current Status and\n  Future Directions", "abstract": "Open Information Extraction (OpenIE) facilitates domain-independent discovery\nof relational facts from large corpora. The technique well suits many\nopen-world natural language understanding scenarios, such as automatic\nknowledge base construction, open-domain question answering, and explicit\nreasoning. Thanks to the rapid development in deep learning technologies,\nnumerous neural OpenIE architectures have been proposed and achieve\nconsiderable performance improvement. In this survey, we provide an extensive\noverview of the-state-of-the-art neural OpenIE models, their key design\ndecisions, strengths and weakness. Then, we discuss limitations of current\nsolutions and the open issues in OpenIE problem itself. Finally we list recent\ntrends that could help expand its scope and applicability, setting up promising\ndirections for future research in OpenIE. To our best knowledge, this paper is\nthe first review on this specific topic.", "published": "2022-05-24 02:24:55", "link": "http://arxiv.org/abs/2205.11725v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question\n  Answering", "abstract": "Multi-hop question answering (QA) is a challenging task requiring QA systems\nto perform complex reasoning over multiple documents and provide supporting\nfacts together with the exact answer. Existing works tend to utilize\ngraph-based reasoning and question decomposition to obtain the reasoning chain,\nwhich inevitably introduces additional complexity and cumulative error to the\nsystem. To address the above issue, we propose a simple yet effective novel\nframework, From Easy to Hard (FE2H), to remove distracting information and\nobtain better contextual representations for the multi-hop QA task. Inspired by\nthe iterative document selection process and the progressive learning custom of\nhumans, FE2H divides both the document selector and reader into two stages\nfollowing an easy-to-hard manner. Specifically, we first select the document\nmost relevant to the question and then utilize the question together with this\ndocument to select other pertinent documents. As for the QA phase, our reader\nis first trained on a single-hop QA dataset and then transferred into the\nmulti-hop QA task. We comprehensively evaluate our model on the popular\nmulti-hop QA benchmark HotpotQA. Experimental results demonstrate that our\nmethod ourperforms all other methods in the leaderboard of HotpotQA (distractor\nsetting).", "published": "2022-05-24 02:33:58", "link": "http://arxiv.org/abs/2205.11729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of\n  Multilingual Language Models", "abstract": "The emergent cross-lingual transfer seen in multilingual pretrained models\nhas sparked significant interest in studying their behavior. However, because\nthese analyses have focused on fully trained multilingual models, little is\nknown about the dynamics of the multilingual pretraining process. We\ninvestigate when these models acquire their in-language and cross-lingual\nabilities by probing checkpoints taken from throughout XLM-R pretraining, using\na suite of linguistic tasks. Our analysis shows that the model achieves high\nin-language performance early on, with lower-level linguistic skills acquired\nbefore more complex ones. In contrast, the point in pretraining when the model\nlearns to transfer cross-lingually differs across language pairs.\nInterestingly, we also observe that, across many languages and tasks, the final\nmodel layer exhibits significant performance degradation over time, while\nlinguistic knowledge propagates to lower layers of the network. Taken together,\nthese insights highlight the complexity of multilingual pretraining and the\nresulting varied behavior for different languages over time.", "published": "2022-05-24 03:35:00", "link": "http://arxiv.org/abs/2205.11758v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "D4: a Chinese Dialogue Dataset for Depression-Diagnosis-Oriented Chat", "abstract": "In a depression-diagnosis-directed clinical session, doctors initiate a\nconversation with ample emotional support that guides the patients to expose\ntheir symptoms based on clinical diagnosis criteria. Such a dialogue system is\ndistinguished from existing single-purpose human-machine dialog systems, as it\ncombines task-oriented and chit-chats with uniqueness in dialogue topics and\nprocedures. However, due to the social stigma associated with mental illness,\nthe dialogue data related to depression consultation and diagnosis are rarely\ndisclosed. Based on clinical depression diagnostic criteria ICD-11 and DSM-5,\nwe designed a 3-phase procedure to construct D$^4$: a Chinese Dialogue Dataset\nfor Depression-Diagnosis-Oriented Chat, which simulates the dialogue between\ndoctors and patients during the diagnosis of depression, including diagnosis\nresults and symptom summary given by professional psychiatrists for each\nconversation. Upon the newly-constructed dataset, four tasks mirroring the\ndepression diagnosis process are established: response generation, topic\nprediction, dialog summary, and severity classification of depressive episode\nand suicide risk. Multi-scale evaluation results demonstrate that a more\nempathy-driven and diagnostic-accurate consultation dialogue system trained on\nour dataset can be achieved compared to rule-based bots.", "published": "2022-05-24 03:54:22", "link": "http://arxiv.org/abs/2205.11764v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Maieutic Prompting: Logically Consistent Reasoning with Recursive\n  Explanations", "abstract": "Despite their impressive capabilities, large pre-trained language models\n(LMs) struggle with consistent reasoning; recently, prompting LMs to generate\nexplanations that self-guide the inference has emerged as a promising direction\nto amend this. However, these approaches are fundamentally bounded by the\ncorrectness of explanations, which themselves are often noisy and inconsistent.\nIn this work, we develop Maieutic Prompting, which infers a correct answer to a\nquestion even from the noisy and inconsistent generations of LM. Maieutic\nPrompting induces a tree of explanations abductively (e.g. X is true, because\n...) and recursively, then frames the inference as a satisfiability problem\nover these explanations and their logical relations. We test Maieutic Prompting\nfor true/false QA on three challenging benchmarks that require complex\ncommonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy\nthan state-of-the-art prompting methods, and as a fully unsupervised approach,\nperforms competitively with supervised models. We also show that Maieutic\nPrompting improves robustness in inference while providing interpretable\nrationales.", "published": "2022-05-24 06:36:42", "link": "http://arxiv.org/abs/2205.11822v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lack of Fluency is Hurting Your Translation Model", "abstract": "Many machine translation models are trained on bilingual corpus, which\nconsist of aligned sentence pairs from two different languages with same\nsemantic. However, there is a qualitative discrepancy between train and test\nset in bilingual corpus. While the most train sentences are created via\nautomatic techniques such as crawling and sentence-alignment methods, the test\nsentences are annotated with the consideration of fluency by human. We suppose\nthis discrepancy in training corpus will yield performance drop of translation\nmodel. In this work, we define \\textit{fluency noise} to determine which parts\nof train sentences cause them to seem unnatural. We show that \\textit{fluency\nnoise} can be detected by simple gradient-based method with pre-trained\nclassifier. By removing \\textit{fluency noise} in train sentences, our final\nmodel outperforms the baseline on WMT-14 DE$\\rightarrow$EN and\nRU$\\rightarrow$EN. We also show the compatibility with back-translation\naugmentation, which has been commonly used to improve the fluency of the\ntranslation model. At last, the qualitative analysis of \\textit{fluency noise}\nprovides the insight of what points we should focus on.", "published": "2022-05-24 06:44:17", "link": "http://arxiv.org/abs/2205.11826v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Charon: a FrameNet Annotation Tool for Multimodal Corpora", "abstract": "This paper presents Charon, a web tool for annotating multimodal corpora with\nFrameNet categories. Annotation can be made for corpora containing both static\nimages and video sequences paired - or not - with text sequences. The pipeline\nfeatures, besides the annotation interface, corpus import and pre-processing\ntools.", "published": "2022-05-24 06:58:07", "link": "http://arxiv.org/abs/2205.11836v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lutma: a Frame-Making Tool for Collaborative FrameNet Development", "abstract": "This paper presents Lutma, a collaborative, semi-constrained, tutorial-based\ntool for contributing frames and lexical units to the Global FrameNet\ninitiative. The tool parameterizes the process of frame creation, avoiding\nconsistency violations and promoting the integration of frames contributed by\nthe community with existing frames. Lutma is structured in a wizard-like\nfashion so as to provide users with text and video tutorials relevant for each\nstep in the frame creation process. We argue that this tool will allow for a\nsensible expansion of FrameNet coverage in terms of both languages and cultural\nperspectives encoded by them, positioning frames as a viable alternative for\nrepresenting perspective in language models.", "published": "2022-05-24 07:04:43", "link": "http://arxiv.org/abs/2205.11840v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building a Dialogue Corpus Annotated with Expressed and Experienced\n  Emotions", "abstract": "In communication, a human would recognize the emotion of an interlocutor and\nrespond with an appropriate emotion, such as empathy and comfort. Toward\ndeveloping a dialogue system with such a human-like ability, we propose a\nmethod to build a dialogue corpus annotated with two kinds of emotions. We\ncollect dialogues from Twitter and annotate each utterance with the emotion\nthat a speaker put into the utterance (expressed emotion) and the emotion that\na listener felt after listening to the utterance (experienced emotion). We\nbuilt a dialogue corpus in Japanese using this method, and its statistical\nanalysis revealed the differences between expressed and experienced emotions.\nWe conducted experiments on recognition of the two kinds of emotions. The\nexperimental results indicated the difficulty in recognizing experienced\nemotions and the effectiveness of multi-task learning of the two kinds of\nemotions. We hope that the constructed corpus will facilitate the study on\nemotion recognition in a dialogue and emotion-aware dialogue response\ngeneration.", "published": "2022-05-24 07:40:11", "link": "http://arxiv.org/abs/2205.11867v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accuracy on In-Domain Samples Matters When Building Out-of-Domain\n  detectors: A Reply to Marek et al. (2021)", "abstract": "We have noticed that Marek et al. (2021) try to re-implement our paper Zheng\net al. (2020a) in their work \"OodGAN: Generative Adversarial Network for\nOut-of-Domain Data Generation\". Our paper proposes a model to generate pseudo\nOOD samples that are akin to IN-Domain (IND) input utterances. These pseudo OOD\nsamples can be used to improve the OOD detection performance by optimizing an\nentropy regularization term when building the IND classifier. Marek et al.\n(2021) report a large gap between their re-implemented results and ours on the\nCLINC150 dataset (Larson et al., 2019). This paper discusses some key\nobservations that may have led to such a large gap. Most of these observations\noriginate from our experiments because Marek et al. (2021) have not released\ntheir codes1. One of the most important observations is that stronger IND\nclassifiers usually exhibit a more robust ability to detect OOD samples. We\nhope these observations help other researchers, including Marek et al. (2021),\nto develop better OOD detectors in their applications.", "published": "2022-05-24 08:16:31", "link": "http://arxiv.org/abs/2205.11887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building an Effective Automated Assessment System for C/C++ Introductory\n  Programming Courses in ODL Environment", "abstract": "Assessments help in evaluating the knowledge gained by a learner at any\nspecific point as well as in continuous improvement of the curriculum design\nand the whole learning process. However, with the increase in students'\nenrollment at University level in either conventional or distance education\nenvironment, traditional ways of assessing students' work are becoming\ninsufficient in terms of both time and effort. In distance education\nenvironment, such assessments become additionally more challenging in terms of\nhefty remuneration for hiring large number of tutors. The availability of\nautomated tools to assist the evaluation of students' work and providing\nstudents with appropriate and timely feedback can really help in overcoming\nthese problems. We believe that building such tools for assessing students'\nwork for all kinds of courses in not yet possible. However, courses that\ninvolve some formal language of expression can be automated, such as,\nprogramming courses in Computer Science (CS) discipline. Instructors provide\nvarious practical exercises to students as assignments to build these skills.\nUsually, instructors manually grade and provide feedbacks on these assignments.\nAlthough in literature, various tools have been reported to automate this\nprocess, but most of these tools have been developed by the host institutions\nthemselves for their own use. We at COMSATS Institute of Information\nTechnology, Lahore are conducting a pioneer effort in Pakistan to automate the\nmarking of assignments of introductory programming courses that involve C or\nC++ languages with the capability of associating appropriate feedbacks for\nstudents. In this paper, we basically identify different components that we\nbelieve are necessary in building an effective automated assessment system in\nthe context of introductory programming courses that involve C/C++ programming.", "published": "2022-05-24 09:20:43", "link": "http://arxiv.org/abs/2205.11915v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures\n  of Soft Prompts", "abstract": "This work introduces a new multi-task, parameter-efficient language model\n(LM) tuning method that learns to transfer knowledge across different tasks via\na mixture of soft prompts-small prefix embedding vectors pre-trained for\ndifferent tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt\nTuning), obtains source prompts as encodings of large-scale source tasks into a\nsmall number of parameters and trains an attention module to interpolate the\nsource prompts and a newly initialized target prompt for every instance in the\ntarget task. During training, only the target task prompt and the attention\nweights, which are shared between tasks in multi-task training, are updated,\nwhile the original LM and source prompts are intact. ATTEMPT is highly\nparameter-efficient (e.g., updates 2,300 times fewer parameters than full\nfine-tuning) while achieving high task performance using knowledge from\nhigh-resource tasks. Moreover, it is modular using pre-trained soft prompts,\nand can flexibly add or remove source prompts for effective knowledge transfer.\nOur experimental results across 21 diverse NLP datasets show that ATTEMPT\nsignificantly outperforms prompt tuning and outperforms or matches fully\nfine-tuned or other parameter-efficient tuning approaches that use over ten\ntimes more parameters. Finally, ATTEMPT outperforms previous work in few-shot\nlearning settings.", "published": "2022-05-24 10:48:33", "link": "http://arxiv.org/abs/2205.11961v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmark Data and Evaluation Framework for Intent Discovery Around\n  COVID-19 Vaccine Hesitancy", "abstract": "The COVID-19 pandemic has made a huge global impact and cost millions of\nlives. As COVID-19 vaccines were rolled out, they were quickly met with\nwidespread hesitancy. To address the concerns of hesitant people, we launched\nVIRA, a public dialogue system aimed at addressing questions and concerns\nsurrounding the COVID-19 vaccines. Here, we release VIRADialogs, a dataset of\nover 8k dialogues conducted by actual users with VIRA, providing a unique\nreal-world conversational dataset. In light of rapid changes in users' intents,\ndue to updates in guidelines or in response to new information, we highlight\nthe important task of intent discovery in this use-case. We introduce a novel\nautomatic evaluation framework for intent discovery, leveraging the existing\nintent classifier of VIRA. We use this framework to report baseline intent\ndiscovery results over VIRADialogs, that highlight the difficulty of this task.", "published": "2022-05-24 10:58:11", "link": "http://arxiv.org/abs/2205.11966v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Universal Dependency Treebank for Odia Language", "abstract": "This paper presents the first publicly available treebank of Odia, a\nmorphologically rich low resource Indian language. The treebank contains\napprox. 1082 tokens (100 sentences) in Odia selected from \"Samantar\", the\nlargest available parallel corpora collection for Indic languages. All the\nselected sentences are manually annotated following the ``Universal Dependency\n(UD)\" guidelines. The morphological analysis of the Odia treebank was performed\nusing machine learning techniques. The Odia annotated treebank will enrich the\nOdia language resource and will help in building language technology tools for\ncross-lingual learning and typological research. We also build a preliminary\nOdia parser using a machine learning approach. The accuracy of the parser is\n86.6% Tokenization, 64.1% UPOS, 63.78% XPOS, 42.04% UAS and 21.34% LAS.\nFinally, the paper briefly discusses the linguistic analysis of the Odia UD\ntreebank.", "published": "2022-05-24 11:19:26", "link": "http://arxiv.org/abs/2205.11976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Word-order typology in Multilingual BERT: A case study in\n  subordinate-clause detection", "abstract": "The capabilities and limitations of BERT and similar models are still unclear\nwhen it comes to learning syntactic abstractions, in particular across\nlanguages. In this paper, we use the task of subordinate-clause detection\nwithin and across languages to probe these properties. We show that this task\nis deceptively simple, with easy gains offset by a long tail of harder cases,\nand that BERT's zero-shot performance is dominated by word-order effects,\nmirroring the SVO/VSO/SOV typology.", "published": "2022-05-24 11:35:39", "link": "http://arxiv.org/abs/2205.11987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analysing the Greek Parliament Records with Emotion Classification", "abstract": "In this project, we tackle emotion classification for the Greek language,\npresenting and releasing a new dataset in Greek. We fine-tune and assess\nTransformer-based masked language models that were pre-trained on monolingual\nand multilingual resources, and we present the results per emotion and by\naggregating at the sentiment and subjectivity level. The potential of the\npresented resources is investigated by detecting and studying the emotion of\n`disgust' in the Greek Parliament records. We: (a) locate the months with the\nhighest values from 1989 to present, (b) rank the Greek political parties based\non the presence of this emotion in their speeches, and (c) study the emotional\ncontext shift of words used to stigmatise people.", "published": "2022-05-24 11:55:51", "link": "http://arxiv.org/abs/2205.12012v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked\n  Auto-Encoder", "abstract": "Despite pre-training's progress in many important NLP tasks, it remains to\nexplore effective pre-training strategies for dense retrieval. In this paper,\nwe propose RetroMAE, a new retrieval oriented pre-training paradigm based on\nMasked Auto-Encoder (MAE). RetroMAE is highlighted by three critical designs.\n1) A novel MAE workflow, where the input sentence is polluted for encoder and\ndecoder with different masks. The sentence embedding is generated from the\nencoder's masked input; then, the original sentence is recovered based on the\nsentence embedding and the decoder's masked input via masked language modeling.\n2) Asymmetric model structure, with a full-scale BERT like transformer as\nencoder, and a one-layer transformer as decoder. 3) Asymmetric masking ratios,\nwith a moderate ratio for encoder: 15~30%, and an aggressive ratio for decoder:\n50~70%. Our framework is simple to realize and empirically competitive: the\npre-trained models dramatically improve the SOTA performances on a wide range\nof dense retrieval benchmarks, like BEIR and MS MARCO. The source code and\npre-trained models are made publicly available at\nhttps://github.com/staoxiao/RetroMAE so as to inspire more interesting\nresearch.", "published": "2022-05-24 12:43:04", "link": "http://arxiv.org/abs/2205.12035v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Curious Case of Control", "abstract": "Children acquiring English make systematic errors on subject control\nsentences even after they have reached near-adult competence (C. Chomsky,\n1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).\nGiven the advanced fluency of large generative language models, we ask whether\nmodel outputs are consistent with these heuristics, and to what degree\ndifferent models are consistent with each other. We find that models can be\ncategorized by behavior into three separate groups, with broad differences\nbetween the groups. The outputs of models in the largest group are consistent\nwith positional heuristics that succeed on subject control but fail on object\ncontrol. This result is surprising, given that object control is orders of\nmagnitude more frequent in the text data used to train such models. We examine\nto what degree the models are sensitive to prompting with agent-patient\ninformation, finding that raising the salience of agent and patient relations\nresults in significant changes in the outputs of most models. Based on this\nobservation, we leverage an existing dataset of semantic proto-role annotations\n(White, et al. 2020) to explore the connections between control and labeling\nevent participants with properties typically associated with agents and\npatients.", "published": "2022-05-24 14:45:16", "link": "http://arxiv.org/abs/2205.12113v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer", "abstract": "Massively multilingual models are promising for transfer learning across\ntasks and languages. However, existing methods are unable to fully leverage\ntraining data when it is available in different task-language combinations. To\nexploit such heterogeneous supervision, we propose Hyper-X, a single\nhypernetwork that unifies multi-task and multilingual learning with efficient\nadaptation. This model generates weights for adapter modules conditioned on\nboth tasks and language embeddings. By learning to combine task and\nlanguage-specific knowledge, our model enables zero-shot transfer for unseen\nlanguages and task-language combinations. Our experiments on a diverse set of\nlanguages demonstrate that Hyper-X achieves the best or competitive gain when a\nmixture of multiple resources is available, while being on par with strong\nbaselines in the standard scenario. Hyper-X is also considerably more efficient\nin terms of parameters and resources compared to methods that train separate\nadapters. Finally, Hyper-X consistently produces strong results in few-shot\nscenarios for new languages, showing the versatility of our approach beyond\nzero-shot transfer.", "published": "2022-05-24 15:28:09", "link": "http://arxiv.org/abs/2205.12148v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Dynamic, Interpreted CheckList for Meaning-oriented NLG Metric\n  Evaluation -- through the Lens of Semantic Similarity Rating", "abstract": "Evaluating the quality of generated text is difficult, since traditional NLG\nevaluation metrics, focusing more on surface form than meaning, often fail to\nassign appropriate scores. This is especially problematic for AMR-to-text\nevaluation, given the abstract nature of AMR. Our work aims to support the\ndevelopment and improvement of NLG evaluation metrics that focus on meaning, by\ndeveloping a dynamic CheckList for NLG metrics that is interpreted by being\norganized around meaning-relevant linguistic phenomena. Each test instance\nconsists of a pair of sentences with their AMR graphs and a human-produced\ntextual semantic similarity or relatedness score. Our CheckList facilitates\ncomparative evaluation of metrics and reveals strengths and weaknesses of novel\nand traditional metrics. We demonstrate the usefulness of CheckList by\ndesigning a new metric GraCo that computes lexical cohesion graphs over AMR\nconcepts. Our analysis suggests that GraCo presents an interesting NLG metric\nworth future investigation and that meaning-oriented NLG metrics can profit\nfrom graph-based metric components using AMR.", "published": "2022-05-24 16:19:32", "link": "http://arxiv.org/abs/2205.12176v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Partial-input baselines show that NLI models can ignore context, but\n  they don't", "abstract": "When strong partial-input baselines reveal artifacts in crowdsourced NLI\ndatasets, the performance of full-input models trained on such datasets is\noften dismissed as reliance on spurious correlations. We investigate whether\nstate-of-the-art NLI models are capable of overriding default inferences made\nby a partial-input baseline. We introduce an evaluation set of 600 examples\nconsisting of perturbed premises to examine a RoBERTa model's sensitivity to\nedited contexts. Our results indicate that NLI models are still capable of\nlearning to condition on context--a necessary component of inferential\nreasoning--despite being trained on artifact-ridden datasets.", "published": "2022-05-24 16:27:25", "link": "http://arxiv.org/abs/2205.12181v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "EdiT5: Semi-Autoregressive Text-Editing with T5 Warm-Start", "abstract": "We present EdiT5 - a novel semi-autoregressive text-editing model designed to\ncombine the strengths of non-autoregressive text-editing and autoregressive\ndecoding. EdiT5 is faster during inference than conventional\nsequence-to-sequence (seq2seq) models, while being capable of modelling\nflexible input-output transformations.\n  This is achieved by decomposing the generation process into three sub-tasks:\n(1) tagging to decide on the subset of input tokens to be preserved in the\noutput, (2) re-ordering to define their order in the output text, and (3)\ninsertion to infill the missing tokens that are not present in the input. The\ntagging and re-ordering steps, which are responsible for generating the largest\nportion of the output, are non-autoregressive, while the insertion step uses an\nautoregressive decoder.\n  Depending on the task, EdiT5 on average requires significantly fewer\nautoregressive steps, demonstrating speedups of up to 25x when compared to\nseq2seq models. Quality-wise, EdiT5 is initialized with a pre-trained T5\ncheckpoint yielding comparable performance to T5 in high-resource settings when\nevaluated on three NLG tasks: Sentence Fusion, Grammatical Error Correction,\nand Decontextualization while clearly outperforming T5 in low-resource\nsettings.", "published": "2022-05-24 17:13:22", "link": "http://arxiv.org/abs/2205.12209v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Principled Paraphrase Generation with Parallel Corpora", "abstract": "Round-trip Machine Translation (MT) is a popular choice for paraphrase\ngeneration, which leverages readily available parallel corpora for supervision.\nIn this paper, we formalize the implicit similarity function induced by this\napproach, and show that it is susceptible to non-paraphrase pairs sharing a\nsingle ambiguous translation. Based on these insights, we design an alternative\nsimilarity metric that mitigates this issue by requiring the entire translation\ndistribution to match, and implement a relaxation of it through the Information\nBottleneck method. Our approach incorporates an adversarial term into MT\ntraining in order to learn representations that encode as much information\nabout the reference translation as possible, while keeping as little\ninformation about the input as possible. Paraphrases can be generated by\ndecoding back to the source from this representation, without having to\ngenerate pivot translations. In addition to being more principled and efficient\nthan round-trip MT, our approach offers an adjustable parameter to control the\nfidelity-diversity trade-off, and obtains better results in our experiments.", "published": "2022-05-24 17:22:42", "link": "http://arxiv.org/abs/2205.12213v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DivEMT: Neural Machine Translation Post-Editing Effort Across\n  Typologically Diverse Languages", "abstract": "We introduce DivEMT, the first publicly available post-editing study of\nNeural Machine Translation (NMT) over a typologically diverse set of target\nlanguages. Using a strictly controlled setup, 18 professional translators were\ninstructed to translate or post-edit the same set of English documents into\nArabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process,\ntheir edits, keystrokes, editing times and pauses were recorded, enabling an\nin-depth, cross-lingual evaluation of NMT quality and post-editing\neffectiveness. Using this new dataset, we assess the impact of two\nstate-of-the-art NMT systems, Google Translate and the multilingual mBART-50\nmodel, on translation productivity. We find that post-editing is consistently\nfaster than translation from scratch. However, the magnitude of productivity\ngains varies widely across systems and languages, highlighting major\ndisparities in post-editing effectiveness for languages at different degrees of\ntypological relatedness to English, even when controlling for system\narchitecture and training data size. We publicly release the complete dataset\nincluding all collected behavioral data, to foster new research on the\ntranslation capabilities of NMT systems for typologically diverse languages.", "published": "2022-05-24 17:22:52", "link": "http://arxiv.org/abs/2205.12215v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine\n  Translation", "abstract": "We present a new approach to perform zero-shot cross-modal transfer between\nspeech and text for translation tasks. Multilingual speech and text are encoded\nin a joint fixed-size representation space. Then, we compare different\napproaches to decode these multimodal and multilingual fixed-size\nrepresentations, enabling zero-shot translation between languages and\nmodalities. All our models are trained without the need of cross-modal labeled\ntranslation data. Despite a fixed-size representation, we achieve very\ncompetitive results on several text and speech translation tasks. In\nparticular, we significantly improve the state-of-the-art for zero-shot speech\ntranslation on Must-C. Incorporating a speech decoder in our framework, we\nintroduce the first results for zero-shot direct speech-to-speech and\ntext-to-speech translation.", "published": "2022-05-24 17:23:35", "link": "http://arxiv.org/abs/2205.12216v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ClaimDiff: Comparing and Contrasting Claims on Contentious Issues", "abstract": "With the growing importance of detecting misinformation, many studies have\nfocused on verifying factual claims by retrieving evidence. However, canonical\nfact verification tasks do not apply to catching subtle differences in\nfactually consistent claims, which might still bias the readers, especially on\ncontentious political or economic issues. Our underlying assumption is that\namong the trusted sources, one's argument is not necessarily more true than the\nother, requiring comparison rather than verification. In this study, we propose\nClaimDiff, a novel dataset that primarily focuses on comparing the nuance\nbetween claim pairs. In ClaimDiff, we provide 2,941 annotated claim pairs from\n268 news articles. We observe that while humans are capable of detecting the\nnuances between claims, strong baselines struggle to detect them, showing over\na 19% absolute gap with the humans. We hope this initial study could help\nreaders to gain an unbiased grasp of contentious issues through machine-aided\ncomparison.", "published": "2022-05-24 17:29:13", "link": "http://arxiv.org/abs/2205.12221v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage\n  Natural Language Understanding Systems", "abstract": "In natural language understanding (NLU) production systems, users' evolving\nneeds necessitate the addition of new features over time, indexed by new\nsymbols added to the meaning representation space. This requires additional\ntraining data and results in ever-growing datasets. We present the first\nsystematic investigation of this incremental symbol learning scenario. Our\nanalysis reveals a troubling quirk in building broad-coverage NLU systems: as\nthe training dataset grows, performance on the new symbol often decreases if we\ndo not accordingly increase its training data. This suggests that it becomes\nmore difficult to learn new symbols with a larger training dataset. We show\nthat this trend holds for multiple mainstream models on two common NLU tasks:\nintent recognition and semantic parsing. Rejecting class imbalance as the sole\nculprit, we reveal that the trend is closely associated with an effect we call\nsource signal dilution, where strong lexical cues for the new symbol become\ndiluted as the training dataset grows. Selectively dropping training examples\nto prevent dilution often reverses the trend, showing the over-reliance of\nmainstream neural NLU models on simple lexical cues. Code, models, and data are\navailable at https://aka.ms/nlu-incremental-symbol-learning", "published": "2022-05-24 17:36:27", "link": "http://arxiv.org/abs/2205.12228v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chunk-based Nearest Neighbor Machine Translation", "abstract": "Semi-parametric models, which augment generation with retrieval, have led to\nimpressive results in language modeling and machine translation, due to their\nability to retrieve fine-grained information from a datastore of examples. One\nof the most prominent approaches, $k$NN-MT, exhibits strong domain adaptation\ncapabilities by retrieving tokens from domain-specific datastores\n\\citep{khandelwal2020nearest}. However, $k$NN-MT requires an expensive\nretrieval operation for every single generated token, leading to a very low\ndecoding speed (around 8 times slower than a parametric model). In this paper,\nwe introduce a \\textit{chunk-based} $k$NN-MT model which retrieves chunks of\ntokens from the datastore, instead of a single token. We propose several\nstrategies for incorporating the retrieved chunks into the generation process,\nand for selecting the steps at which the model needs to search for neighbors in\nthe datastore. Experiments on machine translation in two settings, static and\n``on-the-fly'' domain adaptation, show that the chunk-based $k$NN-MT model\nleads to significant speed-ups (up to 4 times) with only a small drop in\ntranslation quality.", "published": "2022-05-24 17:39:25", "link": "http://arxiv.org/abs/2205.12230v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VIRATrustData: A Trust-Annotated Corpus of Human-Chatbot Conversations\n  About COVID-19 Vaccines", "abstract": "Public trust in medical information is crucial for successful application of\npublic health policies such as vaccine uptake. This is especially true when the\ninformation is offered remotely, by chatbots, which have become increasingly\npopular in recent years. Here, we explore the challenging task of human-bot\nturn-level trust classification. We rely on a recently released data of\nobservationally-collected (rather than crowdsourced) dialogs with VIRA chatbot,\na COVID-19 Vaccine Information Resource Assistant. These dialogs are centered\naround questions and concerns about COVID-19 vaccines, where trust is\nparticularly acute. We annotated $3k$ VIRA system-user conversational turns for\nLow Institutional Trust or Low Agent Trust vs. Neutral or High Trust. We\nrelease the labeled dataset, VIRATrustData, the first of its kind to the best\nof our knowledge. We demonstrate how this task is non-trivial and compare\nseveral models that predict the different levels of trust.", "published": "2022-05-24 17:48:04", "link": "http://arxiv.org/abs/2205.12240v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unsupervised Learning of Hierarchical Conversation Structure", "abstract": "Human conversations can evolve in many different ways, creating challenges\nfor automatic understanding and summarization. Goal-oriented conversations\noften have meaningful sub-dialogue structure, but it can be highly\ndomain-dependent. This work introduces an unsupervised approach to learning\nhierarchical conversation structure, including turn and sub-dialogue segment\nlabels, corresponding roughly to dialogue acts and sub-tasks, respectively. The\ndecoded structure is shown to be useful in enhancing neural models of language\nfor three conversation-level understanding tasks. Further, the learned\nfinite-state sub-dialogue network is made interpretable through automatic\nsummarization.", "published": "2022-05-24 17:52:34", "link": "http://arxiv.org/abs/2205.12244v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained\n  Language Models", "abstract": "Recent work has shown that Pre-trained Language Models (PLMs) store the\nrelational knowledge learned from data and utilize it for performing downstream\ntasks. However, commonsense knowledge across different regions may vary. For\ninstance, the color of bridal dress is white in American weddings whereas it is\nred in Chinese weddings. In this paper, we introduce a benchmark dataset,\nGeo-Diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for\nprobing the diversity of the relational knowledge in multilingual PLMs.\nGeoMLAMA contains 3,125 prompts in English, Chinese, Hindi, Persian, and\nSwahili, with a wide coverage of concepts shared by people from American,\nChinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard\nmultilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger\nmultilingual PLMs variants do not necessarily store geo-diverse concepts better\nthan its smaller variant; 2) multilingual PLMs are not intrinsically biased\ntowards knowledge from the Western countries (the United States); 3) the native\nlanguage of a country may not be the best language to probe its knowledge and\n4) a language may better probe knowledge about a non-native country than its\nnative country. Code and data are released at\nhttps://github.com/WadeYin9712/GeoMLAMA.", "published": "2022-05-24 17:54:50", "link": "http://arxiv.org/abs/2205.12247v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Evaluating the Impact of Model Scale for Compositional Generalization in\n  Semantic Parsing", "abstract": "Despite their strong performance on many tasks, pre-trained language models\nhave been shown to struggle on out-of-distribution compositional\ngeneralization. Meanwhile, recent work has shown considerable improvements on\nmany NLP tasks from model scaling. Can scaling up model size also improve\ncompositional generalization in semantic parsing? We evaluate encoder-decoder\nmodels up to 11B parameters and decoder-only models up to 540B parameters, and\ncompare model scaling curves for three different methods for applying a\npre-trained language model to a new task: fine-tuning all parameters, prompt\ntuning, and in-context learning. We observe that fine-tuning generally has flat\nor negative scaling curves on out-of-distribution compositional generalization\nin semantic parsing evaluations. In-context learning has positive scaling\ncurves, but is generally outperformed by much smaller fine-tuned models.\nPrompt-tuning can outperform fine-tuning, suggesting further potential\nimprovements from scaling as it exhibits a more positive scaling curve.\nAdditionally, we identify several error trends that vary with model scale. For\nexample, larger models are generally better at modeling the syntax of the\noutput space, but are also more prone to certain types of overfitting. Overall,\nour study highlights limitations of current techniques for effectively\nleveraging model scale for compositional generalization, while our analysis\nalso suggests promising directions for future work.", "published": "2022-05-24 17:57:39", "link": "http://arxiv.org/abs/2205.12253v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Garden-Path Traversal in GPT-2", "abstract": "In recent years, large-scale transformer decoders such as the GPT-x family of\nmodels have become increasingly popular. Studies examining the behavior of\nthese models tend to focus only on the output of the language modeling head and\navoid analysis of the internal states of the transformer decoder. In this\nstudy, we present a collection of methods to analyze the hidden states of GPT-2\nand use the model's navigation of garden path sentences as a case study. To\nenable this, we compile the largest currently available dataset of garden path\nsentences. We show that Manhattan distances and cosine similarities provide\nmore reliable insights compared to established surprisal methods that analyze\nnext-token probabilities computed by a language modeling head. Using these\nmethods, we find that negating tokens have minimal impacts on the model's\nrepresentations for unambiguous forms of sentences with ambiguity solely over\nwhat the object of a verb is, but have a more substantial impact of\nrepresentations for unambiguous sentences whose ambiguity would stem from the\nvoice of a verb. Further, we find that analyzing the decoder model's hidden\nstates reveals periods of ambiguity that might conclude in a garden path effect\nbut happen not to, whereas surprisal analyses routinely miss this detail.", "published": "2022-05-24 18:21:58", "link": "http://arxiv.org/abs/2205.12302v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Structured Prompt Tuning", "abstract": "We propose structured prompt tuning, a simple and effective method to improve\nprompt tuning. Instead of prepending a sequence of tunable embeddings to the\ninput, we generate the soft prompt embeddings through a hypernetwork. Our\napproach subsumes the standard prompt tuning, allows more flexibility in model\ndesign and can be applied to both single-task and multi-task training settings.\nEmpirically, structured prompt tuning shows a gain of +1.2$~1.5 points on the\nGLUE benchmark and is less sensitive to the change of learning rate, compared\nto standard prompt tuning.", "published": "2022-05-24 18:36:34", "link": "http://arxiv.org/abs/2205.12309v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Scoring Coreference Chains with Split-Antecedent Anaphors", "abstract": "Anaphoric reference is an aspect of language interpretation covering a\nvariety of types of interpretation beyond the simple case of identity reference\nto entities introduced via nominal expressions covered by the traditional\ncoreference task in its most recent incarnation in ONTONOTES and similar\ndatasets. One of these cases that go beyond simple coreference is anaphoric\nreference to entities that must be added to the discourse model via\naccommodation, and in particular split-antecedent references to entities\nconstructed out of other entities, as in split-antecedent plurals and in some\ncases of discourse deixis. Although this type of anaphoric reference is now\nannotated in many datasets, systems interpreting such references cannot be\nevaluated using the Reference coreference scorer Pradhan et al. (2014). As part\nof the work towards a new scorer for anaphoric reference able to evaluate all\naspects of anaphoric interpretation in the coverage of the Universal Anaphora\ninitiative, we propose in this paper a solution to the technical problem of\ngeneralizing existing metrics for identity anaphora so that they can also be\nused to score cases of split-antecedents. This is the first such proposal in\nthe literature on anaphora or coreference, and has been successfully used to\nscore both split-antecedent plural references and discourse deixis in the\nrecent CODI/CRAC anaphora resolution in dialogue shared tasks.", "published": "2022-05-24 19:07:36", "link": "http://arxiv.org/abs/2205.12323v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Medical Scientific Table-to-Text Generation with Human-in-the-Loop under\n  the Data Sparsity Constraint", "abstract": "Structured (tabular) data in the preclinical and clinical domains contains\nvaluable information about individuals and an efficient table-to-text\nsummarization system can drastically reduce manual efforts to condense this\ndata into reports. However, in practice, the problem is heavily impeded by the\ndata paucity, data sparsity and inability of the state-of-the-art natural\nlanguage generation models (including T5, PEGASUS and GPT-Neo) to produce\naccurate and reliable outputs. In this paper, we propose a novel table-to-text\napproach and tackle these problems with a novel two-step architecture which is\nenhanced by auto-correction, copy mechanism and synthetic data augmentation.\nThe study shows that the proposed approach selects salient biomedical entities\nand values from structured data with improved precision (up to 0.13 absolute\nincrease) of copying the tabular values to generate coherent and accurate text\nfor assay validation reports and toxicology reports. Moreover, we also\ndemonstrate a light-weight adaptation of the proposed system to new datasets by\nfine-tuning with as little as 40\\% training examples. The outputs of our model\nare validated by human experts in the Human-in-the-Loop scenario.", "published": "2022-05-24 21:10:57", "link": "http://arxiv.org/abs/2205.12368v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Challenges and Opportunities in Information Manipulation Detection: An\n  Examination of Wartime Russian Media", "abstract": "NLP research on public opinion manipulation campaigns has primarily focused\non detecting overt strategies such as fake news and disinformation. However,\ninformation manipulation in the ongoing Russia-Ukraine war exemplifies how\ngovernments and media also employ more nuanced strategies. We release a new\ndataset, VoynaSlov, containing 38M+ posts from Russian media outlets on Twitter\nand VKontakte, as well as public activity and responses, immediately preceding\nand during the 2022 Russia-Ukraine war. We apply standard and\nrecently-developed NLP models on VoynaSlov to examine agenda setting, framing,\nand priming, several strategies underlying information manipulation, and reveal\nvariation across media outlet control, social media platform, and time. Our\nexamination of these media effects and extensive discussion of current\napproaches' limitations encourage further development of NLP models for\nunderstanding information manipulation in emerging crises, as well as other\nreal-world and interdisciplinary tasks.", "published": "2022-05-24 21:59:10", "link": "http://arxiv.org/abs/2205.12382v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fine-tuned Language Models are Continual Learners", "abstract": "Recent work on large language models relies on the intuition that most\nnatural language processing tasks can be described via natural language\ninstructions. Language models trained on these instructions show strong\nzero-shot performance on several standard datasets. However, these models even\nthough impressive still perform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To address this limitation, we\nargue that a model should be able to keep extending its knowledge and\nabilities, without forgetting previous skills. In spite of the limited success\nof Continual Learning we show that Language Models can be continual learners.\nWe empirically investigate the reason for this success and conclude that\nContinual Learning emerges from self-supervision pre-training. Our resulting\nmodel Continual-T0 (CT0) is able to learn diverse new tasks, while still\nmaintaining good performance on previous tasks, spanning remarkably through 70\ndatasets in total. Finally, we show that CT0 is able to combine instructions in\nways it was never trained for, demonstrating some compositionality.", "published": "2022-05-24 22:53:34", "link": "http://arxiv.org/abs/2205.12393v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MaskEval: Weighted MLM-Based Evaluation for Text Summarization and\n  Simplification", "abstract": "In text summarization and simplification, system outputs must be evaluated\nalong multiple dimensions such as relevance, factual consistency, fluency, and\ngrammaticality, and a wide range of possible outputs could be of high quality.\nThese properties make the development of an adaptable, reference-less\nevaluation metric both necessary and challenging. We introduce MaskEval, a\nreference-less metric for text summarization and simplification that operates\nby performing masked language modeling (MLM) on the concatenation of the\ncandidate and the source texts. It features an attention-like weighting\nmechanism to modulate the relative importance of each MLM step, which crucially\nallows it to be adapted to evaluate different quality dimensions. We\ndemonstrate its effectiveness on English summarization and simplification in\nterms of correlations with human judgments, and explore transfer scenarios\nbetween the two tasks.", "published": "2022-05-24 22:57:16", "link": "http://arxiv.org/abs/2205.12394v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FLUTE: Figurative Language Understanding through Textual Explanations", "abstract": "Figurative language understanding has been recently framed as a recognizing\ntextual entailment (RTE) task (a.k.a. natural language inference, or NLI).\nHowever, similar to classical RTE/NLI datasets, the current benchmarks suffer\nfrom spurious correlations and annotation artifacts. To tackle this problem,\nwork on NLI has built explanation-based datasets such as e-SNLI, allowing us to\nprobe whether language models are right for the right reasons.Yet no such data\nexists for figurative language, making it harder to assess genuine\nunderstanding of such expressions. To address this issue, we release FLUTE, a\ndataset of 9,000 figurative NLI instances with explanations, spanning four\ncategories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through\na model-in-the-loop framework based on GPT-3, crowd workers, and expert\nannotators. We show how utilizing GPT-3 in conjunction with human annotators\n(novices and experts) can aid in scaling up the creation of datasets even for\nsuch complex linguistic phenomena as figurative language. The baseline\nperformance of the T5 model fine-tuned on FLUTE shows that our dataset can\nbring us a step closer to developing models that understand figurative language\nthrough textual explanations.", "published": "2022-05-24 23:25:02", "link": "http://arxiv.org/abs/2205.12404v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Paradigm Change for Formal Syntax: Computational Algorithms in the\n  Grammar of English", "abstract": "Language sciences rely less and less on formal syntax as their base. The\nreason is probably its lack of psychological reality, knowingly avoided.\nPhilosophers of science call for a paradigm shift in which explanations are by\nmechanisms, as in biology. We turned to programming languages as heuristic\nmodels for a process-based syntax of English. The combination of a functional\nword and a content word was chosen as the topic of modeling. Such combinations\nare very frequent, and their output is the important immediate constituents of\nsentences. We found their parallel in Object Oriented Programming where an\nall-methods element serves as an interface, and the content-full element serves\nas its implementation, defining computational objects. The fit of the model was\ntested by deriving three functional characteristics crucial for the algorithm\nand checking their presence in English grammar. We tested the reality of the\ninterface-implementation mechanism on psycholinguistic and neurolinguistic\nevidence concerning processing, development and loss of syntax. The close fit\nand psychological reality of the mechanism suggests that a paradigm shift to an\nalgorithmic theory of syntax is a possibility.", "published": "2022-05-24 07:28:47", "link": "http://arxiv.org/abs/2205.12825v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Auxiliary Task Guided Interactive Attention Model for Question\n  Difficulty Prediction", "abstract": "Online learning platforms conduct exams to evaluate the learners in a\nmonotonous way, where the questions in the database may be classified into\nBloom's Taxonomy as varying levels in complexity from basic knowledge to\nadvanced evaluation. The questions asked in these exams to all learners are\nvery much static. It becomes important to ask new questions with different\ndifficulty levels to each learner to provide a personalized learning\nexperience. In this paper, we propose a multi-task method with an interactive\nattention mechanism, Qdiff, for jointly predicting Bloom's Taxonomy and\ndifficulty levels of academic questions. We model the interaction between the\npredicted bloom taxonomy representations and the input representations using an\nattention mechanism to aid in difficulty prediction. The proposed learning\nmethod would help learn representations that capture the relationship between\nBloom's taxonomy and difficulty labels. The proposed multi-task method learns a\ngood input representation by leveraging the relationship between the related\ntasks and can be used in similar settings where the tasks are related. The\nresults demonstrate that the proposed method performs better than training only\non difficulty prediction. However, Bloom's labels may not always be given for\nsome datasets. Hence we soft label another dataset with a model fine-tuned to\npredict Bloom's labels to demonstrate the applicability of our method to\ndatasets with only difficulty labels.", "published": "2022-05-24 19:55:30", "link": "http://arxiv.org/abs/2207.01494v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "On Advances in Text Generation from Images Beyond Captioning: A Case\n  Study in Self-Rationalization", "abstract": "Combining the visual modality with pretrained language models has been\nsurprisingly effective for simple descriptive tasks such as image captioning.\nMore general text generation however remains elusive. We take a step back and\nask: How do these models work for more complex generative tasks, i.e.\nconditioning on both text and images? Are multimodal models simply visually\nadapted language models, or do they combine they reason jointly over\nmodalities?\n  We investigate these questions in the context of self-rationalization\n(jointly generating task labels/answers and free-text explanations) of three\ntasks: (i) visual question answering in VQA-X, (ii) visual commonsense\nreasoning in VCR, and (iii) visual-textual entailment in e-SNLI-VE. We show\nthat recent unimodal advances, CLIP image representations and scaling of\nlanguage models, do not consistently improve self-rationalization in multimodal\ntasks. We find that no single model type works universally best across tasks,\ndatasets, and finetuning data sizes. Our findings motivate the need for novel\ngeneral backbones approach that move text generation from images and text\nbeyond image captioning.", "published": "2022-05-24 00:52:40", "link": "http://arxiv.org/abs/2205.11686v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "PERT: A New Solution to Pinyin to Character Conversion Task", "abstract": "Pinyin to Character conversion (P2C) task is the key task of Input Method\nEngine (IME) in commercial input software for Asian languages, such as Chinese,\nJapanese, Thai language and so on. It's usually treated as sequence labelling\ntask and resolved by language model, i.e. n-gram or RNN. However, the low\ncapacity of the n-gram or RNN limits its performance. This paper introduces a\nnew solution named PERT which stands for bidirectional Pinyin Encoder\nRepresentations from Transformers. It achieves significant improvement of\nperformance over baselines. Furthermore, we combine PERT with n-gram under a\nMarkov framework, and improve performance further. Lastly, the external lexicon\nis incorporated into PERT so as to resolve the OOD issue of IME.", "published": "2022-05-24 03:08:27", "link": "http://arxiv.org/abs/2205.11737v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A\n  Pilot Study on Named Entity Recognition", "abstract": "Fine-tuning pre-trained language models has recently become a common practice\nin building NLP models for various tasks, especially few-shot tasks. We argue\nthat under the few-shot setting, formulating fine-tuning closer to the\npre-training objectives shall be able to unleash more benefits from the\npre-trained language models. In this work, we take few-shot named entity\nrecognition (NER) for a pilot study, where existing fine-tuning strategies are\nmuch different from pre-training. We propose a novel few-shot fine-tuning\nframework for NER, FFF-NER. Specifically, we introduce three new types of\ntokens, \"is-entity\", \"which-type\" and bracket, so we can formulate the NER\nfine-tuning as (masked) token prediction or generation, depending on the choice\nof pre-trained language models. In our experiments, we apply FFF-NER to\nfine-tune both BERT and BART for few-shot NER on several benchmark datasets and\nobserve significant improvements over existing fine-tuning strategies,\nincluding sequence labeling, prototype meta-learning, and prompt-based\napproaches. We further perform a series of ablation studies, showing few-shot\nNER performance is strongly correlated with the similarity between fine-tuning\nand pre-training.", "published": "2022-05-24 05:36:13", "link": "http://arxiv.org/abs/2205.11799v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WeDef: Weakly Supervised Backdoor Defense for Text Classification", "abstract": "Existing backdoor defense methods are only effective for limited trigger\ntypes. To defend different trigger types at once, we start from the\nclass-irrelevant nature of the poisoning process and propose a novel weakly\nsupervised backdoor defense framework WeDef. Recent advances in weak\nsupervision make it possible to train a reasonably accurate text classifier\nusing only a small number of user-provided, class-indicative seed words. Such\nseed words shall be considered independent of the triggers. Therefore, a weakly\nsupervised text classifier trained by only the poisoned documents without their\nlabels will likely have no backdoor. Inspired by this observation, in WeDef, we\ndefine the reliability of samples based on whether the predictions of the weak\nclassifier agree with their labels in the poisoned training set. We further\nimprove the results through a two-phase sanitization: (1) iteratively refine\nthe weak classifier based on the reliable samples and (2) train a binary poison\nclassifier by distinguishing the most unreliable samples from the most reliable\nsamples. Finally, we train the sanitized model on the samples that the poison\nclassifier predicts as benign. Extensive experiments show that WeDefis\neffective against popular trigger-based attacks (e.g., words, sentences, and\nparaphrases), outperforming existing defense methods.", "published": "2022-05-24 05:53:11", "link": "http://arxiv.org/abs/2205.11803v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Diverse Lottery Tickets Boost Ensemble from a Single Pretrained Model", "abstract": "Ensembling is a popular method used to improve performance as a last resort.\nHowever, ensembling multiple models finetuned from a single pretrained model\nhas been not very effective; this could be due to the lack of diversity among\nensemble members. This paper proposes Multi-Ticket Ensemble, which finetunes\ndifferent subnetworks of a single pretrained model and ensembles them. We\nempirically demonstrated that winning-ticket subnetworks produced more diverse\npredictions than dense networks, and their ensemble outperformed the standard\nensemble on some tasks.", "published": "2022-05-24 06:54:33", "link": "http://arxiv.org/abs/2205.11833v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Community Question Answering Entity Linking via Leveraging Auxiliary\n  Data", "abstract": "Community Question Answering (CQA) platforms contain plenty of CQA texts\n(i.e., questions and answers corresponding to the question) where named\nentities appear ubiquitously. In this paper, we define a new task of CQA entity\nlinking (CQAEL) as linking the textual entity mentions detected from CQA texts\nwith their corresponding entities in a knowledge base. This task can facilitate\nmany downstream applications including expert finding and knowledge base\nenrichment. Traditional entity linking methods mainly focus on linking entities\nin news documents, and are suboptimal over this new task of CQAEL since they\ncannot effectively leverage various informative auxiliary data involved in the\nCQA platform to aid entity linking, such as parallel answers and two types of\nmeta-data (i.e., topic tags and users). To remedy this crucial issue, we\npropose a novel transformer-based framework to effectively harness the\nknowledge delivered by different kinds of auxiliary data to promote the linking\nperformance. We validate the superiority of our framework through extensive\nexperiments over a newly released CQAEL data set against state-of-the-art\nentity linking methods.", "published": "2022-05-24 09:25:18", "link": "http://arxiv.org/abs/2205.11917v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal\n  Skip-connections", "abstract": "Large-scale pretrained foundation models have been an emerging paradigm for\nbuilding artificial intelligence (AI) systems, which can be quickly adapted to\na wide range of downstream tasks. This paper presents mPLUG, a new\nvision-language foundation model for both cross-modal understanding and\ngeneration. Most existing pre-trained models suffer from the problems of low\ncomputational efficiency and information asymmetry brought by the long visual\nsequence in cross-modal alignment. To address these problems, mPLUG introduces\nan effective and efficient vision-language architecture with novel cross-modal\nskip-connections, which creates inter-layer shortcuts that skip a certain\nnumber of layers for time-consuming full self-attention on the vision side.\nmPLUG is pre-trained end-to-end on large-scale image-text pairs with both\ndiscriminative and generative objectives. It achieves state-of-the-art results\non a wide range of vision-language downstream tasks, such as image captioning,\nimage-text retrieval, visual grounding and visual question answering. mPLUG\nalso demonstrates strong zero-shot transferability when directly transferred to\nmultiple video-language tasks.", "published": "2022-05-24 11:52:06", "link": "http://arxiv.org/abs/2205.12005v2", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Classification of Phonological Parameters in Sign Languages", "abstract": "Signers compose sign language phonemes that enable communication by combining\nphonological parameters such as handshape, orientation, location, movement, and\nnon-manual features. Linguistic research often breaks down signs into their\nconstituent parts to study sign languages and often a lot of effort is invested\ninto the annotation of the videos. In this work we show how a single model can\nbe used to recognise the individual phonological parameters within sign\nlanguages with the aim of either to assist linguistic annotations or to\ndescribe the signs for the sign recognition models. We use Danish Sign Language\ndata set `Ordbog over Dansk Tegnsprog' to generate multiple data sets using\npose estimation model, which are then used for training the multi-label Fast\nR-CNN model to support multi-label modelling. Moreover, we show that there is a\nsignificant co-dependence between the orientation and location phonological\nparameters in the generated data and we incorporate this co-dependence in the\nmodel to achieve better performance.", "published": "2022-05-24 13:40:45", "link": "http://arxiv.org/abs/2205.12072v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "GraphQ IR: Unifying the Semantic Parsing of Graph Query Languages with\n  One Intermediate Representation", "abstract": "Subject to the huge semantic gap between natural and formal languages, neural\nsemantic parsing is typically bottlenecked by its complexity of dealing with\nboth input semantics and output syntax. Recent works have proposed several\nforms of supplementary supervision but none is generalized across multiple\nformal languages. This paper proposes a unified intermediate representation\n(IR) for graph query languages, named GraphQ IR. It has a natural-language-like\nexpression that bridges the semantic gap and formally defined syntax that\nmaintains the graph structure. Therefore, a neural semantic parser can more\nprecisely convert user queries into GraphQ IR, which can be later losslessly\ncompiled into various downstream graph query languages. Extensive experiments\non several benchmarks including KQA Pro, Overnight, GrailQA, and MetaQA-Cypher\nunder standard i.i.d., out-of-distribution, and low-resource settings validate\nGraphQ IR's superiority over the previous state-of-the-arts with a maximum 11%\naccuracy improvement.", "published": "2022-05-24 13:59:53", "link": "http://arxiv.org/abs/2205.12078v2", "categories": ["cs.CL", "cs.DB"], "primary_category": "cs.CL"}
{"title": "HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text\n  Retrieval", "abstract": "In the past few years, the emergence of vision-language pre-training (VLP)\nhas brought cross-modal retrieval to a new era. However, due to the latency and\ncomputation demand, it is commonly challenging to apply VLP in a real-time\nonline retrieval system. To alleviate the defect, this paper proposes a\n\\textbf{Hi}erarchical \\textbf{V}ision-\\textbf{}Language \\textbf{P}re-Training\n(\\textbf{HiVLP}) for fast Image-Text Retrieval (ITR). Specifically, we design a\nnovel hierarchical retrieval objective, which uses the representation of\ndifferent dimensions for coarse-to-fine ITR, i.e., using low-dimensional\nrepresentation for large-scale coarse retrieval and high-dimensional\nrepresentation for small-scale fine retrieval. We evaluate our proposed HiVLP\non two popular image-text retrieval benchmarks, i.e., Flickr30k and COCO.\nExtensive experiments demonstrate that our HiVLP not only has fast inference\nspeed but also can be easily scaled to large-scale ITR scenarios. The detailed\nresults show that HiVLP is $1,427$$\\sim$$120,649\\times$ faster than the\nfusion-based model UNITER and 2$\\sim$5 faster than the fastest embedding-based\nmodel LightingDot in different candidate scenarios. It also achieves about +4.9\nAR on COCO and +3.8 AR on Flickr30K than LightingDot and achieves comparable\nperformance with the state-of-the-art (SOTA) fusion-based model METER.", "published": "2022-05-24 14:32:57", "link": "http://arxiv.org/abs/2205.12105v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised\n  Poetry Generation", "abstract": "Formal verse poetry imposes strict constraints on the meter and rhyme scheme\nof poems. Most prior work on generating this type of poetry uses existing poems\nfor supervision, which are difficult to obtain for most languages and poetic\nforms. In this work, we propose an unsupervised approach to generate poems\nfollowing any given meter and rhyme scheme, without requiring any poetic text\nfor training. Our method works by splitting a regular, non-poetic corpus into\nphrases, prepending control codes that describe the length and end rhyme of\neach phrase, and training a transformer language model in the augmented corpus.\nDuring inference, we build control codes for the desired meter and rhyme\nscheme, and condition our language model on them to generate formal verse\npoetry. Experiments in Spanish and Basque show that our approach is able to\ngenerate valid poems, which are often comparable in quality to those written by\nhumans.", "published": "2022-05-24 17:09:55", "link": "http://arxiv.org/abs/2205.12206v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RevUp: Revise and Update Information Bottleneck for Event Representation", "abstract": "The existence of external (``side'') semantic knowledge has been shown to\nresult in more expressive computational event models. To enable the use of side\ninformation that may be noisy or missing, we propose a semi-supervised\ninformation bottleneck-based discrete latent variable model. We reparameterize\nthe model's discrete variables with auxiliary continuous latent variables and a\nlight-weight hierarchical structure. Our model is learned to minimize the\nmutual information between the observed data and optional side knowledge that\nis not already captured by the new, auxiliary variables. We theoretically show\nthat our approach generalizes past approaches, and perform an empirical case\nstudy of our approach on event modeling. We corroborate our theoretical results\nwith strong empirical experiments, showing that the proposed method outperforms\nprevious proposed approaches on multiple datasets.", "published": "2022-05-24 17:54:59", "link": "http://arxiv.org/abs/2205.12248v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Interpretation Quality Score for Measuring the Quality of\n  interpretability methods", "abstract": "Machine learning (ML) models have been applied to a wide range of natural\nlanguage processing (NLP) tasks in recent years. In addition to making accurate\ndecisions, the necessity of understanding how models make their decisions has\nbecome apparent in many applications. To that end, many interpretability\nmethods that help explain the decision processes of ML models have been\ndeveloped. Yet, there currently exists no widely-accepted metric to evaluate\nthe quality of explanations generated by these methods. As a result, there\ncurrently is no standard way of measuring to what degree an interpretability\nmethod achieves an intended objective. Moreover, there is no accepted standard\nof performance by which we can compare and rank the current existing\ninterpretability methods. In this paper, we propose a novel metric for\nquantifying the quality of explanations generated by interpretability methods.\nWe compute the metric on three NLP tasks using six interpretability methods and\npresent our results.", "published": "2022-05-24 17:57:55", "link": "http://arxiv.org/abs/2205.12254v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "TALM: Tool Augmented Language Models", "abstract": "Transformer based language models (LMs) demonstrate increasing performance\nwith scale across a wide variety of tasks. Scale alone however cannot enable\nmodels to solve tasks that require access to ephemeral, changing, or private\ndata that was unavailable at training time. Many useful tasks may also benefit\nfrom LMs being able to access APIs that read or modify state. In this work, we\npresent Tool Augmented Language Models (TALM), combining a text-only approach\nto augment language models with non-differentiable tools, and an iterative\n\"self-play\" technique to bootstrap performance starting from few tool\ndemonstrations. TALM exhibits strong performance on both a knowledge-heavy QA\ntask and a reasoning oriented math task with simple tools. At a given model\nscale, TALM significantly outperforms non-augmented LMs. We further demonstrate\nthat TALM successfully performs out-of-distribution inferences on both QA and\nmath tasks, where non-augmented LMs fail. Our results suggest that Tool\nAugmented Language Models are a promising direction to enrich LMs'\ncapabilities, with less dependence on scale.", "published": "2022-05-24 17:58:13", "link": "http://arxiv.org/abs/2205.12255v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Policy Compliance Detection via Expression Tree Inference", "abstract": "Policy Compliance Detection (PCD) is a task we encounter when reasoning over\ntexts, e.g. legal frameworks. Previous work to address PCD relies heavily on\nmodeling the task as a special case of Recognizing Textual Entailment.\nEntailment is applicable to the problem of PCD, however viewing the policy as a\nsingle proposition, as opposed to multiple interlinked propositions, yields\npoor performance and lacks explainability. To address this challenge, more\nrecent proposals for PCD have argued for decomposing policies into expression\ntrees consisting of questions connected with logic operators. Question\nanswering is used to obtain answers to these questions with respect to a\nscenario. Finally, the expression tree is evaluated in order to arrive at an\noverall solution. However, this work assumes expression trees are provided by\nexperts, thus limiting its applicability to new policies. In this work, we\nlearn how to infer expression trees automatically from policy texts. We ensure\nthe validity of the inferred trees by introducing constrained decoding using a\nfinite state automaton to ensure the generation of valid trees. We determine\nthrough automatic evaluation that 63% of the expression trees generated by our\nconstrained generation model are logically equivalent to gold trees. Human\nevaluation shows that 88% of trees generated by our model are correct.", "published": "2022-05-24 17:59:31", "link": "http://arxiv.org/abs/2205.12259v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilevel sentiment analysis in arabic", "abstract": "In this study, we aimed to improve the performance results of Arabic\nsentiment analysis. This can be achieved by investigating the most successful\nmachine learning method and the most useful feature vector to classify\nsentiments in both term and document levels into two (positive or negative)\ncategories. Moreover, specification of one polarity degree for the term that\nhas more than one is investigated. Also to handle the negations and\nintensifications, some rules are developed. According to the obtained results,\nArtificial Neural Network classifier is nominated as the best classifier in\nboth term and document level sentiment analysis (SA) for Arabic Language.\nFurthermore, the average F-score achieved in the term level SA for both\npositive and negative testing classes is 0.92. In the document level SA, the\naverage F-score for positive testing classes is 0.94, while for negative\nclasses is 0.93.", "published": "2022-05-24 19:16:06", "link": "http://arxiv.org/abs/2205.12328v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "K-12BERT: BERT for K-12 education", "abstract": "Online education platforms are powered by various NLP pipelines, which\nutilize models like BERT to aid in content curation. Since the inception of the\npre-trained language models like BERT, there have also been many efforts toward\nadapting these pre-trained models to specific domains. However, there has not\nbeen a model specifically adapted for the education domain (particularly K-12)\nacross subjects to the best of our knowledge. In this work, we propose to train\na language model on a corpus of data curated by us across multiple subjects\nfrom various sources for K-12 education. We also evaluate our model, K12-BERT,\non downstream tasks like hierarchical taxonomy tagging.", "published": "2022-05-24 19:35:41", "link": "http://arxiv.org/abs/2205.12335v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to Model Editing Processes", "abstract": "Most existing sequence generation models produce outputs in one pass, usually\nleft-to-right. However, this is in contrast with a more natural approach that\nhumans use in generating content; iterative refinement and editing. Recent work\nhas introduced edit-based models for various tasks (such as neural machine\ntranslation and text style transfer), but these generally model a single edit\nstep. In this work, we propose modeling editing processes, modeling the whole\nprocess of iteratively generating sequences. We form a conceptual framework to\ndescribe the likelihood of multi-step edits, and describe neural models that\ncan learn a generative model of sequences based on these multistep edits. We\nintroduce baseline results and metrics on this task, finding that modeling\nediting processes improves performance on a variety of axes on both our\nproposed task and related downstream tasks compared to previous single-step\nmodels of edits.", "published": "2022-05-24 21:32:52", "link": "http://arxiv.org/abs/2205.12374v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Toxicity Detection with Generative Prompt-based Inference", "abstract": "Due to the subtleness, implicity, and different possible interpretations\nperceived by different people, detecting undesirable content from text is a\nnuanced difficulty. It is a long-known risk that language models (LMs), once\ntrained on corpus containing undesirable content, have the power to manifest\nbiases and toxicity. However, recent studies imply that, as a remedy, LMs are\nalso capable of identifying toxic content without additional fine-tuning.\nPrompt-methods have been shown to effectively harvest this surprising\nself-diagnosing capability. However, existing prompt-based methods usually\nspecify an instruction to a language model in a discriminative way. In this\nwork, we explore the generative variant of zero-shot prompt-based toxicity\ndetection with comprehensive trials on prompt engineering. We evaluate on three\ndatasets with toxicity labels annotated on social media posts. Our analysis\nhighlights the strengths of our generative classification approach both\nquantitatively and qualitatively. Interesting aspects of self-diagnosis and its\nethical implications are discussed.", "published": "2022-05-24 22:44:43", "link": "http://arxiv.org/abs/2205.12390v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Toward Understanding Bias Correlations for Mitigation in NLP", "abstract": "Natural Language Processing (NLP) models have been found discriminative\nagainst groups of different social identities such as gender and race. With the\nnegative consequences of these undesired biases, researchers have responded\nwith unprecedented effort and proposed promising approaches for bias\nmitigation. In spite of considerable practical importance, current algorithmic\nfairness literature lacks an in-depth understanding of the relations between\ndifferent forms of biases. Social bias is complex by nature. Numerous studies\nin social psychology identify the \"generalized prejudice\", i.e., generalized\ndevaluing sentiments across different groups. For example, people who devalue\nethnic minorities are also likely to devalue women and gays. Therefore, this\nwork aims to provide a first systematic study toward understanding bias\ncorrelations in mitigation. In particular, we examine bias mitigation in two\ncommon NLP tasks -- toxicity detection and word embeddings -- on three social\nidentities, i.e., race, gender, and religion. Our findings suggest that biases\nare correlated and present scenarios in which independent debiasing approaches\ndominant in current literature may be insufficient. We further investigate\nwhether jointly mitigating correlated biases is more desired than independent\nand individual debiasing. Lastly, we shed light on the inherent issue of\ndebiasing-accuracy trade-off in bias mitigation. This study serves to motivate\nfuture research on joint bias mitigation that accounts for correlated biases.", "published": "2022-05-24 22:48:47", "link": "http://arxiv.org/abs/2205.12391v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emergent Communication through Metropolis-Hastings Naming Game with Deep\n  Generative Models", "abstract": "Constructive studies on symbol emergence systems seek to investigate\ncomputational models that can better explain human language evolution, the\ncreation of symbol systems, and the construction of internal representations.\nThis study provides a new model for emergent communication, which is based on a\nprobabilistic generative model (PGM) instead of a discriminative model based on\ndeep reinforcement learning. We define the Metropolis-Hastings (MH) naming game\nby generalizing previously proposed models. It is not a referential game with\nexplicit feedback, as assumed by many emergent communication studies. Instead,\nit is a game based on joint attention without explicit feedback.\nMathematically, the MH naming game is proved to be a type of MH algorithm for\nan integrative PGM that combines two agents that play the naming game. From\nthis viewpoint, symbol emergence is regarded as decentralized Bayesian\ninference, and semiotic communication is regarded as inter-personal cross-modal\ninference. This notion leads to the collective predictive coding hypothesis}\nregarding language evolution and, in general, the emergence of symbols. We also\npropose the inter-Gaussian mixture model (GMM)+ variational autoencoder (VAE),\na deep generative model for emergent communication based on the MH naming game.\nThe model has been validated on MNIST and Fruits 360 datasets. Experimental\nfindings demonstrate that categories are formed from real images observed by\nagents, and signs are correctly shared across agents by successfully utilizing\nboth of the observations of agents via the MH naming game. Furthermore,\nscholars verified that visual images were recalled from signs uttered by\nagents. Notably, emergent communication without supervision and reward feedback\nimproved the performance of the unsupervised representation learning of agents.", "published": "2022-05-24 22:49:53", "link": "http://arxiv.org/abs/2205.12392v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural\n  Networks", "abstract": "Learning effective recipe representations is essential in food studies.\nUnlike what has been developed for image-based recipe retrieval or learning\nstructural text embeddings, the combined effect of multi-modal information\n(i.e., recipe images, text, and relation data) receives less attention. In this\npaper, we formalize the problem of multi-modal recipe representation learning\nto integrate the visual, textual, and relational information into recipe\nembeddings. In particular, we first present Large-RG, a new recipe graph data\nwith over half a million nodes, making it the largest recipe graph to date. We\nthen propose Recipe2Vec, a novel graph neural network based recipe embedding\nmodel to capture multi-modal information. Additionally, we introduce an\nadversarial attack strategy to ensure stable learning and improve performance.\nFinally, we design a joint objective function of node classification and\nadversarial learning to optimize the model. Extensive experiments demonstrate\nthat Recipe2Vec outperforms state-of-the-art baselines on two classic food\nstudy tasks, i.e., cuisine category classification and region prediction.\nDataset and codes are available at https://github.com/meettyj/Recipe2Vec.", "published": "2022-05-24 23:04:02", "link": "http://arxiv.org/abs/2205.12396v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT", "abstract": "We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the\nspeed and stability of linear, mixing transformations to design the Sparse\nMixer encoder model. Sparse Mixer slightly outperforms (<1%) BERT on GLUE and\nSuperGLUE, but more importantly trains 65% faster and runs inference 61%\nfaster. We also present a faster variant, prosaically named Fast Sparse Mixer,\nthat marginally underperforms BERT on SuperGLUE, but trains and runs nearly\ntwice as fast. We justify the design of these two models by carefully ablating\nthrough various mixing mechanisms, MoE configurations and hyperparameters.\nSparse Mixer overcomes many of the latency and stability concerns of MoE models\nand offers the prospect of serving sparse student models, without resorting to\ndistilling them to dense variants.", "published": "2022-05-24 23:08:54", "link": "http://arxiv.org/abs/2205.12399v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Linear Connectivity Reveals Generalization Strategies", "abstract": "It is widely accepted in the mode connectivity literature that when two\nneural networks are trained similarly on the same data, they are connected by a\npath through parameter space over which test set accuracy is maintained. Under\nsome circumstances, including transfer learning from pretrained models, these\npaths are presumed to be linear. In contrast to existing results, we find that\namong text classifiers (trained on MNLI, QQP, and CoLA), some pairs of\nfinetuned models have large barriers of increasing loss on the linear paths\nbetween them. On each task, we find distinct clusters of models which are\nlinearly connected on the test loss surface, but are disconnected from models\noutside the cluster -- models that occupy separate basins on the surface. By\nmeasuring performance on specially-crafted diagnostic datasets, we find that\nthese clusters correspond to different generalization strategies: one cluster\nbehaves like a bag of words model under domain shift, while another cluster\nuses syntactic heuristics. Our work demonstrates how the geometry of the loss\nsurface can guide models towards different heuristic functions.", "published": "2022-05-24 23:43:02", "link": "http://arxiv.org/abs/2205.12411v5", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "On the Role of Bidirectionality in Language Model Pre-Training", "abstract": "Prior work on language model pre-training has explored different\narchitectures and learning objectives, but differences in data, hyperparameters\nand evaluation make a principled comparison difficult. In this work, we focus\non bidirectionality as a key factor that differentiates existing approaches,\nand present a comprehensive study of its role in next token prediction, text\ninfilling, zero-shot priming and fine-tuning. We propose a new framework that\ngeneralizes prior approaches, including fully unidirectional models like GPT,\nfully bidirectional models like BERT, and hybrid models like CM3 and prefix LM.\nOur framework distinguishes between two notions of bidirectionality\n(bidirectional context and bidirectional attention) and allows us to control\neach of them separately. We find that the optimal configuration is largely\napplication-dependent (e.g., bidirectional attention is beneficial for\nfine-tuning and infilling, but harmful for next token prediction and zero-shot\npriming). We train models with up to 6.7B parameters, and find differences to\nremain consistent at scale. While prior work on scaling has focused on\nleft-to-right autoregressive models, our results suggest that this approach\ncomes with some trade-offs, and it might be worthwhile to develop very large\nbidirectional models.", "published": "2022-05-24 02:25:05", "link": "http://arxiv.org/abs/2205.11726v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "BabyBear: Cheap inference triage for expensive language models", "abstract": "Transformer language models provide superior accuracy over previous models\nbut they are computationally and environmentally expensive. Borrowing the\nconcept of model cascading from computer vision, we introduce BabyBear, a\nframework for cascading models for natural language processing (NLP) tasks to\nminimize cost. The core strategy is inference triage, exiting early when the\nleast expensive model in the cascade achieves a sufficiently high-confidence\nprediction. We test BabyBear on several open source data sets related to\ndocument classification and entity recognition. We find that for common NLP\ntasks a high proportion of the inference load can be accomplished with cheap,\nfast models that have learned by observing a deep learning model. This allows\nus to reduce the compute cost of large-scale classification jobs by more than\n50% while retaining overall accuracy. For named entity recognition, we save 33%\nof the deep learning compute while maintaining an F1 score higher than 95% on\nthe CoNLL benchmark.", "published": "2022-05-24 03:21:07", "link": "http://arxiv.org/abs/2205.11747v1", "categories": ["cs.CL", "cs.LG", "cs.PF"], "primary_category": "cs.CL"}
{"title": "Overview of STEM Science as Process, Method, Material, and Data Named\n  Entities", "abstract": "We are faced with an unprecedented production in scholarly publications\nworldwide. Stakeholders in the digital libraries posit that the document-based\npublishing paradigm has reached the limits of adequacy. Instead, structured,\nmachine-interpretable, fine-grained scholarly knowledge publishing as Knowledge\nGraphs (KG) is strongly advocated. In this work, we develop and analyze a\nlarge-scale structured dataset of STEM articles across 10 different\ndisciplines, viz. Agriculture, Astronomy, Biology, Chemistry, Computer Science,\nEarth Science, Engineering, Material Science, Mathematics, and Medicine. Our\nanalysis is defined over a large-scale corpus comprising 60K abstracts\nstructured as four scientific entities process, method, material, and data.\nThus our study presents, for the first-time, an analysis of a large-scale\nmultidisciplinary corpus under the construct of four named entity labels that\nare specifically defined and selected to be domain-independent as opposed to\ndomain-specific. The work is then inadvertently a feasibility test of\ncharacterizing multidisciplinary science with domain-independent concepts.\nFurther, to summarize the distinct facets of scientific knowledge per concept\nper discipline, a set of word cloud visualizations are offered. The\nSTEM-NER-60k corpus, created in this work, comprises over 1M extracted entities\nfrom 60k STEM articles obtained from a major publishing platform and is\npublicly released https://github.com/jd-coderepos/stem-ner-60k.", "published": "2022-05-24 07:35:24", "link": "http://arxiv.org/abs/2205.11863v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Large Language Models are Zero-Shot Reasoners", "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.", "published": "2022-05-24 09:22:26", "link": "http://arxiv.org/abs/2205.11916v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "The Authenticity Gap in Human Evaluation", "abstract": "Human ratings are the gold standard in NLG evaluation. The standard protocol\nis to collect ratings of generated text, average across annotators, and rank\nNLG systems by their average scores. However, little consideration has been\ngiven as to whether this approach faithfully captures human preferences.\nAnalyzing this standard protocol through the lens of utility theory in\neconomics, we identify the implicit assumptions it makes about annotators.\nThese assumptions are often violated in practice, in which case annotator\nratings cease to reflect their preferences. The most egregious violations come\nfrom using Likert scales, which provably reverse the direction of the true\npreference in certain cases. We suggest improvements to the standard protocol\nto make it more theoretically sound, but even in its improved form, it cannot\nbe used to evaluate open-ended tasks like story generation. For the latter, we\npropose a new human evaluation protocol called $\\textit{system-level\nprobabilistic assessment}$ (SPA). When human evaluation of stories is done with\nSPA, we can recover the ordering of GPT-3 models by size, with statistically\nsignificant results. However, when human evaluation is done with the standard\nprotocol, less than half of the expected preferences can be recovered (e.g.,\nthere is no significant difference between $\\texttt{curie}$ and\n$\\texttt{davinci}$, despite using a highly powered test).", "published": "2022-05-24 09:51:27", "link": "http://arxiv.org/abs/2205.11930v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-Level Modeling Units for End-to-End Mandarin Speech Recognition", "abstract": "The choice of modeling units is crucial for automatic speech recognition\n(ASR) tasks. In mandarin scenarios, the Chinese characters represent meaning\nbut are not directly related to the pronunciation. Thus only considering the\nwriting of Chinese characters as modeling units is insufficient to capture\nspeech features. In this paper, we present a novel method involves with\nmulti-level modeling units, which integrates multi-level information for\nmandarin speech recognition. Specifically, the encoder block considers\nsyllables as modeling units and the decoder block deals with character-level\nmodeling units. To facilitate the incremental conversion from syllable features\nto character features, we design an auxiliary task that applies cross-entropy\n(CE) loss to intermediate decoder layers. During inference, the input feature\nsequences are converted into syllable sequences by the encoder block and then\nconverted into Chinese characters by the decoder block. Experiments on the\nwidely used AISHELL-1 corpus demonstrate that our method achieves promising\nresults with CER of 4.1%/4.6% and 4.6%/5.2%, using the Conformer and the\nTransformer backbones respectively.", "published": "2022-05-24 11:43:54", "link": "http://arxiv.org/abs/2205.11998v3", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Enhancing Continual Learning with Global Prototypes: Counteracting\n  Negative Representation Drift", "abstract": "Continual learning (CL) aims to learn a sequence of tasks over time, with\ndata distributions shifting from one task to another. When training on new task\ndata, data representations from old tasks may drift. Some negative\nrepresentation drift can result in catastrophic forgetting, by causing the\nlocally learned class prototypes and data representations to correlate poorly\nacross tasks. To mitigate such representation drift, we propose a method that\nfinds global prototypes to guide the learning, and learns data representations\nwith the regularization of the self-supervised information. Specifically, for\nNLP tasks, we formulate each task in a masked language modeling style, and\nlearn the task via a neighbor attention mechanism over a pre-trained language\nmodel. Experimental results show that our proposed method can learn fairly\nconsistent representations with less representation drift, and significantly\nreduce catastrophic forgetting in CL without resampling data from past tasks.", "published": "2022-05-24 16:41:30", "link": "http://arxiv.org/abs/2205.12186v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reassessing Evaluation Practices in Visual Question Answering: A Case\n  Study on Out-of-Distribution Generalization", "abstract": "Vision-and-language (V&L) models pretrained on large-scale multimodal data\nhave demonstrated strong performance on various tasks such as image captioning\nand visual question answering (VQA). The quality of such models is commonly\nassessed by measuring their performance on unseen data that typically comes\nfrom the same distribution as the training data. However, when evaluated under\nout-of-distribution (out-of-dataset) settings for VQA, we observe that these\nmodels exhibit poor generalization. We comprehensively evaluate two pretrained\nV&L models under different settings (i.e. classification and open-ended text\ngeneration) by conducting cross-dataset evaluations. We find that these models\ntend to learn to solve the benchmark, rather than learning the high-level\nskills required by the VQA task. We also find that in most cases generative\nmodels are less susceptible to shifts in data distribution compared to\ndiscriminative ones, and that multimodal pretraining is generally helpful for\nOOD generalization. Finally, we revisit assumptions underlying the use of\nautomatic VQA evaluation metrics, and empirically show that their stringent\nnature repeatedly penalizes models for correct responses.", "published": "2022-05-24 16:44:45", "link": "http://arxiv.org/abs/2205.12191v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of\n  Angela Merkel's Weekly Video Podcasts", "abstract": "We introduce the Merkel Podcast Corpus, an audio-visual-text corpus in German\ncollected from 16 years of (almost) weekly Internet podcasts of former German\nchancellor Angela Merkel. To the best of our knowledge, this is the first\nsingle speaker corpus in the German language consisting of audio, visual and\ntext modalities of comparable size and temporal extent. We describe the methods\nused with which we have collected and edited the data which involves\ndownloading the videos, transcripts and other metadata, forced alignment,\nperforming active speaker recognition and face detection to finally curate the\nsingle speaker dataset consisting of utterances spoken by Angela Merkel. The\nproposed pipeline is general and can be used to curate other datasets of\nsimilar nature, such as talk show contents. Through various statistical\nanalyses and applications of the dataset in talking face generation and TTS, we\nshow the utility of the dataset. We argue that it is a valuable contribution to\nthe research community, in particular, due to its realistic and challenging\nmaterial at the boundary between prepared and spontaneous speech.", "published": "2022-05-24 16:48:07", "link": "http://arxiv.org/abs/2205.12194v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Aerial Vision-and-Dialog Navigation", "abstract": "The ability to converse with humans and follow natural language commands is\ncrucial for intelligent unmanned aerial vehicles (a.k.a. drones). It can\nrelieve people's burden of holding a controller all the time, allow\nmultitasking, and make drone control more accessible for people with\ndisabilities or with their hands occupied. To this end, we introduce Aerial\nVision-and-Dialog Navigation (AVDN), to navigate a drone via natural language\nconversation. We build a drone simulator with a continuous photorealistic\nenvironment and collect a new AVDN dataset of over 3k recorded navigation\ntrajectories with asynchronous human-human dialogs between commanders and\nfollowers. The commander provides initial navigation instruction and further\nguidance by request, while the follower navigates the drone in the simulator\nand asks questions when needed. During data collection, followers' attention on\nthe drone's visual observation is also recorded. Based on the AVDN dataset, we\nstudy the tasks of aerial navigation from (full) dialog history and propose an\neffective Human Attention Aided Transformer model (HAA-Transformer), which\nlearns to predict both navigation waypoints and human attention.", "published": "2022-05-24 17:28:14", "link": "http://arxiv.org/abs/2205.12219v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "History Compression via Language Models in Reinforcement Learning", "abstract": "In a partially observable Markov decision process (POMDP), an agent typically\nuses a representation of the past to approximate the underlying MDP. We propose\nto utilize a frozen Pretrained Language Transformer (PLT) for history\nrepresentation and compression to improve sample efficiency. To avoid training\nof the Transformer, we introduce FrozenHopfield, which automatically associates\nobservations with pretrained token embeddings. To form these associations, a\nmodern Hopfield network stores these token embeddings, which are retrieved by\nqueries that are obtained by a random but fixed projection of observations. Our\nnew method, HELM, enables actor-critic network architectures that contain a\npretrained language Transformer for history representation as a memory module.\nSince a representation of the past need not be learned, HELM is much more\nsample efficient than competitors. On Minigrid and Procgen environments HELM\nachieves new state-of-the-art results. Our code is available at\nhttps://github.com/ml-jku/helm.", "published": "2022-05-24 17:59:29", "link": "http://arxiv.org/abs/2205.12258v4", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Adaptive multilingual speech recognition with pretrained models", "abstract": "Multilingual speech recognition with supervised learning has achieved great\nresults as reflected in recent research. With the development of pretraining\nmethods on audio and text data, it is imperative to transfer the knowledge from\nunsupervised multilingual models to facilitate recognition, especially in many\nlanguages with limited data. Our work investigated the effectiveness of using\ntwo pretrained models for two modalities: wav2vec 2.0 for audio and MBART50 for\ntext, together with the adaptive weight techniques to massively improve the\nrecognition quality on the public datasets containing CommonVoice and Europarl.\nOverall, we noticed an 44% improvement over purely supervised learning, and\nmore importantly, each technique provides a different reinforcement in\ndifferent languages. We also explore other possibilities to potentially obtain\nthe best model by slightly adding either depth or relative attention to the\narchitecture.", "published": "2022-05-24 18:29:07", "link": "http://arxiv.org/abs/2205.12304v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Certified Robustness Against Natural Language Attacks by Causal\n  Intervention", "abstract": "Deep learning models have achieved great success in many fields, yet they are\nvulnerable to adversarial examples. This paper follows a causal perspective to\nlook into the adversarial vulnerability and proposes Causal Intervention by\nSemantic Smoothing (CISS), a novel framework towards robustness against natural\nlanguage attacks. Instead of merely fitting observational data, CISS learns\ncausal effects p(y|do(x)) by smoothing in the latent semantic space to make\nrobust predictions, which scales to deep architectures and avoids tedious\nconstruction of noise customized for specific attacks. CISS is provably robust\nagainst word substitution attacks, as well as empirically robust even when\nperturbations are strengthened by unknown attack algorithms. For example, on\nYELP, CISS surpasses the runner-up by 6.7% in terms of certified robustness\nagainst word substitutions, and achieves 79.4% empirical robustness when\nsyntactic attacks are integrated.", "published": "2022-05-24 19:20:48", "link": "http://arxiv.org/abs/2205.12331v3", "categories": ["cs.LG", "cs.CL", "cs.CR"], "primary_category": "cs.LG"}
{"title": "PLAtE: A Large-scale Dataset for List Page Web Extraction", "abstract": "Recently, neural models have been leveraged to significantly improve the\nperformance of information extraction from semi-structured websites. However, a\nbarrier for continued progress is the small number of datasets large enough to\ntrain these models. In this work, we introduce the PLAtE (Pages of Lists\nAttribute Extraction) benchmark dataset as a challenging new web extraction\ntask. PLAtE focuses on shopping data, specifically extractions from product\nreview pages with multiple items encompassing the tasks of: (1) finding\nproduct-list segmentation boundaries and (2) extracting attributes for each\nproduct. PLAtE is composed of 52, 898 items collected from 6, 694 pages and\n156, 014 attributes, making it the first largescale list page web extraction\ndataset. We use a multi-stage approach to collect and annotate the dataset and\nadapt three state-of-the-art web extraction models to the two tasks comparing\ntheir strengths and weaknesses both quantitatively and qualitatively.", "published": "2022-05-24 22:26:58", "link": "http://arxiv.org/abs/2205.12386v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning", "abstract": "Standard fine-tuning of large pre-trained language models (PLMs) for\ndownstream tasks requires updating hundreds of millions to billions of\nparameters, and storing a large copy of the PLM weights for every task\nresulting in increased cost for storing, sharing and serving the models. To\naddress this, parameter-efficient fine-tuning (PEFT) techniques were introduced\nwhere small trainable components are injected in the PLM and updated during\nfine-tuning. We propose AdaMix as a general PEFT method that tunes a mixture of\nadaptation modules -- given the underlying PEFT method of choice -- introduced\nin each Transformer layer while keeping most of the PLM weights frozen. For\ninstance, AdaMix can leverage a mixture of adapters like Houlsby or a mixture\nof low rank decomposition matrices like LoRA to improve downstream task\nperformance over the corresponding PEFT methods for fully supervised and\nfew-shot NLU and NLG tasks. Further, we design AdaMix such that it matches the\nsame computational cost and the number of tunable parameters as the underlying\nPEFT method. By only tuning 0.1-0.2% of PLM parameters, we show that AdaMix\noutperforms SOTA parameter-efficient fine-tuning and full model fine-tuning for\nboth NLU and NLG tasks.", "published": "2022-05-24 23:41:22", "link": "http://arxiv.org/abs/2205.12410v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Symbol Emergence as Inter-personal Categorization with Head-to-head\n  Latent Word", "abstract": "In this study, we propose a head-to-head type (H2H-type) inter-personal\nmultimodal Dirichlet mixture (Inter-MDM) by modifying the original Inter-MDM,\nwhich is a probabilistic generative model that represents the symbol emergence\nbetween two agents as multiagent multimodal categorization. A\nMetropolis--Hastings method-based naming game based on the Inter-MDM enables\ntwo agents to collaboratively perform multimodal categorization and share signs\nwith a solid mathematical foundation of convergence. However, the conventional\nInter-MDM presumes a tail-to-tail connection across a latent word variable,\ncausing inflexibility of the further extension of Inter-MDM for modeling a more\ncomplex symbol emergence. Therefore, we propose herein a head-to-head type\n(H2H-type) Inter-MDM that treats a latent word variable as a child node of an\ninternal variable of each agent in the same way as many prior studies of\nmultimodal categorization. On the basis of the H2H-type Inter-MDM, we propose a\nnaming game in the same way as the conventional Inter-MDM. The experimental\nresults show that the H2H-type Inter-MDM yields almost the same performance as\nthe conventional Inter-MDM from the viewpoint of multimodal categorization and\nsign sharing.", "published": "2022-05-24 23:18:00", "link": "http://arxiv.org/abs/2205.15027v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "FabKG: A Knowledge graph of Manufacturing Science domain utilizing\n  structured and unconventional unstructured knowledge source", "abstract": "As the demands for large-scale information processing have grown, knowledge\ngraph-based approaches have gained prominence for representing general and\ndomain knowledge. The development of such general representations is essential,\nparticularly in domains such as manufacturing which intelligent processes and\nadaptive education can enhance. Despite the continuous accumulation of text in\nthese domains, the lack of structured data has created information extraction\nand knowledge transfer barriers. In this paper, we report on work towards\ndeveloping robust knowledge graphs based upon entity and relation data for both\ncommercial and educational uses. To create the FabKG (Manufacturing knowledge\ngraph), we have utilized textbook index words, research paper keywords, FabNER\n(manufacturing NER), to extract a sub knowledge base contained within Wikidata.\nMoreover, we propose a novel crowdsourcing method for KG creation by leveraging\nstudent notes, which contain invaluable information but are not captured as\nmeaningful information, excluding their use in personal preparation for\nlearning and written exams. We have created a knowledge graph containing 65000+\ntriples using all data sources. We have also shown the use case of\ndomain-specific question answering and expression/formula-based question\nanswering for educational purposes.", "published": "2022-05-24 02:32:04", "link": "http://arxiv.org/abs/2206.10318v1", "categories": ["cs.CL", "cs.AI", "cs.SC"], "primary_category": "cs.CL"}
{"title": "Singer Identification for Metaverse with Timbral and Middle-Level\n  Perceptual Features", "abstract": "Metaverse is an interactive world that combines reality and virtuality, where\nparticipants can be virtual avatars. Anyone can hold a concert in a virtual\nconcert hall, and users can quickly identify the real singer behind the virtual\nidol through the singer identification. Most singer identification methods are\nprocessed using the frame-level features. However, expect the singer's timbre,\nthe music frame includes music information, such as melodiousness, rhythm, and\ntonal. It means the music information is noise for using frame-level features\nto identify the singers. In this paper, instead of only the frame-level\nfeatures, we propose to use another two features that address this problem.\nMiddle-level feature, which represents the music's melodiousness, rhythmic\nstability, and tonal stability, and is able to capture the perceptual features\nof music. The timbre feature, which is used in speaker identification,\nrepresents the singers' voice features. Furthermore, we propose a convolutional\nrecurrent neural network (CRNN) to combine three features for singer\nidentification. The model firstly fuses the frame-level feature and timbre\nfeature and then combines middle-level features to the mix features. In\nexperiments, the proposed method achieves comparable performance on an average\nF1 score of 0.81 on the benchmark dataset of Artist20, which significantly\nimproves related works.", "published": "2022-05-24 06:30:50", "link": "http://arxiv.org/abs/2205.11817v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MetaSID: Singer Identification with Domain Adaptation for Metaverse", "abstract": "Metaverse has stretched the real world into unlimited space. There will be\nmore live concerts in Metaverse. The task of singer identification is to\nidentify the song belongs to which singer. However, there has been a tough\nproblem in singer identification, which is the different live effects. The\nstudio version is different from the live version, the data distribution of the\ntraining set and the test set are different, and the performance of the\nclassifier decreases. This paper proposes the use of the domain adaptation\nmethod to solve the live effect in singer identification. Three methods of\ndomain adaptation combined with Convolutional Recurrent Neural Network (CRNN)\nare designed, which are Maximum Mean Discrepancy (MMD), gradient reversal\n(Revgrad), and Contrastive Adaptation Network (CAN). MMD is a distance-based\nmethod, which adds domain loss. Revgrad is based on the idea that learned\nfeatures can represent different domain samples. CAN is based on class\nadaptation, it takes into account the correspondence between the categories of\nthe source domain and target domain. Experimental results on the public dataset\nof Artist20 show that CRNN-MMD leads to an improvement over the baseline CRNN\nby 0.14. The CRNN-RevGrad outperforms the baseline by 0.21. The CRNN-CAN\nachieved state of the art with the F1 measure value of 0.83 on album split.", "published": "2022-05-24 06:36:23", "link": "http://arxiv.org/abs/2205.11821v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "TDASS: Target Domain Adaptation Speech Synthesis Framework for\n  Multi-speaker Low-Resource TTS", "abstract": "Recently, synthesizing personalized speech by text-to-speech (TTS)\napplication is highly demanded. But the previous TTS models require a mass of\ntarget speaker speeches for training. It is a high-cost task, and hard to\nrecord lots of utterances from the target speaker. Data augmentation of the\nspeeches is a solution but leads to the low-quality synthesis speech problem.\nSome multi-speaker TTS models are proposed to address the issue. But the\nquantity of utterances of each speaker imbalance leads to the voice similarity\nproblem. We propose the Target Domain Adaptation Speech Synthesis Network\n(TDASS) to address these issues. Based on the backbone of the Tacotron2 model,\nwhich is the high-quality TTS model, TDASS introduces a self-interested\nclassifier for reducing the non-target influence. Besides, a special gradient\nreversal layer with different operations for target and non-target is added to\nthe classifier. We evaluate the model on a Chinese speech corpus, the\nexperiments show the proposed method outperforms the baseline method in terms\nof voice quality and voice similarity.", "published": "2022-05-24 06:41:05", "link": "http://arxiv.org/abs/2205.11824v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SUSing: SU-net for Singing Voice Synthesis", "abstract": "Singing voice synthesis is a generative task that involves multi-dimensional\ncontrol of the singing model, including lyrics, pitch, and duration, and\nincludes the timbre of the singer and singing skills such as vibrato. In this\npaper, we proposed SU-net for singing voice synthesis named SUSing.\nSynthesizing singing voice is treated as a translation task between lyrics and\nmusic score and spectrum. The lyrics and music score information is encoded\ninto a two-dimensional feature representation through the convolution layer.\nThe two-dimensional feature and its frequency spectrum are mapped to the target\nspectrum in an autoregressive manner through a SU-net network. Within the\nSU-net the stripe pooling method is used to replace the alternate global\npooling method to learn the vertical frequency relationship in the spectrum and\nthe changes of frequency in the time domain. The experimental results on the\npublic dataset Kiritan show that the proposed method can synthesize more\nnatural singing voices.", "published": "2022-05-24 07:05:10", "link": "http://arxiv.org/abs/2205.11841v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive Few-Shot Learning Algorithm for Rare Sound Event Detection", "abstract": "Sound event detection is to infer the event by understanding the surrounding\nenvironmental sounds. Due to the scarcity of rare sound events, it becomes\nchallenging for the well-trained detectors which have learned too much prior\nknowledge. Meanwhile, few-shot learning methods promise a good generalization\nability when facing a new limited-data task. Recent approaches have achieved\npromising results in this field. However, these approaches treat each support\nexample independently, ignoring the information of other examples from the\nwhole task. Because of this, most of previous methods are constrained to\ngenerate a same feature embedding for all test-time tasks, which is not\nadaptive to each inputted data. In this work, we propose a novel task-adaptive\nmodule which is easy to plant into any metric-based few-shot learning\nframeworks. The module could identify the task-relevant feature dimension.\nIncorporating our module improves the performance considerably on two datasets\nover baseline methods, especially for the transductive propagation network.\nSuch as +6.8% for 5-way 1-shot accuracy on ESC-50, and +5.9% on noiseESC-50. We\ninvestigate our approach in the domain-mismatch setting and also achieve better\nresults than previous methods.", "published": "2022-05-24 03:13:12", "link": "http://arxiv.org/abs/2205.11738v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Deep Learning-based automated classification of Chinese Speech Sound\n  Disorders", "abstract": "This article describes a system for analyzing acoustic data to assist in the\ndiagnosis and classification of children's speech sound disorders (SSDs) using\na computer. The analysis concentrated on identifying and categorizing four\ndistinct types of Chinese SSDs. The study collected and generated a speech\ncorpus containing 2540 stopping, backing, final consonant deletion process\n(FCDP), and affrication samples from 90 children aged 3--6 years with normal or\npathological articulatory features. Each recording was accompanied by a\ndetailed diagnostic annotation by two speech-language pathologists (SLPs).\nClassification of the speech samples was accomplished using three\nwell-established neural network models for image classification. The feature\nmaps were created using three sets of Mel-frequency cepstral coefficients\n(MFCC) parameters extracted from speech sounds and aggregated into a\nthree-dimensional data structure as model input. We employed six techniques for\ndata augmentation to augment the available dataset while avoiding overfitting.\nThe experiments examine the usability of four different categories of Chinese\nphrases and characters. Experiments with different data subsets demonstrate the\nsystem's ability to accurately detect the analyzed pronunciation disorders. The\nbest multi-class classification using a single Chinese phrase achieves an\naccuracy of 74.4~percent.", "published": "2022-05-24 03:23:22", "link": "http://arxiv.org/abs/2205.11748v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "SepIt: Approaching a Single Channel Speech Separation Bound", "abstract": "We present an upper bound for the Single Channel Speech Separation task,\nwhich is based on an assumption regarding the nature of short segments of\nspeech. Using the bound, we are able to show that while the recent methods have\nmade significant progress for a few speakers, there is room for improvement for\nfive and ten speakers. We then introduce a Deep neural network, SepIt, that\niteratively improves the different speakers' estimation. At test time, SpeIt\nhas a varying number of iterations per test sample, based on a mutual\ninformation criterion that arises from our analysis. In an extensive set of\nexperiments, SepIt outperforms the state-of-the-art neural networks for 2, 3,\n5, and 10 speakers.", "published": "2022-05-24 05:40:36", "link": "http://arxiv.org/abs/2205.11801v4", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "primary_category": "eess.AS"}
{"title": "Defending a Music Recommender Against Hubness-Based Adversarial Attacks", "abstract": "Adversarial attacks can drastically degrade performance of recommenders and\nother machine learning systems, resulting in an increased demand for defence\nmechanisms. We present a new line of defence against attacks which exploit a\nvulnerability of recommenders that operate in high dimensional data spaces (the\nso-called hubness problem). We use a global data scaling method, namely Mutual\nProximity (MP), to defend a real-world music recommender which previously was\nsusceptible to attacks that inflated the number of times a particular song was\nrecommended. We find that using MP as a defence greatly increases robustness of\nthe recommender against a range of attacks, with success rates of attacks\naround 44% (before defence) dropping to less than 6% (after defence).\nAdditionally, adversarial examples still able to fool the defended system do so\nat the price of noticeably lower audio quality as shown by a decreased average\nSNR.", "published": "2022-05-24 12:31:20", "link": "http://arxiv.org/abs/2205.12032v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Boosting Tail Neural Network for Realtime Custom Keyword Spotting", "abstract": "In this paper, we propose a Boosting Tail Neural Network (BTNN) for improving\nthe performance of Realtime Custom Keyword Spotting (RCKS) that is still an\nindustrial challenge for demanding powerful classification ability with limited\ncomputation resources. Inspired by Brain Science that a brain is only partly\nactivated for a nerve simulation and numerous machine learning algorithms are\ndeveloped to use a batch of weak classifiers to resolve arduous problems, which\nare often proved to be effective. We show that this method is helpful to the\nRCKS problem. The proposed approach achieve better performances in terms of\nwakeup rate and false alarm.\n  In our experiments compared with those traditional algorithms that use only\none strong classifier, it gets 18\\% relative improvement. We also point out\nthat this approach may be promising in future ASR exploration.", "published": "2022-05-24 13:26:39", "link": "http://arxiv.org/abs/2205.12933v2", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
