{"title": "EmotionX-DLC: Self-Attentive BiLSTM for Detecting Sequential Emotions in\n  Dialogue", "abstract": "In this paper, we propose a self-attentive bidirectional long short-term\nmemory (SA-BiLSTM) network to predict multiple emotions for the EmotionX\nchallenge. The BiLSTM exhibits the power of modeling the word dependencies, and\nextracting the most relevant features for emotion classification. Building on\ntop of BiLSTM, the self-attentive network can model the contextual dependencies\nbetween utterances which are helpful for classifying the ambiguous emotions. We\nachieve 59.6 and 55.0 unweighted accuracy scores in the \\textit{Friends} and\nthe \\textit{EmotionPush} test sets, respectively.", "published": "2018-06-19 05:03:23", "link": "http://arxiv.org/abs/1806.07039v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Response Generation by Context-aware Prototype Editing", "abstract": "Open domain response generation has achieved remarkable progress in recent\nyears, but sometimes yields short and uninformative responses. We propose a new\nparadigm for response generation, that is response generation by editing, which\nsignificantly increases the diversity and informativeness of the generation\nresults. Our assumption is that a plausible response can be generated by\nslightly revising an existing response prototype. The prototype is retrieved\nfrom a pre-defined index and provides a good start-point for generation because\nit is grammatical and informative. We design a response editing model, where an\nedit vector is formed by considering differences between a prototype context\nand a current context, and then the edit vector is fed to a decoder to revise\nthe prototype response for the current context. Experiment results on a large\nscale dataset demonstrate that the response editing model outperforms\ngenerative and retrieval-based models on various aspects.", "published": "2018-06-19 05:13:34", "link": "http://arxiv.org/abs/1806.07042v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Learning from Chunk-based Feedback in Neural Machine Translation", "abstract": "We empirically investigate learning from partial feedback in neural machine\ntranslation (NMT), when partial feedback is collected by asking users to\nhighlight a correct chunk of a translation. We propose a simple and effective\nway of utilizing such feedback in NMT training. We demonstrate how the common\nmachine translation problem of domain mismatch between training and deployment\ncan be reduced solely based on chunk-level user feedback. We conduct a series\nof simulation experiments to test the effectiveness of the proposed method. Our\nresults show that chunk-level feedback outperforms sentence based feedback by\nup to 2.61% BLEU absolute.", "published": "2018-06-19 11:56:22", "link": "http://arxiv.org/abs/1806.07169v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Recurrent DNNs and its Ensembles on the TIMIT Phone Recognition Task", "abstract": "In this paper, we have investigated recurrent deep neural networks (DNNs) in\ncombination with regularization techniques as dropout, zoneout, and\nregularization post-layer. As a benchmark, we chose the TIMIT phone recognition\ntask due to its popularity and broad availability in the community. It also\nsimulates a low-resource scenario that is helpful in minor languages. Also, we\nprefer the phone recognition task because it is much more sensitive to an\nacoustic model quality than a large vocabulary continuous speech recognition\ntask. In recent years, recurrent DNNs pushed the error rates in automatic\nspeech recognition down. But, there was no clear winner in proposed\narchitectures. The dropout was used as the regularization technique in most\ncases, but combination with other regularization techniques together with model\nensembles was omitted. However, just an ensemble of recurrent DNNs performed\nbest and achieved an average phone error rate from 10 experiments 14.84 %\n(minimum 14.69 %) on core test set that is slightly lower then the\nbest-published PER to date, according to our knowledge. Finally, in contrast of\nthe most papers, we published the open-source scripts to easily replicate the\nresults and to help continue the development.", "published": "2018-06-19 12:39:22", "link": "http://arxiv.org/abs/1806.07186v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Syntactically Constrained Bidirectional-Asynchronous Approach for\n  Emotional Conversation Generation", "abstract": "Traditional neural language models tend to generate generic replies with poor\nlogic and no emotion. In this paper, a syntactically constrained\nbidirectional-asynchronous approach for emotional conversation generation\n(E-SCBA) is proposed to address this issue. In our model, pre-generated emotion\nkeywords and topic keywords are asynchronously introduced into the process of\ndecoding. It is much different from most existing methods which generate\nreplies from the first word to the last. Through experiments, the results\nindicate that our approach not only improves the diversity of replies, but\ngains a boost on both logic and emotion compared with baselines.", "published": "2018-06-19 01:28:58", "link": "http://arxiv.org/abs/1806.07000v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Using J-K fold Cross Validation to Reduce Variance When Tuning NLP\n  Models", "abstract": "K-fold cross validation (CV) is a popular method for estimating the true\nperformance of machine learning models, allowing model selection and parameter\ntuning. However, the very process of CV requires random partitioning of the\ndata and so our performance estimates are in fact stochastic, with variability\nthat can be substantial for natural language processing tasks. We demonstrate\nthat these unstable estimates cannot be relied upon for effective parameter\ntuning. The resulting tuned parameters are highly sensitive to how our data is\npartitioned, meaning that we often select sub-optimal parameter choices and\nhave serious reproducibility issues.\n  Instead, we propose to use the less variable J-K-fold CV, in which J\nindependent K-fold cross validations are used to assess performance. Our main\ncontributions are extending J-K-fold CV from performance estimation to\nparameter tuning and investigating how to choose J and K. We argue that\nvariability is more important than bias for effective tuning and so advocate\nlower choices of K than are typically seen in the NLP literature, instead use\nthe saved computation to increase J. To demonstrate the generality of our\nrecommendations we investigate a wide range of case-studies: sentiment\nclassification (both general and target-specific), part-of-speech tagging and\ndocument classification.", "published": "2018-06-19 10:12:25", "link": "http://arxiv.org/abs/1806.07139v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Self-adaptive Privacy Concern Detection for User-generated Content", "abstract": "To protect user privacy in data analysis, a state-of-the-art strategy is\ndifferential privacy in which scientific noise is injected into the real\nanalysis output. The noise masks individual's sensitive information contained\nin the dataset. However, determining the amount of noise is a key challenge,\nsince too much noise will destroy data utility while too little noise will\nincrease privacy risk. Though previous research works have designed some\nmechanisms to protect data privacy in different scenarios, most of the existing\nstudies assume uniform privacy concerns for all individuals. Consequently,\nputting an equal amount of noise to all individuals leads to insufficient\nprivacy protection for some users, while over-protecting others. To address\nthis issue, we propose a self-adaptive approach for privacy concern detection\nbased on user personality. Our experimental studies demonstrate the\neffectiveness to address a suitable personalized privacy protection for\ncold-start users (i.e., without their privacy-concern information in training\ndata).", "published": "2018-06-19 13:40:27", "link": "http://arxiv.org/abs/1806.07221v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Joint Neural Entity Disambiguation with Output Space Search", "abstract": "In this paper, we present a novel model for entity disambiguation that\ncombines both local contextual information and global evidences through Limited\nDiscrepancy Search (LDS). Given an input document, we start from a complete\nsolution constructed by a local model and conduct a search in the space of\npossible corrections to improve the local solution from a global view point.\nOur search utilizes a heuristic function to focus more on the least confident\nlocal decisions and a pruning function to score the global solutions based on\ntheir local fitness and the global coherences among the predicted entities.\nExperimental results on CoNLL 2003 and TAC 2010 benchmarks verify the\neffectiveness of our model.", "published": "2018-06-19 23:05:18", "link": "http://arxiv.org/abs/1806.07495v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Survey of Recent DNN Architectures on the TIMIT Phone Recognition Task", "abstract": "In this survey paper, we have evaluated several recent deep neural network\n(DNN) architectures on a TIMIT phone recognition task. We chose the TIMIT\ncorpus due to its popularity and broad availability in the community. It also\nsimulates a low-resource scenario that is helpful in minor languages. Also, we\nprefer the phone recognition task because it is much more sensitive to an\nacoustic model quality than a large vocabulary continuous speech recognition\n(LVCSR) task. In recent years, many DNN published papers reported results on\nTIMIT. However, the reported phone error rates (PERs) were often much higher\nthan a PER of a simple feed-forward (FF) DNN. That was the main motivation of\nthis paper: To provide a baseline DNNs with open-source scripts to easily\nreplicate the baseline results for future papers with lowest possible PERs.\nAccording to our knowledge, the best-achieved PER of this survey is better than\nthe best-published PER to date.", "published": "2018-06-19 12:43:41", "link": "http://arxiv.org/abs/1806.07974v1", "categories": ["cs.CL", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Private Text Classification", "abstract": "Confidential text corpora exist in many forms, but do not allow arbitrary\nsharing. We explore how to use such private corpora using privacy preserving\ntext analytics. We construct typical text processing applications using\nappropriate privacy preservation techniques (including homomorphic encryption,\nRademacher operators and secure computation). We set out the preliminary\nmaterials from Rademacher operators for binary classifiers, and then construct\nbasic text processing approaches to match those binary classifiers.", "published": "2018-06-19 01:24:32", "link": "http://arxiv.org/abs/1806.06998v1", "categories": ["cs.CL", "cs.CR", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A New COLD Feature based Handwriting Analysis for Ethnicity/Nationality\n  Identification", "abstract": "Identifying crime for forensic investigating teams when crimes involve people\nof different nationals is challenging. This paper proposes a new method for\nethnicity (nationality) identification based on Cloud of Line Distribution\n(COLD) features of handwriting components. The proposed method, at first,\nexplores tangent angle for the contour pixels in each row and the mean of\nintensity values of each row in an image for segmenting text lines. For\nsegmented text lines, we use tangent angle and direction of base lines to\nremove rule lines in the image. We use polygonal approximation for finding\ndominant points for contours of edge components. Then the proposed method\nconnects the nearest dominant points of every dominant point, which results in\nline segments of dominant point pairs. For each line segment, the proposed\nmethod estimates angle and length, which gives a point in polar domain. For all\nthe line segments, the proposed method generates dense points in polar domain,\nwhich results in COLD distribution. As character component shapes change,\naccording to nationals, the shape of the distribution changes. This observation\nis extracted based on distance from pixels of distribution to Principal Axis of\nthe distribution. Then the features are subjected to an SVM classifier for\nidentifying nationals. Experiments are conducted on a complex dataset, which\nshow the proposed method is effective and outperforms the existing method", "published": "2018-06-19 07:14:54", "link": "http://arxiv.org/abs/1806.07072v1", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.CL"], "primary_category": "cs.CV"}
{"title": "End-to-End Speech Recognition From the Raw Waveform", "abstract": "State-of-the-art speech recognition systems rely on fixed, hand-crafted\nfeatures such as mel-filterbanks to preprocess the waveform before the training\npipeline. In this paper, we study end-to-end systems trained directly from the\nraw waveform, building on two alternatives for trainable replacements of\nmel-filterbanks that use a convolutional architecture. The first one is\ninspired by gammatone filterbanks (Hoshen et al., 2015; Sainath et al, 2015),\nand the second one by the scattering transform (Zeghidour et al., 2017). We\npropose two modifications to these architectures and systematically compare\nthem to mel-filterbanks, on the Wall Street Journal dataset. The first\nmodification is the addition of an instance normalization layer, which greatly\nimproves on the gammatone-based trainable filterbanks and speeds up the\ntraining of the scattering-based filterbanks. The second one relates to the\nlow-pass filter used in these approaches. These modifications consistently\nimprove performances for both approaches, and remove the need for a careful\ninitialization in scattering-based trainable filterbanks. In particular, we\nshow a consistent improvement in word error rate of the trainable filterbanks\nrelatively to comparable mel-filterbanks. It is the first time end-to-end\nmodels trained from the raw signal significantly outperform mel-filterbanks on\na large vocabulary task under clean recording conditions.", "published": "2018-06-19 08:32:49", "link": "http://arxiv.org/abs/1806.07098v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Dynamic Multi-Level Multi-Task Learning for Sentence Simplification", "abstract": "Sentence simplification aims to improve readability and understandability,\nbased on several operations such as splitting, deletion, and paraphrasing.\nHowever, a valid simplified sentence should also be logically entailed by its\ninput sentence. In this work, we first present a strong pointer-copy mechanism\nbased sequence-to-sequence sentence simplification model, and then improve its\nentailment and paraphrasing capabilities via multi-task learning with related\nauxiliary tasks of entailment and paraphrase generation. Moreover, we propose a\nnovel 'multi-level' layered soft sharing approach where each auxiliary task\nshares different (higher versus lower) level layers of the sentence\nsimplification model, depending on the task's semantic versus lexico-syntactic\nnature. We also introduce a novel multi-armed bandit based training approach\nthat dynamically learns how to effectively switch across tasks during\nmulti-task learning. Experiments on multiple popular datasets demonstrate that\nour model outperforms competitive simplification systems in SARI and FKGL\nautomatic metrics, and human evaluation. Further, we present several ablation\nanalyses on alternative layer sharing methods, soft versus hard sharing,\ndynamic multi-armed bandit sampling approaches, and our model's learned\nentailment and paraphrasing skills.", "published": "2018-06-19 15:21:37", "link": "http://arxiv.org/abs/1806.07304v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Speaker Adapted Beamforming for Multi-Channel Automatic Speech\n  Recognition", "abstract": "This paper presents, in the context of multi-channel ASR, a method to adapt a\nmask based, statistically optimal beamforming approach to a speaker of\ninterest. The beamforming vector of the statistically optimal beamformer is\ncomputed by utilizing speech and noise masks, which are estimated by a neural\nnetwork. The proposed adaptation approach is based on the integration of the\nbeamformer, which includes the mask estimation network, and the acoustic model\nof the ASR system. This allows for the propagation of the training error, from\nthe acoustic modeling cost function, all the way through the beamforming\noperation and through the mask estimation network. By using the results of a\nfirst pass recognition and by keeping all other parameters fixed, the mask\nestimation network can therefore be fine tuned by retraining. Utterances of a\nspeaker of interest can thus be used in a two pass approach, to optimize the\nbeamforming for the speech characteristics of that specific speaker. It is\nshown that this approach improves the ASR performance of a state-of-the-art\nmulti-channel ASR system on the CHiME-4 data. Furthermore the effect of the\nadaptation on the estimated speech masks is discussed.", "published": "2018-06-19 18:03:33", "link": "http://arxiv.org/abs/1806.07407v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "A Simple Fusion of Deep and Shallow Learning for Acoustic Scene\n  Classification", "abstract": "In the past, Acoustic Scene Classification systems have been based on hand\ncrafting audio features that are input to a classifier. Nowadays, the common\ntrend is to adopt data driven techniques, e.g., deep learning, where audio\nrepresentations are learned from data. In this paper, we propose a system that\nconsists of a simple fusion of two methods of the aforementioned types: a deep\nlearning approach where log-scaled mel-spectrograms are input to a\nconvolutional neural network, and a feature engineering approach, where a\ncollection of hand-crafted features is input to a gradient boosting machine. We\nfirst show that both methods provide complementary information to some extent.\nThen, we use a simple late fusion strategy to combine both methods. We report\nclassification accuracy of each method individually and the combined system on\nthe TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms\neach of the individual methods and attains a classification accuracy of 72.8%\non the evaluation set, improving the baseline system by 11.8%.", "published": "2018-06-19 23:42:54", "link": "http://arxiv.org/abs/1806.07506v2", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
