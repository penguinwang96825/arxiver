{"title": "QANA: LLM-based Question Generation and Network Analysis for Zero-shot\n  Key Point Analysis and Beyond", "abstract": "The proliferation of social media has led to information overload and\nincreased interest in opinion mining. We propose \"Question-Answering Network\nAnalysis\" (QANA), a novel opinion mining framework that utilizes Large Language\nModels (LLMs) to generate questions from users' comments, constructs a\nbipartite graph based on the comments' answerability to the questions, and\napplies centrality measures to examine the importance of opinions. We\ninvestigate the impact of question generation styles, LLM selections, and the\nchoice of embedding model on the quality of the constructed QA networks by\ncomparing them with annotated Key Point Analysis datasets. QANA achieves\ncomparable performance to previous state-of-the-art supervised models in a\nzero-shot manner for Key Point Matching task, also reducing the computational\ncost from quadratic to linear. For Key Point Generation, questions with high\nPageRank or degree centrality align well with manually annotated key points.\nNotably, QANA enables analysts to assess the importance of key points from\nvarious aspects according to their selection of centrality measure. QANA's\nprimary contribution lies in its flexibility to extract key points from a wide\nrange of perspectives, which enhances the quality and impartiality of opinion\nmining.", "published": "2024-04-29 02:17:31", "link": "http://arxiv.org/abs/2404.18371v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring the Limits of Fine-grained LLM-based Physics Inference via\n  Premise Removal Interventions", "abstract": "Language models (LMs) can hallucinate when performing complex mathematical\nreasoning. Physics provides a rich domain for assessing their mathematical\ncapabilities, where physical context requires that any symbolic manipulation\nsatisfies complex semantics (\\textit{e.g.,} units, tensorial order). In this\nwork, we systematically remove crucial context from prompts to force instances\nwhere model inference may be algebraically coherent, yet unphysical. We assess\nLM capabilities in this domain using a curated dataset encompassing multiple\nnotations and Physics subdomains. Further, we improve zero-shot scores using\nsynthetic in-context examples, and demonstrate non-linear degradation of\nderivation quality with perturbation strength via the progressive omission of\nsupporting premises. We find that the models' mathematical reasoning is not\nphysics-informed in this setting, where physical context is predominantly\nignored in favour of reverse-engineering solutions.", "published": "2024-04-29 02:43:23", "link": "http://arxiv.org/abs/2404.18384v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Mixture-of-Instructions: Aligning Large Language Models via Mixture\n  Prompting", "abstract": "With the proliferation of large language models (LLMs), the comprehensive\nalignment of such models across multiple tasks has emerged as a critical area\nof research. Existing alignment methodologies primarily address single task,\nsuch as multi-turn dialogue, coding, mathematical problem-solving, and tool\nusage. Although there is a large amount of high-quality data available for\nthose tasks, most of them provide only questions and answers without including\nthe system prompt. Though a detailed analysis of the Qwen language model, we\nfound that the system prompt has a significant impact on both training and\ninference processes of LLM. We attributes this phenomenon to overfitting to the\nsystem prompt. In address this issue, we introduce a novel technique termed\nMixture-of-Instructions (MoI), which employs a strategy of instruction packing\ncombined with diverse system prompts to boost the alignment efficiency of\nlanguage models. We have also compiled a diverse set of seven benchmark\ndatasets to rigorously evaluate the alignment efficacy of the MoI-enhanced\nlanguage model. Our methodology was applied to the open-source Qwen-7B-chat\nmodel, culminating in the development of Qwen-SFT-MoI. This enhanced model\ndemonstrates significant advancements in generative capabilities across coding,\nmathematics, and tool use tasks.", "published": "2024-04-29 03:58:12", "link": "http://arxiv.org/abs/2404.18410v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HFT: Half Fine-Tuning for Large Language Models", "abstract": "Large language models (LLMs) with one or more fine-tuning phases have become\na necessary step to unlock various capabilities, enabling LLMs to follow\nnatural language instructions or align with human preferences. However, it\ncarries the risk of catastrophic forgetting during sequential training, the\nparametric knowledge or the ability learned in previous stages may be\noverwhelmed by incoming training data. In this paper, we find that by regularly\nresetting partial parameters, LLMs can restore some of the original knowledge.\nInspired by this, we introduce Half Fine-Tuning (HFT) for LLMs, as a substitute\nfor full fine-tuning (FFT), to mitigate the forgetting issues, where half of\nthe parameters are selected to learn new tasks while the other half are frozen\nto remain previous knowledge. We provide a feasibility analysis from the\nperspective of optimization and interpret the parameter selection operation as\na regularization term. Without changing the model architecture, HFT could be\nseamlessly integrated into existing fine-tuning frameworks. Extensive\nexperiments and analysis on supervised fine-tuning, direct preference\noptimization, and continual learning consistently demonstrate the\neffectiveness, robustness, and efficiency of HFT. Compared with FFT, HFT not\nonly significantly alleviates the forgetting problem, but also achieves the\nbest performance in a series of downstream benchmarks, with an approximately\n30% reduction in training time.", "published": "2024-04-29 07:07:58", "link": "http://arxiv.org/abs/2404.18466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Explainability of machine learning approaches in forensic linguistics: a\n  case study in geolinguistic authorship profiling", "abstract": "Forensic authorship profiling uses linguistic markers to infer\ncharacteristics about an author of a text. This task is paralleled in dialect\nclassification, where a prediction is made about the linguistic variety of a\ntext based on the text itself. While there have been significant advances in\nrecent years in variety classification, forensic linguistics rarely relies on\nthese approaches due to their lack of transparency, among other reasons. In\nthis paper we therefore explore the explainability of machine learning\napproaches considering the forensic context. We focus on variety classification\nas a means of geolinguistic profiling of unknown texts based on social media\ndata from the German-speaking area. For this, we identify the lexical items\nthat are the most impactful for the variety classification. We find that the\nextracted lexical features are indeed representative of their respective\nvarieties and note that the trained models also rely on place names for\nclassifications.", "published": "2024-04-29 08:52:52", "link": "http://arxiv.org/abs/2404.18510v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can GPT-4 do L2 analytic assessment?", "abstract": "Automated essay scoring (AES) to evaluate second language (L2) proficiency\nhas been a firmly established technology used in educational contexts for\ndecades. Although holistic scoring has seen advancements in AES that match or\neven exceed human performance, analytic scoring still encounters issues as it\ninherits flaws and shortcomings from the human scoring process. The recent\nintroduction of large language models presents new opportunities for automating\nthe evaluation of specific aspects of L2 writing proficiency. In this paper, we\nperform a series of experiments using GPT-4 in a zero-shot fashion on a\npublicly available dataset annotated with holistic scores based on the Common\nEuropean Framework of Reference and aim to extract detailed information about\ntheir underlying analytic components. We observe significant correlations\nbetween the automatically predicted analytic scores and multiple features\nassociated with the individual proficiency components.", "published": "2024-04-29 10:00:00", "link": "http://arxiv.org/abs/2404.18557v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing Semantic Change through Lexical Replacements", "abstract": "Modern language models are capable of contextualizing words based on their\nsurrounding context. However, this capability is often compromised due to\nsemantic change that leads to words being used in new, unexpected contexts not\nencountered during pre-training. In this paper, we model \\textit{semantic\nchange} by studying the effect of unexpected contexts introduced by\n\\textit{lexical replacements}. We propose a \\textit{replacement schema} where a\ntarget word is substituted with lexical replacements of varying relatedness,\nthus simulating different kinds of semantic change. Furthermore, we leverage\nthe replacement schema as a basis for a novel \\textit{interpretable} model for\nsemantic change. We are also the first to evaluate the use of LLaMa for\nsemantic change detection.", "published": "2024-04-29 10:20:41", "link": "http://arxiv.org/abs/2404.18570v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table\n  Question Answering", "abstract": "Table Question Answering (TQA) aims at composing an answer to a question\nbased on tabular data. While prior research has shown that TQA models lack\nrobustness, understanding the underlying cause and nature of this issue remains\npredominantly unclear, posing a significant obstacle to the development of\nrobust TQA systems. In this paper, we formalize three major desiderata for a\nfine-grained evaluation of robustness of TQA systems. They should (i) answer\nquestions regardless of alterations in table structure, (ii) base their\nresponses on the content of relevant cells rather than on biases, and (iii)\ndemonstrate robust numerical reasoning capabilities. To investigate these\naspects, we create and publish a novel TQA evaluation benchmark in English. Our\nextensive experimental analysis reveals that none of the examined\nstate-of-the-art TQA systems consistently excels in these three aspects. Our\nbenchmark is a crucial instrument for monitoring the behavior of TQA systems\nand paves the way for the development of robust TQA systems. We release our\nbenchmark publicly.", "published": "2024-04-29 10:55:08", "link": "http://arxiv.org/abs/2404.18585v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The SAMER Arabic Text Simplification Corpus", "abstract": "We present the SAMER Corpus, the first manually annotated Arabic parallel\ncorpus for text simplification targeting school-aged learners. Our corpus\ncomprises texts of 159K words selected from 15 publicly available Arabic\nfiction novels most of which were published between 1865 and 1955. Our corpus\nincludes readability level annotations at both the document and word levels, as\nwell as two simplified parallel versions for each text targeting learners at\ntwo different readability levels. We describe the corpus selection process, and\noutline the guidelines we followed to create the annotations and ensure their\nquality. Our corpus is publicly available to support and encourage research on\nArabic text simplification, Arabic automatic readability assessment, and the\ndevelopment of Arabic pedagogical language technologies.", "published": "2024-04-29 11:34:06", "link": "http://arxiv.org/abs/2404.18615v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Iconic Gesture Semantics", "abstract": "The \"meaning\" of an iconic gesture is conditioned on its informational\nevaluation. Only informational evaluation lifts a gesture to a quasi-linguistic\nlevel that can interact with verbal content. Interaction is either vacuous or\nregimented by usual lexicon-driven inferences. Informational evaluation is\nspelled out as extended exemplification (extemplification) in terms of\nperceptual classification of a gesture's visual iconic model. The iconic model\nis derived from Frege/Montague-like truth-functional evaluation of a gesture's\nform within spatially extended domains. We further argue that the perceptual\nclassification of instances of visual communication requires a notion of\nmeaning different from Frege/Montague frameworks. Therefore, a heuristic for\ngesture interpretation is provided that can guide the working semanticist. In\nsum, an iconic gesture semantics is introduced which covers the full range from\nkinematic gesture representations over model-theoretic evaluation to\ninferential interpretation in dynamic semantic frameworks.", "published": "2024-04-29 13:58:03", "link": "http://arxiv.org/abs/2404.18708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Constant in HATE: Analyzing Toxicity in Reddit across Topics and\n  Languages", "abstract": "Toxic language remains an ongoing challenge on social media platforms,\npresenting significant issues for users and communities. This paper provides a\ncross-topic and cross-lingual analysis of toxicity in Reddit conversations. We\ncollect 1.5 million comment threads from 481 communities in six languages:\nEnglish, German, Spanish, Turkish,Arabic, and Dutch, covering 80 topics such as\nCulture, Politics, and News. We thoroughly analyze how toxicity spikes within\ndifferent communities in relation to specific topics. We observe consistent\npatterns of increased toxicity across languages for certain topics, while also\nnoting significant variations within specific language communities.", "published": "2024-04-29 14:14:33", "link": "http://arxiv.org/abs/2404.18726v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Dog Bark Decoding: Leveraging Human Speech Processing for\n  Automated Bark Classification", "abstract": "Similar to humans, animals make extensive use of verbal and non-verbal forms\nof communication, including a large range of audio signals. In this paper, we\naddress dog vocalizations and explore the use of self-supervised speech\nrepresentation models pre-trained on human speech to address dog bark\nclassification tasks that find parallels in human-centered tasks in speech\nrecognition. We specifically address four tasks: dog recognition, breed\nidentification, gender classification, and context grounding. We show that\nusing speech embedding representations significantly improves over simpler\nclassification baselines. Further, we also find that models pre-trained on\nlarge human speech acoustics can provide additional performance boosts on\nseveral tasks.", "published": "2024-04-29 14:41:59", "link": "http://arxiv.org/abs/2404.18739v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unknown Script: Impact of Script on Cross-Lingual Transfer", "abstract": "Cross-lingual transfer has become an effective way of transferring knowledge\nbetween languages. In this paper, we explore an often overlooked aspect in this\ndomain: the influence of the source language of a language model on language\ntransfer performance. We consider a case where the target language and its\nscript are not part of the pre-trained model. We conduct a series of\nexperiments on monolingual and multilingual models that are pre-trained on\ndifferent tokenization methods to determine factors that affect cross-lingual\ntransfer to a new language with a unique script. Our findings reveal the\nimportance of the tokenizer as a stronger factor than the shared script,\nlanguage similarity, and model size.", "published": "2024-04-29 15:48:01", "link": "http://arxiv.org/abs/2404.18810v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "It's Difficult to be Neutral -- Human and LLM-based Sentiment Annotation\n  of Patient Comments", "abstract": "Sentiment analysis is an important tool for aggregating patient voices, in\norder to provide targeted improvements in healthcare services. A prerequisite\nfor this is the availability of in-domain data annotated for sentiment. This\narticle documents an effort to add sentiment annotations to free-text comments\nin patient surveys collected by the Norwegian Institute of Public Health\n(NIPH). However, annotation can be a time-consuming and resource-intensive\nprocess, particularly when it requires domain expertise. We therefore also\nevaluate a possible alternative to human annotation, using large language\nmodels (LLMs) as annotators. We perform an extensive evaluation of the approach\nfor two openly available pretrained LLMs for Norwegian, experimenting with\ndifferent configurations of prompts and in-context learning, comparing their\nperformance to human annotators. We find that even for zero-shot runs, models\nperform well above the baseline for binary sentiment, but still cannot compete\nwith human annotators on the full dataset.", "published": "2024-04-29 16:19:47", "link": "http://arxiv.org/abs/2404.18832v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Comprehensive Rubric for Annotating Pathological Speech", "abstract": "Rubrics are a commonly used tool for labeling voice corpora in speech quality\nassessment, although their application in the context of pathological speech\nremains relatively limited. In this study, we introduce a comprehensive rubric\nbased on various dimensions of speech quality, including phonetics, fluency,\nand prosody. The objective is to establish standardized criteria for\nidentifying errors within the speech of individuals with Down syndrome, thereby\nenabling the development of automated assessment systems. To achieve this\nobjective, we utilized the Prautocal corpus. To assess the quality of\nannotations using our rubric, two experiments were conducted, focusing on\nphonetics and fluency. For phonetic evaluation, we employed the Goodness of\nPronunciation (GoP) metric, utilizing automatic segmentation systems and\ncorrelating the results with evaluations conducted by a specialized speech\ntherapist. While the obtained correlation values were not notably high, a\npositive trend was observed. In terms of fluency assessment, deep learning\nmodels like wav2vec were used to extract audio features, and we employed an SVM\nclassifier trained on a corpus focused on identifying fluency issues to\ncategorize Prautocal corpus samples. The outcomes highlight the complexities of\nevaluating such phenomena, with variability depending on the specific type of\ndisfluency detected.", "published": "2024-04-29 16:44:27", "link": "http://arxiv.org/abs/2404.18851v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Truth-value judgment in language models: belief directions are context\n  sensitive", "abstract": "Recent work has demonstrated that the latent spaces of large language models\n(LLMs) contain directions predictive of the truth of sentences. Multiple\nmethods recover such directions and build probes that are described as getting\nat a model's \"knowledge\" or \"beliefs\". We investigate this phenomenon, looking\nclosely at the impact of context on the probes. Our experiments establish where\nin the LLM the probe's predictions can be described as being conditional on the\npreceding (related) sentences. Specifically, we quantify the responsiveness of\nthe probes to the presence of (negated) supporting and contradicting sentences,\nand score the probes on their consistency. We also perform a causal\nintervention experiment, investigating whether moving the representation of a\npremise along these belief directions influences the position of the hypothesis\nalong that same direction. We find that the probes we test are generally\ncontext sensitive, but that contexts which should not affect the truth often\nstill impact the probe outputs. Our experiments show that the type of errors\ndepend on the layer, the (type of) model, and the kind of data. Finally, our\nresults suggest that belief directions are (one of the) causal mediators in the\ninference process that incorporates in-context information.", "published": "2024-04-29 16:52:57", "link": "http://arxiv.org/abs/2404.18865v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spivavtor: An Instruction Tuned Ukrainian Text Editing Model", "abstract": "We introduce Spivavtor, a dataset, and instruction-tuned models for text\nediting focused on the Ukrainian language. Spivavtor is the Ukrainian-focused\nadaptation of the English-only CoEdIT model. Similar to CoEdIT, Spivavtor\nperforms text editing tasks by following instructions in Ukrainian. This paper\ndescribes the details of the Spivavtor-Instruct dataset and Spivavtor models.\nWe evaluate Spivavtor on a variety of text editing tasks in Ukrainian, such as\nGrammatical Error Correction (GEC), Text Simplification, Coherence, and\nParaphrasing, and demonstrate its superior performance on all of them. We\npublicly release our best-performing models and data as resources to the\ncommunity to advance further research in this space.", "published": "2024-04-29 17:16:22", "link": "http://arxiv.org/abs/2404.18880v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Holmes: A Benchmark to Assess the Linguistic Competence of Language\n  Models", "abstract": "We introduce Holmes, a new benchmark designed to assess language models (LMs)\nlinguistic competence - their unconscious understanding of linguistic\nphenomena. Specifically, we use classifier-based probing to examine LMs'\ninternal representations regarding distinct linguistic phenomena (e.g.,\npart-of-speech tagging). As a result, we meet recent calls to disentangle LMs'\nlinguistic competence from other cognitive abilities, such as following\ninstructions in prompting-based evaluations. Composing Holmes, we review over\n270 probing studies and include more than 200 datasets to assess syntax,\nmorphology, semantics, reasoning, and discourse phenomena. Analyzing over 50\nLMs reveals that, aligned with known trends, their linguistic competence\ncorrelates with model size. However, surprisingly, model architecture and\ninstruction tuning also significantly influence performance, particularly in\nmorphology and syntax. Finally, we propose FlashHolmes, a streamlined version\nthat reduces the computation load while maintaining high-ranking precision.", "published": "2024-04-29 17:58:36", "link": "http://arxiv.org/abs/2404.18923v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Computational Job Market Analysis with Natural Language Processing", "abstract": "[Abridged Abstract]\n  Recent technological advances underscore labor market dynamics, yielding\nsignificant consequences for employment prospects and increasing job vacancy\ndata across platforms and languages. Aggregating such data holds potential for\nvaluable insights into labor market demands, new skills emergence, and\nfacilitating job matching for various stakeholders. However, despite prevalent\ninsights in the private sector, transparent language technology systems and\ndata for this domain are lacking. This thesis investigates Natural Language\nProcessing (NLP) technology for extracting relevant information from job\ndescriptions, identifying challenges including scarcity of training data, lack\nof standardized annotation guidelines, and shortage of effective extraction\nmethods from job ads. We frame the problem, obtaining annotated data, and\nintroducing extraction methodologies. Our contributions include job description\ndatasets, a de-identification dataset, and a novel active learning algorithm\nfor efficient model training. We propose skill extraction using weak\nsupervision, a taxonomy-aware pre-training methodology adapting multilingual\nlanguage models to the job market domain, and a retrieval-augmented model\nleveraging multiple skill extraction datasets to enhance overall performance.\nFinally, we ground extracted information within a designated taxonomy.", "published": "2024-04-29 14:52:38", "link": "http://arxiv.org/abs/2404.18977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Markovian Transformers for Informative Language Modeling", "abstract": "Chain-of-Thought (CoT) reasoning often fails to faithfully reflect a language\nmodel's underlying decision process. We address this by making CoT text\ncausally essential in a \"Markovian\" language model, factoring next-token\nprediction through an intermediate CoT and training it to predict future tokens\nindependently of the original prompt. We formalize this via an\n\"informativeness\" objective that quantifies how much a trained CoT improves\nnext-token predictions over a baseline. Using policy gradient, we show that\nLlama 3.1 8B achieves a 33.2% absolute accuracy improvement on GSM8K.\nPerturbation tests confirm stronger reliance on the CoT, while cross-model\ntransfers indicate these reasoning traces generalize across interpreters. Our\napproach enhances both accuracy and interpretability, potentially extending CoT\nreasoning to arbitrarily long contexts and diverse tasks.", "published": "2024-04-29 17:36:58", "link": "http://arxiv.org/abs/2404.18988v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Plan of Thoughts: Heuristic-Guided Problem Solving with Large Language\n  Models", "abstract": "While language models (LMs) offer significant capability in zero-shot\nreasoning tasks across a wide range of domains, they do not perform\nsatisfactorily in problems which requires multi-step reasoning. Previous\napproaches to mitigate this involves breaking a larger, multi-step task into\nsub-tasks and asking the language model to generate proposals (\"thoughts\") for\neach sub-task and using exhaustive planning approaches such as DFS to compose a\nsolution. In this work, we leverage this idea to introduce two new\ncontributions: first, we formalize a planning-based approach to perform\nmulti-step problem solving with LMs via Partially Observable Markov Decision\nProcesses (POMDPs), with the LM's own reflections about the value of a state\nused as a search heuristic; second, leveraging the online POMDP solver POMCP,\nwe demonstrate a superior success rate of 89.4% on the Game of 24 task as\ncompared to existing approaches while also offering better anytime performance\ncharacteristics than fixed tree-search which is used previously. Taken\ntogether, these contributions allow modern LMs to decompose and solve\nlarger-scale reasoning tasks more effectively.", "published": "2024-04-29 18:51:17", "link": "http://arxiv.org/abs/2404.19055v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SuperCLUE-Fin: Graded Fine-Grained Analysis of Chinese LLMs on Diverse\n  Financial Tasks and Applications", "abstract": "The SuperCLUE-Fin (SC-Fin) benchmark is a pioneering evaluation framework\ntailored for Chinese-native financial large language models (FLMs). It assesses\nFLMs across six financial application domains and twenty-five specialized\ntasks, encompassing theoretical knowledge and practical applications such as\ncompliance, risk management, and investment analysis. Using multi-turn,\nopen-ended conversations that mimic real-life scenarios, SC-Fin measures models\non a range of criteria, including accurate financial understanding, logical\nreasoning, clarity, computational efficiency, business acumen, risk perception,\nand compliance with Chinese regulations.\n  In a rigorous evaluation involving over a thousand questions, SC-Fin\nidentifies a performance hierarchy where domestic models like GLM-4 and\nMoonShot-v1-128k outperform others with an A-grade, highlighting the potential\nfor further development in transforming theoretical knowledge into pragmatic\nfinancial solutions. This benchmark serves as a critical tool for refining FLMs\nin the Chinese context, directing improvements in financial knowledge\ndatabases, standardizing financial interpretations, and promoting models that\nprioritize compliance, risk management, and secure practices.\n  We create a contextually relevant and comprehensive benchmark that drives the\ndevelopment of AI in the Chinese financial sector. SC-Fin facilitates the\nadvancement and responsible deployment of FLMs, offering valuable insights for\nenhancing model performance and usability for both individual and institutional\nusers in the Chinese market..~\\footnote{Our benchmark can be found at\n\\url{https://www.CLUEbenchmarks.com}}.", "published": "2024-04-29 19:04:35", "link": "http://arxiv.org/abs/2404.19063v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Effects of Added Emphasis and Pause in Audio Delivery of Health\n  Information", "abstract": "Health literacy is crucial to supporting good health and is a major national\ngoal. Audio delivery of information is becoming more popular for informing\noneself. In this study, we evaluate the effect of audio enhancements in the\nform of information emphasis and pauses with health texts of varying difficulty\nand we measure health information comprehension and retention. We produced\naudio snippets from difficult and easy text and conducted the study on Amazon\nMechanical Turk (AMT). Our findings suggest that emphasis matters for both\ninformation comprehension and retention. When there is no added pause,\nemphasizing significant information can lower the perceived difficulty for\ndifficult and easy texts. Comprehension is higher (54%) with correctly placed\nemphasis for the difficult texts compared to not adding emphasis (50%). Adding\na pause lowers perceived difficulty and can improve retention but adversely\naffects information comprehension.", "published": "2024-04-29 21:36:16", "link": "http://arxiv.org/abs/2404.19119v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Accelerating Production LLMs with Combined Token/Embedding Speculators", "abstract": "This technical report describes the design and training of novel speculative\ndecoding draft models, for accelerating the inference speeds of large language\nmodels in a production environment. By conditioning draft predictions on both\ncontext vectors and sampled tokens, we can train our speculators to efficiently\npredict high-quality n-grams, which the base model then accepts or rejects.\nThis allows us to effectively predict multiple tokens per inference forward\npass, accelerating wall-clock inference speeds of highly optimized base model\nimplementations by a factor of 2-3x. We explore these initial results and\ndescribe next steps for further improvements.", "published": "2024-04-29 21:59:07", "link": "http://arxiv.org/abs/2404.19124v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RTF: Region-based Table Filling Method for Relational Triple Extraction", "abstract": "Relational triple extraction is crucial work for the automatic construction\nof knowledge graphs. Existing methods only construct shallow representations\nfrom a token or token pair-level. However, previous works ignore local spatial\ndependencies of relational triples, resulting in a weakness of entity pair\nboundary detection. To tackle this problem, we propose a novel Region-based\nTable Filling method (RTF). We devise a novel region-based tagging scheme and\nbi-directional decoding strategy, which regard each relational triple as a\nregion on the relation-specific table, and identifies triples by determining\ntwo endpoints of each region. We also introduce convolution to construct\nregion-level table representations from a spatial perspective which makes\ntriples easier to be captured. In addition, we share partial tagging scores\namong different relations to improve learning efficiency of relation\nclassifier. Experimental results show that our method achieves state-of-the-art\nwith better generalization capability on three variants of two widely used\nbenchmark datasets.", "published": "2024-04-29 23:36:38", "link": "http://arxiv.org/abs/2404.19154v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What Drives Performance in Multilingual Language Models?", "abstract": "This study investigates the factors influencing the performance of\nmultilingual large language models (MLLMs) across diverse languages. We study 6\nMLLMs, including masked language models, autoregressive models, and\ninstruction-tuned LLMs, on the SIB-200 dataset, a topic classification dataset\nencompassing 204 languages. Our analysis considers three scenarios: ALL\nlanguages, SEEN languages (present in the model's pretraining data), and UNSEEN\nlanguages (not present or documented in the model's pretraining data in any\nmeaningful way). We examine the impact of factors such as pretraining data\nsize, general resource availability, language family, and script type on model\nperformance. Decision tree analysis reveals that pretraining data size is the\nmost influential factor for SEEN languages. However, interestingly, script type\nand language family are crucial for UNSEEN languages, highlighting the\nimportance of cross-lingual transfer learning. Notably, model size and\narchitecture do not significantly alter the most important features identified.\nOur findings provide valuable insights into the strengths and limitations of\ncurrent MLLMs and hope to guide the development of more effective and equitable\nmultilingual NLP systems.", "published": "2024-04-29 23:49:19", "link": "http://arxiv.org/abs/2404.19159v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "101 Billion Arabic Words Dataset", "abstract": "In recent years, Large Language Models have revolutionized the field of\nnatural language processing, showcasing an impressive rise predominantly in\nEnglish-centric domains. These advancements have set a global benchmark,\ninspiring significant efforts toward developing Arabic LLMs capable of\nunderstanding and generating the Arabic language with remarkable accuracy.\nDespite these advancements, a critical challenge persists: the potential bias\nin Arabic LLMs, primarily attributed to their reliance on datasets comprising\nEnglish data that has been translated into Arabic. This reliance not only\ncompromises the authenticity of the generated content but also reflects a\nbroader issue -the scarcity of original quality Arabic linguistic data. This\nstudy aims to address the data scarcity in the Arab world and to encourage the\ndevelopment of Arabic Language Models that are true to both the linguistic and\nnuances of the region. We undertook a large-scale data mining project,\nextracting a substantial volume of text from the Common Crawl WET files,\nspecifically targeting Arabic content. The extracted data underwent a rigorous\ncleaning and deduplication process, using innovative techniques to ensure the\nintegrity and uniqueness of the dataset. The result is the 101 Billion Arabic\nWords Dataset, the largest Arabic dataset available to date, which can\nsignificantly contribute to the development of authentic Arabic LLMs. This\nstudy not only highlights the potential for creating linguistically and\nculturally accurate Arabic LLMs but also sets a precedent for future research\nin enhancing the authenticity of Arabic language models.", "published": "2024-04-29 13:15:03", "link": "http://arxiv.org/abs/2405.01590v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "FoundaBench: Evaluating Chinese Fundamental Knowledge Capabilities of\n  Large Language Models", "abstract": "In the burgeoning field of large language models (LLMs), the assessment of\nfundamental knowledge remains a critical challenge, particularly for models\ntailored to Chinese language and culture. This paper introduces FoundaBench, a\npioneering benchmark designed to rigorously evaluate the fundamental knowledge\ncapabilities of Chinese LLMs. FoundaBench encompasses a diverse array of 3354\nmultiple-choice questions across common sense and K-12 educational subjects,\nmeticulously curated to reflect the breadth and depth of everyday and academic\nknowledge. We present an extensive evaluation of 12 state-of-the-art LLMs using\nFoundaBench, employing both traditional assessment methods and our CircularEval\nprotocol to mitigate potential biases in model responses. Our results highlight\nthe superior performance of models pre-trained on Chinese corpora, and reveal a\nsignificant disparity between models' reasoning and memory recall capabilities.\nThe insights gleaned from FoundaBench evaluations set a new standard for\nunderstanding the fundamental knowledge of LLMs, providing a robust framework\nfor future advancements in the field.", "published": "2024-04-29 01:49:07", "link": "http://arxiv.org/abs/2404.18359v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "UMETTS: A Unified Framework for Emotional Text-to-Speech Synthesis with\n  Multimodal Prompts", "abstract": "Emotional Text-to-Speech (E-TTS) synthesis has garnered significant attention\nin recent years due to its potential to revolutionize human-computer\ninteraction. However, current E-TTS approaches often struggle to capture the\nintricacies of human emotions, primarily relying on oversimplified emotional\nlabels or single-modality input. In this paper, we introduce the Unified\nMultimodal Prompt-Induced Emotional Text-to-Speech System (UMETTS), a novel\nframework that leverages emotional cues from multiple modalities to generate\nhighly expressive and emotionally resonant speech. The core of UMETTS consists\nof two key components: the Emotion Prompt Alignment Module (EP-Align) and the\nEmotion Embedding-Induced TTS Module (EMI-TTS). (1) EP-Align employs\ncontrastive learning to align emotional features across text, audio, and visual\nmodalities, ensuring a coherent fusion of multimodal information. (2)\nSubsequently, EMI-TTS integrates the aligned emotional embeddings with\nstate-of-the-art TTS models to synthesize speech that accurately reflects the\nintended emotions. Extensive evaluations show that UMETTS achieves significant\nimprovements in emotion accuracy and speech naturalness, outperforming\ntraditional E-TTS methods on both objective and subjective metrics.", "published": "2024-04-29 03:19:39", "link": "http://arxiv.org/abs/2404.18398v2", "categories": ["cs.CL", "cs.MM"], "primary_category": "cs.CL"}
{"title": "Ethical Reasoning and Moral Value Alignment of LLMs Depend on the\n  Language we Prompt them in", "abstract": "Ethical reasoning is a crucial skill for Large Language Models (LLMs).\nHowever, moral values are not universal, but rather influenced by language and\nculture. This paper explores how three prominent LLMs -- GPT-4, ChatGPT, and\nLlama2-70B-Chat -- perform ethical reasoning in different languages and if\ntheir moral judgement depend on the language in which they are prompted. We\nextend the study of ethical reasoning of LLMs by Rao et al. (2023) to a\nmultilingual setup following their framework of probing LLMs with ethical\ndilemmas and policies from three branches of normative ethics: deontology,\nvirtue, and consequentialism. We experiment with six languages: English,\nSpanish, Russian, Chinese, Hindi, and Swahili. We find that GPT-4 is the most\nconsistent and unbiased ethical reasoner across languages, while ChatGPT and\nLlama2-70B-Chat show significant moral value bias when we move to languages\nother than English. Interestingly, the nature of this bias significantly vary\nacross languages for all LLMs, including GPT-4.", "published": "2024-04-29 06:42:27", "link": "http://arxiv.org/abs/2404.18460v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Injecting Salesperson's Dialogue Strategies in Large Language Models\n  with Chain-of-Thought Reasoning", "abstract": "Recent research in dialogue systems and corpora has focused on two main\ncategories: task-oriented (TOD) and open-domain (chit-chat) dialogues. TOD\nsystems help users accomplish specific tasks, while open-domain systems aim to\ncreate engaging conversations. However, in real-world scenarios, user intents\nare often revealed during interactions. A recent study introduced SalesBot,\nwhich simulates dialogues transitioning from chit-chat to task-oriented\nscenarios to train sales agents. Unfortunately, the initial data lacked smooth\ntransitions and coherent long-turn dialogues, resulting in poor naturalness in\nsales-customer interactions. To address these issues, this paper presents\nSalesBot 2.0, an improved dataset. It leverages commonsense knowledge from\nlarge language models (LLMs) through strategic prompting. Additionally, we\nintroduce a novel model called SalesAgent, trained on salesperson's\ninteractions, using chain-of-thought (CoT) reasoning. This model excels in\ntransitioning topics, understanding user intents, and selecting appropriate\nstrategies. Experiments using diverse user simulations validate the\neffectiveness of our method in controlling dialogue strategies in LLMs.\nFurthermore, SalesBot 2.0 enhances coherence and reduces aggression,\nfacilitating better model learning for sales-customer interactions.", "published": "2024-04-29 10:12:04", "link": "http://arxiv.org/abs/2404.18564v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reinforcement Learning Problem Solving with Large Language Models", "abstract": "Large Language Models (LLMs) encapsulate an extensive amount of world\nknowledge, and this has enabled their application in various domains to improve\nthe performance of a variety of Natural Language Processing (NLP) tasks. This\nhas also facilitated a more accessible paradigm of conversation-based\ninteractions between humans and AI systems to solve intended problems. However,\none interesting avenue that shows untapped potential is the use of LLMs as\nReinforcement Learning (RL) agents to enable conversational RL problem solving.\nTherefore, in this study, we explore the concept of formulating Markov Decision\nProcess-based RL problems as LLM prompting tasks. We demonstrate how LLMs can\nbe iteratively prompted to learn and optimize policies for specific RL tasks.\nIn addition, we leverage the introduced prompting technique for episode\nsimulation and Q-Learning, facilitated by LLMs. We then show the practicality\nof our approach through two detailed case studies for \"Research Scientist\" and\n\"Legal Matter Intake\" workflows.", "published": "2024-04-29 12:16:08", "link": "http://arxiv.org/abs/2404.18638v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Revealing the Parametric Knowledge of Language Models: A Unified\n  Framework for Attribution Methods", "abstract": "Language Models (LMs) acquire parametric knowledge from their training\nprocess, embedding it within their weights. The increasing scalability of LMs,\nhowever, poses significant challenges for understanding a model's inner\nworkings and further for updating or correcting this embedded knowledge without\nthe significant cost of retraining. This underscores the importance of\nunveiling exactly what knowledge is stored and its association with specific\nmodel components. Instance Attribution (IA) and Neuron Attribution (NA) offer\ninsights into this training-acquired knowledge, though they have not been\ncompared systematically. Our study introduces a novel evaluation framework to\nquantify and compare the knowledge revealed by IA and NA. To align the results\nof the methods we introduce the attribution method NA-Instances to apply NA for\nretrieving influential training instances, and IA-Neurons to discover important\nneurons of influential instances discovered by IA. We further propose a\ncomprehensive list of faithfulness tests to evaluate the comprehensiveness and\nsufficiency of the explanations provided by both methods. Through extensive\nexperiments and analysis, we demonstrate that NA generally reveals more diverse\nand comprehensive information regarding the LM's parametric knowledge compared\nto IA. Nevertheless, IA provides unique and valuable insights into the LM's\nparametric knowledge, which are not revealed by NA. Our findings further\nsuggest the potential of a synergistic approach of combining the diverse\nfindings of IA and NA for a more holistic understanding of an LM's parametric\nknowledge.", "published": "2024-04-29 12:38:26", "link": "http://arxiv.org/abs/2404.18655v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Improving Automatic Text Recognition with Language Models in the PyLaia\n  Open-Source Library", "abstract": "PyLaia is one of the most popular open-source software for Automatic Text\nRecognition (ATR), delivering strong performance in terms of speed and\naccuracy. In this paper, we outline our recent contributions to the PyLaia\nlibrary, focusing on the incorporation of reliable confidence scores and the\nintegration of statistical language modeling during decoding. Our\nimplementation provides an easy way to combine PyLaia with n-grams language\nmodels at different levels. One of the highlights of this work is that language\nmodels are completely auto-tuned: they can be built and used easily without any\nexpert knowledge, and without requiring any additional data. To demonstrate the\nsignificance of our contribution, we evaluate PyLaia's performance on twelve\ndatasets, both with and without language modelling. The results show that\ndecoding with small language models improves the Word Error Rate by 13% and the\nCharacter Error Rate by 12% in average. Additionally, we conduct an analysis of\nconfidence scores and highlight the importance of calibration techniques. Our\nimplementation is publicly available in the official PyLaia repository at\nhttps://gitlab.teklia.com/atr/pylaia, and twelve open-source models are\nreleased on Hugging Face.", "published": "2024-04-29 14:11:16", "link": "http://arxiv.org/abs/2404.18722v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Towards A Structured Overview of Use Cases for Natural Language\n  Processing in the Legal Domain: A German Perspective", "abstract": "In recent years, the field of Legal Tech has risen in prevalence, as the\nNatural Language Processing (NLP) and legal disciplines have combined forces to\ndigitalize legal processes. Amidst the steady flow of research solutions\nstemming from the NLP domain, the study of use cases has fallen behind, leading\nto a number of innovative technical methods without a place in practice. In\nthis work, we aim to build a structured overview of Legal Tech use cases,\ngrounded in NLP literature, but also supplemented by voices from legal practice\nin Germany. Based upon a Systematic Literature Review, we identify seven\ncategories of NLP technologies for the legal domain, which are then studied in\njuxtaposition to 22 legal use cases. In the investigation of these use cases,\nwe identify 15 ethical, legal, and social aspects (ELSA), shedding light on the\npotential concerns of digitally transforming the legal domain.", "published": "2024-04-29 14:56:47", "link": "http://arxiv.org/abs/2404.18759v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Where on Earth Do Users Say They Are?: Geo-Entity Linking for Noisy\n  Multilingual User Input", "abstract": "Geo-entity linking is the task of linking a location mention to the\nreal-world geographic location. In this paper we explore the challenging task\nof geo-entity linking for noisy, multilingual social media data. There are few\nopen-source multilingual geo-entity linking tools available and existing ones\nare often rule-based, which break easily in social media settings, or\nLLM-based, which are too expensive for large-scale datasets. We present a\nmethod which represents real-world locations as averaged embeddings from\nlabeled user-input location names and allows for selective prediction via an\ninterpretable confidence score. We show that our approach improves geo-entity\nlinking on a global and multilingual social media dataset, and discuss progress\nand problems with evaluating at different geographic granularities.", "published": "2024-04-29 15:18:33", "link": "http://arxiv.org/abs/2404.18784v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of\n  Diverse Models", "abstract": "As Large Language Models (LLMs) have become more advanced, they have outpaced\nour abilities to accurately evaluate their quality. Not only is finding data to\nadequately probe particular model properties difficult, but evaluating the\ncorrectness of a model's freeform generation alone is a challenge. To address\nthis, many evaluations now rely on using LLMs themselves as judges to score the\nquality of outputs from other LLMs. Evaluations most commonly use a single\nlarge model like GPT4. While this method has grown in popularity, it is costly,\nhas been shown to introduce intramodel bias, and in this work, we find that\nvery large models are often unnecessary. We propose instead to evaluate models\nusing a Panel of LLm evaluators (PoLL). Across three distinct judge settings\nand spanning six different datasets, we find that using a PoLL composed of a\nlarger number of smaller models outperforms a single large judge, exhibits less\nintra-model bias due to its composition of disjoint model families, and does so\nwhile being over seven times less expensive.", "published": "2024-04-29 15:33:23", "link": "http://arxiv.org/abs/2404.18796v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "More RLHF, More Trust? On The Impact of Preference Alignment On\n  Trustworthiness", "abstract": "The trustworthiness of Large Language Models (LLMs) refers to the extent to\nwhich their outputs are reliable, safe, and ethically aligned, and it has\nbecome a crucial consideration alongside their cognitive performance. In\npractice, Reinforcement Learning From Human Feedback (RLHF) has been widely\nused to align LLMs with labeled human preferences, but its assumed effect on\nmodel trustworthiness hasn't been rigorously evaluated. To bridge this\nknowledge gap, this study investigates how models aligned with general-purpose\npreference data perform across five trustworthiness verticals: toxicity,\nstereotypical bias, machine ethics, truthfulness, and privacy. Our results\ndemonstrate that RLHF on human preferences doesn't automatically guarantee\ntrustworthiness, and reverse effects are often observed. Furthermore, we\npropose to adapt efficient influence function based data attribution methods to\nthe RLHF setting to better understand the influence of fine-tuning data on\nindividual trustworthiness benchmarks, and show its feasibility by providing\nour estimated attribution scores. Together, our results underscore the need for\nmore nuanced approaches for model alignment from both the data and framework\nperspectives, and we hope this research will guide the community towards\ndeveloping language models that are increasingly capable without sacrificing\ntrustworthiness.", "published": "2024-04-29 17:00:53", "link": "http://arxiv.org/abs/2404.18870v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting", "abstract": "Speculative decoding has demonstrated its effectiveness in accelerating the\ninference of large language models while maintaining a consistent sampling\ndistribution. However, the conventional approach of training a separate draft\nmodel to achieve a satisfactory token acceptance rate can be costly. Drawing\ninspiration from early exiting, we propose a novel self-speculative decoding\nframework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a\nself-draft model, with the remaining layers serving as the larger target model.\nWe train a lightweight and efficient adapter module on top of the sub-network\nto bridge the gap between the sub-network and the full model's representation\nability. It is noteworthy that the inference latency of the self-draft model\nmay no longer be negligible compared to the large model, necessitating\nstrategies to increase the token acceptance rate while minimizing the drafting\nsteps of the small model. To address this challenge, we introduce an additional\nearly exiting mechanism for generating draft tokens. Specifically, we halt the\nsmall model's subsequent prediction during the drafting phase once the\nconfidence level for the current token falls below a certain threshold.\nExtensive experiments on the Spec-Bench demonstrate the effectiveness of\nKangaroo. Under single-sequence verification, Kangaroo achieves speedups up to\n$1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional\nparameters (67M compared to 591M). The code for Kangaroo is available at\nhttps://github.com/Equationliu/Kangaroo.", "published": "2024-04-29 17:53:54", "link": "http://arxiv.org/abs/2404.18911v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RE-GrievanceAssist: Enhancing Customer Experience through ML-Powered\n  Complaint Management", "abstract": "In recent years, digital platform companies have faced increasing challenges\nin managing customer complaints, driven by widespread consumer adoption. This\npaper introduces an end-to-end pipeline, named RE-GrievanceAssist, designed\nspecifically for real estate customer complaint management. The pipeline\nconsists of three key components: i) response/no-response ML model using TF-IDF\nvectorization and XGBoost classifier ; ii) user type classifier using fasttext\nclassifier; iii) issue/sub-issue classifier using TF-IDF vectorization and\nXGBoost classifier. Finally, it has been deployed as a batch job in Databricks,\nresulting in a remarkable 40% reduction in overall manual effort with monthly\ncost reduction of Rs 1,50,000 since August 2023.", "published": "2024-04-29 07:03:23", "link": "http://arxiv.org/abs/2404.18963v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Framework for Real-time Safeguarding the Text Generation of Large\n  Language Model", "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing (NLP) tasks but also pose ethical and societal risks due to their\npropensity to generate harmful content. To address this, various approaches\nhave been developed to safeguard LLMs from producing unsafe content. However,\nexisting methods have limitations, including the need for training specific\ncontrol models and proactive intervention during text generation, that lead to\nquality degradation and increased computational overhead. To mitigate those\nlimitations, we propose LLMSafeGuard, a lightweight framework to safeguard LLM\ntext generation in real-time. LLMSafeGuard integrates an external validator\ninto the beam search algorithm during decoding, rejecting candidates that\nviolate safety constraints while allowing valid ones to proceed. We introduce a\nsimilarity based validation approach, simplifying constraint introduction and\neliminating the need for control model training. Additionally, LLMSafeGuard\nemploys a context-wise timing selection strategy, intervening LLMs only when\nnecessary. We evaluate LLMSafeGuard on two tasks, detoxification and copyright\nsafeguarding, and demonstrate its superior performance over SOTA baselines. For\ninstance, LLMSafeGuard reduces the average toxic score of. LLM output by 29.7%\ncompared to the best baseline meanwhile preserving similar linguistic quality\nas natural output in detoxification task. Similarly, in the copyright task,\nLLMSafeGuard decreases the Longest Common Subsequence (LCS) by 56.2% compared\nto baselines. Moreover, our context-wise timing selection strategy reduces\ninference time by at least 24% meanwhile maintaining comparable effectiveness\nas validating each time step. LLMSafeGuard also offers tunable parameters to\nbalance its effectiveness and efficiency.", "published": "2024-04-29 18:40:01", "link": "http://arxiv.org/abs/2404.19048v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Blind Spots and Biases: Exploring the Role of Annotator Cognitive Biases\n  in NLP", "abstract": "With the rapid proliferation of artificial intelligence, there is growing\nconcern over its potential to exacerbate existing biases and societal\ndisparities and introduce novel ones. This issue has prompted widespread\nattention from academia, policymakers, industry, and civil society. While\nevidence suggests that integrating human perspectives can mitigate bias-related\nissues in AI systems, it also introduces challenges associated with cognitive\nbiases inherent in human decision-making. Our research focuses on reviewing\nexisting methodologies and ongoing investigations aimed at understanding\nannotation attributes that contribute to bias.", "published": "2024-04-29 19:28:35", "link": "http://arxiv.org/abs/2404.19071v1", "categories": ["cs.HC", "cs.CL"], "primary_category": "cs.HC"}
{"title": "In-Context Symbolic Regression: Leveraging Large Language Models for\n  Function Discovery", "abstract": "State of the art Symbolic Regression (SR) methods currently build specialized\nmodels, while the application of Large Language Models (LLMs) remains largely\nunexplored. In this work, we introduce the first comprehensive framework that\nutilizes LLMs for the task of SR. We propose In-Context Symbolic Regression\n(ICSR), an SR method which iteratively refines a functional form with an LLM\nand determines its coefficients with an external optimizer. ICSR leverages\nLLMs' strong mathematical prior both to propose an initial set of possible\nfunctions given the observations and to refine them based on their errors. Our\nfindings reveal that LLMs are able to successfully find symbolic equations that\nfit the given data, matching or outperforming the overall performance of the\nbest SR baselines on four popular benchmarks, while yielding simpler equations\nwith better out of distribution generalization.", "published": "2024-04-29 20:19:25", "link": "http://arxiv.org/abs/2404.19094v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Towards Unbiased Evaluation of Detecting Unanswerable Questions in\n  EHRSQL", "abstract": "Incorporating unanswerable questions into EHR QA systems is crucial for\ntesting the trustworthiness of a system, as providing non-existent responses\ncan mislead doctors in their diagnoses. The EHRSQL dataset stands out as a\npromising benchmark because it is the only dataset that incorporates\nunanswerable questions in the EHR QA system alongside practical questions.\nHowever, in this work, we identify a data bias in these unanswerable questions;\nthey can often be discerned simply by filtering with specific N-gram patterns.\nSuch biases jeopardize the authenticity and reliability of QA system\nevaluations. To tackle this problem, we propose a simple debiasing method of\nadjusting the split between the validation and test sets to neutralize the\nundue influence of N-gram filtering. By experimenting on the MIMIC-III dataset,\nwe demonstrate both the existing data bias in EHRSQL and the effectiveness of\nour data split strategy in mitigating this bias.", "published": "2024-04-29 02:26:15", "link": "http://arxiv.org/abs/2405.01588v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "GPT-4 passes most of the 297 written Polish Board Certification\n  Examinations", "abstract": "Introduction: Recently, the effectiveness of Large Language Models (LLMs) has\nincreased rapidly, allowing them to be used in a great number of applications.\nHowever, the risks posed by the generation of false information through LLMs\nsignificantly limit their applications in sensitive areas such as healthcare,\nhighlighting the necessity for rigorous validations to determine their utility\nand reliability. To date, no study has extensively compared the performance of\nLLMs on Polish medical examinations across a broad spectrum of specialties on a\nvery large dataset. Objectives: This study evaluated the performance of three\nGenerative Pretrained Transformer (GPT) models on the Polish Board\nCertification Exam (Pa\\'nstwowy Egzamin Specjalizacyjny, PES) dataset, which\nconsists of 297 tests. Methods: We developed a software program to download and\nprocess PES exams and tested the performance of GPT models using OpenAI\nApplication Programming Interface. Results: Our findings reveal that GPT-3.5\ndid not pass any of the analyzed exams. In contrast, the GPT-4 models\ndemonstrated the capability to pass the majority of the exams evaluated, with\nthe most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The\nperformance of the GPT models varied significantly, displaying excellence in\nexams related to certain specialties while completely failing others.\nConclusions: The significant progress and impressive performance of LLM models\nhold great promise for the increased application of AI in the field of medicine\nin Poland. For instance, this advancement could lead to the development of\nAI-based medical assistants for healthcare professionals, enhancing the\nefficiency and accuracy of medical services.", "published": "2024-04-29 09:08:22", "link": "http://arxiv.org/abs/2405.01589v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Text and Audio Simplification: Human vs. ChatGPT", "abstract": "Text and audio simplification to increase information comprehension are\nimportant in healthcare. With the introduction of ChatGPT, an evaluation of its\nsimplification performance is needed. We provide a systematic comparison of\nhuman and ChatGPT simplified texts using fourteen metrics indicative of text\ndifficulty. We briefly introduce our online editor where these simplification\ntools, including ChatGPT, are available. We scored twelve corpora using our\nmetrics: six text, one audio, and five ChatGPT simplified corpora. We then\ncompare these corpora with texts simplified and verified in a prior user study.\nFinally, a medical domain expert evaluated these texts and five, new ChatGPT\nsimplified versions. We found that simple corpora show higher similarity with\nthe human simplified texts. ChatGPT simplification moves metrics in the right\ndirection. The medical domain expert evaluation showed a preference for the\nChatGPT style, but the text itself was rated lower for content retention.", "published": "2024-04-29 21:00:33", "link": "http://arxiv.org/abs/2405.01592v1", "categories": ["cs.CL", "cs.AI", "H.4"], "primary_category": "cs.CL"}
{"title": "LLM-SR: Scientific Equation Discovery via Programming with Large\n  Language Models", "abstract": "Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR", "published": "2024-04-29 03:30:06", "link": "http://arxiv.org/abs/2404.18400v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Capabilities of Gemini Models in Medicine", "abstract": "Excellence in a wide variety of medical applications poses considerable\nchallenges for AI, requiring advanced reasoning, access to up-to-date medical\nknowledge and understanding of complex multimodal data. Gemini models, with\nstrong general capabilities in multimodal and long-context reasoning, offer\nexciting possibilities in medicine. Building on these core strengths of Gemini,\nwe introduce Med-Gemini, a family of highly capable multimodal models that are\nspecialized in medicine with the ability to seamlessly use web search, and that\ncan be efficiently tailored to novel modalities using custom encoders. We\nevaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art\n(SoTA) performance on 10 of them, and surpass the GPT-4 model family on every\nbenchmark where a direct comparison is viable, often by a wide margin. On the\npopular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves\nSoTA performance of 91.1% accuracy, using a novel uncertainty-guided search\nstrategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU\n(health & medicine), Med-Gemini improves over GPT-4V by an average relative\nmargin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context\ncapabilities through SoTA performance on a needle-in-a-haystack retrieval task\nfrom long de-identified health records and medical video question answering,\nsurpassing prior bespoke methods using only in-context learning. Finally,\nMed-Gemini's performance suggests real-world utility by surpassing human\nexperts on tasks such as medical text summarization, alongside demonstrations\nof promising potential for multimodal medical dialogue, medical research and\neducation. Taken together, our results offer compelling evidence for\nMed-Gemini's potential, although further rigorous evaluation will be crucial\nbefore real-world deployment in this safety-critical domain.", "published": "2024-04-29 04:11:28", "link": "http://arxiv.org/abs/2404.18416v2", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text\n  Retrievers", "abstract": "Developing effective biomedical retrieval models is important for excelling\nat knowledge-intensive biomedical tasks but still challenging due to the\ndeficiency of sufficient publicly annotated biomedical data and computational\nresources. We present BMRetriever, a series of dense retrievers for enhancing\nbiomedical retrieval via unsupervised pre-training on large biomedical corpora,\nfollowed by instruction fine-tuning on a combination of labeled datasets and\nsynthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify\nBMRetriever's efficacy on various biomedical applications. BMRetriever also\nexhibits strong parameter efficiency, with the 410M variant outperforming\nbaselines up to 11.7 times larger, and the 2B variant matching the performance\nof models with over 5B parameters. The training data and model checkpoints are\nreleased at \\url{https://huggingface.co/BMRetriever} to ensure transparency,\nreproducibility, and application to new domains.", "published": "2024-04-29 05:40:08", "link": "http://arxiv.org/abs/2404.18443v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "q-bio.QM"], "primary_category": "cs.CL"}
{"title": "From ChatGPT, DALL-E 3 to Sora: How has Generative AI Changed Digital\n  Humanities Research and Services?", "abstract": "Generative large-scale language models create the fifth paradigm of\nscientific research, organically combine data science and computational\nintelligence, transform the research paradigm of natural language processing\nand multimodal information processing, promote the new trend of AI-enabled\nsocial science research, and provide new ideas for digital humanities research\nand application. This article profoundly explores the application of\nlarge-scale language models in digital humanities research, revealing their\nsignificant potential in ancient book protection, intelligent processing, and\nacademic innovation. The article first outlines the importance of ancient book\nresources and the necessity of digital preservation, followed by a detailed\nintroduction to developing large-scale language models, such as ChatGPT, and\ntheir applications in document management, content understanding, and\ncross-cultural research. Through specific cases, the article demonstrates how\nAI can assist in the organization, classification, and content generation of\nancient books. Then, it explores the prospects of AI applications in artistic\ninnovation and cultural heritage preservation. Finally, the article explores\nthe challenges and opportunities in the interaction of technology, information,\nand society in the digital humanities triggered by AI technologies.", "published": "2024-04-29 09:03:19", "link": "http://arxiv.org/abs/2404.18518v1", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.DL"}
{"title": "MileBench: Benchmarking MLLMs in Long Context", "abstract": "Despite the advancements and impressive performance of Multimodal Large\nLanguage Models (MLLMs) on benchmarks, their effectiveness in real-world,\nlong-context, and multi-image tasks is unclear due to the benchmarks' limited\nscope. Existing benchmarks often focus on single-image and short-text samples,\nand when assessing multi-image tasks, they either limit the image count or\nfocus on specific task (e.g time-series captioning), potentially obscuring the\nperformance challenges of MLLMs. To address these limitations, we introduce\nMileBench, a pioneering benchmark designed to test the MultImodal Long-contExt\ncapabilities of MLLMs. This benchmark comprises not only multimodal long\ncontexts, but also multiple tasks requiring both comprehension and generation.\nWe establish two distinct evaluation sets, diagnostic and realistic, to\nsystematically assess MLLMs' long-context adaptation capacity and their ability\nto complete tasks in long-context scenarios. Our experimental results, obtained\nfrom testing 22 models, revealed that while the closed-source GPT-4o\noutperforms others, most open-source MLLMs struggle in long-context situations.\nInterestingly, the performance gap tends to widen with an increase in the\nnumber of images. We strongly encourage an intensification of research efforts\ntowards enhancing MLLMs' long-context capabilities, especially in scenarios\ninvolving multiple images.", "published": "2024-04-29 09:19:05", "link": "http://arxiv.org/abs/2404.18532v2", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating and Mitigating Linguistic Discrimination in Large Language\n  Models", "abstract": "By training on text in various languages, large language models (LLMs)\ntypically possess multilingual support and demonstrate remarkable capabilities\nin solving tasks described in different languages. However, LLMs can exhibit\nlinguistic discrimination due to the uneven distribution of training data\nacross languages. That is, LLMs are hard to keep the consistency of responses\nwhen faced with the same task but depicted in different languages.\n  In this study, we first explore the consistency in the LLMs' outputs\nresponding to queries in various languages from two aspects: safety and\nquality. We conduct this analysis with two datasets (AdvBench and NQ) based on\nfour LLMs (Llama2-13b, Gemma-7b, GPT-3.5-turbo and Gemini-pro). The results\nshow that LLMs exhibit stronger human alignment capabilities with queries in\nEnglish, French, Russian, and Spanish (only 1.04\\% of harmful queries\nsuccessfully jailbreak on average) compared to queries in Bengali, Georgian,\nNepali and Maithili (27.7\\% of harmful queries jailbreak successfully on\naverage). Moreover, for queries in English, Danish, Czech and Slovenian, LLMs\ntend to produce responses with a higher quality (with 0.1494 $F_1$ score on\naverage) compared to the other languages. Upon these findings, we propose\nLDFighter, a similarity-based voting, to mitigate the linguistic discrimination\nin LLMs. LDFighter ensures consistent service for different language speakers.\nWe evaluate LDFighter with both benign queries and harmful queries. The results\nshow that LDFighter not only significantly reduces the jailbreak success rate\nbut also improve the response quality on average, demonstrating its\neffectiveness.", "published": "2024-04-29 09:22:54", "link": "http://arxiv.org/abs/2404.18534v2", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.SE"], "primary_category": "cs.CL"}
{"title": "Time Machine GPT", "abstract": "Large language models (LLMs) are often trained on extensive, temporally\nindiscriminate text corpora, reflecting the lack of datasets with temporal\nmetadata. This approach is not aligned with the evolving nature of language.\nConventional methods for creating temporally adapted language models often\ndepend on further pre-training static models on time-specific data. This paper\npresents a new approach: a series of point-in-time LLMs called Time Machine GPT\n(TiMaGPT), specifically designed to be nonprognosticative. This ensures they\nremain uninformed about future factual information and linguistic changes. This\nstrategy is beneficial for understanding language evolution and is of critical\nimportance when applying models in dynamic contexts, such as time-series\nforecasting, where foresight of future information can prove problematic. We\nprovide access to both the models and training datasets.", "published": "2024-04-29 09:34:25", "link": "http://arxiv.org/abs/2404.18543v1", "categories": ["cs.CL", "cs.CE", "cs.LG", "I.2.1, I.2.7"], "primary_category": "cs.CL"}
{"title": "Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?", "abstract": "Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to answers, they are able to produce\nnatural language explanations, either in post-hoc or CoT settings. However, it\nis not clear to what extent they are using the input vision and text modalities\nwhen generating answers or explanations. In this work, we investigate if VLMs\nrely on their input modalities differently when they produce explanations as\nopposed to answers. We also evaluate the self-consistency of VLM decoders in\nboth post-hoc and CoT explanation settings, by extending existing unimodal\ntests and measures to VLM decoders. We find that most tested VLMs are less\nself-consistent than LLMs. Text contributions in all tested VL decoders are\nmore important than image contributions in all examined tasks. However, when\ncomparing explanation generation to answer generation, the contributions of\nimages are significantly stronger for generating explanations compared to\nanswers. This difference is even larger in CoT compared to post-hoc\nexplanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art\nVL decoders on the VALSE benchmark, which before was restricted to VL encoders.\nWe find that the tested VL decoders still struggle with most phenomena tested\nby VALSE.", "published": "2024-04-29 11:52:20", "link": "http://arxiv.org/abs/2404.18624v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "68Txx", "I.2.7; I.2.10"], "primary_category": "cs.CL"}
{"title": "Work Smarter...Not Harder: Efficient Minimization of Dependency Length\n  in SOV Languages", "abstract": "Dependency length minimization is a universally observed quantitative\nproperty of natural languages. However, the extent of dependency length\nminimization, and the cognitive mechanisms through which the language processor\nachieves this minimization remain unclear. This research offers mechanistic\ninsights by postulating that moving a short preverbal constituent next to the\nmain verb explains preverbal constituent ordering decisions better than global\nminimization of dependency length in SOV languages. This approach constitutes a\nleast-effort strategy because it's just one operation but simultaneously\nreduces the length of all preverbal dependencies linked to the main verb. We\ncorroborate this strategy using large-scale corpus evidence across all seven\nSOV languages that are prominently represented in the Universal Dependency\nTreebank. These findings align with the concept of bounded rationality, where\ndecision-making is influenced by 'quick-yet-economical' heuristics rather than\nexhaustive searches for optimal solutions. Overall, this work sheds light on\nthe role of bounded rationality in linguistic decision-making and language\nevolution.", "published": "2024-04-29 13:30:27", "link": "http://arxiv.org/abs/2404.18684v2", "categories": ["cs.CL", "econ.TH", "math.OC"], "primary_category": "cs.CL"}
{"title": "Benchmarking Benchmark Leakage in Large Language Models", "abstract": "Amid the expanding use of pre-training data, the phenomenon of benchmark\ndataset leakage has become increasingly prominent, exacerbated by opaque\ntraining processes and the often undisclosed inclusion of supervised data in\ncontemporary Large Language Models (LLMs). This issue skews benchmark\neffectiveness and fosters potentially unfair comparisons, impeding the field's\nhealthy development. To address this, we introduce a detection pipeline\nutilizing Perplexity and N-gram accuracy, two simple and scalable metrics that\ngauge a model's prediction precision on benchmark, to identify potential data\nleakages. By analyzing 31 LLMs under the context of mathematical reasoning, we\nreveal substantial instances of training even test set misuse, resulting in\npotentially unfair comparisons. These findings prompt us to offer several\nrecommendations regarding model documentation, benchmark setup, and future\nevaluations. Notably, we propose the \"Benchmark Transparency Card\" to encourage\nclear documentation of benchmark utilization, promoting transparency and\nhealthy developments of LLMs. we have made our leaderboard, pipeline\nimplementation, and model predictions publicly available, fostering future\nresearch.", "published": "2024-04-29 16:05:36", "link": "http://arxiv.org/abs/2404.18824v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning\n  Leveraging Weight Decomposition", "abstract": "Despite their exceptional performance on various tasks after fine-tuning,\npre-trained language models (PLMs) face significant challenges due to growing\nprivacy concerns with data in centralized training methods. We consider\nfederated learning (FL) to fine-tune PLMs in this paper. However, the\nsubstantial number of parameters in PLMs poses significant difficulties for\nclient devices with limited communication and computational resources. One\npromising solution is to exploit parameter-efficient fine-tuning (PEFT) into\nFL, which trains a much smaller set of parameters than full parameter\nfine-tuning (FFT). Although remarkably improving training efficiency, PEFT\nmethods may lead to degraded performance especially when data across different\nclients are non i.i.d, as revealed by experimental results. To overcome this,\nwe propose FeDeRA, which extends and improves a widely used PEFT method, i.e.,\nlow-rank adaption (LoRA). FeDeRA follows LoRA by decomposing the weight\nmatrices of the PLMs into low-rank matrices, which allows for more efficient\ncomputation and parameter updates during fine-tuning. Different from LoRA which\nsimply initializes these low-rank matrices by random sampling or zeros, the\nproposed FeDeRA initializes these matrices by the results of performing\nsingular value decomposition (SVD) on the pre-trained weight matrices.\nExtensive experiments across various tasks and datasets show that FeDeRA\noutperforms the considered PEFT baselines and is comparable to or even\nsurpasses FFT method within the FL setting in terms of task performance.\nMoreover, FeDeRA requires only 1% trainable paramentes compared to FFT,\nsignificantly reducing training time costs by more than 90% to achieve the same\ntask performance level. The experimental results also highlight the robustness\nof FeDeRA against data heterogeneity, as it maintains stable task performance\neven as data heterogeneity increases.", "published": "2024-04-29 16:42:26", "link": "http://arxiv.org/abs/2404.18848v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "DPO Meets PPO: Reinforced Token Optimization for RLHF", "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.", "published": "2024-04-29 17:58:30", "link": "http://arxiv.org/abs/2404.18922v3", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Credible, Unreliable or Leaked?: Evidence Verification for Enhanced\n  Automated Fact-checking", "abstract": "Automated fact-checking (AFC) is garnering increasing attention by\nresearchers aiming to help fact-checkers combat the increasing spread of\nmisinformation online. While many existing AFC methods incorporate external\ninformation from the Web to help examine the veracity of claims, they often\noverlook the importance of verifying the source and quality of collected\n\"evidence\". One overlooked challenge involves the reliance on \"leaked\nevidence\", information gathered directly from fact-checking websites and used\nto train AFC systems, resulting in an unrealistic setting for early\nmisinformation detection. Similarly, the inclusion of information from\nunreliable sources can undermine the effectiveness of AFC systems. To address\nthese challenges, we present a comprehensive approach to evidence verification\nand filtering. We create the \"CREDible, Unreliable or LEaked\" (CREDULE)\ndataset, which consists of 91,632 articles classified as Credible, Unreliable\nand Fact checked (Leaked). Additionally, we introduce the EVidence VERification\nNetwork (EVVER-Net), trained on CREDULE to detect leaked and unreliable\nevidence in both short and long texts. EVVER-Net can be used to filter evidence\ncollected from the Web, thus enhancing the robustness of end-to-end AFC\nsystems. We experiment with various language models and show that EVVER-Net can\ndemonstrate impressive performance of up to 91.5% and 94.4% accuracy, while\nleveraging domain credibility scores along with short or long texts,\nrespectively. Finally, we assess the evidence provided by widely-used\nfact-checking datasets including LIAR-PLUS, MOCHEG, FACTIFY, NewsCLIPpings+ and\nVERITE, some of which exhibit concerning rates of leaked and unreliable\nevidence.", "published": "2024-04-29 13:47:04", "link": "http://arxiv.org/abs/2404.18971v1", "categories": ["cs.CL", "cs.CY", "cs.IR", "cs.SI"], "primary_category": "cs.CL"}
{"title": "How Did We Get Here? Summarizing Conversation Dynamics", "abstract": "Throughout a conversation, the way participants interact with each other is\nin constant flux: their tones may change, they may resort to different\nstrategies to convey their points, or they might alter their interaction\npatterns. An understanding of these dynamics can complement that of the actual\nfacts and opinions discussed, offering a more holistic view of the trajectory\nof the conversation: how it arrived at its current state and where it is likely\nheading.\n  In this work, we introduce the task of summarizing the dynamics of\nconversations, by constructing a dataset of human-written summaries, and\nexploring several automated baselines. We evaluate whether such summaries can\ncapture the trajectory of conversations via an established downstream task:\nforecasting whether an ongoing conversation will eventually derail into toxic\nbehavior. We show that they help both humans and automated systems with this\nforecasting task. Humans make predictions three times faster, and with greater\nconfidence, when reading the summaries than when reading the transcripts.\nFurthermore, automated forecasting systems are more accurate when constructing,\nand then predicting based on, summaries of conversation dynamics, compared to\ndirectly predicting on the transcripts.", "published": "2024-04-29 18:00:03", "link": "http://arxiv.org/abs/2404.19007v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "HELPER-X: A Unified Instructable Embodied Agent to Tackle Four\n  Interactive Vision-Language Domains with Memory-Augmented Language Models", "abstract": "Recent research on instructable agents has used memory-augmented Large\nLanguage Models (LLMs) as task planners, a technique that retrieves\nlanguage-program examples relevant to the input instruction and uses them as\nin-context examples in the LLM prompt to improve the performance of the LLM in\ninferring the correct action and task plans. In this technical report, we\nextend the capabilities of HELPER, by expanding its memory with a wider array\nof examples and prompts, and by integrating additional APIs for asking\nquestions. This simple expansion of HELPER into a shared memory enables the\nagent to work across the domains of executing plans from dialogue, natural\nlanguage instruction following, active question asking, and commonsense room\nreorganization. We evaluate the agent on four diverse interactive\nvisual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the\nTidy Task. HELPER-X achieves few-shot, state-of-the-art performance across\nthese benchmarks using a single agent, without requiring in-domain training,\nand remains competitive with agents that have undergone in-domain training.", "published": "2024-04-29 19:12:42", "link": "http://arxiv.org/abs/2404.19065v1", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Q-GroundCAM: Quantifying Grounding in Vision Language Models via GradCAM", "abstract": "Vision and Language Models (VLMs) continue to demonstrate remarkable\nzero-shot (ZS) performance across various tasks. However, many probing studies\nhave revealed that even the best-performing VLMs struggle to capture aspects of\ncompositional scene understanding, lacking the ability to properly ground and\nlocalize linguistic phrases in images. Recent VLM advancements include scaling\nup both model and dataset sizes, additional training objectives and levels of\nsupervision, and variations in the model architectures. To characterize the\ngrounding ability of VLMs, such as phrase grounding, referring expressions\ncomprehension, and relationship understanding, Pointing Game has been used as\nan evaluation metric for datasets with bounding box annotations. In this paper,\nwe introduce a novel suite of quantitative metrics that utilize GradCAM\nactivations to rigorously evaluate the grounding capabilities of pre-trained\nVLMs like CLIP, BLIP, and ALBEF. These metrics offer an explainable and\nquantifiable approach for a more detailed comparison of the zero-shot\ncapabilities of VLMs and enable measuring models' grounding uncertainty. This\ncharacterization reveals interesting tradeoffs between the size of the model,\nthe dataset size, and their performance.", "published": "2024-04-29 22:06:17", "link": "http://arxiv.org/abs/2404.19128v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report", "abstract": "Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted\nmethods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models\n(LLMs). LoRA reduces the number of trainable parameters and memory usage while\nachieving comparable performance to full fine-tuning. We aim to assess the\nviability of training and serving LLMs fine-tuned with LoRA in real-world\napplications. First, we measure the quality of LLMs fine-tuned with quantized\nlow rank adapters across 10 base models and 31 tasks for a total of 310 models.\nWe find that 4-bit LoRA fine-tuned models outperform base models by 34 points\nand GPT-4 by 10 points on average. Second, we investigate the most effective\nbase models for fine-tuning and assess the correlative and predictive\ncapacities of task complexity heuristics in forecasting the outcomes of\nfine-tuning. Finally, we evaluate the latency and concurrency capabilities of\nLoRAX, an open-source Multi-LoRA inference server that facilitates the\ndeployment of multiple LoRA fine-tuned models on a single GPU using shared base\nmodel weights and dynamic adapter loading. LoRAX powers LoRA Land, a web\napplication that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA\nA100 GPU with 80GB memory. LoRA Land highlights the quality and\ncost-effectiveness of employing multiple specialized LLMs over a single,\ngeneral-purpose LLM.", "published": "2024-04-29 04:01:45", "link": "http://arxiv.org/abs/2405.00732v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in\n  Radiology with General-Domain Large Language Model", "abstract": "Recent advancements in Large Multimodal Models (LMMs) have attracted interest\nin their generalization capability with only a few samples in the prompt. This\nprogress is particularly relevant to the medical domain, where the quality and\nsensitivity of data pose unique challenges for model training and application.\nHowever, the dependency on high-quality data for effective in-context learning\nraises questions about the feasibility of these models when encountering with\nthe inevitable variations and errors inherent in real-world medical data. In\nthis paper, we introduce MID-M, a novel framework that leverages the in-context\nlearning capabilities of a general-domain Large Language Model (LLM) to process\nmultimodal data via image descriptions. MID-M achieves a comparable or superior\nperformance to task-specific fine-tuned LMMs and other general-domain ones,\nwithout the extensive domain-specific training or pre-training on multimodal\ndata, with significantly fewer parameters. This highlights the potential of\nleveraging general-domain LLMs for domain-specific tasks and offers a\nsustainable and cost-effective alternative to traditional LMM developments.\nMoreover, the robustness of MID-M against data quality issues demonstrates its\npractical utility in real-world medical domain applications.", "published": "2024-04-29 13:23:33", "link": "http://arxiv.org/abs/2405.01591v1", "categories": ["cs.CL", "cs.AI", "eess.IV"], "primary_category": "cs.CL"}
{"title": "PoPE: Legendre Orthogonal Polynomials Based Position Encoding for Large\n  Language Models", "abstract": "There are several improvements proposed over the baseline Absolute Positional\nEncoding (APE) method used in original transformer. In this study, we aim to\ninvestigate the implications of inadequately representing positional encoding\nin higher dimensions on crucial aspects of the attention mechanism, the model's\ncapacity to learn relative positional information, and the convergence of\nmodels, all stemming from the choice of sinusoidal basis functions. Through a\ncombination of theoretical insights and empirical analyses, we elucidate how\nthese challenges extend beyond APEs and may adversely affect the performance of\nRelative Positional Encoding (RPE) methods, such as Rotatory Positional\nEncoding (RoPE).\n  Subsequently, we introduce an innovative solution termed Orthogonal\nPolynomial Based Positional Encoding (PoPE) to address some of the limitations\nassociated with existing methods. The PoPE method encodes positional\ninformation by leveraging Orthogonal Legendre polynomials. Legendre polynomials\nas basis functions offers several desirable properties for positional encoding,\nincluding improved correlation structure, non-periodicity, orthogonality, and\ndistinct functional forms among polynomials of varying orders. Our experimental\nfindings demonstrate that transformer models incorporating PoPE outperform\nbaseline transformer models on the $Multi30k$ English-to-German translation\ntask, thus establishing a new performance benchmark. Furthermore, PoPE-based\ntransformers exhibit significantly accelerated convergence rates.\n  Additionally, we will present novel theoretical perspectives on position\nencoding based on the superior performance of PoPE.", "published": "2024-04-29 10:30:59", "link": "http://arxiv.org/abs/2405.04585v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A cost minimization approach to fix the vocabulary size in a tokenizer\n  for an End-to-End ASR system", "abstract": "Unlike hybrid speech recognition systems where the use of tokens was\nrestricted to phones, biphones or triphones the choice of tokens in the\nend-to-end ASR systems is derived from the text corpus of the training data.\nThe use of tokenization algorithms like Byte Pair Encoding (BPE) and WordPiece\nis popular in identifying the tokens that are used in the overall training\nprocess of the speech recognition system. Popular toolkits, like ESPNet use a\npre-defined vocabulary size (number of tokens) for these tokenization\nalgorithms, but there is no discussion on how vocabulary size was derived. In\nthis paper, we build a cost function, assuming the tokenization process to be a\nblack-box to enable choosing the number of tokens which might most benefit\nbuilding an end-to-end ASR. We show through experiments on LibriSpeech 100 hour\nset that the performance of an end-to-end ASR system improves when the number\nof tokens are chosen carefully.", "published": "2024-04-29 12:16:21", "link": "http://arxiv.org/abs/2406.02563v1", "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls\n  using Large Language Model for Stock Performance Prediction", "abstract": "In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis.", "published": "2024-04-29 07:11:39", "link": "http://arxiv.org/abs/2404.18470v2", "categories": ["cs.CE", "cs.AI", "cs.CL", "q-fin.RM", "q-fin.TR"], "primary_category": "cs.CE"}
{"title": "Stylus: Automatic Adapter Selection for Diffusion Models", "abstract": "Beyond scaling base models with more data or parameters, fine-tuned adapters\nprovide an alternative way to generate high fidelity, custom images at reduced\ncosts. As such, adapters have been widely adopted by open-source communities,\naccumulating a database of over 100K adapters-most of which are highly\ncustomized with insufficient descriptions. This paper explores the problem of\nmatching the prompt to a set of relevant adapters, built on recent work that\nhighlight the performance gains of composing adapters. We introduce Stylus,\nwhich efficiently selects and automatically composes task-specific adapters\nbased on a prompt's keywords. Stylus outlines a three-stage approach that first\nsummarizes adapters with improved descriptions and embeddings, retrieves\nrelevant adapters, and then further assembles adapters based on prompts'\nkeywords by checking how well they fit the prompt. To evaluate Stylus, we\ndeveloped StylusDocs, a curated dataset featuring 75K adapters with\npre-computed adapter embeddings. In our evaluation on popular Stable Diffusion\ncheckpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as\npreferred, with humans and multimodal models as evaluators, over the base\nmodel. See stylus-diffusion.github.io for more.", "published": "2024-04-29 17:59:16", "link": "http://arxiv.org/abs/2404.18928v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.GR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Foundations of Multisensory Artificial Intelligence", "abstract": "Building multisensory AI systems that learn from multiple sensory inputs such\nas text, speech, video, real-world sensors, wearable devices, and medical data\nholds great promise for impact in many scientific areas with practical\nbenefits, such as in supporting human health and well-being, enabling\nmultimedia content processing, and enhancing real-world autonomous agents. By\nsynthesizing a range of theoretical frameworks and application domains, this\nthesis aims to advance the machine learning foundations of multisensory AI. In\nthe first part, we present a theoretical framework formalizing how modalities\ninteract with each other to give rise to new information for a task. These\ninteractions are the basic building blocks in all multimodal problems, and\ntheir quantification enables users to understand their multimodal datasets,\ndesign principled approaches to learn these interactions, and analyze whether\ntheir model has succeeded in learning. In the second part, we study the design\nof practical multimodal foundation models that generalize over many modalities\nand tasks, which presents a step toward grounding large language models to\nreal-world sensory modalities. We introduce MultiBench, a unified large-scale\nbenchmark across a wide range of modalities, tasks, and research areas,\nfollowed by the cross-modal attention and multimodal transformer architectures\nthat now underpin many of today's multimodal foundation models. Scaling these\narchitectures on MultiBench enables the creation of general-purpose\nmultisensory AI systems, and we discuss our collaborative efforts in applying\nthese models for real-world impact in affective computing, mental health,\ncancer prognosis, and robotics. Finally, we conclude this thesis by discussing\nhow future work can leverage these ideas toward more general, interactive, and\nsafe multisensory AI.", "published": "2024-04-29 14:45:28", "link": "http://arxiv.org/abs/2404.18976v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.MM"], "primary_category": "cs.LG"}
{"title": "Audio-Visual Target Speaker Extraction with Reverse Selective Auditory\n  Attention", "abstract": "Audio-visual target speaker extraction (AV-TSE) aims to extract the specific\nperson's speech from the audio mixture given auxiliary visual cues. Previous\nmethods usually search for the target voice through speech-lip synchronization.\nHowever, this strategy mainly focuses on the existence of target speech, while\nignoring the variations of the noise characteristics, i.e., interference\nspeaker and the background noise. That may result in extracting noisy signals\nfrom the incorrect sound source in challenging acoustic situations. To this\nend, we propose a novel selective auditory attention mechanism, which can\nsuppress interference speakers and non-speech signals to avoid incorrect\nspeaker extraction. By estimating and utilizing the undesired noisy signal\nthrough this mechanism, we design an AV-TSE framework named\nSubtraction-and-ExtrAction network (SEANet) to suppress the noisy signals. We\nconduct abundant experiments by re-implementing three popular AV-TSE methods as\nthe baselines and involving nine metrics for evaluation. The experimental\nresults show that our proposed SEANet achieves state-of-the-art results and\nperforms well for all five datasets. The code can be found in:\nhttps://github.com/TaoRuijie/SEANet.git", "published": "2024-04-29 08:43:57", "link": "http://arxiv.org/abs/2404.18501v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Pi\u00e8ces de viole des Cinq Livres and their statistical signatures: the\n  musical work of Marin Marais and Jordi Savall", "abstract": "This study analyzes the spectrum of audio signals related to the work of\n\"Pi\\`eces de viole des Cinq Livres\" based on the collaborative work between\nMarin Marais and Jordi Savall for the underlying musical information. In\nparticular, we explore the identification of possible statistical signatures\nrelated to this musical work. Based on the complex systems approach, we compute\nthe spectrum of audio signals, analyze and identify their best-fit statistical\ndistributions, and plot their relative frequencies using the scientific pitch\nnotation. Findings suggest that the collection of frequency components related\nto the spectrum of each of the books that form this audio work show highly\nskewed and associated statistical distributions. Therefore, the most frequent\nstatistical distribution that best describes the collection of these audio data\nand may be associated with a singular statistical signature is the exponential.", "published": "2024-04-29 01:35:58", "link": "http://arxiv.org/abs/2404.18355v1", "categories": ["cs.SD", "eess.AS", "stat.AP"], "primary_category": "cs.SD"}
{"title": "A Systematic Evaluation of Adversarial Attacks against Speech Emotion\n  Recognition Models", "abstract": "Speech emotion recognition (SER) is constantly gaining attention in recent\nyears due to its potential applications in diverse fields and thanks to the\npossibility offered by deep learning technologies. However, recent studies have\nshown that deep learning models can be vulnerable to adversarial attacks. In\nthis paper, we systematically assess this problem by examining the impact of\nvarious adversarial white-box and black-box attacks on different languages and\ngenders within the context of SER. We first propose a suitable methodology for\naudio data processing, feature extraction, and CNN-LSTM architecture. The\nobserved outcomes highlighted the significant vulnerability of CNN-LSTM models\nto adversarial examples (AEs). In fact, all the considered adversarial attacks\nare able to significantly reduce the performance of the constructed models.\nFurthermore, when assessing the efficacy of the attacks, minor differences were\nnoted between the languages analyzed as well as between male and female speech.\nIn summary, this work contributes to the understanding of the robustness of\nCNN-LSTM models, particularly in SER scenarios, and the impact of AEs.\nInterestingly, our findings serve as a baseline for a) developing more robust\nalgorithms for SER, b) designing more effective attacks, c) investigating\npossible defenses, d) improved understanding of the vocal differences between\ndifferent languages and genders, and e) overall, enhancing our comprehension of\nthe SER task.", "published": "2024-04-29 09:00:32", "link": "http://arxiv.org/abs/2404.18514v1", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Certification of Speaker Recognition Models to Additive Perturbations", "abstract": "Speaker recognition technology is applied to various tasks, from personal\nvirtual assistants to secure access systems. However, the robustness of these\nsystems against adversarial attacks, particularly to additive perturbations,\nremains a significant challenge. In this paper, we pioneer applying robustness\ncertification techniques to speaker recognition, initially developed for the\nimage domain. Our work covers this gap by transferring and improving randomized\nsmoothing certification techniques against norm-bounded additive perturbations\nfor classification and few-shot learning tasks to speaker recognition. We\ndemonstrate the effectiveness of these methods on VoxCeleb 1 and 2 datasets for\nseveral models. We expect this work to improve the robustness of voice\nbiometrics and accelerate the research of certification methods in the audio\ndomain.", "published": "2024-04-29 15:23:26", "link": "http://arxiv.org/abs/2404.18791v2", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
