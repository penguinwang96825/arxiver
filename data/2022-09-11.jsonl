{"title": "Probing for Understanding of English Verb Classes and Alternations in\n  Large Pre-trained Language Models", "abstract": "We investigate the extent to which verb alternation classes, as described by\nLevin (1993), are encoded in the embeddings of Large Pre-trained Language\nModels (PLMs) such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively\nconstructed diagnostic classifiers for word and sentence-level prediction\ntasks. We follow and expand upon the experiments of Kann et al. (2019), which\naim to probe whether static embeddings encode frame-selectional properties of\nverbs. At both the word and sentence level, we find that contextual embeddings\nfrom PLMs not only outperform non-contextual embeddings, but achieve\nastonishingly high accuracies on tasks across most alternation classes.\nAdditionally, we find evidence that the middle-to-upper layers of PLMs achieve\nbetter performance on average than the lower layers across all probing tasks.", "published": "2022-09-11 08:04:40", "link": "http://arxiv.org/abs/2209.04811v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Suicide Risk in Online Counseling Services: A Study in a\n  Low-Resource Language", "abstract": "With the increased awareness of situations of mental crisis and their\nsocietal impact, online services providing emergency support are becoming\ncommonplace in many countries. Computational models, trained on discussions\nbetween help-seekers and providers, can support suicide prevention by\nidentifying at-risk individuals. However, the lack of domain-specific models,\nespecially in low-resource languages, poses a significant challenge for the\nautomatic detection of suicide risk. We propose a model that combines\npre-trained language models (PLM) with a fixed set of manually crafted (and\nclinically approved) set of suicidal cues, followed by a two-stage fine-tuning\nprocess. Our model achieves 0.91 ROC-AUC and an F2-score of 0.55, significantly\noutperforming an array of strong baselines even early on in the conversation,\nwhich is critical for real-time detection in the field. Moreover, the model\nperforms well across genders and age groups.", "published": "2022-09-11 10:06:14", "link": "http://arxiv.org/abs/2209.04830v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chain of Explanation: New Prompting Method to Generate Higher Quality\n  Natural Language Explanation for Implicit Hate Speech", "abstract": "Recent studies have exploited advanced generative language models to generate\nNatural Language Explanations (NLE) for why a certain text could be hateful. We\npropose the Chain of Explanation (CoE) Prompting method, using the heuristic\nwords and target group, to generate high-quality NLE for implicit hate speech.\nWe improved the BLUE score from 44.0 to 62.3 for NLE generation by providing\naccurate target information. We then evaluate the quality of generated NLE\nusing various automatic metrics and human annotations of informativeness and\nclarity scores.", "published": "2022-09-11 15:04:11", "link": "http://arxiv.org/abs/2209.04889v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Keyphrase Extraction with Data Augmentation and Information\n  Filtering", "abstract": "Keyphrase extraction is one of the essential tasks for document understanding\nin NLP. While the majority of the prior works are dedicated to the formal\nsetting, e.g., books, news or web-blogs, informal texts such as video\ntranscripts are less explored. To address this limitation, in this work we\npresent a novel corpus and method for keyphrase extraction from the transcripts\nof the videos streamed on the Behance platform. More specifically, in this\nwork, a novel data augmentation is proposed to enrich the model with the\nbackground knowledge about the keyphrase extraction task from other domains.\nExtensive experiments on the proposed dataset dataset show the effectiveness of\nthe introduced method.", "published": "2022-09-11 22:38:02", "link": "http://arxiv.org/abs/2209.04951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Tutorial Recommendation for Livestream Videos using Discourse-Level\n  Consistency and Ontology-Based Filtering", "abstract": "Streaming videos is one of the methods for creators to share their creative\nworks with their audience. In these videos, the streamer share how they achieve\ntheir final objective by using various tools in one or several programs for\ncreative projects. To this end, the steps required to achieve the final goal\ncan be discussed. As such, these videos could provide substantial educational\ncontent that can be used to learn how to employ the tools used by the streamer.\nHowever, one of the drawbacks is that the streamer might not provide enough\ndetails for every step. Therefore, for the learners, it might be difficult to\ncatch up with all the steps. In order to alleviate this issue, one solution is\nto link the streaming videos with the relevant tutorial available for the tools\nused in the streaming video. More specifically, a system can analyze the\ncontent of the live streaming video and recommend the most relevant tutorials.\nSince the existing document recommendation models cannot handle this situation,\nin this work, we present a novel dataset and model for the task of tutorial\nrecommendation for live-streamed videos. We conduct extensive analyses on the\nproposed dataset and models, revealing the challenging nature of this task.", "published": "2022-09-11 22:45:57", "link": "http://arxiv.org/abs/2209.04953v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Stability of Syntactic Dialect Classification Over Space and Time", "abstract": "This paper analyses the degree to which dialect classifiers based on\nsyntactic representations remain stable over space and time. While previous\nwork has shown that the combination of grammar induction and geospatial text\nclassification produces robust dialect models, we do not know what influence\nboth changing grammars and changing populations have on dialect models. This\npaper constructs a test set for 12 dialects of English that spans three years\nat monthly intervals with a fixed spatial distribution across 1,120 cities.\nSyntactic representations are formulated within the usage-based Construction\nGrammar paradigm (CxG). The decay rate of classification performance for each\ndialect over time allows us to identify regions undergoing syntactic change.\nAnd the distribution of classification accuracy within dialect regions allows\nus to identify the degree to which the grammar of a dialect is internally\nheterogeneous. The main contribution of this paper is to show that a rigorous\nevaluation of dialect classification models can be used to find both variation\nover space and change over time.", "published": "2022-09-11 23:14:59", "link": "http://arxiv.org/abs/2209.04958v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Testing Pre-trained Language Models' Understanding of Distributivity via\n  Causal Mediation Analysis", "abstract": "To what extent do pre-trained language models grasp semantic knowledge\nregarding the phenomenon of distributivity? In this paper, we introduce\nDistNLI, a new diagnostic dataset for natural language inference that targets\nthe semantic difference arising from distributivity, and employ the causal\nmediation analysis framework to quantify the model behavior and explore the\nunderlying mechanism in this semantically-related task. We find that the extent\nof models' understanding is associated with model size and vocabulary size. We\nalso provide insights into how models encode such high-level semantic\nknowledge.", "published": "2022-09-11 00:33:28", "link": "http://arxiv.org/abs/2209.04761v2", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Adaptive Perturbation-Based Gradient Estimation for Discrete Latent\n  Variable Models", "abstract": "The integration of discrete algorithmic components in deep learning\narchitectures has numerous applications. Recently, Implicit Maximum Likelihood\nEstimation (IMLE, Niepert, Minervini, and Franceschi 2021), a class of gradient\nestimators for discrete exponential family distributions, was proposed by\ncombining implicit differentiation through perturbation with the path-wise\ngradient estimator. However, due to the finite difference approximation of the\ngradients, it is especially sensitive to the choice of the finite difference\nstep size, which needs to be specified by the user. In this work, we present\nAdaptive IMLE (AIMLE), the first adaptive gradient estimator for complex\ndiscrete distributions: it adaptively identifies the target distribution for\nIMLE by trading off the density of gradient information with the degree of bias\nin the gradient estimates. We empirically evaluate our estimator on synthetic\nexamples, as well as on Learning to Explain, Discrete Variational\nAuto-Encoders, and Neural Relational Inference tasks. In our experiments, we\nshow that our adaptive gradient estimator can produce faithful estimates while\nrequiring orders of magnitude fewer samples than other gradient estimators.", "published": "2022-09-11 13:32:39", "link": "http://arxiv.org/abs/2209.04862v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Instruction-driven history-aware policies for robotic manipulations", "abstract": "In human environments, robots are expected to accomplish a variety of\nmanipulation tasks given simple natural language instructions. Yet, robotic\nmanipulation is extremely challenging as it requires fine-grained motor\ncontrol, long-term memory as well as generalization to previously unseen tasks\nand environments. To address these challenges, we propose a unified\ntransformer-based approach that takes into account multiple inputs. In\nparticular, our transformer architecture integrates (i) natural language\ninstructions and (ii) multi-view scene observations while (iii) keeping track\nof the full history of observations and actions. Such an approach enables\nlearning dependencies between history and instructions and improves\nmanipulation precision using multiple views. We evaluate our method on the\nchallenging RLBench benchmark and on a real-world robot. Notably, our approach\nscales to 74 diverse RLBench tasks and outperforms the state of the art. We\nalso address instruction-conditioned tasks and demonstrate excellent\ngeneralization to previously unseen variations.", "published": "2022-09-11 16:28:25", "link": "http://arxiv.org/abs/2209.04899v3", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "primary_category": "cs.RO"}
{"title": "Applying wav2vec2 for Speech Recognition on Bengali Common Voices\n  Dataset", "abstract": "Speech is inherently continuous, where discrete words, phonemes and other\nunits are not clearly segmented, and so speech recognition has been an active\nresearch problem for decades. In this work we have fine-tuned wav2vec 2.0 to\nrecognize and transcribe Bengali speech -- training it on the Bengali Common\nVoice Speech Dataset. After training for 71 epochs, on a training set\nconsisting of 36919 mp3 files, we achieved a training loss of 0.3172 and WER of\n0.2524 on a validation set of size 7,747. Using a 5-gram language model, the\nLevenshtein Distance was 2.6446 on a test set of size 7,747. Then the training\nset and validation set were combined, shuffled and split into 85-15 ratio.\nTraining for 7 more epochs on this combined dataset yielded an improved\nLevenshtein Distance of 2.60753 on the test set. Our model was the best\nperforming one, achieving a Levenshtein Distance of 6.234 on a hidden dataset,\nwhich was 1.1049 units lower than other competing submissions.", "published": "2022-09-11 15:05:42", "link": "http://arxiv.org/abs/2209.06581v1", "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "eess.AS"}
