{"title": "SIT at MixMT 2022: Fluent Translation Built on Giant Pre-trained Models", "abstract": "This paper describes the Stevens Institute of Technology's submission for the\nWMT 2022 Shared Task: Code-mixed Machine Translation (MixMT). The task\nconsisted of two subtasks, subtask $1$ Hindi/English to Hinglish and subtask\n$2$ Hinglish to English translation. Our findings lie in the improvements made\nthrough the use of large pre-trained multilingual NMT models and in-domain\ndatasets, as well as back-translation and ensemble techniques. The translation\noutput is automatically evaluated against the reference translations using\nROUGE-L and WER. Our system achieves the $1^{st}$ position on subtask $2$\naccording to ROUGE-L, WER, and human evaluation, $1^{st}$ position on subtask\n$1$ according to WER and human evaluation, and $3^{rd}$ position on subtask $1$\nwith respect to ROUGE-L metric.", "published": "2022-10-21 01:49:40", "link": "http://arxiv.org/abs/2210.11670v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SLING: Sino Linguistic Evaluation of Large Language Models", "abstract": "To understand what kinds of linguistic knowledge are encoded by pretrained\nChinese language models (LMs), we introduce the benchmark of Sino LINGuistics\n(SLING), which consists of 38K minimal sentence pairs in Mandarin Chinese\ngrouped into 9 high-level linguistic phenomena. Each pair demonstrates the\nacceptability contrast of a specific syntactic or semantic phenomenon (e.g.,\nThe keys are lost vs. The keys is lost), and an LM should assign lower\nperplexity to the acceptable sentence. In contrast to the CLiMP dataset (Xiang\net al., 2021), which also contains Chinese minimal pairs and was created by\ntranslating the vocabulary of the English BLiMP dataset, the minimal pairs in\nSLING are derived primarily by applying syntactic and lexical transformations\nto naturally-occurring, linguist-annotated sentences from the Chinese Treebank\n9.0, thus addressing severe issues in CLiMP's data generation process. We test\n18 publicly available pretrained monolingual (e.g., BERT-base-zh, CPM) and\nmulti-lingual (e.g., mT5, XLM) language models on SLING. Our experiments show\nthat the average accuracy for LMs is far below human performance (69.7% vs.\n97.1%), while BERT-base-zh achieves the highest accuracy (84.8%) of all tested\nLMs, even much larger ones. Additionally, we find that most LMs have a strong\ngender and number (singular/plural) bias, and they perform better on local\nphenomena than hierarchical ones.", "published": "2022-10-21 02:29:39", "link": "http://arxiv.org/abs/2210.11689v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Metric-guided Distillation: Distilling Knowledge from the Metric to\n  Ranker and Retriever for Generative Commonsense Reasoning", "abstract": "Commonsense generation aims to generate a realistic sentence describing a\ndaily scene under the given concepts, which is very challenging, since it\nrequires models to have relational reasoning and compositional generalization\ncapabilities. Previous work focuses on retrieving prototype sentences for the\nprovided concepts to assist generation. They first use a sparse retriever to\nretrieve candidate sentences, then re-rank the candidates with a ranker.\nHowever, the candidates returned by their ranker may not be the most relevant\nsentences, since the ranker treats all candidates equally without considering\ntheir relevance to the reference sentences of the given concepts. Another\nproblem is that re-ranking is very expensive, but only using retrievers will\nseriously degrade the performance of their generation models. To solve these\nproblems, we propose the metric distillation rule to distill knowledge from the\nmetric (e.g., BLEU) to the ranker. We further transfer the critical knowledge\nsummarized by the distilled ranker to the retriever. In this way, the relevance\nscores of candidate sentences predicted by the ranker and retriever will be\nmore consistent with their quality measured by the metric. Experimental results\non the CommonGen benchmark verify the effectiveness of our proposed method: (1)\nOur generation model with the distilled ranker achieves a new state-of-the-art\nresult. (2) Our generation model with the distilled retriever even surpasses\nthe previous SOTA.", "published": "2022-10-21 03:34:24", "link": "http://arxiv.org/abs/2210.11708v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Empathetic Dialogue Generation via Sensitive Emotion Recognition and\n  Sensible Knowledge Selection", "abstract": "Empathy, which is widely used in psychological counselling, is a key trait of\neveryday human conversations. Equipped with commonsense knowledge, current\napproaches to empathetic response generation focus on capturing implicit\nemotion within dialogue context, where the emotions are treated as a static\nvariable throughout the conversations. However, emotions change dynamically\nbetween utterances, which makes previous works difficult to perceive the\nemotion flow and predict the correct emotion of the target response, leading to\ninappropriate response. Furthermore, simply importing commonsense knowledge\nwithout harmonization may trigger the conflicts between knowledge and emotion,\nwhich confuse the model to choose incorrect information to guide the generation\nprocess. To address the above problems, we propose a Serial Encoding and\nEmotion-Knowledge interaction (SEEK) method for empathetic dialogue generation.\nWe use a fine-grained encoding strategy which is more sensitive to the emotion\ndynamics (emotion flow) in the conversations to predict the emotion-intent\ncharacteristic of response. Besides, we design a novel framework to model the\ninteraction between knowledge and emotion to generate more sensible response.\nExtensive experiments on EmpatheticDialogues demonstrate that SEEK outperforms\nthe strong baselines in both automatic and manual evaluations.", "published": "2022-10-21 03:51:18", "link": "http://arxiv.org/abs/2210.11715v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MCSCSet: A Specialist-annotated Dataset for Medical-domain Chinese\n  Spelling Correction", "abstract": "Chinese Spelling Correction (CSC) is gaining increasing attention due to its\npromise of automatically detecting and correcting spelling errors in Chinese\ntexts. Despite its extensive use in many applications, like search engines and\noptical character recognition systems, little has been explored in medical\nscenarios in which complex and uncommon medical entities are easily misspelled.\nCorrecting the misspellings of medical entities is arguably more difficult than\nthose in the open domain due to its requirements of specificdomain knowledge.\nIn this work, we define the task of Medical-domain Chinese Spelling Correction\nand propose MCSCSet, a large scale specialist-annotated dataset that contains\nabout 200k samples. In contrast to the existing open-domain CSC datasets,\nMCSCSet involves: i) extensive real-world medical queries collected from\nTencent Yidian, ii) corresponding misspelled sentences manually annotated by\nmedical specialists. To ensure automated dataset curation, MCSCSet further\noffers a medical confusion set consisting of the commonly misspelled characters\nof given Chinese medical terms. This enables one to create the medical\nmisspelling dataset automatically. Extensive empirical studies have shown\nsignificant performance gaps between the open-domain and medical-domain\nspelling correction, highlighting the need to develop high-quality datasets\nthat allow for Chinese spelling correction in specific domains. Moreover, our\nwork benchmarks several representative Chinese spelling correction models,\nestablishing baselines for future work.", "published": "2022-10-21 04:11:25", "link": "http://arxiv.org/abs/2210.11720v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An Exploration of Data Efficiency in Intra-Dataset Task Transfer for\n  Dialog Understanding", "abstract": "Transfer learning is an exciting area of Natural Language Processing that has\nthe potential to both improve model performance and increase data efficiency.\nThis study explores the effects of varying quantities of target task training\ndata on sequential transfer learning in the dialog domain. We hypothesize that\na model can utilize the information learned from a source task to better learn\na target task, thereby reducing the number of target task training samples\nrequired. Unintuitively, our data shows that often target task training data\nsize has minimal effect on how sequential transfer learning performs compared\nto the same model without transfer learning. Our results lead us to believe\nthat this unexpected result could be due to the effects of catastrophic\nforgetting, motivating further work into methods that prevent such forgetting.", "published": "2022-10-21 04:36:46", "link": "http://arxiv.org/abs/2210.11729v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TransLIST: A Transformer-Based Linguistically Informed Sanskrit\n  Tokenizer", "abstract": "Sanskrit Word Segmentation (SWS) is essential in making digitized texts\navailable and in deploying downstream tasks. It is, however, non-trivial\nbecause of the sandhi phenomenon that modifies the characters at the word\nboundaries, and needs special treatment. Existing lexicon driven approaches for\nSWS make use of Sanskrit Heritage Reader, a lexicon-driven shallow parser, to\ngenerate the complete candidate solution space, over which various methods are\napplied to produce the most valid solution. However, these approaches fail\nwhile encountering out-of-vocabulary tokens. On the other hand, purely\nengineering methods for SWS have made use of recent advances in deep learning,\nbut cannot make use of the latent word information on availability.\n  To mitigate the shortcomings of both families of approaches, we propose\nTransformer based Linguistically Informed Sanskrit Tokenizer (TransLIST)\nconsisting of (1) a module that encodes the character input along with\nlatent-word information, which takes into account the sandhi phenomenon\nspecific to SWS and is apt to work with partial or no candidate solutions, (2)\na novel soft-masked attention to prioritize potential candidate words and (3) a\nnovel path ranking algorithm to rectify the corrupted predictions. Experiments\non the benchmark datasets for SWS show that TransLIST outperforms the current\nstate-of-the-art system by an average 7.2 points absolute gain in terms of\nperfect match (PM) metric. The codebase and datasets are publicly available at\nhttps://github.com/rsingha108/TransLIST", "published": "2022-10-21 06:15:40", "link": "http://arxiv.org/abs/2210.11753v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "University of Cape Town's WMT22 System: Multilingual Machine Translation\n  for Southern African Languages", "abstract": "The paper describes the University of Cape Town's submission to the\nconstrained track of the WMT22 Shared Task: Large-Scale Machine Translation\nEvaluation for African Languages. Our system is a single multilingual\ntranslation model that translates between English and 8 South / South East\nAfrican Languages, as well as between specific pairs of the African languages.\nWe used several techniques suited for low-resource machine translation (MT),\nincluding overlap BPE, back-translation, synthetic training data generation,\nand adding more translation directions during training. Our results show the\nvalue of these techniques, especially for directions where very little or no\nbilingual training data is available.", "published": "2022-10-21 06:31:24", "link": "http://arxiv.org/abs/2210.11757v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntax-guided Localized Self-attention by Constituency Syntactic\n  Distance", "abstract": "Recent works have revealed that Transformers are implicitly learning the\nsyntactic information in its lower layers from data, albeit is highly dependent\non the quality and scale of the training data. However, learning syntactic\ninformation from data is not necessary if we can leverage an external syntactic\nparser, which provides better parsing quality with well-defined syntactic\nstructures. This could potentially improve Transformer's performance and sample\nefficiency. In this work, we propose a syntax-guided localized self-attention\nfor Transformer that allows directly incorporating grammar structures from an\nexternal constituency parser. It prohibits the attention mechanism to\noverweight the grammatically distant tokens over close ones. Experimental\nresults show that our model could consistently improve translation performance\non a variety of machine translation datasets, ranging from small to large\ndataset sizes, and with different source languages.", "published": "2022-10-21 06:37:25", "link": "http://arxiv.org/abs/2210.11759v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Unintended Social Bias in Toxic Language Datasets", "abstract": "With the rise of online hate speech, automatic detection of Hate Speech,\nOffensive texts as a natural language processing task is getting popular.\nHowever, very little research has been done to detect unintended social bias\nfrom these toxic language datasets. This paper introduces a new dataset\nToxicBias curated from the existing dataset of Kaggle competition named \"Jigsaw\nUnintended Bias in Toxicity Classification\". We aim to detect social biases,\ntheir categories, and targeted groups. The dataset contains instances annotated\nfor five different bias categories, viz., gender, race/ethnicity, religion,\npolitical, and LGBTQ. We train transformer-based models using our curated\ndatasets and report baseline performance for bias identification, target\ngeneration, and bias implications. Model biases and their mitigation are also\ndiscussed in detail. Our study motivates a systematic extraction of social bias\ndata from toxic language datasets. All the codes and dataset used for\nexperiments in this work are publicly available", "published": "2022-10-21 06:50:12", "link": "http://arxiv.org/abs/2210.11762v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CEFR-Based Sentence Difficulty Annotation and Assessment", "abstract": "Controllable text simplification is a crucial assistive technique for\nlanguage learning and teaching. One of the primary factors hindering its\nadvancement is the lack of a corpus annotated with sentence difficulty levels\nbased on language ability descriptions. To address this problem, we created the\nCEFR-based Sentence Profile (CEFR-SP) corpus, containing 17k English sentences\nannotated with the levels based on the Common European Framework of Reference\nfor Languages assigned by English-education professionals. In addition, we\npropose a sentence-level assessment model to handle unbalanced level\ndistribution because the most basic and highly proficient sentences are\nnaturally scarce. In the experiments in this study, our method achieved a\nmacro-F1 score of 84.5% in the level assessment, thus outperforming strong\nbaselines employed in readability assessment.", "published": "2022-10-21 07:03:30", "link": "http://arxiv.org/abs/2210.11766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "InforMask: Unsupervised Informative Masking for Language Model\n  Pretraining", "abstract": "Masked language modeling is widely used for pretraining large language models\nfor natural language understanding (NLU). However, random masking is\nsuboptimal, allocating an equal masking rate for all tokens. In this paper, we\npropose InforMask, a new unsupervised masking strategy for training masked\nlanguage models. InforMask exploits Pointwise Mutual Information (PMI) to\nselect the most informative tokens to mask. We further propose two\noptimizations for InforMask to improve its efficiency. With a one-off\npreprocessing step, InforMask outperforms random masking and previously\nproposed masking strategies on the factual recall benchmark LAMA and the\nquestion answering benchmark SQuAD v1 and v2.", "published": "2022-10-21 07:10:56", "link": "http://arxiv.org/abs/2210.11771v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Analyzing and Evaluating Faithfulness in Dialogue Summarization", "abstract": "Dialogue summarization is abstractive in nature, making it suffer from\nfactual errors. The factual correctness of summaries has the highest priority\nbefore practical applications. Many efforts have been made to improve\nfaithfulness in text summarization. However, there is a lack of systematic\nstudy on dialogue summarization systems. In this work, we first perform the\nfine-grained human analysis on the faithfulness of dialogue summaries and\nobserve that over 35% of generated summaries are faithfully inconsistent\nrespective the source dialogues. Furthermore, we present a new model-level\nfaithfulness evaluation method. It examines generation models with multi-choice\nquestions created by rule-based transformations. Experimental results show that\nour evaluation schema is a strong proxy for the factual correctness of\nsummarization models. The human-annotated faithfulness samples and the\nevaluation toolkit are released to facilitate future research toward faithful\ndialogue summarization.", "published": "2022-10-21 07:22:43", "link": "http://arxiv.org/abs/2210.11777v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modeling Document-level Temporal Structures for Building Temporal\n  Dependency Graphs", "abstract": "We propose to leverage news discourse profiling to model document-level\ntemporal structures for building temporal dependency graphs. Our key\nobservation is that the functional roles of sentences used for profiling news\ndiscourse signify different time frames relevant to a news story and can,\ntherefore, help to recover the global temporal structure of a document. Our\nanalyses and experiments with the widely used knowledge distillation technique\nshow that discourse profiling effectively identifies distant inter-sentence\nevent and (or) time expression pairs that are temporally related and otherwise\ndifficult to locate.", "published": "2022-10-21 07:45:17", "link": "http://arxiv.org/abs/2210.11787v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Rescue Implicit and Long-tail Cases: Nearest Neighbor Relation\n  Extraction", "abstract": "Relation extraction (RE) has achieved remarkable progress with the help of\npre-trained language models. However, existing RE models are usually incapable\nof handling two situations: implicit expressions and long-tail relation types,\ncaused by language complexity and data sparsity. In this paper, we introduce a\nsimple enhancement of RE using $k$ nearest neighbors ($k$NN-RE). $k$NN-RE\nallows the model to consult training relations at test time through a\nnearest-neighbor search and provides a simple yet effective means to tackle the\ntwo issues above. Additionally, we observe that $k$NN-RE serves as an effective\nway to leverage distant supervision (DS) data for RE. Experimental results show\nthat the proposed $k$NN-RE achieves state-of-the-art performances on a variety\nof supervised RE datasets, i.e., ACE05, SciERC, and Wiki80, along with\noutperforming the best model to date on the i2b2 and Wiki80 datasets in the\nsetting of allowing using DS. Our code and models are available at:\nhttps://github.com/YukinoWan/kNN-RE.", "published": "2022-10-21 08:25:10", "link": "http://arxiv.org/abs/2210.11800v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Discovering New Intents Using Latent Variables", "abstract": "Discovering new intents is of great significance to establishing Bootstrapped\nTask-Oriented Dialogue System. Most existing methods either lack the ability to\ntransfer prior knowledge in the known intent data or fall into the dilemma of\nforgetting prior knowledge in the follow-up. More importantly, these methods do\nnot deeply explore the intrinsic structure of unlabeled data, so they can not\nseek out the characteristics that make an intent in general. In this paper,\nstarting from the intuition that discovering intents could be beneficial to the\nidentification of the known intents, we propose a probabilistic framework for\ndiscovering intents where intent assignments are treated as latent variables.\nWe adopt Expectation Maximization framework for optimization. Specifically, In\nE-step, we conduct discovering intents and explore the intrinsic structure of\nunlabeled data by the posterior of intent assignments. In M-step, we alleviate\nthe forgetting of prior knowledge transferred from known intents by optimizing\nthe discrimination of labeled data. Extensive experiments conducted in three\nchallenging real-world datasets demonstrate our method can achieve substantial\nimprovements.", "published": "2022-10-21 08:29:45", "link": "http://arxiv.org/abs/2210.11804v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Robustifying Sentiment Classification by Maximally Exploiting Few\n  Counterfactuals", "abstract": "For text classification tasks, finetuned language models perform remarkably\nwell. Yet, they tend to rely on spurious patterns in training data, thus\nlimiting their performance on out-of-distribution (OOD) test data. Among recent\nmodels aiming to avoid this spurious pattern problem, adding extra\ncounterfactual samples to the training data has proven to be very effective.\nYet, counterfactual data generation is costly since it relies on human\nannotation. Thus, we propose a novel solution that only requires annotation of\na small fraction (e.g., 1%) of the original training data, and uses automatic\ngeneration of extra counterfactuals in an encoding vector space. We demonstrate\nthe effectiveness of our approach in sentiment classification, using IMDb data\nfor training and other sets for OOD tests (i.e., Amazon, SemEval and Yelp). We\nachieve noticeable accuracy improvements by adding only 1% manual\ncounterfactuals: +3% compared to adding +100% in-distribution training samples,\n+1.3% compared to alternate counterfactual approaches.", "published": "2022-10-21 08:30:09", "link": "http://arxiv.org/abs/2210.11805v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The use of the word\n  \"\\{gamma}\\u{psion}\u03bd\u03b1\u03b9\\k{appa}\u03bf\\k{appa}\u03c4\u03bf\u03bd\u03b9\u03b1\"\n  (femicide) in Greek-speaking Twitter", "abstract": "Between 2019 and 2022, Greek media attention has been attracted by a rather\nunusually high number of femicide cases which have been trending for several\nweeks up to months in the public debate and one of the contributing factors is\nthe feedback loop between traditional media and social media. In this paper we\nare investigating the use of the term\n\"\\{gamma}\\u{psion}{\\nu}{\\alpha}{\\iota}\\k{appa}{\\omicron}\\k{appa}{\\tau}{\\omicron}{\\nu}{\\iota}{\\alpha}\"\n(femicide) in Greek speaking twitter. More specifically, we approach the\nproblem from a stance detection perspective, aiming to automatically identify\nuser position with regards to the feministic semantics of the word. We also\ndiscuss findings from an identity analysis perspective and intercorrelations\nwith hate speech that have been identified in the collected corpus of tweets.", "published": "2022-10-21 09:31:06", "link": "http://arxiv.org/abs/2210.11837v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Spectral Probing", "abstract": "Linguistic information is encoded at varying timescales (subwords, phrases,\netc.) and communicative levels, such as syntax and semantics. Contextualized\nembeddings have analogously been found to capture these phenomena at\ndistinctive layers and frequencies. Leveraging these findings, we develop a\nfully learnable frequency filter to identify spectral profiles for any given\ntask. It enables vastly more granular analyses than prior handcrafted filters,\nand improves on efficiency. After demonstrating the informativeness of spectral\nprobing over manual filters in a monolingual setting, we investigate its\nmultilingual characteristics across seven diverse NLP tasks in six languages.\nOur analyses identify distinctive spectral profiles which quantify cross-task\nsimilarity in a linguistically intuitive manner, while remaining consistent\nacross languages-highlighting their potential as robust, lightweight task\ndescriptors.", "published": "2022-10-21 10:29:20", "link": "http://arxiv.org/abs/2210.11860v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "STAR: SQL Guided Pre-Training for Context-dependent Text-to-SQL Parsing", "abstract": "In this paper, we propose a novel SQL guided pre-training framework STAR for\ncontext-dependent text-to-SQL parsing, which leverages contextual information\nto enrich natural language (NL) utterance and table schema representations for\ntext-to-SQL conversations. Concretely, we propose two novel pre-training\nobjectives which respectively explore the context-dependent interactions of NL\nutterances and SQL queries within each text-to-SQL conversation: (i) schema\nstate tracking (SST) objective that tracks and explores the schema states of\ncontext-dependent SQL queries in the form of schema-states by predicting and\nupdating the value of each schema slot during interaction; (ii) utterance\ndependency tracking (UDT) objective that employs weighted contrastive learning\nto pull together two semantically similar NL utterances and push away the\nrepresentations of semantically dissimilar NL utterances within each\nconversation. In addition, we construct a high-quality large-scale\ncontext-dependent text-to-SQL conversation corpus to pre-train STAR. Extensive\nexperiments show that STAR achieves new state-of-the-art performance on two\ndownstream benchmarks (SParC and CoSQL), significantly outperforming previous\npre-training methods and ranking first on the leaderboard. We believe the\nrelease of the constructed corpus, codebase and pre-trained STAR checkpoints\nwould push forward the research in this area. For reproducibility, we release\nour code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/star.", "published": "2022-10-21 11:30:07", "link": "http://arxiv.org/abs/2210.11888v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Semi-supervised Approach for a Better Translation of Sentiment in\n  Dialectical Arabic UGT", "abstract": "In the online world, Machine Translation (MT) systems are extensively used to\ntranslate User-Generated Text (UGT) such as reviews, tweets, and social media\nposts, where the main message is often the author's positive or negative\nattitude towards the topic of the text. However, MT systems still lack accuracy\nin some low-resource languages and sometimes make critical translation errors\nthat completely flip the sentiment polarity of the target word or phrase and\nhence delivers a wrong affect message. This is particularly noticeable in texts\nthat do not follow common lexico-grammatical standards such as the dialectical\nArabic (DA) used on online platforms. In this research, we aim to improve the\ntranslation of sentiment in UGT written in the dialectical versions of the\nArabic language to English. Given the scarcity of gold-standard parallel data\nfor DA-EN in the UGT domain, we introduce a semi-supervised approach that\nexploits both monolingual and parallel data for training an NMT system\ninitialised by a cross-lingual language model trained with supervised and\nunsupervised modeling objectives. We assess the accuracy of sentiment\ntranslation by our proposed system through a numerical 'sentiment-closeness'\nmeasure as well as human evaluation. We will show that our semi-supervised MT\nsystem can significantly help with correcting sentiment errors detected in the\nonline translation of dialectical Arabic UGT.", "published": "2022-10-21 11:55:55", "link": "http://arxiv.org/abs/2210.11899v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploration of the Usage of Color Terms by Color-blind Participants in\n  Online Discussion Platforms", "abstract": "Prominent questions about the role of sensory vs. linguistic input in the way\nwe acquire and use language have been extensively studied in the\npsycholinguistic literature. However, the relative effect of various factors in\na person's overall experience on their linguistic system remains unclear. We\nstudy this question by making a step forward towards a better understanding of\nthe conceptual perception of colors by color-blind individuals, as reflected in\ntheir spontaneous linguistic productions. Using a novel and carefully curated\ndataset, we show that red-green color-blind speakers use the \"red\" and \"green\"\ncolor terms in less predictable contexts, and in linguistic environments\nevoking mental image to a lower extent, when compared to their normal-sighted\ncounterparts. These findings shed some new and interesting light on the role of\nsensory experience on our linguistic system.", "published": "2022-10-21 12:11:10", "link": "http://arxiv.org/abs/2210.11905v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "$m^4Adapter$: Multilingual Multi-Domain Adaptation for Machine\n  Translation with a Meta-Adapter", "abstract": "Multilingual neural machine translation models (MNMT) yield state-of-the-art\nperformance when evaluated on data from a domain and language pair seen at\ntraining time. However, when a MNMT model is used to translate under domain\nshift or to a new language pair, performance drops dramatically. We consider a\nvery challenging scenario: adapting the MNMT model both to a new domain and to\na new language pair at the same time. In this paper, we propose $m^4Adapter$\n(Multilingual Multi-Domain Adaptation for Machine Translation with a\nMeta-Adapter), which combines domain and language knowledge using meta-learning\nwith adapters. We present results showing that our approach is a\nparameter-efficient solution which effectively adapts a model to both a new\nlanguage pair and a new domain, while outperforming other adapter methods. An\nablation study also shows that our approach more effectively transfers domain\nknowledge across different languages and language information across different\ndomains.", "published": "2022-10-21 12:25:05", "link": "http://arxiv.org/abs/2210.11912v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Named Entity Detection and Injection for Direct Speech Translation", "abstract": "In a sentence, certain words are critical for its semantic. Among them, named\nentities (NEs) are notoriously challenging for neural models. Despite their\nimportance, their accurate handling has been neglected in speech-to-text (S2T)\ntranslation research, and recent work has shown that S2T models perform poorly\nfor locations and notably person names, whose spelling is challenging unless\nknown in advance. In this work, we explore how to leverage dictionaries of NEs\nknown to likely appear in a given context to improve S2T model outputs. Our\nexperiments show that we can reliably detect NEs likely present in an utterance\nstarting from S2T encoder outputs. Indeed, we demonstrate that the current\ndetection quality is sufficient to improve NE accuracy in the translation with\na 31% reduction in person name errors.", "published": "2022-10-21 14:16:51", "link": "http://arxiv.org/abs/2210.11981v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Shift-Reduce Task-Oriented Semantic Parsing with Stack-Transformers", "abstract": "Intelligent voice assistants, such as Apple Siri and Amazon Alexa, are widely\nused nowadays. These task-oriented dialogue systems require a semantic parsing\nmodule in order to process user utterances and understand the action to be\nperformed. This semantic parsing component was initially implemented by\nrule-based or statistical slot-filling approaches for processing simple\nqueries; however, the appearance of more complex utterances demanded the\napplication of shift-reduce parsers or sequence-to-sequence models. Although\nshift-reduce approaches were initially considered the most promising option,\nthe emergence of sequence-to-sequence neural systems has propelled them to the\nforefront as the highest-performing method for this particular task. In this\narticle, we advance the research on shift-reduce semantic parsing for\ntask-oriented dialogue. We implement novel shift-reduce parsers that rely on\nStack-Transformers. This framework allows to adequately model transition\nsystems on the Transformer neural architecture, notably boosting shift-reduce\nparsing performance. Furthermore, our approach goes beyond the conventional\ntop-down algorithm: we incorporate alternative bottom-up and in-order\ntransition systems derived from constituency parsing into the realm of\ntask-oriented parsing. We extensively test our approach on multiple domains\nfrom the Facebook TOP benchmark, improving over existing shift-reduce parsers\nand state-of-the-art sequence-to-sequence models in both high-resource and\nlow-resource settings. We also empirically prove that the in-order algorithm\nsubstantially outperforms the commonly-used top-down strategy. Through the\ncreation of innovative transition systems and harnessing the capabilities of a\nrobust neural architecture, our study showcases the superiority of shift-reduce\nparsers over leading sequence-to-sequence methods on the main benchmark.", "published": "2022-10-21 14:19:47", "link": "http://arxiv.org/abs/2210.11984v2", "categories": ["cs.CL", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Joint Speech Translation and Named Entity Recognition", "abstract": "Modern automatic translation systems aim at place the human at the center by\nproviding contextual support and knowledge. In this context, a critical task is\nenriching the output with information regarding the mentioned entities, which\nis currently achieved processing the generated translation with named entity\nrecognition (NER) and entity linking systems. In light of the recent promising\nresults shown by direct speech translation (ST) models and the known weaknesses\nof cascades (error propagation and additional latency), in this paper we\npropose multitask models that jointly perform ST and NER, and compare them with\na cascade baseline. The experimental results show that our models significantly\noutperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in\nterms of translation quality, and with the same computational efficiency of a\nplain direct ST model.", "published": "2022-10-21 14:24:46", "link": "http://arxiv.org/abs/2210.11987v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing text representations to capture (dis)similarity between\n  political parties", "abstract": "Even though fine-tuned neural language models have been pivotal in enabling\n\"deep\" automatic text analysis, optimizing text representations for specific\napplications remains a crucial bottleneck. In this study, we look at this\nproblem in the context of a task from computational social science, namely\nmodeling pairwise similarities between political parties. Our research question\nis what level of structural information is necessary to create robust text\nrepresentation, contrasting a strongly informed approach (which uses both claim\nspan and claim category annotations) with approaches that forgo one or both\ntypes of annotation with document structure-based heuristics. Evaluating our\nmodels on the manifestos of German parties for the 2021 federal election. We\nfind that heuristics that maximize within-party over between-party similarity\nalong with a normalization step lead to reliable party similarity prediction,\nwithout the need for manual annotation.", "published": "2022-10-21 14:24:57", "link": "http://arxiv.org/abs/2210.11989v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Performance-Efficiency Trade-Offs in Adapting Language Models to Text\n  Classification Tasks", "abstract": "Pre-trained language models (LMs) obtain state-of-the-art performance when\nadapted to text classification tasks. However, when using such models in\nreal-world applications, efficiency considerations are paramount. In this\npaper, we study how different training procedures that adapt LMs to text\nclassification perform, as we vary model and train set size. More specifically,\nwe compare standard fine-tuning, prompting, and knowledge distillation (KD)\nwhen the teacher was trained with either fine-tuning or prompting. Our findings\nsuggest that even though fine-tuning and prompting work well to train large LMs\non large train sets, there are more efficient alternatives that can reduce\ncompute or data cost. Interestingly, we find that prompting combined with KD\ncan reduce compute and data cost at the same time.", "published": "2022-10-21 15:10:09", "link": "http://arxiv.org/abs/2210.12022v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bootstrapping NLP tools across low-resourced African languages: an\n  overview and prospects", "abstract": "Computing and Internet access are substantially growing markets in Southern\nAfrica, which brings with it increasing demands for local content and tools in\nindigenous African languages. Since most of those languages are low-resourced,\nefforts have gone into the notion of bootstrapping tools for one African\nlanguage from another. This paper provides an overview of these efforts for\nNiger-Congo B (`Bantu') languages. Bootstrapping grammars for geographically\ndistant languages has been shown to still have positive outcomes for morphology\nand rules or grammar-based natural language generation. Bootstrapping with\ndata-driven approaches to NLP tasks is difficult to use meaningfully regardless\ngeographic proximity, which is largely due to lexical diversity due to both\northography and vocabulary. Cladistic approaches in comparative linguistics may\ninform bootstrapping strategies and similarity measures might serve as proxy\nfor bootstrapping potential as well, with both fertile ground for further\nresearch.", "published": "2022-10-21 15:16:45", "link": "http://arxiv.org/abs/2210.12027v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Clip-Tuning: Towards Derivative-free Prompt Learning with a Mixture of\n  Rewards", "abstract": "Derivative-free prompt learning has emerged as a lightweight alternative to\nprompt tuning, which only requires model inference to optimize the prompts.\nHowever, existing work did not take full advantage of the over-parameterized\ncharacteristics of large pre-trained language models (PLMs). In this paper, we\npropose Clip-Tuning, a simple yet effective method that adopts diverse frozen\n\"thinned\" networks of PLMs to obtain a mixture of rewards and thus advance the\nderivative-free prompt learning. The thinned networks consist of all the hidden\nunits that survive a stationary dropout strategy, whose inference predictions\nreflect an ensemble of partial views over prompted training samples. Our method\noutperforms previous gradient-free prompt learning methods and achieves parity\nwith gradient-based counterparts on seven language understanding benchmarks\nunder few-shot settings.", "published": "2022-10-21 15:42:41", "link": "http://arxiv.org/abs/2210.12050v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Experiencer-Specific Emotion and Appraisal Prediction", "abstract": "Emotion classification in NLP assigns emotions to texts, such as sentences or\nparagraphs. With texts like \"I felt guilty when he cried\", focusing on the\nsentence level disregards the standpoint of each participant in the situation:\nthe writer (\"I\") and the other entity (\"he\") could in fact have different\naffective states. The emotions of different entities have been considered only\npartially in emotion semantic role labeling, a task that relates semantic roles\nto emotion cue words. Proposing a related task, we narrow the focus on the\nexperiencers of events, and assign an emotion (if any holds) to each of them.\nTo this end, we represent each emotion both categorically and with appraisal\nvariables, as a psychological access to explaining why a person develops a\nparticular emotion. On an event description corpus, our experiencer-aware\nmodels of emotions and appraisals outperform the experiencer-agnostic\nbaselines, showing that disregarding event participants is an\noversimplification for the emotion detection task.", "published": "2022-10-21 16:04:27", "link": "http://arxiv.org/abs/2210.12078v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Augmenting Multi-Turn Text-to-SQL Datasets with Self-Play", "abstract": "The task of context-dependent text-to-SQL aims to convert multi-turn user\nutterances to formal SQL queries. This is a challenging task due to both the\nscarcity of training data from which to learn complex contextual dependencies\nand to generalize to unseen databases. In this paper we explore augmenting the\ntraining datasets using self-play, which leverages contextual information to\nsynthesize new interactions to adapt the model to new databases. We first\ndesign a SQL-to-text model conditioned on a sampled goal query, which\nrepresents a user's intent, that then converses with a text-to-SQL semantic\nparser to generate new interactions. We then filter the synthesized\ninteractions and retrain the models with the augmented data. We find that\nself-play improves the accuracy of a strong baseline on SParC and CoSQL, two\nwidely used cross-domain text-to-SQL datasets. Our analysis shows that\nself-play simulates various conversational thematic relations, enhances\ncross-domain generalization and improves beam-search.", "published": "2022-10-21 16:40:07", "link": "http://arxiv.org/abs/2210.12096v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Syntactic Surprisal From Neural Models Predicts, But Underestimates,\n  Human Processing Difficulty From Syntactic Ambiguities", "abstract": "Humans exhibit garden path effects: When reading sentences that are\ntemporarily structurally ambiguous, they slow down when the structure is\ndisambiguated in favor of the less preferred alternative. Surprisal theory\n(Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes\nthat these slowdowns are due to the unpredictability of each of the words that\noccur in these sentences. Challenging this hypothesis, van Schijndel & Linzen\n(2021) find that estimates of the cost of word predictability derived from\nlanguage models severely underestimate the magnitude of human garden path\neffects. In this work, we consider whether this underestimation is due to the\nfact that humans weight syntactic factors in their predictions more highly than\nlanguage models do. We propose a method for estimating syntactic predictability\nfrom a language model, allowing us to weigh the cost of lexical and syntactic\npredictability independently. We find that treating syntactic predictability\nindependently from lexical predictability indeed results in larger estimates of\ngarden path. At the same time, even when syntactic predictability is\nindependently weighted, surprisal still greatly underestimate the magnitude of\nhuman garden path effects. Our results support the hypothesis that\npredictability is not the only factor responsible for the processing cost\nassociated with garden path sentences.", "published": "2022-10-21 18:30:56", "link": "http://arxiv.org/abs/2210.12187v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Gui at MixMT 2022 : English-Hinglish: An MT approach for translation of\n  code mixed data", "abstract": "Code-mixed machine translation has become an important task in multilingual\ncommunities and extending the task of machine translation to code mixed data\nhas become a common task for these languages. In the shared tasks of WMT 2022,\nwe try to tackle the same for both English + Hindi to Hinglish and Hinglish to\nEnglish. The first task dealt with both Roman and Devanagari script as we had\nmonolingual data in both English and Hindi whereas the second task only had\ndata in Roman script. To our knowledge, we achieved one of the top ROUGE-L and\nWER scores for the first task of Monolingual to Code-Mixed machine translation.\nIn this paper, we discuss the use of mBART with some special pre-processing and\npost-processing (transliteration from Devanagari to Roman) for the first task\nin detail and the experiments that we performed for the second task of\ntranslating code-mixed Hinglish to monolingual English.", "published": "2022-10-21 19:48:18", "link": "http://arxiv.org/abs/2210.12215v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Enhancing Tabular Reasoning with Pattern Exploiting Training", "abstract": "Recent methods based on pre-trained language models have exhibited superior\nperformance over tabular tasks (e.g., tabular NLI), despite showing inherent\nproblems such as not using the right evidence and inconsistent predictions\nacross inputs while reasoning over the tabular data. In this work, we utilize\nPattern-Exploiting Training (PET) (i.e., strategic MLM) on pre-trained language\nmodels to strengthen these tabular reasoning models' pre-existing knowledge and\nreasoning abilities. Our upgraded model exhibits a superior understanding of\nknowledge facts and tabular reasoning compared to current baselines.\nAdditionally, we demonstrate that such models are more effective for underlying\ndownstream tasks of tabular inference on InfoTabs. Furthermore, we show our\nmodel's robustness against adversarial sets generated through various character\nand word level perturbations.", "published": "2022-10-21 21:28:18", "link": "http://arxiv.org/abs/2210.12259v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On the Calibration of Massively Multilingual Language Models", "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained\npopularity due to their surprising effectiveness in cross-lingual transfer.\nWhile there has been much work in evaluating these models for their performance\non a variety of tasks and languages, little attention has been paid on how well\ncalibrated these models are with respect to the confidence in their\npredictions. We first investigate the calibration of MMLMs in the zero-shot\nsetting and observe a clear case of miscalibration in low-resource languages or\nthose which are typologically diverse from English. Next, we empirically show\nthat calibration methods like temperature scaling and label smoothing do\nreasonably well towards improving calibration in the zero-shot scenario. We\nalso find that few-shot examples in the language can further help reduce the\ncalibration errors, often substantially. Overall, our work contributes towards\nbuilding more reliable multilingual models by highlighting the issue of their\nmiscalibration, understanding what language and model specific factors\ninfluence it, and pointing out the strategies to improve the same.", "published": "2022-10-21 21:41:56", "link": "http://arxiv.org/abs/2210.12265v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Graphemic Normalization of the Perso-Arabic Script", "abstract": "Since its original appearance in 1991, the Perso-Arabic script representation\nin Unicode has grown from 169 to over 440 atomic isolated characters spread\nover several code pages representing standard letters, various diacritics and\npunctuation for the original Arabic and numerous other regional orthographic\ntraditions. This paper documents the challenges that Perso-Arabic presents\nbeyond the best-documented languages, such as Arabic and Persian, building on\nearlier work by the expert community. We particularly focus on the situation in\nnatural language processing (NLP), which is affected by multiple, often\nneglected, issues such as the use of visually ambiguous yet canonically\nnonequivalent letters and the mixing of letters from different orthographies.\nAmong the contributing conflating factors are the lack of input methods, the\ninstability of modern orthographies, insufficient literacy, and loss or lack of\northographic tradition. We evaluate the effects of script normalization on\neight languages from diverse language families in the Perso-Arabic script\ndiaspora on machine translation and statistical language modeling tasks. Our\nresults indicate statistically significant improvements in performance in most\nconditions for all the languages considered when normalization is applied. We\nargue that better understanding and representation of Perso-Arabic script\nvariation within regional orthographic traditions, where those are present, is\ncrucial for further progress of modern computational NLP techniques especially\nfor languages with a paucity of resources.", "published": "2022-10-21 21:59:44", "link": "http://arxiv.org/abs/2210.12273v3", "categories": ["cs.CL", "I.2.7; I.7.2; I.7.1"], "primary_category": "cs.CL"}
{"title": "Text Editing as Imitation Game", "abstract": "Text editing, such as grammatical error correction, arises naturally from\nimperfect textual data. Recent works frame text editing as a multi-round\nsequence tagging task, where operations -- such as insertion and substitution\n-- are represented as a sequence of tags. While achieving good results, this\nencoding is limited in flexibility as all actions are bound to token-level\ntags. In this work, we reformulate text editing as an imitation game using\nbehavioral cloning. Specifically, we convert conventional sequence-to-sequence\ndata into state-to-action demonstrations, where the action space can be as\nflexible as needed. Instead of generating the actions one at a time, we\nintroduce a dual decoders structure to parallel the decoding while retaining\nthe dependencies between action tokens, coupled with trajectory augmentation to\nalleviate the distribution shift that imitation learning often suffers. In\nexperiments on a suite of Arithmetic Equation benchmarks, our model\nconsistently outperforms the autoregressive baselines in terms of performance,\nefficiency, and robustness. We hope our findings will shed light on future\nstudies in reinforcement learning applying sequence-level action generation to\nnatural language processing.", "published": "2022-10-21 22:07:04", "link": "http://arxiv.org/abs/2210.12276v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "What do Large Language Models Learn beyond Language?", "abstract": "Large language models (LMs) have rapidly become a mainstay in Natural\nLanguage Processing. These models are known to acquire rich linguistic\nknowledge from training on large amounts of text. In this paper, we investigate\nif pre-training on text also confers these models with helpful `inductive\nbiases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic\ntasks involving quantitative computations, recognizing regular expressions and\nreasoning over strings. We find that pretrained models significantly outperform\ncomparable non-pretrained neural models. This remains true also in experiments\nwith training non-pretrained models with fewer parameters to account for model\nregularization effects. We further explore the effect of text domain on LMs by\npretraining models from text from different domains and provenances. Our\nexperiments surprisingly reveal that the positive effects of pre-training\npersist even when pretraining on multi-lingual text or computer code, and even\nfor text generated from synthetic languages. Our findings suggest a hitherto\nunexplored deep connection between pre-training and inductive learning\nabilities of language models.", "published": "2022-10-21 23:43:13", "link": "http://arxiv.org/abs/2210.12302v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Modelling Multi-relations for Convolutional-based Knowledge Graph\n  Embedding", "abstract": "Representation learning of knowledge graphs aims to embed entities and\nrelations into low-dimensional vectors. Most existing works only consider the\ndirect relations or paths between an entity pair. It is considered that such\napproaches disconnect the semantic connection of multi-relations between an\nentity pair, and we propose a convolutional and multi-relational representation\nlearning model, ConvMR. The proposed ConvMR model addresses the multi-relation\nissue in two aspects: (1) Encoding the multi-relations between an entity pair\ninto a unified vector that maintains the semantic connection. (2) Since not all\nrelations are necessary while joining multi-relations, we propose an\nattention-based relation encoder to automatically assign weights to different\nrelations based on semantic hierarchy. Experimental results on two popular\ndatasets, FB15k-237 and WN18RR, achieved consistent improvements on the mean\nrank. We also found that ConvMR is efficient to deal with less frequent\nentities.", "published": "2022-10-21 03:43:06", "link": "http://arxiv.org/abs/2210.11711v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Design a Sustainable Micro-mobility Future: Trends and Challenges in the\n  United States and European Union Using Natural Language Processing Techniques", "abstract": "Micro-mobility is promising to contribute to sustainable cities in the future\nwith its efficiency and low cost. To better design such a sustainable future,\nit is necessary to understand the trends and challenges. Thus, we examined\npeople's opinions on micro-mobility in the US and the EU using Tweets. We used\ntopic modeling based on advanced natural language processing techniques and\ncategorized the data into seven topics: promotion and service, mobility,\ntechnical features, acceptance, recreation, infrastructure and regulations.\nFurthermore, using sentiment analysis, we investigated people's positive and\nnegative attitudes towards specific aspects of these topics and compared the\npatterns of the trends and challenges in the US and the EU. We found that 1)\npromotion and service included the majority of Twitter discussions in the both\nregions, 2) the EU had more positive opinions than the US, 3) micro-mobility\ndevices were more widely used for utilitarian mobility and recreational\npurposes in the EU than in the US, and 4) compared to the EU, people in the US\nhad many more concerns related to infrastructure and regulation issues. These\nfindings help us understand the trends and challenges and prioritize different\naspects in micro-mobility to improve their safety and experience across the two\nareas for designing a more sustainable micro-mobility future.", "published": "2022-10-21 03:47:59", "link": "http://arxiv.org/abs/2210.11714v2", "categories": ["cs.CL", "cs.HC", "I.5"], "primary_category": "cs.CL"}
{"title": "AfroLID: A Neural Language Identification Tool for African Languages", "abstract": "Language identification (LID) is a crucial precursor for NLP, especially for\nmining web data. Problematically, most of the world's 7000+ languages today are\nnot covered by LID technologies. We address this pressing issue for Africa by\nintroducing AfroLID, a neural LID toolkit for $517$ African languages and\nvarieties. AfroLID exploits a multi-domain web dataset manually curated from\nacross 14 language families utilizing five orthographic systems. When evaluated\non our blind Test set, AfroLID achieves 95.89 F_1-score. We also compare\nAfroLID to five existing LID tools that each cover a small number of African\nlanguages, finding it to outperform them on most languages. We further show the\nutility of AfroLID in the wild by testing it on the acutely under-served\nTwitter domain. Finally, we offer a number of controlled case studies and\nperform a linguistically-motivated error analysis that allow us to both\nshowcase AfroLID's powerful capabilities and limitations.", "published": "2022-10-21 05:45:50", "link": "http://arxiv.org/abs/2210.11744v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Augmentation with Projection: Towards an Effective and Efficient Data\n  Augmentation Paradigm for Distillation", "abstract": "Knowledge distillation is one of the primary methods of transferring\nknowledge from large to small models. However, it requires massive\ntask-specific data, which may not be plausible in many real-world applications.\nData augmentation methods such as representation interpolation, token\nreplacement, or augmentation with models are applied to tackle this problem.\nHowever, these data augmentation methods either potentially cause shifts in\ndecision boundaries (representation interpolation), are not expressive enough\n(token replacement), or introduce too much computational overhead (augmentation\nwith models). To this end, we propose AugPro (Augmentation with Projection), an\neffective and efficient data augmentation method for distillation. Our method\nbuilds on top of representation interpolation augmentation methods to maintain\nthe diversity of expressions and converts the augmented data to tokens to avoid\nshifting decision boundaries. It uses simple operations that come with little\ncomputational overhead. The results on multiple GLUE tasks show that our\nmethods can improve distillation performance by a large margin at a low time\ncost. Codes are available at\nhttps://github.com/google-research/google-research/tree/master/augpro.", "published": "2022-10-21 07:08:31", "link": "http://arxiv.org/abs/2210.11768v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval", "abstract": "Sampling proper negatives from a large document pool is vital to effectively\ntrain a dense retrieval model. However, existing negative sampling strategies\nsuffer from the uninformative or false negative problem. In this work, we\nempirically show that according to the measured relevance scores, the negatives\nranked around the positives are generally more informative and less likely to\nbe false negatives. Intuitively, these negatives are not too hard (\\emph{may be\nfalse negatives}) or too easy (\\emph{uninformative}). They are the ambiguous\nnegatives and need more attention during training. Thus, we propose a simple\nambiguous negatives sampling method, SimANS, which incorporates a new sampling\nprobability distribution to sample more ambiguous negatives. Extensive\nexperiments on four public and one industry datasets show the effectiveness of\nour approach. We made the code and models publicly available in\n\\url{https://github.com/microsoft/SimXNS}.", "published": "2022-10-21 07:18:05", "link": "http://arxiv.org/abs/2210.11773v2", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for\n  Long Sequences", "abstract": "Efficient Transformers have been developed for long sequence modeling, due to\ntheir subquadratic memory and time complexity. Sparse Transformer is a popular\napproach to improving the efficiency of Transformers by restricting\nself-attention to locations specified by the predefined sparse patterns.\nHowever, leveraging sparsity may sacrifice expressiveness compared to\nfull-attention, when important token correlations are multiple hops away. To\ncombine advantages of both the efficiency of sparse transformer and the\nexpressiveness of full-attention Transformer, we propose \\textit{Diffuser}, a\nnew state-of-the-art efficient Transformer. Diffuser incorporates all token\ninteractions within one attention layer while maintaining low computation and\nmemory costs. The key idea is to expand the receptive field of sparse attention\nusing Attention Diffusion, which computes multi-hop token correlations based on\nall paths between corresponding disconnected tokens, besides attention among\nneighboring tokens. Theoretically, we show the expressiveness of Diffuser as a\nuniversal sequence approximator for sequence-to-sequence modeling, and\ninvestigate its ability to approximate full-attention by analyzing the graph\nexpander property from the spectral perspective. Experimentally, we investigate\nthe effectiveness of Diffuser with extensive evaluations, including language\nmodeling, image modeling, and Long Range Arena (LRA). Evaluation results show\nthat Diffuser achieves improvements by an average of 0.94% on text\nclassification tasks and 2.30% on LRA, with 1.67$\\times$ memory savings\ncompared to state-of-the-art benchmarks, which demonstrates superior\nperformance of Diffuser in both expressiveness and efficiency aspects.", "published": "2022-10-21 08:13:34", "link": "http://arxiv.org/abs/2210.11794v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LittleBird: Efficient Faster & Longer Transformer for Question Answering", "abstract": "BERT has shown a lot of sucess in a wide variety of NLP tasks. But it has a\nlimitation dealing with long inputs due to its attention mechanism. Longformer,\nETC and BigBird addressed this issue and effectively solved the quadratic\ndependency problem. However we find that these models are not sufficient, and\npropose LittleBird, a novel model based on BigBird with improved speed and\nmemory footprint while maintaining accuracy. In particular, we devise a more\nflexible and efficient position representation method based on Attention with\nLinear Biases (ALiBi). We also show that replacing the method of global\ninformation represented in the BigBird with pack and unpack attention is more\neffective. The proposed model can work on long inputs even after being\npre-trained on short inputs, and can be trained efficiently reusing existing\npre-trained language model for short inputs. This is a significant benefit for\nlow-resource languages where large amounts of long text data are difficult to\nobtain. As a result, our experiments show that LittleBird works very well in a\nvariety of languages, achieving high performance in question answering tasks,\nparticularly in KorQuAD2.0, Korean Question Answering Dataset for long\nparagraphs.", "published": "2022-10-21 10:46:41", "link": "http://arxiv.org/abs/2210.11870v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "BioLORD: Learning Ontological Representations from Definitions (for\n  Biomedical Concepts and their Textual Descriptions)", "abstract": "This work introduces BioLORD, a new pre-training strategy for producing\nmeaningful representations for clinical sentences and biomedical concepts.\nState-of-the-art methodologies operate by maximizing the similarity in\nrepresentation of names referring to the same concept, and preventing collapse\nthrough contrastive learning. However, because biomedical names are not always\nself-explanatory, it sometimes results in non-semantic representations. BioLORD\novercomes this issue by grounding its concept representations using\ndefinitions, as well as short descriptions derived from a multi-relational\nknowledge graph consisting of biomedical ontologies. Thanks to this grounding,\nour model produces more semantic concept representations that match more\nclosely the hierarchical structure of ontologies. BioLORD establishes a new\nstate of the art for text similarity on both clinical sentences (MedSTS) and\nbiomedical concepts (MayoSRS).", "published": "2022-10-21 11:43:59", "link": "http://arxiv.org/abs/2210.11892v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Turning Fixed to Adaptive: Integrating Post-Evaluation into Simultaneous\n  Machine Translation", "abstract": "Simultaneous machine translation (SiMT) starts its translation before reading\nthe whole source sentence and employs either fixed or adaptive policy to\ngenerate the target sentence. Compared to the fixed policy, the adaptive policy\nachieves better latency-quality tradeoffs by adopting a flexible translation\npolicy. If the policy can evaluate rationality before taking action, the\nprobability of incorrect actions will also decrease. However, previous methods\nlack evaluation of actions before taking them. In this paper, we propose a\nmethod of performing the adaptive policy via integrating post-evaluation into\nthe fixed policy. Specifically, whenever a candidate token is generated, our\nmodel will evaluate the rationality of the next action by measuring the change\nin the source content. Our model will then take different actions based on the\nevaluation results. Experiments on three translation tasks show that our method\ncan exceed strong baselines under all latency.", "published": "2022-10-21 11:57:14", "link": "http://arxiv.org/abs/2210.11900v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "NEREL-BIO: A Dataset of Biomedical Abstracts Annotated with Nested Named\n  Entities", "abstract": "This paper describes NEREL-BIO -- an annotation scheme and corpus of PubMed\nabstracts in Russian and smaller number of abstracts in English. NEREL-BIO\nextends the general domain dataset NEREL by introducing domain-specific entity\ntypes. NEREL-BIO annotation scheme covers both general and biomedical domains\nmaking it suitable for domain transfer experiments. NEREL-BIO provides\nannotation for nested named entities as an extension of the scheme employed for\nNEREL. Nested named entities may cross entity boundaries to connect to shorter\nentities nested within longer entities, making them harder to detect.\n  NEREL-BIO contains annotations for 700+ Russian and 100+ English abstracts.\nAll English PubMed annotations have corresponding Russian counterparts. Thus,\nNEREL-BIO comprises the following specific features: annotation of nested named\nentities, it can be used as a benchmark for cross-domain (NEREL -> NEREL-BIO)\nand cross-language (English -> Russian) transfer. We experiment with both\ntransformer-based sequence models and machine reading comprehension (MRC)\nmodels and report their results.\n  The dataset is freely available at https://github.com/nerel-ds/NEREL-BIO.", "published": "2022-10-21 12:28:43", "link": "http://arxiv.org/abs/2210.11913v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal\n  Modeling", "abstract": "Recent large-scale video-language pre-trained models have shown appealing\nperformance on various downstream tasks. However, the pre-training process is\ncomputationally expensive due to the requirement of millions of video-text\npairs and the redundant data structure of each video. To mitigate these\nproblems, we propose LiteVL, which adapts a pre-trained image-language model\nBLIP into a video-text model directly on downstream tasks, without heavy\npre-training. To enhance the temporal modeling lacking in the image-language\nmodel, we propose to add temporal attention modules in the image encoder of\nBLIP with dynamic temporal scaling. Besides the model-wise adaptation, we also\npropose a non-parametric pooling mechanism to adaptively reweight the\nfine-grained video embedding conditioned on the text. Experimental results on\ntext-video retrieval and video question answering show that the proposed LiteVL\neven outperforms previous video-language pre-trained models by a clear margin,\nthough without any video-language pre-training.", "published": "2022-10-21 13:03:49", "link": "http://arxiv.org/abs/2210.11929v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "A Causal Framework to Quantify the Robustness of Mathematical Reasoning\n  with Language Models", "abstract": "We have recently witnessed a number of impressive results on hard\nmathematical reasoning problems with language models. At the same time, the\nrobustness of these models has also been called into question; recent works\nhave shown that models can rely on shallow patterns in the problem description\nwhen generating a solution. Building on the idea of behavioral testing, we\npropose a novel framework, which pins down the causal effect of various factors\nin the input, e.g., the surface form of the problem text, the operands, and\nmath operators on the output solution. By grounding the behavioral analysis in\na causal graph describing an intuitive reasoning process, we study the behavior\nof language models in terms of robustness and sensitivity to direct\ninterventions in the input space. We apply our framework on a test bed of math\nword problems. Our analysis shows that robustness does not appear to\ncontinuously improve as a function of size, but the GPT-3 Davinci models (175B)\nachieve a dramatic improvement in both robustness and sensitivity compared to\nall other GPT variants.", "published": "2022-10-21 15:12:37", "link": "http://arxiv.org/abs/2210.12023v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Do Vision-and-Language Transformers Learn Grounded Predicate-Noun\n  Dependencies?", "abstract": "Recent advances in vision-and-language modeling have seen the development of\nTransformer architectures that achieve remarkable performance on multimodal\nreasoning tasks. Yet, the exact capabilities of these black-box models are\nstill poorly understood. While much of previous work has focused on studying\ntheir ability to learn meaning at the word-level, their ability to track\nsyntactic dependencies between words has received less attention. We take a\nfirst step in closing this gap by creating a new multimodal task targeted at\nevaluating understanding of predicate-noun dependencies in a controlled setup.\nWe evaluate a range of state-of-the-art models and find that their performance\non the task varies considerably, with some models performing relatively well\nand others at chance level. In an effort to explain this variability, our\nanalyses indicate that the quality (and not only sheer quantity) of pretraining\ndata is essential. Additionally, the best performing models leverage\nfine-grained multimodal pretraining objectives in addition to the standard\nimage-text matching objectives. This study highlights that targeted and\ncontrolled evaluations are a crucial step for a precise and rigorous test of\nthe multimodal knowledge of vision-and-language models.", "published": "2022-10-21 16:07:00", "link": "http://arxiv.org/abs/2210.12079v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "WikiWhy: Answering and Explaining Cause-and-Effect Questions", "abstract": "As large language models (LLMs) grow larger and more sophisticated, assessing\ntheir \"reasoning\" capabilities in natural language grows more challenging.\nRecent question answering (QA) benchmarks that attempt to assess reasoning are\noften limited by a narrow scope of covered situations and subject matters. We\nintroduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining\nwhy an answer is true in natural language. WikiWhy contains over 9,000 \"why\"\nquestion-answer-rationale triples, grounded on Wikipedia facts across a diverse\nset of topics. Each rationale is a set of supporting statements connecting the\nquestion to the answer. WikiWhy serves as a benchmark for the reasoning\ncapabilities of LLMs because it demands rigorous explicit rationales for each\nanswer to demonstrate the acquisition of implicit commonsense knowledge, which\nis unlikely to be easily memorized. GPT-3 baselines achieve only 38.7%\nhuman-evaluated correctness in the end-to-end answer & explain condition,\nleaving significant room for future improvements.", "published": "2022-10-21 17:59:03", "link": "http://arxiv.org/abs/2210.12152v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Coreference Resolution for Zeros and non-Zeros in Arabic", "abstract": "Most existing proposals about anaphoric zero pronoun (AZP) resolution regard\nfull mention coreference and AZP resolution as two independent tasks, even\nthough the two tasks are clearly related. The main issues that need tackling to\ndevelop a joint model for zero and non-zero mentions are the difference between\nthe two types of arguments (zero pronouns, being null, provide no nominal\ninformation) and the lack of annotated datasets of a suitable size in which\nboth types of arguments are annotated for languages other than Chinese and\nJapanese. In this paper, we introduce two architectures for jointly resolving\nAZPs and non-AZPs, and evaluate them on Arabic, a language for which, as far as\nwe know, there has been no prior work on joint resolution. Doing this also\nrequired creating a new version of the Arabic subset of the standard\ncoreference resolution dataset used for the CoNLL-2012 shared task (Pradhan et\nal.,2012) in which both zeros and non-zeros are included in a single dataset.", "published": "2022-10-21 18:01:01", "link": "http://arxiv.org/abs/2210.12169v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Discovering Differences in the Representation of People using\n  Contextualized Semantic Axes", "abstract": "A common paradigm for identifying semantic differences across social and\ntemporal contexts is the use of static word embeddings and their distances. In\nparticular, past work has compared embeddings against \"semantic axes\" that\nrepresent two opposing concepts. We extend this paradigm to BERT embeddings,\nand construct contextualized axes that mitigate the pitfall where antonyms have\nneighboring representations. We validate and demonstrate these axes on two\npeople-centric datasets: occupations from Wikipedia, and multi-platform\ndiscussions in extremist, men's communities over fourteen years. In both\nstudies, contextualized semantic axes can characterize differences among\ninstances of the same word type. In the latter study, we show that references\nto women and the contexts around them have become more detestable over time.", "published": "2022-10-21 18:02:19", "link": "http://arxiv.org/abs/2210.12170v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Improving the Factual Correctness of Radiology Report Generation with\n  Semantic Rewards", "abstract": "Neural image-to-text radiology report generation systems offer the potential\nto improve radiology reporting by reducing the repetitive process of report\ndrafting and identifying possible medical errors. These systems have achieved\npromising performance as measured by widely used NLG metrics such as BLEU and\nCIDEr. However, the current systems face important limitations. First, they\npresent an increased complexity in architecture that offers only marginal\nimprovements on NLG metrics. Secondly, these systems that achieve high\nperformance on these metrics are not always factually complete or consistent\ndue to both inadequate training and evaluation. Recent studies have shown the\nsystems can be substantially improved by using new methods encouraging 1) the\ngeneration of domain entities consistent with the reference and 2) describing\nthese entities in inferentially consistent ways. So far, these methods rely on\nweakly-supervised approaches (rule-based) and named entity recognition systems\nthat are not specific to the chest X-ray domain. To overcome this limitation,\nwe propose a new method, the RadGraph reward, to further improve the factual\ncompleteness and correctness of generated radiology reports. More precisely, we\nleverage the RadGraph dataset containing annotated chest X-ray reports with\nentities and relations between entities. On two open radiology report datasets,\nour system substantially improves the scores up to 14.2% and 25.3% on metrics\nevaluating the factual correctness and completeness of reports.", "published": "2022-10-21 18:27:45", "link": "http://arxiv.org/abs/2210.12186v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Life is a Circus and We are the Clowns: Automatically Finding Analogies\n  between Situations and Processes", "abstract": "Analogy-making gives rise to reasoning, abstraction, flexible categorization\nand counterfactual inference -- abilities lacking in even the best AI systems\ntoday. Much research has suggested that analogies are key to non-brittle\nsystems that can adapt to new domains. Despite their importance, analogies\nreceived little attention in the NLP community, with most research focusing on\nsimple word analogies. Work that tackled more complex analogies relied heavily\non manually constructed, hard-to-scale input representations. In this work, we\nexplore a more realistic, challenging setup: our input is a pair of natural\nlanguage procedural texts, describing a situation or a process (e.g., how the\nheart works/how a pump works). Our goal is to automatically extract entities\nand their relations from the text and find a mapping between the different\ndomains based on relational similarity (e.g., blood is mapped to water). We\ndevelop an interpretable, scalable algorithm and demonstrate that it identifies\nthe correct mappings 87% of the time for procedural texts and 94% for stories\nfrom cognitive-psychology literature. We show it can extract analogies from a\nlarge dataset of procedural texts, achieving 79% precision (analogy prevalence\nin data: 3%). Lastly, we demonstrate that our algorithm is robust to\nparaphrasing the input texts.", "published": "2022-10-21 18:54:17", "link": "http://arxiv.org/abs/2210.12197v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity\n  Representation", "abstract": "Named geographic entities (geo-entities for short) are the building blocks of\nmany geographic datasets. Characterizing geo-entities is integral to various\napplication domains, such as geo-intelligence and map comprehension, while a\nkey challenge is to capture the spatial-varying context of an entity. We\nhypothesize that we shall know the characteristics of a geo-entity by its\nsurrounding entities, similar to knowing word meanings by their linguistic\ncontext. Accordingly, we propose a novel spatial language model, SpaBERT, which\nprovides a general-purpose geo-entity representation based on neighboring\nentities in geospatial data. SpaBERT extends BERT to capture linearized spatial\ncontext, while incorporating a spatial coordinate embedding mechanism to\npreserve spatial relations of entities in the 2-dimensional space. SpaBERT is\npretrained with masked language modeling and masked entity prediction tasks to\nlearn spatial dependencies. We apply SpaBERT to two downstream tasks:\ngeo-entity typing and geo-entity linking. Compared with the existing language\nmodels that do not use spatial context, SpaBERT shows significant performance\nimprovement on both tasks. We also analyze the entity representation from\nSpaBERT in various settings and the effect of spatial coordinate embedding.", "published": "2022-10-21 19:42:32", "link": "http://arxiv.org/abs/2210.12213v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Entailer: Answering Questions with Faithful and Truthful Chains of\n  Reasoning", "abstract": "Our goal is a question-answering (QA) system that can show how its answers\nare implied by its own internal beliefs via a systematic chain of reasoning.\nSuch a capability would allow better understanding of why a model produced the\nanswer it did. Our approach is to recursively combine a trained\nbackward-chaining model, capable of generating a set of premises entailing an\nanswer hypothesis, with a verifier that checks that the model itself believes\nthose premises (and the entailment itself) through self-querying. To our\nknowledge, this is the first system to generate multistep chains that are both\nfaithful (the answer follows from the reasoning) and truthful (the chain\nreflects the system's own internal beliefs). In evaluation using two different\ndatasets, users judge that a majority (70%+) of generated chains clearly show\nhow an answer follows from a set of facts - substantially better than a\nhigh-performance baseline - while preserving answer accuracy. By materializing\nmodel beliefs that systematically support an answer, new opportunities arise\nfor understanding the model's system of belief, and diagnosing and correcting\nits misunderstandings when an answer is wrong.", "published": "2022-10-21 19:51:56", "link": "http://arxiv.org/abs/2210.12217v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Dataset for Plain Language Adaptation of Biomedical Abstracts", "abstract": "Though exponentially growing health-related literature has been made\navailable to a broad audience online, the language of scientific articles can\nbe difficult for the general public to understand. Therefore, adapting this\nexpert-level language into plain language versions is necessary for the public\nto reliably comprehend the vast health-related literature. Deep Learning\nalgorithms for automatic adaptation are a possible solution; however, gold\nstandard datasets are needed for proper evaluation. Proposed datasets thus far\nconsist of either pairs of comparable professional- and general public-facing\ndocuments or pairs of semantically similar sentences mined from such documents.\nThis leads to a trade-off between imperfect alignments and small test sets. To\naddress this issue, we created the Plain Language Adaptation of Biomedical\nAbstracts dataset. This dataset is the first manually adapted dataset that is\nboth document- and sentence-aligned. The dataset contains 750 adapted\nabstracts, totaling 7643 sentence pairs. Along with describing the dataset, we\nbenchmark automatic adaptation on the dataset with state-of-the-art Deep\nLearning approaches, setting baselines for future research.", "published": "2022-10-21 20:47:34", "link": "http://arxiv.org/abs/2210.12242v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination", "abstract": "Large-scale pretrained language models have made significant advances in\nsolving downstream language understanding tasks. However, they generally suffer\nfrom reporting bias, the phenomenon describing the lack of explicit commonsense\nknowledge in written text, e.g., ''an orange is orange''. To overcome this\nlimitation, we develop a novel approach, Z-LaVI, to endow language models with\nvisual imagination capabilities. Specifically, we leverage two complementary\ntypes of ''imaginations'': (i) recalling existing images through retrieval and\n(ii) synthesizing nonexistent images via text-to-image generation. Jointly\nexploiting the language inputs and the imagination, a pretrained\nvision-language model (e.g., CLIP) eventually composes a zero-shot solution to\nthe original language tasks. Notably, fueling language models with imagination\ncan effectively leverage visual knowledge to solve plain language tasks. In\nconsequence, Z-LaVI consistently improves the zero-shot performance of existing\nlanguage models across a diverse set of language tasks.", "published": "2022-10-21 21:33:10", "link": "http://arxiv.org/abs/2210.12261v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "Combining Contrastive and Non-Contrastive Losses for Fine-Tuning\n  Pretrained Models in Speech Analysis", "abstract": "Embedding paralinguistic properties is a challenging task as there are only a\nfew hours of training data available for domains such as emotional speech. One\nsolution to this problem is to pretrain a general self-supervised speech\nrepresentation model on large amounts of unlabeled speech. This pretrained\nmodel is then finetuned to a specific task. Paralinguistic properties however\nhave notoriously high class variance, making the finetuning ineffective. In\nthis work, we propose a two step approach to this. First we improve the\nembedding space, then we train an adapter to bridge the gap from the embedding\nspace to a classification task. In order to improve the class invariance we use\na combination of contrastive and non-contrastive losses to explicitly optimize\nfor class invariant, yet discriminative features. Our approach consistently\noutperforms baselines that are finetuned end-to-end on multiple tasks and\nsurpasses a benchmark on state-of-the-art emotion classification.", "published": "2022-10-21 19:58:37", "link": "http://arxiv.org/abs/2211.01964v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Multi-View Reasoning: Consistent Contrastive Learning for Math Word\n  Problem", "abstract": "Math word problem solver requires both precise relation reasoning about\nquantities in the text and reliable generation for the diverse equation.\nCurrent sequence-to-tree or relation extraction methods regard this only from a\nfixed view, struggling to simultaneously handle complex semantics and diverse\nequations. However, human solving naturally involves two consistent reasoning\nviews: top-down and bottom-up, just as math equations also can be expressed in\nmultiple equivalent forms: pre-order and post-order. We propose a multi-view\nconsistent contrastive learning for a more complete semantics-to-equation\nmapping. The entire process is decoupled into two independent but consistent\nviews: top-down decomposition and bottom-up construction, and the two reasoning\nviews are aligned in multi-granularity for consistency, enhancing global\ngeneration and precise reasoning. Experiments on multiple datasets across two\nlanguages show our approach significantly outperforms the existing baselines,\nespecially on complex problems. We also show after consistent alignment,\nmulti-view can absorb the merits of both views and generate more diverse\nresults consistent with the mathematical laws.", "published": "2022-10-21 02:44:55", "link": "http://arxiv.org/abs/2210.11694v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Efficiently Tuned Parameters are Task Embeddings", "abstract": "Intermediate-task transfer can benefit a wide range of NLP tasks with\nproperly selected source datasets. However, it is computationally infeasible to\nexperiment with all intermediate transfer combinations, making choosing a\nuseful source task a challenging problem. In this paper, we anticipate that\ntask-specific parameters updated in parameter-efficient tuning methods are\nlikely to encode task-specific information. Therefore, such parameters can be\npredictive for inter-task transferability. Thus, we propose to exploit these\nefficiently tuned parameters as off-the-shelf task embeddings for the efficient\nselection of source datasets for intermediate-task transfer. We experiment with\n11 text classification tasks and 11 question answering tasks. Experimental\nresults show that our approach can consistently outperform existing inter-task\ntransferability prediction methods while being conceptually simple and\ncomputationally efficient. Our analysis also reveals that the ability of\nefficiently tuned parameters on transferability prediction is disentangled with\ntheir in-task performance. This allows us to use parameters from early\ncheckpoints as task embeddings to further improve efficiency.", "published": "2022-10-21 03:19:54", "link": "http://arxiv.org/abs/2210.11705v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Revisiting Checkpoint Averaging for Neural Machine Translation", "abstract": "Checkpoint averaging is a simple and effective method to boost the\nperformance of converged neural machine translation models. The calculation is\ncheap to perform and the fact that the translation improvement almost comes for\nfree, makes it widely adopted in neural machine translation research. Despite\nthe popularity, the method itself simply takes the mean of the model parameters\nfrom several checkpoints, the selection of which is mostly based on empirical\nrecipes without many justifications. In this work, we revisit the concept of\ncheckpoint averaging and consider several extensions. Specifically, we\nexperiment with ideas such as using different checkpoint selection strategies,\ncalculating weighted average instead of simple mean, making use of gradient\ninformation and fine-tuning the interpolation weights on development data. Our\nresults confirm the necessity of applying checkpoint averaging for optimal\nperformance, but also suggest that the landscape between the converged\ncheckpoints is rather flat and not much further improvement compared to simple\naveraging is to be obtained.", "published": "2022-10-21 08:29:23", "link": "http://arxiv.org/abs/2210.11803v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Is Encoder-Decoder Redundant for Neural Machine Translation?", "abstract": "Encoder-decoder architecture is widely adopted for sequence-to-sequence\nmodeling tasks. For machine translation, despite the evolution from long\nshort-term memory networks to Transformer networks, plus the introduction and\ndevelopment of attention mechanism, encoder-decoder is still the de facto\nneural network architecture for state-of-the-art models. While the motivation\nfor decoding information from some hidden space is straightforward, the strict\nseparation of the encoding and decoding steps into an encoder and a decoder in\nthe model architecture is not necessarily a must. Compared to the task of\nautoregressive language modeling in the target language, machine translation\nsimply has an additional source sentence as context. Given the fact that neural\nlanguage models nowadays can already handle rather long contexts in the target\nlanguage, it is natural to ask whether simply concatenating the source and\ntarget sentences and training a language model to do translation would work. In\nthis work, we investigate the aforementioned concept for machine translation.\nSpecifically, we experiment with bilingual translation, translation with\nadditional target monolingual data, and multilingual translation. In all cases,\nthis alternative approach performs on par with the baseline encoder-decoder\nTransformer, suggesting that an encoder-decoder architecture might be redundant\nfor neural machine translation.", "published": "2022-10-21 08:33:55", "link": "http://arxiv.org/abs/2210.11807v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Textless Metric for Speech-to-Speech Comparison", "abstract": "In this paper, we introduce a new and simple method for comparing speech\nutterances without relying on text transcripts. Our speech-to-speech comparison\nmetric utilizes state-of-the-art speech2unit encoders like HuBERT to convert\nspeech utterances into discrete acoustic units. We then propose a simple and\neasily replicable neural architecture that learns a speech-based metric that\nclosely corresponds to its text-based counterpart. This textless metric has\nnumerous potential applications, including evaluating speech-to-speech\ntranslation for oral languages, languages without dependable ASR systems, or to\navoid the need for ASR transcription altogether. This paper also shows that for\nspeech-to-speech translation evaluation, ASR-BLEU (which consists in\nautomatically transcribing both speech hypothesis and reference and compute\nsentence-level BLEU between transcripts) is a poor proxy to real text-BLEU even\nwhen ASR system is strong.", "published": "2022-10-21 09:28:54", "link": "http://arxiv.org/abs/2210.11835v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Deep LSTM Spoken Term Detection using Wav2Vec 2.0 Recognizer", "abstract": "In recent years, the standard hybrid DNN-HMM speech recognizers are\noutperformed by the end-to-end speech recognition systems. One of the very\npromising approaches is the grapheme Wav2Vec 2.0 model, which uses the\nself-supervised pretraining approach combined with transfer learning of the\nfine-tuned speech recognizer. Since it lacks the pronunciation vocabulary and\nlanguage model, the approach is suitable for tasks where obtaining such models\nis not easy or almost impossible.\n  In this paper, we use the Wav2Vec speech recognizer in the task of spoken\nterm detection over a large set of spoken documents. The method employs a deep\nLSTM network which maps the recognized hypothesis and the searched term into a\nshared pronunciation embedding space in which the term occurrences and the\nassigned scores are easily computed.\n  The paper describes a bootstrapping approach that allows the transfer of the\nknowledge contained in traditional pronunciation vocabulary of DNN-HMM hybrid\nASR into the context of grapheme-based Wav2Vec. The proposed method outperforms\nthe previously published system based on the combination of the DNN-HMM hybrid\nASR and phoneme recognizer by a large margin on the MALACH data in both English\nand Czech languages.", "published": "2022-10-21 11:26:59", "link": "http://arxiv.org/abs/2210.11885v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Spoken Term Detection and Relevance Score Estimation using Dot-Product\n  of Pronunciation Embeddings", "abstract": "The paper describes a novel approach to Spoken Term Detection (STD) in large\nspoken archives using deep LSTM networks. The work is based on the previous\napproach of using Siamese neural networks for STD and naturally extends it to\ndirectly localize a spoken term and estimate its relevance score. The phoneme\nconfusion network generated by a phoneme recognizer is processed by the deep\nLSTM network which projects each segment of the confusion network into an\nembedding space. The searched term is projected into the same embedding space\nusing another deep LSTM network. The relevance score is then computed using a\nsimple dot-product in the embedding space and calibrated using a sigmoid\nfunction to predict the probability of occurrence. The location of the searched\nterm is then estimated from the sequence of output probabilities. The deep LSTM\nnetworks are trained in a self-supervised manner from paired recognition\nhypotheses on word and phoneme levels. The method is experimentally evaluated\non MALACH data in English and Czech languages.", "published": "2022-10-21 11:44:53", "link": "http://arxiv.org/abs/2210.11895v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Generalizing over Long Tail Concepts for Medical Term Normalization", "abstract": "Medical term normalization consists in mapping a piece of text to a large\nnumber of output classes. Given the small size of the annotated datasets and\nthe extremely long tail distribution of the concepts, it is of utmost\nimportance to develop models that are capable to generalize to scarce or unseen\nconcepts. An important attribute of most target ontologies is their\nhierarchical structure. In this paper we introduce a simple and effective\nlearning strategy that leverages such information to enhance the\ngeneralizability of both discriminative and generative models. The evaluation\nshows that the proposed strategy produces state-of-the-art performance on seen\nconcepts and consistent improvements on unseen ones, allowing also for\nefficient zero-shot knowledge transfer across text typologies and datasets.", "published": "2022-10-21 13:17:36", "link": "http://arxiv.org/abs/2210.11947v2", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Neuro-Symbolic Causal Reasoning Meets Signaling Game for Emergent\n  Semantic Communications", "abstract": "Semantic communication (SC) aims to communicate reliably with minimal data\ntransfer while simultaneously providing seamless connectivity to heterogeneous\nservices and users. In this paper, a novel emergent SC (ESC) system framework\nis proposed and is composed of a signaling game for emergent language design\nand a neuro-symbolic (NeSy) artificial intelligence (AI) approach for causal\nreasoning. In order to design the language, the signaling game is solved using\nan alternating maximization between the communicating node's utilities. The\nemergent language helps create a context-aware transmit vocabulary (minimal\nsemantic representation) and aids the reasoning process (enabling\ngeneralization to unseen scenarios) by splitting complex messages into simpler\nreasoning tasks for the receiver. The causal description at the transmitter is\nthen modeled (a neural component) as a posterior distribution of the relevant\nattributes present in the data. Using the reconstructed causal state, the\nreceiver evaluates a set of logical formulas (symbolic part) to execute its\ntask. The nodes NeSy reasoning components are implemented by the recently\nproposed AI tool called Generative Flow Networks, and they are optimized for\nhigher semantic reliability. The ESC system is designed to enhance the novel\nmetrics of semantic information, reliability, distortion and similarity that\nare designed using rigorous algebraic properties from category theory thereby\ngeneralizing the metrics beyond Shannon's notion of uncertainty. Simulation\nresults validate the ability of ESC to communicate efficiently (with reduced\nbits) and achieve better semantic reliability than conventional wireless and\nstate-of-the-art systems that do not exploit causal reasoning capabilities.", "published": "2022-10-21 15:33:37", "link": "http://arxiv.org/abs/2210.12040v3", "categories": ["cs.LG", "cs.CL", "cs.IT", "math.IT"], "primary_category": "cs.LG"}
{"title": "Decoding a Neural Retriever's Latent Space for Query Suggestion", "abstract": "Neural retrieval models have superseded classic bag-of-words methods such as\nBM25 as the retrieval framework of choice. However, neural systems lack the\ninterpretability of bag-of-words models; it is not trivial to connect a query\nchange to a change in the latent space that ultimately determines the retrieval\nresults. To shed light on this embedding space, we learn a \"query decoder\"\nthat, given a latent representation of a neural search engine, generates the\ncorresponding query. We show that it is possible to decode a meaningful query\nfrom its latent representation and, when moving in the right direction in\nlatent space, to decode a query that retrieves the relevant paragraph. In\nparticular, the query decoder can be useful to understand \"what should have\nbeen asked\" to retrieve a particular paragraph from the collection. We employ\nthe query decoder to generate a large synthetic dataset of query reformulations\nfor MSMarco, leading to improved retrieval performance. On this data, we train\na pseudo-relevance feedback (PRF) T5 model for the application of query\nsuggestion that outperforms both query reformulation and PRF information\nretrieval baselines.", "published": "2022-10-21 16:19:31", "link": "http://arxiv.org/abs/2210.12084v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Describing Sets of Images with Textual-PCA", "abstract": "We seek to semantically describe a set of images, capturing both the\nattributes of single images and the variations within the set. Our procedure is\nanalogous to Principle Component Analysis, in which the role of projection\nvectors is replaced with generated phrases. First, a centroid phrase that has\nthe largest average semantic similarity to the images in the set is generated,\nwhere both the computation of the similarity and the generation are based on\npretrained vision-language models. Then, the phrase that generates the highest\nvariation among the similarity scores is generated, using the same models. The\nnext phrase maximizes the variance subject to being orthogonal, in the latent\nspace, to the highest-variance phrase, and the process continues. Our\nexperiments show that our method is able to convincingly capture the essence of\nimage sets and describe the individual elements in a semantically meaningful\nway within the context of the entire set. Our code is available at:\nhttps://github.com/OdedH/textual-pca.", "published": "2022-10-21 17:10:49", "link": "http://arxiv.org/abs/2210.12112v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Audio-to-Intent Using Acoustic-Textual Subword Representations from\n  End-to-End ASR", "abstract": "Accurate prediction of the user intent to interact with a voice assistant\n(VA) on a device (e.g. on the phone) is critical for achieving naturalistic,\nengaging, and privacy-centric interactions with the VA. To this end, we present\na novel approach to predict the user's intent (the user speaking to the device\nor not) directly from acoustic and textual information encoded at subword\ntokens which are obtained via an end-to-end ASR model. Modeling directly the\nsubword tokens, compared to modeling of the phonemes and/or full words, has at\nleast two advantages: (i) it provides a unique vocabulary representation, where\neach token has a semantic meaning, in contrast to the phoneme-level\nrepresentations, (ii) each subword token has a reusable \"sub\"-word acoustic\npattern (that can be used to construct multiple full words), resulting in a\nlargely reduced vocabulary space than of the full words. To learn the subword\nrepresentations for the audio-to-intent classification, we extract: (i)\nacoustic information from an E2E-ASR model, which provides frame-level CTC\nposterior probabilities for the subword tokens, and (ii) textual information\nfrom a pre-trained continuous bag-of-words model capturing the semantic meaning\nof the subword tokens. The key to our approach is the way it combines acoustic\nsubword-level posteriors with text information using the notion of\npositional-encoding in order to account for multiple ASR hypotheses\nsimultaneously. We show that our approach provides more robust and richer\nrepresentations for audio-to-intent classification, and is highly accurate with\ncorrectly mitigating 93.3% of unintended user audio from invoking the smart\nassistant at 99% true positive rate.", "published": "2022-10-21 17:45:00", "link": "http://arxiv.org/abs/2210.12134v1", "categories": ["cs.CL", "cs.HC", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Probing with Noise: Unpicking the Warp and Weft of Embeddings", "abstract": "Improving our understanding of how information is encoded in vector space can\nyield valuable interpretability insights. Alongside vector dimensions, we argue\nthat it is possible for the vector norm to also carry linguistic information.\nWe develop a method to test this: an extension of the probing framework which\nallows for relative intrinsic interpretations of probing results. It relies on\nintroducing noise that ablates information encoded in embeddings, grounded in\nrandom baselines and confidence intervals. We apply the method to\nwell-established probing tasks and find evidence that confirms the existence of\nseparate information containers in English GloVe and BERT embeddings. Our\ncorrelation analysis aligns with the experimental findings that different\nencoders use the norm to encode different kinds of information: GloVe stores\nsyntactic and sentence length information in the vector norm, while BERT uses\nit to encode contextual incongruity.", "published": "2022-10-21 19:33:33", "link": "http://arxiv.org/abs/2210.12206v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "68Uxx"], "primary_category": "cs.CL"}
{"title": "Optimizing Bilingual Neural Transducer with Synthetic Code-switching\n  Text Generation", "abstract": "Code-switching describes the practice of using more than one language in the\nsame sentence. In this study, we investigate how to optimize a neural\ntransducer based bilingual automatic speech recognition (ASR) model for\ncode-switching speech. Focusing on the scenario where the ASR model is trained\nwithout supervised code-switching data, we found that semi-supervised training\nand synthetic code-switched data can improve the bilingual ASR system on\ncode-switching speech. We analyze how each of the neural transducer's encoders\ncontributes towards code-switching performance by measuring encoder-specific\nrecall values, and evaluate our English/Mandarin system on the ASCEND data set.\nOur final system achieves 25% mixed error rate (MER) on the ASCEND\nEnglish/Mandarin code-switching test set -- reducing the MER by 2.1% absolute\ncompared to the previous literature -- while maintaining good accuracy on the\nmonolingual test sets.", "published": "2022-10-21 19:42:41", "link": "http://arxiv.org/abs/2210.12214v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Low-Resource Multilingual and Zero-Shot Multispeaker TTS", "abstract": "While neural methods for text-to-speech (TTS) have shown great advances in\nmodeling multiple speakers, even in zero-shot settings, the amount of data\nneeded for those approaches is generally not feasible for the vast majority of\nthe world's over 6,000 spoken languages. In this work, we bring together the\ntasks of zero-shot voice cloning and multilingual low-resource TTS. Using the\nlanguage agnostic meta learning (LAML) procedure and modifications to a TTS\nencoder, we show that it is possible for a system to learn speaking a new\nlanguage using just 5 minutes of training data while retaining the ability to\ninfer the voice of even unseen speakers in the newly learned language. We show\nthe success of our proposed approach in terms of intelligibility, naturalness\nand similarity to target speaker using objective metrics as well as human\nstudies and provide our code and trained models open source.", "published": "2022-10-21 20:03:37", "link": "http://arxiv.org/abs/2210.12223v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "EDUKG: a Heterogeneous Sustainable K-12 Educational Knowledge Graph", "abstract": "Web and artificial intelligence technologies, especially semantic web and\nknowledge graph (KG), have recently raised significant attention in educational\nscenarios. Nevertheless, subject-specific KGs for K-12 education still lack\nsufficiency and sustainability from knowledge and data perspectives. To tackle\nthese issues, we propose EDUKG, a heterogeneous sustainable K-12 Educational\nKnowledge Graph. We first design an interdisciplinary and fine-grained ontology\nfor uniformly modeling knowledge and resource in K-12 education, where we\ndefine 635 classes, 445 object properties, and 1314 datatype properties in\ntotal. Guided by this ontology, we propose a flexible methodology for\ninteractively extracting factual knowledge from textbooks. Furthermore, we\nestablish a general mechanism based on our proposed generalized entity linking\nsystem for EDUKG's sustainable maintenance, which can dynamically index\nnumerous heterogeneous resources and data with knowledge topics in EDUKG. We\nfurther evaluate EDUKG to illustrate its sufficiency, richness, and\nvariability. We publish EDUKG with more than 252 million entities and 3.86\nbillion triplets. Our code and data repository is now available at\nhttps://github.com/THU-KEG/EDUKG.", "published": "2022-10-21 20:14:41", "link": "http://arxiv.org/abs/2210.12228v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multimodal Model with Text and Drug Embeddings for Adverse Drug Reaction\n  Classification", "abstract": "In this paper, we focus on the classification of tweets as sources of\npotential signals for adverse drug effects (ADEs) or drug reactions (ADRs).\nFollowing the intuition that text and drug structure representations are\ncomplementary, we introduce a multimodal model with two components. These\ncomponents are state-of-the-art BERT-based models for language understanding\nand molecular property prediction. Experiments were carried out on multilingual\nbenchmarks of the Social Media Mining for Health Research and Applications\n(#SMM4H) initiative. Our models obtained state-of-the-art results of 0.61 F1\nand 0.57 F1 on #SMM4H 2021 Shared Tasks 1a and 2 in English and Russian,\nrespectively. On the classification of French tweets from SMM4H 2020 Task 1,\nour approach pushes the state of the art by an absolute gain of 8% F1. Our\nexperiments show that the molecular information obtained from neural networks\nis more beneficial for ADE classification than traditional molecular\ndescriptors. The source code for our models is freely available at\nhttps://github.com/Andoree/smm4h_2021_classification.", "published": "2022-10-21 11:41:45", "link": "http://arxiv.org/abs/2210.13238v1", "categories": ["q-bio.QM", "cs.CL", "cs.LG"], "primary_category": "q-bio.QM"}
{"title": "Can Visual Context Improve Automatic Speech Recognition for an Embodied\n  Agent?", "abstract": "The usage of automatic speech recognition (ASR) systems are becoming\nomnipresent ranging from personal assistant to chatbots, home, and industrial\nautomation systems, etc. Modern robots are also equipped with ASR capabilities\nfor interacting with humans as speech is the most natural interaction modality.\nHowever, ASR in robots faces additional challenges as compared to a personal\nassistant. Being an embodied agent, a robot must recognize the physical\nentities around it and therefore reliably recognize the speech containing the\ndescription of such entities. However, current ASR systems are often unable to\ndo so due to limitations in ASR training, such as generic datasets and\nopen-vocabulary modeling. Also, adverse conditions during inference, such as\nnoise, accented, and far-field speech makes the transcription inaccurate. In\nthis work, we present a method to incorporate a robot's visual information into\nan ASR system and improve the recognition of a spoken utterance containing a\nvisible entity. Specifically, we propose a new decoder biasing technique to\nincorporate the visual context while ensuring the ASR output does not degrade\nfor incorrect context. We achieve a 59% relative reduction in WER from an\nunmodified ASR system.", "published": "2022-10-21 11:16:05", "link": "http://arxiv.org/abs/2210.13189v1", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "eess.AS"}
{"title": "Improved Normalizing Flow-Based Speech Enhancement using an All-pole\n  Gammatone Filterbank for Conditional Input Representation", "abstract": "Deep generative models for Speech Enhancement (SE) received increasing\nattention in recent years. The most prominent example are Generative\nAdversarial Networks (GANs), while normalizing flows (NF) received less\nattention despite their potential. Building on previous work, architectural\nmodifications are proposed, along with an investigation of different\nconditional input representations. Despite being a common choice in related\nworks, Mel-spectrograms demonstrate to be inadequate for the given scenario.\nAlternatively, a novel All-Pole Gammatone filterbank (APG) with high temporal\nresolution is proposed. Although computational evaluation metric results would\nsuggest that state-of-the-art GAN-based methods perform best, a perceptual\nevaluation via a listening test indicates that the presented NF approach (based\non time domain and APG) performs best, especially at lower SNRs. On average,\nAPG outputs are rated as having good quality, which is unmatched by the other\nmethods, including GAN.", "published": "2022-10-21 01:01:01", "link": "http://arxiv.org/abs/2210.11654v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BEANS: The Benchmark of Animal Sounds", "abstract": "The use of machine learning (ML) based techniques has become increasingly\npopular in the field of bioacoustics over the last years. Fundamental\nrequirements for the successful application of ML based techniques are curated,\nagreed upon, high-quality datasets and benchmark tasks to be learned on a given\ndataset. However, the field of bioacoustics so far lacks such public benchmarks\nwhich cover multiple tasks and species to measure the performance of ML\ntechniques in a controlled and standardized way and that allows for\nbenchmarking newly proposed techniques to existing ones. Here, we propose BEANS\n(the BEnchmark of ANimal Sounds), a collection of bioacoustics tasks and public\ndatasets, specifically designed to measure the performance of machine learning\nalgorithms in the field of bioacoustics. The benchmark proposed here consists\nof two common tasks in bioacoustics: classification and detection. It includes\n12 datasets covering various species, including birds, land and marine mammals,\nanurans, and insects. In addition to the datasets, we also present the\nperformance of a set of standard ML methods as the baseline for task\nperformance. The benchmark and baseline code is made publicly available at\n\\url{https://github.com/earthspecies/beans} in the hope of establishing a new\nstandard dataset for ML-based bioacoustic research.", "published": "2022-10-21 23:34:06", "link": "http://arxiv.org/abs/2210.12300v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Adaptive re-calibration of channel-wise features for Adversarial Audio\n  Classification", "abstract": "DeepFake Audio, unlike DeepFake images and videos, has been relatively less\nexplored from detection perspective, and the solutions which exist for the\nsynthetic speech classification either use complex networks or dont generalize\nto different varieties of synthetic speech obtained using different generative\nand optimization-based methods. Through this work, we propose a channel-wise\nrecalibration of features using attention feature fusion for synthetic speech\ndetection and compare its performance against different detection methods\nincluding End2End models and Resnet-based models on synthetic speech generated\nusing Text to Speech and Vocoder systems like WaveNet, WaveRNN, Tactotron, and\nWaveGlow. We also experiment with Squeeze Excitation (SE) blocks in our Resnet\nmodels and found that the combination was able to get better performance. In\naddition to the analysis, we also demonstrate that the combination of Linear\nfrequency cepstral coefficients (LFCC) and Mel Frequency cepstral coefficients\n(MFCC) using the attentional feature fusion technique creates better input\nfeatures representations which can help even simpler models generalize well on\nsynthetic speech classification tasks. Our models (Resnet based using feature\nfusion) trained on Fake or Real (FoR) dataset and were able to achieve 95% test\naccuracy with the FoR data, and an average of 90% accuracy with samples we\ngenerated using different generative models after adapting this framework.", "published": "2022-10-21 04:21:56", "link": "http://arxiv.org/abs/2210.11722v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Evidence of Vocal Tract Articulation in Self-Supervised Learning of\n  Speech", "abstract": "Recent self-supervised learning (SSL) models have proven to learn rich\nrepresentations of speech, which can readily be utilized by diverse downstream\ntasks. To understand such utilities, various analyses have been done for speech\nSSL models to reveal which and how information is encoded in the learned\nrepresentations. Although the scope of previous analyses is extensive in\nacoustic, phonetic, and semantic perspectives, the physical grounding by speech\nproduction has not yet received full attention. To bridge this gap, we conduct\na comprehensive analysis to link speech representations to articulatory\ntrajectories measured by electromagnetic articulography (EMA). Our analysis is\nbased on a linear probing approach where we measure articulatory score as an\naverage correlation of linear mapping to EMA. We analyze a set of SSL models\nselected from the leaderboard of the SUPERB benchmark and perform further\nlayer-wise analyses on two most successful models, Wav2Vec 2.0 and HuBERT.\nSurprisingly, representations from the recent speech SSL models are highly\ncorrelated with EMA traces (best: r = 0.81), and only 5 minutes are sufficient\nto train a linear model with high performance (r = 0.77). Our findings suggest\nthat SSL models learn to align closely with continuous articulations, and\nprovide a novel insight into speech SSL.", "published": "2022-10-21 04:24:29", "link": "http://arxiv.org/abs/2210.11723v3", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Adversarial Permutation Invariant Training for Universal Sound\n  Separation", "abstract": "Universal sound separation consists of separating mixes with arbitrary sounds\nof different types, and permutation invariant training (PIT) is used to train\nsource agnostic models that do so. In this work, we complement PIT with\nadversarial losses but find it challenging with the standard formulation used\nin speech source separation. We overcome this challenge with a novel\nI-replacement context-based adversarial loss, and by training with multiple\ndiscriminators. Our experiments show that by simply improving the loss (keeping\nthe same model and dataset) we obtain a non-negligible improvement of 1.4 dB\nSI-SNRi in the reverberant FUSS dataset. We also find adversarial PIT to be\neffective at reducing spectral holes, ubiquitous in mask-based separation\nmodels, which highlights the potential relevance of adversarial losses for\nsource separation.", "published": "2022-10-21 17:04:17", "link": "http://arxiv.org/abs/2210.12108v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
