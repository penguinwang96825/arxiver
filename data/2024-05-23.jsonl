{"title": "FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models", "abstract": "As financial institutions and professionals increasingly incorporate Large\nLanguage Models (LLMs) into their workflows, substantial barriers, including\nproprietary data and specialized knowledge, persist between the finance sector\nand the AI community. These challenges impede the AI community's ability to\nenhance financial tasks effectively. Acknowledging financial analysis's\ncritical role, we aim to devise financial-specialized LLM-based toolchains and\ndemocratize access to them through open-source initiatives, promoting wider AI\nadoption in financial decision-making. In this paper, we introduce FinRobot, a\nnovel open-source AI agent platform supporting multiple financially specialized\nAI agents, each powered by LLM. Specifically, the platform consists of four\nmajor layers: 1) the Financial AI Agents layer that formulates Financial\nChain-of-Thought (CoT) by breaking sophisticated financial problems down into\nlogical sequences; 2) the Financial LLM Algorithms layer dynamically configures\nappropriate model application strategies for specific tasks; 3) the LLMOps and\nDataOps layer produces accurate models by applying training/fine-tuning\ntechniques and using task-relevant data; 4) the Multi-source LLM Foundation\nModels layer that integrates various LLMs and enables the above layers to\naccess them directly. Finally, FinRobot provides hands-on for both\nprofessional-grade analysts and laypersons to utilize powerful AI techniques\nfor advanced financial analysis. We open-source FinRobot at\n\\url{https://github.com/AI4Finance-Foundation/FinRobot}.", "published": "2024-05-23 16:35:20", "link": "http://arxiv.org/abs/2405.14767v2", "categories": ["q-fin.ST", "cs.CL", "cs.LG", "q-fin.TR"], "primary_category": "q-fin.ST"}
{"title": "Large Language Models Can Self-Correct with Key Condition Verification", "abstract": "Intrinsic self-correct was a method that instructed large language models\n(LLMs) to verify and correct their responses without external feedback.\nUnfortunately, the study concluded that the LLMs could not self-correct\nreasoning yet. We find that a simple yet effective verification method can\nunleash inherent capabilities of the LLMs. That is to mask a key condition in\nthe question, add the current response to construct a verification question,\nand predict the condition to verify the response. The condition can be an\nentity in an open-domain question or a numeric value in a math question, which\nrequires minimal effort (via prompting) to identify. We propose an iterative\nverify-then-correct framework to progressively identify and correct (probably)\nfalse responses, named ProCo. We conduct experiments on three reasoning tasks.\nOn average, ProCo, with GPT-3.5-Turbo as the backend LLM, yields $+6.8$ exact\nmatch on four open-domain question answering datasets, $+14.1$ accuracy on\nthree arithmetic reasoning datasets, and $+9.6$ accuracy on a commonsense\nreasoning dataset, compared to Self-Correct. Our implementation is made\npublicly available at https://wzy6642.github.io/proco.github.io/.", "published": "2024-05-23 01:43:45", "link": "http://arxiv.org/abs/2405.14092v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ViHateT5: Enhancing Hate Speech Detection in Vietnamese With A Unified\n  Text-to-Text Transformer Model", "abstract": "Recent advancements in hate speech detection (HSD) in Vietnamese have made\nsignificant progress, primarily attributed to the emergence of\ntransformer-based pre-trained language models, particularly those built on the\nBERT architecture. However, the necessity for specialized fine-tuned models has\nresulted in the complexity and fragmentation of developing a multitasking HSD\nsystem. Moreover, most current methodologies focus on fine-tuning general\npre-trained models, primarily trained on formal textual datasets like\nWikipedia, which may not accurately capture human behavior on online platforms.\nIn this research, we introduce ViHateT5, a T5-based model pre-trained on our\nproposed large-scale domain-specific dataset named VOZ-HSD. By harnessing the\npower of a text-to-text architecture, ViHateT5 can tackle multiple tasks using\na unified model and achieve state-of-the-art performance across all standard\nHSD benchmarks in Vietnamese. Our experiments also underscore the significance\nof label distribution in pre-training data on model efficacy. We provide our\nexperimental materials for research purposes, including the VOZ-HSD dataset,\npre-trained checkpoint, the unified HSD-multitask ViHateT5 model, and related\nsource code on GitHub publicly.", "published": "2024-05-23 03:31:50", "link": "http://arxiv.org/abs/2405.14141v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "jp-evalb: Robust Alignment-based PARSEVAL Measures", "abstract": "We introduce an evaluation system designed to compute PARSEVAL measures,\noffering a viable alternative to \\texttt{evalb} commonly used for constituency\nparsing evaluation. The widely used \\texttt{evalb} script has traditionally\nbeen employed for evaluating the accuracy of constituency parsing results,\nalbeit with the requirement for consistent tokenization and sentence\nboundaries. In contrast, our approach, named \\texttt{jp-evalb}, is founded on\nan alignment method. This method aligns sentences and words when discrepancies\narise. It aims to overcome several known issues associated with \\texttt{evalb}\nby utilizing the `jointly preprocessed (JP)' alignment-based method. We\nintroduce a more flexible and adaptive framework, ultimately contributing to a\nmore accurate assessment of constituency parsing performance.", "published": "2024-05-23 03:54:25", "link": "http://arxiv.org/abs/2405.14150v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UzMorphAnalyser: A Morphological Analysis Model for the Uzbek Language\n  Using Inflectional Endings", "abstract": "As Uzbek language is agglutinative, has many morphological features which\nwords formed by combining root and affixes. Affixes play an important role in\nthe morphological analysis of words, by adding additional meanings and\ngrammatical functions to words. Inflectional endings are utilized to express\nvarious morphological features within the language. This feature introduces\nnumerous possibilities for word endings, thereby significantly expanding the\nword vocabulary and exacerbating issues related to data sparsity in statistical\nmodels. This paper present modeling of the morphological analysis of Uzbek\nwords, including stemming, lemmatizing, and the extraction of morphological\ninformation while considering morpho-phonetic exceptions. Main steps of the\nmodel involve developing a complete set of word-ending with assigned\nmorphological information, and additional datasets for morphological analysis.\nThe proposed model was evaluated using a curated test set comprising 5.3K\nwords. Through manual verification of stemming, lemmatizing, and morphological\nfeature corrections carried out by linguistic specialists, it obtained a\nword-level accuracy of over 91%. The developed tool based on the proposed model\nis available as a web-based application and an open-source Python library.", "published": "2024-05-23 05:06:55", "link": "http://arxiv.org/abs/2405.14179v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChronosLex: Time-aware Incremental Training for Temporal Generalization\n  of Legal Classification Tasks", "abstract": "This study investigates the challenges posed by the dynamic nature of legal\nmulti-label text classification tasks, where legal concepts evolve over time.\nExisting models often overlook the temporal dimension in their training\nprocess, leading to suboptimal performance of those models over time, as they\ntreat training data as a single homogeneous block. To address this, we\nintroduce ChronosLex, an incremental training paradigm that trains models on\nchronological splits, preserving the temporal order of the data. However, this\nincremental approach raises concerns about overfitting to recent data,\nprompting an assessment of mitigation strategies using continual learning and\ntemporal invariant methods. Our experimental results over six legal multi-label\ntext classification datasets reveal that continual learning methods prove\neffective in preventing overfitting thereby enhancing temporal\ngeneralizability, while temporal invariant methods struggle to capture these\ndynamics of temporal shifts.", "published": "2024-05-23 06:09:16", "link": "http://arxiv.org/abs/2405.14211v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "From Role-Play to Drama-Interaction: An LLM Solution", "abstract": "Drama is a form of storytelling inspired by human creativity, proceeding with\na predefined storyline, carrying emotions and thoughts. This paper introduces\n\\emph{LLM-based interactive drama}, which endows traditional drama with an\nunprecedented immersion, where a person is allowed to walk into it and interact\nwith the characters and scenes. We define this new artistic genre by 6\nessential elements-plot, character, thought, diction, spectacle and\ninteraction-and study the entire pipeline to forge a backbone \\emph{drama LLM}\nto drive the playing process, which is challenged by limited drama resources,\nuncontrollable narrative development, and complicated instruction following. We\npropose \\emph{Narrative Chain} to offer finer control over the narrative\nprogression during interaction with players; \\emph{Auto-Drama} to synthesize\ndrama scripts given arbitrary stories; \\emph{Sparse Instruction Tuning} to\nallow the model to follow sophisticated instructions. We manually craft 3\nscripts, \\emph{Detective Conan}, \\emph{Harry Potter}, \\emph{Romeo and Juliet},\nand design a 5-dimension principle to evaluate the drama LLM comprehensively.", "published": "2024-05-23 07:03:56", "link": "http://arxiv.org/abs/2405.14231v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text-Based Correlation Matrix in Multi-Asset Allocation", "abstract": "The purpose of this study is to estimate the correlation structure between\nmultiple assets using financial text analysis. In recent years, as the\nbackground of elevating inflation in the global economy and monetary policy\ntightening by central banks, the correlation structure between assets,\nespecially interest rate sensitivity and inflation sensitivity, has changed\ndramatically, increasing the impact on the performance of investors'\nportfolios. Therefore, the importance of estimating a robust correlation\nstructure in portfolio management has increased. On the other hand, the\ncorrelation coefficient using only the historical price data observed in the\nfinancial market is accompanied by a certain degree of time lag, and also has\nthe aspect that prediction errors can occur due to the nonstationarity of\nfinancial time series data, and that the interpretability from the viewpoint of\nfundamentals is a little poor when a phase change occurs. In this study, we\nperformed natural language processing on news text and central bank text to\nverify the prediction accuracy of future correlation coefficient changes. As a\nresult, it was suggested that this method is useful in comparison with the\nprediction from ordinary time series data.", "published": "2024-05-23 07:25:51", "link": "http://arxiv.org/abs/2405.14247v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Language Models Trained on Translated Data with Continual\n  Pre-Training and Dictionary Learning Analysis", "abstract": "Training LLMs for low-resource languages usually utilizes data augmentation\nfrom English using machine translation (MT). This, however, brings a number of\nchallenges to LLM training: there are large costs attached to translating and\ncurating huge amounts of content with high-end machine translation solutions;\nthe translated content carries over cultural biases; and if the translation is\nnot faithful and accurate, data quality degrades causing issues in the trained\nmodel. In this work, we investigate the role of translation and synthetic data\nin training language models. We translate TinyStories, a dataset of 2.2M short\nstories for 3-4 year old children, from English to Arabic using the open\nNLLB-3B MT model. We train a number of story generation models of size 1M-33M\nparameters using this data. We identify a number of quality and task-specific\nissues in the resulting models. To rectify these issues, we further pre-train\nthe models with a small dataset of synthesized high-quality Arabic stories\ngenerated by a capable LLM, representing 1% of the original training data. We\nshow, using GPT-4 as a judge and Dictionary Learning Analysis from mechanistic\ninterpretability, that the suggested approach is a practical means to resolve\nsome of the machine translation pitfalls. We illustrate the improvements\nthrough case studies of linguistic and cultural bias issues.", "published": "2024-05-23 07:53:04", "link": "http://arxiv.org/abs/2405.14277v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Combining Denoising Autoencoders with Contrastive Learning to fine-tune\n  Transformer Models", "abstract": "Recently, using large pretrained Transformer models for transfer learning\ntasks has evolved to the point where they have become one of the flagship\ntrends in the Natural Language Processing (NLP) community, giving rise to\nvarious outlooks such as prompt-based, adapters or combinations with\nunsupervised approaches, among many others. This work proposes a 3 Phase\ntechnique to adjust a base model for a classification task. First, we adapt the\nmodel's signal to the data distribution by performing further training with a\nDenoising Autoencoder (DAE). Second, we adjust the representation space of the\noutput to the corresponding classes by clustering through a Contrastive\nLearning (CL) method. In addition, we introduce a new data augmentation\napproach for Supervised Contrastive Learning to correct the unbalanced\ndatasets. Third, we apply fine-tuning to delimit the predefined categories.\nThese different phases provide relevant and complementary knowledge to the\nmodel to learn the final task. We supply extensive experimental results on\nseveral datasets to demonstrate these claims. Moreover, we include an ablation\nstudy and compare the proposed method against other ways of combining these\ntechniques.", "published": "2024-05-23 11:08:35", "link": "http://arxiv.org/abs/2405.14437v1", "categories": ["cs.CL", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Which Information Matters? Dissecting Human-written Multi-document\n  Summaries with Partial Information Decomposition", "abstract": "Understanding the nature of high-quality summaries is crucial to further\nimprove the performance of multi-document summarization. We propose an approach\nto characterize human-written summaries using partial information\ndecomposition, which decomposes the mutual information provided by all source\ndocuments into union, redundancy, synergy, and unique information. Our\nempirical analysis on different MDS datasets shows that there is a direct\ndependency between the number of sources and their contribution to the summary.", "published": "2024-05-23 11:56:54", "link": "http://arxiv.org/abs/2405.14470v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RefChecker: Reference-based Fine-grained Hallucination Checker and\n  Benchmark for Large Language Models", "abstract": "Large Language Models (LLMs) have shown impressive capabilities but also a\nconcerning tendency to hallucinate. This paper presents RefChecker, a framework\nthat introduces claim-triplets to represent claims in LLM responses, aiming to\ndetect fine-grained hallucinations. In RefChecker, an extractor generates\nclaim-triplets from a response, which are then evaluated by a checker against a\nreference. We delineate three task settings: Zero, Noisy and Accurate Context,\nto reflect various real-world use cases. We curated a benchmark spanning\nvarious NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven\nLLMs. RefChecker supports both proprietary and open-source models as the\nextractor and checker. Experiments demonstrate that claim-triplets enable\nsuperior hallucination detection, compared to other granularities such as\nresponse, sentence and sub-sentence level claims. RefChecker outperforms prior\nmethods by 6.8 to 26.1 points on our benchmark and the checking results of\nRefChecker are strongly aligned with human judgments. This work is open sourced\nat https://github.com/amazon-science/RefChecker", "published": "2024-05-23 12:18:11", "link": "http://arxiv.org/abs/2405.14486v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While\n  Preserving Their Usability", "abstract": "Large Language Models (LLMs) are increasingly deployed in various\napplications. As their usage grows, concerns regarding their safety are rising,\nespecially in maintaining harmless responses when faced with malicious\ninstructions. Many defense strategies have been developed to enhance the safety\nof LLMs. However, our research finds that existing defense strategies lead LLMs\nto predominantly adopt a rejection-oriented stance, thereby diminishing the\nusability of their responses to benign instructions. To solve this problem, we\nintroduce the MoGU framework, designed to enhance LLMs' safety while preserving\ntheir usability. Our MoGU framework transforms the base LLM into two variants:\nthe usable LLM and the safe LLM, and further employs dynamic routing to balance\ntheir contribution. When encountering malicious instructions, the router will\nassign a higher weight to the safe LLM to ensure that responses are harmless.\nConversely, for benign instructions, the router prioritizes the usable LLM,\nfacilitating usable and helpful responses. On various open-sourced LLMs, we\ncompare multiple defense strategies to verify the superiority of our MoGU\nframework. Besides, our analysis provides key insights into the effectiveness\nof MoGU and verifies that our designed routing mechanism can effectively\nbalance the contribution of each variant by assigning weights. Our work\nreleased the safer Llama2, Vicuna, Falcon, Dolphin, and Baichuan2.", "published": "2024-05-23 12:19:59", "link": "http://arxiv.org/abs/2405.14488v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Impact of Non-Standard Unicode Characters on Security and Comprehension\n  in Large Language Models", "abstract": "The advancement of large language models has significantly improved natural\nlanguage processing. However, challenges such as jailbreaks (prompt injections\nthat cause an LLM to follow instructions contrary to its intended use),\nhallucinations (generating incorrect or misleading information), and\ncomprehension errors remain prevalent. In this report, we present a comparative\nanalysis of the performance of fifteen distinct models, with each model\nundergoing a standardized test comprising 38 queries across three key metrics:\njailbreaks, hallucinations, and comprehension errors. The models are assessed\nbased on the total occurrences of jailbreaks, hallucinations, and comprehension\nerrors. Our work exposes these models' inherent vulnerabilities and challenges\nthe notion of human-level language comprehension of these models. We have\nempirically analysed the impact of non-standard Unicode characters on LLMs and\ntheir safeguarding mechanisms on the best-performing LLMs, including GPT-4,\nGemini 1.5 Pro, LlaMA-3-70B, and Claude 3 Opus. By incorporating alphanumeric\nsymbols from Unicode outside the standard Latin block and variants of\ncharacters in other languages, we observed a reduction in the efficacy of\nguardrails implemented through Reinforcement Learning Human Feedback (RLHF).\nConsequently, these models exhibit heightened vulnerability to content policy\nbreaches and prompt leakage. Our study also suggests a need to incorporate\nnon-standard Unicode text in LLM training data to enhance the capabilities of\nthese models.", "published": "2024-05-23 12:24:38", "link": "http://arxiv.org/abs/2405.14490v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Base of RoPE Bounds Context Length", "abstract": "Position embedding is a core component of current Large Language Models\n(LLMs). Rotary position embedding (RoPE), a technique that encodes the position\ninformation with a rotation matrix, has been the de facto choice for position\nembedding in many LLMs, such as the Llama series. RoPE has been further\nutilized to extend long context capability, which is roughly based on adjusting\nthe \\textit{base} parameter of RoPE to mitigate out-of-distribution (OOD)\nproblems in position embedding. However, in this paper, we find that LLMs may\nobtain a superficial long-context ability based on the OOD theory. We revisit\nthe role of RoPE in LLMs and propose a novel property of long-term decay, we\nderive that the \\textit{base of RoPE bounds context length}: there is an\nabsolute lower bound for the base value to obtain certain context length\ncapability. Our work reveals the relationship between context length and RoPE\nbase both theoretically and empirically, which may shed light on future long\ncontext training.", "published": "2024-05-23 14:03:31", "link": "http://arxiv.org/abs/2405.14591v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Watermarking Low-entropy Generation for Large Language Models: An\n  Unbiased and Low-risk Method", "abstract": "Recent advancements in large language models (LLMs) have highlighted the risk\nof misusing them, raising the need for accurate detection of LLM-generated\ncontent. In response, a viable solution is to inject imperceptible identifiers\ninto LLMs, known as watermarks. Our research extends the existing watermarking\nmethods by proposing the novel Sampling One Then Accepting (STA-1) method.\nSTA-1 is an unbiased watermark that preserves the original token distribution\nin expectation and has a lower risk of producing unsatisfactory outputs in\nlow-entropy scenarios compared to existing unbiased watermarks. In watermark\ndetection, STA-1 does not require prompts or a white-box LLM, provides\nstatistical guarantees, demonstrates high efficiency in detection time, and\nremains robust against various watermarking attacks. Experimental results on\nlow-entropy and high-entropy datasets demonstrate that STA-1 achieves the above\nproperties simultaneously, making it a desirable solution for watermarking\nLLMs. Implementation codes for this study are available online.", "published": "2024-05-23 14:17:29", "link": "http://arxiv.org/abs/2405.14604v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial\n  Framework Driven by Large Language Models", "abstract": "The automatic evaluation of natural language generation (NLG) systems\npresents a long-lasting challenge. Recent studies have highlighted various\nneural metrics that align well with human evaluations. Yet, the robustness of\nthese evaluators against adversarial perturbations remains largely\nunder-explored due to the unique challenges in obtaining adversarial data for\ndifferent NLG evaluation tasks. To address the problem, we introduce AdvEval, a\nnovel black-box adversarial framework against NLG evaluators. AdvEval is\nspecially tailored to generate data that yield strong disagreements between\nhuman and victim evaluators. Specifically, inspired by the recent success of\nlarge language models (LLMs) in text generation and evaluation, we adopt strong\nLLMs as both the data generator and gold evaluator. Adversarial data are\nautomatically optimized with feedback from the gold and victim evaluator. We\nconduct experiments on 12 victim evaluators and 11 NLG datasets, spanning tasks\nincluding dialogue, summarization, and question evaluation. The results show\nthat AdvEval can lead to significant performance degradation of various victim\nmetrics, thereby validating its efficacy.", "published": "2024-05-23 14:48:15", "link": "http://arxiv.org/abs/2405.14646v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DAPE: Data-Adaptive Positional Encoding for Length Extrapolation", "abstract": "Positional encoding plays a crucial role in transformers, significantly\nimpacting model performance and length generalization. Prior research has\nintroduced absolute positional encoding (APE) and relative positional encoding\n(RPE) to distinguish token positions in given sequences. However, both APE and\nRPE remain fixed after model training regardless of input data, limiting their\nadaptability and flexibility. Hence, we expect that the desired positional\nencoding should be data-adaptive and can be dynamically adjusted with the given\nattention. In this paper, we propose a Data-Adaptive Positional Encoding (DAPE)\nmethod, which dynamically and semantically adjusts based on input context and\nlearned fixed priors. Experimental validation on real-world datasets (Arxiv,\nBooks3, and CHE) demonstrates that DAPE enhances model performances in terms of\ntrained length and length generalization, where the improvements are\nstatistically significant. The model visualization suggests that our model can\nkeep both local and anti-local information. Finally, we successfully train the\nmodel on sequence length 128 and achieve better performance at evaluation\nsequence length 8192, compared with other static positional encoding methods,\nrevealing the benefit of the adaptive positional encoding method.", "published": "2024-05-23 15:51:24", "link": "http://arxiv.org/abs/2405.14722v6", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Lessons from the Trenches on Reproducible Evaluation of Language Models", "abstract": "Effective evaluation of language models remains an open challenge in NLP.\nResearchers and engineers face methodological issues such as the sensitivity of\nmodels to evaluation setup, difficulty of proper comparisons across methods,\nand the lack of reproducibility and transparency. In this paper we draw on\nthree years of experience in evaluating large language models to provide\nguidance and lessons for researchers. First, we provide an overview of common\nchallenges faced in language model evaluation. Second, we delineate best\npractices for addressing or lessening the impact of these challenges on\nresearch. Third, we present the Language Model Evaluation Harness (lm-eval): an\nopen source library for independent, reproducible, and extensible evaluation of\nlanguage models that seeks to address these issues. We describe the features of\nthe library as well as case studies in which the library has been used to\nalleviate these methodological concerns.", "published": "2024-05-23 16:50:49", "link": "http://arxiv.org/abs/2405.14782v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Can LLMs Solve longer Math Word Problems Better?", "abstract": "Math Word Problems (MWPs) play a vital role in assessing the capabilities of\nLarge Language Models (LLMs), yet current research primarily focuses on\nquestions with concise contexts. The impact of longer contexts on mathematical\nreasoning remains under-explored. This study pioneers the investigation of\nContext Length Generalizability (CoLeG), which refers to the ability of LLMs to\nsolve MWPs with extended narratives. We introduce Extended Grade-School Math\n(E-GSM), a collection of MWPs featuring lengthy narratives, and propose two\nnovel metrics to evaluate the efficacy and resilience of LLMs in tackling these\nproblems. Our analysis of existing zero-shot prompting techniques with\nproprietary LLMs along with open-source LLMs reveals a general deficiency in\nCoLeG. To alleviate these issues, we propose tailored approaches for different\ncategories of LLMs. For proprietary LLMs, we introduce a new instructional\nprompt designed to mitigate the impact of long contexts. For open-source LLMs,\nwe develop a novel auxiliary task for fine-tuning to enhance CoLeG. Our\ncomprehensive results demonstrate the effectiveness of our proposed methods,\nshowing improved performance on E-GSM. Additionally, we conduct an in-depth\nanalysis to differentiate the effects of semantic understanding and reasoning\nefficacy, showing that our methods improves the latter. We also establish the\ngeneralizability of our methods across several other MWP benchmarks. Our\nfindings highlight the limitations of current LLMs and offer practical\nsolutions correspondingly, paving the way for further exploration of model\ngeneralizability and training methodologies.", "published": "2024-05-23 17:13:50", "link": "http://arxiv.org/abs/2405.14804v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Bitune: Bidirectional Instruction-Tuning", "abstract": "We introduce Bitune, a method that improves instruction-tuning of pretrained\ndecoder-only large language models, leading to consistent gains on downstream\ntasks. Bitune applies both causal and bidirectional attention to the prompt, to\nobtain a better representation of the query or instruction. We realize this by\nintroducing two sets of parameters, for which we apply parameter-efficient\nfinetuning techniques. These causal and bidirectional features are then\ncombined into a weighted average with trainable coefficients, which is\nsubsequently used to generate new tokens. We demonstrate significant\nimprovements in zero-shot performance on commonsense reasoning, arithmetic, and\nlanguage understanding tasks, while extensive ablation studies validate the\nrole of each component and demonstrate the method's agnosticism to different\nPEFT techniques.", "published": "2024-05-23 17:59:22", "link": "http://arxiv.org/abs/2405.14862v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data Augmentation Method Utilizing Template Sentences for Variable\n  Definition Extraction", "abstract": "The extraction of variable definitions from scientific and technical papers\nis essential for understanding these documents. However, the characteristics of\nvariable definitions, such as the length and the words that make up the\ndefinition, differ among fields, which leads to differences in the performance\nof existing extraction methods across fields. Although preparing training data\nspecific to each field can improve the performance of the methods, it is costly\nto create high-quality training data. To address this challenge, this study\nproposes a new method that generates new definition sentences from template\nsentences and variable-definition pairs in the training data. The proposed\nmethod has been tested on papers about chemical processes, and the results show\nthat the model trained with the definition sentences generated by the proposed\nmethod achieved a higher accuracy of 89.6%, surpassing existing models.", "published": "2024-05-23 18:14:05", "link": "http://arxiv.org/abs/2405.14962v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Aya 23: Open Weight Releases to Further Multilingual Progress", "abstract": "This technical report introduces Aya 23, a family of multilingual language\nmodels. Aya 23 builds on the recent release of the Aya model (\\\"Ust\\\"un et al.,\n2024), focusing on pairing a highly performant pre-trained model with the\nrecently released Aya collection (Singh et al., 2024). The result is a powerful\nmultilingual large language model serving 23 languages, expanding state-of-art\nlanguage modeling capabilities to approximately half of the world's population.\nThe Aya model covered 101 languages whereas Aya 23 is an experiment in depth vs\nbreadth, exploring the impact of allocating more capacity to fewer languages\nthat are included during pre-training. Aya 23 outperforms both previous\nmassively multilingual models like Aya 101 for the languages it covers, as well\nas widely used models like Gemma, Mistral and Mixtral on an extensive range of\ndiscriminative and generative tasks. We release the open weights for both the\n8B and 35B models as part of our continued commitment for expanding access to\nmultilingual progress.", "published": "2024-05-23 20:10:38", "link": "http://arxiv.org/abs/2405.15032v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Promoting Constructive Deliberation: Reframing for Receptiveness", "abstract": "To promote constructive discussion of controversial topics online, we propose\nautomatic reframing of disagreeing responses to signal receptiveness to a\npreceding comment. Drawing on research from psychology, communications, and\nlinguistics, we identify six strategies for reframing. We automatically reframe\nreplies to comments according to each strategy, using a Reddit dataset. Through\nhuman-centered experiments, we find that the replies generated with our\nframework are perceived to be significantly more receptive than the original\nreplies and a generic receptiveness baseline. We illustrate how transforming\nreceptiveness, a particular social science construct, into a computational\nframework, can make LLM generations more aligned with human perceptions. We\nanalyze and discuss the implications of our results, and highlight how a tool\nbased on our framework might be used for more teachable and creative content\nmoderation.", "published": "2024-05-23 21:35:22", "link": "http://arxiv.org/abs/2405.15067v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Optimizing example selection for retrieval-augmented machine translation\n  with translation memories", "abstract": "Retrieval-augmented machine translation leverages examples from a translation\nmemory by retrieving similar instances. These examples are used to condition\nthe predictions of a neural decoder. We aim to improve the upstream retrieval\nstep and consider a fixed downstream edit-based model: the multi-Levenshtein\nTransformer. The task consists of finding a set of examples that maximizes the\noverall coverage of the source sentence. To this end, we rely on the theory of\nsubmodular functions and explore new algorithms to optimize this coverage. We\nevaluate the resulting performance gains for the machine translation task.", "published": "2024-05-23 21:42:03", "link": "http://arxiv.org/abs/2405.15070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to\n  the Edge of Generalization", "abstract": "We study whether transformers can learn to implicitly reason over parametric\nknowledge, a skill that even the most capable language models struggle with.\nFocusing on two representative reasoning types, composition and comparison, we\nconsistently find that transformers can learn implicit reasoning, but only\nthrough grokking, i.e., extended training far beyond overfitting. The levels of\ngeneralization also vary across reasoning types: when faced with\nout-of-distribution examples, transformers fail to systematically generalize\nfor composition but succeed for comparison. We delve into the model's internals\nthroughout training, conducting analytical experiments that reveal: 1) the\nmechanism behind grokking, such as the formation of the generalizing circuit\nand its relation to the relative efficiency of generalizing and memorizing\ncircuits, and 2) the connection between systematicity and the configuration of\nthe generalizing circuit. Our findings guide data and training setup to better\ninduce implicit reasoning and suggest potential improvements to the transformer\narchitecture, such as encouraging cross-layer knowledge sharing. Furthermore,\nwe demonstrate that for a challenging reasoning task with a large search space,\nGPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly\nregardless of prompting styles or retrieval augmentation, while a fully grokked\ntransformer can achieve near-perfect accuracy, showcasing the power of\nparametric memory for complex reasoning.", "published": "2024-05-23 21:42:19", "link": "http://arxiv.org/abs/2405.15071v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DuanzAI: Slang-Enhanced LLM with Prompt for Humor Understanding", "abstract": "Language's complexity is evident in the rich tapestry of slang expressions,\noften laden with humor and cultural nuances. This linguistic phenomenon has\nbecome increasingly prevalent, especially in digital communication. However,\nexisting AI models, including ChatGPT-3.5, face challenges in comprehending\nthese nuances, particularly in Chinese slang. In this study, we present\nDuanzAI, an innovative approach enhancing Large Language Models (LLMs) with\ndeep Chinese slang comprehension. Leveraging curated datasets and advanced\ntechniques, DuanzAI bridges the gap between human expression and AI\ncomprehension, enabling contextually relevant responses. Our experiments\ncontrast LLMs' performance with a custom Punchline Entity Recognition (PER)\nsystem, integrating phonetic matching and pinyin2hanzi techniques. Applying\nthese insights, we developed ChatDAI, an advanced chatbot and released our code\nat \\url{https://github.com/YesianRohn/DuanzAI}.", "published": "2024-05-23 03:13:50", "link": "http://arxiv.org/abs/2405.15818v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploration of Attention Mechanism-Enhanced Deep Learning Models in the\n  Mining of Medical Textual Data", "abstract": "The research explores the utilization of a deep learning model employing an\nattention mechanism in medical text mining. It targets the challenge of\nanalyzing unstructured text information within medical data. This research\nseeks to enhance the model's capability to identify essential medical\ninformation by incorporating deep learning and attention mechanisms. This paper\nreviews the basic principles and typical model architecture of attention\nmechanisms and shows the effectiveness of their application in the tasks of\ndisease prediction, drug side effect monitoring, and entity relationship\nextraction. Aiming at the particularity of medical texts, an adaptive attention\nmodel integrating domain knowledge is proposed, and its ability to understand\nmedical terms and process complex contexts is optimized. The experiment\nverifies the model's effectiveness in improving task accuracy and robustness,\nespecially when dealing with long text. The future research path of enhancing\nmodel interpretation, realizing cross-domain knowledge transfer, and adapting\nto low-resource scenarios is discussed in the research outlook, which provides\na new perspective and method support for intelligent medical information\nprocessing and clinical decision assistance. Finally, cross-domain knowledge\ntransfer and adaptation strategies for low-resource scenarios, providing\ntheoretical basis and technical reference for promoting the development of\nintelligent medical information processing and clinical decision support\nsystems.", "published": "2024-05-23 00:20:14", "link": "http://arxiv.org/abs/2406.00016v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Knowledge Localization: Mission Not Accomplished? Enter Query\n  Localization!", "abstract": "Large language models (LLMs) store extensive factual knowledge, but the\nmechanisms behind how they store and express this knowledge remain unclear. The\nKnowledge Neuron (KN) thesis is a prominent theory for explaining these\nmechanisms. This theory is based on the Knowledge Localization (KL) assumption,\nwhich suggests that a fact can be localized to a few knowledge storage units,\nnamely knowledge neurons.\n  However, this assumption has two limitations: first, it may be too rigid\nregarding knowledge storage, and second, it neglects the role of the attention\nmodule in knowledge expression.\n  In this paper, we first re-examine the KL assumption and demonstrate that its\nlimitations do indeed exist. To address these, we then present two new\nfindings, each targeting one of the limitations: one focusing on knowledge\nstorage and the other on knowledge expression. We summarize these findings as\n\\textbf{Query Localization} (QL) assumption and argue that the KL assumption\ncan be viewed as a simplification of the QL assumption. Based on QL assumption,\nwe further propose the Consistency-Aware KN modification method, which improves\nthe performance of knowledge modification, further validating our new\nassumption. We conduct 39 sets of experiments, along with additional\nvisualization experiments, to rigorously confirm our conclusions. Code is\navailable at https://github.com/heng840/KnowledgeLocalization.", "published": "2024-05-23 02:44:12", "link": "http://arxiv.org/abs/2405.14117v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based\n  Evaluation", "abstract": "Large Language Models (LLMs) can elicit unintended and even harmful content\nwhen misaligned with human values, posing severe risks to users and society. To\nmitigate these risks, current evaluation benchmarks predominantly employ\nexpert-designed contextual scenarios to assess how well LLMs align with human\nvalues. However, the labor-intensive nature of these benchmarks limits their\ntest scope, hindering their ability to generalize to the extensive variety of\nopen-world use cases and identify rare but crucial long-tail risks.\nAdditionally, these static tests fail to adapt to the rapid evolution of LLMs,\nmaking it hard to evaluate timely alignment issues. To address these\nchallenges, we propose ALI-Agent, an evaluation framework that leverages the\nautonomous abilities of LLM-powered agents to conduct in-depth and adaptive\nalignment assessments. ALI-Agent operates through two principal stages:\nEmulation and Refinement. During the Emulation stage, ALI-Agent automates the\ngeneration of realistic test scenarios. In the Refinement stage, it iteratively\nrefines the scenarios to probe long-tail risks. Specifically, ALI-Agent\nincorporates a memory module to guide test scenario generation, a tool-using\nmodule to reduce human labor in tasks such as evaluating feedback from target\nLLMs, and an action module to refine tests. Extensive experiments across three\naspects of human values--stereotypes, morality, and legality--demonstrate that\nALI-Agent, as a general evaluation framework, effectively identifies model\nmisalignment. Systematic analysis also validates that the generated test\nscenarios represent meaningful use cases, as well as integrate enhanced\nmeasures to probe long-tail risks. Our code is available at\nhttps://github.com/SophieZheng998/ALI-Agent.git", "published": "2024-05-23 02:57:42", "link": "http://arxiv.org/abs/2405.14125v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Super Tiny Language Models", "abstract": "The rapid advancement of large language models (LLMs) has led to significant\nimprovements in natural language processing but also poses challenges due to\ntheir high computational and energy demands. This paper introduces a series of\nresearch efforts focused on Super Tiny Language Models (STLMs), which aim to\ndeliver high performance with significantly reduced parameter counts. We\nexplore innovative techniques such as byte-level tokenization with a pooling\nmechanism, weight tying, and efficient training strategies. These methods aim\nto significantly reduce reduce the parameter count compared to traditional\nmodels -- in future works, we aim to build on these in a way that maintains and\nimproves upon the performance of base transformer models. This series of papers\nwill explore into various subproblems, including tokenizer-free models,\nself-play based training, and alternative training objectives. We will target\nmodels with 10M, 50M, and 100M parameters. Our ultimate goal is to make\nhigh-performance language models more accessible and practical for a wide range\nof applications.", "published": "2024-05-23 04:12:49", "link": "http://arxiv.org/abs/2405.14159v2", "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning", "abstract": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks.", "published": "2024-05-23 04:54:37", "link": "http://arxiv.org/abs/2405.14170v3", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Semantic-guided Prompt Organization for Universal Goal Hijacking against\n  LLMs", "abstract": "With the rising popularity of Large Language Models (LLMs), assessing their\ntrustworthiness through security tasks has gained critical importance.\nRegarding the new task of universal goal hijacking, previous efforts have\nconcentrated solely on optimization algorithms, overlooking the crucial role of\nthe prompt. To fill this gap, we propose a universal goal hijacking method\ncalled POUGH that incorporates semantic-guided prompt processing strategies.\nSpecifically, the method starts with a sampling strategy to select\nrepresentative prompts from a candidate pool, followed by a ranking strategy\nthat prioritizes the prompts. Once the prompts are organized sequentially, the\nmethod employs an iterative optimization algorithm to generate the universal\nfixed suffix for the prompts. Experiments conducted on four popular LLMs and\nten types of target responses verified the effectiveness of our method.", "published": "2024-05-23 05:31:41", "link": "http://arxiv.org/abs/2405.14189v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "S-Eval: Towards Automated and Comprehensive Safety Evaluation for Large\n  Language Models", "abstract": "Generative large language models (LLMs) have revolutionized natural language\nprocessing with their transformative and emergent capabilities. However, recent\nevidence indicates that LLMs can produce harmful content that violates social\nnorms, raising significant concerns regarding the safety and ethical\nramifications of deploying these advanced models. Thus, it is both critical and\nimperative to perform a rigorous and comprehensive safety evaluation of LLMs\nbefore deployment. Despite this need, owing to the extensiveness of LLM\ngeneration space, it still lacks a unified and standardized risk taxonomy to\nsystematically reflect the LLM content safety, as well as automated safety\nassessment techniques to explore the potential risk efficiently.\n  To bridge the striking gap, we propose S-Eval, a novel LLM-based automated\nSafety Evaluation framework with a newly defined comprehensive risk taxonomy.\nS-Eval incorporates two key components, i.e., an expert testing LLM ${M}_t$ and\na novel safety critique LLM ${M}_c$. ${M}_t$ is responsible for automatically\ngenerating test cases in accordance with the proposed risk taxonomy. ${M}_c$\ncan provide quantitative and explainable safety evaluations for better risk\nawareness of LLMs. In contrast to prior works, S-Eval is efficient and\neffective in test generation and safety evaluation. Moreover, S-Eval can be\nflexibly configured and adapted to the rapid evolution of LLMs and accompanying\nnew safety threats, test generation methods and safety critique methods thanks\nto the LLM-based architecture. S-Eval has been deployed in our industrial\npartner for the automated safety evaluation of multiple LLMs serving millions\nof users, demonstrating its effectiveness in real-world scenarios. Our\nbenchmark is publicly available at https://github.com/IS2Lab/S-Eval.", "published": "2024-05-23 05:34:31", "link": "http://arxiv.org/abs/2405.14191v4", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "Federated Domain-Specific Knowledge Transfer on Large Language Models\n  Using Synthetic Data", "abstract": "As large language models (LLMs) demonstrate unparalleled performance and\ngeneralization ability, LLMs are widely used and integrated into various\napplications. When it comes to sensitive domains, as commonly described in\nfederated learning scenarios, directly using external LLMs on private data is\nstrictly prohibited by stringent data security and privacy regulations. For\nlocal clients, the utilization of LLMs to improve the domain-specific small\nlanguage models (SLMs), characterized by limited computational resources and\ndomain-specific data, has attracted considerable research attention. By\nobserving that LLMs can empower domain-specific SLMs, existing methods\npredominantly concentrate on leveraging the public data or LLMs to generate\nmore data to transfer knowledge from LLMs to SLMs. However, due to the\ndiscrepancies between LLMs' generated data and clients' domain-specific data,\nthese methods cannot yield substantial improvements in the domain-specific\ntasks. In this paper, we introduce a Federated Domain-specific Knowledge\nTransfer (FDKT) framework, which enables domain-specific knowledge transfer\nfrom LLMs to SLMs while preserving clients' data privacy. The core insight is\nto leverage LLMs to augment data based on domain-specific few-shot\ndemonstrations, which are synthesized from private domain data using\ndifferential privacy. Such synthetic samples share similar data distribution\nwith clients' private data and allow the server LLM to generate particular\nknowledge to improve clients' SLMs. The extensive experimental results\ndemonstrate that the proposed FDKT framework consistently and greatly improves\nSLMs' task performance by around 5\\% with a privacy budget of less than 10,\ncompared to local training on private data.", "published": "2024-05-23 06:14:35", "link": "http://arxiv.org/abs/2405.14212v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "From Text to Pixel: Advancing Long-Context Understanding in MLLMs", "abstract": "The rapid progress in Multimodal Large Language Models (MLLMs) has\nsignificantly advanced their ability to process and understand complex visual\nand textual information. However, the integration of multiple images and\nextensive textual contexts remains a challenge due to the inherent limitation\nof the models' capacity to handle long input sequences efficiently. In this\npaper, we introduce SEEKER, a multimodal large language model designed to\ntackle this issue. SEEKER aims to optimize the compact encoding of long text by\ncompressing the text sequence into the visual pixel space via images, enabling\nthe model to handle long text within a fixed token-length budget efficiently.\nOur empirical experiments on six long-context multimodal tasks demonstrate that\nSEEKER can leverage fewer image tokens to convey the same amount of textual\ninformation compared with the OCR-based approach, and is more efficient in\nunderstanding long-form multimodal input and generating long-form textual\noutput, outperforming all existing proprietary and open-source MLLMs by large\nmargins.", "published": "2024-05-23 06:17:23", "link": "http://arxiv.org/abs/2405.14213v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with\n  LLMs for Multi-modal Text Recognition", "abstract": "We introduce \"Generative Fusion Decoding\" (GFD), a novel shallow fusion\nframework, utilized to integrate Large Language Models (LLMs) into multi-modal\ntext recognition systems such as automatic speech recognition (ASR) and optical\ncharacter recognition (OCR). We derive the formulas necessary to enable GFD to\noperate across mismatched token spaces of different models by mapping text\ntoken space to byte token space, enabling seamless fusion during the decoding\nprocess. The framework is plug-and-play, compatible with various\nauto-regressive models, and does not require re-training for feature alignment,\nthus overcoming limitations of previous fusion techniques. We highlight three\nmain advantages of GFD: First, by simplifying the complexity of aligning\ndifferent model sample spaces, GFD allows LLMs to correct errors in tandem with\nthe recognition model, reducing computation latencies. Second, the in-context\nlearning ability of LLMs is fully capitalized by GFD, increasing robustness in\nlong-form speech recognition and instruction aware speech recognition. Third,\nGFD enables fusing recognition models deficient in Chinese text recognition\nwith LLMs extensively trained on Chinese. Our evaluation demonstrates that GFD\nsignificantly improves performance in ASR and OCR tasks, with ASR reaching\nstate-of-the-art in the NTUML2021 benchmark. GFD provides a significant step\nforward in model integration, offering a unified solution that could be widely\napplicable to leveraging existing pre-trained models through step by step\nfusion.", "published": "2024-05-23 07:39:42", "link": "http://arxiv.org/abs/2405.14259v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training\n  Small Data Synthesis Models", "abstract": "Mathematical reasoning is an important capability of large language\nmodels~(LLMs) for real-world applications. To enhance this capability, existing\nwork either collects large-scale math-related texts for pre-training, or relies\non stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of\nwork generally lead to large costs in training or synthesis. To reduce the\ncost, based on open-source available texts, we propose an efficient way that\ntrains a small LLM for math problem synthesis, to efficiently generate\nsufficient high-quality pre-training data. To achieve it, we create a dataset\nusing GPT-4 to distill its data synthesis capability into the small LLM.\nConcretely, we craft a set of prompts based on human education stages to guide\nGPT-4, to synthesize problems covering diverse math knowledge and difficulty\nlevels. Besides, we adopt the gradient-based influence estimation method to\nselect the most valuable math-related texts. The both are fed into GPT-4 for\ncreating the knowledge distillation dataset to train the small LLM. We leverage\nit to synthesize 6 million math problems for pre-training our JiuZhang3.0\nmodel, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B\ndata. Experimental results have shown that JiuZhang3.0 achieves\nstate-of-the-art performance on several mathematical reasoning datasets, under\nboth natural language reasoning and tool manipulation settings. Our code and\ndata will be publicly released in\n\\url{https://github.com/RUCAIBox/JiuZhang3.0}.", "published": "2024-05-23 09:43:19", "link": "http://arxiv.org/abs/2405.14365v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Can Large Language Models Create New Knowledge for Spatial Reasoning\n  Tasks?", "abstract": "The potential for Large Language Models (LLMs) to generate new information\noffers a potential step change for research and innovation. This is challenging\nto assert as it can be difficult to determine what an LLM has previously seen\nduring training, making \"newness\" difficult to substantiate. In this paper we\nobserve that LLMs are able to perform sophisticated reasoning on problems with\na spatial dimension, that they are unlikely to have previously directly\nencountered. While not perfect, this points to a significant level of\nunderstanding that state-of-the-art LLMs can now achieve, supporting the\nproposition that LLMs are able to yield significant emergent properties. In\nparticular, Claude 3 is found to perform well in this regard.", "published": "2024-05-23 09:54:54", "link": "http://arxiv.org/abs/2405.14379v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Perception of Knowledge Boundary for Large Language Models through\n  Semi-open-ended Question Answering", "abstract": "Large Language Models (LLMs) are widely used for knowledge-seeking yet suffer\nfrom hallucinations. The knowledge boundary (KB) of an LLM limits its factual\nunderstanding, beyond which it may begin to hallucinate. Investigating the\nperception of LLMs' KB is crucial for detecting hallucinations and LLMs'\nreliable generation. Current studies perceive LLMs' KB on questions with a\nconcrete answer (close-ended questions) while paying limited attention to\nsemi-open-ended questions (SoeQ) that correspond to many potential answers.\nSome researchers achieve it by judging whether the question is answerable or\nnot. However, this paradigm is unsuitable for SoeQ, which are usually partially\nanswerable, containing both answerable and ambiguous (unanswerable) answers.\nAmbiguous answers are essential for knowledge-seeking, but they may go beyond\nthe KB of LLMs. In this paper, we perceive the LLMs' KB with SoeQ by\ndiscovering more ambiguous answers. First, we apply an LLM-based approach to\nconstruct SoeQ and obtain answers from a target LLM. Unfortunately, the output\nprobabilities of mainstream black-box LLMs are inaccessible to sample for\nlow-probability ambiguous answers. Therefore, we apply an open-sourced\nauxiliary model to explore ambiguous answers for the target LLM. We calculate\nthe nearest semantic representation for existing answers to estimate their\nprobabilities, with which we reduce the generation probability of\nhigh-probability answers to achieve a more effective generation. Finally, we\ncompare the results from the RAG-based evaluation and LLM self-evaluation to\ncategorize four types of ambiguous answers that are beyond the KB of the target\nLLM. Following our method, we construct a dataset to perceive the KB for GPT-4.\nWe find that GPT-4 performs poorly on SoeQ and is often unaware of its KB.\nBesides, our auxiliary model, LLaMA-2-13B, is effective in discovering more\nambiguous answers.", "published": "2024-05-23 10:00:14", "link": "http://arxiv.org/abs/2405.14383v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Emotion Identification for French in Written Texts: Considering their\n  Modes of Expression as a Step Towards Text Complexity Analysis", "abstract": "The objective of this paper is to predict (A) whether a sentence in a written\ntext expresses an emotion, (B) the mode(s) in which it is expressed, (C)\nwhether it is basic or complex, and (D) its emotional category.\n  One of our major contributions, through a dataset and a model, is to\nintegrate the fact that an emotion can be expressed in different modes: from a\ndirect mode, essentially lexicalized, to a more indirect mode, where emotions\nwill only be suggested, a mode that NLP approaches generally don't take into\naccount.\n  Another originality is that the scope is on written texts, as opposed usual\nwork focusing on conversational (often multi-modal) data. In this context,\nmodes of expression are seen as a factor towards the automatic analysis of\ncomplexity in texts.\n  Experiments on French texts show acceptable results compared to the human\nannotators' agreement, and outperforming results compared to using a large\nlanguage model with in-context learning (i.e. no fine-tuning).", "published": "2024-05-23 10:02:13", "link": "http://arxiv.org/abs/2405.14385v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Instruction Tuning With Loss Over Instructions", "abstract": "Instruction tuning plays a crucial role in shaping the outputs of language\nmodels (LMs) to desired styles. In this work, we propose a simple yet effective\nmethod, Instruction Modelling (IM), which trains LMs by applying a loss\nfunction to the instruction and prompt part rather than solely to the output\npart. Through experiments across 21 diverse benchmarks, we show that, in many\nscenarios, IM can effectively improve the LM performance on both NLP tasks\n(e.g., MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks\n(e.g., MT-Bench and AlpacaEval). Remarkably, in the most advantageous case, IM\nboosts model performance on AlpacaEval 1.0 by over 100%. We identify two key\nfactors influencing the effectiveness of IM: (1) The ratio between instruction\nlength and output length in the training data; and (2) The number of training\nexamples. We observe that IM is especially beneficial when trained on datasets\nwith lengthy instructions paired with brief outputs, or under the Superficial\nAlignment Hypothesis (SAH) where a small amount of training examples are used\nfor instruction tuning. Further analysis substantiates our hypothesis that our\nimprovement can be attributed to reduced overfitting to instruction tuning\ndatasets. It is worth noting that we are not proposing \\ours as a replacement\nfor current fine-tuning processes. Instead, our work aims to provide practical\nguidance for instruction tuning LMs, especially in low-resource scenarios.", "published": "2024-05-23 10:12:03", "link": "http://arxiv.org/abs/2405.14394v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mitigating Quantization Errors Due to Activation Spikes in GLU-Based\n  LLMs", "abstract": "Modern large language models (LLMs) have established state-of-the-art\nperformance through architectural improvements, but still require significant\ncomputational cost for inference. In an effort to reduce the inference cost,\npost-training quantization (PTQ) has become a popular approach, quantizing\nweights and activations to lower precision, such as INT8. In this paper, we\nreveal the challenges of activation quantization in GLU variants, which are\nwidely used in feed-forward network (FFN) of modern LLMs, such as LLaMA family.\nThe problem is that severe local quantization errors, caused by excessive\nmagnitudes of activation in GLU variants, significantly degrade the performance\nof the quantized LLM. We denote these activations as activation spikes. Our\nfurther observations provide a systematic pattern of activation spikes: 1) The\nactivation spikes occur in the FFN of specific layers, particularly in the\nearly and late layers, 2) The activation spikes are dedicated to a couple of\ntokens, rather than being shared across a sequence. Based on our observations,\nwe propose two empirical methods, Quantization-free Module (QFeM) and\nQuantization-free Prefix (QFeP), to isolate the activation spikes during\nquantization. Our extensive experiments validate the effectiveness of the\nproposed methods for the activation quantization, especially with\ncoarse-grained scheme, of latest LLMs with GLU variants, including LLaMA-2/3,\nMistral, Mixtral, SOLAR, and Gemma. In particular, our methods enhance the\ncurrent alleviation techniques (e.g., SmoothQuant) that fail to control the\nactivation spikes. Code is available at\nhttps://github.com/onnoo/activation-spikes.", "published": "2024-05-23 10:54:14", "link": "http://arxiv.org/abs/2405.14428v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Exploring the use of a Large Language Model for data extraction in\n  systematic reviews: a rapid feasibility study", "abstract": "This paper describes a rapid feasibility study of using GPT-4, a large\nlanguage model (LLM), to (semi)automate data extraction in systematic reviews.\nDespite the recent surge of interest in LLMs there is still a lack of\nunderstanding of how to design LLM-based automation tools and how to robustly\nevaluate their performance. During the 2023 Evidence Synthesis Hackathon we\nconducted two feasibility studies. Firstly, to automatically extract study\ncharacteristics from human clinical, animal, and social science domain studies.\nWe used two studies from each category for prompt-development; and ten for\nevaluation. Secondly, we used the LLM to predict Participants, Interventions,\nControls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP\ndataset. Overall, results indicated an accuracy of around 80%, with some\nvariability between domains (82% for human clinical, 80% for animal, and 72%\nfor studies of human social sciences). Causal inference methods and study\ndesign were the data extraction items with the most errors. In the PICO study,\nparticipants and intervention/control showed high accuracy (>80%), outcomes\nwere more challenging. Evaluation was done manually; scoring methods such as\nBLEU and ROUGE showed limited value. We observed variability in the LLMs\npredictions and changes in response quality. This paper presents a template for\nfuture evaluations of LLMs in the context of data extraction for systematic\nreview automation. Our results show that there might be value in using LLMs,\nfor example as second or third reviewers. However, caution is advised when\nintegrating models such as GPT-4 into tools. Further research on stability and\nreliability in practical settings is warranted for each type of data that is\nprocessed by the LLM.", "published": "2024-05-23 11:24:23", "link": "http://arxiv.org/abs/2405.14445v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Unchosen Experts Can Contribute Too: Unleashing MoE Models' Power by\n  Self-Contrast", "abstract": "Mixture-of-Experts (MoE) has emerged as a prominent architecture for scaling\nmodel size while maintaining computational efficiency. In MoE, each token in\nthe input sequence activates a different subset of experts determined by a\nrouting mechanism. However, the unchosen experts in MoE models do not\ncontribute to the output, potentially leading to underutilization of the\nmodel's capacity. In this work, we first conduct exploratory studies to\ndemonstrate that increasing the number of activated experts does not\nnecessarily improve and can even degrade the output quality. Then, we show that\noutput distributions from an MoE model using different routing strategies\nsubstantially differ, indicating that different experts do not always act\nsynergistically. Motivated by these findings, we propose Self-Contrast\nMixture-of-Experts (SCMoE), a training-free strategy that utilizes unchosen\nexperts in a self-contrast manner during inference. In SCMoE, the next-token\nprobabilities are determined by contrasting the outputs from strong and weak\nactivation using the same MoE model. Our method is conceptually simple and\ncomputationally lightweight, as it incurs minimal latency compared to greedy\ndecoding. Experiments on several benchmarks (GSM8K, StrategyQA, MBPP and\nHumanEval) demonstrate that SCMoE can consistently enhance Mixtral 8x7B's\nreasoning capability across various domains. For example, it improves the\naccuracy on GSM8K from 61.79 to 66.94. Moreover, combining SCMoE with\nself-consistency yields additional gains, increasing major@20 accuracy from\n75.59 to 78.31.", "published": "2024-05-23 12:45:29", "link": "http://arxiv.org/abs/2405.14507v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Synthetic Data Generation for Intersectional Fairness by Leveraging\n  Hierarchical Group Structure", "abstract": "In this paper, we introduce a data augmentation approach specifically\ntailored to enhance intersectional fairness in classification tasks. Our method\ncapitalizes on the hierarchical structure inherent to intersectionality, by\nviewing groups as intersections of their parent categories. This perspective\nallows us to augment data for smaller groups by learning a transformation\nfunction that combines data from these parent groups. Our empirical analysis,\nconducted on four diverse datasets including both text and images, reveals that\nclassifiers trained with this data augmentation approach achieve superior\nintersectional fairness and are more robust to ``leveling down'' when compared\nto methods optimizing traditional group fairness metrics.", "published": "2024-05-23 13:03:23", "link": "http://arxiv.org/abs/2405.14521v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Exploring Alignment in Shared Cross-lingual Spaces", "abstract": "Despite their remarkable ability to capture linguistic nuances across diverse\nlanguages, questions persist regarding the degree of alignment between\nlanguages in multilingual embeddings. Drawing inspiration from research on\nhigh-dimensional representations in neural language models, we employ\nclustering to uncover latent concepts within multilingual models. Our analysis\nfocuses on quantifying the \\textit{alignment} and \\textit{overlap} of these\nconcepts across various languages within the latent space. To this end, we\nintroduce two metrics \\CA{} and \\CO{} aimed at quantifying these aspects,\nenabling a deeper exploration of multilingual embeddings. Our study encompasses\nthree multilingual models (\\texttt{mT5}, \\texttt{mBERT}, and \\texttt{XLM-R})\nand three downstream tasks (Machine Translation, Named Entity Recognition, and\nSentiment Analysis). Key findings from our analysis include: i) deeper layers\nin the network demonstrate increased cross-lingual \\textit{alignment} due to\nthe presence of language-agnostic concepts, ii) fine-tuning of the models\nenhances \\textit{alignment} within the latent space, and iii) such\ntask-specific calibration helps in explaining the emergence of zero-shot\ncapabilities in the models.\\footnote{The code is available at\n\\url{https://github.com/baselmousi/multilingual-latent-concepts}}", "published": "2024-05-23 13:20:24", "link": "http://arxiv.org/abs/2405.14535v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Representation Noising: A Defence Mechanism Against Harmful Finetuning", "abstract": "Releasing open-source large language models (LLMs) presents a dual-use risk\nsince bad actors can easily fine-tune these models for harmful purposes. Even\nwithout the open release of weights, weight stealing and fine-tuning APIs make\nclosed models vulnerable to harmful fine-tuning attacks (HFAs). While safety\nmeasures like preventing jailbreaks and improving safety guardrails are\nimportant, such measures can easily be reversed through fine-tuning. In this\nwork, we propose Representation Noising (RepNoise), a defence mechanism that\noperates even when attackers have access to the weights. RepNoise works by\nremoving information about harmful representations such that it is difficult to\nrecover them during fine-tuning. Importantly, our defence is also able to\ngeneralize across different subsets of harm that have not been seen during the\ndefence process as long as they are drawn from the same distribution of the\nattack set. Our method does not degrade the general capability of LLMs and\nretains the ability to train the model on harmless tasks. We provide empirical\nevidence that the efficacy of our defence lies in its ``depth'': the degree to\nwhich information about harmful representations is removed across all layers of\nthe LLM. We also find areas where RepNoise still remains ineffective and\nhighlight how those limitations can inform future research.", "published": "2024-05-23 13:51:55", "link": "http://arxiv.org/abs/2405.14577v4", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Data Augmentation Techniques for Process Extraction from Scientific\n  Publications", "abstract": "We present data augmentation techniques for process extraction tasks in\nscientific publications. We cast the process extraction task as a sequence\nlabeling task where we identify all the entities in a sentence and label them\naccording to their process-specific roles. The proposed method attempts to\ncreate meaningful augmented sentences by utilizing (1) process-specific\ninformation from the original sentence, (2) role label similarity, and (3)\nsentence similarity. We demonstrate that the proposed methods substantially\nimprove the performance of the process extraction model trained on chemistry\ndomain datasets, up to 12.3 points improvement in performance accuracy\n(F-score). The proposed methods could potentially reduce overfitting as well,\nespecially when training on small datasets or in a low-resource setting such as\nin chemistry and other scientific domains.", "published": "2024-05-23 14:09:02", "link": "http://arxiv.org/abs/2405.14594v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "A FAIR and Free Prompt-based Research Assistant", "abstract": "This demo will present the Research Assistant (RA) tool developed to assist\nwith six main types of research tasks defined as standardized instruction\ntemplates, instantiated with user input, applied finally as prompts to\nwell-known--for their sophisticated natural language processing abilities--AI\ntools, such as ChatGPT (https://chat.openai.com/) and Gemini\n(https://gemini.google.com/app). The six research tasks addressed by RA are:\ncreating FAIR research comparisons, ideating research topics, drafting grant\napplications, writing scientific blogs, aiding preliminary peer reviews, and\nformulating enhanced literature search queries. RA's reliance on generative AI\ntools like ChatGPT or Gemini means the same research task assistance can be\noffered in any scientific discipline. We demonstrate its versatility by sharing\nRA outputs in Computer Science, Virology, and Climate Science, where the output\nwith the RA tool assistance mirrored that from a domain expert who performed\nthe same research task.", "published": "2024-05-23 14:16:46", "link": "http://arxiv.org/abs/2405.14601v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Medical Question Answering with Knowledge-Augmented Question\n  Generation", "abstract": "In the expanding field of language model applications, medical knowledge\nrepresentation remains a significant challenge due to the specialized nature of\nthe domain. Large language models, such as GPT-4, obtain reasonable scores on\nmedical question answering tasks, but smaller models are far behind. In this\nwork, we introduce a method to improve the proficiency of a small language\nmodel in the medical domain by employing a two-fold approach. We first\nfine-tune the model on a corpus of medical textbooks. Then, we use GPT-4 to\ngenerate questions similar to the downstream task, prompted with textbook\nknowledge, and use them to fine-tune the model. Additionally, we introduce\nECN-QA, a novel medical question answering dataset containing ``progressive\nquestions'' composed of related sequential questions. We show the benefits of\nour training strategy on this dataset. The study's findings highlight the\npotential of small language models in the medical domain when appropriately\nfine-tuned. The code and weights are available at\nhttps://github.com/raidium-med/MQG.", "published": "2024-05-23 14:53:52", "link": "http://arxiv.org/abs/2405.14654v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SimPO: Simple Preference Optimization with a Reference-Free Reward", "abstract": "Direct Preference Optimization (DPO) is a widely used offline preference\noptimization algorithm that reparameterizes reward functions in reinforcement\nlearning from human feedback (RLHF) to enhance simplicity and training\nstability. In this work, we propose SimPO, a simpler yet more effective\napproach. The effectiveness of SimPO is attributed to a key design: using the\naverage log probability of a sequence as the implicit reward. This reward\nformulation better aligns with model generation and eliminates the need for a\nreference model, making it more compute and memory efficient. Additionally, we\nintroduce a target reward margin to the Bradley-Terry objective to encourage a\nlarger margin between the winning and losing responses, further improving the\nalgorithm's performance. We compare SimPO to DPO and its latest variants across\nvarious state-of-the-art training setups, including both base and\ninstruction-tuned models such as Mistral, Llama 3, and Gemma 2. We evaluate on\nextensive chat-based evaluation benchmarks, including AlpacaEval 2, MT-Bench,\nand Arena-Hard. Our results demonstrate that SimPO consistently and\nsignificantly outperforms existing approaches without substantially increasing\nresponse length. Specifically, SimPO outperforms DPO by up to 6.4 points on\nAlpacaEval 2 and by up to 7.5 points on Arena-Hard. Our top-performing model,\nbuilt on Gemma-2-9B-it, achieves a 72.4% length-controlled win rate on\nAlpacaEval 2, a 59.1% win rate on Arena-Hard, and ranks 1st on Chatbot Arena\namong <10B models with real user votes.", "published": "2024-05-23 16:01:46", "link": "http://arxiv.org/abs/2405.14734v3", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluating Large Language Models for Public Health Classification and\n  Extraction Tasks", "abstract": "Advances in Large Language Models (LLMs) have led to significant interest in\ntheir potential to support human experts across a range of domains, including\npublic health. In this work we present automated evaluations of LLMs for public\nhealth tasks involving the classification and extraction of free text. We\ncombine six externally annotated datasets with seven new internally annotated\ndatasets to evaluate LLMs for processing text related to: health burden,\nepidemiological risk factors, and public health interventions. We evaluate\neleven open-weight LLMs (7-123 billion parameters) across all tasks using\nzero-shot in-context learning. We find that Llama-3.3-70B-Instruct is the\nhighest performing model, achieving the best results on 8/16 tasks (using\nmicro-F1 scores). We see significant variation across tasks with all\nopen-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as\nContact Classification, while all LLMs achieve greater than 80% micro-F1 on\nothers, such as GI Illness Classification. For a subset of 11 tasks, we also\nevaluate three GPT-4 and GPT-4o series models and find comparable results to\nLlama-3.3-70B-Instruct. Overall, based on these initial results we find\npromising signs that LLMs may be useful tools for public health experts to\nextract information from a wide variety of free text sources, and support\npublic health surveillance, research, and interventions.", "published": "2024-05-23 16:33:18", "link": "http://arxiv.org/abs/2405.14766v2", "categories": ["cs.CL", "cs.LG", "68T50"], "primary_category": "cs.CL"}
{"title": "Pragmatic Feature Preferences: Learning Reward-Relevant Preferences from\n  Human Input", "abstract": "Humans use social context to specify preferences over behaviors, i.e. their\nreward functions. Yet, algorithms for inferring reward models from preference\ndata do not take this social learning view into account. Inspired by pragmatic\nhuman communication, we study how to extract fine-grained data regarding why an\nexample is preferred that is useful for learning more accurate reward models.\nWe propose to enrich binary preference queries to ask both (1) which features\nof a given example are preferable in addition to (2) comparisons between\nexamples themselves. We derive an approach for learning from these\nfeature-level preferences, both for cases where users specify which features\nare reward-relevant, and when users do not. We evaluate our approach on linear\nbandit settings in both vision- and language-based domains. Results support the\nefficiency of our approach in quickly converging to accurate rewards with fewer\ncomparisons vs. example-only labels. Finally, we validate the real-world\napplicability with a behavioral experiment on a mushroom foraging task. Our\nfindings suggest that incorporating pragmatic feature preferences is a\npromising approach for more efficient user-aligned reward learning.", "published": "2024-05-23 16:36:16", "link": "http://arxiv.org/abs/2405.14769v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Smart Bilingual Focused Crawling of Parallel Documents", "abstract": "Crawling parallel texts $\\unicode{x2014}$texts that are mutual\ntranslations$\\unicode{x2014}$ from the Internet is usually done following a\nbrute-force approach: documents are massively downloaded in an unguided\nprocess, and only a fraction of them end up leading to actual parallel content.\nIn this work we propose a smart crawling method that guides the crawl towards\nfinding parallel content more rapidly. Our approach builds on two different\nmodels: one that infers the language of a document from its URL, and another\nthat infers whether a pair of URLs link to parallel documents. We evaluate both\nmodels in isolation and their integration into a crawling tool. The results\ndemonstrate the individual effectiveness of both models and highlight that\ntheir combination enables the early discovery of parallel content during\ncrawling, leading to a reduction in the amount of downloaded documents deemed\nuseless, and yielding a greater quantity of parallel documents compared to\nconventional crawling approaches.", "published": "2024-05-23 16:45:59", "link": "http://arxiv.org/abs/2405.14779v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language\n  Models", "abstract": "In order to thrive in hostile and ever-changing natural environments,\nmammalian brains evolved to store large amounts of knowledge about the world\nand continually integrate new information while avoiding catastrophic\nforgetting. Despite the impressive accomplishments, large language models\n(LLMs), even with retrieval-augmented generation (RAG), still struggle to\nefficiently and effectively integrate a large amount of new experiences after\npre-training. In this work, we introduce HippoRAG, a novel retrieval framework\ninspired by the hippocampal indexing theory of human long-term memory to enable\ndeeper and more efficient knowledge integration over new experiences. HippoRAG\nsynergistically orchestrates LLMs, knowledge graphs, and the Personalized\nPageRank algorithm to mimic the different roles of neocortex and hippocampus in\nhuman memory. We compare HippoRAG with existing RAG methods on multi-hop\nquestion answering and show that our method outperforms the state-of-the-art\nmethods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves\ncomparable or better performance than iterative retrieval like IRCoT while\nbeing 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into\nIRCoT brings further substantial gains. Finally, we show that our method can\ntackle new types of scenarios that are out of reach of existing methods. Code\nand data are available at https://github.com/OSU-NLP-Group/HippoRAG.", "published": "2024-05-23 17:47:55", "link": "http://arxiv.org/abs/2405.14831v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image\n  Analysis", "abstract": "While deep networks have achieved broad success in analyzing natural images,\nwhen applied to medical scans, they often fail in unexcepted situations. We\ninvestigate this challenge and focus on model sensitivity to domain shifts,\nsuch as data sampled from different hospitals or data confounded by demographic\nvariables such as sex, race, etc, in the context of chest X-rays and skin\nlesion images. A key finding we show empirically is that existing visual\nbackbones lack an appropriate prior from the architecture for reliable\ngeneralization in these settings. Taking inspiration from medical training, we\npropose giving deep networks a prior grounded in explicit medical knowledge\ncommunicated in natural language. To this end, we introduce Knowledge-enhanced\nBottlenecks (KnoBo), a class of concept bottleneck models that incorporates\nknowledge priors that constrain it to reason with clinically relevant factors\nfound in medical textbooks or PubMed. KnoBo uses retrieval-augmented language\nmodels to design an appropriate concept space paired with an automatic training\nprocedure for recognizing the concept. We evaluate different resources of\nknowledge and recognition architectures on a broad range of domain shifts\nacross 20 datasets. In our comprehensive evaluation with two imaging\nmodalities, KnoBo outperforms fine-tuned models on confounded datasets by 32.4%\non average. Finally, evaluations reveal that PubMed is a promising resource for\nmaking medical models less sensitive to domain shift, outperforming other\nresources on both diversity of information and final prediction performance.", "published": "2024-05-23 17:55:02", "link": "http://arxiv.org/abs/2405.14839v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large\n  Language Models", "abstract": "Large language models (LLMs) achieve remarkable performance in natural\nlanguage understanding but require substantial computation and memory\nresources. Post-training quantization (PTQ) is a powerful compression technique\nextensively investigated in LLMs. However, existing PTQ methods are still not\nideal in terms of accuracy and efficiency, especially with below 4 bit-widths.\nStandard PTQ methods using group-wise quantization suffer difficulties in\nquantizing LLMs accurately to such low-bit, but advanced methods remaining\nhigh-precision weights element-wisely are hard to realize their theoretical\nhardware efficiency. This paper presents a Salience-Driven Mixed-Precision\nQuantization scheme for LLMs, namely SliM-LLM. The scheme exploits the salience\ndistribution of weights to determine optimal bit-width and quantizers for\naccurate LLM quantization, while aligning bit-width partition to groups for\ncompact memory usage and fast integer inference. Specifically, the proposed\nSliM-LLM mainly relies on two novel techniques: (1) Salience-Determined Bit\nAllocation utilizes the clustering characteristics of salience distribution to\nallocate the bit-widths of each group, increasing the accuracy of quantized\nLLMs and maintaining the inference efficiency; (2) Salience-Weighted Quantizer\nCalibration optimizes the parameters of the quantizer by considering the\nelement-wise salience within the group, balancing the maintenance of salient\ninformation and minimization of errors. Comprehensive experiments show that\nSliM-LLM significantly improves the accuracy of LLMs at ultra-low bits, e.g.,\n2-bit LLaMA-7B achieves a 5.5-times memory-saving than original model on NVIDIA\nA800 GPUs, and 48% decrease of perplexity compared to the state-of-the-art\ngradient-free PTQ method. Moreover, SliM-LLM+, which is integrated from the\nextension of SliM-LLM with gradient-based quantizers, further reduces\nperplexity by 35.1%.", "published": "2024-05-23 16:21:48", "link": "http://arxiv.org/abs/2405.14917v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Linking In-context Learning in Transformers to Human Episodic Memory", "abstract": "Understanding connections between artificial and biological intelligent\nsystems can reveal fundamental principles of general intelligence. While many\nartificial intelligence models have a neuroscience counterpart, such\nconnections are largely missing in Transformer models and the self-attention\nmechanism. Here, we examine the relationship between interacting attention\nheads and human episodic memory. We focus on induction heads, which contribute\nto in-context learning in Transformer-based large language models (LLMs). We\ndemonstrate that induction heads are behaviorally, functionally, and\nmechanistically similar to the contextual maintenance and retrieval (CMR) model\nof human episodic memory. Our analyses of LLMs pre-trained on extensive text\ndata show that CMR-like heads often emerge in the intermediate and late layers,\nqualitatively mirroring human memory biases. The ablation of CMR-like heads\nsuggests their causal role in in-context learning. Our findings uncover a\nparallel between the computational mechanisms of LLMs and human memory,\noffering valuable insights into both research fields.", "published": "2024-05-23 18:51:47", "link": "http://arxiv.org/abs/2405.14992v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Extracting Prompts by Inverting LLM Outputs", "abstract": "We consider the problem of language model inversion: given outputs of a\nlanguage model, we seek to extract the prompt that generated these outputs. We\ndevelop a new black-box method, output2prompt, that learns to extract prompts\nwithout access to the model's logits and without adversarial or jailbreaking\nqueries. In contrast to previous work, output2prompt only needs outputs of\nnormal user queries. To improve memory efficiency, output2prompt employs a new\nsparse encoding techique. We measure the efficacy of output2prompt on a variety\nof user and system prompts and demonstrate zero-shot transferability across\ndifferent LLMs.", "published": "2024-05-23 19:35:03", "link": "http://arxiv.org/abs/2405.15012v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "OAC: Output-adaptive Calibration for Accurate Post-training Quantization", "abstract": "Deployment of Large Language Models (LLMs) has major computational costs, due\nto their rapidly expanding size. Compression of LLMs reduces the memory\nfootprint, latency, and energy required for their inference. Post-training\nQuantization (PTQ) techniques have been developed to compress LLMs while\navoiding expensive re-training. Most PTQ approaches formulate the quantization\nerror based on a layer-wise $\\ell_2$ loss, ignoring the model output. Then,\neach layer is calibrated using its layer-wise Hessian to update the weights\ntowards minimizing the $\\ell_2$ quantization error. The Hessian is also used\nfor detecting the most salient weights to quantization. Such PTQ approaches are\nprone to accuracy drop in low-precision quantization. We propose\nOutput-adaptive Calibration (OAC) to incorporate the model output in the\ncalibration process. We formulate the quantization error based on the\ndistortion of the output cross-entropy loss. OAC approximates the\noutput-adaptive Hessian for each layer under reasonable assumptions to reduce\nthe computational complexity. The output-adaptive Hessians are used to update\nthe weight matrices and detect the salient weights towards maintaining the\nmodel output. Our proposed method outperforms the state-of-the-art baselines\nsuch as SpQR and BiLLM, especially, at extreme low-precision (2-bit and binary)\nquantization.", "published": "2024-05-23 20:01:17", "link": "http://arxiv.org/abs/2405.15025v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "AGRaME: Any-Granularity Ranking with Multi-Vector Embeddings", "abstract": "Ranking is a fundamental and popular problem in search. However, existing\nranking algorithms usually restrict the granularity of ranking to full passages\nor require a specific dense index for each desired level of granularity. Such\nlack of flexibility in granularity negatively affects many applications that\ncan benefit from more granular ranking, such as sentence-level ranking for\nopen-domain question-answering, or proposition-level ranking for attribution.\nIn this work, we introduce the idea of any-granularity ranking, which leverages\nmulti-vector embeddings to rank at varying levels of granularity while\nmaintaining encoding at a single (coarser) level of granularity. We propose a\nmulti-granular contrastive loss for training multi-vector approaches, and\nvalidate its utility with both sentences and propositions as ranking units.\nFinally, we demonstrate the application of proposition-level ranking to\npost-hoc citation addition in retrieval-augmented generation, surpassing the\nperformance of prompt-driven citation generation.", "published": "2024-05-23 20:04:54", "link": "http://arxiv.org/abs/2405.15028v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CEEBERT: Cross-Domain Inference in Early Exit BERT", "abstract": "Pre-trained Language Models (PLMs), like BERT, with self-supervision\nobjectives exhibit remarkable performance and generalization across various\ntasks. However, they suffer in inference latency due to their large size. To\naddress this issue, side branches are attached at intermediate layers, enabling\nearly inference of samples without requiring them to pass through all layers.\nHowever, the challenge is to decide which layer to infer and exit each sample\nso that the accuracy and latency are balanced. Moreover, the distribution of\nthe samples to be inferred may differ from that used for training necessitating\ncross-domain adaptation. We propose an online learning algorithm named\nCross-Domain Inference in Early Exit BERT (CeeBERT) that dynamically determines\nearly exits of samples based on the level of confidence at each exit point.\nCeeBERT learns optimal thresholds from domain-specific confidence observed at\nintermediate layers on the fly, eliminating the need for labeled data.\nExperimental results on five distinct datasets with BERT and ALBERT models\ndemonstrate CeeBERT's ability to improve latency by reducing unnecessary\ncomputations with minimal drop in performance. By adapting to the threshold\nvalues, CeeBERT can speed up the BERT/ALBERT models by $2\\times$ - $3.5\\times$\nwith minimal drop in accuracy.", "published": "2024-05-23 20:36:10", "link": "http://arxiv.org/abs/2405.15039v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Dissociation of Faithful and Unfaithful Reasoning in LLMs", "abstract": "Large language models (LLMs) often improve their performance in downstream\ntasks when they generate Chain of Thought reasoning text before producing an\nanswer. We investigate how LLMs recover from errors in Chain of Thought.\nThrough analysis of error recovery behaviors, we find evidence for\nunfaithfulness in Chain of Thought, which occurs when models arrive at the\ncorrect answer despite invalid reasoning text. We identify factors that shift\nLLM recovery behavior: LLMs recover more frequently from obvious errors and in\ncontexts that provide more evidence for the correct answer. Critically, these\nfactors have divergent effects on faithful and unfaithful recoveries. Our\nresults indicate that there are distinct mechanisms driving faithful and\nunfaithful error recoveries. Selective targeting of these mechanisms may be\nable to drive down the rate of unfaithful reasoning and improve model\ninterpretability.", "published": "2024-05-23 22:38:58", "link": "http://arxiv.org/abs/2405.15092v2", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Contrastive and Consistency Learning for Neural Noisy-Channel Model in\n  Spoken Language Understanding", "abstract": "Recently, deep end-to-end learning has been studied for intent classification\nin Spoken Language Understanding (SLU). However, end-to-end models require a\nlarge amount of speech data with intent labels, and highly optimized models are\ngenerally sensitive to the inconsistency between the training and evaluation\nconditions. Therefore, a natural language understanding approach based on\nAutomatic Speech Recognition (ASR) remains attractive because it can utilize a\npre-trained general language model and adapt to the mismatch of the speech\ninput environment. Using this module-based approach, we improve a noisy-channel\nmodel to handle transcription inconsistencies caused by ASR errors. We propose\na two-stage method, Contrastive and Consistency Learning (CCL), that correlates\nerror patterns between clean and noisy ASR transcripts and emphasizes the\nconsistency of the latent features of the two transcripts. Experiments on four\nbenchmark datasets show that CCL outperforms existing methods and improves the\nASR robustness in various noisy environments. Code is available at\nhttps://github.com/syoung7388/CCL.", "published": "2024-05-23 23:10:23", "link": "http://arxiv.org/abs/2405.15097v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Large Language Models' Detection of Political Orientation in Newspapers", "abstract": "Democratic opinion-forming may be manipulated if newspapers' alignment to\npolitical or economical orientation is ambiguous. Various methods have been\ndeveloped to better understand newspapers' positioning. Recently, the advent of\nLarge Language Models (LLM), and particularly the pre-trained LLM chatbots like\nChatGPT or Gemini, hold disruptive potential to assist researchers and citizens\nalike. However, little is know on whether LLM assessment is trustworthy: do\nsingle LLM agrees with experts' assessment, and do different LLMs answer\nconsistently with one another? In this paper, we address specifically the\nsecond challenge. We compare how four widely employed LLMs rate the positioning\nof newspapers, and compare if their answers align with one another. We observe\nthat this is not the case. Over a woldwide dataset, articles in newspapers are\npositioned strikingly differently by single LLMs, hinting to inconsistent\ntraining or excessive randomness in the algorithms. We thus raise a warning\nwhen deciding which tools to use, and we call for better training and algorithm\ndevelopment, to cover such significant gap in a highly sensitive matter for\ndemocracy and societies worldwide. We also call for community engagement in\nbenchmark evaluation, through our open initiative navai.pro.", "published": "2024-05-23 06:18:03", "link": "http://arxiv.org/abs/2406.00018v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Harmful Speech Detection by Language Models Exhibits Gender-Queer\n  Dialect Bias", "abstract": "Content moderation on social media platforms shapes the dynamics of online\ndiscourse, influencing whose voices are amplified and whose are suppressed.\nRecent studies have raised concerns about the fairness of content moderation\npractices, particularly for aggressively flagging posts from transgender and\nnon-binary individuals as toxic. In this study, we investigate the presence of\nbias in harmful speech classification of gender-queer dialect online, focusing\nspecifically on the treatment of reclaimed slurs. We introduce a novel dataset,\nQueerReclaimLex, based on 109 curated templates exemplifying non-derogatory\nuses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators\nfor potential harm depending on additional context about speaker identity. We\nsystematically evaluate the performance of five off-the-shelf language models\nin assessing the harm of these texts and explore the effectiveness of\nchain-of-thought prompting to teach large language models (LLMs) to leverage\nauthor identity context. We reveal a tendency for these models to inaccurately\nflag texts authored by gender-queer individuals as harmful. Strikingly, across\nall LLMs the performance is poorest for texts that show signs of being written\nby individuals targeted by the featured slur (F1 <= 0.24). We highlight an\nurgent need for fairness and inclusivity in content moderation systems. By\nuncovering these biases, this work aims to inform the development of more\nequitable content moderation practices and contribute to the creation of\ninclusive online spaces for all users.", "published": "2024-05-23 18:07:28", "link": "http://arxiv.org/abs/2406.00020v2", "categories": ["cs.CL", "cs.CY"], "primary_category": "cs.CL"}
{"title": "$T^2$ of Thoughts: Temperature Tree Elicits Reasoning in Large Language\n  Models", "abstract": "Large Language Models (LLMs) have emerged as powerful tools in artificial\nintelligence, especially in complex decision-making scenarios, but their static\nproblem-solving strategies often limit their adaptability to dynamic\nenvironments. We explore the enhancement of reasoning capabilities in LLMs\nthrough Temperature Tree ($T^2$) prompting via a heuristic algorithm, termed as\n$T^2$ of Thoughts ($T^2oT$). The primary focus is on enhancing decision-making\nprocesses by dynamically adjusting search parameters, especially temperature,\nto improve accuracy without increasing computational demands. We empirically\nvalidate that our hybrid $T^2oT$ approach yields enhancements in,\nsingle-solution accuracy, multi-solution generation and text generation\nquality. Our findings suggest that while dynamic search depth adjustments based\non temperature can yield mixed results, a fixed search depth, when coupled with\nadaptive capabilities of $T^2oT$, provides a more reliable and versatile\nproblem-solving strategy. This work highlights the potential for future\nexplorations in optimizing algorithmic interactions with foundational language\nmodels, particularly illustrated by our development for the Game of 24 and\nCreative Writing tasks.", "published": "2024-05-23 00:40:43", "link": "http://arxiv.org/abs/2405.14075v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Survey on Vision-Language-Action Models for Embodied AI", "abstract": "Embodied AI is widely recognized as a key element of artificial general\nintelligence because it involves controlling embodied agents to perform tasks\nin the physical world. Building on the success of large language models and\nvision-language models, a new category of multimodal models -- referred to as\nvision-language-action models (VLAs) -- has emerged to address\nlanguage-conditioned robotic tasks in embodied AI by leveraging their distinct\nability to generate actions. In recent years, a myriad of VLAs have been\ndeveloped, making it imperative to capture the rapidly evolving landscape\nthrough a comprehensive survey. To this end, we present the first survey on\nVLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized\ninto three major lines of research. The first line focuses on individual\ncomponents of VLAs. The second line is dedicated to developing control policies\nadept at predicting low-level actions. The third line comprises high-level task\nplanners capable of decomposing long-horizon tasks into a sequence of subtasks,\nthereby guiding VLAs to follow more general user instructions. Furthermore, we\nprovide an extensive summary of relevant resources, including datasets,\nsimulators, and benchmarks. Finally, we discuss the challenges faced by VLAs\nand outline promising future directions in embodied AI. We have created a\nproject associated with this survey, which is available at\nhttps://github.com/yueen-ma/Awesome-VLA.", "published": "2024-05-23 01:43:54", "link": "http://arxiv.org/abs/2405.14093v4", "categories": ["cs.RO", "cs.CL", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Distributed Speculative Inference (DSI): Speculation Parallelism for\n  Provably Faster Lossless Language Model Inference", "abstract": "This paper introduces distributed speculative inference (DSI), a novel\ninference algorithm that is provably faster than speculative inference (SI)\n[leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard\nautoregressive inference (non-SI). Like other SI algorithms, DSI operates on\nfrozen language models (LMs), requiring no training or architectural\nmodifications, and it preserves the target distribution. Prior studies on SI\nhave demonstrated empirical speedups over non-SI--but rely on sufficiently fast\nand accurate drafters, which are often unavailable in practice. We identify a\ngap where SI can be slower than non-SI if drafters are too slow or inaccurate.\nWe close this gap by proving that DSI is faster than both SI and non-SI--given\nany drafters. DSI is therefore not only faster than SI, but also unlocks the\nacceleration of LMs for which SI fails. DSI leverages speculation parallelism\n(SP), a novel type of task parallelism, to orchestrate target and drafter\ninstances that overlap in time, establishing a new foundational tradeoff\nbetween computational resources and latency. Our simulations show that DSI is\n1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs\nand tasks. We open-source all our code.", "published": "2024-05-23 02:14:17", "link": "http://arxiv.org/abs/2405.14105v5", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.DC"}
{"title": "AlignGPT: Multi-modal Large Language Models with Adaptive Alignment\n  Capability", "abstract": "Multimodal Large Language Models (MLLMs) are widely regarded as crucial in\nthe exploration of Artificial General Intelligence (AGI). The core of MLLMs\nlies in their capability to achieve cross-modal alignment. To attain this goal,\ncurrent MLLMs typically follow a two-phase training paradigm: the pre-training\nphase and the instruction-tuning phase. Despite their success, there are\nshortcomings in the modeling of alignment capabilities within these models.\nFirstly, during the pre-training phase, the model usually assumes that all\nimage-text pairs are uniformly aligned, but in fact the degree of alignment\nbetween different image-text pairs is inconsistent. Secondly, the instructions\ncurrently used for finetuning incorporate a variety of tasks and different\ntasks usually require different levels of alignment capabilities, but previous\nMLLMs overlook these differentiated alignment needs. To tackle these issues, we\npropose a new multimodal large language model AlignGPT. In the pre-training\nstage, instead of treating all image-text pairs equally, we divide them into\ndifferent groups according to the degrees of alignment of them. Then, the model\nis trained to learn the representations of different alignment levels. In the\ninstruction-tuning phase, we adaptively combine these representations of\nalignment levels to meet the dynamic alignment needs of different tasks.\nExtensive experimental results show that our model achieves competitive\nperformance on 12 benchmarks.", "published": "2024-05-23 03:07:56", "link": "http://arxiv.org/abs/2405.14129v2", "categories": ["cs.CL", "cs.AI", "cs.CV"], "primary_category": "cs.CL"}
{"title": "ReactXT: Understanding Molecular \"Reaction-ship\" via\n  Reaction-Contextualized Molecule-Text Pretraining", "abstract": "Molecule-text modeling, which aims to facilitate molecule-relevant tasks with\na textual interface and textual knowledge, is an emerging research direction.\nBeyond single molecules, studying reaction-text modeling holds promise for\nhelping the synthesis of new materials and drugs. However, previous works\nmostly neglect reaction-text modeling: they primarily focus on modeling\nindividual molecule-text pairs or learning chemical reactions without texts in\ncontext. Additionally, one key task of reaction-text modeling -- experimental\nprocedure prediction -- is less explored due to the absence of an open-source\ndataset. The task is to predict step-by-step actions of conducting chemical\nexperiments and is crucial to automating chemical synthesis. To resolve the\nchallenges above, we propose a new pretraining method, ReactXT, for\nreaction-text modeling, and a new dataset, OpenExp, for experimental procedure\nprediction. Specifically, ReactXT features three types of input contexts to\nincrementally pretrain LMs. Each of the three input contexts corresponds to a\npretraining task to improve the text-based understanding of either reactions or\nsingle molecules. ReactXT demonstrates consistent improvements in experimental\nprocedure prediction and molecule captioning and offers competitive results in\nretrosynthesis. Our code is available at https://github.com/syr-cn/ReactXT.", "published": "2024-05-23 06:55:59", "link": "http://arxiv.org/abs/2405.14225v1", "categories": ["q-bio.QM", "cs.CL", "cs.MM"], "primary_category": "q-bio.QM"}
{"title": "Boosting Medical Image-based Cancer Detection via Text-guided\n  Supervision from Reports", "abstract": "The absence of adequately sufficient expert-level tumor annotations hinders\nthe effectiveness of supervised learning based opportunistic cancer screening\non medical imaging. Clinical reports (that are rich in descriptive textual\ndetails) can offer a \"free lunch'' supervision information and provide tumor\nlocation as a type of weak label to cope with screening tasks, thus saving\nhuman labeling workloads, if properly leveraged. However, predicting cancer\nonly using such weak labels can be very changeling since tumors are usually\npresented in small anatomical regions compared to the whole 3D medical scans.\nWeakly semi-supervised learning (WSSL) utilizes a limited set of voxel-level\ntumor annotations and incorporates alongside a substantial number of medical\nimages that have only off-the-shelf clinical reports, which may strike a good\nbalance between minimizing expert annotation workload and optimizing screening\nefficacy. In this paper, we propose a novel text-guided learning method to\nachieve highly accurate cancer detection results. Through integrating\ndiagnostic and tumor location text prompts into the text encoder of a\nvision-language model (VLM), optimization of weakly supervised learning can be\neffectively performed in the latent space of VLM, thereby enhancing the\nstability of training. Our approach can leverage clinical knowledge by\nlarge-scale pre-trained VLM to enhance generalization ability, and produce\nreliable pseudo tumor masks to improve cancer detection. Our extensive\nquantitative experimental results on a large-scale cancer dataset, including\n1,651 unique patients, validate that our approach can reduce human annotation\nefforts by at least 70% while maintaining comparable cancer detection accuracy\nto competing fully supervised methods (AUC value 0.961 versus 0.966).", "published": "2024-05-23 07:03:38", "link": "http://arxiv.org/abs/2405.14230v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Language processing in humans and computers", "abstract": "Machine-learned language models have transformed everyday life: they steer us\nwhen we study, drive, manage money. They have the potential to transform our\ncivilization. But they hallucinate. Their realities are virtual. This note\nprovides a high-level overview of language models and outlines a low-level\nmodel of learning machines. It turns out that, after they become capable of\nrecognizing hallucinations and dreaming safely, as humans tend to be, the\nlanguage-learning machines proceed to generate broader systems of false beliefs\nand self-confirming theories, as humans tend to do.", "published": "2024-05-23 07:08:57", "link": "http://arxiv.org/abs/2405.14233v1", "categories": ["cs.CL", "cs.LG", "cs.LO", "68T07, 68T50", "I.2.7; I.2.6; H.3.1; H.3.3"], "primary_category": "cs.CL"}
{"title": "Improving Gloss-free Sign Language Translation by Reducing\n  Representation Density", "abstract": "Gloss-free sign language translation (SLT) aims to develop well-performing\nSLT systems with no requirement for the costly gloss annotations, but currently\nstill lags behind gloss-based approaches significantly. In this paper, we\nidentify a representation density problem that could be a bottleneck in\nrestricting the performance of gloss-free SLT. Specifically, the representation\ndensity problem describes that the visual representations of semantically\ndistinct sign gestures tend to be closely packed together in feature space,\nwhich makes gloss-free methods struggle with distinguishing different sign\ngestures and suffer from a sharp performance drop. To address the\nrepresentation density problem, we introduce a simple but effective contrastive\nlearning strategy, namely SignCL, which encourages gloss-free models to learn\nmore discriminative feature representation in a self-supervised manner. Our\nexperiments demonstrate that the proposed SignCL can significantly reduce the\nrepresentation density and improve performance across various translation\nframeworks. Specifically, SignCL achieves a significant improvement in BLEU\nscore for the Sign Language Transformer and GFSLT-VLP on the CSL-Daily dataset\nby 39% and 46%, respectively, without any increase of model parameters.\nCompared to Sign2GPT, a state-of-the-art method based on large-scale\npre-trained vision and language models, SignCL achieves better performance with\nonly 35% of its parameters. Implementation and Checkpoints are available at\nhttps://github.com/JinhuiYE/SignCL.", "published": "2024-05-23 08:32:58", "link": "http://arxiv.org/abs/2405.14312v2", "categories": ["cs.CV", "cs.CL", "cs.MM"], "primary_category": "cs.CV"}
{"title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models", "abstract": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.", "published": "2024-05-23 09:43:52", "link": "http://arxiv.org/abs/2405.14366v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Evaluation of the Programming Skills of Large Language Models", "abstract": "The advent of Large Language Models (LLM) has revolutionized the efficiency\nand speed with which tasks are completed, marking a significant leap in\nproductivity through technological innovation. As these chatbots tackle\nincreasingly complex tasks, the challenge of assessing the quality of their\noutputs has become paramount. This paper critically examines the output quality\nof two leading LLMs, OpenAI's ChatGPT and Google's Gemini AI, by comparing the\nquality of programming code generated in both their free versions. Through the\nlens of a real-world example coupled with a systematic dataset, we investigate\nthe code quality produced by these LLMs. Given their notable proficiency in\ncode generation, this aspect of chatbot capability presents a particularly\ncompelling area for analysis. Furthermore, the complexity of programming code\noften escalates to levels where its verification becomes a formidable task,\nunderscoring the importance of our study. This research aims to shed light on\nthe efficacy and reliability of LLMs in generating high-quality programming\ncode, an endeavor that has significant implications for the field of software\ndevelopment and beyond.", "published": "2024-05-23 10:04:36", "link": "http://arxiv.org/abs/2405.14388v1", "categories": ["cs.SE", "cs.CL", "cs.CR"], "primary_category": "cs.SE"}
{"title": "Explainable Few-shot Knowledge Tracing", "abstract": "Knowledge tracing (KT), aiming to mine students' mastery of knowledge by\ntheir exercise records and predict their performance on future test questions,\nis a critical task in educational assessment. While researchers achieved\ntremendous success with the rapid development of deep learning techniques,\ncurrent knowledge tracing tasks fall into the cracks from real-world teaching\nscenarios. Relying heavily on extensive student data and solely predicting\nnumerical performances differs from the settings where teachers assess\nstudents' knowledge state from limited practices and provide explanatory\nfeedback. To fill this gap, we explore a new task formulation: Explainable\nFew-shot Knowledge Tracing. By leveraging the powerful reasoning and generation\nabilities of large language models (LLMs), we then propose a cognition-guided\nframework that can track the student knowledge from a few student records while\nproviding natural language explanations. Experimental results from three widely\nused datasets show that LLMs can perform comparable or superior to competitive\ndeep knowledge tracing methods. We also discuss potential directions and call\nfor future improvements in relevant topics.", "published": "2024-05-23 10:07:21", "link": "http://arxiv.org/abs/2405.14391v2", "categories": ["cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.AI"}
{"title": "RaFe: Ranking Feedback Improves Query Rewriting for RAG", "abstract": "As Large Language Models (LLMs) and Retrieval Augmentation Generation (RAG)\ntechniques have evolved, query rewriting has been widely incorporated into the\nRAG system for downstream tasks like open-domain QA. Many works have attempted\nto utilize small models with reinforcement learning rather than costly LLMs to\nimprove query rewriting. However, current methods require annotations (e.g.,\nlabeled relevant documents or downstream answers) or predesigned rewards for\nfeedback, which lack generalization, and fail to utilize signals tailored for\nquery rewriting. In this paper, we propose ours, a framework for training query\nrewriting models free of annotations. By leveraging a publicly available\nreranker, ours~provides feedback aligned well with the rewriting objectives.\nExperimental results demonstrate that ours~can obtain better performance than\nbaselines.", "published": "2024-05-23 11:00:19", "link": "http://arxiv.org/abs/2405.14431v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Worldwide Federated Training of Language Models", "abstract": "The reliance of language model training on massive amounts of computation and\nvast datasets scraped from potentially low-quality, copyrighted, or sensitive\ndata has come into question practically, legally, and ethically. Federated\nlearning provides a plausible alternative by enabling previously untapped data\nto be voluntarily gathered from collaborating organizations. However, when\nscaled globally, federated learning requires collaboration across heterogeneous\nlegal, security, and privacy regimes while accounting for the inherent locality\nof language data; this further exacerbates the established challenge of\nfederated statistical heterogeneity. We propose a Worldwide Federated Language\nModel Training~(WorldLM) system based on federations of federations, where each\nfederation has the autonomy to account for factors such as its industry,\noperating jurisdiction, or competitive environment. WorldLM enables such\nautonomy in the presence of statistical heterogeneity via partial model\nlocalization by allowing sub-federations to attentively aggregate key layers\nfrom their constituents. Furthermore, it can adaptively share information\nacross federations via residual layer embeddings. Evaluations of language\nmodeling on naturally heterogeneous datasets show that WorldLM outperforms\nstandard federations by up to $1.91\\times$, approaches the personalized\nperformance of fully local models, and maintains these advantages under\nprivacy-enhancing techniques.", "published": "2024-05-23 11:25:19", "link": "http://arxiv.org/abs/2405.14446v2", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "I.2.7"], "primary_category": "cs.LG"}
{"title": "Explainable automatic industrial carbon footprint estimation from bank\n  transaction classification using natural language processing", "abstract": "Concerns about the effect of greenhouse gases have motivated the development\nof certification protocols to quantify the industrial carbon footprint (CF).\nThese protocols are manual, work-intensive, and expensive. All of the above\nhave led to a shift towards automatic data-driven approaches to estimate the\nCF, including Machine Learning (ML) solutions. Unfortunately, the\ndecision-making processes involved in these solutions lack transparency from\nthe end user's point of view, who must blindly trust their outcomes compared to\nintelligible traditional manual approaches. In this research, manual and\nautomatic methodologies for CF estimation were reviewed, taking into account\ntheir transparency limitations. This analysis led to the proposal of a new\nexplainable ML solution for automatic CF calculations through bank transaction\nclassification. Consideration should be given to the fact that no previous\nresearch has considered the explainability of bank transaction classification\nfor this purpose. For classification, different ML models have been employed\nbased on their promising performance in the literature, such as Support Vector\nMachine, Random Forest, and Recursive Neural Networks. The results obtained\nwere in the 90 % range for accuracy, precision, and recall evaluation metrics.\nFrom their decision paths, the proposed solution estimates the CO2 emissions\nassociated with bank transactions. The explainability methodology is based on\nan agnostic evaluation of the influence of the input terms extracted from the\ndescriptions of transactions using locally interpretable models. The\nexplainability terms were automatically validated using a similarity metric\nover the descriptions of the target categories. Conclusively, the explanation\nperformance is satisfactory in terms of the proximity of the explanations to\nthe associated activity sector descriptions.", "published": "2024-05-23 12:43:06", "link": "http://arxiv.org/abs/2405.14505v1", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating\n  Representative and Affinity Bias in Large Language Models", "abstract": "Research on Large Language Models (LLMs) has often neglected subtle biases\nthat, although less apparent, can significantly influence the models' outputs\ntoward particular social narratives. This study addresses two such biases\nwithin LLMs: representative bias, which denotes a tendency of LLMs to generate\noutputs that mirror the experiences of certain identity groups, and affinity\nbias, reflecting the models' evaluative preferences for specific narratives or\nviewpoints. We introduce two novel metrics to measure these biases: the\nRepresentative Bias Score (RBS) and the Affinity Bias Score (ABS), and present\nthe Creativity-Oriented Generation Suite (CoGS), a collection of open-ended\ntasks such as short story writing and poetry composition, designed with\ncustomized rubrics to detect these subtle biases. Our analysis uncovers marked\nrepresentative biases in prominent LLMs, with a preference for identities\nassociated with being white, straight, and men. Furthermore, our investigation\nof affinity bias reveals distinctive evaluative patterns within each model,\nakin to `bias fingerprints'. This trend is also seen in human evaluators,\nhighlighting a complex interplay between human and machine bias perceptions.", "published": "2024-05-23 13:35:34", "link": "http://arxiv.org/abs/2405.14555v4", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Calibrated Self-Rewarding Vision Language Models", "abstract": "Large Vision-Language Models (LVLMs) have made substantial progress by\nintegrating pre-trained large language models (LLMs) and vision models through\ninstruction tuning. Despite these advancements, LVLMs often exhibit the\nhallucination phenomenon, where generated text responses appear linguistically\nplausible but contradict the input image, indicating a misalignment between\nimage and text pairs. This misalignment arises because the model tends to\nprioritize textual information over visual input, even when both the language\nmodel and visual representations are of high quality. Existing methods leverage\nadditional models or human annotations to curate preference data and enhance\nmodality alignment through preference optimization. These approaches may not\neffectively reflect the target LVLM's preferences, making the curated\npreferences easily distinguishable. Our work addresses these challenges by\nproposing the Calibrated Self-Rewarding (CSR) approach, which enables the model\nto self-improve by iteratively generating candidate responses, evaluating the\nreward for each response, and curating preference data for fine-tuning. In the\nreward modeling, we employ a step-wise strategy and incorporate visual\nconstraints into the self-rewarding process to place greater emphasis on visual\ninput. Empirical results demonstrate that CSR enhances performance and reduces\nhallucinations across ten benchmarks and tasks, achieving substantial\nimprovements over existing methods by 7.62%. Our empirical results are further\nsupported by rigorous theoretical analysis, under mild assumptions, verifying\nthe effectiveness of introducing visual constraints into the self-rewarding\nparadigm. Additionally, CSR shows compatibility with different vision-language\nmodels and the ability to incrementally improve performance through iterative\nfine-tuning. Our data and code are available at\nhttps://github.com/YiyangZhou/CSR.", "published": "2024-05-23 14:30:33", "link": "http://arxiv.org/abs/2405.14622v4", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Implicit In-context Learning", "abstract": "In-context Learning (ICL) empowers large language models (LLMs) to swiftly\nadapt to unseen tasks at inference-time by prefixing a few demonstration\nexamples before queries. Despite its versatility, ICL incurs substantial\ncomputational and memory overheads compared to zero-shot learning and is\nsensitive to the selection and order of demonstration examples. In this work,\nwe introduce Implicit In-context Learning (I2CL), an innovative paradigm that\nreduces the inference cost of ICL to that of zero-shot learning with minimal\ninformation loss. I2CL operates by first generating a condensed vector\nrepresentation, namely a context vector, extracted from the demonstration\nexamples. It then conducts an inference-time intervention through injecting a\nlinear combination of the context vector and query activations back into the\nmodel's residual streams. Empirical evaluation on nine real-world tasks across\nthree model architectures demonstrates that I2CL achieves few-shot level\nperformance at zero-shot inference cost, and it exhibits robustness against\nvariations in demonstration examples. Furthermore, I2CL facilitates a novel\nrepresentation of task-ids, enhancing task similarity detection and fostering\neffective transfer learning. We also perform a comprehensive analysis and\nablation study on I2CL, offering deeper insights into its internal mechanisms.\nCode is available at https://github.com/LzVv123456/I2CL.", "published": "2024-05-23 14:57:52", "link": "http://arxiv.org/abs/2405.14660v2", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "A Declarative System for Optimizing AI Workloads", "abstract": "A long-standing goal of data management systems has been to build systems\nwhich can compute quantitative insights over large corpora of unstructured data\nin a cost-effective manner. Until recently, it was difficult and expensive to\nextract facts from company documents, data from scientific papers, or metrics\nfrom image and video corpora. Today's models can accomplish these tasks with\nhigh accuracy. However, a programmer who wants to answer a substantive\nAI-powered query must orchestrate large numbers of models, prompts, and data\noperations. For even a single query, the programmer has to make a vast number\nof decisions such as the choice of model, the right inference method, the most\ncost-effective inference hardware, the ideal prompt design, and so on. The\noptimal set of decisions can change as the query changes and as the\nrapidly-evolving technical landscape shifts. In this paper we present\nPalimpzest, a system that enables anyone to process AI-powered analytical\nqueries simply by defining them in a declarative language. The system uses its\ncost optimization framework to implement the query plan with the best\ntrade-offs between runtime, financial cost, and output data quality. We\ndescribe the workload of AI-powered analytics tasks, the optimization methods\nthat Palimpzest uses, and the prototype system itself. We evaluate Palimpzest\non tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching.\nWe show that even our simple prototype offers a range of appealing plans,\nincluding one that is 3.3x faster and 2.9x cheaper than the baseline method,\nwhile also offering better data quality. With parallelism enabled, Palimpzest\ncan produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a\nsingle-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the\nbaseline. These require no additional work by the user.", "published": "2024-05-23 15:31:18", "link": "http://arxiv.org/abs/2405.14696v2", "categories": ["cs.CL", "cs.AI", "cs.DB", "H.2.3; I.2.5"], "primary_category": "cs.CL"}
{"title": "From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by\n  Step", "abstract": "When leveraging language models for reasoning tasks, generating explicit\nchain-of-thought (CoT) steps often proves essential for achieving high accuracy\nin final outputs. In this paper, we investigate if models can be taught to\ninternalize these CoT steps. To this end, we propose a simple yet effective\nmethod for internalizing CoT steps: starting with a model trained for explicit\nCoT reasoning, we gradually remove the intermediate steps and finetune the\nmodel. This process allows the model to internalize the intermediate reasoning\nsteps, thus simplifying the reasoning process while maintaining high\nperformance. Our approach enables a GPT-2 Small model to solve 9-by-9\nmultiplication with up to 99% accuracy, whereas standard training cannot solve\nbeyond 4-by-4 multiplication. Furthermore, our method proves effective on\nlarger language models, such as Mistral 7B, achieving over 50% accuracy on\nGSM8K without producing any intermediate steps.", "published": "2024-05-23 17:54:14", "link": "http://arxiv.org/abs/2405.14838v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Nurse is Blue and Elephant is Rugby: Cross Domain Alignment in Large\n  Language Models Reveal Human-like Patterns", "abstract": "Cross-domain alignment refers to the task of mapping a concept from one\ndomain to another. For example, ``If a \\textit{doctor} were a \\textit{color},\nwhat color would it be?''. This seemingly peculiar task is designed to\ninvestigate how people represent concrete and abstract concepts through their\nmappings between categories and their reasoning processes over those mappings.\nIn this paper, we adapt this task from cognitive science to evaluate the\nconceptualization and reasoning abilities of large language models (LLMs)\nthrough a behavioral study. We examine several LLMs by prompting them with a\ncross-domain mapping task and analyzing their responses at both the population\nand individual levels. Additionally, we assess the models' ability to reason\nabout their predictions by analyzing and categorizing their explanations for\nthese mappings. The results reveal several similarities between humans' and\nmodels' mappings and explanations, suggesting that models represent concepts\nsimilarly to humans. This similarity is evident not only in the model\nrepresentation but also in their behavior. Furthermore, the models mostly\nprovide valid explanations and deploy reasoning paths that are similar to those\nof humans.", "published": "2024-05-23 17:59:26", "link": "http://arxiv.org/abs/2405.14863v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Structural Entities Extraction and Patient Indications Incorporation for\n  Chest X-ray Report Generation", "abstract": "The automated generation of imaging reports proves invaluable in alleviating\nthe workload of radiologists. A clinically applicable reports generation\nalgorithm should demonstrate its effectiveness in producing reports that\naccurately describe radiology findings and attend to patient-specific\nindications. In this paper, we introduce a novel method, \\textbf{S}tructural\n\\textbf{E}ntities extraction and patient indications \\textbf{I}ncorporation\n(SEI) for chest X-ray report generation. Specifically, we employ a structural\nentities extraction (SEE) approach to eliminate presentation-style vocabulary\nin reports and improve the quality of factual entity sequences. This reduces\nthe noise in the following cross-modal alignment module by aligning X-ray\nimages with factual entity sequences in reports, thereby enhancing the\nprecision of cross-modal alignment and further aiding the model in\ngradient-free retrieval of similar historical cases. Subsequently, we propose a\ncross-modal fusion network to integrate information from X-ray images, similar\nhistorical cases, and patient-specific indications. This process allows the\ntext decoder to attend to discriminative features of X-ray images, assimilate\nhistorical diagnostic information from similar cases, and understand the\nexamination intention of patients. This, in turn, assists in triggering the\ntext decoder to produce high-quality reports. Experiments conducted on\nMIMIC-CXR validate the superiority of SEI over state-of-the-art approaches on\nboth natural language generation and clinical efficacy metrics.", "published": "2024-05-23 01:29:47", "link": "http://arxiv.org/abs/2405.14905v1", "categories": ["eess.IV", "cs.AI", "cs.CL"], "primary_category": "eess.IV"}
{"title": "BiMix: A Bivariate Data Mixing Law for Language Model Pretraining", "abstract": "Large language models have demonstrated remarkable capabilities across\nvarious tasks, primarily attributed to the utilization of diversely sourced\ndata. However, the impact of pretraining data composition on model performance\nremains poorly understood. This paper introduces $\\textbf{BiMix}$, a novel\nbivariate data mixing law that models the joint scaling behavior of domain\nproportions and data volume in LLM pretraining. $\\textbf{BiMix}$ provides a\nsystematic framework for understanding and optimizing data mixtures across\ndiverse domains. Through extensive experiments on two large-scale datasets, we\ndemonstrate $\\textbf{BiMix}$'s high accuracy in loss extrapolation (mean\nrelative error < 0.2%) and its generalization to unseen mixtures (R${}^{2}$ >\n0.97). Optimization of domain proportions yields superior model performance\ncompared to existing methods. Furthermore, we establish entropy-based measures\nas efficient proxies for data mixing, offering a computationally lightweight\nstrategy. Our work contributes both theoretical insights into data mixing\ndynamics and practical tools for enhancing LLM training efficiency, paving the\nway for more effective scaling strategies in language model development.", "published": "2024-05-23 09:44:02", "link": "http://arxiv.org/abs/2405.14908v4", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "LOVA3: Learning to Visual Question Answering, Asking and Assessment", "abstract": "Question answering, asking, and assessment are three innate human traits\ncrucial for understanding the world and acquiring knowledge. By enhancing these\ncapabilities, humans can more effectively utilize data, leading to better\ncomprehension and learning outcomes. Current Multimodal Large Language Models\n(MLLMs) primarily focus on question answering, often neglecting the full\npotential of questioning and assessment skills. Inspired by the human learning\nmechanism, we introduce LOVA3, an innovative framework named \"Learning tO\nVisual question Answering, Asking and Assessment,\" designed to equip MLLMs with\nthese additional capabilities. Our approach involves the creation of two\nsupplementary training tasks GenQA and EvalQA, aiming at fostering the skills\nof asking and assessing questions in the context of images. To develop the\nquestioning ability, we compile a comprehensive set of multimodal foundational\ntasks. For assessment, we introduce a new benchmark called EvalQABench,\ncomprising 64,000 training samples (split evenly between positive and negative\nsamples) and 5,000 validation and testing samples. We posit that enhancing\nMLLMs with the capabilities to answer, ask, and assess questions will enhance\ntheir multimodal comprehension, ultimately improving overall performance. To\nvalidate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate\nthem on a range of multimodal datasets and benchmarks. Our results demonstrate\nconsistent performance gains, underscoring the critical role of these\nadditional tasks in fostering comprehensive intelligence in MLLMs. The code is\navailable at https://github.com/showlab/LOVA3.", "published": "2024-05-23 18:21:59", "link": "http://arxiv.org/abs/2405.14974v3", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "In-context Time Series Predictor", "abstract": "Recent Transformer-based large language models (LLMs) demonstrate in-context\nlearning ability to perform various functions based solely on the provided\ncontext, without updating model parameters. To fully utilize the in-context\ncapabilities in time series forecasting (TSF) problems, unlike previous\nTransformer-based or LLM-based time series forecasting methods, we reformulate\n\"time series forecasting tasks\" as input tokens by constructing a series of\n(lookback, future) pairs within the tokens. This method aligns more closely\nwith the inherent in-context mechanisms, and is more parameter-efficient\nwithout the need of using pre-trained LLM parameters. Furthermore, it addresses\nissues such as overfitting in existing Transformer-based TSF models,\nconsistently achieving better performance across full-data, few-shot, and\nzero-shot settings compared to previous architectures.", "published": "2024-05-23 18:37:00", "link": "http://arxiv.org/abs/2405.14982v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RE-Adapt: Reverse Engineered Adaptation of Large Language Models", "abstract": "We introduce RE-Adapt, an approach to fine-tuning large language models on\nnew domains without degrading any pre-existing instruction-tuning. We reverse\nengineer an adapter which isolates what an instruction-tuned model has learned\nbeyond its corresponding pretrained base model. Importantly, this requires no\nadditional data or training. We can then fine-tune the base model on a new\ndomain and readapt it to instruction following with the reverse engineered\nadapter. RE-Adapt and our low-rank variant LoRE-Adapt both outperform other\nmethods of fine-tuning, across multiple popular LLMs and datasets, even when\nthe models are used in conjunction with retrieval-augmented generation.", "published": "2024-05-23 19:23:40", "link": "http://arxiv.org/abs/2405.15007v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Reframing Spatial Reasoning Evaluation in Language Models: A Real-World\n  Simulation Benchmark for Qualitative Reasoning", "abstract": "Spatial reasoning plays a vital role in both human cognition and machine\nintelligence, prompting new research into language models' (LMs) capabilities\nin this regard. However, existing benchmarks reveal shortcomings in evaluating\nqualitative spatial reasoning (QSR). These benchmarks typically present\noversimplified scenarios or unclear natural language descriptions, hindering\neffective evaluation. We present a novel benchmark for assessing QSR in LMs,\nwhich is grounded in realistic 3D simulation data, offering a series of diverse\nroom layouts with various objects and their spatial relationships. This\napproach provides a more detailed and context-rich narrative for spatial\nreasoning evaluation, diverging from traditional, toy-task-oriented scenarios.\nOur benchmark encompasses a broad spectrum of qualitative spatial\nrelationships, including topological, directional, and distance relations.\nThese are presented with different viewing points, varied granularities, and\ndensity of relation constraints to mimic real-world complexities. A key\ncontribution is our logic-based consistency-checking tool, which enables the\nassessment of multiple plausible solutions, aligning with real-world scenarios\nwhere spatial relationships are often open to interpretation. Our benchmark\nevaluation of advanced LMs reveals their strengths and limitations in spatial\nreasoning. They face difficulties with multi-hop spatial reasoning and\ninterpreting a mix of different view descriptions, pointing to areas for future\nimprovement.", "published": "2024-05-23 21:22:00", "link": "http://arxiv.org/abs/2405.15064v1", "categories": ["cs.CL", "cs.AI", "cs.DB"], "primary_category": "cs.CL"}
{"title": "Eliciting Informative Text Evaluations with Large Language Models", "abstract": "Peer prediction mechanisms motivate high-quality feedback with provable\nguarantees. However, current methods only apply to rather simple reports, like\nmultiple-choice or scalar numbers. We aim to broaden these techniques to the\nlarger domain of text-based reports, drawing on the recent developments in\nlarge language models. This vastly increases the applicability of peer\nprediction mechanisms as textual feedback is the norm in a large variety of\nfeedback channels: peer reviews, e-commerce customer reviews, and comments on\nsocial media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM)\nand the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms\nutilize LLMs as predictors, mapping from one agent's report to a prediction of\nher peer's report. Theoretically, we show that when the LLM prediction is\nsufficiently accurate, our mechanisms can incentivize high effort and\ntruth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we\nconfirm the efficacy of our mechanisms through experiments conducted on two\nreal datasets: the Yelp review dataset and the ICLR OpenReview dataset. We\nhighlight the results that on the ICLR dataset, our mechanisms can\ndifferentiate three quality levels -- human-written reviews, GPT-4-generated\nreviews, and GPT-3.5-generated reviews in terms of expected scores.\nAdditionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.", "published": "2024-05-23 21:56:12", "link": "http://arxiv.org/abs/2405.15077v4", "categories": ["cs.CL", "cs.AI", "cs.GT"], "primary_category": "cs.CL"}
{"title": "Integrating Medical Imaging and Clinical Reports Using Multimodal Deep\n  Learning for Advanced Disease Analysis", "abstract": "In this paper, an innovative multi-modal deep learning model is proposed to\ndeeply integrate heterogeneous information from medical images and clinical\nreports. First, for medical images, convolutional neural networks were used to\nextract high-dimensional features and capture key visual information such as\nfocal details, texture and spatial distribution. Secondly, for clinical report\ntext, a two-way long and short-term memory network combined with an attention\nmechanism is used for deep semantic understanding, and key statements related\nto the disease are accurately captured. The two features interact and integrate\neffectively through the designed multi-modal fusion layer to realize the joint\nrepresentation learning of image and text. In the empirical study, we selected\na large medical image database covering a variety of diseases, combined with\ncorresponding clinical reports for model training and validation. The proposed\nmultimodal deep learning model demonstrated substantial superiority in the\nrealms of disease classification, lesion localization, and clinical description\ngeneration, as evidenced by the experimental results.", "published": "2024-05-23 02:22:10", "link": "http://arxiv.org/abs/2405.17459v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Small Language Models for Application Interactions: A Case Study", "abstract": "We study the efficacy of Small Language Models (SLMs) in facilitating\napplication usage through natural language interactions. Our focus here is on a\nparticular internal application used in Microsoft for cloud supply chain\nfulfilment. Our experiments show that small models can outperform much larger\nones in terms of both accuracy and running time, even when fine-tuned on small\ndatasets. Alongside these results, we also highlight SLM-based system design\nconsiderations.", "published": "2024-05-23 17:33:32", "link": "http://arxiv.org/abs/2405.20347v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "PTA: Enhancing Multimodal Sentiment Analysis through Pipelined\n  Prediction and Translation-based Alignment", "abstract": "Multimodal aspect-based sentiment analysis (MABSA) aims to understand\nopinions in a granular manner, advancing human-computer interaction and other\nfields. Traditionally, MABSA methods use a joint prediction approach to\nidentify aspects and sentiments simultaneously. However, we argue that joint\nmodels are not always superior. Our analysis shows that joint models struggle\nto align relevant text tokens with image patches, leading to misalignment and\nineffective image utilization.\n  In contrast, a pipeline framework first identifies aspects through MATE\n(Multimodal Aspect Term Extraction) and then aligns these aspects with image\npatches for sentiment classification (MASC: Multimodal Aspect-Oriented\nSentiment Classification). This method is better suited for multimodal\nscenarios where effective image use is crucial. We present three key\nobservations: (a) MATE and MASC have different feature requirements, with MATE\nfocusing on token-level features and MASC on sequence-level features; (b) the\naspect identified by MATE is crucial for effective image utilization; and (c)\nimages play a trivial role in previous MABSA methods due to high noise.\n  Based on these observations, we propose a pipeline framework that first\npredicts the aspect and then uses translation-based alignment (TBA) to enhance\nmultimodal semantic consistency for better image utilization. Our method\nachieves state-of-the-art (SOTA) performance on widely used MABSA datasets\nTwitter-15 and Twitter-17. This demonstrates the effectiveness of the pipeline\napproach and its potential to provide valuable insights for future MABSA\nresearch.\n  For reproducibility, the code and checkpoint will be released.", "published": "2024-05-23 01:16:45", "link": "http://arxiv.org/abs/2406.00017v2", "categories": ["cs.CL", "cs.AI", "cs.MM"], "primary_category": "cs.CL"}
{"title": "EHR-SeqSQL : A Sequential Text-to-SQL Dataset For Interactively\n  Exploring Electronic Health Records", "abstract": "In this paper, we introduce EHR-SeqSQL, a novel sequential text-to-SQL\ndataset for Electronic Health Record (EHR) databases. EHR-SeqSQL is designed to\naddress critical yet underexplored aspects in text-to-SQL parsing:\ninteractivity, compositionality, and efficiency. To the best of our knowledge,\nEHR-SeqSQL is not only the largest but also the first medical text-to-SQL\ndataset benchmark to include sequential and contextual questions. We provide a\ndata split and the new test set designed to assess compositional generalization\nability. Our experiments demonstrate the superiority of a multi-turn approach\nover a single-turn approach in learning compositionality. Additionally, our\ndataset integrates specially crafted tokens into SQL queries to improve\nexecution efficiency. With EHR-SeqSQL, we aim to bridge the gap between\npractical needs and academic research in the text-to-SQL domain. EHR-SeqSQL is\navailable at https://github.com/seonhee99/EHR-SeqSQL.", "published": "2024-05-23 07:14:21", "link": "http://arxiv.org/abs/2406.00019v3", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CrossVoice: Crosslingual Prosody Preserving Cascade-S2ST using Transfer\n  Learning", "abstract": "This paper presents CrossVoice, a novel cascade-based Speech-to-Speech\nTranslation (S2ST) system employing advanced ASR, MT, and TTS technologies with\ncross-lingual prosody preservation through transfer learning. We conducted\ncomprehensive experiments comparing CrossVoice with direct-S2ST systems,\nshowing improved BLEU scores on tasks such as Fisher Es-En, VoxPopuli Fr-En and\nprosody preservation on benchmark datasets CVSS-T and IndicTTS. With an average\nmean opinion score of 3.75 out of 4, speech synthesized by CrossVoice closely\nrivals human speech on the benchmark, highlighting the efficacy of\ncascade-based systems and transfer learning in multilingual S2ST with prosody\ntransfer.", "published": "2024-05-23 20:30:54", "link": "http://arxiv.org/abs/2406.00021v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Multilingual Prosody Transfer: Comparing Supervised & Transfer Learning", "abstract": "The field of prosody transfer in speech synthesis systems is rapidly\nadvancing. This research is focused on evaluating learning methods for adapting\npre-trained monolingual text-to-speech (TTS) models to multilingual conditions,\ni.e., Supervised Fine-Tuning (SFT) and Transfer Learning (TL). This comparison\nutilizes three distinct metrics: Mean Opinion Score (MOS), Recognition Accuracy\n(RA), and Mel Cepstral Distortion (MCD). Results demonstrate that, in\ncomparison to SFT, TL leads to significantly enhanced performance, with an\naverage MOS higher by 1.53 points, a 37.5% increase in RA, and approximately a\n7.8-point improvement in MCD. These findings are instrumental in helping build\nTTS models for low-resource languages.", "published": "2024-05-23 20:43:24", "link": "http://arxiv.org/abs/2406.00022v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "In Silico Sociology: Forecasting COVID-19 Polarization with Large\n  Language Models", "abstract": "By training deep neural networks on massive archives of digitized text, large\nlanguage models (LLMs) learn the complex linguistic patterns that constitute\nhistoric and contemporary discourses. We argue that LLMs can serve as a\nvaluable tool for sociological inquiry by enabling accurate simulation of\nrespondents from specific social and cultural contexts. Applying LLMs in this\ncapacity, we reconstruct the public opinion landscape of 2019 to examine the\nextent to which the future polarization over COVID-19 was prefigured in\nexisting political discourse. Using an LLM trained on texts published through\n2019, we simulate the responses of American liberals and conservatives to a\nbattery of pandemic-related questions. We find that the simulated respondents\nreproduce observed partisan differences in COVID-19 attitudes in 84% of cases,\nsignificantly greater than chance. Prompting the simulated respondents to\njustify their responses, we find that much of the observed partisan gap\ncorresponds to differing appeals to freedom, safety, and institutional trust.\nOur findings suggest that the politicization of COVID-19 was largely consistent\nwith the prior ideological landscape, and this unprecedented event served to\nadvance history along its track rather than change the rails.", "published": "2024-05-23 22:10:12", "link": "http://arxiv.org/abs/2407.11190v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech\n  Foundation Models", "abstract": "We propose an unsupervised adaptation framework, Self-TAught Recognizer\n(STAR), which leverages unlabeled data to enhance the robustness of automatic\nspeech recognition (ASR) systems in diverse target domains, such as noise and\naccents. STAR is developed for prevalent speech foundation models based on\nTransformer-related architecture with auto-regressive decoding (e.g., Whisper,\nCanary). Specifically, we propose a novel indicator that empirically integrates\nstep-wise information during decoding to assess the token-level quality of\npseudo labels without ground truth, thereby guiding model updates for effective\nunsupervised adaptation. Experimental results show that STAR achieves an\naverage of 13.5% relative reduction in word error rate across 14 target\ndomains, and it sometimes even approaches the upper-bound performance of\nsupervised adaptation. Surprisingly, we also observe that STAR prevents the\nadapted model from the common catastrophic forgetting problem without recalling\nsource-domain data. Furthermore, STAR exhibits high data efficiency that only\nrequires less than one-hour unlabeled data, and seamless generality to\nalternative large speech models and speech translation tasks. Our code aims to\nopen source to the research communities.", "published": "2024-05-23 04:27:11", "link": "http://arxiv.org/abs/2405.14161v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Agent Planning with World Knowledge Model", "abstract": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.", "published": "2024-05-23 06:03:19", "link": "http://arxiv.org/abs/2405.14205v4", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "primary_category": "cs.CL"}
{"title": "Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration", "abstract": "Grounding the reasoning ability of large language models (LLMs) for embodied\ntasks is challenging due to the complexity of the physical world. Especially,\nLLM planning for multi-agent collaboration requires communication of agents or\ncredit assignment as the feedback to re-adjust the proposed plans and achieve\neffective coordination. However, existing methods that overly rely on physical\nverification or self-reflection suffer from excessive and inefficient querying\nof LLMs. In this paper, we propose a novel framework for multi-agent\ncollaboration that introduces Reinforced Advantage feedback (ReAd) for\nefficient self-refinement of plans. Specifically, we perform critic regression\nto learn a sequential advantage function from LLM-planned data, and then treat\nthe LLM planner as an optimizer to generate actions that maximize the advantage\nfunction. It endows the LLM with the foresight to discern whether the action\ncontributes to accomplishing the final task. We provide theoretical analysis by\nextending advantage-weighted regression in reinforcement learning to\nmulti-agent systems. Experiments on Overcooked-AI and a difficult variant of\nRoCoBench show that ReAd surpasses baselines in success rate, and also\nsignificantly decreases the interaction steps of agents and query rounds of\nLLMs, demonstrating its high efficiency for grounding LLMs. More results are\ngiven at https://read-llm.github.io/.", "published": "2024-05-23 08:33:19", "link": "http://arxiv.org/abs/2405.14314v2", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Explaining Black-box Model Predictions via Two-level Nested Feature\n  Attributions with Consistency Property", "abstract": "Techniques that explain the predictions of black-box machine learning models\nare crucial to make the models transparent, thereby increasing trust in AI\nsystems. The input features to the models often have a nested structure that\nconsists of high- and low-level features, and each high-level feature is\ndecomposed into multiple low-level features. For such inputs, both high-level\nfeature attributions (HiFAs) and low-level feature attributions (LoFAs) are\nimportant for better understanding the model's decision. In this paper, we\npropose a model-agnostic local explanation method that effectively exploits the\nnested structure of the input to estimate the two-level feature attributions\nsimultaneously. A key idea of the proposed method is to introduce the\nconsistency property that should exist between the HiFAs and LoFAs, thereby\nbridging the separate optimization problems for estimating them. Thanks to this\nconsistency property, the proposed method can produce HiFAs and LoFAs that are\nboth faithful to the black-box models and consistent with each other, using a\nsmaller number of queries to the models. In experiments on image classification\nin multiple instance learning and text classification using language models, we\ndemonstrate that the HiFAs and LoFAs estimated by the proposed method are\naccurate, faithful to the behaviors of the black-box models, and provide\nconsistent explanations.", "published": "2024-05-23 13:03:26", "link": "http://arxiv.org/abs/2405.14522v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "stat.ML"], "primary_category": "cs.LG"}
{"title": "WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of\n  Large Language Models", "abstract": "Large language models (LLMs) need knowledge updates to meet the ever-growing\nworld facts and correct the hallucinated responses, facilitating the methods of\nlifelong model editing. Where the updated knowledge resides in memories is a\nfundamental question for model editing. In this paper, we find that editing\neither long-term memory (direct model parameters) or working memory\n(non-parametric knowledge of neural network activations/representations by\nretrieval) will result in an impossible triangle -- reliability,\ngeneralization, and locality can not be realized together in the lifelong\nediting settings. For long-term memory, directly editing the parameters will\ncause conflicts with irrelevant pretrained knowledge or previous edits (poor\nreliability and locality). For working memory, retrieval-based activations can\nhardly make the model understand the edits and generalize (poor\ngeneralization). Therefore, we propose WISE to bridge the gap between memories.\nIn WISE, we design a dual parametric memory scheme, which consists of the main\nmemory for the pretrained knowledge and a side memory for the edited knowledge.\nWe only edit the knowledge in the side memory and train a router to decide\nwhich memory to go through when given a query. For continual editing, we devise\na knowledge-sharding mechanism where different sets of edits reside in distinct\nsubspaces of parameters, and are subsequently merged into a shared memory\nwithout conflicts. Extensive experiments show that WISE can outperform previous\nmodel editing methods and overcome the impossible triangle under lifelong model\nediting of question answering, hallucination, and out-of-distribution settings\nacross trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is\navailable at https://github.com/zjunlp/EasyEdit.", "published": "2024-05-23 16:35:52", "link": "http://arxiv.org/abs/2405.14768v3", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Implicit Personalization in Language Models: A Systematic Study", "abstract": "Implicit Personalization (IP) is a phenomenon of language models inferring a\nuser's background from the implicit cues in the input prompts and tailoring the\nresponse based on this inference. While previous work has touched upon various\ninstances of this problem, there lacks a unified framework to study this\nbehavior. This work systematically studies IP through a rigorous mathematical\nformulation, a multi-perspective moral reasoning framework, and a set of case\nstudies. Our theoretical foundation for IP relies on a structural causal model\nand introduces a novel method, indirect intervention, to estimate the causal\neffect of a mediator variable that cannot be directly intervened upon. Beyond\nthe technical approach, we also introduce a set of moral reasoning principles\nbased on three schools of moral philosophy to study when IP may or may not be\nethically appropriate. Equipped with both mathematical and ethical insights, we\npresent three diverse case studies illustrating the varied nature of the IP\nproblem and offer recommendations for future research. Our code is at\nhttps://github.com/jiarui-liu/IP, and our data is at\nhttps://huggingface.co/datasets/Jerry999/ImplicitPersonalizationData.", "published": "2024-05-23 17:18:46", "link": "http://arxiv.org/abs/2405.14808v2", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Real-Time and Accurate: Zero-shot High-Fidelity Singing Voice Conversion\n  with Multi-Condition Flow Synthesis", "abstract": "Singing voice conversion is to convert the source singing voice into the\ntarget singing voice except for the content. Currently, flow-based models can\ncomplete the task of voice conversion, but they struggle to effectively extract\nlatent variables in the more rhythmically rich and emotionally expressive task\nof singing voice conversion, while also facing issues with low efficiency in\nspeech processing. In this paper, we propose a high-fidelity flow-based model\nbased on multi-decoupling feature constraints called RASVC, which enhances the\ncapture of vocal details by integrating multiple latent attribute encoders. We\nalso use Multi-stream inverse short-time Fourier transform(MS-iSTFT) to enhance\nthe speed of speech processing by skipping some complicated decoder processing\nsteps. We compare the synthesized singing voice with other models from multiple\ndimensions, and our proposed model is highly consistent with the current\nstate-of-the-art, with the demo which is available at\n\\url{https://lazycat1119.github.io/RASVC-demo/}.", "published": "2024-05-23 22:51:04", "link": "http://arxiv.org/abs/2405.15093v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Frequency-Domain Sound Field from the Perspective of Band-Limited\n  Functions", "abstract": "In this paper, the frequency-domain sound field is regarded as an element of\nsome band-limited function space, and a representation of the field as a linear\ncombination of the reproducing kernel in that space is proposed. This model has\nthe strongest representational capacity of all function systems when we know\nonly the sound pressure information at arbitrary positions. The proposed model\ncan be considered a generalization of the existing three-dimensional sound\nfield model using the reproducing kernel of the solution space of the Helmholtz\nequation to the spatial dimension. One of the advantages of capturing the\nfrequency-domain sound field in this way is the simplicity achieved for the\nestimation formula of the wavenumber spectrum. Two numerical simulations were\nconducted to validate the proposed methods.", "published": "2024-05-23 08:07:41", "link": "http://arxiv.org/abs/2405.14290v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Leveraging Real Electric Guitar Tones and Effects to Improve Robustness\n  in Guitar Tablature Transcription Modeling", "abstract": "Guitar tablature transcription (GTT) aims at automatically generating\nsymbolic representations from real solo guitar performances. Due to its\napplications in education and musicology, GTT has gained traction in recent\nyears. However, GTT robustness has been limited due to the small size of\navailable datasets. Researchers have recently used synthetic data that\nsimulates guitar performances using pre-recorded or computer-generated tones\nand can be automatically generated at large scales. The present study\ncomplements these efforts by demonstrating that GTT robustness can be improved\nby including synthetic training data created using recordings of real guitar\ntones played with different audio effects. We evaluate our approach on a new\nevaluation dataset with professional solo guitar performances that we composed\nand collected, featuring a wide array of tones, chords, and scales.", "published": "2024-05-23 15:13:40", "link": "http://arxiv.org/abs/2405.14679v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "End-to-End User-Defined Keyword Spotting using Shifted Delta\n  Coefficients", "abstract": "Identifying user-defined keywords is crucial for personalizing interactions\nwith smart devices. Previous approaches of user-defined keyword spotting\n(UDKWS) have relied on short-term spectral features such as mel frequency\ncepstral coefficients (MFCC) to detect the spoken keyword. However, these\nfeatures may face challenges in accurately identifying closely related\npronunciation of audio-text pairs, due to their limited capability in capturing\nthe temporal dynamics of the speech signal. To address this challenge, we\npropose to use shifted delta coefficients (SDC) which help in capturing\npronunciation variability (transition between connecting phonemes) by\nincorporating long-term temporal information. The performance of the SDC\nfeature is compared with various baseline features across four different\ndatasets using a cross-attention based end-to-end system. Additionally, various\nconfigurations of SDC are explored to find the suitable temporal context for\nthe UDKWS task. The experimental results reveal that the SDC feature\noutperforms the MFCC baseline feature, exhibiting an improvement of 8.32% in\narea under the curve (AUC) and 8.69% in terms of equal error rate (EER) on the\nchallenging Libriphrase-hard dataset. Moreover, the proposed approach\ndemonstrated superior performance when compared to state-of-the-art UDKWS\ntechniques.", "published": "2024-05-23 12:24:01", "link": "http://arxiv.org/abs/2405.14489v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Acoustical Features as Knee Health Biomarkers: A Critical Analysis", "abstract": "Acoustical knee health assessment has long promised an alternative to\nclinically available medical imaging tools, but this modality has yet to be\nadopted in medical practice. The field is currently led by machine learning\nmodels processing acoustical features, which have presented promising\ndiagnostic performances. However, these methods overlook the intricate\nmulti-source nature of audio signals and the underlying mechanisms at play. By\naddressing this critical gap, the present paper introduces a novel causal\nframework for validating knee acoustical features. We argue that current\nmachine learning methodologies for acoustical knee diagnosis lack the required\nassurances and thus cannot be used to classify acoustic features as biomarkers.\nOur framework establishes a set of essential theoretical guarantees necessary\nto validate this claim. We apply our methodology to three real-world\nexperiments investigating the effect of researchers' expectations, the\nexperimental protocol and the wearable employed sensor. This investigation\nreveals latent issues such as underlying shortcut learning and performance\ninflation. This study is the first independent result reproduction study in the\nfield of acoustical knee health evaluation. We conclude with actionable\ninsights from our findings, offering valuable guidance to navigate these\ncrucial limitations in future research.", "published": "2024-05-23 22:14:50", "link": "http://arxiv.org/abs/2405.15085v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Music Genre Classification: Training an AI model", "abstract": "Music genre classification is an area that utilizes machine learning models\nand techniques for the processing of audio signals, in which applications range\nfrom content recommendation systems to music recommendation systems. In this\nresearch I explore various machine learning algorithms for the purpose of music\ngenre classification, using features extracted from audio signals.The systems\nare namely, a Multilayer Perceptron (built from scratch), a k-Nearest\nNeighbours (also built from scratch), a Convolutional Neural Network and lastly\na Random Forest wide model. In order to process the audio signals, feature\nextraction methods such as Short-Time Fourier Transform, and the extraction of\nMel Cepstral Coefficients (MFCCs), is performed. Through this extensive\nresearch, I aim to asses the robustness of machine learning models for genre\nclassification, and to compare their results.", "published": "2024-05-23 23:07:01", "link": "http://arxiv.org/abs/2405.15096v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "The Rarity of Musical Audio Signals Within the Space of Possible Audio\n  Generation", "abstract": "A white noise signal can access any possible configuration of values, though\nstatistically over many samples tends to a uniform spectral distribution, and\nis highly unlikely to produce intelligible sound. But how unlikely? The\nprobability that white noise generates a music-like signal over different\ndurations is analyzed, based on some necessary features observed in real music\naudio signals such as mostly proximate movement and zero crossing rate. Given\nthe mathematical results, the rarity of music as a signal is considered\noverall. The applicability of this study is not just to show that music has a\nprecious rarity value, but that examination of the size of music relative to\nthe overall size of audio signal space provides information to inform new\ngenerations of algorithmic music system (which are now often founded on audio\nsignal generation directly, and may relate to white noise via such machine\nlearning processes as diffusion). Estimated upper bounds on the rarity of music\nto the size of various physical and musical spaces are compared, to better\nunderstand the magnitude of the results (pun intended). Underlying the research\nare the questions `how much music is still out there?' and `how much music\ncould a machine learning process actually reach?'.", "published": "2024-05-23 23:25:46", "link": "http://arxiv.org/abs/2405.15103v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Visual Echoes: A Simple Unified Transformer for Audio-Visual Generation", "abstract": "In recent years, with the realistic generation results and a wide range of\npersonalized applications, diffusion-based generative models gain huge\nattention in both visual and audio generation areas. Compared to the\nconsiderable advancements of text2image or text2audio generation, research in\naudio2visual or visual2audio generation has been relatively slow. The recent\naudio-visual generation methods usually resort to huge large language model or\ncomposable diffusion models. Instead of designing another giant model for\naudio-visual generation, in this paper we take a step back showing a simple and\nlightweight generative transformer, which is not fully investigated in\nmulti-modal generation, can achieve excellent results on image2audio\ngeneration. The transformer operates in the discrete audio and visual\nVector-Quantized GAN space, and is trained in the mask denoising manner. After\ntraining, the classifier-free guidance could be deployed off-the-shelf\nachieving better performance, without any extra training or modification. Since\nthe transformer model is modality symmetrical, it could also be directly\ndeployed for audio2image generation and co-generation. In the experiments, we\nshow that our simple method surpasses recent image2audio generation methods.\nGenerated audio samples can be found at\nhttps://docs.google.com/presentation/d/1ZtC0SeblKkut4XJcRaDsSTuCRIXB3ypxmSi7HTY3IyQ/", "published": "2024-05-23 14:13:16", "link": "http://arxiv.org/abs/2405.14598v2", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.CV"}
