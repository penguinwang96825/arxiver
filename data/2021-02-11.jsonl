{"title": "Toward Improving Coherence and Diversity of Slogan Generation", "abstract": "Previous work in slogan generation focused on utilising slogan skeletons\nmined from existing slogans. While some generated slogans can be catchy, they\nare often not coherent with the company's focus or style across their marketing\ncommunications because the skeletons are mined from other companies' slogans.\nWe propose a sequence-to-sequence (seq2seq) transformer model to generate\nslogans from a brief company description. A naive seq2seq model fine-tuned for\nslogan generation is prone to introducing false information. We use company\nname delexicalisation and entity masking to alleviate this problem and improve\nthe generated slogans' quality and truthfulness. Furthermore, we apply\nconditional training based on the first words' POS tag to generate\nsyntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score\nof 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that\nour method generates significantly more factual, diverse and catchy slogans\nthan strong LSTM and transformer seq2seq baselines.", "published": "2021-02-11 10:25:08", "link": "http://arxiv.org/abs/2102.05924v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Text Compression-aided Transformer Encoding", "abstract": "Text encoding is one of the most important steps in Natural Language\nProcessing (NLP). It has been done well by the self-attention mechanism in the\ncurrent state-of-the-art Transformer encoder, which has brought about\nsignificant improvements in the performance of many NLP tasks. Though the\nTransformer encoder may effectively capture general information in its\nresulting representations, the backbone information, meaning the gist of the\ninput text, is not specifically focused on. In this paper, we propose explicit\nand implicit text compression approaches to enhance the Transformer encoding\nand evaluate models using this approach on several typical downstream tasks\nthat rely on the encoding heavily. Our explicit text compression approaches use\ndedicated models to compress text, while our implicit text compression approach\nsimply adds an additional module to the main model to handle text compression.\nWe propose three ways of integration, namely backbone source-side fusion,\ntarget-side fusion, and both-side fusion, to integrate the backbone information\ninto Transformer-based models for various downstream tasks. Our evaluation on\nbenchmark datasets shows that the proposed explicit and implicit text\ncompression approaches improve results in comparison to strong baselines. We\ntherefore conclude, when comparing the encodings to the baseline models, text\ncompression helps the encoders to learn better language representations.", "published": "2021-02-11 11:28:39", "link": "http://arxiv.org/abs/2102.05951v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "An End-to-end Model for Entity-level Relation Extraction using\n  Multi-instance Learning", "abstract": "We present a joint model for entity-level relation extraction from documents.\nIn contrast to other approaches - which focus on local intra-sentence mention\npairs and thus require annotations on mention level - our model operates on\nentity level. To do so, a multi-task approach is followed that builds upon\ncoreference resolution and gathers relevant signals via multi-instance learning\nwith multi-level representations combining global entity and local mention\ninformation. We achieve state-of-the-art relation extraction results on the\nDocRED dataset and report the first entity-level end-to-end relation extraction\nresults for future reference. Finally, our experimental results suggest that a\njoint approach is on par with task-specific learning, though more efficient due\nto shared parameters and training steps.", "published": "2021-02-11 12:49:39", "link": "http://arxiv.org/abs/2102.05980v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Personalised and Document-level Machine Translation of Dialogue", "abstract": "State-of-the-art (SOTA) neural machine translation (NMT) systems translate\ntexts at sentence level, ignoring context: intra-textual information, like the\nprevious sentence, and extra-textual information, like the gender of the\nspeaker. Because of that, some sentences are translated incorrectly.\nPersonalised NMT (PersNMT) and document-level NMT (DocNMT) incorporate this\ninformation into the translation process. Both fields are relatively new and\nprevious work within them is limited. Moreover, there are no readily available\nrobust evaluation metrics for them, which makes it difficult to develop better\nsystems, as well as track global progress and compare different methods. This\nthesis proposal focuses on PersNMT and DocNMT for the domain of dialogue\nextracted from TV subtitles in five languages: English, Brazilian Portuguese,\nGerman, French and Polish. Three main challenges are addressed: (1)\nincorporating extra-textual information directly into NMT systems; (2)\nimproving the machine translation of cohesion devices; (3) reliable evaluation\nfor PersNMT and DocNMT.", "published": "2021-02-11 09:18:20", "link": "http://arxiv.org/abs/2102.10979v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Less is More: ClipBERT for Video-and-Language Learning via Sparse\n  Sampling", "abstract": "The canonical approach to video-and-language learning (e.g., video question\nanswering) dictates a neural model to learn from offline-extracted dense video\nfeatures from vision models and text features from language models. These\nfeature extractors are trained independently and usually on tasks different\nfrom the target domains, rendering these fixed features sub-optimal for\ndownstream tasks. Moreover, due to the high computational overload of dense\nvideo features, it is often difficult (or infeasible) to plug feature\nextractors directly into existing approaches for easy finetuning. To provide a\nremedy to this dilemma, we propose a generic framework ClipBERT that enables\naffordable end-to-end learning for video-and-language tasks, by employing\nsparse sampling, where only a single or a few sparsely sampled short clips from\na video are used at each training step. Experiments on text-to-video retrieval\nand video question answering on six datasets demonstrate that ClipBERT\noutperforms (or is on par with) existing methods that exploit full-length\nvideos, suggesting that end-to-end learning with just a few sparsely sampled\nclips is often more accurate than using densely extracted offline features from\nfull-length videos, proving the proverbial less-is-more principle. Videos in\nthe datasets are from considerably different domains and lengths, ranging from\n3-second generic domain GIF videos to 180-second YouTube human activity videos,\nshowing the generalization ability of our approach. Comprehensive ablation\nstudies and thorough analyses are provided to dissect what factors lead to this\nsuccess. Our code is publicly available at https://github.com/jayleicn/ClipBERT", "published": "2021-02-11 18:50:16", "link": "http://arxiv.org/abs/2102.06183v1", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Unsupervised Extractive Summarization using Pointwise Mutual Information", "abstract": "Unsupervised approaches to extractive summarization usually rely on a notion\nof sentence importance defined by the semantic similarity between a sentence\nand the document. We propose new metrics of relevance and redundancy using\npointwise mutual information (PMI) between sentences, which can be easily\ncomputed by a pre-trained language model. Intuitively, a relevant sentence\nallows readers to infer the document content (high PMI with the document), and\na redundant sentence can be inferred from the summary (high PMI with the\nsummary). We then develop a greedy sentence selection algorithm to maximize\nrelevance and minimize redundancy of extracted sentences. We show that our\nmethod outperforms similarity-based methods on datasets in a range of domains\nincluding news, medical journal articles, and personal anecdotes.", "published": "2021-02-11 21:05:50", "link": "http://arxiv.org/abs/2102.06272v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A reproduction of Apple's bi-directional LSTM models for language\n  identification in short strings", "abstract": "Language Identification is the task of identifying a document's language. For\napplications like automatic spell checker selection, language identification\nmust use very short strings such as text message fragments. In this work, we\nreproduce a language identification architecture that Apple briefly sketched in\na blog post. We confirm the bi-LSTM model's performance and find that it\noutperforms current open-source language identifiers. We further find that its\nlanguage identification mistakes are due to confusion between related\nlanguages.", "published": "2021-02-11 21:46:43", "link": "http://arxiv.org/abs/2102.06282v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Scaling Up Visual and Vision-Language Representation Learning With Noisy\n  Text Supervision", "abstract": "Pre-trained representations are becoming crucial for many NLP and perception\ntasks. While representation learning in NLP has transitioned to training on raw\ntext without human annotations, visual and vision-language representations\nstill rely heavily on curated training datasets that are expensive or require\nexpert knowledge. For vision applications, representations are mostly learned\nusing datasets with explicit class labels such as ImageNet or OpenImages. For\nvision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all\ninvolve a non-trivial data collection (and cleaning) process. This costly\ncuration process limits the size of datasets and hence hinders the scaling of\ntrained models. In this paper, we leverage a noisy dataset of over one billion\nimage alt-text pairs, obtained without expensive filtering or post-processing\nsteps in the Conceptual Captions dataset. A simple dual-encoder architecture\nlearns to align visual and language representations of the image and text pairs\nusing a contrastive loss. We show that the scale of our corpus can make up for\nits noise and leads to state-of-the-art representations even with such a simple\nlearning scheme. Our visual representation achieves strong performance when\ntransferred to classification tasks such as ImageNet and VTAB. The aligned\nvisual and language representations enables zero-shot image classification and\nalso set new state-of-the-art results on Flickr30K and MSCOCO image-text\nretrieval benchmarks, even when compared with more sophisticated\ncross-attention models. The representations also enable cross-modality search\nwith complex text and text + image queries.", "published": "2021-02-11 10:08:12", "link": "http://arxiv.org/abs/2102.05918v2", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Language Independent Emotion Quantification using Non linear Modelling\n  of Speech", "abstract": "At present emotion extraction from speech is a very important issue due to\nits diverse applications. Hence, it becomes absolutely necessary to obtain\nmodels that take into consideration the speaking styles of a person, vocal\ntract information, timbral qualities and other congenital information regarding\nhis voice. Our speech production system is a nonlinear system like most other\nreal world systems. Hence the need arises for modelling our speech information\nusing nonlinear techniques. In this work we have modelled our articulation\nsystem using nonlinear multifractal analysis. The multifractal spectral width\nand scaling exponents reveals essentially the complexity associated with the\nspeech signals taken. The multifractal spectrums are well distinguishable the\nin low fluctuation region in case of different emotions. The source\ncharacteristics have been quantified with the help of different non-linear\nmodels like Multi-Fractal Detrended Fluctuation Analysis, Wavelet Transform\nModulus Maxima. The Results obtained from this study gives a very good result\nin emotion clustering.", "published": "2021-02-11 13:48:25", "link": "http://arxiv.org/abs/2102.06003v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Cross-Domain Multi-Task Learning for Sequential Sentence Classification\n  in Research Papers", "abstract": "Sequential sentence classification deals with the categorisation of sentences\nbased on their content and context. Applied to scientific texts, it enables the\nautomatic structuring of research papers and the improvement of academic search\nengines. However, previous work has not investigated the potential of transfer\nlearning for sentence classification across different scientific domains and\nthe issue of different text structure of full papers and abstracts. In this\npaper, we derive seven related research questions and present several\ncontributions to address them: First, we suggest a novel uniform deep learning\narchitecture and multi-task learning for cross-domain sequential sentence\nclassification in scientific texts. Second, we tailor two common transfer\nlearning methods, sequential transfer learning and multi-task learning, to deal\nwith the challenges of the given task. Semantic relatedness of tasks is a\nprerequisite for successful transfer learning of neural models. Consequently,\nour third contribution is an approach to semi-automatically identify\nsemantically related classes from different annotation schemes and we present\nan analysis of four annotation schemes. Comprehensive experimental results\nindicate that models, which are trained on datasets from different scientific\ndomains, benefit from one another when using the proposed multi-task learning\narchitecture. We also report comparisons with several state-of-the-art\napproaches. Our approach outperforms the state of the art on full paper\ndatasets significantly while being on par for datasets consisting of abstracts.", "published": "2021-02-11 13:54:10", "link": "http://arxiv.org/abs/2102.06008v2", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Fractal Approach to Characterize Emotions in Audio and Visual Domain:\n  A Study on Cross-Modal Interaction", "abstract": "It is already known that both auditory and visual stimulus is able to convey\nemotions in human mind to different extent. The strength or intensity of the\nemotional arousal vary depending on the type of stimulus chosen. In this study,\nwe try to investigate the emotional arousal in a cross-modal scenario involving\nboth auditory and visual stimulus while studying their source characteristics.\nA robust fractal analytic technique called Detrended Fluctuation Analysis (DFA)\nand its 2D analogue has been used to characterize three (3) standardized audio\nand video signals quantifying their scaling exponent corresponding to positive\nand negative valence. It was found that there is significant difference in\nscaling exponents corresponding to the two different modalities. Detrended\nCross Correlation Analysis (DCCA) has also been applied to decipher degree of\ncross-correlation among the individual audio and visual stimulus. This is the\nfirst of its kind study which proposes a novel algorithm with which emotional\narousal can be classified in cross-modal scenario using only the source audio\nand visual signals while also attempting a correlation between them.", "published": "2021-02-11 14:30:22", "link": "http://arxiv.org/abs/2102.06038v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech-language Pre-training for End-to-end Spoken Language\n  Understanding", "abstract": "End-to-end (E2E) spoken language understanding (SLU) can infer semantics\ndirectly from speech signal without cascading an automatic speech recognizer\n(ASR) with a natural language understanding (NLU) module. However, paired\nutterance recordings and corresponding semantics may not always be available or\nsufficient to train an E2E SLU model in a real production environment. In this\npaper, we propose to unify a well-optimized E2E ASR encoder (speech) and a\npre-trained language model encoder (language) into a transformer decoder. The\nunified speech-language pre-trained model (SLP) is continually enhanced on\nlimited labeled data from a target domain by using a conditional masked\nlanguage model (MLM) objective, and thus can effectively generate a sequence of\nintent, slot type, and slot value for given input speech in the inference. The\nexperimental results on two public corpora show that our approach to E2E SLU is\nsuperior to the conventional cascaded method. It also outperforms the present\nstate-of-the-art approaches to E2E SLU with much less paired data.", "published": "2021-02-11 21:55:48", "link": "http://arxiv.org/abs/2102.06283v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Embracing Domain Differences in Fake News: Cross-domain Fake News\n  Detection using Multi-modal Data", "abstract": "With the rapid evolution of social media, fake news has become a significant\nsocial problem, which cannot be addressed in a timely manner using manual\ninvestigation. This has motivated numerous studies on automating fake news\ndetection. Most studies explore supervised training models with different\nmodalities (e.g., text, images, and propagation networks) of news records to\nidentify fake news. However, the performance of such techniques generally drops\nif news records are coming from different domains (e.g., politics,\nentertainment), especially for domains that are unseen or rarely-seen during\ntraining. As motivation, we empirically show that news records from different\ndomains have significantly different word usage and propagation patterns.\nFurthermore, due to the sheer volume of unlabelled news records, it is\nchallenging to select news records for manual labelling so that the\ndomain-coverage of the labelled dataset is maximized. Hence, this work: (1)\nproposes a novel framework that jointly preserves domain-specific and\ncross-domain knowledge in news records to detect fake news from different\ndomains; and (2) introduces an unsupervised technique to select a set of\nunlabelled informative news records for manual labelling, which can be\nultimately used to train a fake news detection model that performs well for\nmany domains while minimizing the labelling cost. Our experiments show that the\nintegration of the proposed fake news model and the selective annotation\napproach achieves state-of-the-art performance for cross-domain news datasets,\nwhile yielding notable improvements for rarely-appearing domains in news\ndatasets.", "published": "2021-02-11 23:31:14", "link": "http://arxiv.org/abs/2102.06314v4", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Lie-Sensor: A Live Emotion Verifier or a Licensor for Chat Applications\n  using Emotional Intelligence", "abstract": "Veracity is an essential key in research and development of innovative\nproducts. Live Emotion analysis and verification nullify deceit made to\ncomplainers on live chat, corroborate messages of both ends in messaging apps\nand promote an honest conversation between users. The main concept behind this\nemotion artificial intelligent verifier is to license or decline message\naccountability by comparing variegated emotions of chat app users recognized\nthrough facial expressions and text prediction. In this paper, a proposed\nemotion intelligent live detector acts as an honest arbiter who distributes\nfacial emotions into labels namely, Happiness, Sadness, Surprise, and Hate.\nFurther, it separately predicts a label of messages through text\nclassification. Finally, it compares both labels and declares the message as a\nfraud or a bonafide. For emotion detection, we deployed Convolutional Neural\nNetwork (CNN) using a miniXception model and for text prediction, we selected\nSupport Vector Machine (SVM) natural language processing probability classifier\ndue to receiving the best accuracy on training dataset after applying Support\nVector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and\nLogistic regression.", "published": "2021-02-11 02:47:30", "link": "http://arxiv.org/abs/2102.11318v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Onoma-to-wave: Environmental sound synthesis from onomatopoeic words", "abstract": "In this paper, we propose a framework for environmental sound synthesis from\nonomatopoeic words. As one way of expressing an environmental sound, we can use\nan onomatopoeic word, which is a character sequence for phonetically imitating\na sound. An onomatopoeic word is effective for describing diverse sound\nfeatures. Therefore, using onomatopoeic words for environmental sound synthesis\nwill enable us to generate diverse environmental sounds. To generate diverse\nsounds, we propose a method based on a sequence-to-sequence framework for\nsynthesizing environmental sounds from onomatopoeic words. We also propose a\nmethod of environmental sound synthesis using onomatopoeic words and sound\nevent labels. The use of sound event labels in addition to onomatopoeic words\nenables us to capture each sound event's feature depending on the input sound\nevent label. Our subjective experiments show that our proposed methods achieve\nhigher diversity and naturalness than conventional methods using sound event\nlabels.", "published": "2021-02-11 07:15:14", "link": "http://arxiv.org/abs/2102.05872v4", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multichannel-based learning for audio object extraction", "abstract": "The current paradigm for creating and deploying immersive audio content is\nbased on audio objects, which are composed of an audio track and position\nmetadata. While rendering an object-based production into a multichannel mix is\nstraightforward, the reverse process involves sound source separation and\nestimating the spatial trajectories of the extracted sources. Besides,\ncinematic object-based productions are often composed by dozens of simultaneous\naudio objects, which poses a scalability challenge for audio object extraction.\nHere, we propose a novel deep learning approach to object extraction that\nlearns from the multichannel renders of object-based productions, instead of\ndirectly learning from the audio objects themselves. This approach allows\ntackling the object scalability challenge and also offers the possibility to\nformulate the problem in a supervised or an unsupervised fashion. Since, to our\nknowledge, no other works have previously addressed this topic, we first define\nthe task and propose an evaluation methodology, and then discuss under what\ncircumstances our methods outperform the proposed baselines.", "published": "2021-02-11 17:32:39", "link": "http://arxiv.org/abs/2102.06142v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Efficient neural networks for real-time modeling of analog dynamic range\n  compression", "abstract": "Deep learning approaches have demonstrated success in modeling analog audio\neffects. Nevertheless, challenges remain in modeling more complex effects that\ninvolve time-varying nonlinear elements, such as dynamic range compressors.\nExisting neural network approaches for modeling compression either ignore the\ndevice parameters, do not attain sufficient accuracy, or otherwise require\nlarge noncausal models prohibiting real-time operation. In this work, we\npropose a modification to temporal convolutional networks (TCNs) enabling\ngreater efficiency without sacrificing performance. By utilizing very sparse\nconvolutional kernels through rapidly growing dilations, our model attains a\nsignificant receptive field using fewer layers, reducing computation. Through a\ndetailed evaluation we demonstrate our efficient and causal approach achieves\nstate-of-the-art performance in modeling the analog LA-2A, is capable of\nreal-time operation on CPU, and only requires 10 minutes of training data.", "published": "2021-02-11 18:58:50", "link": "http://arxiv.org/abs/2102.06200v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "ASVspoof 2019: spoofing countermeasures for the detection of\n  synthesized, converted and replayed speech", "abstract": "The ASVspoof initiative was conceived to spearhead research in anti-spoofing\nfor automatic speaker verification (ASV). This paper describes the third in a\nseries of bi-annual challenges: ASVspoof 2019. With the challenge database and\nprotocols being described elsewhere, the focus of this paper is on results and\nthe top performing single and ensemble system submissions from 62 teams, all of\nwhich out-perform the two baseline systems, often by a substantial margin.\nDeeper analyses shows that performance is dominated by specific conditions\ninvolving either specific spoofing attacks or specific acoustic environments.\nWhile fusion is shown to be particularly effective for the logical access\nscenario involving speech synthesis and voice conversion attacks, participants\nlargely struggled to apply fusion successfully for the physical access scenario\ninvolving simulated replay attacks. This is likely the result of a lack of\nsystem complementarity, while oracle fusion experiments show clear potential to\nimprove performance. Furthermore, while results for simulated data are\npromising, experiments with real replay data show a substantial gap, most\nlikely due to the presence of additive noise in the latter. This finding, among\nothers, leads to a number of ideas for further research and directions for\nfuture editions of the ASVspoof challenge.", "published": "2021-02-11 08:41:42", "link": "http://arxiv.org/abs/2102.05889v1", "categories": ["eess.AS", "cs.CR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "CASA-Based Speaker Identification Using Cascaded GMM-CNN Classifier in\n  Noisy and Emotional Talking Conditions", "abstract": "This work aims at intensifying text-independent speaker identification\nperformance in real application situations such as noisy and emotional talking\nconditions. This is achieved by incorporating two different modules: a\nComputational Auditory Scene Analysis CASA based pre-processing module for\nnoise reduction and cascaded Gaussian Mixture Model Convolutional Neural\nNetwork GMM-CNN classifier for speaker identification followed by emotion\nrecognition. This research proposes and evaluates a novel algorithm to improve\nthe accuracy of speaker identification in emotional and highly-noise\nsusceptible conditions. Experiments demonstrate that the proposed model yields\npromising results in comparison with other classifiers when Speech Under\nSimulated and Actual Stress SUSAS database, Emirati Speech Database ESD, the\nRyerson Audio-Visual Database of Emotional Speech and Song RAVDESS database and\nthe Fluent Speech Commands database are used in a noisy environment.", "published": "2021-02-11 08:56:12", "link": "http://arxiv.org/abs/2102.05894v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speech enhancement with mixture-of-deep-experts with clean clustering\n  pre-training", "abstract": "In this study we present a mixture of deep experts (MoDE) neural-network\narchitecture for single microphone speech enhancement. Our architecture\ncomprises a set of deep neural networks (DNNs), each of which is an 'expert' in\na different speech spectral pattern such as phoneme. A gating DNN is\nresponsible for the latent variables which are the weights assigned to each\nexpert's output given a speech segment. The experts estimate a mask from the\nnoisy input and the final mask is then obtained as a weighted average of the\nexperts' estimates, with the weights determined by the gating DNN. A soft\nspectral attenuation, based on the estimated mask, is then applied to enhance\nthe noisy speech signal. As a byproduct, we gain reduction at the complexity in\ntest time. We show that the experts specialization allows better robustness to\nunfamiliar noise types.", "published": "2021-02-11 14:18:47", "link": "http://arxiv.org/abs/2102.06034v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Investigation of End-to-End Models for Robust Speech Recognition", "abstract": "End-to-end models for robust automatic speech recognition (ASR) have not been\nsufficiently well-explored in prior work. With end-to-end models, one could\nchoose to preprocess the input speech using speech enhancement techniques and\ntrain the model using enhanced speech. Another alternative is to pass the noisy\nspeech as input and modify the model architecture to adapt to noisy speech. A\nsystematic comparison of these two approaches for end-to-end robust ASR has not\nbeen attempted before. We address this gap and present a detailed comparison of\nspeech enhancement-based techniques and three different model-based adaptation\ntechniques covering data augmentation, multi-task learning, and adversarial\nlearning for robust ASR. While adversarial learning is the best-performing\ntechnique on certain noise types, it comes at the cost of degrading clean\nspeech WER. On other relatively stationary noise types, a new speech\nenhancement technique outperformed all the model-based adaptation techniques.\nThis suggests that knowledge of the underlying noise type can meaningfully\ninform the choice of adaptation technique.", "published": "2021-02-11 19:47:13", "link": "http://arxiv.org/abs/2102.06237v1", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Disentanglement for audio-visual emotion recognition using multitask\n  setup", "abstract": "Deep learning models trained on audio-visual data have been successfully used\nto achieve state-of-the-art performance for emotion recognition. In particular,\nmodels trained with multitask learning have shown additional performance\nimprovements. However, such multitask models entangle information between the\ntasks, encoding the mutual dependencies present in label distributions in the\nreal world data used for training. This work explores the disentanglement of\nmultimodal signal representations for the primary task of emotion recognition\nand a secondary person identification task. In particular, we developed a\nmultitask framework to extract low-dimensional embeddings that aim to capture\nemotion specific information, while containing minimal information related to\nperson identity. We evaluate three different techniques for disentanglement and\nreport results of up to 13% disentanglement while maintaining emotion\nrecognition performance.", "published": "2021-02-11 20:57:37", "link": "http://arxiv.org/abs/2102.06269v1", "categories": ["eess.IV", "cs.SD", "eess.AS"], "primary_category": "eess.IV"}
{"title": "A Multi-View Approach To Audio-Visual Speaker Verification", "abstract": "Although speaker verification has conventionally been an audio-only task,\nsome practical applications provide both audio and visual streams of input. In\nthese cases, the visual stream provides complementary information and can often\nbe leveraged in conjunction with the acoustics of speech to improve\nverification performance. In this study, we explore audio-visual approaches to\nspeaker verification, starting with standard fusion techniques to learn joint\naudio-visual (AV) embeddings, and then propose a novel approach to handle\ncross-modal verification at test time. Specifically, we investigate unimodal\nand concatenation based AV fusion and report the lowest AV equal error rate\n(EER) of 0.7% on the VoxCeleb1 dataset using our best system. As these methods\nlack the ability to do cross-modal verification, we introduce a multi-view\nmodel which uses a shared classifier to map audio and video into the same\nspace. This new approach achieves 28% EER on VoxCeleb1 in the challenging\ntesting condition of cross-modal verification.", "published": "2021-02-11 22:29:25", "link": "http://arxiv.org/abs/2102.06291v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.IV"], "primary_category": "cs.SD"}
{"title": "DEEPF0: End-To-End Fundamental Frequency Estimation for Music and Speech\n  Signals", "abstract": "We propose a novel pitch estimation technique called DeepF0, which leverages\nthe available annotated data to directly learns from the raw audio in a\ndata-driven manner. F0 estimation is important in various speech processing and\nmusic information retrieval applications. Existing deep learning models for\npitch estimations have relatively limited learning capabilities due to their\nshallow receptive field. The proposed model addresses this issue by extending\nthe receptive field of a network by introducing the dilated convolutional\nblocks into the network. The dilation factor increases the network receptive\nfield exponentially without increasing the parameters of the model\nexponentially. To make the training process more efficient and faster, DeepF0\nis augmented with residual blocks with residual connections. Our empirical\nevaluation demonstrates that the proposed model outperforms the baselines in\nterms of raw pitch accuracy and raw chroma accuracy even using 77.4% fewer\nnetwork parameters. We also show that our model can capture reasonably well\npitch estimation even under the various levels of accompaniment noise.", "published": "2021-02-11 23:11:22", "link": "http://arxiv.org/abs/2102.06306v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
