{"title": "EviDR: Evidence-Emphasized Discrete Reasoning for Reasoning Machine\n  Reading Comprehension", "abstract": "Reasoning machine reading comprehension (R-MRC) aims to answer complex\nquestions that require discrete reasoning based on text. To support discrete\nreasoning, evidence, typically the concise textual fragments that describe\nquestion-related facts, including topic entities and attribute values, are\ncrucial clues from question to answer. However, previous end-to-end methods\nthat achieve state-of-the-art performance rarely solve the problem by paying\nenough emphasis on the modeling of evidence, missing the opportunity to further\nimprove the model's reasoning ability for R-MRC. To alleviate the above issue,\nin this paper, we propose an evidence-emphasized discrete reasoning approach\n(EviDR), in which sentence and clause level evidence is first detected based on\ndistant supervision, and then used to drive a reasoning module implemented with\na relational heterogeneous graph convolutional network to derive answers.\nExtensive experiments are conducted on DROP (discrete reasoning over\nparagraphs) dataset, and the results demonstrate the effectiveness of our\nproposed approach. In addition, qualitative analysis verifies the capability of\nthe proposed evidence-emphasized discrete reasoning for R-MRC.", "published": "2021-08-18 06:49:58", "link": "http://arxiv.org/abs/2108.07994v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GGP: A Graph-based Grouping Planner for Explicit Control of Long Text\n  Generation", "abstract": "Existing data-driven methods can well handle short text generation. However,\nwhen applied to the long-text generation scenarios such as story generation or\nadvertising text generation in the commercial scenario, these methods may\ngenerate illogical and uncontrollable texts. To address these aforementioned\nissues, we propose a graph-based grouping planner(GGP) following the idea of\nfirst-plan-then-generate. Specifically, given a collection of key phrases, GGP\nfirstly encodes these phrases into an instance-level sequential representation\nand a corpus-level graph-based representation separately. With these two\nsynergic representations, we then regroup these phrases into a fine-grained\nplan, based on which we generate the final long text. We conduct our\nexperiments on three long text generation datasets and the experimental results\nreveal that GGP significantly outperforms baselines, which proves that GGP can\ncontrol the long text generation by knowing how to say and in what order.", "published": "2021-08-18 06:55:55", "link": "http://arxiv.org/abs/2108.07998v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "CUSTOM: Aspect-Oriented Product Summarization for E-Commerce", "abstract": "Product summarization aims to automatically generate product descriptions,\nwhich is of great commercial potential. Considering the customer preferences on\ndifferent product aspects, it would benefit from generating aspect-oriented\ncustomized summaries. However, conventional systems typically focus on\nproviding general product summaries, which may miss the opportunity to match\nproducts with customer interests. To address the problem, we propose CUSTOM,\naspect-oriented product summarization for e-commerce, which generates diverse\nand controllable summaries towards different product aspects. To support the\nstudy of CUSTOM and further this line of research, we construct two Chinese\ndatasets, i.e., SMARTPHONE and COMPUTER, including 76,279 / 49,280 short\nsummaries for 12,118 / 11,497 real-world commercial products, respectively.\nFurthermore, we introduce EXT, an extraction-enhanced generation framework for\nCUSTOM, where two famous sequence-to-sequence models are implemented in this\npaper. We conduct extensive experiments on the two proposed datasets for CUSTOM\nand show results of two famous baseline models and EXT, which indicates that\nEXT can generate diverse, high-quality, and consistent summaries.", "published": "2021-08-18 07:26:22", "link": "http://arxiv.org/abs/2108.08010v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Joint Multiple Intent Detection and Slot Filling via Self-distillation", "abstract": "Intent detection and slot filling are two main tasks in natural language\nunderstanding (NLU) for identifying users' needs from their utterances. These\ntwo tasks are highly related and often trained jointly. However, most previous\nworks assume that each utterance only corresponds to one intent, ignoring the\nfact that a user utterance in many cases could include multiple intents. In\nthis paper, we propose a novel Self-Distillation Joint NLU model (SDJN) for\nmulti-intent NLU. First, we formulate multiple intent detection as a weakly\nsupervised problem and approach with multiple instance learning (MIL). Then, we\ndesign an auxiliary loop via self-distillation with three orderly arranged\ndecoders: Initial Slot Decoder, MIL Intent Decoder, and Final Slot Decoder. The\noutput of each decoder will serve as auxiliary information for the next\ndecoder. With the auxiliary knowledge provided by the MIL Intent Decoder, we\nset Final Slot Decoder as the teacher model that imparts knowledge back to\nInitial Slot Decoder to complete the loop. The auxiliary loop enables intents\nand slots to guide mutually in-depth and further boost the overall NLU\nperformance. Experimental results on two public multi-intent datasets indicate\nthat our model achieves strong performance compared to others.", "published": "2021-08-18 08:45:03", "link": "http://arxiv.org/abs/2108.08042v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Affective Decoding for Empathetic Response Generation", "abstract": "Understanding speaker's feelings and producing appropriate responses with\nemotion connection is a key communicative skill for empathetic dialogue\nsystems. In this paper, we propose a simple technique called Affective Decoding\nfor empathetic response generation. Our method can effectively incorporate\nemotion signals during each decoding step, and can additionally be augmented\nwith an auxiliary dual emotion encoder, which learns separate embeddings for\nthe speaker and listener given the emotion base of the dialogue. Extensive\nempirical studies show that our models are perceived to be more empathetic by\nhuman evaluations, in comparison to several strong mainstream methods for\nempathetic responding.", "published": "2021-08-18 11:48:40", "link": "http://arxiv.org/abs/2108.08102v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "AdapterHub Playground: Simple and Flexible Few-Shot Learning with\n  Adapters", "abstract": "The open-access dissemination of pretrained language models through online\nrepositories has led to a democratization of state-of-the-art natural language\nprocessing (NLP) research. This also allows people outside of NLP to use such\nmodels and adapt them to specific use-cases. However, a certain amount of\ntechnical proficiency is still required which is an entry barrier for users who\nwant to apply these models to a certain task but lack the necessary knowledge\nor resources. In this work, we aim to overcome this gap by providing a tool\nwhich allows researchers to leverage pretrained models without writing a single\nline of code. Built upon the parameter-efficient adapter modules for transfer\nlearning, our AdapterHub Playground provides an intuitive interface, allowing\nthe usage of adapters for prediction, training and analysis of textual data for\na variety of NLP tasks. We present the tool's architecture and demonstrate its\nadvantages with prototypical use-cases, where we show that predictive\nperformance can easily be increased in a few-shot learning scenario. Finally,\nwe evaluate its usability in a user study. We provide the code and a live\ninterface at https://adapter-hub.github.io/playground.", "published": "2021-08-18 11:56:01", "link": "http://arxiv.org/abs/2108.08103v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Table Caption Generation in Scholarly Documents Leveraging Pre-trained\n  Language Models", "abstract": "This paper addresses the problem of generating table captions for scholarly\ndocuments, which often require additional information outside the table. To\nthis end, we propose a method of retrieving relevant sentences from the paper\nbody, and feeding the table content as well as the retrieved sentences into\npre-trained language models (e.g. T5 and GPT-2) for generating table captions.\nThe contributions of this paper are: (1) discussion on the challenges in table\ncaptioning for scholarly documents; (2) development of a dataset DocBank-TB,\nwhich is publicly available; and (3) comparison of caption generation methods\nfor scholarly documents with different strategies to retrieve relevant\nsentences from the paper body. Our experimental results showed that T5 is the\nbetter generation model for this task, as it outperformed GPT-2 in BLEU and\nMETEOR implying that the generated text are clearer and more precise. Moreover,\ninputting relevant sentences matching the row header or whole table is\neffective.", "published": "2021-08-18 12:25:43", "link": "http://arxiv.org/abs/2108.08111v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "RTE: A Tool for Annotating Relation Triplets from Text", "abstract": "In this work, we present a Web-based annotation tool `Relation Triplets\nExtractor' \\footnote{https://abera87.github.io/annotate/} (RTE) for annotating\nrelation triplets from the text. Relation extraction is an important task for\nextracting structured information about real-world entities from the\nunstructured text available on the Web. In relation extraction, we focus on\nbinary relation that refers to relations between two entities. Recently, many\nsupervised models are proposed to solve this task, but they mostly use noisy\ntraining data obtained using the distant supervision method. In many cases,\nevaluation of the models is also done based on a noisy test dataset. The lack\nof annotated clean dataset is a key challenge in this area of research. In this\nwork, we built a web-based tool where researchers can annotate datasets for\nrelation extraction on their own very easily. We use a server-less architecture\nfor this tool, and the entire annotation operation is processed using\nclient-side code. Thus it does not suffer from any network latency, and the\nprivacy of the user's data is also maintained. We hope that this tool will be\nbeneficial for the researchers to advance the field of relation extraction.", "published": "2021-08-18 14:54:22", "link": "http://arxiv.org/abs/2108.08184v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MeDiaQA: A Question Answering Dataset on Medical Dialogues", "abstract": "In this paper, we introduce MeDiaQA, a novel question answering(QA) dataset,\nwhich constructed on real online Medical Dialogues. It contains 22k\nmultiple-choice questions annotated by human for over 11k dialogues with 120k\nutterances between patients and doctors, covering 150 specialties of diseases,\nwhich are collected from haodf.com and dxy.com. MeDiaQA is the first QA dataset\nwhere reasoning over medical dialogues, especially their quantitative contents.\nThe dataset has the potential to test the computing, reasoning and\nunderstanding ability of models across multi-turn dialogues, which is\nchallenging compared with the existing datasets. To address the challenges, we\ndesign MeDia-BERT, and it achieves 64.3% accuracy, while human performance of\n93% accuracy, which indicates that there still remains a large room for\nimprovement.", "published": "2021-08-18 09:57:16", "link": "http://arxiv.org/abs/2108.08074v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SHAQ: Single Headed Attention with Quasi-Recurrence", "abstract": "Natural Language Processing research has recently been dominated by large\nscale transformer models. Although they achieve state of the art on many\nimportant language tasks, transformers often require expensive compute\nresources, and days spanning to weeks to train. This is feasible for\nresearchers at big tech companies and leading research universities, but not\nfor scrappy start-up founders, students, and independent researchers. Stephen\nMerity's SHA-RNN, a compact, hybrid attention-RNN model, is designed for\nconsumer-grade modeling as it requires significantly fewer parameters and less\ntraining time to reach near state of the art results. We analyze Merity's model\nhere through an exploratory model analysis over several units of the\narchitecture considering both training time and overall quality in our\nassessment. Ultimately, we combine these findings into a new architecture which\nwe call SHAQ: Single Headed Attention Quasi-recurrent Neural Network. With our\nnew architecture we achieved similar accuracy results as the SHA-RNN while\naccomplishing a 4x speed boost in training.", "published": "2021-08-18 15:38:35", "link": "http://arxiv.org/abs/2108.08207v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "TSI: an Ad Text Strength Indicator using Text-to-CTR and\n  Semantic-Ad-Similarity", "abstract": "Coming up with effective ad text is a time consuming process, and\nparticularly challenging for small businesses with limited advertising\nexperience. When an inexperienced advertiser onboards with a poorly written ad\ntext, the ad platform has the opportunity to detect low performing ad text, and\nprovide improvement suggestions. To realize this opportunity, we propose an ad\ntext strength indicator (TSI) which: (i) predicts the click-through-rate (CTR)\nfor an input ad text, (ii) fetches similar existing ads to create a\nneighborhood around the input ad, (iii) and compares the predicted CTRs in the\nneighborhood to declare whether the input ad is strong or weak. In addition, as\nsuggestions for ad text improvement, TSI shows anonymized versions of superior\nads (higher predicted CTR) in the neighborhood. For (i), we propose a BERT\nbased text-to-CTR model trained on impressions and clicks associated with an ad\ntext. For (ii), we propose a sentence-BERT based semantic-ad-similarity model\ntrained using weak labels from ad campaign setup data. Offline experiments\ndemonstrate that our BERT based text-to-CTR model achieves a significant lift\nin CTR prediction AUC for cold start (new) advertisers compared to bag-of-words\nbased baselines. In addition, our semantic-textual-similarity model for similar\nads retrieval achieves a precision@1 of 0.93 (for retrieving ads from the same\nproduct category); this is significantly higher compared to unsupervised\nTF-IDF, word2vec, and sentence-BERT baselines. Finally, we share promising\nonline results from advertisers in the Yahoo (Verizon Media) ad platform where\na variant of TSI was implemented with sub-second end-to-end latency.", "published": "2021-08-18 16:24:40", "link": "http://arxiv.org/abs/2108.08226v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Contributions of Transformer Attention Heads in Multi- and Cross-lingual\n  Tasks", "abstract": "This paper studies the relative importance of attention heads in\nTransformer-based models to aid their interpretability in cross-lingual and\nmulti-lingual tasks. Prior research has found that only a few attention heads\nare important in each mono-lingual Natural Language Processing (NLP) task and\npruning the remaining heads leads to comparable or improved performance of the\nmodel. However, the impact of pruning attention heads is not yet clear in\ncross-lingual and multi-lingual tasks. Through extensive experiments, we show\nthat (1) pruning a number of attention heads in a multi-lingual\nTransformer-based model has, in general, positive effects on its performance in\ncross-lingual and multi-lingual tasks and (2) the attention heads to be pruned\ncan be ranked using gradients and identified with a few trial experiments. Our\nexperiments focus on sequence labeling tasks, with potential applicability on\nother cross-lingual and multi-lingual tasks. For comprehensiveness, we examine\ntwo pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and\nXLM-R, on three tasks across 9 languages each. We also discuss the validity of\nour findings and their extensibility to truly resource-scarce languages and\nother task settings.", "published": "2021-08-18 20:17:46", "link": "http://arxiv.org/abs/2108.08375v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FeelsGoodMan: Inferring Semantics of Twitch Neologisms", "abstract": "Twitch chats pose a unique problem in natural language understanding due to a\nlarge presence of neologisms, specifically emotes. There are a total of 8.06\nmillion emotes, over 400k of which were used in the week studied. There is\nvirtually no information on the meaning or sentiment of emotes, and with a\nconstant influx of new emotes and drift in their frequencies, it becomes\nimpossible to maintain an updated manually-labeled dataset. Our paper makes a\ntwo fold contribution. First we establish a new baseline for sentiment analysis\non Twitch data, outperforming the previous supervised benchmark by 7.9% points.\nSecondly, we introduce a simple but powerful unsupervised framework based on\nword embeddings and k-NN to enrich existing models with out-of-vocabulary\nknowledge. This framework allows us to auto-generate a pseudo-dictionary of\nemotes and we show that we can nearly match the supervised benchmark above even\nwhen injecting such emote knowledge into sentiment classifiers trained on\nextraneous datasets such as movie reviews or Twitter.", "published": "2021-08-18 23:46:46", "link": "http://arxiv.org/abs/2108.08411v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning Implicit User Profiles for Personalized Retrieval-Based Chatbot", "abstract": "In this paper, we explore the problem of developing personalized chatbots. A\npersonalized chatbot is designed as a digital chatting assistant for a user.\nThe key characteristic of a personalized chatbot is that it should have a\nconsistent personality with the corresponding user. It can talk the same way as\nthe user when it is delegated to respond to others' messages. We present a\nretrieval-based personalized chatbot model, namely IMPChat, to learn an\nimplicit user profile from the user's dialogue history. We argue that the\nimplicit user profile is superior to the explicit user profile regarding\naccessibility and flexibility. IMPChat aims to learn an implicit user profile\nthrough modeling user's personalized language style and personalized\npreferences separately. To learn a user's personalized language style, we\nelaborately build language models from shallow to deep using the user's\nhistorical responses; To model a user's personalized preferences, we explore\nthe conditional relations underneath each post-response pair of the user. The\npersonalized preferences are dynamic and context-aware: we assign higher\nweights to those historical pairs that are topically related to the current\nquery when aggregating the personalized preferences. We match each response\ncandidate with the personalized language style and personalized preference,\nrespectively, and fuse the two matching signals to determine the final ranking\nscore. Comprehensive experiments on two large datasets show that our method\noutperforms all baseline models.", "published": "2021-08-18 02:07:28", "link": "http://arxiv.org/abs/2108.07935v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "De-identification of Unstructured Clinical Texts from Sequence to\n  Sequence Perspective", "abstract": "In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.", "published": "2021-08-18 04:48:58", "link": "http://arxiv.org/abs/2108.07971v2", "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Integrating Dialog History into End-to-End Spoken Language Understanding\n  Systems", "abstract": "End-to-end spoken language understanding (SLU) systems that process\nhuman-human or human-computer interactions are often context independent and\nprocess each turn of a conversation independently. Spoken conversations on the\nother hand, are very much context dependent, and dialog history contains useful\ninformation that can improve the processing of each conversational turn. In\nthis paper, we investigate the importance of dialog history and how it can be\neffectively integrated into end-to-end SLU systems. While processing a spoken\nutterance, our proposed RNN transducer (RNN-T) based SLU model has access to\nits dialog history in the form of decoded transcripts and SLU labels of\nprevious turns. We encode the dialog history as BERT embeddings, and use them\nas an additional input to the SLU model along with the speech features for the\ncurrent utterance. We evaluate our approach on a recently released spoken\ndialog data set, the HarperValleyBank corpus. We observe significant\nimprovements: 8% for dialog action and 30% for caller intent recognition tasks,\nin comparison to a competitive context independent end-to-end baseline system.", "published": "2021-08-18 22:24:11", "link": "http://arxiv.org/abs/2108.08405v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "X-modaler: A Versatile and High-performance Codebase for Cross-modal\n  Analytics", "abstract": "With the rise and development of deep learning over the past decade, there\nhas been a steady momentum of innovation and breakthroughs that convincingly\npush the state-of-the-art of cross-modal analytics between vision and language\nin multimedia field. Nevertheless, there has not been an open-source codebase\nin support of training and deploying numerous neural network models for\ncross-modal analytics in a unified and modular fashion. In this work, we\npropose X-modaler -- a versatile and high-performance codebase that\nencapsulates the state-of-the-art cross-modal analytics into several\ngeneral-purpose stages (e.g., pre-processing, encoder, cross-modal interaction,\ndecoder, and decode strategy). Each stage is empowered with the functionality\nthat covers a series of modules widely adopted in state-of-the-arts and allows\nseamless switching in between. This way naturally enables a flexible\nimplementation of state-of-the-art algorithms for image captioning, video\ncaptioning, and vision-language pre-training, aiming to facilitate the rapid\ndevelopment of research community. Meanwhile, since the effective modular\ndesigns in several stages (e.g., cross-modal interaction) are shared across\ndifferent vision-language tasks, X-modaler can be simply extended to power\nstartup prototypes for other tasks in cross-modal analytics, including visual\nquestion answering, visual commonsense reasoning, and cross-modal retrieval.\nX-modaler is an Apache-licensed codebase, and its source codes, sample projects\nand pre-trained models are available on-line:\nhttps://github.com/YehLi/xmodaler.", "published": "2021-08-18 16:05:30", "link": "http://arxiv.org/abs/2108.08217v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "cs.CV"}
{"title": "FDN: Finite Difference Network with Hierarchical Convolutional Features\n  for Text-independent Speaker Verification", "abstract": "In recent years, using raw waveforms as input for deep networks has been\nwidely explored for the speaker verification system. For example, RawNet and\nRawNet2 extracted speaker's feature embeddings from waveforms automatically for\nrecognizing their voice, which can vastly reduce the front-end computation and\nobtain state-of-the-art performance. However, these models do not consider the\nspeaker's high-level behavioral features, such as intonation, indicating each\nspeaker's universal style, rhythm, \\textit{etc}. This paper presents a novel\nnetwork that can handle the intonation information by computing the finite\ndifference of different speakers' utterance variations. Furthermore, a\nhierarchical way is also designed to enhance the intonation property from\ncoarse to fine to improve the system accuracy. The high-level intonation\nfeatures are then fused with the low-level embedding features. Experimental\nresults on official VoxCeleb1 test data, VoxCeleb1-E, and VoxCeleb-H protocols\nshow our method outperforms and robustness existing state-of-the-art systems.\nTo facilitate further research, code is available at\nhttps://github.com/happyjin/FDN", "published": "2021-08-18 05:01:00", "link": "http://arxiv.org/abs/2108.07974v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Multi-level Acoustic Feature Extraction Framework for Transformer\n  Based End-to-End Speech Recognition", "abstract": "Transformer based end-to-end modelling approaches with multiple stream inputs\nhave been achieved great success in various automatic speech recognition (ASR)\ntasks. An important issue associated with such approaches is that the\nintermediate features derived from each stream might have similar\nrepresentations and thus it is lacking of feature diversity, such as the\ndescriptions related to speaker characteristics. To address this issue, this\npaper proposed a novel multi-level acoustic feature extraction framework that\ncan be easily combined with Transformer based ASR models. The framework\nconsists of two input streams: a shallow stream with high-resolution\nspectrograms and a deep stream with low-resolution spectrograms. The shallow\nstream is used to acquire traditional shallow features that is beneficial for\nthe classification of phones or words while the deep stream is used to obtain\nutterance-level speaker-invariant deep features for improving the feature\ndiversity. A feature correlation based fusion strategy is used to aggregate\nboth features across the frequency and time domains and then fed into the\nTransformer encoder-decoder module. By using the proposed multi-level acoustic\nfeature extraction framework, state-of-the-art word error rate of 21.7% and\n2.5% were obtained on the HKUST Mandarin telephone and Librispeech speech\nrecognition tasks respectively.", "published": "2021-08-18 05:28:27", "link": "http://arxiv.org/abs/2108.07980v3", "categories": ["eess.AS"], "primary_category": "eess.AS"}
