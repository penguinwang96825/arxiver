{"title": "PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning", "abstract": "Vulnerability to lexical perturbation is a critical weakness of automatic\nevaluation metrics for image captioning. This paper proposes Perturbation\nRobust Multi-Lingual CLIPScore(PR-MCS), which exhibits robustness to such\nperturbations, as a novel reference-free image captioning metric applicable to\nmultiple languages. To achieve perturbation robustness, we fine-tune the text\nencoder of CLIP with our language-agnostic method to distinguish the perturbed\ntext from the original text. To verify the robustness of PR-MCS, we introduce a\nnew fine-grained evaluation dataset consisting of detailed captions, critical\nobjects, and the relationships between the objects for 3, 000 images in five\nlanguages. In our experiments, PR-MCS significantly outperforms baseline\nmetrics in capturing lexical noise of all various perturbation types in all\nfive languages, proving that PR-MCS is highly robust to lexical perturbations.", "published": "2023-03-15 06:37:26", "link": "http://arxiv.org/abs/2303.08389v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation", "abstract": "Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.", "published": "2023-03-15 10:53:49", "link": "http://arxiv.org/abs/2303.08518v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GCRE-GPT: A Generative Model for Comparative Relation Extraction", "abstract": "Given comparative text, comparative relation extraction aims to extract two\ntargets (\\eg two cameras) in comparison and the aspect they are compared for\n(\\eg image quality). The extracted comparative relations form the basis of\nfurther opinion analysis.Existing solutions formulate this task as a sequence\nlabeling task, to extract targets and aspects. However, they cannot directly\nextract comparative relation(s) from text. In this paper, we show that\ncomparative relations can be directly extracted with high accuracy, by\ngenerative model. Based on GPT-2, we propose a Generation-based Comparative\nRelation Extractor (GCRE-GPT). Experiment results show that \\modelname achieves\nstate-of-the-art accuracy on two datasets.", "published": "2023-03-15 13:15:22", "link": "http://arxiv.org/abs/2303.08601v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for\n  Generative Large Language Models", "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nthe responses of black-box models in a zero-resource fashion, i.e. without an\nexternal database. SelfCheckGPT leverages the simple idea that if an LLM has\nknowledge of a given concept, sampled responses are likely to be similar and\ncontain consistent facts. However, for hallucinated facts, stochastically\nsampled responses are likely to diverge and contradict one another. We\ninvestigate this approach by using GPT-3 to generate passages about individuals\nfrom the WikiBio dataset, and manually annotate the factuality of the generated\npassages. We demonstrate that SelfCheckGPT can: i) detect non-factual and\nfactual sentences; and ii) rank passages in terms of factuality. We compare our\napproach to several baselines and show that our approach has considerably\nhigher AUC-PR scores in sentence-level hallucination detection and higher\ncorrelation scores in passage-level factuality assessment compared to grey-box\nmethods.", "published": "2023-03-15 19:31:21", "link": "http://arxiv.org/abs/2303.08896v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PRESTO: A Multilingual Dataset for Parsing Realistic Task-Oriented\n  Dialogs", "abstract": "Research interest in task-oriented dialogs has increased as systems such as\nGoogle Assistant, Alexa and Siri have become ubiquitous in everyday life.\nHowever, the impact of academic research in this area has been limited by the\nlack of datasets that realistically capture the wide array of user pain points.\nTo enable research on some of the more challenging aspects of parsing realistic\nconversations, we introduce PRESTO, a public dataset of over 550K contextual\nmultilingual conversations between humans and virtual assistants. PRESTO\ncontains a diverse array of challenges that occur in real-world NLU tasks such\nas disfluencies, code-switching, and revisions. It is the only large scale\nhuman generated conversational parsing dataset that provides structured context\nsuch as a user's contacts and lists for each example. Our mT5 model based\nbaselines demonstrate that the conversational phenomenon present in PRESTO are\nchallenging to model, which is further pronounced in a low-resource setup.", "published": "2023-03-15 21:51:13", "link": "http://arxiv.org/abs/2303.08954v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Cross-domain Sentiment Classification in Spanish", "abstract": "Sentiment Classification is a fundamental task in the field of Natural\nLanguage Processing, and has very important academic and commercial\napplications. It aims to automatically predict the degree of sentiment present\nin a text that contains opinions and subjectivity at some level, like product\nand movie reviews, or tweets. This can be really difficult to accomplish, in\npart, because different domains of text contains different words and\nexpressions. In addition, this difficulty increases when text is written in a\nnon-English language due to the lack of databases and resources. As a\nconsequence, several cross-domain and cross-language techniques are often\napplied to this task in order to improve the results. In this work we perform a\nstudy on the ability of a classification system trained with a large database\nof product reviews to generalize to different Spanish domains. Reviews were\ncollected from the MercadoLibre website from seven Latin American countries,\nallowing the creation of a large and balanced dataset. Results suggest that\ngeneralization across domains is feasible though very challenging when trained\nwith these product reviews, and can be improved by pre-training and fine-tuning\nthe classification model.", "published": "2023-03-15 23:11:30", "link": "http://arxiv.org/abs/2303.08985v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "DeltaScore: Fine-Grained Story Evaluation with Perturbations", "abstract": "Numerous evaluation metrics have been developed for natural language\ngeneration tasks, but their effectiveness in evaluating stories is limited as\nthey are not specifically tailored to assess intricate aspects of storytelling,\nsuch as fluency and interestingness. In this paper, we introduce DELTASCORE, a\nnovel methodology that employs perturbation techniques for the evaluation of\nnuanced story aspects. Our central proposition posits that the extent to which\na story excels in a specific aspect (e.g., fluency) correlates with the\nmagnitude of its susceptibility to particular perturbations (e.g., the\nintroduction of typos). Given this, we measure the quality of an aspect by\ncalculating the likelihood difference between pre- and post-perturbation states\nusing pre-trained language models. We compare DELTASCORE with existing metrics\non storytelling datasets from two domains in five fine-grained story aspects:\nfluency, coherence, relatedness, logicality, and interestingness. DELTASCORE\ndemonstrates remarkable performance, revealing a surprising finding that a\nspecific perturbation proves highly effective in capturing multiple aspects.", "published": "2023-03-15 23:45:54", "link": "http://arxiv.org/abs/2303.08991v5", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction\n  Benchmark", "abstract": "ChatGPT is a cutting-edge artificial intelligence language model developed by\nOpenAI, which has attracted a lot of attention due to its surprisingly strong\nability in answering follow-up questions. In this report, we aim to evaluate\nChatGPT on the Grammatical Error Correction(GEC) task, and compare it with\ncommercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g.,\nGECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT\nperforms not as well as those baselines in terms of the automatic evaluation\nmetrics (e.g., $F_{0.5}$ score), particularly on long sentences. We inspect the\noutputs and find that ChatGPT goes beyond one-by-one corrections. Specifically,\nit prefers to change the surface expression of certain phrases or sentence\nstructure while maintaining grammatical correctness. Human evaluation\nquantitatively confirms this and suggests that ChatGPT produces less\nunder-correction or mis-correction issues but more over-corrections. These\nresults demonstrate that ChatGPT is severely under-estimated by the automatic\nevaluation metrics and could be a promising tool for GEC.", "published": "2023-03-15 00:35:50", "link": "http://arxiv.org/abs/2303.13648v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Attention-likelihood relationship in transformers", "abstract": "We analyze how large language models (LLMs) represent out-of-context words,\ninvestigating their reliance on the given context to capture their semantics.\nOur likelihood-guided text perturbations reveal a correlation between token\nlikelihood and attention values in transformer-based language models. Extensive\nexperiments reveal that unexpected tokens cause the model to attend less to the\ninformation coming from themselves to compute their representations,\nparticularly at higher layers. These findings have valuable implications for\nassessing the robustness of LLMs in real-world scenarios. Fully reproducible\ncodebase at https://github.com/Flegyas/AttentionLikelihood.", "published": "2023-03-15 00:23:49", "link": "http://arxiv.org/abs/2303.08288v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Rediscovery of CNN's Versatility for Text-based Encoding of Raw\n  Electronic Health Records", "abstract": "Making the most use of abundant information in electronic health records\n(EHR) is rapidly becoming an important topic in the medical domain. Recent work\npresented a promising framework that embeds entire features in raw EHR data\nregardless of its form and medical code standards. The framework, however, only\nfocuses on encoding EHR with minimal preprocessing and fails to consider how to\nlearn efficient EHR representation in terms of computation and memory usage. In\nthis paper, we search for a versatile encoder not only reducing the large data\ninto a manageable size but also well preserving the core information of\npatients to perform diverse clinical tasks. We found that hierarchically\nstructured Convolutional Neural Network (CNN) often outperforms the\nstate-of-the-art model on diverse tasks such as reconstruction, prediction, and\ngeneration, even with fewer parameters and less training time. Moreover, it\nturns out that making use of the inherent hierarchy of EHR data can boost the\nperformance of any kind of backbone models and clinical tasks performed.\nThrough extensive experiments, we present concrete evidence to generalize our\nresearch findings into real-world practice. We give a clear guideline on\nbuilding the encoder based on the research findings captured while exploring\nnumerous settings.", "published": "2023-03-15 00:37:18", "link": "http://arxiv.org/abs/2303.08290v2", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "FactReranker: Fact-guided Reranker for Faithful Radiology Report\n  Summarization", "abstract": "Automatic radiology report summarization is a crucial clinical task, whose\nkey challenge is to maintain factual accuracy between produced summaries and\nground truth radiology findings. Existing research adopts reinforcement\nlearning to directly optimize factual consistency metrics such as CheXBert or\nRadGraph score. However, their decoding method using greedy search or beam\nsearch considers no factual consistency when picking the optimal candidate,\nleading to limited factual consistency improvement. To address it, we propose a\nnovel second-stage summarizing approach FactReranker, the first attempt that\nlearns to choose the best summary from all candidates based on their estimated\nfactual consistency score. We propose to extract medical facts of the input\nmedical report, its gold summary, and candidate summaries based on the RadGraph\nschema and design the fact-guided reranker to efficiently incorporate the\nextracted medical facts for selecting the optimal summary. We decompose the\nfact-guided reranker into the factual knowledge graph generation and the\nfactual scorer, which allows the reranker to model the mapping between the\nmedical facts of the input text and its gold summary, thus can select the\noptimal summary even the gold summary can't be observed during inference. We\nalso present a fact-based ranking metric (RadMRR) for measuring the ability of\nthe reranker on selecting factual consistent candidates. Experimental results\non two benchmark datasets demonstrate the superiority of our method in\ngenerating summaries with higher factual consistency scores when compared with\nexisting methods.", "published": "2023-03-15 02:51:57", "link": "http://arxiv.org/abs/2303.08335v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "The Image of the Process Interpretation of Regular Expressions is Not\n  Closed under Bisimulation Collapse", "abstract": "Axiomatization and expressibility problems for Milner's process semantics\n(1984) of regular expressions modulo bisimilarity have turned out to be\ndifficult for the full class of expressions with deadlock 0 and empty step~1.\nWe report on a phenomenon that arises from the added presence of 1 when 0 is\navailable, and that brings a crucial reason for this difficulty into focus. To\nwit, while interpretations of 1-free regular expressions are closed under\nbisimulation collapse, this is not the case for the interpretations of\narbitrary regular expressions.\n  Process graph interpretations of 1-free regular expressions satisfy the loop\nexistence and elimination property LEE, which is preserved under bisimulation\ncollapse. These features of LEE were applied for showing that an equational\nproof system for 1-free regular expressions modulo bisimilarity is complete,\nand that it is decidable in polynomial time whether a process graph is\nbisimilar to the interpretation of a 1-free regular expression.\n  While interpretations of regular expressions do not satisfy the property LEE\nin general, we show that LEE can be recovered by refined interpretations as\ngraphs with 1-transitions refined interpretations with 1-transitions (which are\nsimilar to silent steps for automata). This suggests that LEE can be expedient\nalso for the general axiomatization and expressibility problems. But a new\nphenomenon emerges that needs to be addressed: the property of a process graph\n`to can be refined into a process graph with 1-transitions and with LEE' is not\npreserved under bisimulation collapse. We provide a 10-vertex graph with two\n1-transitions that satisfies LEE, and in which a pair of bisimilar vertices\ncannot be collapsed on to each other while preserving the refinement property.\nThis implies that the image of the process interpretation of regular\nexpressions is not closed under bisimulation collapse.", "published": "2023-03-15 12:13:30", "link": "http://arxiv.org/abs/2303.08553v2", "categories": ["cs.LO", "cs.CL", "F.4.1"], "primary_category": "cs.LO"}
{"title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a\n  Good Reranker for Hard Samples!", "abstract": "Large Language Models (LLMs) have made remarkable strides in various tasks.\nWhether LLMs are competitive few-shot solvers for information extraction (IE)\ntasks, however, remains an open problem. In this work, we aim to provide a\nthorough answer to this question. Through extensive experiments on nine\ndatasets across four IE tasks, we demonstrate that current advanced LLMs\nconsistently exhibit inferior performance, higher latency, and increased budget\nrequirements compared to fine-tuned SLMs under most settings. Therefore, we\nconclude that LLMs are not effective few-shot information extractors in\ngeneral. Nonetheless, we illustrate that with appropriate prompting strategies,\nLLMs can effectively complement SLMs and tackle challenging samples that SLMs\nstruggle with. And moreover, we propose an adaptive filter-then-rerank paradigm\nto combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as\nfilters and LLMs serve as rerankers. By prompting LLMs to rerank a small\nportion of difficult samples identified by SLMs, our preliminary system\nconsistently achieves promising improvements (2.4% F1-gain on average) on\nvarious IE tasks, with an acceptable time and cost investment.", "published": "2023-03-15 12:20:13", "link": "http://arxiv.org/abs/2303.08559v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Efficient Uncertainty Estimation with Gaussian Process for Reliable\n  Dialog Response Retrieval", "abstract": "Deep neural networks have achieved remarkable performance in retrieval-based\ndialogue systems, but they are shown to be ill calibrated. Though basic\ncalibration methods like Monte Carlo Dropout and Ensemble can calibrate well,\nthese methods are time-consuming in the training or inference stages. To tackle\nthese challenges, we propose an efficient uncertainty calibration framework\nGPF-BERT for BERT-based conversational search, which employs a Gaussian Process\nlayer and the focal loss on top of the BERT architecture to achieve a\nhigh-quality neural ranker. Extensive experiments are conducted to verify the\neffectiveness of our method. In comparison with basic calibration methods,\nGPF-BERT achieves the lowest empirical calibration error (ECE) in three\nin-domain datasets and the distributional shift tasks, while yielding the\nhighest $R_{10}@1$ and MAP performance on most cases. In terms of time\nconsumption, our GPF-BERT has an 8$\\times$ speedup.", "published": "2023-03-15 13:12:16", "link": "http://arxiv.org/abs/2303.08599v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "On the Calibration and Uncertainty with P\u00f3lya-Gamma Augmentation for\n  Dialog Retrieval Models", "abstract": "Deep neural retrieval models have amply demonstrated their power but\nestimating the reliability of their predictions remains challenging. Most\ndialog response retrieval models output a single score for a response on how\nrelevant it is to a given question. However, the bad calibration of deep neural\nnetwork results in various uncertainty for the single score such that the\nunreliable predictions always misinform user decisions. To investigate these\nissues, we present an efficient calibration and uncertainty estimation\nframework PG-DRR for dialog response retrieval models which adds a Gaussian\nProcess layer to a deterministic deep neural network and recovers conjugacy for\ntractable posterior inference by P\\'{o}lya-Gamma augmentation. Finally, PG-DRR\nachieves the lowest empirical calibration error (ECE) in the in-domain datasets\nand the distributional shift task while keeping $R_{10}@1$ and MAP performance.", "published": "2023-03-15 13:26:25", "link": "http://arxiv.org/abs/2303.08606v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automated Query Generation for Evidence Collection from Web Search\n  Engines", "abstract": "It is widely accepted that so-called facts can be checked by searching for\ninformation on the Internet. This process requires a fact-checker to formulate\na search query based on the fact and to present it to a search engine. Then,\nrelevant and believable passages need to be identified in the search results\nbefore a decision is made. This process is carried out by sub-editors at many\nnews and media organisations on a daily basis. Here, we ask the question as to\nwhether it is possible to automate the first step, that of query generation.\nCan we automatically formulate search queries based on factual statements which\nare similar to those formulated by human experts? Here, we consider similarity\nboth in terms of textual similarity and with respect to relevant documents\nbeing returned by a search engine. First, we introduce a moderate-sized\nevidence collection dataset which includes 390 factual statements together with\nassociated human-generated search queries and search results. Then, we\ninvestigate generating queries using a number of rule-based and automatic text\ngeneration methods based on pre-trained large language models (LLMs). We show\nthat these methods have different merits and propose a hybrid approach which\nhas superior performance in practice.", "published": "2023-03-15 14:32:00", "link": "http://arxiv.org/abs/2303.08652v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "GPT-4 Technical Report", "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.", "published": "2023-03-15 17:15:04", "link": "http://arxiv.org/abs/2303.08774v6", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cascading and Direct Approaches to Unsupervised Constituency Parsing on\n  Spoken Sentences", "abstract": "Past work on unsupervised parsing is constrained to written form. In this\npaper, we present the first study on unsupervised spoken constituency parsing\ngiven unlabeled spoken sentences and unpaired textual data. The goal is to\ndetermine the spoken sentences' hierarchical syntactic structure in the form of\nconstituency parse trees, such that each node is a span of audio that\ncorresponds to a constituent. We compare two approaches: (1) cascading an\nunsupervised automatic speech recognition (ASR) model and an unsupervised\nparser to obtain parse trees on ASR transcripts, and (2) direct training an\nunsupervised parser on continuous word-level speech representations. This is\ndone by first splitting utterances into sequences of word-level segments, and\naggregating self-supervised speech representations within segments to obtain\nsegment embeddings. We find that separately training a parser on the unpaired\ntext and directly applying it on ASR transcripts for inference produces better\nresults for unsupervised parsing. Additionally, our results suggest that\naccurate segmentation alone may be sufficient to parse spoken sentences\naccurately. Finally, we show the direct approach may learn head-directionality\ncorrectly for both head-initial and head-final languages without any explicit\ninductive bias.", "published": "2023-03-15 17:57:22", "link": "http://arxiv.org/abs/2303.08809v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "ROSE: A Neurocomputational Architecture for Syntax", "abstract": "A comprehensive model of natural language processing in the brain must\naccommodate four components: representations, operations, structures and\nencoding. It further requires a principled account of how these components\nmechanistically, and causally, relate to each another. While previous models\nhave isolated regions of interest for structure-building and lexical access,\nmany gaps remain with respect to bridging distinct scales of neural complexity.\nBy expanding existing accounts of how neural oscillations can index various\nlinguistic processes, this article proposes a neurocomputational architecture\nfor syntax, termed the ROSE model (Representation, Operation, Structure,\nEncoding). Under ROSE, the basic data structures of syntax are atomic features,\ntypes of mental representations (R), and are coded at the single-unit and\nensemble level. Elementary computations (O) that transform these units into\nmanipulable objects accessible to subsequent structure-building levels are\ncoded via high frequency gamma activity. Low frequency synchronization and\ncross-frequency coupling code for recursive categorial inferences (S). Distinct\nforms of low frequency coupling and phase-amplitude coupling (delta-theta\ncoupling via pSTS-IFG; theta-gamma coupling via IFG to conceptual hubs) then\nencode these structures onto distinct workspaces (E). Causally connecting R to\nO is spike-phase/LFP coupling; connecting O to S is phase-amplitude coupling;\nconnecting S to E is a system of frontotemporal traveling oscillations;\nconnecting E to lower levels is low-frequency phase resetting of spike-LFP\ncoupling. ROSE is reliant on neurophysiologically plausible mechanisms, is\nsupported at all four levels by a range of recent empirical research, and\nprovides an anatomically precise and falsifiable grounding for the basic\nproperty of natural language syntax: hierarchical, recursive\nstructure-building.", "published": "2023-03-15 18:44:37", "link": "http://arxiv.org/abs/2303.08877v1", "categories": ["cs.CL", "q-bio.NC"], "primary_category": "cs.CL"}
{"title": "Applying unsupervised keyphrase methods on concepts extracted from\n  discharge sheets", "abstract": "Clinical notes containing valuable patient information are written by\ndifferent health care providers with various scientific levels and writing\nstyles. It might be helpful for clinicians and researchers to understand what\ninformation is essential when dealing with extensive electronic medical\nrecords. Entities recognizing and mapping them to standard terminologies is\ncrucial in reducing ambiguity in processing clinical notes. Although named\nentity recognition and entity linking are critical steps in clinical natural\nlanguage processing, they can also result in the production of repetitive and\nlow-value concepts. In other hand, all parts of a clinical text do not share\nthe same importance or content in predicting the patient's condition. As a\nresult, it is necessary to identify the section in which each content is\nrecorded and also to identify key concepts to extract meaning from clinical\ntexts. In this study, these challenges have been addressed by using clinical\nnatural language processing techniques. In addition, in order to identify key\nconcepts, a set of popular unsupervised key phrase extraction methods has been\nverified and evaluated. Considering that most of the clinical concepts are in\nthe form of multi-word expressions and their accurate identification requires\nthe user to specify n-gram range, we have proposed a shortcut method to\npreserve the structure of the expression based on TF-IDF. In order to evaluate\nthe pre-processing method and select the concepts, we have designed two types\nof downstream tasks (multiple and binary classification) using the capabilities\nof transformer-based models. The obtained results show the superiority of\nproposed method in combination with SciBERT model, also offer an insight into\nthe efficacy of general extracting essential phrase methods for clinical notes.", "published": "2023-03-15 20:55:25", "link": "http://arxiv.org/abs/2303.08928v1", "categories": ["cs.CL", "cs.LG", "68P20", "H.1; J.3"], "primary_category": "cs.CL"}
{"title": "Reevaluating Data Partitioning for Emotion Detection in EmoWOZ", "abstract": "This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that\nprovides emotion labels for the dialogues. MultiWOZ was partitioned initially\nfor another purpose, resulting in a distributional shift when considering the\nnew purpose of emotion recognition. The emotion tags in EmoWoz are highly\nimbalanced and unevenly distributed across the partitions, which causes\nsub-optimal performance and poor comparison of models. We propose a stratified\nsampling scheme based on emotion tags to address this issue, improve the\ndataset's distribution, and reduce dataset shift. We also introduce a special\ntechnique to handle conversation (sequential) data with many emotional tags.\nUsing our proposed sampling method, models built upon EmoWoz can perform\nbetter, making it a more reliable resource for training conversational agents\nwith emotional intelligence. We recommend that future researchers use this new\npartitioning to ensure consistent and accurate performance evaluations.", "published": "2023-03-15 03:06:13", "link": "http://arxiv.org/abs/2303.13364v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from\n  Comprehensive Study to Low Rank Compensation", "abstract": "Post-training quantization (PTQ) has emerged as a promising technique for\nmitigating memory consumption and computational costs in large language models\n(LLMs). However, a systematic examination of various quantization schemes,\nmodel families, and quantization bit precision has been absent from the\nliterature. In this paper, we conduct a comprehensive analysis of these factors\nby investigating the effects of PTQ on weight-only, activation-only, and\nweight-and-activation quantization using diverse methods such as\nround-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these\nmethods to two distinct model families with parameters ranging from 125M to\n176B. Our contributions include: (1) a sensitivity analysis revealing that\nactivation quantization is generally more susceptible to weight quantization,\nwith smaller models often outperforming larger models in terms of activation\nquantization; (2) an evaluation and comparison of existing PTQ methods to\noptimize model size reduction while minimizing the impact on accuracy,\nrevealing that none of the current methods can achieve the original model\nquality for quantization with either INT4-weight or\nINT4-weight-and-INT8-activation; (3) based on these insights, we propose an\noptimized method called Low-Rank Compensation (LoRC), which employs low-rank\nmatrices to enhance model quality recovery with a minimal increase in model\nsize.", "published": "2023-03-15 01:27:15", "link": "http://arxiv.org/abs/2303.08302v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Cross-speaker Emotion Transfer by Manipulating Speech Style Latents", "abstract": "In recent years, emotional text-to-speech has shown considerable progress.\nHowever, it requires a large amount of labeled data, which is not easily\naccessible. Even if it is possible to acquire an emotional speech dataset,\nthere is still a limitation in controlling emotion intensity. In this work, we\npropose a novel method for cross-speaker emotion transfer and manipulation\nusing vector arithmetic in latent style space. By leveraging only a few labeled\nsamples, we generate emotional speech from reading-style speech without losing\nthe speaker identity. Furthermore, emotion strength is readily controllable\nusing a scalar value, providing an intuitive way for users to manipulate\nspeech. Experimental results show the proposed method affords superior\nperformance in terms of expressiveness, naturalness, and controllability,\npreserving speaker identity.", "published": "2023-03-15 02:34:03", "link": "http://arxiv.org/abs/2303.08329v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP\n  Algorithms on Electronic Health Records", "abstract": "Objective: The generalizability of clinical large language models is usually\nignored during the model development process. This study evaluated the\ngeneralizability of BERT-based clinical NLP models across different clinical\nsettings through a breast cancer phenotype extraction task.\n  Materials and Methods: Two clinical corpora of breast cancer patients were\ncollected from the electronic health records from the University of Minnesota\nand the Mayo Clinic, and annotated following the same guideline. We developed\nthree types of NLP models (i.e., conditional random field, bi-directional long\nshort-term memory and CancerBERT) to extract cancer phenotypes from clinical\ntexts. The models were evaluated for their generalizability on different test\nsets with different learning strategies (model transfer vs. locally trained).\nThe entity coverage score was assessed with their association with the model\nperformances.\n  Results: We manually annotated 200 and 161 clinical documents at UMN and MC,\nrespectively. The corpora of the two institutes were found to have higher\nsimilarity between the target entities than the overall corpora. The CancerBERT\nmodels obtained the best performances among the independent test sets from two\nclinical institutes and the permutation test set. The CancerBERT model\ndeveloped in one institute and further fine-tuned in another institute achieved\nreasonable performance compared to the model developed on local data (micro-F1:\n0.925 vs 0.932).\n  Conclusions: The results indicate the CancerBERT model has the best learning\nability and generalizability among the three types of clinical NLP models. The\ngeneralizability of the models was found to be correlated with the similarity\nof the target entities between the corpora.", "published": "2023-03-15 08:44:07", "link": "http://arxiv.org/abs/2303.08448v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Mirror: A Natural Language Interface for Data Querying, Summarization,\n  and Visualization", "abstract": "We present Mirror, an open-source platform for data exploration and analysis\npowered by large language models. Mirror offers an intuitive natural language\ninterface for querying databases, and automatically generates executable SQL\ncommands to retrieve relevant data and summarize it in natural language. In\naddition, users can preview and manually edit the generated SQL commands to\nensure the accuracy of their queries. Mirror also generates visualizations to\nfacilitate understanding of the data. Designed with flexibility and human input\nin mind, Mirror is suitable for both experienced data analysts and\nnon-technical professionals looking to gain insights from their data.", "published": "2023-03-15 15:31:51", "link": "http://arxiv.org/abs/2303.08697v1", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.SE"], "primary_category": "cs.DB"}
{"title": "Artificial Influence: An Analysis Of AI-Driven Persuasion", "abstract": "Persuasion is a key aspect of what it means to be human, and is central to\nbusiness, politics, and other endeavors. Advancements in artificial\nintelligence (AI) have produced AI systems that are capable of persuading\nhumans to buy products, watch videos, click on search results, and more. Even\nsystems that are not explicitly designed to persuade may do so in practice. In\nthe future, increasingly anthropomorphic AI systems may form ongoing\nrelationships with users, increasing their persuasive power. This paper\ninvestigates the uncertain future of persuasive AI systems. We examine ways\nthat AI could qualitatively alter our relationship to and views regarding\npersuasion by shifting the balance of persuasive power, allowing personalized\npersuasion to be deployed at scale, powering misinformation campaigns, and\nchanging the way humans can shape their own discourse. We consider ways\nAI-driven persuasion could differ from human-driven persuasion. We warn that\nubiquitous highlypersuasive AI systems could alter our information environment\nso significantly so as to contribute to a loss of human control of our own\nfuture. In response, we examine several potential responses to AI-driven\npersuasion: prohibition, identification of AI agents, truthful AI, and legal\nremedies. We conclude that none of these solutions will be airtight, and that\nindividuals and governments will need to take active steps to guard against the\nmost pernicious effects of persuasive AI.", "published": "2023-03-15 16:05:11", "link": "http://arxiv.org/abs/2303.08721v1", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CY"}
{"title": "Understanding Post-hoc Explainers: The Case of Anchors", "abstract": "In many scenarios, the interpretability of machine learning models is a\nhighly required but difficult task. To explain the individual predictions of\nsuch models, local model-agnostic approaches have been proposed. However, the\nprocess generating the explanations can be, for a user, as mysterious as the\nprediction to be explained. Furthermore, interpretability methods frequently\nlack theoretical guarantees, and their behavior on simple models is frequently\nunknown. While it is difficult, if not impossible, to ensure that an explainer\nbehaves as expected on a cutting-edge model, we can at least ensure that\neverything works on simple, already interpretable models. In this paper, we\npresent a theoretical analysis of Anchors (Ribeiro et al., 2018): a popular\nrule-based interpretability method that highlights a small set of words to\nexplain a text classifier's decision. After formalizing its algorithm and\nproviding useful insights, we demonstrate mathematically that Anchors produces\nmeaningful results when used with linear text classifiers on top of a TF-IDF\nvectorization. We believe that our analysis framework can aid in the\ndevelopment of new explainability methods based on solid theoretical\nfoundations.", "published": "2023-03-15 17:56:34", "link": "http://arxiv.org/abs/2303.08806v1", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "stat.ML"}
{"title": "A Formalization of Operads in Coq", "abstract": "What provides the highest level of assurance for correctness of execution\nwithin a programming language? One answer, and our solution in particular, to\nthis problem is to provide a formalization for, if it exists, the denotational\nsemantics of a programming language. Achieving such a formalization provides a\ngold standard for ensuring a programming language is correct-by-construction.\nIn our effort on the DARPA V-SPELLS program, we worked to provide a foundation\nfor the denotational semantics of a meta-language using a mathematical object\nknown as an operad. This object has compositional properties which are vital to\nbuilding languages from smaller pieces. In this paper, we discuss our\nformalization of an operad in the proof assistant Coq. Moreover, our definition\nwithin Coq is capable of providing proofs that objects specified within Coq are\noperads. This work within Coq provides a formal mathematical basis for our\nmeta-language development within V-SPELLS. Our work also provides, to our\nknowledge, the first known formalization of operads within a proof assistant\nthat has significant automation, as well as a model that can be replicated\nwithout knowledge of Homotopy Type Theory.", "published": "2023-03-15 19:29:40", "link": "http://arxiv.org/abs/2303.08894v1", "categories": ["math.CT", "cs.CL", "cs.PL"], "primary_category": "math.CT"}
{"title": "Finding Similar Exercises in Retrieval Manner", "abstract": "When students make a mistake in an exercise, they can consolidate it by\n``similar exercises'' which have the same concepts, purposes and methods.\nCommonly, for a certain subject and study stage, the size of the exercise bank\nis in the range of millions to even tens of millions, how to find similar\nexercises for a given exercise becomes a crucial technical problem. Generally,\nwe can assign a variety of explicit labels to the exercise, and then query\nthrough the labels, but the label annotation is time-consuming, laborious and\ncostly, with limited precision and granularity, so it is not feasible. In\npractice, we define ``similar exercises'' as a retrieval process of finding a\nset of similar exercises based on recall, ranking and re-rank procedures,\ncalled the \\textbf{FSE} problem (Finding similar exercises). Furthermore,\ncomprehensive representation of the semantic information of exercises was\nobtained through representation learning. In addition to the reasonable\narchitecture, we also explore what kind of tasks are more conducive to the\nlearning of exercise semantic information from pre-training and supervised\nlearning. It is difficult to annotate similar exercises and the annotation\nconsistency among experts is low. Therefore this paper also provides solutions\nto solve the problem of low-quality annotated data. Compared with other\nmethods, this paper has obvious advantages in both architecture rationality and\nalgorithm precision, which now serves the daily teaching of hundreds of\nschools.", "published": "2023-03-15 01:40:32", "link": "http://arxiv.org/abs/2303.11163v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CY"], "primary_category": "cs.IR"}
{"title": "A practical distributed active noise control algorithm overcoming\n  communication restrictions", "abstract": "By assigning the massive computing tasks of the traditional multichannel\nactive noise control (MCANC) system to several distributed control nodes,\ndistributed multichannel active noise control (DMCANC) techniques have become\neffective global noise reduction solutions with low computational costs.\nHowever, existing DMCANC algorithms simply complete the distribution of\ntraditional centralized algorithms by combining neighbour nodes' information\nbut rarely consider the degraded control performance and system stability of\ndistributed units caused by delays and interruptions in communication. Hence,\nthis paper develops a novel DMCANC algorithm that utilizes the compensation\nfilters and neighbour nodes' information to counterbalance the cross-talk\neffect between channels while maintaining independent weight updating. Since\nthe neighbours' information required barely affects the local control filter\nupdating in each node, this approach can tolerate communication delay and\ninterruption to some extent. Numerical simulations demonstrate that the\nproposed algorithm can achieve satisfactory noise reduction performance and\nhigh robustness to real-world communication challenges.", "published": "2023-03-15 07:25:16", "link": "http://arxiv.org/abs/2303.08411v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Autonomous Soundscape Augmentation with Multimodal Fusion of Visual and\n  Participant-linked Inputs", "abstract": "Autonomous soundscape augmentation systems typically use trained models to\npick optimal maskers to effect a desired perceptual change. While acoustic\ninformation is paramount to such systems, contextual information, including\nparticipant demographics and the visual environment, also influences acoustic\nperception. Hence, we propose modular modifications to an existing\nattention-based deep neural network, to allow early, mid-level, and late\nfeature fusion of participant-linked, visual, and acoustic features. Ablation\nstudies on module configurations and corresponding fusion methods using the\nARAUS dataset show that contextual features improve the model performance in a\nstatistically significant manner on the normalized ISO Pleasantness, to a mean\nsquared error of $0.1194\\pm0.0012$ for the best-performing all-modality model,\nagainst $0.1217\\pm0.0009$ for the audio-only model. Soundscape augmentation\nsystems can thereby leverage multimodal inputs for improved performance. We\nalso investigate the impact of individual participant-linked factors using\ntrained models to illustrate improvements in model explainability.", "published": "2023-03-15 03:17:31", "link": "http://arxiv.org/abs/2303.08342v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Target Sound Extraction with Variable Cross-modality Clues", "abstract": "Automatic target sound extraction (TSE) is a machine learning approach to\nmimic the human auditory perception capability of attending to a sound source\nof interest from a mixture of sources. It often uses a model conditioned on a\nfixed form of target sound clues, such as a sound class label, which limits the\nways in which users can interact with the model to specify the target sounds.\nTo leverage variable number of clues cross modalities available in the\ninference phase, including a video, a sound event class, and a text caption, we\npropose a unified transformer-based TSE model architecture, where a multi-clue\nattention module integrates all the clues across the modalities. Since there is\nno off-the-shelf benchmark to evaluate our proposed approach, we build a\ndataset based on public corpora, Audioset and AudioCaps. Experimental results\nfor seen and unseen target-sound evaluation sets show that our proposed TSE\nmodel can effectively deal with a varying number of clues which improves the\nTSE performance and robustness against partially compromised clues.", "published": "2023-03-15 05:17:53", "link": "http://arxiv.org/abs/2303.08372v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Implementing Continuous HRTF Measurement in Near-Field", "abstract": "Head-related transfer function (HRTF) is an essential component to create an\nimmersive listening experience over headphones for virtual reality (VR) and\naugmented reality (AR) applications. Metaverse combines VR and AR to create\nimmersive digital experiences, and users are very likely to interact with\nvirtual objects in the near-field (NF). The HRTFs of such objects are highly\nindividualized and dependent on directions and distances. Hence, a significant\nnumber of HRTF measurements at different distances in the NF would be needed.\nUsing conventional static stop-and-go HRTF measurement methods to acquire these\nmeasurements would be time-consuming and tedious for human listeners. In this\npaper, we propose a continuous measurement system targeted for the NF, and\nefficiently capturing HRTFs in the horizontal plane within 45 secs. Comparative\nexperiments are performed on head and torso simulator (HATS) and human\nlisteners to evaluate system consistency and robustness.", "published": "2023-03-15 05:41:35", "link": "http://arxiv.org/abs/2303.08379v4", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Momentum Two-gradient Direction Algorithm with Variable Step Size\n  Applied to Solve Practical Output Constraint Issue for Active Noise Control", "abstract": "Active noise control (ANC) has been widely utilized to reduce unwanted\nenvironmental noise. The primary objective of ANC is to generate an anti-noise\nwith the same amplitude but the opposite phase of the primary noise using the\nsecondary source. However, the effectiveness of the ANC application is impacted\nby the speaker's output saturation. This paper proposes a two-gradient\ndirection ANC algorithm with a momentum factor to solve the saturation with\nfaster convergence. In order to make it implemented in real-time, a\ncomputation-effective variable step size approach is applied to further reduce\nthe steady-state error brought on by the changing gradient directions. The time\nconstant and step size bound for the momentum two-gradient direction algorithm\nis analyzed. Simulation results show that the proposed algorithm performs\neffectively in the time-unvaried and time-varied environment.", "published": "2023-03-15 07:01:01", "link": "http://arxiv.org/abs/2303.08397v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Acoustic source localization in the spherical harmonics domain\n  exploiting low-rank approximations", "abstract": "Acoustic signal processing in the spherical harmonics domain (SHD) is an\nactive research area that exploits the signals acquired by higher order\nmicrophone arrays. A very important task is that concerning the localization of\nactive sound sources. In this paper, we propose a simple yet effective method\nto localize prominent acoustic sources in adverse acoustic scenarios. By using\na proper normalization and arrangement of the estimated spherical harmonic\ncoefficients, we exploit low-rank approximations to estimate the far field\nmodal directional pattern of the dominant source at each time-frame. The\nexperiments confirm the validity of the proposed approach, with superior\nperformance compared to other recent SHD-based approaches.", "published": "2023-03-15 09:35:57", "link": "http://arxiv.org/abs/2303.08480v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Enhancing Unsupervised Audio Representation Learning via Adversarial\n  Sample Generation", "abstract": "Existing audio analysis methods generally first transform the audio stream to\nspectrogram, and then feed it into CNN for further analysis. A standard CNN\nrecognizes specific visual patterns over feature map, then pools for high-level\nrepresentation, which overlooks the positional information of recognized\npatterns. However, unlike natural image, the semantic of an audio spectrogram\nis sensitive to positional change, as its vertical and horizontal axes indicate\nthe frequency and temporal information of the audio, instead of naive\nrectangular coordinates. Thus, the insensitivity of CNN to positional change\nplays a negative role on audio spectrogram encoding. To address this issue,\nthis paper proposes a new self-supervised learning mechanism, which enhances\nthe audio representation by first generating adversarial samples\n(\\textit{i.e.}, negative samples), then driving CNN to distinguish the\nembeddings of negative pairs in the latent space. Extensive experiments show\nthat the proposed approach achieves best or competitive results on 9 downstream\ndatasets compared with previous methods, which verifies its effectiveness on\naudio representation learning.", "published": "2023-03-15 12:25:29", "link": "http://arxiv.org/abs/2303.08561v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "PHONEix: Acoustic Feature Processing Strategy for Enhanced Singing\n  Pronunciation with Phoneme Distribution Predictor", "abstract": "Singing voice synthesis (SVS), as a specific task for generating the vocal\nsinging voice from a music score, has drawn much attention in recent years. SVS\nfaces the challenge that the singing has various pronunciation flexibility\nconditioned on the same music score. Most of the previous works of SVS can not\nwell handle the misalignment between the music score and actual singing. In\nthis paper, we propose an acoustic feature processing strategy, named PHONEix,\nwith a phoneme distribution predictor, to alleviate the gap between the music\nscore and the singing voice, which can be easily adopted in different SVS\nsystems. Extensive experiments in various settings demonstrate the\neffectiveness of our PHONEix in both objective and subjective evaluations.", "published": "2023-03-15 13:27:00", "link": "http://arxiv.org/abs/2303.08607v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Blind Estimation of Audio Processing Graph", "abstract": "Musicians and audio engineers sculpt and transform their sounds by connecting\nmultiple processors, forming an audio processing graph. However, most\ndeep-learning methods overlook this real-world practice and assume fixed graph\nsettings. To bridge this gap, we develop a system that reconstructs the entire\ngraph from a given reference audio. We first generate a realistic\ngraph-reference pair dataset and train a simple blind estimation system\ncomposed of a convolutional reference encoder and a transformer-based graph\ndecoder. We apply our model to singing voice effects and drum mixing estimation\ntasks. Evaluation results show that our method can reconstruct complex signal\nroutings, including multi-band processing and sidechaining.", "published": "2023-03-15 13:31:56", "link": "http://arxiv.org/abs/2303.08610v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "HYBRIDFORMER: improving SqueezeFormer with hybrid attention and NSR\n  mechanism", "abstract": "SqueezeFormer has recently shown impressive performance in automatic speech\nrecognition (ASR). However, its inference speed suffers from the quadratic\ncomplexity of softmax-attention (SA). In addition, limited by the large\nconvolution kernel size, the local modeling ability of SqueezeFormer is\ninsufficient. In this paper, we propose a novel method HybridFormer to improve\nSqueezeFormer in a fast and efficient way. Specifically, we first incorporate\nlinear attention (LA) and propose a hybrid LASA paradigm to increase the\nmodel's inference speed. Second, a hybrid neural architecture search (NAS)\nguided structural re-parameterization (SRep) mechanism, termed NSR, is proposed\nto enhance the ability of the model to extract local interactions. Extensive\nexperiments conducted on the LibriSpeech dataset demonstrate that our proposed\nHybridFormer can achieve a 9.1% relative word error rate (WER) reduction over\nSqueezeFormer on the test-other dataset. Furthermore, when input speech is 30s,\nthe HybridFormer can improve the model's inference speed up to 18%. Our source\ncode is available online.", "published": "2023-03-15 14:03:16", "link": "http://arxiv.org/abs/2303.08636v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speech Signal Improvement Using Causal Generative Diffusion Models", "abstract": "In this paper, we present a causal speech signal improvement system that is\ndesigned to handle different types of distortions. The method is based on a\ngenerative diffusion model which has been shown to work well in scenarios with\nmissing data and non-linear corruptions. To guarantee causal processing, we\nmodify the network architecture of our previous work and replace global\nnormalization with causal adaptive gain control. We generate diverse training\ndata containing a broad range of distortions. This work was performed in the\ncontext of an \"ICASSP Signal Processing Grand Challenge\" and submitted to the\nnon-real-time track of the \"Speech Signal Improvement Challenge 2023\", where it\nwas ranked fifth.", "published": "2023-03-15 14:58:40", "link": "http://arxiv.org/abs/2303.08674v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Beamformer-Guided Target Speaker Extraction", "abstract": "We propose a Beamformer-guided Target Speaker Extraction (BG-TSE) method to\nextract a target speaker's voice from a multi-channel recording informed by the\ndirection of arrival of the target. The proposed method employs a front-end\nbeamformer steered towards the target speaker to provide an auxiliary signal to\na single-channel TSE system. By allowing for time-varying embeddings in the\nsingle-channel TSE block, the proposed method fully exploits the correspondence\nbetween the front-end beamformer output and the target speech in the microphone\nsignal. Experimental evaluation on simulated multi-channel 2-speaker mixtures,\nin both anechoic and reverberant conditions, demonstrates the advantage of the\nproposed method compared to recent single-channel and multi-channel baselines.", "published": "2023-03-15 15:37:55", "link": "http://arxiv.org/abs/2303.08702v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Subspace Hybrid Beamforming for Head-worn Microphone Arrays", "abstract": "A two-stage multi-channel speech enhancement method is proposed which\nconsists of a novel adaptive beamformer, Hybrid Minimum Variance Distortionless\nResponse (MVDR), Isotropic-MVDR (Iso), and a novel multi-channel spectral\nPrincipal Components Analysis (PCA) denoising. In the first stage, the\nHybrid-MVDR performs multiple MVDRs using a dictionary of pre-defined noise\nfield models and picks the minimum-power outcome, which benefits from the\nrobustness of signal-independent beamforming and the performance of adaptive\nbeamforming. In the second stage, the outcomes of Hybrid and Iso are jointly\nused in a two-channel PCA-based denoising to remove the 'musical noise'\nproduced by Hybrid beamformer. On a dataset of real 'cocktail-party' recordings\nwith head-worn array, the proposed method outperforms the baseline\nsuperdirective beamformer in noise suppression (fwSegSNR, SDR, SIR, SAR) and\nspeech intelligibility (STOI) with similar speech quality (PESQ) improvement.", "published": "2023-03-15 22:33:36", "link": "http://arxiv.org/abs/2303.08967v1", "categories": ["eess.AS", "eess.SP"], "primary_category": "eess.AS"}
{"title": "A large-scale multimodal dataset of human speech recognition", "abstract": "Nowadays, non-privacy small-scale motion detection has attracted an\nincreasing amount of research in remote sensing in speech recognition. These\nnew modalities are employed to enhance and restore speech information from\nspeakers of multiple types of data. In this paper, we propose a dataset\ncontains 7.5 GHz Channel Impulse Response (CIR) data from ultra-wideband (UWB)\nradars, 77-GHz frequency modulated continuous wave (FMCW) data from millimetre\nwave (mmWave) radar, and laser data. Meanwhile, a depth camera is adopted to\nrecord the landmarks of the subject's lip and voice. Approximately 400 minutes\nof annotated speech profiles are provided, which are collected from 20\nparticipants speaking 5 vowels, 15 words and 16 sentences. The dataset has been\nvalidated and has potential for the research of lip reading and multimodal\nspeech recognition.", "published": "2023-03-15 01:06:17", "link": "http://arxiv.org/abs/2303.08295v1", "categories": ["eess.SP", "cs.SD", "eess.AS"], "primary_category": "eess.SP"}
{"title": "Sharing Low Rank Conformer Weights for Tiny Always-On Ambient Speech\n  Recognition Models", "abstract": "Continued improvements in machine learning techniques offer exciting new\nopportunities through the use of larger models and larger training datasets.\nHowever, there is a growing need to offer these new capabilities on-board\nlow-powered devices such as smartphones, wearables and other embedded\nenvironments where only low memory is available. Towards this, we consider\nmethods to reduce the model size of Conformer-based speech recognition models\nwhich typically require models with greater than 100M parameters down to just\n$5$M parameters while minimizing impact on model quality. Such a model allows\nus to achieve always-on ambient speech recognition on edge devices with\nlow-memory neural processors. We propose model weight reuse at different levels\nwithin our model architecture: (i) repeating full conformer block layers, (ii)\nsharing specific conformer modules across layers, (iii) sharing sub-components\nper conformer module, and (iv) sharing decomposed sub-component weights after\nlow-rank decomposition. By sharing weights at different levels of our model, we\ncan retain the full model in-memory while increasing the number of virtual\ntransformations applied to the input. Through a series of ablation studies and\nevaluations, we find that with weight sharing and a low-rank architecture, we\ncan achieve a WER of 2.84 and 2.94 for Librispeech dev-clean and test-clean\nrespectively with a $5$M parameter model.", "published": "2023-03-15 03:21:38", "link": "http://arxiv.org/abs/2303.08343v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Transfer Learning Based Diagnosis and Analysis of Lung Sound Aberrations", "abstract": "With the development of computer -systems that can collect and analyze\nenormous volumes of data, the medical profession is establishing several\nnon-invasive tools. This work attempts to develop a non-invasive technique for\nidentifying respiratory sounds acquired by a stethoscope and voice recording\nsoftware via machine learning techniques. This study suggests a trained and\nproven CNN-based approach for categorizing respiratory sounds. A visual\nrepresentation of each audio sample is constructed, allowing resource\nidentification for classification using methods like those used to effectively\ndescribe visuals. We used a technique called Mel Frequency Cepstral\nCoefficients (MFCCs). Here, features are retrieved and categorized via VGG16\n(transfer learning) and prediction is accomplished using 5-fold\ncross-validation. Employing various data splitting techniques, Respiratory\nSound Database obtained cutting-edge results, including accuracy of 95%,\nprecision of 88%, recall score of 86%, and F1 score of 81%. The ICBHI dataset\nis used to train and test the model.", "published": "2023-03-15 04:46:57", "link": "http://arxiv.org/abs/2303.08362v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Generating symbolic music using diffusion models", "abstract": "Denoising Diffusion Probabilistic models have emerged as simple yet very\npowerful generative models. Unlike other generative models, diffusion models do\nnot suffer from mode collapse or require a discriminator to generate\nhigh-quality samples. In this paper, a diffusion model that uses a binomial\nprior distribution to generate piano rolls is proposed. The paper also proposes\nan efficient method to train the model and generate samples. The generated\nmusic has coherence at time scales up to the length of the training piano roll\nsegments. The paper demonstrates how this model is conditioned on the input and\ncan be used to harmonize a given melody, complete an incomplete piano roll, or\ngenerate a variation of a given piece. The code is publicly shared to encourage\nthe use and development of the method by the community.", "published": "2023-03-15 06:01:02", "link": "http://arxiv.org/abs/2303.08385v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Watch or Listen: Robust Audio-Visual Speech Recognition with Visual\n  Corruption Modeling and Reliability Scoring", "abstract": "This paper deals with Audio-Visual Speech Recognition (AVSR) under multimodal\ninput corruption situations where audio inputs and visual inputs are both\ncorrupted, which is not well addressed in previous research directions.\nPrevious studies have focused on how to complement the corrupted audio inputs\nwith the clean visual inputs with the assumption of the availability of clean\nvisual inputs. However, in real life, clean visual inputs are not always\naccessible and can even be corrupted by occluded lip regions or noises. Thus,\nwe firstly analyze that the previous AVSR models are not indeed robust to the\ncorruption of multimodal input streams, the audio and the visual inputs,\ncompared to uni-modal models. Then, we design multimodal input corruption\nmodeling to develop robust AVSR models. Lastly, we propose a novel AVSR\nframework, namely Audio-Visual Reliability Scoring module (AV-RelScore), that\nis robust to the corrupted multimodal inputs. The AV-RelScore can determine\nwhich input modal stream is reliable or not for the prediction and also can\nexploit the more reliable streams in prediction. The effectiveness of the\nproposed method is evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that the reliability scores\nobtained by AV-RelScore well reflect the degree of corruption and make the\nproposed model focus on the reliable multimodal representations.", "published": "2023-03-15 11:29:36", "link": "http://arxiv.org/abs/2303.08536v2", "categories": ["cs.MM", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
