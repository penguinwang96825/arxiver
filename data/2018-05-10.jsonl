{"title": "Discourse-Aware Neural Rewards for Coherent Text Generation", "abstract": "In this paper, we investigate the use of discourse-aware rewards with\nreinforcement learning to guide a model to generate long, coherent text. In\nparticular, we propose to learn neural rewards to model cross-sentence ordering\nas a means to approximate desired discourse structure. Empirical results\ndemonstrate that a generator trained with the learned reward produces more\ncoherent and less repetitive text than models trained with cross-entropy or\nwith reinforcement learning with commonly used scores as rewards.", "published": "2018-05-10 00:51:06", "link": "http://arxiv.org/abs/1805.03766v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "The Evolution of Popularity and Images of Characters in Marvel Cinematic\n  Universe Fanfictions", "abstract": "This analysis proposes a new topic model to study the yearly trends in Marvel\nCinematic Universe fanfictions on three levels: character popularity, character\nimages/topics, and vocabulary pattern of topics. It is found that character\nappearances in fanfictions have become more diverse over the years thanks to\nconstant introduction of new characters in feature films, and in the case of\nCaptain America, multi-dimensional character development is well-received by\nthe fanfiction world.", "published": "2018-05-10 01:27:58", "link": "http://arxiv.org/abs/1805.03774v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SlugNERDS: A Named Entity Recognition Tool for Open Domain Dialogue\n  Systems", "abstract": "In dialogue systems, the tasks of named entity recognition (NER) and named\nentity linking (NEL) are vital preprocessing steps for understanding user\nintent, especially in open domain interaction where we cannot rely on\ndomain-specific inference. UCSC's effort as one of the funded teams in the 2017\nAmazon Alexa Prize Contest has yielded Slugbot, an open domain social bot,\naimed at casual conversation. We discovered several challenges specifically\nassociated with both NER and NEL when building Slugbot, such as that the NE\nlabels are too coarse-grained or the entity types are not linked to a useful\nontology. Moreover, we have discovered that traditional approaches do not\nperform well in our context: even systems designed to operate on tweets or\nother social media data do not work well in dialogue systems. In this paper, we\nintroduce Slugbot's Named Entity Recognition for dialogue Systems (SlugNERDS),\na NER and NEL tool which is optimized to address these issues. We describe two\nnew resources that we are building as part of this work: SlugEntityDB and\nSchemaActuator. We believe these resources will be useful for the research\ncommunity.", "published": "2018-05-10 02:07:02", "link": "http://arxiv.org/abs/1805.03784v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Training Classifiers with Natural Language Explanations", "abstract": "Training accurate classifiers requires many labels, but each label provides\nonly limited information (one bit for binary classification). In this work, we\npropose BabbleLabble, a framework for training classifiers in which an\nannotator provides a natural language explanation for each labeling decision. A\nsemantic parser converts these explanations into programmatic labeling\nfunctions that generate noisy labels for an arbitrary amount of unlabeled data,\nwhich is used to train a classifier. On three relation extraction tasks, we\nfind that users are able to train classifiers with comparable F1 scores from\n5-100$\\times$ faster by providing explanations instead of just labels.\nFurthermore, given the inherent imperfection of labeling functions, we find\nthat a simple rule-based semantic parser suffices.", "published": "2018-05-10 04:59:59", "link": "http://arxiv.org/abs/1805.03818v4", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Hybrid semi-Markov CRF for Neural Sequence Labeling", "abstract": "This paper proposes hybrid semi-Markov conditional random fields (SCRFs) for\nneural sequence labeling in natural language processing. Based on conventional\nconditional random fields (CRFs), SCRFs have been designed for the tasks of\nassigning labels to segments by extracting features from and describing\ntransitions between segments instead of words. In this paper, we improve the\nexisting SCRF methods by employing word-level and segment-level information\nsimultaneously. First, word-level labels are utilized to derive the segment\nscores in SCRFs. Second, a CRF output layer and an SCRF output layer are\nintegrated into an unified neural network and trained jointly. Experimental\nresults on CoNLL 2003 named entity recognition (NER) shared task show that our\nmodel achieves state-of-the-art performance when no external knowledge is used.", "published": "2018-05-10 06:14:36", "link": "http://arxiv.org/abs/1805.03838v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Obligation and Prohibition Extraction Using Hierarchical RNNs", "abstract": "We consider the task of detecting contractual obligations and prohibitions.\nWe show that a self-attention mechanism improves the performance of a BILSTM\nclassifier, the previous state of the art for this task, by allowing it to\nfocus on indicative tokens. We also introduce a hierarchical BILSTM, which\nconverts each sentence to an embedding, and processes the sentence embeddings\nto classify each sentence. Apart from being faster to train, the hierarchical\nBILSTM outperforms the flat one, even when the latter considers surrounding\nsentences, because the hierarchical model has a broader discourse view.", "published": "2018-05-10 08:04:17", "link": "http://arxiv.org/abs/1805.03871v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improv Chat: Second Response Generation for Chatbot", "abstract": "Existing research on response generation for chatbot focuses on \\textbf{First\nResponse Generation} which aims to teach the chatbot to say the first response\n(e.g. a sentence) appropriate to the conversation context (e.g. the user's\nquery). In this paper, we introduce a new task \\textbf{Second Response\nGeneration}, termed as Improv chat, which aims to teach the chatbot to say the\nsecond response after saying the first response with respect the conversation\ncontext, so as to lighten the burden on the user to keep the conversation\ngoing. Specifically, we propose a general learning based framework and develop\na retrieval based system which can generate the second responses with the\nusers' query and the chatbot's first response as input. We present the approach\nto building the conversation corpus for Improv chat from public forums and\nsocial networks, as well as the neural networks based models for response\nmatching and ranking. We include the preliminary experiments and results in\nthis paper. This work could be further advanced with better deep matching\nmodels for retrieval base systems or generative models for generation based\nsystems as well as extensive evaluations in real-life applications.", "published": "2018-05-10 09:22:56", "link": "http://arxiv.org/abs/1805.03900v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Academic Paper Rating Based on Modularized Hierarchical\n  Convolutional Neural Network", "abstract": "As more and more academic papers are being submitted to conferences and\njournals, evaluating all these papers by professionals is time-consuming and\ncan cause inequality due to the personal factors of the reviewers. In this\npaper, in order to assist professionals in evaluating academic papers, we\npropose a novel task: automatic academic paper rating (AAPR), which\nautomatically determine whether to accept academic papers. We build a new\ndataset for this task and propose a novel modularized hierarchical\nconvolutional neural network to achieve automatic academic paper rating.\nEvaluation results show that the proposed model outperforms the baselines by a\nlarge margin. The dataset and code are available at\n\\url{https://github.com/lancopku/AAPR}", "published": "2018-05-10 13:42:29", "link": "http://arxiv.org/abs/1805.03977v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Automatic Estimation of Simultaneous Interpreter Performance", "abstract": "Simultaneous interpretation, translation of the spoken word in real-time, is\nboth highly challenging and physically demanding. Methods to predict\ninterpreter confidence and the adequacy of the interpreted message have a\nnumber of potential applications, such as in computer-assisted interpretation\ninterfaces or pedagogical tools. We propose the task of predicting simultaneous\ninterpreter performance by building on existing methodology for quality\nestimation (QE) of machine translation output. In experiments over five\nsettings in three language pairs, we extend a QE pipeline to estimate\ninterpreter performance (as approximated by the METEOR evaluation metric) and\npropose novel features reflecting interpretation strategy and evaluation\nmeasures that further improve prediction accuracy.", "published": "2018-05-10 15:07:22", "link": "http://arxiv.org/abs/1805.04016v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Regularizing Output Distribution of Abstractive Chinese Social Media\n  Text Summarization for Improved Semantic Consistency", "abstract": "Abstractive text summarization is a highly difficult problem, and the\nsequence-to-sequence model has shown success in improving the performance on\nthe task. However, the generated summaries are often inconsistent with the\nsource content in semantics. In such cases, when generating summaries, the\nmodel selects semantically unrelated words with respect to the source content\nas the most probable output. The problem can be attributed to heuristically\nconstructed training data, where summaries can be unrelated to the source\ncontent, thus containing semantically unrelated words and spurious word\ncorrespondence. In this paper, we propose a regularization approach for the\nsequence-to-sequence model and make use of what the model has learned to\nregularize the learning objective to alleviate the effect of the problem. In\naddition, we propose a practical human evaluation method to address the problem\nthat the existing automatic evaluation method does not evaluate the semantic\nconsistency with the source content properly. Experimental results demonstrate\nthe effectiveness of the proposed approach, which outperforms almost all the\nexisting models. Especially, the proposed approach improves the semantic\nconsistency by 4\\% in terms of human evaluation.", "published": "2018-05-10 15:58:09", "link": "http://arxiv.org/abs/1805.04033v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "End-to-End Reinforcement Learning for Automatic Taxonomy Induction", "abstract": "We present a novel end-to-end reinforcement learning approach to automatic\ntaxonomy induction from a set of terms. While prior methods treat the problem\nas a two-phase task (i.e., detecting hypernymy pairs followed by organizing\nthese pairs into a tree-structured hierarchy), we argue that such two-phase\nmethods may suffer from error propagation, and cannot effectively optimize\nmetrics that capture the holistic structure of a taxonomy. In our approach, the\nrepresentations of term pairs are learned using multiple sources of information\nand used to determine \\textit{which} term to select and \\textit{where} to place\nit on the taxonomy via a policy network. All components are trained in an\nend-to-end manner with cumulative rewards, measured by a holistic tree metric\nover the training taxonomies. Experiments on two public datasets of different\ndomains show that our approach outperforms prior state-of-the-art taxonomy\ninduction methods up to 19.6\\% on ancestor F1.", "published": "2018-05-10 16:19:14", "link": "http://arxiv.org/abs/1805.04044v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Deep Neural Machine Translation with Weakly-Recurrent Units", "abstract": "Recurrent neural networks (RNNs) have represented for years the state of the\nart in neural machine translation. Recently, new architectures have been\nproposed, which can leverage parallel computation on GPUs better than classical\nRNNs. Faster training and inference combined with different\nsequence-to-sequence modeling also lead to performance improvements. While the\nnew models completely depart from the original recurrent architecture, we\ndecided to investigate how to make RNNs more efficient. In this work, we\npropose a new recurrent NMT architecture, called Simple Recurrent NMT, built on\na class of fast and weakly-recurrent units that use layer normalization and\nmultiple attentions. Our experiments on the WMT14 English-to-German and WMT16\nEnglish-Romanian benchmarks show that our model represents a valid alternative\nto LSTMs, as it can achieve better results at a significantly lower\ncomputational cost.", "published": "2018-05-10 21:55:32", "link": "http://arxiv.org/abs/1805.04185v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "hyperdoc2vec: Distributed Representations of Hypertext Documents", "abstract": "Hypertext documents, such as web pages and academic papers, are of great\nimportance in delivering information in our daily life. Although being\neffective on plain documents, conventional text embedding methods suffer from\ninformation loss if directly adapted to hyper-documents. In this paper, we\npropose a general embedding approach for hyper-documents, namely, hyperdoc2vec,\nalong with four criteria characterizing necessary information that\nhyper-document embedding models should preserve. Systematic comparisons are\nconducted between hyperdoc2vec and several competitors on two tasks, i.e.,\npaper classification and citation recommendation, in the academic paper domain.\nAnalyses and experiments both validate the superiority of hyperdoc2vec to other\nmodels w.r.t. the four criteria.", "published": "2018-05-10 02:42:03", "link": "http://arxiv.org/abs/1805.03793v1", "categories": ["cs.CL", "cs.SI"], "primary_category": "cs.CL"}
{"title": "Learning Domain-Sensitive and Sentiment-Aware Word Embeddings", "abstract": "Word embeddings have been widely used in sentiment classification because of\ntheir efficacy for semantic representations of words. Given reviews from\ndifferent domains, some existing methods for word embeddings exploit sentiment\ninformation, but they cannot produce domain-sensitive embeddings. On the other\nhand, some other existing methods can generate domain-sensitive word\nembeddings, but they cannot distinguish words with similar contexts but\nopposite sentiment polarity. We propose a new method for learning\ndomain-sensitive and sentiment-aware embeddings that simultaneously capture the\ninformation of sentiment semantics and domain sensitivity of individual words.\nOur method can automatically determine and produce domain-common embeddings and\ndomain-specific embeddings. The differentiation of domain-common and\ndomain-specific words enables the advantage of data augmentation of common\nsemantics from multiple domains and capture the varied semantics of specific\nwords from different domains at the same time. Experimental results show that\nour model provides an effective way to learn domain-sensitive and\nsentiment-aware word embeddings which benefit sentiment classification at both\nsentence level and lexicon term level.", "published": "2018-05-10 03:39:32", "link": "http://arxiv.org/abs/1805.03801v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Inference-Oriented Reading Comprehension: ParallelQA", "abstract": "In this paper, we investigate the tendency of end-to-end neural Machine\nReading Comprehension (MRC) models to match shallow patterns rather than\nperform inference-oriented reasoning on RC benchmarks. We aim to test the\nability of these systems to answer questions which focus on referential\ninference. We propose ParallelQA, a strategy to formulate such questions using\nparallel passages. We also demonstrate that existing neural models fail to\ngeneralize well to this setting.", "published": "2018-05-10 05:47:46", "link": "http://arxiv.org/abs/1805.03830v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A comparable study of modeling units for end-to-end Mandarin speech\n  recognition", "abstract": "End-To-End speech recognition have become increasingly popular in mandarin\nspeech recognition and achieved delightful performance.\n  Mandarin is a tonal language which is different from English and requires\nspecial treatment for the acoustic modeling units. There have been several\ndifferent kinds of modeling units for mandarin such as phoneme, syllable and\nChinese character.\n  In this work, we explore two major end-to-end models: connectionist temporal\nclassification (CTC) model and attention based encoder-decoder model for\nmandarin speech recognition. We compare the performance of three different\nscaled modeling units: context dependent phoneme(CDP), syllable with tone and\nChinese character.\n  We find that all types of modeling units can achieve approximate character\nerror rate (CER) in CTC model and the performance of Chinese character\nattention model is better than syllable attention model. Furthermore, we find\nthat Chinese character is a reasonable unit for mandarin speech recognition. On\nDidiCallcenter task, Chinese character attention model achieves a CER of 5.68%\nand CTC model gets a CER of 7.29%, on the other DidiReading task, CER are 4.89%\nand 5.79%, respectively. Moreover, attention model achieves a better\nperformance than CTC model on both datasets.", "published": "2018-05-10 05:54:32", "link": "http://arxiv.org/abs/1805.03832v2", "categories": ["cs.CL", "eess.AS"], "primary_category": "cs.CL"}
{"title": "From Word to Sense Embeddings: A Survey on Vector Representations of\n  Meaning", "abstract": "Over the past years, distributed semantic representations have proved to be\neffective and flexible keepers of prior knowledge to be integrated into\ndownstream applications. This survey focuses on the representation of meaning.\nWe start from the theoretical background behind word vector space models and\nhighlight one of their major limitations: the meaning conflation deficiency,\nwhich arises from representing a word with all its possible meanings as a\nsingle vector. Then, we explain how this deficiency can be addressed through a\ntransition from the word level to the more fine-grained level of word senses\n(in its broader acceptation) as a method for modelling unambiguous lexical\nmeaning. We present a comprehensive overview of the wide range of techniques in\nthe two main branches of sense representation, i.e., unsupervised and\nknowledge-based. Finally, this survey covers the main evaluation procedures and\napplications for this type of representation, and provides an analysis of four\nof its important aspects: interpretability, sense granularity, adaptability to\ndifferent domains and compositionality.", "published": "2018-05-10 15:56:48", "link": "http://arxiv.org/abs/1805.04032v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Joint Embedding of Words and Labels for Text Classification", "abstract": "Word embeddings are effective intermediate representations for capturing\nsemantic regularities between words, when learning the representations of text\nsequences. We propose to view text classification as a label-word joint\nembedding problem: each label is embedded in the same space with the word\nvectors. We introduce an attention framework that measures the compatibility of\nembeddings between text sequences and labels. The attention is learned on a\ntraining set of labeled samples to ensure that, given a text sequence, the\nrelevant words are weighted higher than the irrelevant ones. Our method\nmaintains the interpretability of word embeddings, and enjoys a built-in\nability to leverage alternative sources of information, in addition to input\ntext sequences. Extensive results on the several large text datasets show that\nthe proposed framework outperforms the state-of-the-art methods by a large\nmargin, in terms of both accuracy and speed.", "published": "2018-05-10 20:42:52", "link": "http://arxiv.org/abs/1805.04174v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Global Encoding for Abstractive Summarization", "abstract": "In neural abstractive summarization, the conventional sequence-to-sequence\n(seq2seq) model often suffers from repetition and semantic irrelevance. To\ntackle the problem, we propose a global encoding framework, which controls the\ninformation flow from the encoder to the decoder based on the global\ninformation of the source context. It consists of a convolutional gated unit to\nperform global encoding to improve the representations of the source-side\ninformation. Evaluations on the LCSTS and the English Gigaword both demonstrate\nthat our model outperforms the baseline models, and the analysis shows that our\nmodel is capable of reducing repetition.", "published": "2018-05-10 14:11:51", "link": "http://arxiv.org/abs/1805.03989v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "First Experiments with Neural Translation of Informal to Formal\n  Mathematics", "abstract": "We report on our experiments to train deep neural networks that automatically\ntranslate informalized LaTeX-written Mizar texts into the formal Mizar\nlanguage. To the best of our knowledge, this is the first time when neural\nnetworks have been adopted in the formalization of mathematics. Using Luong et\nal.'s neural machine translation model (NMT), we tested our aligned\ninformal-formal corpora against various hyperparameters and evaluated their\nresults. Our experiments show that our best performing model configurations are\nable to generate correct Mizar statements on 65.73\\% of the inference data,\nwith the union of all models covering 79.17\\%. These results indicate that\nformalization through artificial neural network is a promising approach for\nautomated formalization of mathematics. We present several case studies to\nillustrate our results.", "published": "2018-05-10 09:11:43", "link": "http://arxiv.org/abs/1805.06502v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "primary_category": "cs.CL"}
{"title": "Hybrid Adaptive Fuzzy Extreme Learning Machine for text classification", "abstract": "In traditional ELM and its improved versions suffer from the problems of\noutliers or noises due to overfitting and imbalance due to distribution. We\npropose a novel hybrid adaptive fuzzy ELM(HA-FELM), which introduces a fuzzy\nmembership function to the traditional ELM method to deal with the above\nproblems. We define the fuzzy membership function not only basing on the\ndistance between each sample and the center of the class but also the density\namong samples which based on the quantum harmonic oscillator model. The\nproposed fuzzy membership function overcomes the shortcoming of the traditional\nfuzzy membership function and could make itself adjusted according to the\nspecific distribution of different samples adaptively. Experiments show the\nproposed HA-FELM can produce better performance than SVM, ELM, and RELM in text\nclassification.", "published": "2018-05-10 06:11:27", "link": "http://arxiv.org/abs/1805.06524v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Text classification based on ensemble extreme learning machine", "abstract": "In this paper, we propose a novel approach based on cost-sensitive ensemble\nweighted extreme learning machine; we call this approach AE1-WELM. We apply\nthis approach to text classification. AE1-WELM is an algorithm including\nbalanced and imbalanced multiclassification for text classification. Weighted\nELM assigning the different weights to the different samples improves the\nclassification accuracy to a certain extent, but weighted ELM considers the\ndifferences between samples in the different categories only and ignores the\ndifferences between samples within the same categories. We measure the\nimportance of the documents by the sample information entropy, and generate\ncost-sensitive matrix and factor based on the document importance, then embed\nthe cost-sensitive weighted ELM into the AdaBoost.M1 framework seamlessly.\nVector space model(VSM) text representation produces the high dimensions and\nsparse features which increase the burden of ELM. To overcome this problem, we\ndevelop a text classification framework combining the word vector and AE1-WELM.\nThe experimental results show that our method provides an accurate, reliable\nand effective solution for text classification.", "published": "2018-05-10 06:10:46", "link": "http://arxiv.org/abs/1805.06525v1", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
