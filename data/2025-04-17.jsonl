{"title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "abstract": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.", "published": "2025-04-17 17:59:56", "link": "http://arxiv.org/abs/2504.13180v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Aligning Constraint Generation with Design Intent in Parametric CAD", "abstract": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.", "published": "2025-04-17 17:59:54", "link": "http://arxiv.org/abs/2504.13178v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization", "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.", "published": "2025-04-17 17:59:33", "link": "http://arxiv.org/abs/2504.13173v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MIB: A Mechanistic Interpretability Benchmark", "abstract": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.", "published": "2025-04-17 17:55:45", "link": "http://arxiv.org/abs/2504.13151v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction", "abstract": "Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.", "published": "2025-04-17 17:51:38", "link": "http://arxiv.org/abs/2504.13142v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo", "abstract": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.", "published": "2025-04-17 17:49:40", "link": "http://arxiv.org/abs/2504.13139v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Energy-Based Reward Models for Robust Language Model Alignment", "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "published": "2025-04-17 17:47:15", "link": "http://arxiv.org/abs/2504.13134v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis", "abstract": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.", "published": "2025-04-17 17:44:19", "link": "http://arxiv.org/abs/2504.13129v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "abstract": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.", "published": "2025-04-17 17:42:02", "link": "http://arxiv.org/abs/2504.13125v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models", "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.", "published": "2025-04-17 17:39:41", "link": "http://arxiv.org/abs/2504.13122v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?", "abstract": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.", "published": "2025-04-17 14:07:11", "link": "http://arxiv.org/abs/2504.12961v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis", "abstract": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.", "published": "2025-04-17 09:18:04", "link": "http://arxiv.org/abs/2504.12777v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems", "abstract": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.", "published": "2025-04-17 08:21:28", "link": "http://arxiv.org/abs/2504.12735v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric", "abstract": "This paper presents an adversary model and a simulation framework\nspecifically tailored for analyzing attacks on distributed systems composed of\nmultiple distributed protocols, with a focus on assessing the security of\nblockchain networks. Our model classifies and constrains adversarial actions\nbased on the assumptions of the target protocols, defined by failure models,\ncommunication models, and the fault tolerance thresholds of Byzantine Fault\nTolerant (BFT) protocols. The goal is to study not only the intended effects of\nadversarial strategies but also their unintended side effects on critical\nsystem properties. We apply this framework to analyze fairness properties in a\nHyperledger Fabric (HF) blockchain network. Our focus is on novel fairness\nattacks that involve coordinated adversarial actions across various HF\nservices. Simulations show that even a constrained adversary can violate\nfairness with respect to specific clients (client fairness) and impact related\nguarantees (order fairness), which relate the reception order of transactions\nto their final order in the blockchain. This paper significantly extends our\nprevious work by introducing and evaluating a mitigation mechanism specifically\ndesigned to counter transaction reordering attacks. We implement and integrate\nthis defense into our simulation environment, demonstrating its effectiveness\nunder diverse conditions.", "published": "2025-04-17 08:17:27", "link": "http://arxiv.org/abs/2504.12733v1", "categories": ["cs.CR", "cs.DC", "cs.MA"], "primary_category": "cs.CR"}
{"title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination", "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.", "published": "2025-04-17 07:41:25", "link": "http://arxiv.org/abs/2504.12714v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance", "abstract": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.", "published": "2025-04-17 03:23:17", "link": "http://arxiv.org/abs/2504.12612v1", "categories": ["cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A generalized energy-based modeling framework with application to field/circuit coupled problems", "abstract": "This paper presents a generalized energy-based modeling framework extending\nrecent formulations tailored for differential-algebraic equations. The proposed\nstructure, inspired by the port-Hamiltonian formalism, ensures passivity,\npreserves the power balance, and facilitates the consistent interconnection of\nsubsystems. A particular focus is put on low-frequency power applications in\nelectrical engineering. Stranded, solid, and foil conductor models are\ninvestigated in the context of the eddy current problem. Each conductor model\nis shown to fit into the generalized energy-based structure, which allows their\nstructure-preserving coupling with electrical circuits described by modified\nnodal analysis. Theoretical developments are validated through a numerical\nsimulation of an oscillator circuit, demonstrating energy conservation in\nlossless scenarios and controlled dissipation when eddy currents are present.", "published": "2025-04-17 15:45:20", "link": "http://arxiv.org/abs/2504.13036v1", "categories": ["math.NA", "cs.NA", "35Q61, 65M60, 65L80, 78M10"], "primary_category": "math.NA"}
{"title": "Efficient Chebyshev Reconstruction for the Anisotropic Equilibrium Model in Magnetic Particle Imaging", "abstract": "Magnetic Particle Imaging (MPI) is a tomographic imaging modality capable of\nreal-time, high-sensitivity mapping of superparamagnetic iron oxide\nnanoparticles. Model-based image reconstruction provides an alternative to\nconventional methods that rely on a measured system matrix, eliminating the\nneed for laborious calibration measurements. Nevertheless, model-based\napproaches must account for the complexities of the imaging chain to maintain\nhigh image quality. A recently proposed direct reconstruction method leverages\nweighted Chebyshev polynomials in the frequency domain, removing the need for a\nsimulated system matrix. However, the underlying model neglects key physical\neffects, such as nanoparticle anisotropy, leading to distortions in\nreconstructed images. To mitigate these artifacts, an adapted direct Chebyshev\nreconstruction (DCR) method incorporates a spatially variant deconvolution\nstep, significantly improving reconstruction accuracy at the cost of increased\ncomputational demands. In this work, we evaluate the adapted DCR on six\nexperimental phantoms, demonstrating enhanced reconstruction quality in real\nmeasurements and achieving image fidelity comparable to or exceeding that of\nsimulated system matrix reconstruction. Furthermore, we introduce an efficient\napproximation for the spatially variable deconvolution, reducing both runtime\nand memory consumption while maintaining accuracy. This method achieves\ncomputational complexity of O(N log N ), making it particularly beneficial for\nhigh-resolution and three-dimensional imaging. Our results highlight the\npotential of the adapted DCR approach for improving model-based MPI\nreconstruction in practical applications.", "published": "2025-04-17 14:37:49", "link": "http://arxiv.org/abs/2504.12981v1", "categories": ["physics.med-ph", "cs.NA", "eess.IV", "math.NA"], "primary_category": "physics.med-ph"}
{"title": "RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs", "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs). However, their performance\nheavily relies on the strategy used to select training points. Conventional\nadaptive sampling methods, such as residual-based refinement, often require\nmulti-round sampling and repeated retraining of PINNs, leading to computational\ninefficiency due to redundant points and costly gradient\ncomputations-particularly in high-dimensional or high-order derivative\nscenarios. To address these limitations, we propose RL-PINNs, a reinforcement\nlearning(RL)-driven adaptive sampling framework that enables efficient training\nwith only a single round of sampling. Our approach formulates adaptive sampling\nas a Markov decision process, where an RL agent dynamically selects optimal\ntraining points by maximizing a long-term utility metric. Critically, we\nreplace gradient-dependent residual metrics with a computationally efficient\nfunction variation as the reward signal, eliminating the overhead of derivative\ncalculations. Furthermore, we employ a delayed reward mechanism to prioritize\nlong-term training stability over short-term gains. Extensive experiments\nacross diverse PDE benchmarks, including low-regular, nonlinear,\nhigh-dimensional, and high-order problems, demonstrate that RL-PINNs\nsignificantly outperforms existing residual-driven adaptive methods in\naccuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,\nmaking them scalable to high-dimensional and high-order problems.", "published": "2025-04-17 13:50:55", "link": "http://arxiv.org/abs/2504.12949v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs", "abstract": "Cross-modal retrieval (CMR) is a fundamental task in multimedia research,\nfocused on retrieving semantically relevant targets across different\nmodalities. While traditional CMR methods match text and image via\nembedding-based similarity calculations, recent advancements in pre-trained\ngenerative models have established generative retrieval as a promising\nalternative. This paradigm assigns each target a unique identifier and\nleverages a generative model to directly predict identifiers corresponding to\ninput queries without explicit indexing. Despite its great potential, current\ngenerative CMR approaches still face semantic information insufficiency in both\nidentifier construction and generation processes. To address these limitations,\nwe propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval\nframework (SemCORE), designed to unleash the semantic understanding\ncapabilities in generative cross-modal retrieval task. Specifically, we first\nconstruct a Structured natural language IDentifier (SID) that effectively\naligns target identifiers with generative models optimized for natural language\ncomprehension and generation. Furthermore, we introduce a Generative Semantic\nVerification (GSV) strategy enabling fine-grained target discrimination.\nAdditionally, to the best of our knowledge, SemCORE is the first framework to\nsimultaneously consider both text-to-image and image-to-text retrieval tasks\nwithin generative cross-modal retrieval. Extensive experiments demonstrate that\nour framework outperforms state-of-the-art generative cross-modal retrieval\nmethods. Notably, SemCORE achieves substantial improvements across benchmark\ndatasets, with an average increase of 8.65 points in Recall@1 for text-to-image\nretrieval.", "published": "2025-04-17 17:59:27", "link": "http://arxiv.org/abs/2504.13172v1", "categories": ["cs.IR", "cs.CL", "cs.MM"], "primary_category": "cs.IR"}
{"title": "Sleep-time Compute: Beyond Inference Scaling at Test-time", "abstract": "Scaling test-time compute has emerged as a key ingredient for enabling large\nlanguage models (LLMs) to solve difficult problems, but comes with high latency\nand inference cost. We introduce sleep-time compute, which allows models to\n\"think\" offline about contexts before queries are presented: by anticipating\nwhat queries users might ask and pre-computing useful quantities, we can\nsignificantly reduce the compute requirements at test-time. To demonstrate the\nefficacy of our method, we create modified versions of two reasoning tasks -\nStateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can\nreduce the amount of test-time compute needed to achieve the same accuracy by ~\n5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time\ncompute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic\nand 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic,\nwhich extends GSM-Symbolic by including multiple related queries per context.\nBy amortizing sleep-time compute across related queries about the same context\nusing Multi-Query GSM-Symbolic, we can decrease the average cost per query by\n2.5x. We then conduct additional analysis to understand when sleep-time compute\nis most effective, finding the predictability of the user query to be well\ncorrelated with the efficacy of sleep-time compute. Finally, we conduct a\ncase-study of applying sleep-time compute to a realistic agentic SWE task.", "published": "2025-04-17 17:59:25", "link": "http://arxiv.org/abs/2504.13171v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training", "abstract": "Pre-training datasets are typically collected from web content and lack\ninherent domain divisions. For instance, widely used datasets like Common Crawl\ndo not include explicit domain labels, while manually curating labeled datasets\nsuch as The Pile is labor-intensive. Consequently, identifying an optimal\npre-training data mixture remains a challenging problem, despite its\nsignificant benefits for pre-training performance. To address these challenges,\nwe propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an\nautomated framework that discovers, evaluates, and refines data mixtures in a\npre-training setting. Specifically, CLIMB embeds and clusters large-scale\ndatasets in a semantic space and then iteratively searches for optimal mixtures\nusing a smaller proxy model and a predictor. When continuously trained on 400B\ntokens with this mixture, our 1B model exceeds the state-of-the-art\nLlama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific\ndomain (e.g., Social Sciences) yields a 5% improvement over random sampling.\nFinally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20\nclusters as a research playground, and ClimbMix, a compact yet powerful\n400-billion-token dataset designed for efficient pre-training that delivers\nsuperior performance under an equal token budget. We analyze the final data\nmixture, elucidating the characteristics of an optimal data mixture. Our data\nis available at: https://research.nvidia.com/labs/lpr/climb/", "published": "2025-04-17 17:58:13", "link": "http://arxiv.org/abs/2504.13161v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MIB: A Mechanistic Interpretability Benchmark", "abstract": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.", "published": "2025-04-17 17:55:45", "link": "http://arxiv.org/abs/2504.13151v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Antidistillation Sampling", "abstract": "Frontier models that generate extended reasoning traces inadvertently produce\nrich token sequences that can facilitate model distillation. Recognizing this\nvulnerability, model owners may seek sampling strategies that limit the\neffectiveness of distillation without compromising model performance.\n\\emph{Antidistillation sampling} provides exactly this capability. By\nstrategically modifying a model's next-token probability distribution,\nantidistillation sampling poisons reasoning traces, rendering them\nsignificantly less effective for distillation while preserving the model's\npractical utility. For further details, see https://antidistillation.com.", "published": "2025-04-17 17:54:14", "link": "http://arxiv.org/abs/2504.13146v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo", "abstract": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.", "published": "2025-04-17 17:49:40", "link": "http://arxiv.org/abs/2504.13139v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Energy-Based Reward Models for Robust Language Model Alignment", "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "published": "2025-04-17 17:47:15", "link": "http://arxiv.org/abs/2504.13134v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents", "abstract": "We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.", "published": "2025-04-17 17:44:06", "link": "http://arxiv.org/abs/2504.13128v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "abstract": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.", "published": "2025-04-17 17:42:02", "link": "http://arxiv.org/abs/2504.13125v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Probing and Inducing Combinational Creativity in Vision-Language Models", "abstract": "The ability to combine existing concepts into novel ideas stands as a\nfundamental hallmark of human intelligence. Recent advances in Vision-Language\nModels (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their\noutputs reflect combinational creativity--defined by M. A. Boden (1998) as\nsynthesizing novel ideas through combining existing concepts--or sophisticated\npattern matching of training data. Drawing inspiration from cognitive science,\nwe investigate the combinational creativity of VLMs from the lens of concept\nblending. We propose the Identification-Explanation-Implication (IEI)\nframework, which decomposes creative processes into three levels: identifying\ninput spaces, extracting shared attributes, and deriving novel semantic\nimplications. To validate this framework, we curate CreativeMashup, a\nhigh-quality dataset of 666 artist-generated visual mashups annotated according\nto the IEI framework. Through extensive experiments, we demonstrate that in\ncomprehension tasks, best VLMs have surpassed average human performance while\nfalling short of expert-level understanding; in generation tasks, incorporating\nour IEI framework into the generation pipeline significantly enhances the\ncreative quality of VLMs outputs. Our findings establish both a theoretical\nfoundation for evaluating artificial creativity and practical guidelines for\nimproving creative generation in VLMs.", "published": "2025-04-17 17:38:18", "link": "http://arxiv.org/abs/2504.13120v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia", "abstract": "Eradicating poverty is the first goal in the United Nations Sustainable\nDevelopment Goals. However, aporophobia -- the societal bias against people\nliving in poverty -- constitutes a major obstacle to designing, approving and\nimplementing poverty-mitigation policies. This work presents an initial step\ntowards operationalizing the concept of aporophobia to identify and track\nharmful beliefs and discriminative actions against poor people on social media.\nIn close collaboration with non-profits and governmental organizations, we\nconduct data collection and exploration. Then we manually annotate a corpus of\nEnglish tweets from five world regions for the presence of (1) direct\nexpressions of aporophobia, and (2) statements referring to or criticizing\naporophobic views or actions of others, to comprehensively characterize the\nsocial media discourse related to bias and discrimination against the poor.\nBased on the annotated data, we devise a taxonomy of categories of aporophobic\nattitudes and actions expressed through speech on social media. Finally, we\ntrain several classifiers and identify the main challenges for automatic\ndetection of aporophobia in social networks. This work paves the way towards\nidentifying, tracking, and mitigating aporophobic views on social media at\nscale.", "published": "2025-04-17 16:53:14", "link": "http://arxiv.org/abs/2504.13085v1", "categories": ["cs.CY", "cs.CL"], "primary_category": "cs.CY"}
{"title": "Retrieval-Augmented Generation with Conflicting Evidence", "abstract": "Large language model (LLM) agents are increasingly employing\nretrieval-augmented generation (RAG) to improve the factuality of their\nresponses. However, in practice, these systems often need to handle ambiguous\nuser queries and potentially conflicting information from multiple sources\nwhile also suppressing inaccurate information from noisy or irrelevant\ndocuments. Prior work has generally studied and addressed these challenges in\nisolation, considering only one aspect at a time, such as handling ambiguity or\nrobustness to noise and misinformation. We instead consider multiple factors\nsimultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and\nMisinformation in Documents), a new dataset that simulates complex and\nrealistic scenarios for conflicting evidence for a user query, including\nambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent\napproach in which LLM agents debate over the merits of an answer over multiple\nrounds, allowing an aggregator to collate responses corresponding to\ndisambiguated entities while discarding misinformation and noise, thereby\nhandling diverse sources of conflict jointly. We demonstrate the effectiveness\nof MADAM-RAG using both closed and open-source models on AmbigDocs -- which\nrequires presenting all valid answers for ambiguous queries -- improving over\nstrong RAG baselines by up to 11.40% and on FaithEval -- which requires\nsuppressing misinformation -- where we improve by up to 15.80% (absolute) with\nLlama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for\nexisting RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match\nscore). While MADAM-RAG begins to address these conflicting factors, our\nanalysis indicates that a substantial gap remains especially when increasing\nthe level of imbalance in supporting evidence and misinformation.", "published": "2025-04-17 16:46:11", "link": "http://arxiv.org/abs/2504.13079v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models", "abstract": "This study explores the relationship between deep learning (DL) model\naccuracy and expert agreement in the classification of crash narratives. We\nevaluate five DL models -- including BERT variants, the Universal Sentence\nEncoder (USE), and a zero-shot classifier -- against expert-labeled data and\nnarrative text. The analysis is further extended to four large language models\n(LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive\ntrend: models with higher technical accuracy often exhibit lower agreement with\ndomain experts, whereas LLMs demonstrate greater expert alignment despite\nrelatively lower accuracy scores. To quantify and interpret model-expert\nagreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and\nSHAP-based explainability techniques. Findings indicate that expert-aligned\nmodels tend to rely more on contextual and temporal language cues, rather than\nlocation-specific keywords. These results underscore that accuracy alone is\ninsufficient for evaluating models in safety-critical NLP applications. We\nadvocate for incorporating expert agreement as a complementary metric in model\nevaluation frameworks and highlight the promise of LLMs as interpretable,\nscalable tools for crash analysis pipelines.", "published": "2025-04-17 16:29:08", "link": "http://arxiv.org/abs/2504.13068v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins", "abstract": "In the rapidly advancing field of robotics, dual-arm coordination and complex\nobject manipulation are essential capabilities for developing advanced\nautonomous systems. However, the scarcity of diverse, high-quality\ndemonstration data and real-world-aligned evaluation benchmarks severely limits\nsuch development. To address this, we introduce RoboTwin, a generative digital\ntwin framework that uses 3D generative foundation models and large language\nmodels to produce diverse expert datasets and provide a real-world-aligned\nevaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates\nvaried digital twins of objects from single 2D images, generating realistic and\ninteractive scenarios. It also introduces a spatial relation-aware code\ngeneration framework that combines object annotations with large language\nmodels to break down tasks, determine spatial constraints, and generate precise\nrobotic movement code. Our framework offers a comprehensive benchmark with both\nsimulated and real-world data, enabling standardized evaluation and better\nalignment between simulated training and real-world performance. We validated\nour approach using the open-source COBOT Magic Robot platform. Policies\npre-trained on RoboTwin-generated data and fine-tuned with limited real-world\nsamples demonstrate significant potential for enhancing dual-arm robotic\nmanipulation systems by improving success rates by over 70% for single-arm\ntasks and over 40% for dual-arm tasks compared to models trained solely on\nreal-world data.", "published": "2025-04-17 16:14:24", "link": "http://arxiv.org/abs/2504.13059v1", "categories": ["cs.RO", "cs.AI", "cs.CL"], "primary_category": "cs.RO"}
{"title": "Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation", "abstract": "Aspect-based summarization aims to generate summaries tailored to specific\naspects, addressing the resource constraints and limited generalizability of\ntraditional summarization approaches. Recently, large language models have\nshown promise in this task without the need for training. However, they rely\nexcessively on prompt engineering and face token limits and hallucination\nchallenges, especially with in-context learning. To address these challenges,\nin this paper, we propose a novel framework for aspect-based summarization:\nSelf-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely\non in-context learning, given an aspect, we employ an embedding-driven\nretrieval mechanism to identify its relevant text segments. This approach\nextracts the pertinent content while avoiding unnecessary details, thereby\nmitigating the challenge of token limits. Moreover, our framework optimizes\ntoken usage by deleting unrelated parts of the text and ensuring that the model\ngenerates output strictly based on the given aspect. With extensive experiments\non benchmark datasets, we demonstrate that our framework not only achieves\nsuperior performance but also effectively mitigates the token limitation\nproblem.", "published": "2025-04-17 16:09:57", "link": "http://arxiv.org/abs/2504.13054v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses", "abstract": "The release of ChatGPT in late 2022 caused a flurry of activity and concern\nin the academic and educational communities. Some see the tool's ability to\ngenerate human-like text that passes at least cursory inspections for factual\naccuracy ``often enough'' a golden age of information retrieval and\ncomputer-assisted learning. Some, on the other hand, worry the tool may lead to\nunprecedented levels of academic dishonesty and cheating. In this work, we\nquantify some of the effects of the emergence of Large Language Models (LLMs)\non online education by analyzing a multi-year dataset of student essay\nresponses from a free university-level MOOC on AI ethics. Our dataset includes\nessays submitted both before and after ChatGPT's release. We find that the\nlaunch of ChatGPT coincided with significant changes in both the length and\nstyle of student essays, mirroring observations in other contexts such as\nacademic publishing. We also observe -- as expected based on related public\ndiscourse -- changes in prevalence of key content words related to AI and LLMs,\nbut not necessarily the general themes or topics discussed in the student\nessays as identified through (dynamic) topic modeling.", "published": "2025-04-17 15:51:59", "link": "http://arxiv.org/abs/2504.13038v1", "categories": ["cs.CY", "cs.CL", "K.3.1; I.2.7"], "primary_category": "cs.CY"}
{"title": "ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images", "abstract": "Recent studies have made significant progress in developing large language\nmodels (LLMs) in the medical domain, which can answer expert-level questions\nand demonstrate the potential to assist clinicians in real-world clinical\nscenarios. Studies have also witnessed the importance of integrating various\nmodalities with the existing LLMs for a better understanding of complex\nclinical contexts, which are innately multi-faceted by nature. Although studies\nhave demonstrated the ability of multimodal LLMs in histopathology to answer\nquestions from given images, they lack in understanding of thorough clinical\ncontext due to the patch-level data with limited information from public\ndatasets. Thus, developing WSI-level MLLMs is significant in terms of the\nscalability and applicability of MLLMs in histopathology. In this study, we\nintroduce an expert-level MLLM for histopathology using WSIs, dubbed as\nChatEXAONEPath. We present a retrieval-based data generation pipeline using\n10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas\n(TCGA). We also showcase an AI-based evaluation protocol for a comprehensive\nunderstanding of the medical context from given multimodal information and\nevaluate generated answers compared to the original histopathology reports. We\ndemonstrate the ability of diagnosing the given histopathology images using\nChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and\nreports. Our proposed model can understand pan-cancer WSIs and clinical context\nfrom various cancer types. We argue that our proposed model has the potential\nto assist clinicians by comprehensively understanding complex morphology of\nWSIs for cancer diagnosis through the integration of multiple modalities.", "published": "2025-04-17 15:33:17", "link": "http://arxiv.org/abs/2504.13023v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation", "abstract": "Large language models (LLMs) frequently memorize sensitive information during\ntraining, posing risks when deploying publicly accessible models. Current\nmachine unlearning methods struggle to selectively remove specific data\nassociations without degrading overall model capabilities. This paper presents\nour solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a\ntwo-stage methodology that combines causal mediation analysis with\nlayer-specific optimization. Through systematic causal tracing experiments on\nOLMo architectures (1B and 7B parameters), we identify the critical role of the\nfirst few transformer layers (layers 0-5) in storing subject-attribute\nassociations within MLP modules. Building on this insight, we develop a\nconstrained optimization approach that freezes upper layers while applying a\nnovel joint loss function to lower layers-simultaneously maximizing forget set\nloss via output token cross-entropy penalties and minimizing retain set\ndeviation through adaptive regularization. Our method achieves 2nd place in the\n1B model track, demonstrating strong task performance while maintaining 88% of\nbaseline MMLU accuracy. These results establish causal-informed layer\noptimization as a promising paradigm for efficient, precise unlearning in LLMs,\noffering a significant step forward in addressing data privacy concerns in AI\nsystems.", "published": "2025-04-17 15:05:40", "link": "http://arxiv.org/abs/2504.12996v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild", "abstract": "The proliferation of large language models (LLMs) has significantly advanced\ninformation retrieval systems, particularly in response generation (RG).\nUnfortunately, LLMs often face knowledge conflicts between internal memory and\nretrievaled external information, arising from misinformation, biases, or\noutdated knowledge. These conflicts undermine response reliability and\nintroduce uncertainty in decision-making. In this work, we analyze how LLMs\nnavigate knowledge conflicts from an information-theoretic perspective and\nreveal that when conflicting and supplementary information exhibit significant\ndifferences, LLMs confidently resolve their preferences. However, when the\ndistinction is ambiguous, LLMs experience heightened uncertainty. Based on this\ninsight, we propose Swin-VIB, a novel framework that integrates a pipeline of\nvariational information bottleneck models into adaptive augmentation of\nretrieved information and guiding LLM preference in response generation.\nExtensive experiments on single-choice, open-ended question-answering (QA), and\nretrieval augmented generation (RAG) validate our theoretical findings and\ndemonstrate the efficacy of Swin-VIB. Notably, our method improves\nsingle-choice task accuracy by at least 7.54\\% over competitive baselines.", "published": "2025-04-17 14:40:31", "link": "http://arxiv.org/abs/2504.12982v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology", "abstract": "This paper presents a novel research analytical IT system grounded in Martin\nHeidegger's Fundamental Ontology, distinguishing between beings (das Seiende)\nand Being (das Sein). The system employs two modally distinct, descriptively\ncomplete languages: a categorical language of beings for processing user inputs\nand an existential language of Being for internal analysis. These languages are\nbridged via a phenomenological reduction module, enabling the system to analyze\nuser queries (including questions, answers, and dialogues among IT\nspecialists), identify recursive and self-referential structures, and provide\nactionable insights in categorical terms. Unlike contemporary systems limited\nto categorical analysis, this approach leverages Heidegger's phenomenological\nexistential analysis to uncover deeper ontological patterns in query\nprocessing, aiding in resolving logical traps in complex interactions, such as\nmetaphor usage in IT contexts. The path to full realization involves\nformalizing the language of Being by a research team based on Heidegger's\nFundamental Ontology; given the existing completeness of the language of\nbeings, this reduces the system's computability to completeness, paving the way\nfor a universal query analysis tool. The paper presents the system's\narchitecture, operational principles, technical implementation, use\ncases--including a case based on real IT specialist dialogues--comparative\nevaluation with existing tools, and its advantages and limitations.", "published": "2025-04-17 14:29:25", "link": "http://arxiv.org/abs/2504.12977v1", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.HC"], "primary_category": "cs.SE"}
{"title": "Sparks of Science: Hypothesis Generation Using Structured Paper Data", "abstract": "Generating novel and creative scientific hypotheses is a cornerstone in\nachieving Artificial General Intelligence. Large language and reasoning models\nhave the potential to aid in the systematic creation, selection, and validation\nof scientifically informed hypotheses. However, current foundation models often\nstruggle to produce scientific ideas that are both novel and feasible. One\nreason is the lack of a dedicated dataset that frames Scientific Hypothesis\nGeneration (SHG) as a Natural Language Generation (NLG) task. In this paper, we\nintroduce HypoGen, the first dataset of approximately 5500 structured\nproblem-hypothesis pairs extracted from top-tier computer science conferences\nstructured with a Bit-Flip-Spark schema, where the Bit is the conventional\nassumption, the Spark is the key insight or conceptual leap, and the Flip is\nthe resulting counterproposal. HypoGen uniquely integrates an explicit\nChain-of-Reasoning component that reflects the intellectual process from Bit to\nFlip. We demonstrate that framing hypothesis generation as conditional language\nmodelling, with the model fine-tuned on Bit-Flip-Spark and the\nChain-of-Reasoning (and where, at inference, we only provide the Bit), leads to\nimprovements in the overall quality of the hypotheses. Our evaluation employs\nautomated metrics and LLM judge rankings for overall quality assessment. We\nshow that by fine-tuning on our HypoGen dataset we improve the novelty,\nfeasibility, and overall quality of the generated hypotheses. The HypoGen\ndataset is publicly available at\nhuggingface.co/datasets/UniverseTBD/hypogen-dr1.", "published": "2025-04-17 14:29:18", "link": "http://arxiv.org/abs/2504.12976v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization", "abstract": "Recent advances in long-context reasoning abilities of language models led to\ninteresting applications in large-scale multi-document summarization. However,\nprior work has shown that these long-context models are not effective at their\nclaimed context windows. To this end, retrieval-augmented systems provide an\nefficient and effective alternative. However, their performance can be highly\nsensitive to the choice of retrieval context length. In this work, we present a\nhybrid method that combines retrieval-augmented systems with long-context\nwindows supported by recent language models. Our method first estimates the\noptimal retrieval length as a function of the retriever, summarizer, and\ndataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to\ngenerate a pool of silver references. We use these silver references to\nestimate the optimal context length for a given RAG system configuration. Our\nresults on the multi-document summarization task showcase the effectiveness of\nour method across model classes and sizes. We compare against length estimates\nfrom strong long-context benchmarks such as RULER and HELMET. Our analysis also\nhighlights the effectiveness of our estimation method for very long-context LMs\nand its generalization to new classes of LMs.", "published": "2025-04-17 14:24:51", "link": "http://arxiv.org/abs/2504.12972v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback", "abstract": "Recent advancements in large language models (LLMs) have catalyzed the\ndevelopment of general-purpose autonomous agents, demonstrating remarkable\nperformance in complex reasoning tasks across various domains. This surge has\nspurred the evolution of a plethora of prompt-based reasoning frameworks. A\nrecent focus has been on iterative reasoning strategies that refine outputs\nthrough self-evaluation and verbalized feedback. However, these strategies\nrequire additional computational complexity to enable models to recognize and\ncorrect their mistakes, leading to a significant increase in their cost. In\nthis work, we introduce the concept of ``retrials without feedback'', an\nembarrassingly simple yet powerful mechanism for enhancing reasoning frameworks\nby allowing LLMs to retry problem-solving attempts upon identifying incorrect\nanswers. Unlike conventional iterative refinement methods, our method does not\nrequire explicit self-reflection or verbalized feedback, simplifying the\nrefinement process. Our findings indicate that simpler retrial-based approaches\noften outperform more sophisticated reasoning frameworks, suggesting that the\nbenefits of complex methods may not always justify their computational costs.\nBy challenging the prevailing assumption that more intricate reasoning\nstrategies inherently lead to better performance, our work offers new insights\ninto how simpler, more efficient approaches can achieve optimal results. So,\nare retrials all you need?", "published": "2025-04-17 13:52:48", "link": "http://arxiv.org/abs/2504.12951v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ConExion: Concept Extraction with Large Language Models", "abstract": "In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.", "published": "2025-04-17 13:05:14", "link": "http://arxiv.org/abs/2504.12915v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "MAIN: Mutual Alignment Is Necessary for instruction tuning", "abstract": "Instruction tuning has enabled large language models (LLMs) to achieve\nremarkable performance, but its success heavily depends on the availability of\nlarge-scale, high-quality instruction-response pairs. However, current methods\nfor scaling up data generation often overlook a crucial aspect: the alignment\nbetween instructions and responses. We hypothesize that high-quality\ninstruction-response pairs are not defined by the individual quality of each\ncomponent, but by the extent of their alignment with each other. To address\nthis, we propose a Mutual Alignment Framework (MAIN) that ensures coherence\nbetween the instruction and response through mutual constraints. Experiments\ndemonstrate that models such as LLaMA and Mistral, fine-tuned within this\nframework, outperform traditional methods across multiple benchmarks. This\napproach underscores the critical role of instruction-response alignment in\nenabling scalable and high-quality instruction tuning for LLMs.", "published": "2025-04-17 13:02:44", "link": "http://arxiv.org/abs/2504.12913v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking Multi-National Value Alignment for Large Language Models", "abstract": "Do Large Language Models (LLMs) hold positions that conflict with your\ncountry's values? Occasionally they do! However, existing works primarily focus\non ethical reviews, failing to capture the diversity of national values, which\nencompass broader policy, legal, and moral considerations. Furthermore, current\nbenchmarks that rely on spectrum tests using manually designed questionnaires\nare not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark\nto evaluate the alignment of LLMs with the values of five major nations: China,\nthe United States, the United Kingdom, France, and Germany. NaVAB implements a\nnational value extraction pipeline to efficiently construct value assessment\ndatasets. Specifically, we propose a modeling procedure with instruction\ntagging to process raw data sources, a screening process to filter\nvalue-related topics and a generation process with a Conflict Reduction\nmechanism to filter non-conflicting values.We conduct extensive experiments on\nvarious LLMs across countries, and the results provide insights into assisting\nin the identification of misaligned scenarios. Moreover, we demonstrate that\nNaVAB can be combined with alignment techniques to effectively reduce value\nconcerns by aligning LLMs' values with the target country.", "published": "2025-04-17 13:01:38", "link": "http://arxiv.org/abs/2504.12911v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models", "abstract": "Despite significant progress, recent studies indicate that current large\nlanguage models (LLMs) may still capture dataset biases and utilize them during\ninference, leading to the poor generalizability of LLMs. However, due to the\ndiversity of dataset biases and the insufficient nature of bias suppression\nbased on in-context learning, the effectiveness of previous prior\nknowledge-based debiasing methods and in-context learning based automatic\ndebiasing methods is limited. To address these challenges, we explore the\ncombination of causal mechanisms with information theory and propose an\ninformation gain-guided causal intervention debiasing (IGCIDB) framework. This\nframework first utilizes an information gain-guided causal intervention method\nto automatically and autonomously balance the distribution of\ninstruction-tuning dataset. Subsequently, it employs a standard supervised\nfine-tuning process to train LLMs on the debiased dataset. Experimental results\nshow that IGCIDB can effectively debias LLM to improve its generalizability\nacross different tasks.", "published": "2025-04-17 12:39:25", "link": "http://arxiv.org/abs/2504.12898v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication", "abstract": "The rapid evolution of artificial intelligence (AI) has introduced AI agents\nas a disruptive paradigm across various industries, yet their application in\nmachine translation (MT) remains underexplored. This paper describes and\nanalyses the potential of single- and multi-agent systems for MT, reflecting on\nhow they could enhance multilingual digital communication. While single-agent\nsystems are well-suited for simpler translation tasks, multi-agent systems,\nwhich involve multiple specialized AI agents collaborating in a structured\nmanner, may offer a promising solution for complex scenarios requiring high\naccuracy, domain-specific knowledge, and contextual awareness. To demonstrate\nthe feasibility of multi-agent workflows in MT, we are conducting a pilot study\nin legal MT. The study employs a multi-agent system involving four specialized\nAI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and\n(iv) final editing. Our findings suggest that multi-agent systems may have the\npotential to significantly improve domain-adaptability and contextual\nawareness, with superior translation quality to traditional MT or single-agent\nsystems. This paper also sets the stage for future research into multi-agent\napplications in MT, integration into professional translation workflows, and\nshares a demo of the system analyzed in the paper.", "published": "2025-04-17 12:32:18", "link": "http://arxiv.org/abs/2504.12891v1", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.HC"], "primary_category": "cs.CL"}
{"title": "ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos", "abstract": "The growing influence of video content as a medium for communication and\nmisinformation underscores the urgent need for effective tools to analyze\nclaims in multilingual and multi-topic settings. Existing efforts in\nmisinformation detection largely focus on written text, leaving a significant\ngap in addressing the complexity of spoken text in video transcripts. We\nintroduce ViClaim, a dataset of 1,798 annotated video transcripts across three\nlanguages (English, German, Spanish) and six topics. Each sentence in the\ntranscripts is labeled with three claim-related categories: fact-check-worthy,\nfact-non-check-worthy, or opinion. We developed a custom annotation tool to\nfacilitate the highly complex annotation process. Experiments with\nstate-of-the-art multilingual language models demonstrate strong performance in\ncross-validation (macro F1 up to 0.896) but reveal challenges in generalization\nto unseen topics, particularly for distinct domains. Our findings highlight the\ncomplexity of claim detection in video transcripts. ViClaim offers a robust\nfoundation for advancing misinformation detection in video-based communication,\naddressing a critical gap in multimodal analysis.", "published": "2025-04-17 12:14:38", "link": "http://arxiv.org/abs/2504.12882v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Building Russian Benchmark for Evaluation of Information Retrieval Models", "abstract": "We introduce RusBEIR, a comprehensive benchmark designed for zero-shot\nevaluation of information retrieval (IR) models in the Russian language.\nComprising 17 datasets from various domains, it integrates adapted, translated,\nand newly created datasets, enabling systematic comparison of lexical and\nneural models. Our study highlights the importance of preprocessing for lexical\nmodels in morphologically rich languages and confirms BM25 as a strong baseline\nfor full-document retrieval. Neural models, such as mE5-large and BGE-M3,\ndemonstrate superior performance on most datasets, but face challenges with\nlong-document retrieval due to input size constraints. RusBEIR offers a\nunified, open-source framework that promotes research in Russian-language\ninformation retrieval.", "published": "2025-04-17 12:11:14", "link": "http://arxiv.org/abs/2504.12879v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "abstract": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.", "published": "2025-04-17 11:50:04", "link": "http://arxiv.org/abs/2504.12867v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks", "abstract": "Existing multilingual long-context benchmarks, often based on the popular\nneedle-in-a-haystack test, primarily evaluate a model's ability to locate\nspecific information buried within irrelevant texts. However, such a\nretrieval-centric approach is myopic and inherently limited, as successful\nrecall alone does not indicate a model's capacity to reason over extended\ncontexts. Moreover, these benchmarks are susceptible to data leakage,\nshort-circuiting, and risk making the evaluation a priori identifiable. To\naddress these limitations, we introduce MLRBench, a new synthetic benchmark for\nmultilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes\nbeyond surface-level retrieval by including tasks that assess multi-hop\ninference, aggregation, and epistemic reasoning. Spanning seven languages,\nMLRBench is designed to be parallel, resistant to leakage, and scalable to\narbitrary context lengths. Our extensive experiments with an open-weight large\nlanguage model (LLM) reveal a pronounced gap between high- and low-resource\nlanguages, particularly for tasks requiring the model to aggregate multiple\nfacts or predict the absence of information. We also find that, in multilingual\nsettings, LLMs effectively utilize less than 30% of their claimed context\nlength. Although off-the-shelf Retrieval Augmented Generation helps alleviate\nthis to a certain extent, it does not solve the long-context problem. We\nopen-source MLRBench to enable future research in improved evaluation and\ntraining of multilingual LLMs.", "published": "2025-04-17 11:02:35", "link": "http://arxiv.org/abs/2504.12845v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "SMARTe: Slot-based Method for Accountable Relational Triple extraction", "abstract": "Relational Triple Extraction (RTE) is a fundamental task in Natural Language\nProcessing (NLP). However, prior research has primarily focused on optimizing\nmodel performance, with limited efforts to understand the internal mechanisms\ndriving these models. Many existing methods rely on complex preprocessing to\ninduce specific interactions, often resulting in opaque systems that may not\nfully align with their theoretical foundations. To address these limitations,\nwe propose SMARTe: a Slot-based Method for Accountable Relational Triple\nextraction. SMARTe introduces intrinsic interpretability through a slot\nattention mechanism and frames the task as a set prediction problem. Slot\nattention consolidates relevant information into distinct slots, ensuring all\npredictions can be explicitly traced to learned slot representations and the\ntokens contributing to each predicted relational triple. While emphasizing\ninterpretability, SMARTe achieves performance comparable to state-of-the-art\nmodels. Evaluations on the NYT and WebNLG datasets demonstrate that adding\ninterpretability does not compromise performance. Furthermore, we conducted\nqualitative assessments to showcase the explanations provided by SMARTe, using\nattention heatmaps that map to their respective tokens. We conclude with a\ndiscussion of our findings and propose directions for future research.", "published": "2025-04-17 10:21:15", "link": "http://arxiv.org/abs/2504.12816v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation", "abstract": "This study explored how large language models (LLMs) perform in two areas\nrelated to art: writing critiques of artworks and reasoning about mental states\n(Theory of Mind, or ToM) in art-related situations. For the critique generation\npart, we built a system that combines Noel Carroll's evaluative framework with\na broad selection of art criticism theories. The model was prompted to first\nwrite a full-length critique and then shorter, more coherent versions using a\nstep-by-step prompting process. These AI-generated critiques were then compared\nwith those written by human experts in a Turing test-style evaluation. In many\ncases, human subjects had difficulty telling which was which, and the results\nsuggest that LLMs can produce critiques that are not only plausible in style\nbut also rich in interpretation, as long as they are carefully guided. In the\nsecond part, we introduced new simple ToM tasks based on situations involving\ninterpretation, emotion, and moral tension, which can appear in the context of\nart. These go beyond standard false-belief tests and allow for more complex,\nsocially embedded forms of reasoning. We tested 41 recent LLMs and found that\ntheir performance varied across tasks and models. In particular, tasks that\ninvolved affective or ambiguous situations tended to reveal clearer\ndifferences. Taken together, these results help clarify how LLMs respond to\ncomplex interpretative challenges, revealing both their cognitive limitations\nand potential. While our findings do not directly contradict the so-called\nGenerative AI Paradox--the idea that LLMs can produce expert-like output\nwithout genuine understanding--they suggest that, depending on how LLMs are\ninstructed, such as through carefully designed prompts, these models may begin\nto show behaviors that resemble understanding more closely than we might\nassume.", "published": "2025-04-17 10:10:25", "link": "http://arxiv.org/abs/2504.12805v1", "categories": ["cs.CL", "cs.CY", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Towards Lossless Token Pruning in Late-Interaction Retrieval Models", "abstract": "Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens.", "published": "2025-04-17 09:18:58", "link": "http://arxiv.org/abs/2504.12778v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration", "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved\nremarkable progress in general domains and demonstrated promise in multimodal\nmathematical reasoning. However, applying MLLMs to geometry problem solving\n(GPS) remains challenging due to lack of accurate step-by-step solution data\nand severe hallucinations during reasoning. In this paper, we propose GeoGen, a\npipeline that can automatically generates step-wise reasoning paths for\ngeometry diagrams. By leveraging the precise symbolic reasoning,\n\\textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To\nfurther enhance the logical reasoning ability of MLLMs, we train\n\\textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated\nby GeoGen. Serving as a bridge between natural language and symbolic systems,\nGeoLogic enables symbolic tools to help verifying MLLM outputs, making the\nreasoning process more rigorous and alleviating hallucinations. Experimental\nresults show that our approach consistently improves the performance of MLLMs,\nachieving remarkable results on benchmarks for geometric reasoning tasks. This\nimprovement stems from our integration of the strengths of LLMs and symbolic\nsystems, which enables a more reliable and interpretable approach for the GPS\ntask. Codes are available at https://github.com/ycpNotFound/GeoGen.", "published": "2025-04-17 09:13:46", "link": "http://arxiv.org/abs/2504.12773v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts", "abstract": "We know that language models (LMs) form biases and stereotypes of minorities,\nleading to unfair treatments of members of these groups, thanks to research\nmainly in the US and the broader English-speaking world. As the negative\nbehavior of these models has severe consequences for society and individuals,\nindustry and academia are actively developing methods to reduce the bias in\nLMs. However, there are many under-represented groups and languages that have\nbeen overlooked so far. This includes marginalized groups that are specific to\nindividual countries and regions in the English speaking and Western world, but\ncrucially also almost all marginalized groups in the rest of the world. The UN\nestimates, that between 600 million to 1.2 billion people worldwide are members\nof marginalized groups and in need for special protection. If we want to\ndevelop inclusive LMs that work for everyone, we have to broaden our\nunderstanding to include overlooked marginalized groups and low-resource\nlanguages and dialects.\n  In this work, we contribute to this effort with the first study investigating\noffensive stereotyping bias in 23 LMs for 270 marginalized groups from Egypt,\nthe remaining 21 Arab countries, Germany, the UK, and the US. Additionally, we\ninvestigate the impact of low-resource languages and dialects on the study of\nbias in LMs, demonstrating the limitations of current bias metrics, as we\nmeasure significantly higher bias when using the Egyptian Arabic dialect versus\nModern Standard Arabic. Our results show, LMs indeed show higher bias against\nmany marginalized groups in comparison to dominant groups. However, this is not\nthe case for Arabic LMs, where the bias is high against both marginalized and\ndominant groups in relation to religion and ethnicity.\n  Our results also show higher intersectional bias against Non-binary, LGBTQIA+\nand Black women.", "published": "2025-04-17 09:05:50", "link": "http://arxiv.org/abs/2504.12767v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Chinese-Vicuna: A Chinese Instruction-following Llama-based Model", "abstract": "Chinese-Vicuna is an open-source, resource-efficient language model designed\nto bridge the gap in Chinese instruction-following capabilities by fine-tuning\nMeta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting\nlow-resource environments, it enables cost-effective deployment on consumer\nGPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation\nin fields like healthcare and law. By integrating hybrid datasets (BELLE and\nGuanaco) and 4-bit quantization (QLoRA), the model achieves competitive\nperformance in tasks such as translation, code generation, and domain-specific\nQ\\&A. The project provides a comprehensive toolkit for model conversion, CPU\ninference, and multi-turn dialogue interfaces, emphasizing accessibility for\nresearchers and developers. Evaluations indicate competitive performance across\nmedical tasks, multi-turn dialogue coherence, and real-time legal updates.\nChinese-Vicuna's modular design, open-source ecosystem, and community-driven\nenhancements position it as a versatile foundation for Chinese LLM\napplications.", "published": "2025-04-17 08:27:02", "link": "http://arxiv.org/abs/2504.12737v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge", "abstract": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language\nquestions (NLQs) by using structured sources such as tables, databases, and\nknowledge graphs in a unified way. Existing USKR methods either rely on\nemploying task-specific strategies or custom-defined representations, which\nstruggle to leverage the knowledge transfer between different SKR tasks or\nalign with the prior of LLMs, thereby limiting their performance. This paper\nproposes a novel USKR framework named \\textsc{Pandora}, which takes advantage\nof \\textsc{Python}'s \\textsc{Pandas} API to construct a unified knowledge\nrepresentation for alignment with LLM pre-training. It employs an LLM to\ngenerate textual reasoning steps and executable Python code for each question.\nDemonstrations are drawn from a memory of training examples that cover various\nSKR tasks, facilitating knowledge transfer. Extensive experiments on four\nbenchmarks involving three SKR tasks demonstrate that \\textsc{Pandora}\noutperforms existing unified frameworks and competes effectively with\ntask-specific methods.", "published": "2025-04-17 08:18:09", "link": "http://arxiv.org/abs/2504.12734v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "KODIS: A Multicultural Dispute Resolution Dialogue Corpus", "abstract": "We present KODIS, a dyadic dispute resolution corpus containing thousands of\ndialogues from over 75 countries. Motivated by a theoretical model of culture\nand conflict, participants engage in a typical customer service dispute\ndesigned by experts to evoke strong emotions and conflict. The corpus contains\na rich set of dispositional, process, and outcome measures. The initial\nanalysis supports theories of how anger expressions lead to escalatory spirals\nand highlights cultural differences in emotional expression. We make this\ncorpus and data collection framework available to the community.", "published": "2025-04-17 07:57:31", "link": "http://arxiv.org/abs/2504.12723v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations", "abstract": "Large language models (LLMs) frequently generate hallucinations-content that\ndeviates from factual accuracy or provided context-posing challenges for\ndiagnosis due to the complex interplay of underlying causes. This paper\nintroduces a subsequence association framework to systematically trace and\nunderstand hallucinations. Our key insight is that hallucinations arise when\ndominant hallucinatory associations outweigh faithful ones. Through theoretical\nand empirical analyses, we demonstrate that decoder-only transformers\neffectively function as subsequence embedding models, with linear layers\nencoding input-output associations. We propose a tracing algorithm that\nidentifies causal subsequences by analyzing hallucination probabilities across\nrandomized input contexts. Experiments show our method outperforms standard\nattribution techniques in identifying hallucination causes and aligns with\nevidence from the model's training corpus. This work provides a unified\nperspective on hallucinations and a robust framework for their tracing and\nanalysis.", "published": "2025-04-17 06:34:45", "link": "http://arxiv.org/abs/2504.12691v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Data-efficient LLM Fine-tuning for Code Generation", "abstract": "Large language models (LLMs) have demonstrated significant potential in code\ngeneration tasks. However, there remains a performance gap between open-source\nand closed-source models. To address this gap, existing approaches typically\ngenerate large amounts of synthetic data for fine-tuning, which often leads to\ninefficient training. In this work, we propose a data selection strategy in\norder to improve the effectiveness and efficiency of training for code-based\nLLMs. By prioritizing data complexity and ensuring that the sampled subset\naligns with the distribution of the original dataset, our sampling strategy\neffectively selects high-quality data. Additionally, we optimize the\ntokenization process through a \"dynamic pack\" technique, which minimizes\npadding tokens and reduces computational resource consumption. Experimental\nresults show that when training on 40% of the OSS-Instruct dataset, the\nDeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%,\nsurpassing the 66.1% performance with the full dataset. Moreover, training time\nis reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases\nfrom 61.47 GB to 42.72 GB during a single epoch. Similar improvements are\nobserved with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By\noptimizing both data selection and tokenization, our approach not only improves\nmodel performance but also improves training efficiency.", "published": "2025-04-17 06:29:28", "link": "http://arxiv.org/abs/2504.12687v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "abstract": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.", "published": "2025-04-17 06:16:40", "link": "http://arxiv.org/abs/2504.12682v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs", "abstract": "Large Language Models (LLMs) trained on extensive datasets often learn\nsensitive information, which raises significant social and legal concerns under\nprinciples such as the \"Right to be forgotten.\" Retraining entire models from\nscratch to remove undesired information is both costly and impractical.\nFurthermore, existing single-domain unlearning methods fail to address\nmulti-domain scenarios, where knowledge is interwoven across domains such as\nprivacy and copyright, creating overlapping representations that lead to\nexcessive knowledge removal or degraded performance. To tackle these issues, we\npropose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain\nunlearning framework. GRAIL leverages gradient information from multiple\ndomains to precisely distinguish the unlearning scope from the retention scope,\nand applies an adaptive parameter-wise localization strategy to selectively\nremove targeted knowledge while preserving critical parameters for each domain.\nExperimental results on unlearning benchmarks show that GRAIL achieves\nunlearning success on par with the existing approaches, while also\ndemonstrating up to 17% stronger knowledge retention success compared to the\nprevious state-of-art method. Our findings establish a new paradigm for\neffectively managing and regulating sensitive information in large-scale\npre-trained language models.", "published": "2025-04-17 06:16:32", "link": "http://arxiv.org/abs/2504.12681v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "abstract": "Abstractive compression utilizes smaller langauge models to condense\nquery-relevant context, reducing computational costs in retrieval-augmented\ngeneration (RAG). However,retrieved documents often include information that is\neither irrelevant to answering the query or misleading due to factual incorrect\ncontent, despite having high relevance scores. This behavior indicates that\nabstractive compressors are more likely to omit important information essential\nfor the correct answer, especially in long contexts where attention dispersion\noccurs. To address this issue, we categorize retrieved documents in a more\nfine-grained manner and propose Abstractive Compression Robust against Noise\n(ACoRN), which introduces two novel training steps. First, we use offline data\naugmentation on the training dataset to enhance compressor robustness against\ntwo distinct types of retrieval noise. Second, since the language modelbased\ncompressor cannot fully utilize information from multiple retrieved documents\nand exhibits positional bias, we perform finetuning to generate summaries\ncentered around key information that directly supports the correct answer. Our\nexperiments demonstrate that T5-large, trained with ACoRN as a compressor,\nimproves EM and F1 scores while preserving the answer string, which could serve\nas direct evidence. ACoRN excels on datasets with many accuracy-reducing\ndocuments, making it highly useful in real-world scenarios.", "published": "2025-04-17 06:05:35", "link": "http://arxiv.org/abs/2504.12673v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment", "abstract": "Aligning language models with human preferences presents significant\nchallenges, particularly in achieving personalization without incurring\nexcessive computational costs. Existing methods rely on reward signals and\nadditional annotated data, limiting their scalability and adaptability to\ndiverse human values. To address these challenges, we introduce Persona-judge,\na novel discriminative paradigm that enables training-free personalized\nalignment with unseen preferences. Instead of optimizing policy parameters\nthrough external reward feedback, Persona-judge leverages the intrinsic\npreference judgment capabilities of the model. Specifically, a draft model\ngenerates candidate tokens conditioned on a given preference, while a judge\nmodel, embodying another preference, cross-validates the predicted tokens\nwhether to be accepted. Experimental results demonstrate that Persona-judge,\nusing the inherent preference evaluation mechanisms of the model, offers a\nscalable and computationally efficient solution to personalized alignment,\npaving the way for more adaptive customized alignment.", "published": "2025-04-17 05:50:13", "link": "http://arxiv.org/abs/2504.12663v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization", "abstract": "Aligning Vision-Language Models (VLMs) with safety standards is essential to\nmitigate risks arising from their multimodal complexity, where integrating\nvision and language unveils subtle threats beyond the reach of conventional\nsafeguards. Inspired by the insight that reasoning across modalities is key to\npreempting intricate vulnerabilities, we propose a novel direction for VLM\nsafety: multimodal reasoning-driven prompt rewriting. To this end, we introduce\nVLMGuard-R1, a proactive framework that refines user inputs through a\nreasoning-guided rewriter, dynamically interpreting text-image interactions to\ndeliver refined prompts that bolster safety across diverse VLM architectures\nwithout altering their core parameters. To achieve this, we devise a\nthree-stage reasoning pipeline to synthesize a dataset that trains the rewriter\nto infer subtle threats, enabling tailored, actionable responses over generic\nrefusals. Extensive experiments across three benchmarks with five VLMs reveal\nthat VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1\nachieves a remarkable 43.59\\% increase in average safety across five models on\nthe SIUO benchmark.", "published": "2025-04-17 05:46:41", "link": "http://arxiv.org/abs/2504.12661v1", "categories": ["cs.LG", "cs.CL", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation", "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only\ndue to the quadratic scaling of computational complexity with sequence length\nbut also because of the scarcity and expense of annotating long-context data.\nThere has been barely any open-source work that systematically ablates\nlong-context data, nor is there any openly available instruction tuning dataset\nwith contexts surpassing 100K tokens. To bridge this gap, we introduce a novel\npost-training synthetic data generation strategy designed to efficiently extend\nthe context window of LLMs while preserving their general task performance. Our\napproach scalably extends to arbitrarily long context lengths, unconstrained by\nthe length of available real-world data, which effectively addresses the\nscarcity of raw long-context data. Through a step-by-step rotary position\nembedding (RoPE) scaling training strategy, we demonstrate that our model, with\na context length of up to 1M tokens, performs well on the RULER benchmark and\nInfiniteBench and maintains robust performance on general language tasks.", "published": "2025-04-17 04:46:57", "link": "http://arxiv.org/abs/2504.12637v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs", "abstract": "Large Language Models (LLMs) not only have solved complex reasoning problems\nbut also exhibit remarkable performance in tasks that require subjective\ndecision making. Existing studies suggest that LLM generations can be\nsubjectively grounded to some extent, yet exploring whether LLMs can account\nfor individual-level subjectivity has not been sufficiently studied. In this\npaper, we characterize subjectivity of individuals on social media and infer\ntheir moral judgments using LLMs. We propose a framework, SOLAR (Subjective\nGround with Value Abstraction), that observes value conflicts and trade-offs in\nthe user-generated texts to better represent subjective ground of individuals.\nEmpirical results show that our framework improves overall inference results as\nwell as performance on controversial situations. Additionally, we qualitatively\nshow that SOLAR provides explanations about individuals' value preferences,\nwhich can further account for their judgments.", "published": "2025-04-17 04:20:05", "link": "http://arxiv.org/abs/2504.12633v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning", "abstract": "Geometry problem-solving (GPS), a challenging task requiring both visual\ncomprehension and symbolic reasoning, effectively measures the reasoning\ncapabilities of multimodal large language models (MLLMs). Humans exhibit strong\nreasoning ability in this task through accurate identification and adaptive\napplication of geometric principles within visual contexts. However, existing\nbenchmarks fail to jointly assess both dimensions of the human-like geometric\nreasoning mechanism in MLLMs, remaining a critical gap in assessing their\nability to tackle GPS. To this end, we introduce GeoSense, the first\ncomprehensive bilingual benchmark designed to systematically evaluate the\ngeometric reasoning abilities of MLLMs through the lens of geometric\nprinciples. GeoSense features a five-level hierarchical framework of geometric\nprinciples spanning plane and solid geometry, an intricately annotated dataset\nof 1,789 problems, and an innovative evaluation strategy. Through extensive\nexperiments on GeoSense with various open-source and closed-source MLLMs, we\nobserve that Gemini-2.0-pro-flash performs best, achieving an overall score of\n$65.3$. Our in-depth analysis reveals that the identification and application\nof geometric principles remain a bottleneck for leading MLLMs, jointly\nhindering their reasoning abilities. These findings underscore GeoSense's\npotential to guide future advancements in MLLMs' geometric reasoning\ncapabilities, paving the way for more robust and human-like reasoning in\nartificial intelligence.", "published": "2025-04-17 02:46:27", "link": "http://arxiv.org/abs/2504.12597v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Simplifying Graph Transformers", "abstract": "Transformers have attained outstanding performance across various modalities,\nemploying scaled-dot-product (SDP) attention mechanisms. Researchers have\nattempted to migrate Transformers to graph learning, but most advanced Graph\nTransformers are designed with major architectural differences, either\nintegrating message-passing or incorporating sophisticated attention\nmechanisms. These complexities prevent the easy adoption of Transformer\ntraining advances. We propose three simple modifications to the plain\nTransformer to render it applicable to graphs without introducing major\narchitectural distortions. Specifically, we advocate for the use of (1)\nsimplified $L_2$ attention to measure the magnitude closeness of tokens; (2)\nadaptive root-mean-square normalization to preserve token magnitude\ninformation; and (3) a relative positional encoding bias with a shared encoder.\nSignificant performance gains across a variety of graph datasets justify the\neffectiveness of our proposed modifications. Furthermore, empirical evaluation\non the expressiveness benchmark reveals noteworthy realized expressiveness in\nthe graph isomorphism.", "published": "2025-04-17 02:06:50", "link": "http://arxiv.org/abs/2504.12588v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models", "abstract": "Large language models (LLMs) sometimes fail to respond appropriately to\ndeterministic tasks -- such as counting or forming acronyms -- because the\nimplicit prior distribution they have learned over sequences of tokens\ninfluences their responses. In this work, we show that, in at least some cases,\nLLMs actually compute the information needed to perform these tasks correctly,\nand we identify some interventions that can allow them to access this\ninformation to improve their performance. First, we show that simply prompting\nthe language model to not rely on its prior knowledge leads to dramatic\nimprovements in prior-dominated tasks. We then use mechanistic interpretability\ntechniques to localize the prior within the LLM and manipulate the extent to\nwhich that prior influences its responses. Specifically, we show that it is\npossible to identify layers of the underlying neural network that correlate\nwith the prior probability of a response and that lightweight finetuning of\nthese layers with basic prompts on prior-dominated tasks achieves high\nperformance on held-out answers. These results suggest that the information\nrequired to produce a correct response is contained within the representations\nof the problems formed by the models. Furthermore, we show that this finetuning\nis significantly more effective for prior-dominated tasks, and that the error\nafter finetuning is no longer correlated with the prior. Our results suggest\nthat it may be possible to define effective methods for manipulating the extent\nto which LLMs rely upon their priors in solving problems, potentially\nincreasing their performance in settings where LLMs hallucinate for reasons\nrelated to the prior probability of token sequences.", "published": "2025-04-17 02:00:53", "link": "http://arxiv.org/abs/2504.12585v1", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Provable Secure Steganography Based on Adaptive Dynamic Sampling", "abstract": "The security of private communication is increasingly at risk due to\nwidespread surveillance. Steganography, a technique for embedding secret\nmessages within innocuous carriers, enables covert communication over monitored\nchannels. Provably Secure Steganography (PSS) is state of the art for making\nstego carriers indistinguishable from normal ones by ensuring computational\nindistinguishability between stego and cover distributions. However, current\nPSS methods often require explicit access to the distribution of generative\nmodel for both sender and receiver, limiting their practicality in black box\nscenarios. In this paper, we propose a provably secure steganography scheme\nthat does not require access to explicit model distributions for both sender\nand receiver. Our method incorporates a dynamic sampling strategy, enabling\ngenerative models to embed secret messages within multiple sampling choices\nwithout disrupting the normal generation process of the model. Extensive\nevaluations of three real world datasets and three LLMs demonstrate that our\nblackbox method is comparable with existing white-box steganography methods in\nterms of efficiency and capacity while eliminating the degradation of\nsteganography in model generated outputs.", "published": "2025-04-17 01:52:09", "link": "http://arxiv.org/abs/2504.12579v1", "categories": ["cs.CR", "cs.CL"], "primary_category": "cs.CR"}
{"title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation", "abstract": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data\ngenerated using larger Language models. Questions remain about leveraging\nsynthetic data for other use cases, such as adapting LLMs to specific domains.\nA key limitation of synthetic data is low diversity, which negatively impacts\nits downstream applicability for improving other models. To address this, we\npropose MetaSynth, a method for generating synthetic data that enhances\ndiversity through meta-prompting, where a language model orchestrates multiple\n\"expert\" LLM agents to collaboratively generate data. Using only 25 million\ntokens of synthetic data generated with MetaSynth, we successfully adapt a\nwell-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and\nBiomedicine-without compromising the capabilities of the resulting model in\ngeneral tasks. In addition, we evaluate the diversity of our synthetic data\nusing seven automated metrics, and find that it approaches the diversity of LLM\npre-training corpora.\n  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms\nthe base LLM, showing improvements of up to 4.08% in Finance and 13.75% in\nBiomedicine. The same model shows degraded performance when trained on data\ngenerated using a template prompt, even when the template includes prior\ngenerations and varying In-Context exemplars of real data. Our findings suggest\nthat a few million tokens of diverse synthetic data without mixing any real\ndata, is sufficient for effective domain adaptation when using MetaSynth.", "published": "2025-04-17 01:25:15", "link": "http://arxiv.org/abs/2504.12563v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition", "abstract": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.", "published": "2025-04-17 01:23:50", "link": "http://arxiv.org/abs/2504.12562v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has significantly enhanced large\nlanguage models (LLMs) in knowledge-intensive tasks by incorporating external\nknowledge retrieval. However, existing RAG frameworks primarily rely on\nsemantic similarity and correlation-driven retrieval, limiting their ability to\ndistinguish true causal relationships from spurious associations. This results\nin responses that may be factually grounded but fail to establish\ncause-and-effect mechanisms, leading to incomplete or misleading insights. To\naddress this issue, we introduce Causal Dynamic Feedback for Adaptive\nRetrieval-Augmented Generation (CDF-RAG), a framework designed to improve\ncausal consistency, factual accuracy, and explainability in generative\nreasoning. CDF-RAG iteratively refines queries, retrieves structured causal\ngraphs, and enables multi-hop causal reasoning across interconnected knowledge\nsources. Additionally, it validates responses against causal pathways, ensuring\nlogically coherent and factually grounded outputs. We evaluate CDF-RAG on four\ndiverse datasets, demonstrating its ability to improve response accuracy and\ncausal correctness over existing RAG-based methods. Our code is publicly\navailable at https://github.com/ elakhatibi/CDF-RAG.", "published": "2025-04-17 01:15:13", "link": "http://arxiv.org/abs/2504.12560v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Benchmarking LLM-based Relevance Judgment Methods", "abstract": "Large Language Models (LLMs) are increasingly deployed in both academic and\nindustry settings to automate the evaluation of information seeking systems,\nparticularly by generating graded relevance judgments. Previous work on\nLLM-based relevance assessment has primarily focused on replicating graded\nhuman relevance judgments through various prompting strategies. However, there\nhas been limited exploration of alternative assessment methods or comprehensive\ncomparative studies. In this paper, we systematically compare multiple\nLLM-based relevance assessment methods, including binary relevance judgments,\ngraded relevance assessments, pairwise preference-based methods, and two\nnugget-based evaluation methods~--~document-agnostic and document-dependent. In\naddition to a traditional comparison based on system rankings using Kendall\ncorrelations, we also examine how well LLM judgments align with human\npreferences, as inferred from relevance grades. We conduct extensive\nexperiments on datasets from three TREC Deep Learning tracks 2019, 2020 and\n2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain\nquestion answering. As part of our data release, we include relevance judgments\ngenerated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.\nOur goal is to \\textit{reproduce} various LLM-based relevance judgment methods\nto provide a comprehensive comparison. All code, data, and resources are\npublicly available in our GitHub Repository at\nhttps://github.com/Narabzad/llm-relevance-judgement-comparison.", "published": "2025-04-17 01:13:21", "link": "http://arxiv.org/abs/2504.12558v1", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "ELAB: Extensive LLM Alignment Benchmark in Persian Language", "abstract": "This paper presents a comprehensive evaluation framework for aligning Persian\nLarge Language Models (LLMs) with critical ethical dimensions, including\nsafety, fairness, and social norms. It addresses the gaps in existing LLM\nevaluation frameworks by adapting them to Persian linguistic and cultural\ncontexts. This benchmark creates three types of Persian-language benchmarks:\n(i) translated data, (ii) new data generated synthetically, and (iii) new\nnaturally collected data. We translate Anthropic Red Teaming data, AdvBench,\nHarmBench, and DecodingTrust into Persian. Furthermore, we create\nProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets\nto address harmful and prohibited content in indigenous culture. Moreover, we\ncollect extensive dataset as GuardBench-fa to consider Persian cultural norms.\nBy combining these datasets, our work establishes a unified framework for\nevaluating Persian LLMs, offering a new approach to culturally grounded\nalignment evaluation. A systematic evaluation of Persian LLMs is performed\nacross the three alignment aspects: safety (avoiding harmful content), fairness\n(mitigating biases), and social norms (adhering to culturally accepted\nbehaviors). We present a publicly available leaderboard that benchmarks Persian\nLLMs with respect to safety, fairness, and social norms at:\nhttps://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation.", "published": "2025-04-17 00:50:41", "link": "http://arxiv.org/abs/2504.12553v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Memorization: A Close Look at Books", "abstract": "To what extent can entire books be extracted from LLMs? Using the Llama 3 70B\nfamily of models, and the \"prefix-prompting\" extraction technique, we were able\nto auto-regressively reconstruct, with a very high level of similarity, one\nentire book (Alice's Adventures in Wonderland) from just the first 500 tokens.\nWe were also able to obtain high extraction rates on several other books,\npiece-wise. However, these successes do not extend uniformly to all books. We\nshow that extraction rates of books correlate with book popularity and thus,\nlikely duplication in the training data.\n  We also confirm the undoing of mitigations in the instruction-tuned Llama\n3.1, following recent work (Nasr et al., 2025). We further find that this\nundoing comes from changes to only a tiny fraction of weights concentrated\nprimarily in the lower transformer blocks. Our results provide evidence of the\nlimits of current regurgitation mitigation strategies and introduce a framework\nfor studying how fine-tuning affects the retrieval of verbatim memorization in\naligned LLMs.", "published": "2025-04-17 00:20:18", "link": "http://arxiv.org/abs/2504.12549v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice", "abstract": "Mass-shooting events pose a significant challenge to public safety,\ngenerating large volumes of unstructured textual data that hinder effective\ninvestigations and the formulation of public policy. Despite the urgency, few\nprior studies have effectively automated the extraction of key information from\nthese events to support legal and investigative efforts. This paper presented\nthe first dataset designed for knowledge acquisition on mass-shooting events\nthrough the application of named entity recognition (NER) techniques. It\nfocuses on identifying key entities such as offenders, victims, locations, and\ncriminal instruments, that are vital for legal and investigative purposes. The\nNER process is powered by Large Language Models (LLMs) using few-shot\nprompting, facilitating the efficient extraction and organization of critical\ninformation from diverse sources, including news articles, police reports, and\nsocial media. Experimental results on real-world mass-shooting corpora\ndemonstrate that GPT-4o is the most effective model for mass-shooting NER,\nachieving the highest Micro Precision, Micro Recall, and Micro F1-scores.\nMeanwhile, o1-mini delivers competitive performance, making it a\nresource-efficient alternative for less complex NER tasks. It is also observed\nthat increasing the shot count enhances the performance of all models, but the\ngains are more substantial for GPT-4o and o1-mini, highlighting their superior\nadaptability to few-shot learning scenarios.", "published": "2025-04-17 00:13:04", "link": "http://arxiv.org/abs/2504.12545v1", "categories": ["cs.CY", "cs.AI", "cs.CL"], "primary_category": "cs.CY"}
{"title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "abstract": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.", "published": "2025-04-17 17:59:56", "link": "http://arxiv.org/abs/2504.13180v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization", "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.", "published": "2025-04-17 17:59:33", "link": "http://arxiv.org/abs/2504.13173v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "RUKA: Rethinking the Design of Humanoid Hands with Learning", "abstract": "Dexterous manipulation is a fundamental capability for robotic systems, yet\nprogress has been limited by hardware trade-offs between precision,\ncompactness, strength, and affordability. Existing control methods impose\ncompromises on hand designs and applications. However, learning-based\napproaches present opportunities to rethink these trade-offs, particularly to\naddress challenges with tendon-driven actuation and low-cost materials. This\nwork presents RUKA, a tendon-driven humanoid hand that is compact, affordable,\nand capable. Made from 3D-printed parts and off-the-shelf components, RUKA has\n5 fingers with 15 underactuated degrees of freedom enabling diverse human-like\ngrasps. Its tendon-driven actuation allows powerful grasping in a compact,\nhuman-sized form factor. To address control challenges, we learn\njoint-to-actuator and fingertip-to-actuator models from motion-capture data\ncollected by the MANUS glove, leveraging the hand's morphological accuracy.\nExtensive evaluations demonstrate RUKA's superior reachability, durability, and\nstrength compared to other robotic hands. Teleoperation tasks further showcase\nRUKA's dexterous movements. The open-source design and assembly instructions of\nRUKA, code, and data are available at https://ruka-hand.github.io/.", "published": "2025-04-17 17:58:59", "link": "http://arxiv.org/abs/2504.13165v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Readable Twins of Unreadable Models", "abstract": "Creating responsible artificial intelligence (AI) systems is an important\nissue in contemporary research and development of works on AI. One of the\ncharacteristics of responsible AI systems is their explainability. In the\npaper, we are interested in explainable deep learning (XDL) systems. On the\nbasis of the creation of digital twins of physical objects, we introduce the\nidea of creating readable twins (in the form of imprecise information flow\nmodels) for unreadable deep learning models. The complete procedure for\nswitching from the deep learning model (DLM) to the imprecise information flow\nmodel (IIFM) is presented. The proposed approach is illustrated with an example\nof a deep learning classification model for image recognition of handwritten\ndigits from the MNIST data set.", "published": "2025-04-17 17:55:34", "link": "http://arxiv.org/abs/2504.13150v1", "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Exploring Expert Failures Improves LLM Agent Tuning", "abstract": "Large Language Models (LLMs) have shown tremendous potential as agents,\nexcelling at tasks that require multiple rounds of reasoning and interactions.\nRejection Sampling Fine-Tuning (RFT) has emerged as an effective method for\nfinetuning LLMs as agents: it first imitates expert-generated successful\ntrajectories and further improves agentic skills through iterative fine-tuning\non successful, self-generated trajectories. However, since the expert (e.g.,\nGPT-4) succeeds primarily on simpler subtasks and RFT inherently favors simpler\nscenarios, many complex subtasks remain unsolved and persistently\nout-of-distribution (OOD). Upon investigating these challenging subtasks, we\ndiscovered that previously failed expert trajectories can often provide\nvaluable guidance, e.g., plans and key actions, that can significantly improve\nagent exploration efficiency and acquisition of critical skills. Motivated by\nthese observations, we propose Exploring Expert Failures (EEF), which\nidentifies beneficial actions from failed expert trajectories and integrates\nthem into the training dataset. Potentially harmful actions are meticulously\nexcluded to prevent contamination of the model learning process. By leveraging\nthe beneficial actions in expert failures, EEF successfully solves some\npreviously unsolvable subtasks and improves agent tuning performance.\nRemarkably, our approach achieved a 62\\% win rate in WebShop, outperforming RFT\n(53. 6\\%) and GPT-4 (35. 6\\%), and to the best of our knowledge, setting a new\nstate-of-the-art as the first method to surpass a score of 0.81 in WebShop and\nexceed 81 in SciWorld.", "published": "2025-04-17 17:53:54", "link": "http://arxiv.org/abs/2504.13145v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "$\\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark", "abstract": "We introduce $\\texttt{Complex-Edit}$, a comprehensive benchmark designed to\nsystematically evaluate instruction-based image editing models across\ninstructions of varying complexity. To develop this benchmark, we harness\nGPT-4o to automatically collect a diverse set of editing instructions at scale.\nOur approach follows a well-structured ``Chain-of-Edit'' pipeline: we first\ngenerate individual atomic editing tasks independently and then integrate them\nto form cohesive, complex instructions. Additionally, we introduce a suite of\nmetrics to assess various aspects of editing performance, along with a\nVLM-based auto-evaluation pipeline that supports large-scale assessments. Our\nbenchmark yields several notable insights: 1) Open-source models significantly\nunderperform relative to proprietary, closed-source models, with the\nperformance gap widening as instruction complexity increases; 2) Increased\ninstructional complexity primarily impairs the models' ability to retain key\nelements from the input images and to preserve the overall aesthetic quality;\n3) Decomposing a complex instruction into a sequence of atomic steps, executed\nin a step-by-step manner, substantially degrades performance across multiple\nmetrics; 4) A straightforward Best-of-N selection strategy improves results for\nboth direct editing and the step-by-step sequential approach; and 5) We observe\na ``curse of synthetic data'': when synthetic data is involved in model\ntraining, the edited images from such models tend to appear increasingly\nsynthetic as the complexity of the editing instructions rises -- a phenomenon\nthat intriguingly also manifests in the latest GPT-4o outputs.", "published": "2025-04-17 17:51:59", "link": "http://arxiv.org/abs/2504.13143v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: Methods and Results", "abstract": "This paper presents a review for the NTIRE 2025 Challenge on Short-form UGC\nVideo Quality Assessment and Enhancement. The challenge comprises two tracks:\n(i) Efficient Video Quality Assessment (KVQ), and (ii) Diffusion-based Image\nSuper-Resolution (KwaiSR). Track 1 aims to advance the development of\nlightweight and efficient video quality assessment (VQA) models, with an\nemphasis on eliminating reliance on model ensembles, redundant weights, and\nother computationally expensive components in the previous IQA/VQA\ncompetitions. Track 2 introduces a new short-form UGC dataset tailored for\nsingle image super-resolution, i.e., the KwaiSR dataset. It consists of 1,800\nsynthetically generated S-UGC image pairs and 1,900 real-world S-UGC images,\nwhich are split into training, validation, and test sets using a ratio of\n8:1:1. The primary objective of the challenge is to drive research that\nbenefits the user experience of short-form UGC platforms such as Kwai and\nTikTok. This challenge attracted 266 participants and received 18 valid final\nsubmissions with corresponding fact sheets, significantly contributing to the\nprogress of short-form UGC VQA and image superresolution. The project is\npublicly available at https://github.com/lixinustc/KVQE-\nChallengeCVPR-NTIRE2025.", "published": "2025-04-17 17:45:34", "link": "http://arxiv.org/abs/2504.13131v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis", "abstract": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.", "published": "2025-04-17 17:44:19", "link": "http://arxiv.org/abs/2504.13129v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training", "abstract": "In recent years, the field of vision-language model pre-training has\nexperienced rapid advancements, driven primarily by the continuous enhancement\nof textual capabilities in large language models. However, existing training\nparadigms for multimodal large language models heavily rely on high-quality\nimage-text pairs. As models and data scales grow exponentially, the\navailability of such meticulously curated data has become increasingly scarce\nand saturated, thereby severely limiting further advancements in this domain.\nThis study investigates scalable caption generation techniques for\nvision-language model pre-training and demonstrates that large-scale\nlow-hallucination synthetic captions can serve dual purposes: 1) acting as a\nviable alternative to real-world data for pre-training paradigms and 2)\nachieving superior performance enhancement when integrated into vision-language\nmodels through empirical validation. This paper presents three key\ncontributions: 1) a novel pipeline for generating high-quality,\nlow-hallucination, and knowledge-rich synthetic captions. Our continuous DPO\nmethodology yields remarkable results in reducing hallucinations. Specifically,\nthe non-hallucination caption rate on a held-out test set increases from 48.2%\nto 77.9% for a 7B-size model. 2) Comprehensive empirical validation reveals\nthat our synthetic captions confer superior pre-training advantages over their\ncounterparts. Across 35 vision language tasks, the model trained with our data\nachieves a significant performance gain of at least 6.2% compared to alt-text\npairs and other previous work. Meanwhile, it also offers considerable support\nin the text-to-image domain. With our dataset, the FID score is reduced by 17.1\non a real-world validation benchmark and 13.3 on the MSCOCO validation\nbenchmark. 3) We will release Hunyuan-Recap100M, a low-hallucination and\nknowledge-intensive synthetic caption dataset.", "published": "2025-04-17 17:40:06", "link": "http://arxiv.org/abs/2504.13123v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition", "abstract": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.", "published": "2025-04-17 17:11:32", "link": "http://arxiv.org/abs/2504.13102v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research", "abstract": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.", "published": "2025-04-17 17:10:33", "link": "http://arxiv.org/abs/2504.13101v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Enhancing Person-to-Person Virtual Try-On with Multi-Garment Virtual Try-Off", "abstract": "Computer vision is transforming fashion through Virtual Try-On (VTON) and\nVirtual Try-Off (VTOFF). VTON generates images of a person in a specified\ngarment using a target photo and a standardized garment image, while a more\nchallenging variant, Person-to-Person Virtual Try-On (p2p-VTON), uses a photo\nof another person wearing the garment. VTOFF, on the other hand, extracts\nstandardized garment images from clothed individuals. We introduce TryOffDiff,\na diffusion-based VTOFF model. Built on a latent diffusion framework with\nSigLIP image conditioning, it effectively captures garment properties like\ntexture, shape, and patterns. TryOffDiff achieves state-of-the-art results on\nVITON-HD and strong performance on DressCode dataset, covering upper-body,\nlower-body, and dresses. Enhanced with class-specific embeddings, it pioneers\nmulti-garment VTOFF, the first of its kind. When paired with VTON models, it\nimproves p2p-VTON by minimizing unwanted attribute transfer, such as skin\ncolor. Code is available at: https://rizavelioglu.github.io/tryoffdiff/", "published": "2025-04-17 16:45:18", "link": "http://arxiv.org/abs/2504.13078v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Design Topological Materials by Reinforcement Fine-Tuned Generative Model", "abstract": "Topological insulators (TIs) and topological crystalline insulators (TCIs)\nare materials with unconventional electronic properties, making their discovery\nhighly valuable for practical applications. However, such materials,\nparticularly those with a full band gap, remain scarce. Given the limitations\nof traditional approaches that scan known materials for candidates, we focus on\nthe generation of new topological materials through a generative model.\nSpecifically, we apply reinforcement fine-tuning (ReFT) to a pre-trained\ngenerative model, thereby aligning the model's objectives with our material\ndesign goals. We demonstrate that ReFT is effective in enhancing the model's\nability to generate TIs and TCIs, with minimal compromise on the stability of\nthe generated materials. Using the fine-tuned model, we successfully identify a\nlarge number of new topological materials, with Ge$_2$Bi$_2$O$_6$ serving as a\nrepresentative example--a TI with a full band gap of 0.26 eV, ranking among the\nlargest known in this category.", "published": "2025-04-17 16:05:24", "link": "http://arxiv.org/abs/2504.13048v1", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "primary_category": "cond-mat.mtrl-sci"}
{"title": "Event-Enhanced Blurry Video Super-Resolution", "abstract": "In this paper, we tackle the task of blurry video super-resolution (BVSR),\naiming to generate high-resolution (HR) videos from low-resolution (LR) and\nblurry inputs. Current BVSR methods often fail to restore sharp details at high\nresolutions, resulting in noticeable artifacts and jitter due to insufficient\nmotion information for deconvolution and the lack of high-frequency details in\nLR frames. To address these challenges, we introduce event signals into BVSR\nand propose a novel event-enhanced network, Ev-DeblurVSR. To effectively fuse\ninformation from frames and events for feature deblurring, we introduce a\nreciprocal feature deblurring module that leverages motion information from\nintra-frame events to deblur frame features while reciprocally using global\nscene context from the frames to enhance event features. Furthermore, to\nenhance temporal consistency, we propose a hybrid deformable alignment module\nthat fully exploits the complementary motion information from inter-frame\nevents and optical flow to improve motion estimation in the deformable\nalignment process. Extensive evaluations demonstrate that Ev-DeblurVSR\nestablishes a new state-of-the-art performance on both synthetic and real-world\ndatasets. Notably, on real data, our method is +2.59 dB more accurate and\n7.28$\\times$ faster than the recent best BVSR baseline FMA-Net. Code:\nhttps://github.com/DachunKai/Ev-DeblurVSR.", "published": "2025-04-17 15:55:41", "link": "http://arxiv.org/abs/2504.13042v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Towards Cardiac MRI Foundation Models: Comprehensive Visual-Tabular Representations for Whole-Heart Assessment and Beyond", "abstract": "Cardiac magnetic resonance imaging is the gold standard for non-invasive\ncardiac assessment, offering rich spatio-temporal views of the cardiac anatomy\nand physiology. Patient-level health factors, such as demographics, metabolic,\nand lifestyle, are known to substantially influence cardiovascular health and\ndisease risk, yet remain uncaptured by CMR alone. To holistically understand\ncardiac health and to enable the best possible interpretation of an\nindividual's disease risk, CMR and patient-level factors must be jointly\nexploited within an integrated framework. Recent multi-modal approaches have\nbegun to bridge this gap, yet they often rely on limited spatio-temporal data\nand focus on isolated clinical tasks, thereby hindering the development of a\ncomprehensive representation for cardiac health evaluation. To overcome these\nlimitations, we introduce ViTa, a step toward foundation models that delivers a\ncomprehensive representation of the heart and a precise interpretation of\nindividual disease risk. Leveraging data from 42,000 UK Biobank participants,\nViTa integrates 3D+T cine stacks from short-axis and long-axis views, enabling\na complete capture of the cardiac cycle. These imaging data are then fused with\ndetailed tabular patient-level factors, enabling context-aware insights. This\nmulti-modal paradigm supports a wide spectrum of downstream tasks, including\ncardiac phenotype and physiological feature prediction, segmentation, and\nclassification of cardiac and metabolic diseases within a single unified\nframework. By learning a shared latent representation that bridges rich imaging\nfeatures and patient context, ViTa moves beyond traditional, task-specific\nmodels toward a universal, patient-specific understanding of cardiac health,\nhighlighting its potential to advance clinical utility and scalability in\ncardiac analysis.", "published": "2025-04-17 15:46:19", "link": "http://arxiv.org/abs/2504.13037v1", "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Prototypes are Balanced Units for Efficient and Effective Partially Relevant Video Retrieval", "abstract": "In a retrieval system, simultaneously achieving search accuracy and\nefficiency is inherently challenging. This challenge is particularly pronounced\nin partially relevant video retrieval (PRVR), where incorporating more diverse\ncontext representations at varying temporal scales for each video enhances\naccuracy but increases computational and memory costs. To address this\ndichotomy, we propose a prototypical PRVR framework that encodes diverse\ncontexts within a video into a fixed number of prototypes. We then introduce\nseveral strategies to enhance text association and video understanding within\nthe prototypes, along with an orthogonal objective to ensure that the\nprototypes capture a diverse range of content. To keep the prototypes\nsearchable via text queries while accurately encoding video contexts, we\nimplement cross- and uni-modal reconstruction tasks. The cross-modal\nreconstruction task aligns the prototypes with textual features within a shared\nspace, while the uni-modal reconstruction task preserves all video contexts\nduring encoding. Additionally, we employ a video mixing technique to provide\nweak guidance to further align prototypes and associated textual\nrepresentations. Extensive evaluations on TVR, ActivityNet-Captions, and\nQVHighlights validate the effectiveness of our approach without sacrificing\nefficiency.", "published": "2025-04-17 15:43:29", "link": "http://arxiv.org/abs/2504.13035v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning", "abstract": "Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.", "published": "2025-04-17 15:41:39", "link": "http://arxiv.org/abs/2504.13032v1", "categories": ["cs.AI", "cs.IR"], "primary_category": "cs.AI"}
{"title": "Pose and Facial Expression Transfer by using StyleGAN", "abstract": "We propose a method to transfer pose and expression between face images.\nGiven a source and target face portrait, the model produces an output image in\nwhich the pose and expression of the source face image are transferred onto the\ntarget identity. The architecture consists of two encoders and a mapping\nnetwork that projects the two inputs into the latent space of StyleGAN2, which\nfinally generates the output. The training is self-supervised from video\nsequences of many individuals. Manual labeling is not required. Our model\nenables the synthesis of random identities with controllable pose and\nexpression. Close-to-real-time performance is achieved.", "published": "2025-04-17 15:29:41", "link": "http://arxiv.org/abs/2504.13021v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving", "abstract": "Serving Large Language Models (LLMs) is critical for AI-powered applications\nbut demands substantial computational resources, particularly in memory\nbandwidth and computational throughput. Low-precision computation has emerged\nas a key technique to improve efficiency while reducing resource consumption.\nExisting approaches for generating low-precision kernels are limited to weight\nbit widths that are powers of two and suffer from suboptimal performance due to\nhigh-level GPU programming abstractions. These abstractions restrict critical\noptimizations, such as fine-grained register management and optimized memory\naccess patterns, which are essential for efficient low-precision computations.\nIn this paper, we introduce a virtual machine (VM) designed for General-Purpose\nGPU (GPGPU) computing, enabling support for low-precision data types with\narbitrary bit widths while maintaining GPU programmability. The proposed VM\nfeatures a thread-block-level programming model, a hierarchical memory space, a\nnovel algebraic layout system, and extensive support for diverse low-precision\ndata types. VM programs are compiled into highly efficient GPU programs with\nautomatic vectorization and instruction selection. Extensive experiments\ndemonstrate that our VM efficiently supports a full spectrum of low-precision\ndata types, and outperforms state-of-the-art low-precision kernels on their\nsupported types. Compared to existing compilers like Triton and Ladder, as well\nas hand-optimized kernels such as QuantLLM and Marlin, our VM achieves\nperformance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.", "published": "2025-04-17 14:45:03", "link": "http://arxiv.org/abs/2504.12984v1", "categories": ["cs.LG", "cs.AI", "cs.PL"], "primary_category": "cs.LG"}
{"title": "Transferrable Surrogates in Expressive Neural Architecture Search Spaces", "abstract": "Neural architecture search (NAS) faces a challenge in balancing the\nexploration of expressive, broad search spaces that enable architectural\ninnovation with the need for efficient evaluation of architectures to\neffectively search such spaces. We investigate surrogate model training for\nimproving search in highly expressive NAS search spaces based on context-free\ngrammars. We show that i) surrogate models trained either using zero-cost-proxy\nmetrics and neural graph features (GRAF) or by fine-tuning an off-the-shelf LM\nhave high predictive power for the performance of architectures both within and\nacross datasets, ii) these surrogates can be used to filter out bad\narchitectures when searching on novel datasets, thereby significantly speeding\nup search and achieving better final performances, and iii) the surrogates can\nbe further used directly as the search objective for huge speed-ups.", "published": "2025-04-17 14:22:28", "link": "http://arxiv.org/abs/2504.12971v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?", "abstract": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.", "published": "2025-04-17 14:07:11", "link": "http://arxiv.org/abs/2504.12961v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise", "abstract": "Large pretrained vision foundation models have shown significant potential in\nvarious vision tasks. However, for industrial anomaly detection, the scarcity\nof real defect samples poses a critical challenge in leveraging these models.\nWhile 2D anomaly generation has significantly advanced with established\ngenerative models, the adoption of 3D sensors in industrial manufacturing has\nmade leveraging 3D data for surface quality inspection an emerging trend. In\ncontrast to 2D techniques, 3D anomaly generation remains largely unexplored,\nlimiting the potential of 3D data in industrial quality inspection. To address\nthis gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS,\nbased on Perlin noise and surface parameterization. Our method generates\nrealistic 3D surface anomalies by projecting the point cloud onto a 2D plane,\nsampling multi-scale noise values from a Perlin noise field, and perturbing the\npoint cloud along its normal direction. Through comprehensive visualization\nexperiments, we demonstrate how key parameters - including noise scale,\nperturbation strength, and octaves, provide fine-grained control over the\ngenerated anomalies, enabling the creation of diverse defect patterns from\npronounced deformations to subtle surface variations. Additionally, our\ncross-category experiments show that the method produces consistent yet\ngeometrically plausible anomalies across different object types, adapting to\ntheir specific surface characteristics. We also provide a comprehensive\ncodebase and visualization toolkit to facilitate future research.", "published": "2025-04-17 11:23:17", "link": "http://arxiv.org/abs/2504.12856v1", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO", "I.5.4"], "primary_category": "cs.GR"}
{"title": "ALT: A Python Package for Lightweight Feature Representation in Time Series Classification", "abstract": "We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.", "published": "2025-04-17 10:57:29", "link": "http://arxiv.org/abs/2504.12841v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MS", "stat.ML", "62M10, 62H30, 68T05, 68T10", "I.5.1; I.2.6; G.3; D.2.13"], "primary_category": "cs.LG"}
{"title": "Image-Editing Specialists: An RLAIF Approach for Diffusion Models", "abstract": "We present a novel approach to training specialized instruction-based\nimage-editing diffusion models, addressing key challenges in structural\npreservation with input images and semantic alignment with user prompts. We\nintroduce an online reinforcement learning framework that aligns the diffusion\nmodel with human preferences without relying on extensive human annotations or\ncurating a large dataset. Our method significantly improves the realism and\nalignment with instructions in two ways. First, the proposed models achieve\nprecise and structurally coherent modifications in complex scenes while\nmaintaining high fidelity in instruction-irrelevant areas. Second, they capture\nfine nuances in the desired edit by leveraging a visual prompt, enabling\ndetailed control over visual edits without lengthy textual prompts. This\napproach simplifies users' efforts to achieve highly specific edits, requiring\nonly 5 reference images depicting a certain concept for training. Experimental\nresults demonstrate that our models can perform intricate edits in complex\nscenes, after just 10 training steps. Finally, we showcase the versatility of\nour method by applying it to robotics, where enhancing the visual realism of\nsimulated environments through targeted sim-to-real image edits improves their\nutility as proxies for real-world settings.", "published": "2025-04-17 10:46:39", "link": "http://arxiv.org/abs/2504.12833v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks", "abstract": "This paper investigates the integration of graph neural networks (GNNs) with\nQualitative Explainable Graphs (QXGs) for scene understanding in automated\ndriving. Scene understanding is the basis for any further reactive or proactive\ndecision-making. Scene understanding and related reasoning is inherently an\nexplanation task: why is another traffic participant doing something, what or\nwho caused their actions? While previous work demonstrated QXGs' effectiveness\nusing shallow machine learning models, these approaches were limited to\nanalysing single relation chains between object pairs, disregarding the broader\nscene context. We propose a novel GNN architecture that processes entire graph\nstructures to identify relevant objects in traffic scenes. We evaluate our\nmethod on the nuScenes dataset enriched with DriveLM's human-annotated\nrelevance labels. Experimental results show that our GNN-based approach\nachieves superior performance compared to baseline methods. The model\neffectively handles the inherent class imbalance in relevant object\nidentification tasks while considering the complete spatial-temporal\nrelationships between all objects in the scene. Our work demonstrates the\npotential of combining qualitative representations with deep learning\napproaches for explainable scene understanding in autonomous driving systems.", "published": "2025-04-17 10:21:30", "link": "http://arxiv.org/abs/2504.12817v1", "categories": ["cs.RO", "cs.AI", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Hybrid Dense-UNet201 Optimization for Pap Smear Image Segmentation Using Spider Monkey Optimization", "abstract": "Pap smear image segmentation is crucial for cervical cancer diagnosis.\nHowever, traditional segmentation models often struggle with complex cellular\nstructures and variations in pap smear images. This study proposes a hybrid\nDense-UNet201 optimization approach that integrates a pretrained DenseNet201 as\nthe encoder for the U-Net architecture and optimizes it using the spider monkey\noptimization (SMO) algorithm. The Dense-UNet201 model excelled at feature\nextraction. The SMO was modified to handle categorical and discrete parameters.\nThe SIPaKMeD dataset was used in this study and evaluated using key performance\nmetrics, including loss, accuracy, Intersection over Union (IoU), and Dice\ncoefficient. The experimental results showed that Dense-UNet201 outperformed\nU-Net, Res-UNet50, and Efficient-UNetB0. SMO Dense-UNet201 achieved a\nsegmentation accuracy of 96.16%, an IoU of 91.63%, and a Dice coefficient score\nof 95.63%. These findings underscore the effectiveness of image preprocessing,\npretrained models, and metaheuristic optimization in improving medical image\nanalysis and provide new insights into cervical cell segmentation methods.", "published": "2025-04-17 10:14:05", "link": "http://arxiv.org/abs/2504.12807v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks", "abstract": "The loss landscape of Variational Quantum Neural Networks (VQNNs) is\ncharacterized by local minima that grow exponentially with increasing qubits.\nBecause of this, it is more challenging to recover information from model\ngradients during training compared to classical Neural Networks (NNs). In this\npaper we present a numerical scheme that successfully reconstructs input\ntraining, real-world, practical data from trainable VQNNs' gradients. Our\nscheme is based on gradient inversion that works by combining gradients\nestimation with the finite difference method and adaptive low-pass filtering.\nThe scheme is further optimized with Kalman filter to obtain efficient\nconvergence. Our experiments show that our algorithm can invert even\nbatch-trained data, given the VQNN model is sufficiently over-parameterized.", "published": "2025-04-17 10:12:38", "link": "http://arxiv.org/abs/2504.12806v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Enhancing Explainability and Reliable Decision-Making in Particle Swarm Optimization through Communication Topologies", "abstract": "Swarm intelligence effectively optimizes complex systems across fields like\nengineering and healthcare, yet algorithm solutions often suffer from low\nreliability due to unclear configurations and hyperparameters. This study\nanalyzes Particle Swarm Optimization (PSO), focusing on how different\ncommunication topologies Ring, Star, and Von Neumann affect convergence and\nsearch behaviors. Using an adapted IOHxplainer , an explainable benchmarking\ntool, we investigate how these topologies influence information flow,\ndiversity, and convergence speed, clarifying the balance between exploration\nand exploitation. Through visualization and statistical analysis, the research\nenhances interpretability of PSO's decisions and provides practical guidelines\nfor choosing suitable topologies for specific optimization tasks. Ultimately,\nthis contributes to making swarm based optimization more transparent, robust,\nand trustworthy.", "published": "2025-04-17 10:05:10", "link": "http://arxiv.org/abs/2504.12803v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Set You Straight: Auto-Steering Denoising Trajectories to Sidestep Unwanted Concepts", "abstract": "Ensuring the ethical deployment of text-to-image models requires effective\ntechniques to prevent the generation of harmful or inappropriate content. While\nconcept erasure methods offer a promising solution, existing finetuning-based\napproaches suffer from notable limitations. Anchor-free methods risk disrupting\nsampling trajectories, leading to visual artifacts, while anchor-based methods\nrely on the heuristic selection of anchor concepts. To overcome these\nshortcomings, we introduce a finetuning framework, dubbed ANT, which\nAutomatically guides deNoising Trajectories to avoid unwanted concepts. ANT is\nbuilt on a key insight: reversing the condition direction of classifier-free\nguidance during mid-to-late denoising stages enables precise content\nmodification without sacrificing early-stage structural integrity. This\ninspires a trajectory-aware objective that preserves the integrity of the\nearly-stage score function field, which steers samples toward the natural image\nmanifold, without relying on heuristic anchor concept selection. For\nsingle-concept erasure, we propose an augmentation-enhanced weight saliency map\nto precisely identify the critical parameters that most significantly\ncontribute to the unwanted concept, enabling more thorough and efficient\nerasure. For multi-concept erasure, our objective function offers a versatile\nplug-and-play solution that significantly boosts performance. Extensive\nexperiments demonstrate that ANT achieves state-of-the-art results in both\nsingle and multi-concept erasure, delivering high-quality, safe outputs without\ncompromising the generative fidelity. Code is available at\nhttps://github.com/lileyang1210/ANT", "published": "2025-04-17 09:29:30", "link": "http://arxiv.org/abs/2504.12782v1", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis", "abstract": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.", "published": "2025-04-17 09:18:04", "link": "http://arxiv.org/abs/2504.12777v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "MCP Guardian: A Security-First Layer for Safeguarding MCP-Based AI System", "abstract": "As Agentic AI gain mainstream adoption, the industry invests heavily in model\ncapabilities, achieving rapid leaps in reasoning and quality. However, these\nsystems remain largely confined to data silos, and each new integration\nrequires custom logic that is difficult to scale. The Model Context Protocol\n(MCP) addresses this challenge by defining a universal, open standard for\nsecurely connecting AI-based applications (MCP clients) to data sources (MCP\nservers). However, the flexibility of the MCP introduces new risks, including\nmalicious tool servers and compromised data integrity. We present MCP Guardian,\na framework that strengthens MCP-based communication with authentication,\nrate-limiting, logging, tracing, and Web Application Firewall (WAF) scanning.\nThrough real-world scenarios and empirical testing, we demonstrate how MCP\nGuardian effectively mitigates attacks and ensures robust oversight with\nminimal overheads. Our approach fosters secure, scalable data access for AI\nassistants, underscoring the importance of a defense-in-depth approach that\nenables safer and more transparent innovation in AI-driven environments.", "published": "2025-04-17 08:49:10", "link": "http://arxiv.org/abs/2504.12757v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Trajectory Adaptation using Large Language Models", "abstract": "Adapting robot trajectories based on human instructions as per new situations\nis essential for achieving more intuitive and scalable human-robot\ninteractions. This work proposes a flexible language-based framework to adapt\ngeneric robotic trajectories produced by off-the-shelf motion planners like\nRRT, A-star, etc, or learned from human demonstrations. We utilize pre-trained\nLLMs to adapt trajectory waypoints by generating code as a policy for dense\nrobot manipulation, enabling more complex and flexible instructions than\ncurrent methods. This approach allows us to incorporate a broader range of\ncommands, including numerical inputs. Compared to state-of-the-art\nfeature-based sequence-to-sequence models which require training, our method\ndoes not require task-specific training and offers greater interpretability and\nmore effective feedback mechanisms. We validate our approach through simulation\nexperiments on the robotic manipulator, aerial vehicle, and ground robot in the\nPybullet and Gazebo simulation environments, demonstrating that LLMs can\nsuccessfully adapt trajectories to complex human instructions.", "published": "2025-04-17 08:48:23", "link": "http://arxiv.org/abs/2504.12755v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "GPMFS: Global Foundation and Personalized Optimization for Multi-Label Feature Selection", "abstract": "As artificial intelligence methods are increasingly applied to complex task\nscenarios, high dimensional multi-label learning has emerged as a prominent\nresearch focus. At present, the curse of dimensionality remains one of the\nmajor bottlenecks in high-dimensional multi-label learning, which can be\neffectively addressed through multi-label feature selection methods. However,\nexisting multi-label feature selection methods mostly focus on identifying\nglobal features shared across all labels, which overlooks personalized\ncharacteristics and specific requirements of individual labels. This\nglobal-only perspective may limit the ability to capture label-specific\ndiscriminative information, thereby affecting overall performance. In this\npaper, we propose a novel method called GPMFS (Global Foundation and\nPersonalized Optimization for Multi-Label Feature Selection). GPMFS firstly\nidentifies global features by exploiting label correlations, then adaptively\nsupplements each label with a personalized subset of discriminative features\nusing a threshold-controlled strategy. Experiments on multiple real-world\ndatasets demonstrate that GPMFS achieves superior performance while maintaining\nstrong interpretability and robustness. Furthermore, GPMFS provides insights\ninto the label-specific strength across different multi-label datasets, thereby\ndemonstrating the necessity and potential applicability of personalized feature\nselection approaches.", "published": "2025-04-17 08:29:14", "link": "http://arxiv.org/abs/2504.12740v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems", "abstract": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.", "published": "2025-04-17 08:21:28", "link": "http://arxiv.org/abs/2504.12735v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation", "abstract": "Recommender systems play a central role in numerous real-life applications,\nyet evaluating their performance remains a significant challenge due to the gap\nbetween offline metrics and online behaviors. Given the scarcity and limits\n(e.g., privacy issues) of real user data, we introduce SimUSER, an agent\nframework that serves as believable and cost-effective human proxies. SimUSER\nfirst identifies self-consistent personas from historical data, enriching user\nprofiles with unique backgrounds and personalities. Then, central to this\nevaluation are users equipped with persona, memory, perception, and brain\nmodules, engaging in interactions with the recommender system. SimUSER exhibits\ncloser alignment with genuine humans than prior work, both at micro and macro\nlevels. Additionally, we conduct insightful experiments to explore the effects\nof thumbnails on click rates, the exposure effect, and the impact of reviews on\nuser engagement. Finally, we refine recommender system parameters based on\noffline A/B test results, resulting in improved user engagement in the real\nworld.", "published": "2025-04-17 07:57:23", "link": "http://arxiv.org/abs/2504.12722v1", "categories": ["cs.IR", "cs.AI"], "primary_category": "cs.IR"}
{"title": "TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations", "abstract": "Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.", "published": "2025-04-17 07:54:26", "link": "http://arxiv.org/abs/2504.12721v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
{"title": "TUMLS: Trustful Fully Unsupervised Multi-Level Segmentation for Whole Slide Images of Histology", "abstract": "Digital pathology, augmented by artificial intelligence (AI), holds\nsignificant promise for improving the workflow of pathologists. However,\nchallenges such as the labor-intensive annotation of whole slide images (WSIs),\nhigh computational demands, and trust concerns arising from the absence of\nuncertainty estimation in predictions hinder the practical application of\ncurrent AI methodologies in histopathology. To address these issues, we present\na novel trustful fully unsupervised multi-level segmentation methodology\n(TUMLS) for WSIs. TUMLS adopts an autoencoder (AE) as a feature extractor to\nidentify the different tissue types within low-resolution training data. It\nselects representative patches from each identified group based on an\nuncertainty measure and then does unsupervised nuclei segmentation in their\nrespective higher-resolution space without using any ML algorithms. Crucially,\nthis solution integrates seamlessly into clinicians workflows, transforming the\nexamination of a whole WSI into a review of concise, interpretable cross-level\ninsights. This integration significantly enhances and accelerates the workflow\nwhile ensuring transparency. We evaluated our approach using the UPENN-GBM\ndataset, where the AE achieved a mean squared error (MSE) of 0.0016.\nAdditionally, nucleus segmentation is assessed on the MoNuSeg dataset,\noutperforming all unsupervised approaches with an F1 score of 77.46% and a\nJaccard score of 63.35%. These results demonstrate the efficacy of TUMLS in\nadvancing the field of digital pathology.", "published": "2025-04-17 07:48:05", "link": "http://arxiv.org/abs/2504.12718v1", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.4.6; I.5.3; I.5.4"], "primary_category": "eess.IV"}
{"title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models", "abstract": "Contrastive language image pre-training (CLIP) is an essential component of\nbuilding modern vision-language foundation models. While CLIP demonstrates\nremarkable zero-shot performance on downstream tasks, the multi-modal feature\nspaces still suffer from a modality gap, which is a gap between image and text\nfeature clusters and limits downstream task performance. Although existing\nworks attempt to address the modality gap by modifying pre-training or\nfine-tuning, they struggle with heavy training costs with large datasets or\ndegradations of zero-shot performance. This paper presents CLIP-Refine, a\npost-pre-training method for CLIP models at a phase between pre-training and\nfine-tuning. CLIP-Refine aims to align the feature space with 1 epoch training\non small image-text datasets without zero-shot performance degradations. To\nthis end, we introduce two techniques: random feature alignment (RaFA) and\nhybrid contrastive-distillation (HyCD). RaFA aligns the image and text features\nto follow a shared prior distribution by minimizing the distance to random\nreference vectors sampled from the prior. HyCD updates the model with hybrid\nsoft labels generated by combining ground-truth image-text pair labels and\noutputs from the pre-trained CLIP model. This contributes to achieving both\nmaintaining the past knowledge and learning new knowledge to align features.\nOur extensive experiments with multiple classification and retrieval tasks show\nthat CLIP-Refine succeeds in mitigating the modality gap and improving the\nzero-shot performance.", "published": "2025-04-17 07:46:19", "link": "http://arxiv.org/abs/2504.12717v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination", "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.", "published": "2025-04-17 07:41:25", "link": "http://arxiv.org/abs/2504.12714v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results", "abstract": "This paper reviews the NTIRE 2025 Challenge on Day and Night Raindrop Removal\nfor Dual-Focused Images. This challenge received a wide range of impressive\nsolutions, which are developed and evaluated using our collected real-world\nRaindrop Clarity dataset. Unlike existing deraining datasets, our Raindrop\nClarity dataset is more diverse and challenging in degradation types and\ncontents, which includes day raindrop-focused, day background-focused, night\nraindrop-focused, and night background-focused degradations. This dataset is\ndivided into three subsets for competition: 14,139 images for training, 240\nimages for validation, and 731 images for testing. The primary objective of\nthis challenge is to establish a new and powerful benchmark for the task of\nremoving raindrops under varying lighting and focus conditions. There are a\ntotal of 361 participants in the competition, and 32 teams submitting valid\nsolutions and fact sheets for the final testing phase. These submissions\nachieved state-of-the-art (SOTA) performance on the Raindrop Clarity dataset.\nThe project can be found at\nhttps://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/.", "published": "2025-04-17 07:35:35", "link": "http://arxiv.org/abs/2504.12711v1", "categories": ["cs.CV", "cs.AI", "eess.IV"], "primary_category": "cs.CV"}
{"title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning", "abstract": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.", "published": "2025-04-17 06:16:11", "link": "http://arxiv.org/abs/2504.12680v1", "categories": ["cs.AI", "cs.CV"], "primary_category": "cs.AI"}
{"title": "Post-processing improves accuracy of Artificial Intelligence weather forecasts", "abstract": "Artificial Intelligence (AI) weather models are now reaching\noperational-grade performance for some variables, but like traditional\nNumerical Weather Prediction (NWP) models, they exhibit systematic biases and\nreliability issues. We test the application of the Bureau of Meteorology's\nexisting statistical post-processing system, IMPROVER, to ECMWF's deterministic\nArtificial Intelligence Forecasting System (AIFS), and compare results against\npost-processed outputs from the ECMWF HRES and ENS models. Without any\nmodification to configuration or processing workflows, post-processing yields\ncomparable accuracy improvements for AIFS as for traditional NWP forecasts, in\nboth expected value and probabilistic outputs. We show that blending AIFS with\nNWP models improves overall forecast skill, even when AIFS alone is not the\nmost accurate component. These findings show that statistical post-processing\nmethods developed for NWP are directly applicable to AI models, enabling\nnational meteorological centres to incorporate AI forecasts into existing\nworkflows in a low-risk, incremental fashion.", "published": "2025-04-17 06:05:10", "link": "http://arxiv.org/abs/2504.12672v1", "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "primary_category": "physics.ao-ph"}
{"title": "Quantum Computing Supported Adversarial Attack-Resilient Autonomous Vehicle Perception Module for Traffic Sign Classification", "abstract": "Deep learning (DL)-based image classification models are essential for\nautonomous vehicle (AV) perception modules since incorrect categorization might\nhave severe repercussions. Adversarial attacks are widely studied cyberattacks\nthat can lead DL models to predict inaccurate output, such as incorrectly\nclassified traffic signs by the perception module of an autonomous vehicle. In\nthis study, we create and compare hybrid classical-quantum deep learning\n(HCQ-DL) models with classical deep learning (C-DL) models to demonstrate\nrobustness against adversarial attacks for perception modules. Before feeding\nthem into the quantum system, we used transfer learning models, alexnet and\nvgg-16, as feature extractors. We tested over 1000 quantum circuits in our\nHCQ-DL models for projected gradient descent (PGD), fast gradient sign attack\n(FGSA), and gradient attack (GA), which are three well-known untargeted\nadversarial approaches. We evaluated the performance of all models during\nadversarial attacks and no-attack scenarios. Our HCQ-DL models maintain\naccuracy above 95\\% during a no-attack scenario and above 91\\% for GA and FGSA\nattacks, which is higher than C-DL models. During the PGD attack, our\nalexnet-based HCQ-DL model maintained an accuracy of 85\\% compared to C-DL\nmodels that achieved accuracies below 21\\%. Our results highlight that the\nHCQ-DL models provide improved accuracy for traffic sign classification under\nadversarial settings compared to their classical counterparts.", "published": "2025-04-17 05:08:08", "link": "http://arxiv.org/abs/2504.12644v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "cs.ET"], "primary_category": "cs.LG"}
{"title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance", "abstract": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.", "published": "2025-04-17 03:23:17", "link": "http://arxiv.org/abs/2504.12612v1", "categories": ["cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration", "abstract": "Teaching robots dexterous manipulation skills often requires collecting\nhundreds of demonstrations using wearables or teleoperation, a process that is\nchallenging to scale. Videos of human-object interactions are easier to collect\nand scale, but leveraging them directly for robot learning is difficult due to\nthe lack of explicit action labels from videos and morphological differences\nbetween robot and human hands. We propose Human2Sim2Robot, a novel\nreal-to-sim-to-real framework for training dexterous manipulation policies\nusing only one RGB-D video of a human demonstrating a task. Our method utilizes\nreinforcement learning (RL) in simulation to cross the human-robot embodiment\ngap without relying on wearables, teleoperation, or large-scale data collection\ntypically necessary for imitation learning methods. From the demonstration, we\nextract two task-specific components: (1) the object pose trajectory to define\nan object-centric, embodiment-agnostic reward function, and (2) the\npre-manipulation hand pose to initialize and guide exploration during RL\ntraining. We found that these two components are highly effective for learning\nthe desired task, eliminating the need for task-specific reward shaping and\ntuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop\ntrajectory replay by 55% and imitation learning with data augmentation by 68%\nacross grasping, non-prehensile manipulation, and multi-step tasks. Project\nSite: https://human2sim2robot.github.io", "published": "2025-04-17 03:15:20", "link": "http://arxiv.org/abs/2504.12609v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Code Copycat Conundrum: Demystifying Repetition in LLM-based Code Generation", "abstract": "Despite recent advances in Large Language Models (LLMs) for code generation,\nthe quality of LLM-generated code still faces significant challenges. One\nsignificant issue is code repetition, which refers to the model's tendency to\ngenerate structurally redundant code, resulting in inefficiencies and reduced\nreadability. To address this, we conduct the first empirical study to\ninvestigate the prevalence and nature of repetition across 19 state-of-the-art\ncode LLMs using three widely-used benchmarks. Our study includes both\nquantitative and qualitative analyses, revealing that repetition is pervasive\nand manifests at various granularities and extents, including character,\nstatement, and block levels. We further summarize a taxonomy of 20 repetition\npatterns. Building on our findings, we propose DeRep, a rule-based technique\ndesigned to detect and mitigate repetition in generated code. We evaluate DeRep\nusing both open-source benchmarks and in an industrial setting. Our results\ndemonstrate that DeRep significantly outperforms baselines in reducing\nrepetition (with an average improvements of 91.3%, 93.5%, and 79.9% in rep-3,\nrep-line, and sim-line metrics) and enhancing code quality (with a Pass@1\nincrease of 208.3% over greedy search). Furthermore, integrating DeRep improves\nthe performance of existing repetition mitigation methods, with Pass@1\nimprovements ranging from 53.7% to 215.7%.", "published": "2025-04-17 03:13:39", "link": "http://arxiv.org/abs/2504.12608v1", "categories": ["cs.SE", "cs.AI"], "primary_category": "cs.SE"}
{"title": "Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation", "abstract": "In this paper, we introduce a novel method named Robo-SGG, i.e.,\nLayout-Oriented Normalization and Restitution for Robust Scene Graph\nGeneration. Compared to the existing SGG setting, the robust scene graph\ngeneration aims to perform inference on a diverse range of corrupted images,\nwith the core challenge being the domain shift between the clean and corrupted\nimages. Existing SGG methods suffer from degraded performance due to\ncompromised visual features e.g., corruption interference or occlusions. To\nobtain robust visual features, we exploit the layout information, which is\ndomain-invariant, to enhance the efficacy of existing SGG methods on corrupted\nimages. Specifically, we employ Instance Normalization(IN) to filter out the\ndomain-specific feature and recover the unchangeable structural features, i.e.,\nthe positional and semantic relationships among objects by the proposed\nLayout-Oriented Restitution. Additionally, we propose a Layout-Embedded Encoder\n(LEE) that augments the existing object and predicate encoders within the SGG\nframework, enriching the robust positional and semantic features of objects and\npredicates. Note that our proposed Robo-SGG module is designed as a\nplug-and-play component, which can be easily integrated into any baseline SGG\nmodel. Extensive experiments demonstrate that by integrating the\nstate-of-the-art method into our proposed Robo-SGG, we achieve relative\nimprovements of 5.6%, 8.0%, and 6.5% in mR@50 for PredCls, SGCls, and SGDet\ntasks on the VG-C dataset, respectively, and achieve new state-of-the-art\nperformance in corruption scene graph generation benchmark (VG-C and GQA-C). We\nwill release our source code and model.", "published": "2025-04-17 03:09:22", "link": "http://arxiv.org/abs/2504.12606v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Local Data Quantity-Aware Weighted Averaging for Federated Learning with Dishonest Clients", "abstract": "Federated learning (FL) enables collaborative training of deep learning\nmodels without requiring data to leave local clients, thereby preserving client\nprivacy. The aggregation process on the server plays a critical role in the\nperformance of the resulting FL model. The most commonly used aggregation\nmethod is weighted averaging based on the amount of data from each client,\nwhich is thought to reflect each client's contribution. However, this method is\nprone to model bias, as dishonest clients might report inaccurate training data\nvolumes to the server, which is hard to verify. To address this issue, we\npropose a novel secure \\underline{Fed}erated \\underline{D}ata\nq\\underline{u}antity-\\underline{a}ware weighted averaging method (FedDua). It\nenables FL servers to accurately predict the amount of training data from each\nclient based on their local model gradients uploaded. Furthermore, it can be\nseamlessly integrated into any FL algorithms that involve server-side model\naggregation. Extensive experiments on three benchmarking datasets demonstrate\nthat FedDua improves the global model performance by an average of 3.17%\ncompared to four popular FL aggregation methods in the presence of inaccurate\nclient data volume declarations.", "published": "2025-04-17 01:50:24", "link": "http://arxiv.org/abs/2504.12577v1", "categories": ["cs.LG", "cs.AI", "cs.CR"], "primary_category": "cs.LG"}
{"title": "CM3AE: A Unified RGB Frame and Event-Voxel/-Frame Pre-training Framework", "abstract": "Event cameras have attracted increasing attention in recent years due to\ntheir advantages in high dynamic range, high temporal resolution, low power\nconsumption, and low latency. Some researchers have begun exploring\npre-training directly on event data. Nevertheless, these efforts often fail to\nestablish strong connections with RGB frames, limiting their applicability in\nmulti-modal fusion scenarios. To address these issues, we propose a novel CM3AE\npre-training framework for the RGB-Event perception. This framework accepts\nmulti-modalities/views of data as input, including RGB images, event images,\nand event voxels, providing robust support for both event-based and RGB-event\nfusion based downstream tasks. Specifically, we design a multi-modal fusion\nreconstruction module that reconstructs the original image from fused\nmulti-modal features, explicitly enhancing the model's ability to aggregate\ncross-modal complementary information. Additionally, we employ a multi-modal\ncontrastive learning strategy to align cross-modal feature representations in a\nshared latent space, which effectively enhances the model's capability for\nmulti-modal understanding and capturing global dependencies. We construct a\nlarge-scale dataset containing 2,535,759 RGB-Event data pairs for the\npre-training. Extensive experiments on five downstream tasks fully demonstrated\nthe effectiveness of CM3AE. Source code and pre-trained models will be released\non https://github.com/Event-AHU/CM3AE.", "published": "2025-04-17 01:49:46", "link": "http://arxiv.org/abs/2504.12576v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback", "abstract": "In safe reinforcement learning (RL), auxiliary safety costs are used to align\nthe agent to safe decision making. In practice, safety constraints, including\ncost functions and budgets, are unknown or hard to specify, as it requires\nanticipation of all possible unsafe behaviors. We therefore address a general\nsetting where the true safety definition is unknown, and has to be learned from\nsparsely labeled data. Our key contributions are: first, we design a safety\nmodel that performs credit assignment to estimate each decision step's impact\non the overall safety using a dataset of diverse trajectories and their\ncorresponding binary safety labels (i.e., whether the corresponding trajectory\nis safe/unsafe). Second, we illustrate the architecture of our safety model to\ndemonstrate its ability to learn a separate safety score for each timestep.\nThird, we reformulate the safe RL problem using the proposed safety model and\nderive an effective algorithm to optimize a safe yet rewarding policy. Finally,\nour empirical results corroborate our findings and show that this approach is\neffective in satisfying unknown safety definition, and scalable to various\ncontinuous control tasks.", "published": "2025-04-17 01:11:08", "link": "http://arxiv.org/abs/2504.12557v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins", "abstract": "Purpose: The operating room (OR) is a complex environment where optimizing\nworkflows is critical to reduce costs and improve patient outcomes. The use of\ncomputer vision approaches for the automatic recognition of perioperative\nevents enables identification of bottlenecks for OR optimization. However,\nprivacy concerns limit the use of computer vision for automated event detection\nfrom OR videos, which makes privacy-preserving approaches needed for OR\nworkflow analysis. Methods: We propose a two-stage pipeline for\nprivacy-preserving OR video analysis and event detection. In the first stage,\nwe leverage vision foundation models for depth estimation and semantic\nsegmentation to generate de-identified Digital Twins (DT) of the OR from\nconventional RGB videos. In the second stage, we employ the SafeOR model, a\nfused two-stream approach that processes segmentation masks and depth maps for\nOR event detection. We evaluate this method on an internal dataset of 38\nsimulated surgical trials with five event classes. Results: Our results\nindicate that this DT-based approach to the OR event detection model achieves\nperformance on par and sometimes even better than raw RGB video-based models on\ndetecting OR events. Conclusion: DTs enable privacy-preserving OR workflow\nanalysis, facilitating the sharing of de-identified data across institutions\nand they can potentially enhance model generalizability by mitigating\ndomain-specific appearance differences.", "published": "2025-04-17 00:46:06", "link": "http://arxiv.org/abs/2504.12552v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Anonymous Public Announcements", "abstract": "We formalise the notion of an \\emph{anonymous public announcement} in the\ntradition of public announcement logic. Such announcements can be seen as\nin-between a public announcement from ``the outside\" (an announcement of\n$\\phi$) and a public announcement by one of the agents (an announcement of\n$K_a\\phi$): we get more information than just $\\phi$, but not (necessarily)\nabout exactly who made it. Even if such an announcement is prima facie\nanonymous, depending on the background knowledge of the agents it might reveal\nthe identity of the announcer: if I post something on a message board, the\ninformation might reveal who I am even if I don't sign my name. Furthermore,\nlike in the Russian Cards puzzle, if we assume that the announcer's intention\nwas to stay anonymous, that in fact might reveal more information. In this\npaper we first look at the case when no assumption about intentions are made,\nin which case the logic with an anonymous public announcement operator is\nreducible to epistemic logic. We then look at the case when we assume common\nknowledge of the intention to stay anonymous, which is both more complex and\nmore interesting: in several ways it boils down to the notion of a ``safe\"\nannouncement (again, similarly to Russian Cards). Main results include formal\nexpressivity results and axiomatic completeness for key logical languages.", "published": "2025-04-17 00:14:37", "link": "http://arxiv.org/abs/2504.12546v1", "categories": ["cs.LO", "cs.AI", "cs.CR"], "primary_category": "cs.LO"}
{"title": "Perception Encoder: The best visual embeddings are not at the output of the network", "abstract": "We introduce Perception Encoder (PE), a state-of-the-art encoder for image\nand video understanding trained via simple vision-language learning.\nTraditionally, vision encoders have relied on a variety of pretraining\nobjectives, each tailored to specific downstream tasks such as classification,\ncaptioning, or localization. Surprisingly, after scaling our carefully tuned\nimage pretraining recipe and refining with our robust video data engine, we\nfind that contrastive vision-language training alone can produce strong,\ngeneral embeddings for all of these downstream tasks. There is only one caveat:\nthese embeddings are hidden within the intermediate layers of the network. To\ndraw them out, we introduce two alignment methods, language alignment for\nmultimodal language modeling, and spatial alignment for dense prediction.\nTogether with the core contrastive checkpoint, our PE family of models achieves\nstate-of-the-art performance on a wide variety of tasks, including zero-shot\nimage and video classification and retrieval; document, image, and video Q&A;\nand spatial tasks such as detection, depth estimation, and tracking. To foster\nfurther research, we are releasing our models, code, and a novel dataset of\nsynthetically and human-annotated videos.", "published": "2025-04-17 17:59:57", "link": "http://arxiv.org/abs/2504.13181v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ViTa-Zero: Zero-shot Visuotactile Object 6D Pose Estimation", "abstract": "Object 6D pose estimation is a critical challenge in robotics, particularly\nfor manipulation tasks. While prior research combining visual and tactile\n(visuotactile) information has shown promise, these approaches often struggle\nwith generalization due to the limited availability of visuotactile data. In\nthis paper, we introduce ViTa-Zero, a zero-shot visuotactile pose estimation\nframework. Our key innovation lies in leveraging a visual model as its backbone\nand performing feasibility checking and test-time optimization based on\nphysical constraints derived from tactile and proprioceptive observations.\nSpecifically, we model the gripper-object interaction as a spring-mass system,\nwhere tactile sensors induce attractive forces, and proprioception generates\nrepulsive forces. We validate our framework through experiments on a real-world\nrobot setup, demonstrating its effectiveness across representative visual\nbackbones and manipulation scenarios, including grasping, object picking, and\nbimanual handover. Compared to the visual models, our approach overcomes some\ndrastic failure modes while tracking the in-hand object pose. In our\nexperiments, our approach shows an average increase of 55% in AUC of ADD-S and\n60% in ADD, along with an 80% lower position error compared to FoundationPose.", "published": "2025-04-17 17:59:56", "link": "http://arxiv.org/abs/2504.13179v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Single-Shot Shape and Reflectance with Spatial Polarization Multiplexing", "abstract": "We propose spatial polarization multiplexing (SPM) for reconstructing object\nshape and reflectance from a single polarimetric image and demonstrate its\napplication to dynamic surface recovery. Although single-pattern structured\nlight enables single-shot shape reconstruction, the reflectance is challenging\nto recover due to the lack of angular sampling of incident light and the\nentanglement of the projected pattern and the surface color texture. We design\na spatially multiplexed pattern of polarization that can be robustly and\nuniquely decoded for shape reconstruction by quantizing the AoLP values. At the\nsame time, our spatial-multiplexing enables single-shot ellipsometry of linear\npolarization by projecting differently polarized light within a local region,\nwhich separates the specular and diffuse reflections for BRDF estimation. We\nachieve this spatial polarization multiplexing with a constrained de Bruijn\nsequence. Unlike single-pattern structured light with intensity and color, our\npolarization pattern is invisible to the naked eye and retains the natural\nsurface appearance which is essential for accurate appearance modeling and also\ninteraction with people. We experimentally validate our method on real data.\nThe results show that our method can recover the shape, the Mueller matrix, and\nthe BRDF from a single-shot polarimetric image. We also demonstrate the\napplication of our method to dynamic surfaces.", "published": "2025-04-17 17:59:50", "link": "http://arxiv.org/abs/2504.13177v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "IMAGGarment-1: Fine-Grained Garment Generation for Controllable Fashion Design", "abstract": "This paper presents IMAGGarment-1, a fine-grained garment generation (FGG)\nframework that enables high-fidelity garment synthesis with precise control\nover silhouette, color, and logo placement. Unlike existing methods that are\nlimited to single-condition inputs, IMAGGarment-1 addresses the challenges of\nmulti-conditional controllability in personalized fashion design and digital\napparel applications. Specifically, IMAGGarment-1 employs a two-stage training\nstrategy to separately model global appearance and local details, while\nenabling unified and controllable generation through end-to-end inference. In\nthe first stage, we propose a global appearance model that jointly encodes\nsilhouette and color using a mixed attention module and a color adapter. In the\nsecond stage, we present a local enhancement model with an adaptive\nappearance-aware module to inject user-defined logos and spatial constraints,\nenabling accurate placement and visual consistency. To support this task, we\nrelease GarmentBench, a large-scale dataset comprising over 180K garment\nsamples paired with multi-level design conditions, including sketches, color\nreferences, logo placements, and textual prompts. Extensive experiments\ndemonstrate that our method outperforms existing baselines, achieving superior\nstructural stability, color fidelity, and local controllability performance.\nThe code and model are available at https://github.com/muzishen/IMAGGarment-1.", "published": "2025-04-17 17:59:47", "link": "http://arxiv.org/abs/2504.13176v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling", "abstract": "Vision-Language Models (VLMs) excel at visual understanding but often suffer\nfrom visual hallucinations, where they generate descriptions of nonexistent\nobjects, actions, or concepts, posing significant risks in safety-critical\napplications. Existing hallucination mitigation methods typically follow one of\ntwo paradigms: generation adjustment, which modifies decoding behavior to align\ntext with visual inputs, and post-hoc verification, where external models\nassess and correct outputs. While effective, generation adjustment methods\noften rely on heuristics and lack correction mechanisms, while post-hoc\nverification is complicated, typically requiring multiple models and tending to\nreject outputs rather than refine them. In this work, we introduce REVERSE, a\nunified framework that integrates hallucination-aware training with on-the-fly\nself-verification. By leveraging a new hallucination-verification dataset\ncontaining over 1.3M semi-synthetic samples, along with a novel inference-time\nretrospective resampling technique, our approach enables VLMs to both detect\nhallucinations during generation and dynamically revise those hallucinations.\nOur evaluations show that REVERSE achieves state-of-the-art hallucination\nreduction, outperforming the best existing methods by up to 12% on CHAIR-MSCOCO\nand 28% on HaloQuest. Our dataset, model, and code are available at:\nhttps://reverse-vlm.github.io.", "published": "2025-04-17 17:59:22", "link": "http://arxiv.org/abs/2504.13169v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos", "abstract": "Creating a photorealistic scene and human reconstruction from a single\nmonocular in-the-wild video figures prominently in the perception of a\nhuman-centric 3D world. Recent neural rendering advances have enabled holistic\nhuman-scene reconstruction but require pre-calibrated camera and human poses,\nand days of training time. In this work, we introduce a novel unified framework\nthat simultaneously performs camera tracking, human pose estimation and\nhuman-scene reconstruction in an online fashion. 3D Gaussian Splatting is\nutilized to learn Gaussian primitives for humans and scenes efficiently, and\nreconstruction-based camera tracking and human pose estimation modules are\ndesigned to enable holistic understanding and effective disentanglement of pose\nand appearance. Specifically, we design a human deformation module to\nreconstruct the details and enhance generalizability to out-of-distribution\nposes faithfully. Aiming to learn the spatial correlation between human and\nscene accurately, we introduce occlusion-aware human silhouette rendering and\nmonocular geometric priors, which further improve reconstruction quality.\nExperiments on the EMDB and NeuMan datasets demonstrate superior or on-par\nperformance with existing methods in camera tracking, human pose estimation,\nnovel view synthesis and runtime. Our project page is at\nhttps://eth-ait.github.io/ODHSR.", "published": "2025-04-17 17:59:02", "link": "http://arxiv.org/abs/2504.13167v1", "categories": ["cs.CV", "I.4.5"], "primary_category": "cs.CV"}
{"title": "Personalized Text-to-Image Generation with Auto-Regressive Models", "abstract": "Personalized image synthesis has emerged as a pivotal application in\ntext-to-image generation, enabling the creation of images featuring specific\nsubjects in diverse contexts. While diffusion models have dominated this\ndomain, auto-regressive models, with their unified architecture for text and\nimage modeling, remain underexplored for personalized image generation. This\npaper investigates the potential of optimizing auto-regressive models for\npersonalized image synthesis, leveraging their inherent multimodal capabilities\nto perform this task. We propose a two-stage training strategy that combines\noptimization of text embeddings and fine-tuning of transformer layers. Our\nexperiments on the auto-regressive model demonstrate that this method achieves\ncomparable subject fidelity and prompt following to the leading diffusion-based\npersonalization methods. The results highlight the effectiveness of\nauto-regressive models in personalized image generation, offering a new\ndirection for future research in this area.", "published": "2025-04-17 17:58:26", "link": "http://arxiv.org/abs/2504.13162v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Digital Twin Generation from Visual Data: A Survey", "abstract": "This survey explores recent developments in generating digital twins from\nvideos. Such digital twins can be used for robotics application, media content\ncreation, or design and construction works. We analyze various approaches,\nincluding 3D Gaussian Splatting, generative in-painting, semantic segmentation,\nand foundation models highlighting their advantages and limitations.\nAdditionally, we discuss challenges such as occlusions, lighting variations,\nand scalability, as well as potential future research directions. This survey\naims to provide a comprehensive overview of state-of-the-art methodologies and\ntheir implications for real-world applications. Awesome list:\nhttps://github.com/ndrwmlnk/awesome-digital-twins", "published": "2025-04-17 17:57:41", "link": "http://arxiv.org/abs/2504.13159v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis", "abstract": "We explore the task of geometric reconstruction of images captured from a\nmixture of ground and aerial views. Current state-of-the-art learning-based\napproaches fail to handle the extreme viewpoint variation between aerial-ground\nimage pairs. Our hypothesis is that the lack of high-quality, co-registered\naerial-ground datasets for training is a key reason for this failure. Such data\nis difficult to assemble precisely because it is difficult to reconstruct in a\nscalable way. To overcome this challenge, we propose a scalable framework\ncombining pseudo-synthetic renderings from 3D city-wide meshes (e.g., Google\nEarth) with real, ground-level crowd-sourced images (e.g., MegaDepth). The\npseudo-synthetic data simulates a wide range of aerial viewpoints, while the\nreal, crowd-sourced images help improve visual fidelity for ground-level images\nwhere mesh-based renderings lack sufficient detail, effectively bridging the\ndomain gap between real images and pseudo-synthetic renderings. Using this\nhybrid dataset, we fine-tune several state-of-the-art algorithms and achieve\nsignificant improvements on real-world, zero-shot aerial-ground tasks. For\nexample, we observe that baseline DUSt3R localizes fewer than 5% of\naerial-ground pairs within 5 degrees of camera rotation error, while\nfine-tuning with our data raises accuracy to nearly 56%, addressing a major\nfailure point in handling large viewpoint changes. Beyond camera estimation and\nscene reconstruction, our dataset also improves performance on downstream tasks\nlike novel-view synthesis in challenging aerial-ground scenarios, demonstrating\nthe practical value of our approach in real-world applications.", "published": "2025-04-17 17:57:05", "link": "http://arxiv.org/abs/2504.13157v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Training-Free Hierarchical Scene Understanding for Gaussian Splatting with Superpoint Graphs", "abstract": "Bridging natural language and 3D geometry is a crucial step toward flexible,\nlanguage-driven scene understanding. While recent advances in 3D Gaussian\nSplatting (3DGS) have enabled fast and high-quality scene reconstruction,\nresearch has also explored incorporating open-vocabulary understanding into\n3DGS. However, most existing methods require iterative optimization over\nper-view 2D semantic feature maps, which not only results in inefficiencies but\nalso leads to inconsistent 3D semantics across views. To address these\nlimitations, we introduce a training-free framework that constructs a\nsuperpoint graph directly from Gaussian primitives. The superpoint graph\npartitions the scene into spatially compact and semantically coherent regions,\nforming view-consistent 3D entities and providing a structured foundation for\nopen-vocabulary understanding. Based on the graph structure, we design an\nefficient reprojection strategy that lifts 2D semantic features onto the\nsuperpoints, avoiding costly multi-view iterative training. The resulting\nrepresentation ensures strong 3D semantic coherence and naturally supports\nhierarchical understanding, enabling both coarse- and fine-grained\nopen-vocabulary perception within a unified semantic field. Extensive\nexperiments demonstrate that our method achieves state-of-the-art\nopen-vocabulary segmentation performance, with semantic field reconstruction\ncompleted over $30\\times$ faster. Our code will be available at\nhttps://github.com/Atrovast/THGS.", "published": "2025-04-17 17:56:07", "link": "http://arxiv.org/abs/2504.13153v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World", "abstract": "Dynamic 3D reconstruction and point tracking in videos are typically treated\nas separate tasks, despite their deep connection. We propose St4RTrack, a\nfeed-forward framework that simultaneously reconstructs and tracks dynamic\nvideo content in a world coordinate frame from RGB inputs. This is achieved by\npredicting two appropriately defined pointmaps for a pair of frames captured at\ndifferent moments. Specifically, we predict both pointmaps at the same moment,\nin the same world, capturing both static and dynamic scene geometry while\nmaintaining 3D correspondences. Chaining these predictions through the video\nsequence with respect to a reference frame naturally computes long-range\ncorrespondences, effectively combining 3D reconstruction with 3D tracking.\nUnlike prior methods that rely heavily on 4D ground truth supervision, we\nemploy a novel adaptation scheme based on a reprojection loss. We establish a\nnew extensive benchmark for world-frame reconstruction and tracking,\ndemonstrating the effectiveness and efficiency of our unified, data-driven\nframework. Our code, model, and benchmark will be released.", "published": "2025-04-17 17:55:58", "link": "http://arxiv.org/abs/2504.13152v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "PCBEAR: Pose Concept Bottleneck for Explainable Action Recognition", "abstract": "Human action recognition (HAR) has achieved impressive results with deep\nlearning models, but their decision-making process remains opaque due to their\nblack-box nature. Ensuring interpretability is crucial, especially for\nreal-world applications requiring transparency and accountability. Existing\nvideo XAI methods primarily rely on feature attribution or static textual\nconcepts, both of which struggle to capture motion dynamics and temporal\ndependencies essential for action understanding. To address these challenges,\nwe propose Pose Concept Bottleneck for Explainable Action Recognition (PCBEAR),\na novel concept bottleneck framework that introduces human pose sequences as\nmotion-aware, structured concepts for video action recognition. Unlike methods\nbased on pixel-level features or static textual descriptions, PCBEAR leverages\nhuman skeleton poses, which focus solely on body movements, providing robust\nand interpretable explanations of motion dynamics. We define two types of\npose-based concepts: static pose concepts for spatial configurations at\nindividual frames, and dynamic pose concepts for motion patterns across\nmultiple frames. To construct these concepts, PCBEAR applies clustering to\nvideo pose sequences, allowing for automatic discovery of meaningful concepts\nwithout manual annotation. We validate PCBEAR on KTH, Penn-Action, and HAA500,\nshowing that it achieves high classification performance while offering\ninterpretable, motion-driven explanations. Our method provides both strong\npredictive performance and human-understandable insights into the model's\nreasoning process, enabling test-time interventions for debugging and improving\nmodel behavior.", "published": "2025-04-17 17:50:07", "link": "http://arxiv.org/abs/2504.13140v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models", "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.", "published": "2025-04-17 17:39:41", "link": "http://arxiv.org/abs/2504.13122v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "UniEdit-Flow: Unleashing Inversion and Editing in the Era of Flow Models", "abstract": "Flow matching models have emerged as a strong alternative to diffusion\nmodels, but existing inversion and editing methods designed for diffusion are\noften ineffective or inapplicable to them. The straight-line, non-crossing\ntrajectories of flow models pose challenges for diffusion-based approaches but\nalso open avenues for novel solutions. In this paper, we introduce a\npredictor-corrector-based framework for inversion and editing in flow models.\nFirst, we propose Uni-Inv, an effective inversion method designed for accurate\nreconstruction. Building on this, we extend the concept of delayed injection to\nflow models and introduce Uni-Edit, a region-aware, robust image editing\napproach. Our methodology is tuning-free, model-agnostic, efficient, and\neffective, enabling diverse edits while ensuring strong preservation of\nedit-irrelevant regions. Extensive experiments across various generative models\ndemonstrate the superiority and generalizability of Uni-Inv and Uni-Edit, even\nunder low-cost settings. Project page: https://uniedit-flow.github.io/", "published": "2025-04-17 17:24:23", "link": "http://arxiv.org/abs/2504.13109v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RF-DETR Object Detection vs YOLOv12 : A Study of Transformer-based and CNN-based Architectures for Single-Class and Multi-Class Greenfruit Detection in Complex Orchard Environments Under Label Ambiguity", "abstract": "This study conducts a detailed comparison of RF-DETR object detection base\nmodel and YOLOv12 object detection model configurations for detecting\ngreenfruits in a complex orchard environment marked by label ambiguity,\nocclusions, and background blending. A custom dataset was developed featuring\nboth single-class (greenfruit) and multi-class (occluded and non-occluded\ngreenfruits) annotations to assess model performance under dynamic real-world\nconditions. RF-DETR object detection model, utilizing a DINOv2 backbone and\ndeformable attention, excelled in global context modeling, effectively\nidentifying partially occluded or ambiguous greenfruits. In contrast, YOLOv12\nleveraged CNN-based attention for enhanced local feature extraction, optimizing\nit for computational efficiency and edge deployment. RF-DETR achieved the\nhighest mean Average Precision (mAP50) of 0.9464 in single-class detection,\nproving its superior ability to localize greenfruits in cluttered scenes.\nAlthough YOLOv12N recorded the highest mAP@50:95 of 0.7620, RF-DETR\nconsistently outperformed in complex spatial scenarios. For multi-class\ndetection, RF-DETR led with an mAP@50 of 0.8298, showing its capability to\ndifferentiate between occluded and non-occluded fruits, while YOLOv12L scored\nhighest in mAP@50:95 with 0.6622, indicating better classification in detailed\nocclusion contexts. Training dynamics analysis highlighted RF-DETR's swift\nconvergence, particularly in single-class settings where it plateaued within 10\nepochs, demonstrating the efficiency of transformer-based architectures in\nadapting to dynamic visual data. These findings validate RF-DETR's\neffectiveness for precision agricultural applications, with YOLOv12 suited for\nfast-response scenarios. >Index Terms: RF-DETR object detection, YOLOv12,\nYOLOv13, YOLOv14, YOLOv15, YOLOE, YOLO World, YOLO, You Only Look Once,\nRoboflow, Detection Transformers, CNNs", "published": "2025-04-17 17:08:11", "link": "http://arxiv.org/abs/2504.13099v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "EventVAD: Training-Free Event-Aware Video Anomaly Detection", "abstract": "Video Anomaly Detection~(VAD) focuses on identifying anomalies within videos.\nSupervised methods require an amount of in-domain training data and often\nstruggle to generalize to unseen anomalies. In contrast, training-free methods\nleverage the intrinsic world knowledge of large language models (LLMs) to\ndetect anomalies but face challenges in localizing fine-grained visual\ntransitions and diverse events. Therefore, we propose EventVAD, an event-aware\nvideo anomaly detection framework that combines tailored dynamic graph\narchitectures and multimodal LLMs through temporal-event reasoning.\nSpecifically, EventVAD first employs dynamic spatiotemporal graph modeling with\ntime-decay constraints to capture event-aware video features. Then, it performs\nadaptive noise filtering and uses signal ratio thresholding to detect event\nboundaries via unsupervised statistical features. The statistical boundary\ndetection module reduces the complexity of processing long videos for MLLMs and\nimproves their temporal reasoning through event consistency. Finally, it\nutilizes a hierarchical prompting strategy to guide MLLMs in performing\nreasoning before determining final decisions. We conducted extensive\nexperiments on the UCF-Crime and XD-Violence datasets. The results demonstrate\nthat EventVAD with a 7B MLLM achieves state-of-the-art (SOTA) in training-free\nsettings, outperforming strong baselines that use 7B or larger MLLMs.", "published": "2025-04-17 16:59:04", "link": "http://arxiv.org/abs/2504.13092v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Effective Dual-Region Augmentation for Reduced Reliance on Large Amounts of Labeled Data", "abstract": "This paper introduces a novel dual-region augmentation approach designed to\nreduce reliance on large-scale labeled datasets while improving model\nrobustness and adaptability across diverse computer vision tasks, including\nsource-free domain adaptation (SFDA) and person re-identification (ReID). Our\nmethod performs targeted data transformations by applying random noise\nperturbations to foreground objects and spatially shuffling background patches.\nThis effectively increases the diversity of the training data, improving model\nrobustness and generalization. Evaluations on the PACS dataset for SFDA\ndemonstrate that our augmentation strategy consistently outperforms existing\nmethods, achieving significant accuracy improvements in both single-target and\nmulti-target adaptation settings. By augmenting training data through\nstructured transformations, our method enables model generalization across\ndomains, providing a scalable solution for reducing reliance on manually\nannotated datasets. Furthermore, experiments on Market-1501 and DukeMTMC-reID\ndatasets validate the effectiveness of our approach for person ReID, surpassing\ntraditional augmentation techniques.", "published": "2025-04-17 16:42:33", "link": "http://arxiv.org/abs/2504.13077v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SkyReels-V2: Infinite-length Film Generative Model", "abstract": "Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.", "published": "2025-04-17 16:37:27", "link": "http://arxiv.org/abs/2504.13074v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation", "abstract": "Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.", "published": "2025-04-17 16:33:39", "link": "http://arxiv.org/abs/2504.13072v1", "categories": ["cs.GR", "cs.CV", "cs.MM"], "primary_category": "cs.GR"}
{"title": "EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance", "abstract": "Echocardiography is crucial for cardiovascular disease detection but relies\nheavily on experienced sonographers. Echocardiography probe guidance systems,\nwhich provide real-time movement instructions for acquiring standard plane\nimages, offer a promising solution for AI-assisted or fully autonomous\nscanning. However, developing effective machine learning models for this task\nremains challenging, as they must grasp heart anatomy and the intricate\ninterplay between probe motion and visual signals. To address this, we present\nEchoWorld, a motion-aware world modeling framework for probe guidance that\nencodes anatomical knowledge and motion-induced visual dynamics, while\neffectively leveraging past visual-motion sequences to enhance guidance\nprecision. EchoWorld employs a pre-training strategy inspired by world modeling\nprinciples, where the model predicts masked anatomical regions and simulates\nthe visual outcomes of probe adjustments. Built upon this pre-trained model, we\nintroduce a motion-aware attention mechanism in the fine-tuning stage that\neffectively integrates historical visual-motion data, enabling precise and\nadaptive probe guidance. Trained on more than one million ultrasound images\nfrom over 200 routine scans, EchoWorld effectively captures key\nechocardiographic knowledge, as validated by qualitative analysis. Moreover,\nour method significantly reduces guidance errors compared to existing visual\nbackbones and guidance frameworks, excelling in both single-frame and\nsequential evaluation protocols. Code is available at\nhttps://github.com/LeapLabTHU/EchoWorld.", "published": "2025-04-17 16:19:05", "link": "http://arxiv.org/abs/2504.13065v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models", "abstract": "Text-to-image models based on diffusion processes, such as DALL-E, Stable\nDiffusion, and Midjourney, are capable of transforming texts into detailed\nimages and have widespread applications in art and design. As such, amateur\nusers can easily imitate professional-level paintings by collecting an artist's\nwork and fine-tuning the model, leading to concerns about artworks' copyright\ninfringement. To tackle these issues, previous studies either add visually\nimperceptible perturbation to the artwork to change its underlying styles\n(perturbation-based methods) or embed post-training detectable watermarks in\nthe artwork (watermark-based methods). However, when the artwork or the model\nhas been published online, i.e., modification to the original artwork or model\nretraining is not feasible, these strategies might not be viable.\n  To this end, we propose a novel method for data-use auditing in the\ntext-to-image generation model. The general idea of ArtistAuditor is to\nidentify if a suspicious model has been finetuned using the artworks of\nspecific artists by analyzing the features related to the style. Concretely,\nArtistAuditor employs a style extractor to obtain the multi-granularity style\nrepresentations and treats artworks as samplings of an artist's style. Then,\nArtistAuditor queries a trained discriminator to gain the auditing decisions.\nThe experimental results on six combinations of models and datasets show that\nArtistAuditor can achieve high AUC values (> 0.937). By studying\nArtistAuditor's transferability and core modules, we provide valuable insights\ninto the practical implementation. Finally, we demonstrate the effectiveness of\nArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor\nis open-sourced at https://github.com/Jozenn/ArtistAuditor.", "published": "2025-04-17 16:15:38", "link": "http://arxiv.org/abs/2504.13061v1", "categories": ["cs.CV", "cs.CR", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Imaging for All-Day Wearable Smart Glasses", "abstract": "In recent years smart glasses technology has rapidly advanced, opening up\nentirely new areas for mobile computing. We expect future smart glasses will\nneed to be all-day wearable, adopting a small form factor to meet the\nrequirements of volume, weight, fashionability and social acceptability, which\nputs significant constraints on the space of possible solutions. Additional\nchallenges arise due to the fact that smart glasses are worn in arbitrary\nenvironments while their wearer moves and performs everyday activities. In this\npaper, we systematically analyze the space of imaging from smart glasses and\nderive several fundamental limits that govern this imaging domain. We discuss\nthe impact of these limits on achievable image quality and camera module size\n-- comparing in particular to related devices such as mobile phones. We then\npropose a novel distributed imaging approach that allows to minimize the size\nof the individual camera modules when compared to a standard monolithic camera\ndesign. Finally, we demonstrate the properties of this novel approach in a\nseries of experiments using synthetic data as well as images captured with two\ndifferent prototype implementations.", "published": "2025-04-17 16:14:34", "link": "http://arxiv.org/abs/2504.13060v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation", "abstract": "Recent advances in reinforcement learning (RL) have strengthened the\nreasoning capabilities of vision-language models (VLMs). However, enhancing\npolicy exploration to more effectively scale test-time compute remains\nunderexplored in VLMs. In addition, VLMs continue to struggle with imperfect\nvisual perception, which in turn affects the subsequent reasoning process. To\nthis end, we propose NoisyRollout, a simple yet effective RL approach that\nmixes trajectories from both clean and moderately distorted images to introduce\ntargeted diversity in visual perception and the resulting reasoning patterns.\nWithout additional training cost, NoisyRollout enhances the exploration\ncapabilities of VLMs by incorporating a vision-oriented inductive bias.\nFurthermore, NoisyRollout employs a noise annealing schedule that gradually\nreduces distortion strength over training, ensuring benefit from noisy signals\nearly while maintaining training stability and scalability in later stages.\nWith just 2.1K training samples, NoisyRollout achieves state-of-the-art\nperformance among open-source RL-tuned models on 5 out-of-domain benchmarks\nspanning both reasoning and perception tasks, while preserving comparable or\neven better in-domain performance.", "published": "2025-04-17 16:10:13", "link": "http://arxiv.org/abs/2504.13055v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Expert Kernel Generation Network Driven by Contextual Mapping for Hyperspectral Image Classification", "abstract": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nEKGNet based on an improved 3D-DenseNet model, consisting of a context-aware\nmapping network and a dynamic kernel generation module. The context-aware\nmapping module translates global contextual information of hyperspectral inputs\ninto instructions for combining base convolutional kernels, while the dynamic\nkernels are composed of K groups of base convolutions, analogous to K different\ntypes of experts specializing in fundamental patterns across various\ndimensions. The mapping module and dynamic kernel generation mechanism form a\ntightly coupled system - the former generates meaningful combination weights\nbased on inputs, while the latter constructs an adaptive expert convolution\nsystem using these weights. This dynamic approach enables the model to focus\nmore flexibly on key spatial structures when processing different regions,\nrather than relying on the fixed receptive field of a single static\nconvolutional kernel. EKGNet enhances model representation capability through a\n3D dynamic expert convolution system without increasing network depth or width.\nThe proposed method demonstrates superior performance on IN, UP, and KSC\ndatasets, outperforming mainstream hyperspectral image classification\napproaches.", "published": "2025-04-17 16:00:06", "link": "http://arxiv.org/abs/2504.13045v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TTRD3: Texture Transfer Residual Denoising Dual Diffusion Model for Remote Sensing Image Super-Resolution", "abstract": "Remote Sensing Image Super-Resolution (RSISR) reconstructs high-resolution\n(HR) remote sensing images from low-resolution inputs to support fine-grained\nground object interpretation. Existing methods face three key challenges: (1)\nDifficulty in extracting multi-scale features from spatially heterogeneous RS\nscenes, (2) Limited prior information causing semantic inconsistency in\nreconstructions, and (3) Trade-off imbalance between geometric accuracy and\nvisual quality. To address these issues, we propose the Texture Transfer\nResidual Denoising Dual Diffusion Model (TTRD3) with three innovations: First,\na Multi-scale Feature Aggregation Block (MFAB) employing parallel heterogeneous\nconvolutional kernels for multi-scale feature extraction. Second, a Sparse\nTexture Transfer Guidance (STTG) module that transfers HR texture priors from\nreference images of similar scenes. Third, a Residual Denoising Dual Diffusion\nModel (RDDM) framework combining residual diffusion for deterministic\nreconstruction and noise diffusion for diverse generation. Experiments on\nmulti-source RS datasets demonstrate TTRD3's superiority over state-of-the-art\nmethods, achieving 1.43% LPIPS improvement and 3.67% FID enhancement compared\nto best-performing baselines. Code/model: https://github.com/LED-666/TTRD3.", "published": "2025-04-17 15:37:13", "link": "http://arxiv.org/abs/2504.13026v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Riemannian Patch Assignment Gradient Flows", "abstract": "This paper introduces patch assignment flows for metric data labeling on\ngraphs. Labelings are determined by regularizing initial local labelings\nthrough the dynamic interaction of both labels and label assignments across the\ngraph, entirely encoded by a dictionary of competing labeled patches and\nmediated by patch assignment variables. Maximal consistency of patch\nassignments is achieved by geometric numerical integration of a Riemannian\nascent flow, as critical point of a Lagrangian action functional. Experiments\nillustrate properties of the approach, including uncertainty quantification of\nlabel assignments.", "published": "2025-04-17 15:34:58", "link": "http://arxiv.org/abs/2504.13024v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "CompGS++: Compressed Gaussian Splatting for Static and Dynamic Scene Representation", "abstract": "Gaussian splatting demonstrates proficiency for 3D scene modeling but suffers\nfrom substantial data volume due to inherent primitive redundancy. To enable\nfuture photorealistic 3D immersive visual communication applications,\nsignificant compression is essential for transmission over the existing\nInternet infrastructure. Hence, we propose Compressed Gaussian Splatting\n(CompGS++), a novel framework that leverages compact Gaussian primitives to\nachieve accurate 3D modeling with substantial size reduction for both static\nand dynamic scenes. Our design is based on the principle of eliminating\nredundancy both between and within primitives. Specifically, we develop a\ncomprehensive prediction paradigm to address inter-primitive redundancy through\nspatial and temporal primitive prediction modules. The spatial primitive\nprediction module establishes predictive relationships for scene primitives and\nenables most primitives to be encoded as compact residuals, substantially\nreducing the spatial redundancy. We further devise a temporal primitive\nprediction module to handle dynamic scenes, which exploits primitive\ncorrelations across timestamps to effectively reduce temporal redundancy.\nMoreover, we devise a rate-constrained optimization module that jointly\nminimizes reconstruction error and rate consumption. This module effectively\neliminates parameter redundancy within primitives and enhances the overall\ncompactness of scene representations. Comprehensive evaluations across multiple\nbenchmark datasets demonstrate that CompGS++ significantly outperforms existing\nmethods, achieving superior compression performance while preserving accurate\nscene modeling. Our implementation will be made publicly available on GitHub to\nfacilitate further research.", "published": "2025-04-17 15:33:01", "link": "http://arxiv.org/abs/2504.13022v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Hierarchical Feature Learning for Medical Point Clouds via State Space Model", "abstract": "Deep learning-based point cloud modeling has been widely investigated as an\nindispensable component of general shape analysis. Recently, transformer and\nstate space model (SSM) have shown promising capacities in point cloud\nlearning. However, limited research has been conducted on medical point clouds,\nwhich have great potential in disease diagnosis and treatment. This paper\npresents an SSM-based hierarchical feature learning framework for medical point\ncloud understanding. Specifically, we down-sample input into multiple levels\nthrough the farthest point sampling. At each level, we perform a series of\nk-nearest neighbor (KNN) queries to aggregate multi-scale structural\ninformation. To assist SSM in processing point clouds, we introduce\ncoordinate-order and inside-out scanning strategies for efficient serialization\nof irregular points. Point features are calculated progressively from short\nneighbor sequences and long point sequences through vanilla and group Point SSM\nblocks, to capture both local patterns and long-range dependencies. To evaluate\nthe proposed method, we build a large-scale medical point cloud dataset named\nMedPointS for anatomy classification, completion, and segmentation. Extensive\nexperiments conducted on MedPointS demonstrate that our method achieves\nsuperior performance across all tasks. The dataset is available at\nhttps://flemme-docs.readthedocs.io/en/latest/medpoints.html. Code is merged to\na public medical imaging platform: https://github.com/wlsdzyzl/flemme.", "published": "2025-04-17 15:22:31", "link": "http://arxiv.org/abs/2504.13015v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "GSAC: Leveraging Gaussian Splatting for Photorealistic Avatar Creation with Unity Integration", "abstract": "Photorealistic avatars have become essential for immersive applications in\nvirtual reality (VR) and augmented reality (AR), enabling lifelike interactions\nin areas such as training simulations, telemedicine, and virtual collaboration.\nThese avatars bridge the gap between the physical and digital worlds, improving\nthe user experience through realistic human representation. However, existing\navatar creation techniques face significant challenges, including high costs,\nlong creation times, and limited utility in virtual applications. Manual\nmethods, such as MetaHuman, require extensive time and expertise, while\nautomatic approaches, such as NeRF-based pipelines often lack efficiency,\ndetailed facial expression fidelity, and are unable to be rendered at a speed\nsufficent for real-time applications. By involving several cutting-edge modern\ntechniques, we introduce an end-to-end 3D Gaussian Splatting (3DGS) avatar\ncreation pipeline that leverages monocular video input to create a scalable and\nefficient photorealistic avatar directly compatible with the Unity game engine.\nOur pipeline incorporates a novel Gaussian splatting technique with customized\npreprocessing that enables the user of \"in the wild\" monocular video capture,\ndetailed facial expression reconstruction and embedding within a fully rigged\navatar model. Additionally, we present a Unity-integrated Gaussian Splatting\nAvatar Editor, offering a user-friendly environment for VR/AR application\ndevelopment. Experimental results validate the effectiveness of our\npreprocessing pipeline in standardizing custom data for 3DGS training and\ndemonstrate the versatility of Gaussian avatars in Unity, highlighting the\nscalability and practicality of our approach.", "published": "2025-04-17 15:10:14", "link": "http://arxiv.org/abs/2504.12999v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "All-in-One Transferring Image Compression from Human Perception to Multi-Machine Perception", "abstract": "Efficiently transferring Learned Image Compression (LIC) model from human\nperception to machine perception is an emerging challenge in vision-centric\nrepresentation learning. Existing approaches typically adapt LIC to downstream\ntasks in a single-task manner, which is inefficient, lacks task interaction,\nand results in multiple task-specific bitstreams. To address these limitations,\nwe propose an asymmetric adaptor framework that supports multi-task adaptation\nwithin a single model. Our method introduces a shared adaptor to learn general\nsemantic features and task-specific adaptors to preserve task-level\ndistinctions. With only lightweight plug-in modules and a frozen base codec,\nour method achieves strong performance across multiple tasks while maintaining\ncompression efficiency. Experiments on the PASCAL-Context benchmark demonstrate\nthat our method outperforms both Fully Fine-Tuned and other Parameter Efficient\nFine-Tuned (PEFT) baselines, and validating the effectiveness of multi-vision\ntransferring.", "published": "2025-04-17 15:06:52", "link": "http://arxiv.org/abs/2504.12997v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Enhancing Cocoa Pod Disease Classification via Transfer Learning and Ensemble Methods: Toward Robust Predictive Modeling", "abstract": "This study presents an ensemble-based approach for cocoa pod disease\nclassification by integrating transfer learning with three ensemble learning\nstrategies: Bagging, Boosting, and Stacking. Pre-trained convolutional neural\nnetworks, including VGG16, VGG19, ResNet50, ResNet101, InceptionV3, and\nXception, were fine-tuned and employed as base learners to detect three disease\ncategories: Black Pod Rot, Pod Borer, and Healthy. A balanced dataset of 6,000\ncocoa pod images was curated and augmented to ensure robustness against\nvariations in lighting, orientation, and disease severity. The performance of\neach ensemble method was evaluated using accuracy, precision, recall, and\nF1-score. Experimental results show that Bagging consistently achieved superior\nclassification performance with a test accuracy of 100%, outperforming Boosting\n(97%) and Stacking (92%). The findings confirm that combining transfer learning\nwith ensemble techniques improves model generalization and reliability, making\nit a promising direction for precision agriculture and automated crop disease\nmanagement.", "published": "2025-04-17 15:02:27", "link": "http://arxiv.org/abs/2504.12992v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven Bi-Level Optimization for Anomaly Detection", "abstract": "Anomaly detection is a crucial task in computer vision, yet collecting\nreal-world defect images is inherently difficult due to the rarity and\nunpredictability of anomalies. Consequently, researchers have turned to\nsynthetic methods for training data augmentation. However, existing synthetic\nstrategies (e.g., naive cut-and-paste or inpainting) overlook the underlying\nphysical causes of defects, leading to inconsistent, low-fidelity anomalies\nthat hamper model generalization to real-world complexities. In this thesis, we\nintroduced a novel pipeline that generates synthetic anomalies through\nMath-Physics model guidance, refines them via a Coarse-to-Fine approach and\nemploys a bi-level optimization strategy with a Synthesis Quality\nEstimator(SQE). By incorporating physical modeling of cracks, corrosion, and\ndeformation, our method produces realistic defect masks, which are subsequently\nenhanced in two phases. The first stage (npcF) enforces a PDE-based consistency\nto achieve a globally coherent anomaly structure, while the second stage\n(npcF++) further improves local fidelity using wavelet transforms and boundary\nsynergy blocks. Additionally, we leverage SQE-driven weighting, ensuring that\nhigh-quality synthetic samples receive greater emphasis during training. To\nvalidate our approach, we conducted comprehensive experiments on three widely\nadopted industrial anomaly detection benchmarks: MVTec AD, VisA, and BTAD.\nAcross these datasets, the proposed pipeline achieves state-of-the-art (SOTA)\nresults in both image-AUROC and pixel-AUROC, confirming the effectiveness of\nour MaPhC2F and BiSQAD.", "published": "2025-04-17 14:22:27", "link": "http://arxiv.org/abs/2504.12970v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Vision and Language Integration for Domain Generalization", "abstract": "Domain generalization aims at training on source domains to uncover a\ndomain-invariant feature space, allowing the model to perform robust\ngeneralization ability on unknown target domains. However, due to domain gaps,\nit is hard to find reliable common image feature space, and the reason for that\nis the lack of suitable basic units for images. Different from image in vision\nspace, language has comprehensive expression elements that can effectively\nconvey semantics. Inspired by the semantic completeness of language and\nintuitiveness of image, we propose VLCA, which combine language space and\nvision space, and connect the multiple image domains by using semantic space as\nthe bridge domain. Specifically, in language space, by taking advantage of the\ncompleteness of language basic units, we tend to capture the semantic\nrepresentation of the relations between categories through word vector\ndistance. Then, in vision space, by taking advantage of the intuitiveness of\nimage features, the common pattern of sample features with the same class is\nexplored through low-rank approximation. In the end, the language\nrepresentation is aligned with the vision representation through the multimodal\nspace of text and image. Experiments demonstrate the effectiveness of the\nproposed method.", "published": "2025-04-17 14:19:09", "link": "http://arxiv.org/abs/2504.12966v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction", "abstract": "We present GDFusion, a temporal fusion method for vision-based 3D semantic\noccupancy prediction (VisionOcc). GDFusion opens up the underexplored aspects\nof temporal fusion within the VisionOcc framework, focusing on both temporal\ncues and fusion strategies. It systematically examines the entire VisionOcc\npipeline, identifying three fundamental yet previously overlooked temporal\ncues: scene-level consistency, motion calibration, and geometric\ncomplementation. These cues capture diverse facets of temporal evolution and\nmake distinct contributions across various modules in the VisionOcc framework.\nTo effectively fuse temporal signals across heterogeneous representations, we\npropose a novel fusion strategy by reinterpreting the formulation of vanilla\nRNNs. This reinterpretation leverages gradient descent on features to unify the\nintegration of diverse temporal information, seamlessly embedding the proposed\ntemporal cues into the network. Extensive experiments on nuScenes demonstrate\nthat GDFusion significantly outperforms established baselines. Notably, on\nOcc3D benchmark, it achieves 1.4\\%-4.8\\% mIoU improvements and reduces memory\nconsumption by 27\\%-72\\%.", "published": "2025-04-17 14:05:33", "link": "http://arxiv.org/abs/2504.12959v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Disentangling Polysemantic Channels in Convolutional Neural Networks", "abstract": "Mechanistic interpretability is concerned with analyzing individual\ncomponents in a (convolutional) neural network (CNN) and how they form larger\ncircuits representing decision mechanisms. These investigations are challenging\nsince CNNs frequently learn polysemantic channels that encode distinct\nconcepts, making them hard to interpret. To address this, we propose an\nalgorithm to disentangle a specific kind of polysemantic channel into multiple\nchannels, each responding to a single concept. Our approach restructures\nweights in a CNN, utilizing that different concepts within the same channel\nexhibit distinct activation patterns in the previous layer. By disentangling\nthese polysemantic features, we enhance the interpretability of CNNs,\nultimately improving explanatory techniques such as feature visualizations.", "published": "2025-04-17 13:37:47", "link": "http://arxiv.org/abs/2504.12939v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Efficient Masked Image Compression with Position-Indexed Self-Attention", "abstract": "In recent years, image compression for high-level vision tasks has attracted\nconsiderable attention from researchers. Given that object information in\nimages plays a far more crucial role in downstream tasks than background\ninformation, some studies have proposed semantically structuring the bitstream\nto selectively transmit and reconstruct only the information required by these\ntasks. However, such methods structure the bitstream after encoding, meaning\nthat the coding process still relies on the entire image, even though much of\nthe encoded information will not be transmitted. This leads to redundant\ncomputations. Traditional image compression methods require a two-dimensional\nimage as input, and even if the unimportant regions of the image are set to\nzero by applying a semantic mask, these regions still participate in subsequent\ncomputations as part of the image. To address such limitations, we propose an\nimage compression method based on a position-indexed self-attention mechanism\nthat encodes and decodes only the visible parts of the masked image. Compared\nto existing semantic-structured compression methods, our approach can\nsignificantly reduce computational costs.", "published": "2025-04-17 13:12:39", "link": "http://arxiv.org/abs/2504.12923v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs", "abstract": "Many works have succeeded in reconstructing Gaussian human avatars from\nmulti-view videos. However, they either struggle to capture pose-dependent\nappearance details with a single MLP, or rely on a computationally intensive\nneural network to reconstruct high-fidelity appearance but with rendering\nperformance degraded to non-real-time. We propose a novel Gaussian human avatar\nrepresentation that can reconstruct high-fidelity pose-dependence appearance\nwith details and meanwhile can be rendered in real time. Our Gaussian avatar is\nempowered by spatially distributed MLPs which are explicitly located on\ndifferent positions on human body. The parameters stored in each Gaussian are\nobtained by interpolating from the outputs of its nearby MLPs based on their\ndistances. To avoid undesired smooth Gaussian property changing during\ninterpolation, for each Gaussian we define a set of Gaussian offset basis, and\na linear combination of basis represents the Gaussian property offsets relative\nto the neutral properties. Then we propose to let the MLPs output a set of\ncoefficients corresponding to the basis. In this way, although Gaussian\ncoefficients are derived from interpolation and change smoothly, the Gaussian\noffset basis is learned freely without constraints. The smoothly varying\ncoefficients combined with freely learned basis can still produce distinctly\ndifferent Gaussian property offsets, allowing the ability to learn\nhigh-frequency spatial signals. We further use control points to constrain the\nGaussians distributed on a surface layer rather than allowing them to be\nirregularly distributed inside the body, to help the human avatar generalize\nbetter when animated under novel poses. Compared to the state-of-the-art\nmethod, our method achieves better appearance quality with finer details while\nthe rendering speed is significantly faster under novel views and novel poses.", "published": "2025-04-17 12:57:41", "link": "http://arxiv.org/abs/2504.12909v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation", "abstract": "Tactile sensing is crucial for achieving human-level robotic capabilities in\nmanipulation tasks. VBTSs have emerged as a promising solution, offering high\nspatial resolution and cost-effectiveness by sensing contact through\ncamera-captured deformation patterns of elastic gel pads. However, these\nsensors' complex physical characteristics and visual signal processing\nrequirements present unique challenges for robotic applications. The lack of\nefficient and accurate simulation tools for VBTS has significantly limited the\nscale and scope of tactile robotics research. Here we present Taccel, a\nhigh-performance simulation platform that integrates IPC and ABD to model\nrobots, tactile sensors, and objects with both accuracy and unprecedented\nspeed, achieving an 18-fold acceleration over real-time across thousands of\nparallel environments. Unlike previous simulators that operate at sub-real-time\nspeeds with limited parallelization, Taccel provides precise physics simulation\nand realistic tactile signals while supporting flexible robot-sensor\nconfigurations through user-friendly APIs. Through extensive validation in\nobject recognition, robotic grasping, and articulated object manipulation, we\ndemonstrate precise simulation and successful sim-to-real transfer. These\ncapabilities position Taccel as a powerful tool for scaling up tactile robotics\nresearch and development. By enabling large-scale simulation and\nexperimentation with tactile sensing, Taccel accelerates the development of\nmore capable robotic systems, potentially transforming how robots interact with\nand understand their physical environment.", "published": "2025-04-17 12:57:11", "link": "http://arxiv.org/abs/2504.12908v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Second-order Optimization of Gaussian Splats with Importance Sampling", "abstract": "3D Gaussian Splatting (3DGS) is widely used for novel view synthesis due to\nits high rendering quality and fast inference time. However, 3DGS predominantly\nrelies on first-order optimizers such as Adam, which leads to long training\ntimes. To address this limitation, we propose a novel second-order optimization\nstrategy based on Levenberg-Marquardt (LM) and Conjugate Gradient (CG), which\nwe specifically tailor towards Gaussian Splatting. Our key insight is that the\nJacobian in 3DGS exhibits significant sparsity since each Gaussian affects only\na limited number of pixels. We exploit this sparsity by proposing a matrix-free\nand GPU-parallelized LM optimization. To further improve its efficiency, we\npropose sampling strategies for both the camera views and loss function and,\nconsequently, the normal equation, significantly reducing the computational\ncomplexity. In addition, we increase the convergence rate of the second-order\napproximation by introducing an effective heuristic to determine the learning\nrate that avoids the expensive computation cost of line search methods. As a\nresult, our method achieves a $3\\times$ speedup over standard LM and\noutperforms Adam by $~6\\times$ when the Gaussian count is low while remaining\ncompetitive for moderate counts. Project Page:\nhttps://vcai.mpi-inf.mpg.de/projects/LM-IS", "published": "2025-04-17 12:52:08", "link": "http://arxiv.org/abs/2504.12905v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Tree-NeRV: A Tree-Structured Neural Representation for Efficient Non-Uniform Video Encoding", "abstract": "Implicit Neural Representations for Videos (NeRV) have emerged as a powerful\nparadigm for video representation, enabling direct mappings from frame indices\nto video frames. However, existing NeRV-based methods do not fully exploit\ntemporal redundancy, as they rely on uniform sampling along the temporal axis,\nleading to suboptimal rate-distortion (RD) performance. To address this\nlimitation, we propose Tree-NeRV, a novel tree-structured feature\nrepresentation for efficient and adaptive video encoding. Unlike conventional\napproaches, Tree-NeRV organizes feature representations within a Binary Search\nTree (BST), enabling non-uniform sampling along the temporal axis.\nAdditionally, we introduce an optimization-driven sampling strategy,\ndynamically allocating higher sampling density to regions with greater temporal\nvariation. Extensive experiments demonstrate that Tree-NeRV achieves superior\ncompression efficiency and reconstruction quality, outperforming prior uniform\nsampling-based methods. Code will be released.", "published": "2025-04-17 12:40:33", "link": "http://arxiv.org/abs/2504.12899v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SC3EF: A Joint Self-Correlation and Cross-Correspondence Estimation Framework for Visible and Thermal Image Registration", "abstract": "Multispectral imaging plays a critical role in a range of intelligent\ntransportation applications, including advanced driver assistance systems\n(ADAS), traffic monitoring, and night vision. However, accurate visible and\nthermal (RGB-T) image registration poses a significant challenge due to the\nconsiderable modality differences. In this paper, we present a novel joint\nSelf-Correlation and Cross-Correspondence Estimation Framework (SC3EF),\nleveraging both local representative features and global contextual cues to\neffectively generate RGB-T correspondences. For this purpose, we design a\nconvolution-transformer-based pipeline to extract local representative features\nand encode global correlations of intra-modality for inter-modality\ncorrespondence estimation between unaligned visible and thermal images. After\nmerging the local and global correspondence estimation results, we further\nemploy a hierarchical optical flow estimation decoder to progressively refine\nthe estimated dense correspondence maps. Extensive experiments demonstrate the\neffectiveness of our proposed method, outperforming the current\nstate-of-the-art (SOTA) methods on representative RGB-T datasets. Furthermore,\nit also shows competitive generalization capabilities across challenging\nscenarios, including large parallax, severe occlusions, adverse weather, and\nother cross-modal datasets (e.g., RGB-N and RGB-D).", "published": "2025-04-17 11:54:12", "link": "http://arxiv.org/abs/2504.12869v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Computer-Aided Design of Personalized Occlusal Positioning Splints Using Multimodal 3D Data", "abstract": "Contemporary digital technology has a pivotal role in the design of\ncustomized medical appliances, including occlusal splints used in the treatment\nof stomatognathic system dysfunctions. We present an approach to computer-aided\ndesign and precision assessment of positioning occlusal splints, bridging\nclinical concepts with current digital dental practice. In our model, a 3D\nsplint is generated based on a transformation matrix that represents the\ntherapeutic change in mandibular position, defined by a specialist using a\nvirtual patient model reconstructed from intraoral scans, CBCT, 3D facial scans\nand plaster model digitisation. The paper introduces a novel method for\ngenerating splints that accurately reproduce occlusal conditions in the\ntherapeutic position, including a mechanism for resolving surface conflicts\nthrough virtual embossing. We demonstrate how transformation matrices can be\nacquired through clinical tools and intraoral devices, and evaluate the\naccuracy of the designed and printed splints using profile and surface\ndeviation analysis. The proposed method enables reproducible, patient-specific\nsplint fabrication and opens new possibilities in diagnostics, multimodal image\nregistration and quantification of occlusal discrepancies.", "published": "2025-04-17 11:53:49", "link": "http://arxiv.org/abs/2504.12868v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "High-Fidelity Image Inpainting with Multimodal Guided GAN Inversion", "abstract": "Generative Adversarial Network (GAN) inversion have demonstrated excellent\nperformance in image inpainting that aims to restore lost or damaged image\ntexture using its unmasked content. Previous GAN inversion-based methods\nusually utilize well-trained GAN models as effective priors to generate the\nrealistic regions for missing holes. Despite excellence, they ignore a hard\nconstraint that the unmasked regions in the input and the output should be the\nsame, resulting in a gap between GAN inversion and image inpainting and thus\ndegrading the performance. Besides, existing GAN inversion approaches often\nconsider a single modality of the input image, neglecting other auxiliary cues\nin images for improvements. Addressing these problems, we propose a novel GAN\ninversion approach, dubbed MMInvertFill, for image inpainting. MMInvertFill\ncontains primarily a multimodal guided encoder with a pre-modulation and a GAN\ngenerator with F&W+ latent space. Specifically, the multimodal encoder aims to\nenhance the multi-scale structures with additional semantic segmentation edge\ntexture modalities through a gated mask-aware attention module. Afterwards, a\npre-modulation is presented to encode these structures into style vectors. To\nmitigate issues of conspicuous color discrepancy and semantic inconsistency, we\nintroduce the F&W+ latent space to bridge the gap between GAN inversion and\nimage inpainting. Furthermore, in order to reconstruct faithful and\nphotorealistic images, we devise a simple yet effective Soft-update Mean Latent\nmodule to capture more diversified in-domain patterns for generating\nhigh-fidelity textures for massive corruptions. In our extensive experiments on\nsix challenging datasets, we show that our MMInvertFill qualitatively and\nquantitatively outperforms other state-of-the-arts and it supports the\ncompletion of out-of-domain images effectively.", "published": "2025-04-17 10:58:45", "link": "http://arxiv.org/abs/2504.12844v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "UncAD: Towards Safe End-to-end Autonomous Driving via Online Map Uncertainty", "abstract": "End-to-end autonomous driving aims to produce planning trajectories from raw\nsensors directly. Currently, most approaches integrate perception, prediction,\nand planning modules into a fully differentiable network, promising great\nscalability. However, these methods typically rely on deterministic modeling of\nonline maps in the perception module for guiding or constraining vehicle\nplanning, which may incorporate erroneous perception information and further\ncompromise planning safety. To address this issue, we delve into the importance\nof online map uncertainty for enhancing autonomous driving safety and propose a\nnovel paradigm named UncAD. Specifically, UncAD first estimates the uncertainty\nof the online map in the perception module. It then leverages the uncertainty\nto guide motion prediction and planning modules to produce multi-modal\ntrajectories. Finally, to achieve safer autonomous driving, UncAD proposes an\nuncertainty-collision-aware planning selection strategy according to the online\nmap uncertainty to evaluate and select the best trajectory. In this study, we\nincorporate UncAD into various state-of-the-art (SOTA) end-to-end methods.\nExperiments on the nuScenes dataset show that integrating UncAD, with only a\n1.9% increase in parameters, can reduce collision rates by up to 26% and\ndrivable area conflict rate by up to 42%. Codes, pre-trained models, and demo\nvideos can be accessed at https://github.com/pengxuanyang/UncAD.", "published": "2025-04-17 10:40:36", "link": "http://arxiv.org/abs/2504.12826v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "TwoSquared: 4D Generation from 2D Image Pairs", "abstract": "Despite the astonishing progress in generative AI, 4D dynamic object\ngeneration remains an open challenge. With limited high-quality training data\nand heavy computing requirements, the combination of hallucinating unseen\ngeometry together with unseen movement poses great challenges to generative\nmodels. In this work, we propose TwoSquared as a method to obtain a 4D\nphysically plausible sequence starting from only two 2D RGB images\ncorresponding to the beginning and end of the action. Instead of directly\nsolving the 4D generation problem, TwoSquared decomposes the problem into two\nsteps: 1) an image-to-3D module generation based on the existing generative\nmodel trained on high-quality 3D assets, and 2) a physically inspired\ndeformation module to predict intermediate movements. To this end, our method\ndoes not require templates or object-class-specific prior knowledge and can\ntake in-the-wild images as input. In our experiments, we demonstrate that\nTwoSquared is capable of producing texture-consistent and geometry-consistent\n4D sequences only given 2D images.", "published": "2025-04-17 10:39:52", "link": "http://arxiv.org/abs/2504.12825v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AAA-Gaussians: Anti-Aliased and Artifact-Free 3D Gaussian Rendering", "abstract": "Although 3D Gaussian Splatting (3DGS) has revolutionized 3D reconstruction,\nit still faces challenges such as aliasing, projection artifacts, and view\ninconsistencies, primarily due to the simplification of treating splats as 2D\nentities. We argue that incorporating full 3D evaluation of Gaussians\nthroughout the 3DGS pipeline can effectively address these issues while\npreserving rasterization efficiency. Specifically, we introduce an adaptive 3D\nsmoothing filter to mitigate aliasing and present a stable view-space bounding\nmethod that eliminates popping artifacts when Gaussians extend beyond the view\nfrustum. Furthermore, we promote tile-based culling to 3D with screen-space\nplanes, accelerating rendering and reducing sorting costs for hierarchical\nrasterization. Our method achieves state-of-the-art quality on in-distribution\nevaluation sets and significantly outperforms other approaches for\nout-of-distribution views. Our qualitative evaluations further demonstrate the\neffective removal of aliasing, distortions, and popping artifacts, ensuring\nreal-time, artifact-free rendering.", "published": "2025-04-17 10:16:47", "link": "http://arxiv.org/abs/2504.12811v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Saliency-Aware Diffusion Reconstruction for Effective Invisible Watermark Removal", "abstract": "As digital content becomes increasingly ubiquitous, the need for robust\nwatermark removal techniques has grown due to the inadequacy of existing\nembedding techniques, which lack robustness. This paper introduces a novel\nSaliency-Aware Diffusion Reconstruction (SADRE) framework for watermark\nelimination on the web, combining adaptive noise injection, region-specific\nperturbations, and advanced diffusion-based reconstruction. SADRE disrupts\nembedded watermarks by injecting targeted noise into latent representations\nguided by saliency masks although preserving essential image features. A\nreverse diffusion process ensures high-fidelity image restoration, leveraging\nadaptive noise levels determined by watermark strength. Our framework is\ntheoretically grounded with stability guarantees and achieves robust watermark\nremoval across diverse scenarios. Empirical evaluations on state-of-the-art\n(SOTA) watermarking techniques demonstrate SADRE's superiority in balancing\nwatermark disruption and image quality. SADRE sets a new benchmark for\nwatermark elimination, offering a flexible and reliable solution for real-world\nweb content. Code is available\non~\\href{https://github.com/inzamamulDU/SADRE}{\\textbf{https://github.com/inzamamulDU/SADRE}}.", "published": "2025-04-17 10:15:10", "link": "http://arxiv.org/abs/2504.12809v1", "categories": ["cs.CV", "cs.MM", "I.4.5; I.5.4"], "primary_category": "cs.CV"}
{"title": "Sign-In to the Lottery: Reparameterizing Sparse Training From Scratch", "abstract": "The performance gap between training sparse neural networks from scratch\n(PaI) and dense-to-sparse training presents a major roadblock for efficient\ndeep learning. According to the Lottery Ticket Hypothesis, PaI hinges on\nfinding a problem specific parameter initialization. As we show, to this end,\ndetermining correct parameter signs is sufficient. Yet, they remain elusive to\nPaI. To address this issue, we propose Sign-In, which employs a dynamic\nreparameterization that provably induces sign flips. Such sign flips are\ncomplementary to the ones that dense-to-sparse training can accomplish,\nrendering Sign-In as an orthogonal method. While our experiments and theory\nsuggest performance improvements of PaI, they also carve out the main open\nchallenge to close the gap between PaI and dense-to-sparse training.", "published": "2025-04-17 10:01:59", "link": "http://arxiv.org/abs/2504.12801v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "CAGE-GS: High-fidelity Cage Based 3D Gaussian Splatting Deformation", "abstract": "As 3D Gaussian Splatting (3DGS) gains popularity as a 3D representation of\nreal scenes, enabling user-friendly deformation to create novel scenes while\npreserving fine details from the original 3DGS has attracted significant\nresearch attention. We introduce CAGE-GS, a cage-based 3DGS deformation method\nthat seamlessly aligns a source 3DGS scene with a user-defined target shape.\nOur approach learns a deformation cage from the target, which guides the\ngeometric transformation of the source scene. While the cages effectively\ncontrol structural alignment, preserving the textural appearance of 3DGS\nremains challenging due to the complexity of covariance parameters. To address\nthis, we employ a Jacobian matrix-based strategy to update the covariance\nparameters of each Gaussian, ensuring texture fidelity post-deformation. Our\nmethod is highly flexible, accommodating various target shape representations,\nincluding texts, images, point clouds, meshes and 3DGS models. Extensive\nexperiments and ablation studies on both public datasets and newly proposed\nscenes demonstrate that our method significantly outperforms existing\ntechniques in both efficiency and deformation quality.", "published": "2025-04-17 10:00:15", "link": "http://arxiv.org/abs/2504.12800v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors", "abstract": "Reconstructing transparent surfaces is essential for tasks such as robotic\nmanipulation in labs, yet it poses a significant challenge for 3D\nreconstruction techniques like 3D Gaussian Splatting (3DGS). These methods\noften encounter a transparency-depth dilemma, where the pursuit of\nphotorealistic rendering through standard $\\alpha$-blending undermines\ngeometric precision, resulting in considerable depth estimation errors for\ntransparent materials. To address this issue, we introduce Transparent Surface\nGaussian Splatting (TSGS), a new framework that separates geometry learning\nfrom appearance refinement. In the geometry learning stage, TSGS focuses on\ngeometry by using specular-suppressed inputs to accurately represent surfaces.\nIn the second stage, TSGS improves visual fidelity through anisotropic specular\nmodeling, crucially maintaining the established opacity to ensure geometric\naccuracy. To enhance depth inference, TSGS employs a first-surface depth\nextraction method. This technique uses a sliding window over $\\alpha$-blending\nweights to pinpoint the most likely surface location and calculates a robust\nweighted average depth. To evaluate the transparent surface reconstruction task\nunder realistic conditions, we collect a TransLab dataset that includes complex\ntransparent laboratory glassware. Extensive experiments on TransLab show that\nTSGS achieves accurate geometric reconstruction and realistic rendering of\ntransparent objects simultaneously within the efficient 3DGS framework.\nSpecifically, TSGS significantly surpasses current leading methods, achieving a\n37.3% reduction in chamfer distance and an 8.0% improvement in F1 score\ncompared to the top baseline. The code and dataset will be released at\nhttps://longxiang-ai.github.io/TSGS/.", "published": "2025-04-17 10:00:09", "link": "http://arxiv.org/abs/2504.12799v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "EarthGPT-X: Enabling MLLMs to Flexibly and Comprehensively Understand Multi-Source Remote Sensing Imagery", "abstract": "Recent advances in the visual-language area have developed natural\nmulti-modal large language models (MLLMs) for spatial reasoning through visual\nprompting. However, due to remote sensing (RS) imagery containing abundant\ngeospatial information that differs from natural images, it is challenging to\neffectively adapt natural spatial models to the RS domain. Moreover, current RS\nMLLMs are limited in overly narrow interpretation levels and interaction\nmanner, hindering their applicability in real-world scenarios. To address those\nchallenges, a spatial MLLM named EarthGPT-X is proposed, enabling a\ncomprehensive understanding of multi-source RS imagery, such as optical,\nsynthetic aperture radar (SAR), and infrared. EarthGPT-X offers zoom-in and\nzoom-out insight, and possesses flexible multi-grained interactive abilities.\nMoreover, EarthGPT-X unifies two types of critical spatial tasks (i.e.,\nreferring and grounding) into a visual prompting framework. To achieve these\nversatile capabilities, several key strategies are developed. The first is the\nmulti-modal content integration method, which enhances the interplay between\nimages, visual prompts, and text instructions. Subsequently, a cross-domain\none-stage fusion training strategy is proposed, utilizing the large language\nmodel (LLM) as a unified interface for multi-source multi-task learning.\nFurthermore, by incorporating a pixel perception module, the referring and\ngrounding tasks are seamlessly unified within a single framework. In addition,\nthe experiments conducted demonstrate the superiority of the proposed\nEarthGPT-X in multi-grained tasks and its impressive flexibility in multi-modal\ninteraction, revealing significant advancements of MLLM in the RS field.", "published": "2025-04-17 09:56:35", "link": "http://arxiv.org/abs/2504.12795v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ARAP-GS: Drag-driven As-Rigid-As-Possible 3D Gaussian Splatting Editing with Diffusion Prior", "abstract": "Drag-driven editing has become popular among designers for its ability to\nmodify complex geometric structures through simple and intuitive manipulation,\nallowing users to adjust and reshape content with minimal technical skill. This\ndrag operation has been incorporated into numerous methods to facilitate the\nediting of 2D images and 3D meshes in design. However, few studies have\nexplored drag-driven editing for the widely-used 3D Gaussian Splatting (3DGS)\nrepresentation, as deforming 3DGS while preserving shape coherence and visual\ncontinuity remains challenging. In this paper, we introduce ARAP-GS, a\ndrag-driven 3DGS editing framework based on As-Rigid-As-Possible (ARAP)\ndeformation. Unlike previous 3DGS editing methods, we are the first to apply\nARAP deformation directly to 3D Gaussians, enabling flexible, drag-driven\ngeometric transformations. To preserve scene appearance after deformation, we\nincorporate an advanced diffusion prior for image super-resolution within our\niterative optimization process. This approach enhances visual quality while\nmaintaining multi-view consistency in the edited results. Experiments show that\nARAP-GS outperforms current methods across diverse 3D scenes, demonstrating its\neffectiveness and superiority for drag-driven 3DGS editing. Additionally, our\nmethod is highly efficient, requiring only 10 to 20 minutes to edit a scene on\na single RTX 3090 GPU.", "published": "2025-04-17 09:37:11", "link": "http://arxiv.org/abs/2504.12788v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation", "abstract": "Vision Foundation Models (VFMs) have delivered remarkable performance in\nDomain Generalized Semantic Segmentation (DGSS). However, recent methods often\noverlook the fact that visual cues are susceptible, whereas the underlying\ngeometry remains stable, rendering depth information more robust. In this\npaper, we investigate the potential of integrating depth information with\nfeatures from VFMs, to improve the geometric consistency within an image and\nboost the generalization performance of VFMs. We propose a novel fine-tuning\nDGSS framework, named DepthForge, which integrates the visual cues from frozen\nDINOv2 or EVA02 and depth cues from frozen Depth Anything V2. In each layer of\nthe VFMs, we incorporate depth-aware learnable tokens to continuously decouple\ndomain-invariant visual and spatial information, thereby enhancing depth\nawareness and attention of the VFMs. Finally, we develop a depth refinement\ndecoder and integrate it into the model architecture to adaptively refine\nmulti-layer VFM features and depth-aware learnable tokens. Extensive\nexperiments are conducted based on various DGSS settings and five different\ndatsets as unseen target domains. The qualitative and quantitative results\ndemonstrate that our method significantly outperforms alternative approaches\nwith stronger performance, steadier visual-spatial attention, and superior\ngeneralization ability. In particular, DepthForge exhibits outstanding\nperformance under extreme conditions (e.g., night and snow). Code is available\nat https://github.com/anonymouse-xzrptkvyqc/DepthForge.", "published": "2025-04-17 08:45:33", "link": "http://arxiv.org/abs/2504.12753v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical Anomaly Detection", "abstract": "Recent advances in industrial anomaly detection have highlighted the need for\ndeeper logical anomaly analysis, where unexpected relationships among objects,\ncounts, and spatial configurations must be identified and explained. Existing\napproaches often rely on large-scale external reasoning modules or elaborate\npipeline designs, hindering practical deployment and interpretability. To\naddress these limitations, we introduce a new task, Reasoning Logical Anomaly\nDetection (RLAD), which extends traditional anomaly detection by incorporating\nlogical reasoning. We propose a new framework, LAD-Reasoner, a customized tiny\nmultimodal language model built on Qwen2.5-VL 3B. Our approach leverages a\ntwo-stage training paradigm that first employs Supervised Fine-Tuning (SFT) for\nfine-grained visual understanding, followed by Group Relative Policy\nOptimization (GRPO) to refine logical anomaly detection and enforce coherent,\nhuman-readable reasoning. Crucially, reward signals are derived from both the\ndetection accuracy and the structural quality of the outputs, obviating the\nneed for building chain of thought (CoT) reasoning data. Experiments on the\nMVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller,\nmatches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and further\nexcels in producing concise and interpretable rationales. This unified design\nreduces reliance on large models and complex pipelines, while offering\ntransparent and interpretable insights into logical anomaly detection. Code and\ndata will be released.", "published": "2025-04-17 08:41:23", "link": "http://arxiv.org/abs/2504.12749v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Privacy Protection Against Personalized Text-to-Image Synthesis via Cross-image Consistency Constraints", "abstract": "The rapid advancement of diffusion models and personalization techniques has\nmade it possible to recreate individual portraits from just a few publicly\navailable images. While such capabilities empower various creative\napplications, they also introduce serious privacy concerns, as adversaries can\nexploit them to generate highly realistic impersonations. To counter these\nthreats, anti-personalization methods have been proposed, which add adversarial\nperturbations to published images to disrupt the training of personalization\nmodels. However, existing approaches largely overlook the intrinsic multi-image\nnature of personalization and instead adopt a naive strategy of applying\nperturbations independently, as commonly done in single-image settings. This\nneglects the opportunity to leverage inter-image relationships for stronger\nprivacy protection. Therefore, we advocate for a group-level perspective on\nprivacy protection against personalization. Specifically, we introduce\nCross-image Anti-Personalization (CAP), a novel framework that enhances\nresistance to personalization by enforcing style consistency across perturbed\nimages. Furthermore, we develop a dynamic ratio adjustment strategy that\nadaptively balances the impact of the consistency loss throughout the attack\niterations. Extensive experiments on the classical CelebHQ and VGGFace2\nbenchmarks show that CAP substantially improves existing methods.", "published": "2025-04-17 08:39:32", "link": "http://arxiv.org/abs/2504.12747v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Mask Image Watermarking", "abstract": "We present MaskMark, a simple, efficient and flexible framework for image\nwatermarking. MaskMark has two variants: MaskMark-D, which supports global\nwatermark embedding, watermark localization, and local watermark extraction for\napplications such as tamper detection, and MaskMark-ED, which focuses on local\nwatermark embedding and extraction with enhanced robustness in small regions,\nenabling localized image protection. Built upon the classical Encoder-\nDistortion-Decoder training paradigm, MaskMark-D introduces a simple masking\nmechanism during the decoding stage to support both global and local watermark\nextraction. A mask is applied to the watermarked image before extraction,\nallowing the decoder to focus on selected regions and learn local extraction. A\nlocalization module is also integrated into the decoder to identify watermark\nregions during inference, reducing interference from irrelevant content and\nimproving accuracy. MaskMark-ED extends this design by incorporating the mask\ninto the encoding stage as well, guiding the encoder to embed the watermark in\ndesignated local regions for enhanced robustness. Comprehensive experiments\nshow that MaskMark achieves state-of-the-art performance in global watermark\nextraction, local watermark extraction, watermark localization, and\nmulti-watermark embedding. It outperforms all existing baselines, including the\nrecent leading model WAM for local watermarking, while preserving high visual\nquality of the watermarked images. MaskMark is also flexible, by adjusting the\ndistortion layer, it can adapt to different robustness requirements with just a\nfew steps of fine-tuning. Moreover, our approach is efficient and easy to\noptimize, requiring only 20 hours on a single A6000 GPU with just 1/15 the\ncomputational cost of WAM.", "published": "2025-04-17 08:29:00", "link": "http://arxiv.org/abs/2504.12739v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Self-Supervised Pre-training with Combined Datasets for 3D Perception in Autonomous Driving", "abstract": "The significant achievements of pre-trained models leveraging large volumes\nof data in the field of NLP and 2D vision inspire us to explore the potential\nof extensive data pre-training for 3D perception in autonomous driving. Toward\nthis goal, this paper proposes to utilize massive unlabeled data from\nheterogeneous datasets to pre-train 3D perception models. We introduce a\nself-supervised pre-training framework that learns effective 3D representations\nfrom scratch on unlabeled data, combined with a prompt adapter based domain\nadaptation strategy to reduce dataset bias. The approach significantly improves\nmodel performance on downstream tasks such as 3D object detection, BEV\nsegmentation, 3D object tracking, and occupancy prediction, and shows steady\nperformance increase as the training data volume scales up, demonstrating the\npotential of continually benefit 3D perception models for autonomous driving.\nWe will release the source code to inspire further investigations in the\ncommunity.", "published": "2025-04-17 07:26:11", "link": "http://arxiv.org/abs/2504.12709v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SmartFreeEdit: Mask-Free Spatial-Aware Image Editing with Complex Instruction Understanding", "abstract": "Recent advancements in image editing have utilized large-scale multimodal\nmodels to enable intuitive, natural instruction-driven interactions. However,\nconventional methods still face significant challenges, particularly in spatial\nreasoning, precise region segmentation, and maintaining semantic consistency,\nespecially in complex scenes. To overcome these challenges, we introduce\nSmartFreeEdit, a novel end-to-end framework that integrates a multimodal large\nlanguage model (MLLM) with a hypergraph-enhanced inpainting architecture,\nenabling precise, mask-free image editing guided exclusively by natural\nlanguage instructions. The key innovations of SmartFreeEdit include:(1)the\nintroduction of region aware tokens and a mask embedding paradigm that enhance\nthe spatial understanding of complex scenes;(2) a reasoning segmentation\npipeline designed to optimize the generation of editing masks based on natural\nlanguage instructions;and (3) a hypergraph-augmented inpainting module that\nensures the preservation of both structural integrity and semantic coherence\nduring complex edits, overcoming the limitations of local-based image\ngeneration. Extensive experiments on the Reason-Edit benchmark demonstrate that\nSmartFreeEdit surpasses current state-of-the-art methods across multiple\nevaluation metrics, including segmentation accuracy, instruction adherence, and\nvisual quality preservation, while addressing the issue of local information\nfocus and improving global consistency in the edited image. Our project will be\navailable at https://github.com/smileformylove/SmartFreeEdit.", "published": "2025-04-17 07:17:49", "link": "http://arxiv.org/abs/2504.12704v1", "categories": ["cs.CV", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Unsupervised Cross-Domain 3D Human Pose Estimation via Pseudo-Label-Guided Global Transforms", "abstract": "Existing 3D human pose estimation methods often suffer in performance, when\napplied to cross-scenario inference, due to domain shifts in characteristics\nsuch as camera viewpoint, position, posture, and body size. Among these\nfactors, camera viewpoints and locations {have been shown} to contribute\nsignificantly to the domain gap by influencing the global positions of human\nposes. To address this, we propose a novel framework that explicitly conducts\nglobal transformations between pose positions in the camera coordinate systems\nof source and target domains. We start with a Pseudo-Label Generation Module\nthat is applied to the 2D poses of the target dataset to generate pseudo-3D\nposes. Then, a Global Transformation Module leverages a human-centered\ncoordinate system as a novel bridging mechanism to seamlessly align the\npositional orientations of poses across disparate domains, ensuring consistent\nspatial referencing. To further enhance generalization, a Pose Augmentor is\nincorporated to address variations in human posture and body size. This process\nis iterative, allowing refined pseudo-labels to progressively improve guidance\nfor domain adaptation. Our method is evaluated on various cross-dataset\nbenchmarks, including Human3.6M, MPI-INF-3DHP, and 3DPW. The proposed method\noutperforms state-of-the-art approaches and even outperforms the target-trained\nmodel.", "published": "2025-04-17 06:57:20", "link": "http://arxiv.org/abs/2504.12699v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Collaborative Perception Datasets for Autonomous Driving: A Review", "abstract": "Collaborative perception has attracted growing interest from academia and\nindustry due to its potential to enhance perception accuracy, safety, and\nrobustness in autonomous driving through multi-agent information fusion. With\nthe advancement of Vehicle-to-Everything (V2X) communication, numerous\ncollaborative perception datasets have emerged, varying in cooperation\nparadigms, sensor configurations, data sources, and application scenarios.\nHowever, the absence of systematic summarization and comparative analysis\nhinders effective resource utilization and standardization of model evaluation.\nAs the first comprehensive review focused on collaborative perception datasets,\nthis work reviews and compares existing resources from a multi-dimensional\nperspective. We categorize datasets based on cooperation paradigms, examine\ntheir data sources and scenarios, and analyze sensor modalities and supported\ntasks. A detailed comparative analysis is conducted across multiple dimensions.\nWe also outline key challenges and future directions, including dataset\nscalability, diversity, domain adaptation, standardization, privacy, and the\nintegration of large language models. To support ongoing research, we provide a\ncontinuously updated online repository of collaborative perception datasets and\nrelated literature:\nhttps://github.com/frankwnb/Collaborative-Perception-Datasets-for-Autonomous-Driving.", "published": "2025-04-17 06:49:21", "link": "http://arxiv.org/abs/2504.12696v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "HSS-IAD: A Heterogeneous Same-Sort Industrial Anomaly Detection Dataset", "abstract": "Multi-class Unsupervised Anomaly Detection algorithms (MUAD) are receiving\nincreasing attention due to their relatively low deployment costs and improved\ntraining efficiency. However, the real-world effectiveness of MUAD methods is\nquestioned due to limitations in current Industrial Anomaly Detection (IAD)\ndatasets. These datasets contain numerous classes that are unlikely to be\nproduced by the same factory and fail to cover multiple structures or\nappearances. Additionally, the defects do not reflect real-world\ncharacteristics. Therefore, we introduce the Heterogeneous Same-Sort Industrial\nAnomaly Detection (HSS-IAD) dataset, which contains 8,580 images of\nmetallic-like industrial parts and precise anomaly annotations. These parts\nexhibit variations in structure and appearance, with subtle defects that\nclosely resemble the base materials. We also provide foreground images for\nsynthetic anomaly generation. Finally, we evaluate popular IAD methods on this\ndataset under multi-class and class-separated settings, demonstrating its\npotential to bridge the gap between existing datasets and real factory\nconditions. The dataset is available at\nhttps://github.com/Qiqigeww/HSS-IAD-Dataset.", "published": "2025-04-17 06:31:26", "link": "http://arxiv.org/abs/2504.12689v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SOPHY: Generating Simulation-Ready Objects with Physical Materials", "abstract": "We present SOPHY, a generative model for 3D physics-aware shape synthesis.\nUnlike existing 3D generative models that focus solely on static geometry or 4D\nmodels that produce physics-agnostic animations, our approach jointly\nsynthesizes shape, texture, and material properties related to physics-grounded\ndynamics, making the generated objects ready for simulations and interactive,\ndynamic environments. To train our model, we introduce a dataset of 3D objects\nannotated with detailed physical material attributes, along with an annotation\npipeline for efficient material annotation. Our method enables applications\nsuch as text-driven generation of interactive, physics-aware 3D objects and\nsingle-image reconstruction of physically plausible shapes. Furthermore, our\nexperiments demonstrate that jointly modeling shape and material properties\nenhances the realism and fidelity of generated shapes, improving performance on\ngenerative geometry evaluation metrics.", "published": "2025-04-17 06:17:24", "link": "http://arxiv.org/abs/2504.12684v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "TongUI: Building Generalized GUI Agents by Learning from Multimodal Web Tutorials", "abstract": "Building Graphical User Interface (GUI) agents is a promising research\ndirection, which simulates human interaction with computers or mobile phones to\nperform diverse GUI tasks. However, a major challenge in developing generalized\nGUI agents is the lack of sufficient trajectory data across various operating\nsystems and applications, mainly due to the high cost of manual annotations. In\nthis paper, we propose the TongUI framework that builds generalized GUI agents\nby learning from rich multimodal web tutorials. Concretely, we crawl and\nprocess online GUI tutorials (such as videos and articles) into GUI agent\ntrajectory data, through which we produce the GUI-Net dataset containing 143K\ntrajectory data across five operating systems and more than 200 applications.\nWe develop the TongUI agent by fine-tuning Qwen2.5-VL-3B/7B models on GUI-Net,\nwhich show remarkable performance improvements on commonly used grounding and\nnavigation benchmarks, outperforming baseline agents about 10\\% on multiple\nbenchmarks, showing the effectiveness of the GUI-Net dataset and underscoring\nthe significance of our TongUI framework. We will fully open-source the code,\nthe GUI-Net dataset, and the trained models soon.", "published": "2025-04-17 06:15:56", "link": "http://arxiv.org/abs/2504.12679v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Accurate Tracking of Arabidopsis Root Cortex Cell Nuclei in 3D Time-Lapse Microscopy Images Based on Genetic Algorithm", "abstract": "Arabidopsis is a widely used model plant to gain basic knowledge on plant\nphysiology and development. Live imaging is an important technique to visualize\nand quantify elemental processes in plant development. To uncover novel\ntheories underlying plant growth and cell division, accurate cell tracking on\nlive imaging is of utmost importance. The commonly used cell tracking software,\nTrackMate, adopts tracking-by-detection fashion, which applies Laplacian of\nGaussian (LoG) for blob detection, and Linear Assignment Problem (LAP) tracker\nfor tracking. However, they do not perform sufficiently when cells are densely\narranged. To alleviate the problems mentioned above, we propose an accurate\ntracking method based on Genetic algorithm (GA) using knowledge of Arabidopsis\nroot cellular patterns and spatial relationship among volumes. Our method can\nbe described as a coarse-to-fine method, in which we first conducted relatively\neasy line-level tracking of cell nuclei, then performed complicated nuclear\ntracking based on known linear arrangement of cell files and their spatial\nrelationship between nuclei. Our method has been evaluated on a long-time live\nimaging dataset of Arabidopsis root tips, and with minor manual rectification,\nit accurately tracks nuclei. To the best of our knowledge, this research\nrepresents the first successful attempt to address a long-standing problem in\nthe field of time-lapse microscopy in the root meristem by proposing an\naccurate tracking method for Arabidopsis root nuclei.", "published": "2025-04-17 06:07:17", "link": "http://arxiv.org/abs/2504.12676v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Two Tasks, One Goal: Uniting Motion and Planning for Excellent End To End Autonomous Driving Performance", "abstract": "End-to-end autonomous driving has made impressive progress in recent years.\nFormer end-to-end autonomous driving approaches often decouple planning and\nmotion tasks, treating them as separate modules. This separation overlooks the\npotential benefits that planning can gain from learning out-of-distribution\ndata encountered in motion tasks. However, unifying these tasks poses\nsignificant challenges, such as constructing shared contextual representations\nand handling the unobservability of other vehicles' states. To address these\nchallenges, we propose TTOG, a novel two-stage trajectory generation framework.\nIn the first stage, a diverse set of trajectory candidates is generated, while\nthe second stage focuses on refining these candidates through vehicle state\ninformation. To mitigate the issue of unavailable surrounding vehicle states,\nTTOG employs a self-vehicle data-trained state estimator, subsequently extended\nto other vehicles. Furthermore, we introduce ECSA (equivariant context-sharing\nscene adapter) to enhance the generalization of scene representations across\ndifferent agents. Experimental results demonstrate that TTOG achieves\nstate-of-the-art performance across both planning and motion tasks. Notably, on\nthe challenging open-loop nuScenes dataset, TTOG reduces the L2 distance by\n36.06\\%. Furthermore, on the closed-loop Bench2Drive dataset, our approach\nachieves a 22\\% improvement in the driving score (DS), significantly\noutperforming existing baselines.", "published": "2025-04-17 05:52:35", "link": "http://arxiv.org/abs/2504.12667v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and Scalable Classification", "abstract": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments.", "published": "2025-04-17 05:23:07", "link": "http://arxiv.org/abs/2504.12652v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "RoPETR: Improving Temporal Camera-Only 3D Detection by Integrating Enhanced Rotary Position Embedding", "abstract": "This technical report introduces a targeted improvement to the StreamPETR\nframework, specifically aimed at enhancing velocity estimation, a critical\nfactor influencing the overall NuScenes Detection Score. While StreamPETR\nexhibits strong 3D bounding box detection performance as reflected by its high\nmean Average Precision our analysis identified velocity estimation as a\nsubstantial bottleneck when evaluated on the NuScenes dataset. To overcome this\nlimitation, we propose a customized positional embedding strategy tailored to\nenhance temporal modeling capabilities. Experimental evaluations conducted on\nthe NuScenes test set demonstrate that our improved approach achieves a\nstate-of-the-art NDS of 70.86% using the ViT-L backbone, setting a new\nbenchmark for camera-only 3D object detection.", "published": "2025-04-17 05:05:31", "link": "http://arxiv.org/abs/2504.12643v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Packing Input Frame Context in Next-Frame Prediction Models for Video Generation", "abstract": "We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.", "published": "2025-04-17 04:02:31", "link": "http://arxiv.org/abs/2504.12626v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SAM-Based Building Change Detection with Distribution-Aware Fourier Adaptation and Edge-Constrained Warping", "abstract": "Building change detection remains challenging for urban development, disaster\nassessment, and military reconnaissance. While foundation models like Segment\nAnything Model (SAM) show strong segmentation capabilities, SAM is limited in\nthe task of building change detection due to domain gap issues. Existing\nadapter-based fine-tuning approaches face challenges with imbalanced building\ndistribution, resulting in poor detection of subtle changes and inaccurate edge\nextraction. Additionally, bi-temporal misalignment in change detection,\ntypically addressed by optical flow, remains vulnerable to background noises.\nThis affects the detection of building changes and compromises both detection\naccuracy and edge recognition. To tackle these challenges, we propose a new\nSAM-Based Network with Distribution-Aware Fourier Adaptation and\nEdge-Constrained Warping (FAEWNet) for building change detection. FAEWNet\nutilizes the SAM encoder to extract rich visual features from remote sensing\nimages. To guide SAM in focusing on specific ground objects in remote sensing\nscenes, we propose a Distribution-Aware Fourier Aggregated Adapter to aggregate\ntask-oriented changed information. This adapter not only effectively addresses\nthe domain gap issue, but also pays attention to the distribution of changed\nbuildings. Furthermore, to mitigate noise interference and misalignment in\nheight offset estimation, we design a novel flow module that refines building\nedge extraction and enhances the perception of changed buildings. Our\nstate-of-the-art results on the LEVIR-CD, S2Looking and WHU-CD datasets\nhighlight the effectiveness of FAEWNet. The code is available at\nhttps://github.com/SUPERMAN123000/FAEWNet.", "published": "2025-04-17 03:47:43", "link": "http://arxiv.org/abs/2504.12619v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AdaQual-Diff: Diffusion-Based Image Restoration via Adaptive Quality Prompting", "abstract": "Restoring images afflicted by complex real-world degradations remains\nchallenging, as conventional methods often fail to adapt to the unique mixture\nand severity of artifacts present. This stems from a reliance on indirect cues\nwhich poorly capture the true perceptual quality deficit. To address this\nfundamental limitation, we introduce AdaQual-Diff, a diffusion-based framework\nthat integrates perceptual quality assessment directly into the generative\nrestoration process. Our approach establishes a mathematical relationship\nbetween regional quality scores from DeQAScore and optimal guidance complexity,\nimplemented through an Adaptive Quality Prompting mechanism. This mechanism\nsystematically modulates prompt structure according to measured degradation\nseverity: regions with lower perceptual quality receive computationally\nintensive, structurally complex prompts with precise restoration directives,\nwhile higher quality regions receive minimal prompts focused on preservation\nrather than intervention. The technical core of our method lies in the dynamic\nallocation of computational resources proportional to degradation severity,\ncreating a spatially-varying guidance field that directs the diffusion process\nwith mathematical precision. By combining this quality-guided approach with\ncontent-specific conditioning, our framework achieves fine-grained control over\nregional restoration intensity without requiring additional parameters or\ninference iterations. Experimental results demonstrate that AdaQual-Diff\nachieves visually superior restorations across diverse synthetic and real-world\ndatasets.", "published": "2025-04-17 03:08:27", "link": "http://arxiv.org/abs/2504.12605v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "3DResT: A Strong Baseline for Semi-Supervised 3D Referring Expression Segmentation", "abstract": "3D Referring Expression Segmentation (3D-RES) typically requires extensive\ninstance-level annotations, which are time-consuming and costly.\nSemi-supervised learning (SSL) mitigates this by using limited labeled data\nalongside abundant unlabeled data, improving performance while reducing\nannotation costs. SSL uses a teacher-student paradigm where teacher generates\nhigh-confidence-filtered pseudo-labels to guide student. However, in the\ncontext of 3D-RES, where each label corresponds to a single mask and labeled\ndata is scarce, existing SSL methods treat high-quality pseudo-labels merely as\nauxiliary supervision, which limits the model's learning potential. The\nreliance on high-confidence thresholds for filtering often results in\npotentially valuable pseudo-labels being discarded, restricting the model's\nability to leverage the abundant unlabeled data. Therefore, we identify two\ncritical challenges in semi-supervised 3D-RES, namely, inefficient utilization\nof high-quality pseudo-labels and wastage of useful information from\nlow-quality pseudo-labels. In this paper, we introduce the first\nsemi-supervised learning framework for 3D-RES, presenting a robust baseline\nmethod named 3DResT. To address these challenges, we propose two novel designs\ncalled Teacher-Student Consistency-Based Sampling (TSCS) and Quality-Driven\nDynamic Weighting (QDW). TSCS aids in the selection of high-quality\npseudo-labels, integrating them into the labeled dataset to strengthen the\nlabeled supervision signals. QDW preserves low-quality pseudo-labels by\ndynamically assigning them lower weights, allowing for the effective extraction\nof useful information rather than discarding them. Extensive experiments\nconducted on the widely used benchmark demonstrate the effectiveness of our\nmethod. Notably, with only 1% labeled data, 3DResT achieves an mIoU improvement\nof 8.34 points compared to the fully supervised method.", "published": "2025-04-17 02:50:52", "link": "http://arxiv.org/abs/2504.12599v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Prompt-Driven and Training-Free Forgetting Approach and Dataset for Large Language Models", "abstract": "The widespread adoption of diffusion models in image generation has increased\nthe demand for privacy-compliant unlearning. However, due to the\nhigh-dimensional nature and complex feature representations of diffusion\nmodels, achieving selective unlearning remains challenging, as existing methods\nstruggle to remove sensitive information while preserving the consistency of\nnon-sensitive regions. To address this, we propose an Automatic Dataset\nCreation Framework based on prompt-based layered editing and training-free\nlocal feature removal, constructing the ForgetMe dataset and introducing the\nEntangled evaluation metric. The Entangled metric quantifies unlearning\neffectiveness by assessing the similarity and consistency between the target\nand background regions and supports both paired (Entangled-D) and unpaired\n(Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe\ndataset encompasses a diverse set of real and synthetic scenarios, including\nCUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We\napply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on\nthis dataset and validate the effectiveness of both the ForgetMe dataset and\nthe Entangled metric, establishing them as benchmarks for selective unlearning.\nOur work provides a scalable and adaptable solution for advancing\nprivacy-preserving generative AI.", "published": "2025-04-17 01:44:57", "link": "http://arxiv.org/abs/2504.12574v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Parsimonious Dataset Construction for Laparoscopic Cholecystectomy Structure Segmentation", "abstract": "Labeling has always been expensive in the medical context, which has hindered\nrelated deep learning application. Our work introduces active learning in\nsurgical video frame selection to construct a high-quality, affordable\nLaparoscopic Cholecystectomy dataset for semantic segmentation. Active learning\nallows the Deep Neural Networks (DNNs) learning pipeline to include the dataset\nconstruction workflow, which means DNNs trained by existing dataset will\nidentify the most informative data from the newly collected data. At the same\ntime, DNNs' performance and generalization ability improve over time when the\nnewly selected and annotated data are included in the training data. We\nassessed different data informativeness measurements and found the deep\nfeatures distances select the most informative data in this task. Our\nexperiments show that with half of the data selected by active learning, the\nDNNs achieve almost the same performance with 0.4349 mean Intersection over\nUnion (mIoU) compared to the same DNNs trained on the full dataset (0.4374\nmIoU) on the critical anatomies and surgical instruments.", "published": "2025-04-17 01:40:30", "link": "http://arxiv.org/abs/2504.12573v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "An exact approach for the multi-depot electric vehicle scheduling problem", "abstract": "The \"avoid - shift - improve\" framework and the European Clean Vehicles\nDirective set the path for improving the efficiency and ultimately\ndecarbonizing the transport sector. While electric buses have already been\nadopted in several cities, regional bus lines may pose additional challenges\ndue to the potentially longer distances they have to travel. In this work, we\nmodel and solve the electric bus scheduling problem, lexicographically\nminimizing the size of the bus fleet, the number of charging stops, and the\ntotal energy consumed, to provide decision support for bus operators planning\nto replace their diesel-powered fleet with zero emission vehicles. We propose a\ngraph representation which allows partial charging without explicitly relying\non time variables and derive 3-index and 2-index mixed-integer linear\nprogramming formulations for the multi-depot electric vehicle scheduling\nproblem. While the 3-index model can be solved by an off-the-shelf solver\ndirectly, the 2-index model relies on an exponential number of constraints to\nensure the correct depot pairing. These are separated in a cutting plane\nfashion. We propose a set of instances with up to 80 service trips to compare\nthe two approaches, showing that, with a small number of depots, the compact\n3-index model performs very well. However, as the number of depots increases\nthe developed branch-and-cut algorithm proves to be of value. These findings\nnot only offer algorithmic insights but the developed approaches also provide\nactionable guidance for transit agencies and operators, allowing to quantify\ntrade-offs between fleet size, energy efficiency, and infrastructure needs\nunder realistic operational conditions.", "published": "2025-04-17 16:18:56", "link": "http://arxiv.org/abs/2504.13063v1", "categories": ["math.OC", "cs.DM"], "primary_category": "math.OC"}
{"title": "A note on distance-hereditary graphs whose complement is also distance-hereditary", "abstract": "Distance-hereditary graphs are known to be the graphs that are totally\ndecomposable for the split decomposition. We characterise distance-hereditary\ngraphs whose complement is also distance-hereditary by their split\ndecomposition and by their modular decomposition.", "published": "2025-04-17 11:29:10", "link": "http://arxiv.org/abs/2504.12857v1", "categories": ["math.CO", "cs.DM"], "primary_category": "math.CO"}
{"title": "GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks", "abstract": "In this paper, we presented GraphOmni, a comprehensive benchmark framework\nfor systematically evaluating the graph reasoning capabilities of LLMs. By\nanalyzing critical dimensions, including graph types, serialization formats,\nand prompt schemes, we provided extensive insights into the strengths and\nlimitations of current LLMs. Our empirical findings emphasize that no single\nserialization or prompting strategy consistently outperforms others. Motivated\nby these insights, we propose a reinforcement learning-based approach that\ndynamically selects the best serialization-prompt pairings, resulting in\nsignificant accuracy improvements. GraphOmni's modular and extensible design\nestablishes a robust foundation for future research, facilitating advancements\ntoward general-purpose graph reasoning models.", "published": "2025-04-17 09:01:16", "link": "http://arxiv.org/abs/2504.12764v1", "categories": ["cs.LG", "cs.DM"], "primary_category": "cs.LG"}
{"title": "Discrepancy of Arithmetic Progressions in Boxes and Convex Bodies", "abstract": "The combinatorial discrepancy of arithmetic progressions inside $[N] := \\{1,\n\\ldots, N\\}$ is the smallest integer $D$ for which $[N]$ can be colored with\ntwo colors so that any arithmetic progression in $[N]$ contains at most $D$\nmore elements from one color class than the other. Bounding the discrepancy of\nsuch set systems is a classical problem in discrepancy theory. More recently,\nthis problem was generalized to arithmetic progressions in grids like $[N]^d$\n(Valk{\\'o}) and $[N_1]\\times \\ldots \\times [N_d]$ (Fox, Xu, and Zhou). In the\nlatter setting, Fox, Xu, and Zhou gave upper and lower bounds on the\ndiscrepancy that match within a $\\frac{\\log |\\Omega|}{\\log \\log |\\Omega|}$\nfactor, where $\\Omega := [N_1]\\times \\ldots \\times [N_d]$ is the ground set. In\nthis work, we use the connection between factorization norms and discrepancy to\nimprove their upper bound to be within a $\\sqrt{\\log|\\Omega|}$ factor from the\nlower bound. We also generalize Fox, Xu, and Zhou's lower bound, and our upper\nbounds to arithmetic progressions in arbitrary convex bodies.", "published": "2025-04-17 02:47:49", "link": "http://arxiv.org/abs/2504.12598v1", "categories": ["math.CO", "cs.DM", "11K38 (Primary) 11B25 (Secondary)"], "primary_category": "math.CO"}
{"title": "Should We Tailor the Talk? Understanding the Impact of Conversational Styles on Preference Elicitation in Conversational Recommender Systems", "abstract": "Conversational recommender systems (CRSs) provide users with an interactive\nmeans to express preferences and receive real-time personalized\nrecommendations. The success of these systems is heavily influenced by the\npreference elicitation process. While existing research mainly focuses on what\nquestions to ask during preference elicitation, there is a notable gap in\nunderstanding what role broader interaction patterns including tone, pacing,\nand level of proactiveness play in supporting users in completing a given task.\nThis study investigates the impact of different conversational styles on\npreference elicitation, task performance, and user satisfaction with CRSs. We\nconducted a controlled experiment in the context of scientific literature\nrecommendation, contrasting two distinct conversational styles, high\ninvolvement (fast paced, direct, and proactive with frequent prompts) and high\nconsiderateness (polite and accommodating, prioritizing clarity and user\ncomfort) alongside a flexible experimental condition where users could switch\nbetween the two. Our results indicate that adapting conversational strategies\nbased on user expertise and allowing flexibility between styles can enhance\nboth user satisfaction and the effectiveness of recommendations in CRSs.\nOverall, our findings hold important implications for the design of future\nCRSs.", "published": "2025-04-17 17:01:17", "link": "http://arxiv.org/abs/2504.13095v1", "categories": ["cs.HC", "cs.IR"], "primary_category": "cs.HC"}
{"title": "CSMF: Cascaded Selective Mask Fine-Tuning for Multi-Objective Embedding-Based Retrieval", "abstract": "Multi-objective embedding-based retrieval (EBR) has become increasingly\ncritical due to the growing complexity of user behaviors and commercial\nobjectives. While traditional approaches often suffer from data sparsity and\nlimited information sharing between objectives, recent methods utilizing a\nshared network alongside dedicated sub-networks for each objective partially\naddress these limitations. However, such methods significantly increase the\nmodel parameters, leading to an increased retrieval latency and a limited\nability to model causal relationships between objectives. To address these\nchallenges, we propose the Cascaded Selective Mask Fine-Tuning (CSMF), a novel\nmethod that enhances both retrieval efficiency and serving performance for\nmulti-objective EBR. The CSMF framework selectively masks model parameters to\nfree up independent learning space for each objective, leveraging the cascading\nrelationships between objectives during the sequential fine-tuning. Without\nincreasing network parameters or online retrieval overhead, CSMF computes a\nlinearly weighted fusion score for multiple objective probabilities while\nsupporting flexible adjustment of each objective's weight across various\nrecommendation scenarios. Experimental results on real-world datasets\ndemonstrate the superior performance of CSMF, and online experiments validate\nits significant practical value.", "published": "2025-04-17 13:10:56", "link": "http://arxiv.org/abs/2504.12920v1", "categories": ["cs.IR", "H.3.3"], "primary_category": "cs.IR"}
{"title": "FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct Preference Optimization", "abstract": "Personalized outfit generation aims to construct a set of compatible and\npersonalized fashion items as an outfit. Recently, generative AI models have\nreceived widespread attention, as they can generate fashion items for users to\ncomplete an incomplete outfit or create a complete outfit. However, they have\nlimitations in terms of lacking diversity and relying on the supervised\nlearning paradigm. Recognizing this gap, we propose a novel framework\nFashionDPO, which fine-tunes the fashion outfit generation model using direct\npreference optimization. This framework aims to provide a general fine-tuning\napproach to fashion generative models, refining a pre-trained fashion outfit\ngeneration model using automatically generated feedback, without the need to\ndesign a task-specific reward function. To make sure that the feedback is\ncomprehensive and objective, we design a multi-expert feedback generation\nmodule which covers three evaluation perspectives, \\ie quality, compatibility\nand personalization. Experiments on two established datasets, \\ie iFashion and\nPolyvore-U, demonstrate the effectiveness of our framework in enhancing the\nmodel's ability to align with users' personalized preferences while adhering to\nfashion compatibility principles. Our code and model checkpoints are available\nat https://github.com/Yzcreator/FashionDPO.", "published": "2025-04-17 12:41:41", "link": "http://arxiv.org/abs/2504.12900v1", "categories": ["cs.MM", "cs.IR"], "primary_category": "cs.MM"}
{"title": "Validating LLM-Generated Relevance Labels for Educational Resource Search", "abstract": "Manual relevance judgements in Information Retrieval are costly and require\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\nassessment. While LLMs have shown promise in general web search scenarios,\ntheir effectiveness for evaluating domain-specific search results, such as\neducational resources, remains unexplored. To investigate different ways of\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\ncollected and released a dataset of 401 human relevance judgements from a user\nstudy involving teaching professionals performing search tasks related to\nlesson planning. We compared three approaches to structuring these prompts: a\nsimple two-aspect evaluation baseline from prior work on using LLMs as\nrelevance judges, a comprehensive 12-dimensional rubric derived from\neducational literature, and criteria directly informed by the study\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\nwith human judgements (Cohen's $\\kappa$ up to 0.650), significantly\noutperforming the baseline approach. The participant-derived framework proved\nparticularly robust, with GPT-3.5 achieving $\\kappa$ scores of 0.639 and 0.613\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\nshowed that LLM judgements reliably identified top-performing retrieval\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\neffectively evaluate educational resources when prompted with domain-specific\ncriteria, though performance varies with framework complexity and input\nstructure.", "published": "2025-04-17 08:14:45", "link": "http://arxiv.org/abs/2504.12732v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Degrees of Freedom of Holographic MIMO -- Fundamental Theory and Analytical Methods", "abstract": "Holographic multiple-input multiple-output (MIMO) is envisioned as one of the\nmost promising technology enablers for future sixth-generation (6G) networks.\nThe use of electrically large holographic surface (HoloS) antennas has the\npotential to significantly boost the spatial multiplexing gain by increasing\nthe number of degrees of freedom (DoF), even in line-of-sight (LoS) channels.\nIn this context, the research community has shown a growing interest in\ncharacterizing the fundamental limits of this technology. In this paper, we\ncompare the two analytical methods commonly utilized in the literature for this\npurpose: the cut-set integral and the self-adjoint operator. We provide a\ndetailed description of both methods and discuss their advantages and\nlimitations.", "published": "2025-04-17 15:41:28", "link": "http://arxiv.org/abs/2504.13031v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Query Complexity of Classical and Quantum Channel Discrimination", "abstract": "Quantum channel discrimination has been studied from an information-theoretic\nperspective, wherein one is interested in the optimal decay rate of error\nprobabilities as a function of the number of unknown channel accesses. In this\npaper, we study the query complexity of quantum channel discrimination, wherein\nthe goal is to determine the minimum number of channel uses needed to reach a\ndesired error probability. To this end, we show that the query complexity of\nbinary channel discrimination depends logarithmically on the inverse error\nprobability and inversely on the negative logarithm of the (geometric and\nHolevo) channel fidelity. As a special case of these findings, we precisely\ncharacterize the query complexity of discriminating between two classical\nchannels. We also provide lower and upper bounds on the query complexity of\nbinary asymmetric channel discrimination and multiple quantum channel\ndiscrimination. For the former, the query complexity depends on the geometric\nR\\'enyi and Petz R\\'enyi channel divergences, while for the latter, it depends\non the negative logarithm of (geometric and Uhlmann) channel fidelity. For\nmultiple channel discrimination, the upper bound scales as the logarithm of the\nnumber of channels.", "published": "2025-04-17 14:54:00", "link": "http://arxiv.org/abs/2504.12989v1", "categories": ["quant-ph", "cs.IT", "cs.LG", "math.IT", "math.ST", "stat.TH"], "primary_category": "quant-ph"}
{"title": "Optimizing Movable Antennas in Wideband Multi-User MIMO With Hardware Impairments", "abstract": "Movable antennas represent an emerging field in telecommunication research\nand a potential approach to achieving higher data rates in multiple-input\nmultiple-output (MIMO) communications when the total number of antennas is\nlimited. Most solutions and analyses to date have been limited to\n\\emph{narrowband} setups. This work complements the prior studies by\nquantifying the benefit of using movable antennas in \\emph{wideband} MIMO\ncommunication systems. First, we derive a novel uplink wideband system model\nthat also accounts for distortion from transceiver hardware impairments. We\nthen formulate and solve an optimization task to maximize the average sum rate\nby adjusting the antenna positions using particle swarm optimization. Finally,\nthe performance with movable antennas is compared with fixed uniform arrays and\nthe derived theoretical upper bound. The numerical study concludes that the\ndata rate improvement from movable antennas over other arrays heavily depends\non the level of hardware impairments, the richness of the multi-path\nenvironments, and the number of subcarriers. The present study provides vital\ninsights into the most suitable use cases for movable antennas in future\nwideband systems.", "published": "2025-04-17 12:20:46", "link": "http://arxiv.org/abs/2504.12885v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Codes over Finite Ring $\\mathbb{Z}_k$, MacWilliams Identity and Theta Function", "abstract": "In this paper, we study linear codes over $\\mathbb{Z}_k$ based on lattices\nand theta functions. We obtain the complete weight enumerators MacWilliams\nidentity and the symmetrized weight enumerators MacWilliams identity based on\nthe theory of theta function. We extend the main work by Bannai, Dougherty,\nHarada and Oura to the finite ring $\\mathbb{Z}_k$ for any positive integer $k$\nand present the complete weight enumerators MacWilliams identity in genus $g$.\nWhen $k=p$ is a prime number, we establish the relationship between the theta\nfunction of associated lattices over a cyclotomic field and the complete weight\nenumerators with Hamming weight of codes, which is an analogy of the results by\nG. Van der Geer and F. Hirzebruch since they showed the identity with the Lee\nweight enumerators.", "published": "2025-04-17 03:07:48", "link": "http://arxiv.org/abs/2504.12604v1", "categories": ["cs.IT", "cs.CR", "math.IT"], "primary_category": "cs.IT"}
{"title": "Meta-Dependence in Conditional Independence Testing", "abstract": "Constraint-based causal discovery algorithms utilize many statistical tests\nfor conditional independence to uncover networks of causal dependencies. These\napproaches to causal discovery rely on an assumed correspondence between the\ngraphical properties of a causal structure and the conditional independence\nproperties of observed variables, known as the causal Markov condition and\nfaithfulness. Finite data yields an empirical distribution that is \"close\" to\nthe actual distribution. Across these many possible empirical distributions,\nthe correspondence to the graphical properties can break down for different\nconditional independencies, and multiple violations can occur at the same time.\nWe study this \"meta-dependence\" between conditional independence properties\nusing the following geometric intuition: each conditional independence property\nconstrains the space of possible joint distributions to a manifold. The\n\"meta-dependence\" between conditional independences is informed by the position\nof these manifolds relative to the true probability distribution. We provide a\nsimple-to-compute measure of this meta-dependence using information projections\nand consolidate our findings empirically using both synthetic and real-world\ndata.", "published": "2025-04-17 02:41:22", "link": "http://arxiv.org/abs/2504.12594v1", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Aligning Constraint Generation with Design Intent in Parametric CAD", "abstract": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.", "published": "2025-04-17 17:59:54", "link": "http://arxiv.org/abs/2504.13178v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction", "abstract": "Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.", "published": "2025-04-17 17:51:38", "link": "http://arxiv.org/abs/2504.13142v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Predicting BVD Re-emergence in Irish Cattle From Highly Imbalanced Herd-Level Data Using Machine Learning Algorithms", "abstract": "Bovine Viral Diarrhoea (BVD) has been the focus of a successful eradication\nprogramme in Ireland, with the herd-level prevalence declining from 11.3% in\n2013 to just 0.2% in 2023. As the country moves toward BVD freedom, the\ndevelopment of predictive models for targeted surveillance becomes increasingly\nimportant to mitigate the risk of disease re-emergence. In this study, we\nevaluate the performance of a range of machine learning algorithms, including\nbinary classification and anomaly detection techniques, for predicting\nBVD-positive herds using highly imbalanced herd-level data. We conduct an\nextensive simulation study to assess model performance across varying sample\nsizes and class imbalance ratios, incorporating resampling, class weighting,\nand appropriate evaluation metrics (sensitivity, positive predictive value,\nF1-score and AUC values). Random forests and XGBoost models consistently\noutperformed other methods, with the random forest model achieving the highest\nsensitivity and AUC across scenarios, including real-world prediction of 2023\nherd status, correctly identifying 219 of 250 positive herds while halving the\nnumber of herds that require compared to a blanket-testing strategy.", "published": "2025-04-17 17:33:15", "link": "http://arxiv.org/abs/2504.13116v1", "categories": ["cs.LG", "stat.ME"], "primary_category": "cs.LG"}
{"title": "Quorum: Zero-Training Unsupervised Anomaly Detection using Quantum Autoencoders", "abstract": "Detecting mission-critical anomalous events and data is a crucial challenge\nacross various industries, including finance, healthcare, and energy. Quantum\ncomputing has recently emerged as a powerful tool for tackling several machine\nlearning tasks, but training quantum machine learning models remains\nchallenging, particularly due to the difficulty of gradient calculation. The\nchallenge is even greater for anomaly detection, where unsupervised learning\nmethods are essential to ensure practical applicability. To address these\nissues, we propose Quorum, the first quantum anomaly detection framework\ndesigned for unsupervised learning that operates without requiring any\ntraining.", "published": "2025-04-17 17:27:39", "link": "http://arxiv.org/abs/2504.13113v1", "categories": ["cs.LG", "cs.ET"], "primary_category": "cs.LG"}
{"title": "Hadamard product in deep learning: Introduction, Advances and Challenges", "abstract": "While convolution and self-attention mechanisms have dominated architectural\ndesign in deep learning, this survey examines a fundamental yet understudied\nprimitive: the Hadamard product. Despite its widespread implementation across\nvarious applications, the Hadamard product has not been systematically analyzed\nas a core architectural primitive. We present the first comprehensive taxonomy\nof its applications in deep learning, identifying four principal domains:\nhigher-order correlation, multimodal data fusion, dynamic representation\nmodulation, and efficient pairwise operations. The Hadamard product's ability\nto model nonlinear interactions with linear computational complexity makes it\nparticularly valuable for resource-constrained deployments and edge computing\nscenarios. We demonstrate its natural applicability in multimodal fusion tasks,\nsuch as visual question answering, and its effectiveness in representation\nmasking for applications including image inpainting and pruning. This\nsystematic review not only consolidates existing knowledge about the Hadamard\nproduct's role in deep learning architectures but also establishes a foundation\nfor future architectural innovations. Our analysis reveals the Hadamard product\nas a versatile primitive that offers compelling trade-offs between\ncomputational efficiency and representational power, positioning it as a\ncrucial component in the deep learning toolkit.", "published": "2025-04-17 17:26:29", "link": "http://arxiv.org/abs/2504.13112v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Uncertainty-Aware Trajectory Prediction via Rule-Regularized Heteroscedastic Deep Classification", "abstract": "Deep learning-based trajectory prediction models have demonstrated promising\ncapabilities in capturing complex interactions. However, their\nout-of-distribution generalization remains a significant challenge,\nparticularly due to unbalanced data and a lack of enough data and diversity to\nensure robustness and calibration. To address this, we propose SHIFT (Spectral\nHeteroscedastic Informed Forecasting for Trajectories), a novel framework that\nuniquely combines well-calibrated uncertainty modeling with informative priors\nderived through automated rule extraction. SHIFT reformulates trajectory\nprediction as a classification task and employs heteroscedastic\nspectral-normalized Gaussian processes to effectively disentangle epistemic and\naleatoric uncertainties. We learn informative priors from training labels,\nwhich are automatically generated from natural language driving rules, such as\nstop rules and drivability constraints, using a retrieval-augmented generation\nframework powered by a large language model. Extensive evaluations over the\nnuScenes dataset, including challenging low-data and cross-location scenarios,\ndemonstrate that SHIFT outperforms state-of-the-art methods, achieving\nsubstantial gains in uncertainty calibration and displacement metrics. In\nparticular, our model excels in complex scenarios, such as intersections, where\nuncertainty is inherently higher. Project page:\nhttps://kumarmanas.github.io/SHIFT/.", "published": "2025-04-17 17:24:50", "link": "http://arxiv.org/abs/2504.13111v1", "categories": ["cs.LG", "cs.RO"], "primary_category": "cs.LG"}
{"title": "Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time", "abstract": "We study the approximation gap between the dynamics of a polynomial-width\nneural network and its infinite-width counterpart, both trained using projected\ngradient descent in the mean-field scaling regime. We demonstrate how to\ntightly bound this approximation gap through a differential equation governed\nby the mean-field dynamics. A key factor influencing the growth of this ODE is\nthe local Hessian of each particle, defined as the derivative of the particle's\nvelocity in the mean-field dynamics with respect to its position. We apply our\nresults to the canonical feature learning problem of estimating a\nwell-specified single-index model; we permit the information exponent to be\narbitrarily large, leading to convergence times that grow polynomially in the\nambient dimension $d$. We show that, due to a certain ``self-concordance''\nproperty in these problems -- where the local Hessian of a particle is bounded\nby a constant times the particle's velocity -- polynomially many neurons are\nsufficient to closely approximate the mean-field dynamics throughout training.", "published": "2025-04-17 17:24:38", "link": "http://arxiv.org/abs/2504.13110v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "An All-Atom Generative Model for Designing Protein Complexes", "abstract": "Proteins typically exist in complexes, interacting with other proteins or\nbiomolecules to perform their specific biological roles. Research on\nsingle-chain protein modeling has been extensively and deeply explored, with\nadvancements seen in models like the series of ESM and AlphaFold. Despite these\ndevelopments, the study and modeling of multi-chain proteins remain largely\nuncharted, though they are vital for understanding biological functions.\nRecognizing the importance of these interactions, we introduce APM (All-Atom\nProtein Generative Model), a model specifically designed for modeling\nmulti-chain proteins. By integrating atom-level information and leveraging data\non multi-chain proteins, APM is capable of precisely modeling inter-chain\ninteractions and designing protein complexes with binding capabilities from\nscratch. It also performs folding and inverse-folding tasks for multi-chain\nproteins. Moreover, APM demonstrates versatility in downstream applications: it\nachieves enhanced performance through supervised fine-tuning (SFT) while also\nsupporting zero-shot sampling in certain tasks, achieving state-of-the-art\nresults. Code will be released at https://github.com/bytedance/apm.", "published": "2025-04-17 16:37:41", "link": "http://arxiv.org/abs/2504.13075v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "The Dissipation Theory of Aging: A Quantitative Analysis Using a Cellular Aging Map", "abstract": "We propose a new theory for aging based on dynamical systems and provide a\ndata-driven computational method to quantify the changes at the cellular level.\nWe use ergodic theory to decompose the dynamics of changes during aging and\nshow that aging is fundamentally a dissipative process within biological\nsystems, akin to dynamical systems where dissipation occurs due to\nnon-conservative forces. To quantify the dissipation dynamics, we employ a\ntransformer-based machine learning algorithm to analyze gene expression data,\nincorporating age as a token to assess how age-related dissipation is reflected\nin the embedding space. By evaluating the dynamics of gene and age embeddings,\nwe provide a cellular aging map (CAM) and identify patterns indicative of\ndivergence in gene embedding space, nonlinear transitions, and entropy\nvariations during aging for various tissues and cell types. Our results provide\na novel perspective on aging as a dissipative process and introduce a\ncomputational framework that enables measuring age-related changes with\nmolecular resolution.", "published": "2025-04-17 15:59:15", "link": "http://arxiv.org/abs/2504.13044v1", "categories": ["q-bio.QM", "cs.LG", "physics.bio-ph"], "primary_category": "q-bio.QM"}
{"title": "Inference-friendly Graph Compression for Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) have demonstrated promising performance in graph\nanalysis. Nevertheless, the inference process of GNNs remains costly, hindering\ntheir applications for large graphs. This paper proposes inference-friendly\ngraph compression (IFGC), a graph compression scheme to accelerate GNNs\ninference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed\ngraph $G_c$, to best preserve the inference results of $M$ over $G$, such that\nthe result can be directly inferred by accessing $G_c$ with no or little\ndecompression cost. (1) We characterize IFGC with a class of inference\nequivalence relation. The relation captures the node pairs in $G$ that are not\ndistinguishable for GNN inference. (2) We introduce three practical\nspecifications of IFGC for representative GNNs: structural preserving\ncompression (SPGC), which computes $G_c$ that can be directly processed by GNN\ninference without decompression; ($\\alpha$, $r$)-compression, that allows for a\nconfigurable trade-off between compression ratio and inference quality, and\nanchored compression that preserves inference results for specific nodes of\ninterest. For each scheme, we introduce compression and inference algorithms\nwith guarantees of efficiency and quality of the inferred results. We conduct\nextensive experiments on diverse sets of large-scale graphs, which verifies the\neffectiveness and efficiency of our graph compression approaches.", "published": "2025-04-17 15:42:13", "link": "http://arxiv.org/abs/2504.13034v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Chain-of-Thought Prompting for Out-of-Distribution Samples: A Latent-Variable Study", "abstract": "Chain-of-Thought (CoT) prompting has emerged as a powerful technique to\nimprove in-context learning (ICL) in large language models (LLMs) by breaking\ncomplex reasoning into intermediate steps. However, the ability of CoT to\ngeneralize under distribution shift remains poorly understood. In this work, we\nextend a latent-variable framework for CoT prompting and study its behavior on\ntwo prototypical out-of-distribution (OOD) scenarios: (i) the latent variables\nfor CoT steps are permuted into novel combinations, and (ii) the latent\nvariables uniformly scaled by a factor. Our experiments demonstrate that CoT\ninference generalizes effectively to OOD samples whose latent variables closely\nresemble those seen during training, but its performance degrades as this\nsimilarity decreases. These findings provide foundational insights into the\nstrengths and limitations of CoT prompting under OOD conditions and suggest\ndirections for developing more resilient reasoning strategies in future LLMs.", "published": "2025-04-17 14:59:29", "link": "http://arxiv.org/abs/2504.12991v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set of Experts", "abstract": "Learning-to-Defer (L2D) enables decision-making systems to improve\nreliability by selectively deferring uncertain predictions to more competent\nagents. However, most existing approaches focus exclusively on single-agent\ndeferral, which is often inadequate in high-stakes scenarios that require\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\nthe classical two-stage L2D framework that allocates each query to the $k$ most\nconfident agents instead of a single one. To further enhance flexibility and\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\nextension that learns the optimal number of agents to consult for each query,\nbased on input complexity, agent competency distributions, and consultation\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\nBayes-consistent and $(\\mathcal{R}, \\mathcal{G})$-consistent, ensuring\nconvergence to the Bayes-optimal allocation. Notably, we show that the\nwell-established model cascades paradigm arises as a restricted instance of our\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\nbenchmarks demonstrate the effectiveness of our framework on both\nclassification and regression tasks.", "published": "2025-04-17 14:50:40", "link": "http://arxiv.org/abs/2504.12988v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs", "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs). However, their performance\nheavily relies on the strategy used to select training points. Conventional\nadaptive sampling methods, such as residual-based refinement, often require\nmulti-round sampling and repeated retraining of PINNs, leading to computational\ninefficiency due to redundant points and costly gradient\ncomputations-particularly in high-dimensional or high-order derivative\nscenarios. To address these limitations, we propose RL-PINNs, a reinforcement\nlearning(RL)-driven adaptive sampling framework that enables efficient training\nwith only a single round of sampling. Our approach formulates adaptive sampling\nas a Markov decision process, where an RL agent dynamically selects optimal\ntraining points by maximizing a long-term utility metric. Critically, we\nreplace gradient-dependent residual metrics with a computationally efficient\nfunction variation as the reward signal, eliminating the overhead of derivative\ncalculations. Furthermore, we employ a delayed reward mechanism to prioritize\nlong-term training stability over short-term gains. Extensive experiments\nacross diverse PDE benchmarks, including low-regular, nonlinear,\nhigh-dimensional, and high-order problems, demonstrate that RL-PINNs\nsignificantly outperforms existing residual-driven adaptive methods in\naccuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,\nmaking them scalable to high-dimensional and high-order problems.", "published": "2025-04-17 13:50:55", "link": "http://arxiv.org/abs/2504.12949v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "On the asymptotic behaviour of stochastic processes, with applications to supermartingale convergence, Dvoretzky's approximation theorem, and stochastic quasi-Fej\u00e9r monotonicity", "abstract": "We prove a novel and general result on the asymptotic behavior of stochastic\nprocesses which conform to a certain relaxed supermartingale condition. Our\nresult provides quantitative information in the form of an explicit and\neffective construction of a rate of convergence for this process, both in mean\nand almost surely, that is moreover highly uniform in the sense that it only\ndepends on very few data of the surrounding objects involved in the iteration.\nWe then apply this result to derive new quantitative versions of well-known\nconcepts and theorems from stochastic approximation, in particular providing\neffective rates for a variant of the Robbins-Siegmund theorem, Dvoretzky's\nconvergence theorem, as well as the convergence of stochastic quasi-Fej\\'er\nmonotone sequences, the latter of which formulated in a novel and highly\ngeneral metric context. We utilize the classic and widely studied Robbins-Monro\nprocedure as a template to evaluate our quantitative results and their\napplicability in greater detail. We conclude by illustrating the breadth of\npotential further applications with a brief discussion on a variety of other\nwell-known iterative procedures from stochastic approximation, covering a range\nof different applied scenarios to which our methods can be immediately applied.\nThroughout, we isolate and discuss special cases of our results which even\nallow for the construction of fast, and in particular linear, rates.", "published": "2025-04-17 13:11:26", "link": "http://arxiv.org/abs/2504.12922v1", "categories": ["math.OC", "cs.LG", "math.LO", "math.PR"], "primary_category": "math.OC"}
{"title": "IdentiARAT: Toward Automated Identification of Individual ARAT Items from Wearable Sensors", "abstract": "This study explores the potential of using wrist-worn inertial sensors to\nautomate the labeling of ARAT (Action Research Arm Test) items. While the ARAT\nis commonly used to assess upper limb motor function, its limitations include\nsubjectivity and time consumption of clinical staff. By using IMU (Inertial\nMeasurement Unit) sensors and MiniROCKET as a time series classification\ntechnique, this investigation aims to classify ARAT items based on sensor\nrecordings. We test common preprocessing strategies to efficiently leverage\nincluded information in the data. Afterward, we use the best preprocessing to\nimprove the classification. The dataset includes recordings of 45 participants\nperforming various ARAT items. Results show that MiniROCKET offers a fast and\nreliable approach for classifying ARAT domains, although challenges remain in\ndistinguishing between individual resembling items. Future work may involve\nimproving classification through more advanced machine-learning models and data\nenhancements.", "published": "2025-04-17 13:11:13", "link": "http://arxiv.org/abs/2504.12921v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Sliced-Wasserstein Distance-based Data Selection", "abstract": "We propose a new unsupervised anomaly detection method based on the\nsliced-Wasserstein distance for training data selection in machine learning\napproaches. Our filtering technique is interesting for decision-making\npipelines deploying machine learning models in critical sectors, e.g., power\nsystems, as it offers a conservative data selection and an optimal transport\ninterpretation. To ensure the scalability of our method, we provide two\nefficient approximations. The first approximation processes reduced-cardinality\nrepresentations of the datasets concurrently. The second makes use of a\ncomputationally light Euclidian distance approximation. Additionally, we open\nthe first dataset showcasing localized critical peak rebate demand response in\na northern climate. We present the filtering patterns of our method on\nsynthetic datasets and numerically benchmark our method for training data\nselection. Finally, we employ our method as part of a first forecasting\nbenchmark for our open-source dataset.", "published": "2025-04-17 13:07:26", "link": "http://arxiv.org/abs/2504.12918v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Exact Learning Dynamics of In-Context Learning in Linear Transformers and Its Application to Non-Linear Transformers", "abstract": "Transformer models exhibit remarkable in-context learning (ICL), adapting to\nnovel tasks from examples within their context, yet the underlying mechanisms\nremain largely mysterious. Here, we provide an exact analytical\ncharacterization of ICL emergence by deriving the closed-form stochastic\ngradient descent (SGD) dynamics for a simplified linear transformer performing\nregression tasks. Our analysis reveals key properties: (1) a natural separation\nof timescales directly governed by the input data's covariance structure,\nleading to staged learning; (2) an exact description of how ICL develops,\nincluding fixed points corresponding to learned algorithms and conservation\nlaws constraining the dynamics; and (3) surprisingly nonlinear learning\nbehavior despite the model's linearity. We hypothesize this phenomenology\nextends to non-linear models. To test this, we introduce theory-inspired\nmacroscopic measures (spectral rank dynamics, subspace stability) and use them\nto provide mechanistic explanations for (1) the sudden emergence of ICL in\nattention-only networks and (2) delayed generalization (grokking) in modular\narithmetic models. Our work offers an exact dynamical model for ICL and\ntheoretically grounded tools for analyzing complex transformer training.", "published": "2025-04-17 13:05:33", "link": "http://arxiv.org/abs/2504.12916v1", "categories": ["cs.LG", "cond-mat.dis-nn"], "primary_category": "cs.LG"}
{"title": "Mirror, Mirror of the Flow: How Does Regularization Shape Implicit Bias?", "abstract": "Implicit bias plays an important role in explaining how overparameterized\nmodels generalize well. Explicit regularization like weight decay is often\nemployed in addition to prevent overfitting. While both concepts have been\nstudied separately, in practice, they often act in tandem. Understanding their\ninterplay is key to controlling the shape and strength of implicit bias, as it\ncan be modified by explicit regularization. To this end, we incorporate\nexplicit regularization into the mirror flow framework and analyze its lasting\neffects on the geometry of the training dynamics, covering three distinct\neffects: positional bias, type of bias, and range shrinking. Our analytical\napproach encompasses a broad class of problems, including sparse coding, matrix\nsensing, single-layer attention, and LoRA, for which we demonstrate the utility\nof our insights. To exploit the lasting effect of regularization and highlight\nthe potential benefit of dynamic weight decay schedules, we propose to switch\noff weight decay during training, which can improve generalization, as we\ndemonstrate in experiments.", "published": "2025-04-17 12:17:51", "link": "http://arxiv.org/abs/2504.12883v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Can Masked Autoencoders Also Listen to Birds?", "abstract": "Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\nfine-grained acoustic characteristics of specialized domains such as\nbioacoustic monitoring. Bird sound classification is critical for assessing\nenvironmental health, yet general-purpose models inadequately address its\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\ndownstream tasks, substantially improving multi-label classification\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\nwe propose prototypical probing, a parameter-efficient method for leveraging\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\nprobing by up to 37\\% in MAP and narrow the gap to fine-tuning to approximately\n3\\% on average on BirdSet.", "published": "2025-04-17 12:13:25", "link": "http://arxiv.org/abs/2504.12880v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "A Client-level Assessment of Collaborative Backdoor Poisoning in Non-IID Federated Learning", "abstract": "Federated learning (FL) enables collaborative model training using\ndecentralized private data from multiple clients. While FL has shown robustness\nagainst poisoning attacks with basic defenses, our research reveals new\nvulnerabilities stemming from non-independent and identically distributed\n(non-IID) data among clients. These vulnerabilities pose a substantial risk of\nmodel poisoning in real-world FL scenarios.\n  To demonstrate such vulnerabilities, we develop a novel collaborative\nbackdoor poisoning attack called CollaPois. In this attack, we distribute a\nsingle pre-trained model infected with a Trojan to a group of compromised\nclients. These clients then work together to produce malicious gradients,\ncausing the FL model to consistently converge towards a low-loss region\ncentered around the Trojan-infected model. Consequently, the impact of the\nTrojan is amplified, especially when the benign clients have diverse local data\ndistributions and scattered local gradients. CollaPois stands out by achieving\nits goals while involving only a limited number of compromised clients, setting\nit apart from existing attacks. Also, CollaPois effectively avoids noticeable\nshifts or degradation in the FL model's performance on legitimate data samples,\nallowing it to operate stealthily and evade detection by advanced robust FL\nalgorithms.\n  Thorough theoretical analysis and experiments conducted on various benchmark\ndatasets demonstrate the superiority of CollaPois compared to state-of-the-art\nbackdoor attacks. Notably, CollaPois bypasses existing backdoor defenses,\nespecially in scenarios where clients possess diverse data distributions.\nMoreover, the results show that CollaPois remains effective even when involving\na small number of compromised clients. Notably, clients whose local data is\nclosely aligned with compromised clients experience higher risks of backdoor\ninfections.", "published": "2025-04-17 12:03:02", "link": "http://arxiv.org/abs/2504.12875v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "When do Random Forests work?", "abstract": "We study the effectiveness of randomizing split-directions in random forests.\nPrior literature has shown that, on the one hand, randomization can reduce\nvariance through decorrelation, and, on the other hand, randomization\nregularizes and works in low signal-to-noise ratio (SNR) environments. First,\nwe bring together and revisit decorrelation and regularization by presenting a\nsystematic analysis of out-of-sample mean-squared error (MSE) for different SNR\nscenarios based on commonly-used data-generating processes. We find that\nvariance reduction tends to increase with the SNR and forests outperform\nbagging when the SNR is low because, in low SNR cases, variance dominates bias\nfor both methods. Second, we show that the effectiveness of randomization is a\nquestion that goes beyond the SNR. We present a simulation study with fixed and\nmoderate SNR, in which we examine the effectiveness of randomization for other\ndata characteristics. In particular, we find that (i) randomization can\nincrease bias in the presence of fat tails in the distribution of covariates;\n(ii) in the presence of irrelevant covariates randomization is ineffective\nbecause bias dominates variance; and (iii) when covariates are mutually\ncorrelated randomization tends to be effective because variance dominates bias.\nBeyond randomization, we find that, for both bagging and random forests, bias\ncan be significantly reduced in the presence of correlated covariates. This\nlast finding goes beyond the prevailing view that averaging mostly works by\nvariance reduction. Given that in practice covariates are often correlated, our\nfindings on correlated covariates could open the way for a better understanding\nof why random forests work well in many applications.", "published": "2025-04-17 11:38:17", "link": "http://arxiv.org/abs/2504.12860v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise to Improve Imbalanced Data Classification", "abstract": "Classifying imbalanced datasets remains a significant challenge in machine\nlearning, particularly with big data where instances are unevenly distributed\namong classes, leading to class imbalance issues that impact classifier\nperformance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses\nthis challenge by generating new instances for the under-represented minority\nclass, it faces obstacles in the form of noise and outliers during the creation\nof new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses\nthe limitations of SMOTE by first cleansing the data from noise points. This\nprocess involves employing feature selection using a random forest to identify\nthe most valuable features, followed by applying the Density-Based Spatial\nClustering of Applications with Noise (DBSCAN) algorithm to detect outliers\nbased on the selected features. The identified outliers from the minority\nclasses are then removed, creating a refined dataset for subsequent\noversampling using the hybrid approach called iHHO-SMOTe. The comprehensive\nexperiments across diverse datasets demonstrate the exceptional performance of\nthe proposed model, with an AUC score exceeding 0.99, a high G-means score of\n0.99 highlighting its robustness, and an outstanding F1-score consistently\nexceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a\nformidable contender in addressing imbalanced datasets, focusing on noise\nreduction and outlier handling for improved classification models.", "published": "2025-04-17 11:17:53", "link": "http://arxiv.org/abs/2504.12850v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "FedX: Adaptive Model Decomposition and Quantization for IoT Federated Learning", "abstract": "Federated Learning (FL) allows collaborative training among multiple devices\nwithout data sharing, thus enabling privacy-sensitive applications on mobile or\nInternet of Things (IoT) devices, such as mobile health and asset tracking.\nHowever, designing an FL system with good model utility that works with low\ncomputation/communication overhead on heterogeneous, resource-constrained\nmobile/IoT devices is challenging. To address this problem, this paper proposes\nFedX, a novel adaptive model decomposition and quantization FL system for IoT.\nTo balance utility with resource constraints on IoT devices, FedX decomposes a\nglobal FL model into different sub-networks with adaptive numbers of quantized\nbits for different devices. The key idea is that a device with fewer resources\nreceives a smaller sub-network for lower overhead but utilizes a larger number\nof quantized bits for higher model utility, and vice versa. The quantization\noperations in FedX are done at the server to reduce the computational load on\ndevices. FedX iteratively minimizes the losses in the devices' local data and\nin the server's public data using quantized sub-networks under a regularization\nterm, and thus it maximizes the benefits of combining FL with model\nquantization through knowledge sharing among the server and devices in a\ncost-effective training process. Extensive experiments show that FedX\nsignificantly improves quantization times by up to 8.43X, on-device computation\ntime by 1.5X, and total end-to-end training time by 1.36X, compared with\nbaseline FL systems. We guarantee the global model convergence theoretically\nand validate local model convergence empirically, highlighting FedX's\noptimization efficiency.", "published": "2025-04-17 11:08:51", "link": "http://arxiv.org/abs/2504.12849v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Predicting Stock Prices using Permutation Decision Trees and Strategic Trailing", "abstract": "In this paper, we explore the application of Permutation Decision Trees (PDT)\nand strategic trailing for predicting stock market movements and executing\nprofitable trades in the Indian stock market. We focus on high-frequency data\nusing 5-minute candlesticks for the top 50 stocks listed in the NIFTY 50 index.\nWe implement a trading strategy that aims to buy stocks at lower prices and\nsell them at higher prices, capitalizing on short-term market fluctuations. Due\nto regulatory constraints in India, short selling is not considered in our\nstrategy. The model incorporates various technical indicators and employs\nhyperparameters such as the trailing stop-loss value and support thresholds to\nmanage risk effectively. Our results indicate that the proposed trading bot has\nthe potential to outperform the market average and yield returns higher than\nthe risk-free rate offered by 10-year Indian government bonds. We trained and\ntested data on a 60 day dataset provided by Yahoo Finance. Specifically, 12\ndays for testing and 48 days for training. Our bot based on permutation\ndecision tree achieved a profit of 1.3468 % over a 12-day testing period, where\nas a bot based on LSTM gave a return of 0.1238 % over a 12-day testing period\nand a bot based on RNN gave a return of 0.3096 % over a 12-day testing period.\nAll of the bots outperform the buy-and-hold strategy, which resulted in a loss\nof 2.2508 %.", "published": "2025-04-17 10:42:38", "link": "http://arxiv.org/abs/2504.12828v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Universal Approximation with XL MIMO Systems: OTA Classification via Trainable Analog Combining", "abstract": "In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the XL MIMO channel\ncoefficients as the random nodes of a hidden layer, and the receiver's analog\ncombiner as a trainable output layer, we cast the end-to-end system to the\nExtreme Learning Machine (ELM) framework, leading to a novel formulation for\nOver-The-Air (OTA) edge inference without requiring traditional digital\nprocessing nor pre-processing at the transmitter. Through theoretical analysis\nand numerical evaluation, we showcase that XL-MIMO-ELM enables\nnear-instantaneous training and efficient classification, suggesting the\nparadigm shift of beyond massive MIMO systems as neural networks alongside\ntheir profound communications role. Compared to deep learning approaches and\nconventional ELMs, the proposed framework achieves on par performance with\norders of magnitude lower complexity, making it highly attractive for ultra low\npower wireless devices.", "published": "2025-04-17 08:53:30", "link": "http://arxiv.org/abs/2504.12758v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Decentralized Nonconvex Composite Federated Learning with Gradient Tracking and Momentum", "abstract": "Decentralized Federated Learning (DFL) eliminates the reliance on the\nserver-client architecture inherent in traditional federated learning,\nattracting significant research interest in recent years. Simultaneously, the\nobjective functions in machine learning tasks are often nonconvex and\nfrequently incorporate additional, potentially nonsmooth regularization terms\nto satisfy practical requirements, thereby forming nonconvex composite\noptimization problems. Employing DFL methods to solve such general optimization\nproblems leads to the formulation of Decentralized Nonconvex Composite\nFederated Learning (DNCFL), a topic that remains largely underexplored. In this\npaper, we propose a novel DNCFL algorithm, termed \\bf{DEPOSITUM}. Built upon\nproximal stochastic gradient tracking, DEPOSITUM mitigates the impact of data\nheterogeneity by enabling clients to approximate the global gradient. The\nintroduction of momentums in the proximal gradient descent step, replacing\ntracking variables, reduces the variance introduced by stochastic gradients.\nAdditionally, DEPOSITUM supports local updates of client variables,\nsignificantly reducing communication costs. Theoretical analysis demonstrates\nthat DEPOSITUM achieves an expected $\\epsilon$-stationary point with an\niteration complexity of $\\mathcal{O}(1/\\epsilon^2)$. The proximal gradient,\nconsensus errors, and gradient estimation errors decrease at a sublinear rate\nof $\\mathcal{O}(1/T)$. With appropriate parameter selection, the algorithm\nachieves network-independent linear speedup without requiring mega-batch\nsampling. Finally, we apply DEPOSITUM to the training of neural networks on\nreal-world datasets, systematically examining the influence of various\nhyperparameters on its performance. Comparisons with other federated composite\noptimization algorithms validate the effectiveness of the proposed method.", "published": "2025-04-17 08:32:25", "link": "http://arxiv.org/abs/2504.12742v1", "categories": ["cs.LG", "cs.DC", "math.OC"], "primary_category": "cs.LG"}
{"title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection", "abstract": "Graph self-supervised learning has gained significant attention recently.\nHowever, many existing approaches heavily depend on perturbations, and\ninappropriate perturbations may corrupt the graph's inherent information. The\nVector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder\nextensively used in fields such as computer vision; however, its application to\ngraph data remains underexplored. In this paper, we provide an empirical\nanalysis of vector quantization in the context of graph autoencoders,\ndemonstrating its significant enhancement of the model's capacity to capture\ngraph topology. Furthermore, we identify two key challenges associated with\nvector quantization when applying in graph data: codebook underutilization and\ncodebook space sparsity. For the first challenge, we propose an annealing-based\nencoding strategy that promotes broad code utilization in the early stages of\ntraining, gradually shifting focus toward the most effective codes as training\nprogresses. For the second challenge, we introduce a hierarchical two-layer\ncodebook that captures relationships between embeddings through clustering. The\nsecond layer codebook links similar codes, encouraging the model to learn\ncloser embeddings for nodes with similar features and structural topology in\nthe graph. Our proposed model outperforms 16 representative baseline methods in\nself-supervised link prediction and node classification tasks across multiple\ndatasets.", "published": "2025-04-17 07:43:52", "link": "http://arxiv.org/abs/2504.12715v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification", "abstract": "We study continual learning on multiple linear classification tasks by\nsequentially running gradient descent (GD) for a fixed budget of iterations per\ntask. When all tasks are jointly linearly separable and are presented in a\ncyclic/random order, we show the directional convergence of the trained linear\nclassifier to the joint (offline) max-margin solution. This is surprising\nbecause GD training on a single task is implicitly biased towards the\nindividual max-margin solution for the task, and the direction of the joint\nmax-margin solution can be largely different from these individual solutions.\nAdditionally, when tasks are given in a cyclic order, we present a\nnon-asymptotic analysis on cycle-averaged forgetting, revealing that (1)\nalignment between tasks is indeed closely tied to catastrophic forgetting and\nbackward knowledge transfer and (2) the amount of forgetting vanishes to zero\nas the cycle repeats. Lastly, we analyze the case where the tasks are no longer\njointly separable and show that the model trained in a cyclic order converges\nto the unique minimum of the joint loss function.", "published": "2025-04-17 07:35:48", "link": "http://arxiv.org/abs/2504.12712v1", "categories": ["cs.LG", "math.OC"], "primary_category": "cs.LG"}
{"title": "A Two-Phase Perspective on Deep Learning Dynamics", "abstract": "We propose that learning in deep neural networks proceeds in two phases: a\nrapid curve fitting phase followed by a slower compression or coarse graining\nphase. This view is supported by the shared temporal structure of three\nphenomena: grokking, double descent and the information bottleneck, all of\nwhich exhibit a delayed onset of generalization well after training error\nreaches zero. We empirically show that the associated timescales align in two\nrather different settings. Mutual information between hidden layers and input\ndata emerges as a natural progress measure, complementing circuit-based metrics\nsuch as local complexity and the linear mapping number. We argue that the\nsecond phase is not actively optimized by standard training algorithms and may\nbe unnecessarily prolonged. Drawing on an analogy with the renormalization\ngroup, we suggest that this compression phase reflects a principled form of\nforgetting, critical for generalization.", "published": "2025-04-17 06:57:37", "link": "http://arxiv.org/abs/2504.12700v1", "categories": ["hep-th", "cond-mat.dis-nn", "cs.LG"], "primary_category": "hep-th"}
{"title": "Attractor-merging Crises and Intermittency in Reservoir Computing", "abstract": "Reservoir computing can embed attractors into random neural networks (RNNs),\ngenerating a ``mirror'' of a target attractor because of its inherent\nsymmetrical constraints. In these RNNs, we report that an attractor-merging\ncrisis accompanied by intermittency emerges simply by adjusting the global\nparameter. We further reveal its underlying mechanism through a detailed\nanalysis of the phase-space structure and demonstrate that this bifurcation\nscenario is intrinsic to a general class of RNNs, independent of training data.", "published": "2025-04-17 06:48:51", "link": "http://arxiv.org/abs/2504.12695v1", "categories": ["nlin.CD", "cs.LG", "cs.NE", "math.DS"], "primary_category": "nlin.CD"}
{"title": "Cluster weighted models with multivariate skewed distributions for functional data", "abstract": "We propose a clustering method, funWeightClustSkew, based on mixtures of\nfunctional linear regression models and three skewed multivariate\ndistributions: the variance-gamma distribution, the skew-t distribution, and\nthe normal-inverse Gaussian distribution. Our approach follows the framework of\nthe functional high dimensional data clustering (funHDDC) method, and we extend\nto functional data the cluster weighted models based on skewed distributions\nused for finite dimensional multivariate data. We consider several parsimonious\nmodels, and to estimate the parameters we construct an expectation maximization\n(EM) algorithm. We illustrate the performance of funWeightClustSkew for\nsimulated data and for the Air Quality dataset.", "published": "2025-04-17 06:17:06", "link": "http://arxiv.org/abs/2504.12683v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Physics Informed Constrained Learning of Dynamics from Static Data", "abstract": "A physics-informed neural network (PINN) models the dynamics of a system by\nintegrating the governing physical laws into the architecture of a neural\nnetwork. By enforcing physical laws as constraints, PINN overcomes challenges\nwith data scarsity and potentially high dimensionality. Existing PINN\nframeworks rely on fully observed time-course data, the acquisition of which\ncould be prohibitive for many systems. In this study, we developed a new PINN\nlearning paradigm, namely Constrained Learning, that enables the approximation\nof first-order derivatives or motions using non-time course or partially\nobserved data. Computational principles and a general mathematical formulation\nof Constrained Learning were developed. We further introduced MPOCtrL (Message\nPassing Optimization-based Constrained Learning) an optimization approach\ntailored for the Constrained Learning framework that strives to balance the\nfitting of physical models and observed data. Its code is available at github\nlink: https://github.com/ptdang1001/MPOCtrL Experiments on synthetic and\nreal-world data demonstrated that MPOCtrL can effectively detect the nonlinear\ndependency between observed data and the underlying physical properties of the\nsystem. In particular, on the task of metabolic flux analysis, MPOCtrL\noutperforms all existing data-driven flux estimators.", "published": "2025-04-17 06:06:53", "link": "http://arxiv.org/abs/2504.12675v1", "categories": ["cs.LG", "physics.bio-ph", "q-bio.MN"], "primary_category": "cs.LG"}
{"title": "Predicting Driver's Perceived Risk: a Model Based on Semi-Supervised Learning Strategy", "abstract": "Drivers' perception of risk determines their acceptance, trust, and use of\nthe Automated Driving Systems (ADSs). However, perceived risk is subjective and\ndifficult to evaluate using existing methods. To address this issue, a driver's\nsubjective perceived risk (DSPR) model is proposed, regarding perceived risk as\na dynamically triggered mechanism with anisotropy and attenuation. 20\nparticipants are recruited for a driver-in-the-loop experiment to report their\nreal-time subjective risk ratings (SRRs) when experiencing various automatic\ndriving scenarios. A convolutional neural network and bidirectional long\nshort-term memory network with temporal pattern attention (CNN-Bi-LSTM-TPA) is\nembedded into a semi-supervised learning strategy to predict SRRs, aiming to\nreduce data noise caused by subjective randomness of participants. The results\nillustrate that DSPR achieves the highest prediction accuracy of 87.91% in\npredicting SRRs, compared to three state-of-the-art risk models. The\nsemi-supervised strategy improves accuracy by 20.12%. Besides, CNN-Bi-LSTM-TPA\nnetwork presents the highest accuracy among four different LSTM structures.\nThis study offers an effective method for assessing driver's perceived risk,\nproviding support for the safety enhancement of ADS and driver's trust\nimprovement.", "published": "2025-04-17 05:50:33", "link": "http://arxiv.org/abs/2504.12665v1", "categories": ["cs.LG", "cs.HC"], "primary_category": "cs.LG"}
{"title": "Feature selection based on cluster assumption in PU learning", "abstract": "Feature selection is essential for efficient data mining and sometimes\nencounters the positive-unlabeled (PU) learning scenario, where only a few\npositive labels are available, while most data remains unlabeled. In certain\nreal-world PU learning tasks, data subjected to adequate feature selection\noften form clusters with concentrated positive labels. Conventional feature\nselection methods that treat unlabeled data as negative may fail to capture the\nstatistical characteristics of positive data in such scenarios, leading to\nsuboptimal performance. To address this, we propose a novel feature selection\nmethod based on the cluster assumption in PU learning, called FSCPU. FSCPU\nformulates the feature selection problem as a binary optimization task, with an\nobjective function explicitly designed to incorporate the cluster assumption in\nthe PU learning setting. Experiments on synthetic datasets demonstrate the\neffectiveness of FSCPU across various data conditions. Moreover, comparisons\nwith 10 conventional algorithms on three open datasets show that FSCPU achieves\ncompetitive performance in downstream classification tasks, even when the\ncluster assumption does not strictly hold.", "published": "2025-04-17 05:22:17", "link": "http://arxiv.org/abs/2504.12651v1", "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Uncertainty Quantification in Graph Neural Networks with Shallow Ensembles", "abstract": "Machine-learned potentials (MLPs) have revolutionized materials discovery by\nproviding accurate and efficient predictions of molecular and material\nproperties. Graph Neural Networks (GNNs) have emerged as a state-of-the-art\napproach due to their ability to capture complex atomic interactions. However,\nGNNs often produce unreliable predictions when encountering out-of-domain data\nand it is difficult to identify when that happens. To address this challenge,\nwe explore Uncertainty Quantification (UQ) techniques, focusing on Direct\nPropagation of Shallow Ensembles (DPOSE) as a computationally efficient\nalternative to deep ensembles. By integrating DPOSE into the SchNet model, we\nassess its ability to provide reliable uncertainty estimates across diverse\nDensity Functional Theory datasets, including QM9, OC20, and Gold Molecular\nDynamics. Our findings often demonstrate that DPOSE successfully distinguishes\nbetween in-domain and out-of-domain samples, exhibiting higher uncertainty for\nunobserved molecule and material classes. This work highlights the potential of\nlightweight UQ methods in improving the robustness of GNN-based materials\nmodeling and lays the foundation for future integration with active learning\nstrategies.", "published": "2025-04-17 04:02:53", "link": "http://arxiv.org/abs/2504.12627v1", "categories": ["cs.LG", "physics.comp-ph"], "primary_category": "cs.LG"}
{"title": "Spectral Algorithms under Covariate Shift", "abstract": "Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under distribution shifts, specifically within the\nframework of reproducing kernel Hilbert spaces. Our study focuses on the case\nof covariate shift. In this scenario, the marginal distributions of the input\ndata differ between the training and test datasets, while the conditional\ndistribution of the output given the input remains unchanged. Under this\nsetting, we analyze the generalization error of spectral algorithms and show\nthat they achieve minimax optimality when the density ratios between the\ntraining and test distributions are uniformly bounded. However, we also\nidentify a critical limitation: when the density ratios are unbounded, the\nspectral algorithms may become suboptimal. To address this limitation, we\npropose a weighted spectral algorithm that incorporates density ratio\ninformation into the learning process. Our theoretical analysis shows that this\nweighted approach achieves optimal capacity-independent convergence rates.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm can approach the\noptimal capacity-dependent convergence rates arbitrarily closely. This\nimprovement resolves the suboptimality issue in unbounded density ratio\nscenarios and advances the state-of-the-art by refining existing theoretical\nresults.", "published": "2025-04-17 04:02:06", "link": "http://arxiv.org/abs/2504.12625v1", "categories": ["stat.ML", "cs.LG", "68Q32, 68T05, 62J02"], "primary_category": "stat.ML"}
{"title": "Machine Learning Methods for Gene Regulatory Network Inference", "abstract": "Gene Regulatory Networks (GRNs) are intricate biological systems that control\ngene expression and regulation in response to environmental and developmental\ncues. Advances in computational biology, coupled with high throughput\nsequencing technologies, have significantly improved the accuracy of GRN\ninference and modeling. Modern approaches increasingly leverage artificial\nintelligence (AI), particularly machine learning techniques including\nsupervised, unsupervised, semi-supervised, and contrastive learning to analyze\nlarge scale omics data and uncover regulatory gene interactions. To support\nboth the application of GRN inference in studying gene regulation and the\ndevelopment of novel machine learning methods, we present a comprehensive\nreview of machine learning based GRN inference methodologies, along with the\ndatasets and evaluation metrics commonly used. Special emphasis is placed on\nthe emerging role of cutting edge deep learning techniques in enhancing\ninference performance. The potential future directions for improving GRN\ninference are also discussed.", "published": "2025-04-17 03:19:49", "link": "http://arxiv.org/abs/2504.12610v1", "categories": ["cs.LG", "q-bio.MN"], "primary_category": "cs.LG"}
{"title": "Stochastic Gradient Descent in Non-Convex Problems: Asymptotic Convergence with Relaxed Step-Size via Stopping Time Methods", "abstract": "Stochastic Gradient Descent (SGD) is widely used in machine learning\nresearch. Previous convergence analyses of SGD under the vanishing step-size\nsetting typically require Robbins-Monro conditions. However, in practice, a\nwider variety of step-size schemes are frequently employed, yet existing\nconvergence results remain limited and often rely on strong assumptions. This\npaper bridges this gap by introducing a novel analytical framework based on a\nstopping-time method, enabling asymptotic convergence analysis of SGD under\nmore relaxed step-size conditions and weaker assumptions. In the non-convex\nsetting, we prove the almost sure convergence of SGD iterates for step-sizes $\n\\{ \\epsilon_t \\}_{t \\geq 1} $ satisfying $\\sum_{t=1}^{+\\infty} \\epsilon_t =\n+\\infty$ and $\\sum_{t=1}^{+\\infty} \\epsilon_t^p < +\\infty$ for some $p > 2$.\nCompared with previous studies, our analysis eliminates the global Lipschitz\ncontinuity assumption on the loss function and relaxes the boundedness\nrequirements for higher-order moments of stochastic gradients. Building upon\nthe almost sure convergence results, we further establish $L_2$ convergence.\nThese significantly relaxed assumptions make our theoretical results more\ngeneral, thereby enhancing their applicability in practical scenarios.", "published": "2025-04-17 02:56:20", "link": "http://arxiv.org/abs/2504.12601v1", "categories": ["cs.LG", "math.OC", "math.PR", "40G15", "G.1.0"], "primary_category": "cs.LG"}
{"title": "Efficient MAP Estimation of LLM Judgment Performance with Prior Transfer", "abstract": "LLM ensembles are widely used for LLM judges. However, how to estimate their\naccuracy, especially in an efficient way, is unknown. In this paper, we present\na principled maximum a posteriori (MAP) framework for an economical and precise\nestimation of the performance of LLM ensemble judgment. We first propose a\nmixture of Beta-Binomial distributions to model the judgment distribution,\nrevising from the vanilla Binomial distribution. Next, we introduce a conformal\nprediction-driven approach that enables adaptive stopping during iterative\nsampling to balance accuracy with efficiency. Furthermore, we design a prior\ntransfer mechanism that utilizes learned distributions on open-source datasets\nto improve estimation on a target dataset when only scarce annotations are\navailable. Finally, we present BetaConform, a framework that integrates our\ndistribution assumption, adaptive stopping, and the prior transfer mechanism to\ndeliver a theoretically guaranteed distribution estimation of LLM ensemble\njudgment with minimum labeled samples. BetaConform is also validated\nempirically. For instance, with only 10 samples from the TruthfulQA dataset,\nfor a Llama ensembled judge, BetaConform gauges its performance with error\nmargin as small as 3.37%.", "published": "2025-04-17 02:08:51", "link": "http://arxiv.org/abs/2504.12589v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Software Engineering Principles for Fairer Systems: Experiments with GroupCART", "abstract": "Discrimination-aware classification aims to make accurate predictions while\nsatisfying fairness constraints. Traditional decision tree learners typically\noptimize for information gain in the target attribute alone, which can result\nin models that unfairly discriminate against protected social groups (e.g.,\ngender, ethnicity). Motivated by these shortcomings, we propose GroupCART, a\ntree-based ensemble optimizer that avoids bias during model construction by\noptimizing not only for decreased entropy in the target attribute but also for\nincreased entropy in protected attributes. Our experiments show that GroupCART\nachieves fairer models without data transformation and with minimal performance\ndegradation. Furthermore, the method supports customizable weighting, offering\na smooth and flexible trade-off between predictive performance and fairness\nbased on user requirements. These results demonstrate that algorithmic bias in\ndecision tree models can be mitigated through multi-task, fairness-aware\nlearning. All code and datasets used in this study are available at:\nhttps://github.com/anonymous12138/groupCART.", "published": "2025-04-17 02:06:05", "link": "http://arxiv.org/abs/2504.12587v1", "categories": ["cs.LG", "cs.SE"], "primary_category": "cs.LG"}
{"title": "ChemKANs for Combustion Chemistry Modeling and Acceleration", "abstract": "Efficient chemical kinetic model inference and application for combustion\nproblems is challenging due to large ODE systems and wideley separated time\nscales. Machine learning techniques have been proposed to streamline these\nmodels, though strong nonlinearity and numerical stiffness combined with noisy\ndata sources makes their application challenging. The recently developed\nKolmogorov-Arnold Networks (KANs) and KAN ordinary differential equations\n(KAN-ODEs) have been demonstrated as powerful tools for scientific applications\nthanks to their rapid neural scaling, improved interpretability, and smooth\nactivation functions. Here, we develop ChemKANs by augmenting the KAN-ODE\nframework with physical knowledge of the flow of information through the\nrelevant kinetic and thermodynamic laws, as well as an elemental conservation\nloss term. This novel framework encodes strong inductive bias that enables\nstreamlined training and higher accuracy predictions, while facilitating\nparameter sparsity through full sharing of information across all inputs and\noutputs. In a model inference investigation, we find that ChemKANs exhibit no\noverfitting or model degradation when tasked with extracting predictive models\nfrom data that is both sparse and noisy, a task that a standard DeepONet\nstruggles to accomplish. Next, we find that a remarkably parameter-lean ChemKAN\n(only 344 parameters) can accurately represent hydrogen combustion chemistry,\nproviding a 2x acceleration over the detailed chemistry in a solver that is\ngeneralizable to larger-scale turbulent flow simulations. These demonstrations\nindicate potential for ChemKANs in combustion physics and chemical kinetics,\nand demonstrate the scalability of generic KAN-ODEs in significantly larger and\nmore numerically challenging problems than previously studied.", "published": "2025-04-17 01:53:28", "link": "http://arxiv.org/abs/2504.12580v1", "categories": ["cs.LG", "physics.chem-ph"], "primary_category": "cs.LG"}
{"title": "Featuremetric benchmarking: Quantum computer benchmarks based on circuit features", "abstract": "Benchmarks that concisely summarize the performance of many-qubit quantum\ncomputers are essential for measuring progress towards the goal of useful\nquantum computation. In this work, we present a benchmarking framework that is\nbased on quantifying how a quantum computer's performance on quantum circuits\nvaries as a function of features of those circuits, such as circuit depth,\nwidth, two-qubit gate density, problem input size, or algorithmic depth. Our\nfeaturemetric benchmarking framework generalizes volumetric benchmarking -- a\nwidely-used methodology that quantifies performance versus circuit width and\ndepth -- and we show that it enables richer and more faithful models of quantum\ncomputer performance. We demonstrate featuremetric benchmarking with example\nbenchmarks run on IBM Q and IonQ systems of up to 27 qubits, and we show how to\nproduce performance summaries from the data using Gaussian process regression.\nOur data analysis methods are also of interest in the special case of\nvolumetric benchmarking, as they enable the creation of intuitive\ntwo-dimensional capability regions using data from few circuits.", "published": "2025-04-17 01:49:02", "link": "http://arxiv.org/abs/2504.12575v1", "categories": ["quant-ph", "cs.LG"], "primary_category": "quant-ph"}
{"title": "The Others: Naturally Isolating Out-of-Distribution Samples for Robust Open-Set Semi-Supervised Learning", "abstract": "Open-Set Semi-Supervised Learning (OSSL) tackles the practical challenge of\nlearning from unlabeled data that may include both in-distribution (ID) and\nunknown out-of-distribution (OOD) classes. However, existing OSSL methods form\nsuboptimal feature spaces by either excluding OOD samples, interfering with\nthem, or overtrusting their information during training. In this work, we\nintroduce MagMatch, a novel framework that naturally isolates OOD samples\nthrough a prototype-based contrastive learning paradigm. Unlike conventional\nmethods, MagMatch does not assign any prototypes to OOD samples; instead, it\nselectively aligns ID samples with class prototypes using an ID-Selective\nMagnetic (ISM) module, while allowing OOD samples - the \"others\" - to remain\nunaligned in the feature space. To support this process, we propose Selective\nMagnetic Alignment (SMA) loss for unlabeled data, which dynamically adjusts\nalignment based on sample confidence. Extensive experiments on diverse datasets\ndemonstrate that MagMatch significantly outperforms existing methods in both\nclosed-set classification accuracy and OOD detection AUROC, especially in\ngeneralizing to unseen OOD data.", "published": "2025-04-17 01:37:53", "link": "http://arxiv.org/abs/2504.12569v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Evolutionary Policy Optimization", "abstract": "A key challenge in reinforcement learning (RL) is managing the\nexploration-exploitation trade-off without sacrificing sample efficiency.\nPolicy gradient (PG) methods excel in exploitation through fine-grained,\ngradient-based optimization but often struggle with exploration due to their\nfocus on local search. In contrast, evolutionary computation (EC) methods excel\nin global exploration, but lack mechanisms for exploitation. To address these\nlimitations, this paper proposes Evolutionary Policy Optimization (EPO), a\nhybrid algorithm that integrates neuroevolution with policy gradient methods\nfor policy optimization. EPO leverages the exploration capabilities of EC and\nthe exploitation strengths of PG, offering an efficient solution to the\nexploration-exploitation dilemma in RL. EPO is evaluated on the Atari Pong and\nBreakout benchmarks. Experimental results show that EPO improves both policy\nquality and sample efficiency compared to standard PG and EC methods, making it\neffective for tasks that require both exploration and local optimization.", "published": "2025-04-17 01:33:06", "link": "http://arxiv.org/abs/2504.12568v1", "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks", "abstract": "Hebbian learning limits Hopfield network capacity. While kernel methods like\nKernel Logistic Regression (KLR) improve performance via iterative learning, we\npropose Kernel Ridge Regression (KRR) as an alternative. KRR learns dual\nvariables non-iteratively via a closed-form solution, offering significant\nlearning speed advantages. We show KRR achieves comparably high storage\ncapacity (reaching ratio 1.5 shown) and noise robustness (recalling from around\n80% corrupted patterns) as KLR, while drastically reducing training time,\nestablishing KRR as an efficient method for building high-performance\nassociative memories.", "published": "2025-04-17 01:17:28", "link": "http://arxiv.org/abs/2504.12561v1", "categories": ["cs.LG", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Fine Flood Forecasts: Incorporating local data into global models through fine-tuning", "abstract": "Floods are the most common form of natural disaster and accurate flood\nforecasting is essential for early warning systems. Previous work has shown\nthat machine learning (ML) models are a promising way to improve flood\npredictions when trained on large, geographically-diverse datasets. This\nrequirement of global training can result in a loss of ownership for national\nforecasters who cannot easily adapt the models to improve performance in their\nregion, preventing ML models from being operationally deployed. Furthermore,\ntraditional hydrology research with physics-based models suggests that local\ndata -- which in many cases is only accessible to local agencies -- is valuable\nfor improving model performance. To address these concerns, we demonstrate a\nmethodology of pre-training a model on a large, global dataset and then\nfine-tuning that model on data from individual basins. This results in\nperformance increases, validating our hypothesis that there is extra\ninformation to be captured in local data. In particular, we show that\nperformance increases are most significant in watersheds that underperform\nduring global training. We provide a roadmap for national forecasters who wish\nto take ownership of global models using their own data, aiming to lower the\nbarrier to operational deployment of ML-based hydrological forecast systems.", "published": "2025-04-17 01:14:21", "link": "http://arxiv.org/abs/2504.12559v1", "categories": ["cs.LG", "physics.geo-ph"], "primary_category": "cs.LG"}
{"title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric", "abstract": "This paper presents an adversary model and a simulation framework\nspecifically tailored for analyzing attacks on distributed systems composed of\nmultiple distributed protocols, with a focus on assessing the security of\nblockchain networks. Our model classifies and constrains adversarial actions\nbased on the assumptions of the target protocols, defined by failure models,\ncommunication models, and the fault tolerance thresholds of Byzantine Fault\nTolerant (BFT) protocols. The goal is to study not only the intended effects of\nadversarial strategies but also their unintended side effects on critical\nsystem properties. We apply this framework to analyze fairness properties in a\nHyperledger Fabric (HF) blockchain network. Our focus is on novel fairness\nattacks that involve coordinated adversarial actions across various HF\nservices. Simulations show that even a constrained adversary can violate\nfairness with respect to specific clients (client fairness) and impact related\nguarantees (order fairness), which relate the reception order of transactions\nto their final order in the blockchain. This paper significantly extends our\nprevious work by introducing and evaluating a mitigation mechanism specifically\ndesigned to counter transaction reordering attacks. We implement and integrate\nthis defense into our simulation environment, demonstrating its effectiveness\nunder diverse conditions.", "published": "2025-04-17 08:17:27", "link": "http://arxiv.org/abs/2504.12733v1", "categories": ["cs.CR", "cs.DC", "cs.MA"], "primary_category": "cs.CR"}
{"title": "A generalized energy-based modeling framework with application to field/circuit coupled problems", "abstract": "This paper presents a generalized energy-based modeling framework extending\nrecent formulations tailored for differential-algebraic equations. The proposed\nstructure, inspired by the port-Hamiltonian formalism, ensures passivity,\npreserves the power balance, and facilitates the consistent interconnection of\nsubsystems. A particular focus is put on low-frequency power applications in\nelectrical engineering. Stranded, solid, and foil conductor models are\ninvestigated in the context of the eddy current problem. Each conductor model\nis shown to fit into the generalized energy-based structure, which allows their\nstructure-preserving coupling with electrical circuits described by modified\nnodal analysis. Theoretical developments are validated through a numerical\nsimulation of an oscillator circuit, demonstrating energy conservation in\nlossless scenarios and controlled dissipation when eddy currents are present.", "published": "2025-04-17 15:45:20", "link": "http://arxiv.org/abs/2504.13036v1", "categories": ["math.NA", "cs.NA", "35Q61, 65M60, 65L80, 78M10"], "primary_category": "math.NA"}
{"title": "Efficient Chebyshev Reconstruction for the Anisotropic Equilibrium Model in Magnetic Particle Imaging", "abstract": "Magnetic Particle Imaging (MPI) is a tomographic imaging modality capable of\nreal-time, high-sensitivity mapping of superparamagnetic iron oxide\nnanoparticles. Model-based image reconstruction provides an alternative to\nconventional methods that rely on a measured system matrix, eliminating the\nneed for laborious calibration measurements. Nevertheless, model-based\napproaches must account for the complexities of the imaging chain to maintain\nhigh image quality. A recently proposed direct reconstruction method leverages\nweighted Chebyshev polynomials in the frequency domain, removing the need for a\nsimulated system matrix. However, the underlying model neglects key physical\neffects, such as nanoparticle anisotropy, leading to distortions in\nreconstructed images. To mitigate these artifacts, an adapted direct Chebyshev\nreconstruction (DCR) method incorporates a spatially variant deconvolution\nstep, significantly improving reconstruction accuracy at the cost of increased\ncomputational demands. In this work, we evaluate the adapted DCR on six\nexperimental phantoms, demonstrating enhanced reconstruction quality in real\nmeasurements and achieving image fidelity comparable to or exceeding that of\nsimulated system matrix reconstruction. Furthermore, we introduce an efficient\napproximation for the spatially variable deconvolution, reducing both runtime\nand memory consumption while maintaining accuracy. This method achieves\ncomputational complexity of O(N log N ), making it particularly beneficial for\nhigh-resolution and three-dimensional imaging. Our results highlight the\npotential of the adapted DCR approach for improving model-based MPI\nreconstruction in practical applications.", "published": "2025-04-17 14:37:49", "link": "http://arxiv.org/abs/2504.12981v1", "categories": ["physics.med-ph", "cs.NA", "eess.IV", "math.NA"], "primary_category": "physics.med-ph"}
{"title": "Optimal analysis of penalized lowest-order mixed FEMs for the Stokes-Darcy model", "abstract": "This paper is concerned with non-uniform fully-mixed FEMs for dynamic coupled\nStokes-Darcy model with the well-known Beavers-Joseph-Saffman (BJS) interface\ncondition. In particular, a decoupled algorithm with the lowest-order mixed\nnon-uniform FE approximations (MINI for the Stokes equation and RT0-DG0 for the\nDarcy equation) and the classical Nitsche-type penalty is studied. The method\nwith the combined approximation of different orders is commonly used in\npractical simulations. However, the optimal error analysis of methods with\nnon-uniform approximations for the coupled Stokes-Darcy flow model has remained\nchallenging, although the analysis for uniform approximations has been well\ndone. The key question is how the lower-order approximation to the Darcy flow\ninfluences the accuracy of the Stokes solution through the interface condition.\nIn this paper, we prove that the decoupled algorithm provides a truly optimal\nconvergence rate in L^2-norm in spatial direction: O(h^2) for Stokes velocity\nand O(h) for Darcy flow in the coupled Stokes-Darcy model. This implies that\nthe lower-order approximation to the Darcy flow does not pollute the accuracy\nof numerical velocity for Stokes flow. The analysis presented in this paper is\nbased on a well-designed Stokes-Darcy Ritz projection and given for a dynamic\ncoupled model. The optimal error estimate holds for more general combined\napproximations and more general coupled models, including the corresponding\nmodel of steady-state Stokes-Darcy flows and the model of coupled dynamic\nStokes and steady-state Darcy flows. Numerical results confirm our theoretical\nanalysis and show that the decoupled algorithm is efficient.", "published": "2025-04-17 13:37:39", "link": "http://arxiv.org/abs/2504.12938v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Manifold-valued function approximation from multiple tangent spaces", "abstract": "Approximating a manifold-valued function from samples of input-output pairs\nconsists of modeling the relationship between an input from a vector space and\nan output on a Riemannian manifold. We propose a function approximation method\nthat leverages and unifies two prior techniques: (i) approximating a pullback\nto the tangent space, and (ii) the Riemannian moving least squares method. The\ncore idea of the new scheme is to combine pullbacks to multiple tangent spaces\nwith a weighted Fr\\'echet mean. The effectiveness of this approach is\nillustrated with numerical experiments on model problems from parametric model\norder reduction.", "published": "2025-04-17 12:33:30", "link": "http://arxiv.org/abs/2504.12892v1", "categories": ["math.NA", "cs.NA", "65D15, 65D40, 65J99, 46T20, 53B20, 58C25"], "primary_category": "math.NA"}
{"title": "Inverse iteration method for higher eigenvalues of the $p$-Laplacian", "abstract": "We propose a characterization of a $p$-Laplace higher eigenvalue based on the\ninverse iteration method with balancing the Rayleigh quotients of the positive\nand negative parts of solutions to consecutive $p$-Poisson equations. The\napproach relies on the second eigenvalue's minimax properties, but the actual\nlimiting eigenvalue depends on the choice of initial function. The\nwell-posedness and convergence of the iterative scheme are proved. Moreover, we\nprovide the corresponding numerical computations. As auxiliary results, which\nalso have an independent interest, we provide several properties of certain\n$p$-Poisson problems.", "published": "2025-04-17 10:52:42", "link": "http://arxiv.org/abs/2504.12836v1", "categories": ["math.AP", "cs.NA", "math.NA", "math.SP", "35P30, 35J92, 46-08, 47J10, 47J25, 49R05, 65N25"], "primary_category": "math.AP"}
{"title": "Efficient Primal-dual Forward-backward Splitting Method for Wasserstein-like Gradient Flows with General Nonlinear Mobilities", "abstract": "We construct an efficient primal-dual forward-backward (PDFB) splitting\nmethod for computing a class of minimizing movement schemes with nonlinear\nmobility transport distances, and apply it to computing Wasserstein-like\ngradient flows. This approach introduces a novel saddle point formulation for\nthe minimizing movement schemes, leveraging a support function form from the\nBenamou-Brenier dynamical formulation of optimal transport. The resulting\nframework allows for flexible computation of Wasserstein-like gradient flows by\nsolving the corresponding saddle point problem at the fully discrete level, and\ncan be easily extended to handle general nonlinear mobilities. We also provide\na detailed convergence analysis of the PDFB splitting method, along with\npractical remarks on its implementation and application. The effectiveness of\nthe method is demonstrated through several challenging numerical examples.", "published": "2025-04-17 07:37:08", "link": "http://arxiv.org/abs/2504.12713v1", "categories": ["math.NA", "cs.NA", "math.OC", "35A15, 47J25, 47J35, 49M29, 65K10, 76M30"], "primary_category": "math.NA"}
{"title": "Tangent Space Parametrization for Stochastic Differential Equations on SO(n)", "abstract": "In this paper, we study the numerical simulation of stochastic differential\nequations (SDEs) on the special orthogonal Lie group $\\text{SO}(n)$. We propose\na geometry-preserving numerical scheme based on the stochastic tangent space\nparametrization (S-TaSP) method for state-dependent multiplicative SDEs on\n$\\text{SO}(n)$. The convergence analysis of the S-TaSP scheme establishes a\nstrong convergence order of $\\mathcal{O}(\\delta^{\\frac{1-\\epsilon}{2}})$, which\nmatches the convergence order of the previous stochastic Lie Euler-Maruyama\nscheme while avoiding the computational cost of the exponential map. Numerical\nsimulation illustrates the theoretical results.", "published": "2025-04-17 05:22:04", "link": "http://arxiv.org/abs/2504.12650v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Geometry-preserving Numerical Scheme for Riemannian Stochastic Differential Equations", "abstract": "Stochastic differential equations (SDEs) on Riemannian manifolds have\nnumerous applications in system identification and control. However,\ngeometry-preserving numerical methods for simulating Riemannian SDEs remain\nrelatively underdeveloped. In this paper, we propose the Exponential\nEuler-Maruyama (Exp-EM) scheme for approximating solutions of SDEs on\nRiemannian manifolds. The Exp-EM scheme is both geometry-preserving and\ncomputationally tractable. We establish a strong convergence rate of\n$\\mathcal{O}(\\delta^{\\frac{1 - \\epsilon}{2}})$ for the Exp-EM scheme, which\nextends previous results obtained for specific manifolds to a more general\nsetting. Numerical simulations are provided to illustrate our theoretical\nfindings.", "published": "2025-04-17 04:14:00", "link": "http://arxiv.org/abs/2504.12631v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "The existence of explicit symplectic integrators for general nonseparable Hamiltonian systems", "abstract": "The existence of explicit symplectic integrators for general nonseparable\nHamiltonian systems is an open and important problem in both numerical analysis\nand computing in science and engineering, as explicit integrators are usually\nmore efficient than the implicit integrators of the same order of accuracy. Up\nto now, all responses to this problem are negative. That is, there exist\nexplicit symplectic integrators only for some special nonseparable Hamiltonian\nsystems, whereas the universal design involving explicit symplectic integrators\nfor general nonseparable Hamiltonian systems has not yet been studied\nsufficiently. In this paper, we present a constructive proof for the existence\nof explicit symplectic integrators for general nonseparable Hamiltonian systems\nvia finding explicit symplectic mappings under which the special submanifold of\nthe extended phase space is invariant. It turns out that the proposed explicit\nintegrators are symplectic in both the extended phase space and the original\nphase space. Moreover, on the basis of the global modified Hamiltonians of the\nproposed integrators, the backward error analysis is made via a parameter\nrelaxation and restriction technique to show the linear growth of global errors\nand the near-preservation of first integrals. In particular, the effective\nestimated time interval is nearly the same as classical implicit symplectic\nintegrators when applied to (near-) integrable Hamiltonian systems. Numerical\nexperiments with a completely integrable nonseparable Hamiltonian and a\nnonintegrable nonseparable Hamiltonian illustrate the good long-term behavior\nand high efficiency of the explicit symplectic integrators proposed and\nanalyzed in this paper.", "published": "2025-04-17 01:32:43", "link": "http://arxiv.org/abs/2504.12567v1", "categories": ["math.NA", "cs.NA", "65P10, 37M15"], "primary_category": "math.NA"}
{"title": "Symmetry classification and invariant solutions of the classical geometric mean reversion process", "abstract": "Based on the Lie symmetry method, we investigate a Feynman-Kac formula for\nthe classical geometric mean reversion process, which effectively describing\nthe dynamics of short-term interest rates. The Lie algebra of infinitesimal\nsymmetries and the corresponding one-parameter symmetry groups of the equation\nare obtained. An optimal system of invariant solutions are constructed by a\nderived optimal system of one-dimensional subalgebras. Because of taking into\naccount a supply response to price rises, this equation provides for a more\nrealistic assumption than the geometric Brownian motion in many investment\nscenarios.", "published": "2025-04-17 16:59:55", "link": "http://arxiv.org/abs/2504.13094v1", "categories": ["math.DS", "math.AP", "math.PR", "q-fin.MF"], "primary_category": "math.DS"}
{"title": "Optimal Capital Structure for Life Insurance Companies Offering Surplus Participation", "abstract": "We adapt Leland's dynamic capital structure model to the context of an\ninsurance company selling participating life insurance contracts explaining the\nexistence of life insurance contracts which provide both a guaranteed payment\nand surplus participation to the policyholders. Our derivation of the optimal\nparticipation rate reveals its pronounced sensitivity to the contract duration\nand the associated tax rate. Moreover, the asset substitution effect, which\ndescribes the tendency of equity holders to increase the riskiness of a\ncompany's investment decisions, decreases when adding surplus participation.", "published": "2025-04-17 11:19:17", "link": "http://arxiv.org/abs/2504.12851v1", "categories": ["q-fin.MF", "90B50, 91B06, 91B50, 91G05, 91G10, 91G50"], "primary_category": "q-fin.MF"}
{"title": "Classification-Based Analysis of Price Pattern Differences Between Cryptocurrencies and Stocks", "abstract": "Cryptocurrencies are digital tokens built on blockchain technology, with\nthousands actively traded on centralized exchanges (CEXs). Unlike stocks, which\nare backed by real businesses, cryptocurrencies are recognized as a distinct\nclass of assets by researchers. How do investors treat this new category of\nasset in trading? Are they similar to stocks as an investment tool for\ninvestors? We answer these questions by investigating cryptocurrencies' and\nstocks' price time series which can reflect investors' attitudes towards the\ntargeted assets. Concretely, we use different machine learning models to\nclassify cryptocurrencies' and stocks' price time series in the same period and\nget an extremely high accuracy rate, which reflects that cryptocurrency\ninvestors behave differently in trading from stock investors. We then extract\nfeatures from these price time series to explain the price pattern difference,\nincluding mean, variance, maximum, minimum, kurtosis, skewness, and first to\nthird-order autocorrelation, etc., and then use machine learning methods\nincluding logistic regression (LR), random forest (RF), support vector machine\n(SVM), etc. for classification. The classification results show that these\nextracted features can help to explain the price time series pattern difference\nbetween cryptocurrencies and stocks.", "published": "2025-04-17 09:12:27", "link": "http://arxiv.org/abs/2504.12771v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
<<<<<<< HEAD
{"title": "Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time", "abstract": "We study the approximation gap between the dynamics of a polynomial-width\nneural network and its infinite-width counterpart, both trained using projected\ngradient descent in the mean-field scaling regime. We demonstrate how to\ntightly bound this approximation gap through a differential equation governed\nby the mean-field dynamics. A key factor influencing the growth of this ODE is\nthe local Hessian of each particle, defined as the derivative of the particle's\nvelocity in the mean-field dynamics with respect to its position. We apply our\nresults to the canonical feature learning problem of estimating a\nwell-specified single-index model; we permit the information exponent to be\narbitrarily large, leading to convergence times that grow polynomially in the\nambient dimension $d$. We show that, due to a certain ``self-concordance''\nproperty in these problems -- where the local Hessian of a particle is bounded\nby a constant times the particle's velocity -- polynomially many neurons are\nsufficient to closely approximate the mean-field dynamics throughout training.", "published": "2025-04-17 17:24:38", "link": "http://arxiv.org/abs/2504.13110v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research", "abstract": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.", "published": "2025-04-17 17:10:33", "link": "http://arxiv.org/abs/2504.13101v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Variance-Reduced Fast Operator Splitting Methods for Stochastic Generalized Equations", "abstract": "We develop two classes of variance-reduced fast operator splitting methods to\napproximate solutions of both finite-sum and stochastic generalized equations.\nOur approach integrates recent advances in accelerated fixed-point methods,\nco-hypomonotonicity, and variance reduction. First, we introduce a class of\nvariance-reduced estimators and establish their variance-reduction bounds. This\nclass covers both unbiased and biased instances and comprises common estimators\nas special cases, including SVRG, SAGA, SARAH, and Hybrid-SGD. Next, we design\na novel accelerated variance-reduced forward-backward splitting (FBS) algorithm\nusing these estimators to solve finite-sum and stochastic generalized\nequations. Our method achieves both $\\mathcal{O}(1/k^2)$ and $o(1/k^2)$\nconvergence rates on the expected squared norm $\\mathbb{E}[ \\|\nG_{\\lambda}x^k\\|^2]$ of the FBS residual $G_{\\lambda}$, where $k$ is the\niteration counter. Additionally, we establish, for the first time, almost sure\nconvergence rates and almost sure convergence of iterates to a solution in\nstochastic accelerated methods. Unlike existing stochastic fixed-point\nalgorithms, our methods accommodate co-hypomonotone operators, which\npotentially include nonmonotone problems arising from recent applications. We\nfurther specify our method to derive an appropriate variant for each stochastic\nestimator -- SVRG, SAGA, SARAH, and Hybrid-SGD -- demonstrating that they\nachieve the best-known complexity for each without relying on enhancement\ntechniques. Alternatively, we propose an accelerated variance-reduced\nbackward-forward splitting (BFS) method, which attains similar convergence\nrates and oracle complexity as our FBS method. Finally, we validate our results\nthrough several numerical experiments and compare their performance.", "published": "2025-04-17 16:02:20", "link": "http://arxiv.org/abs/2504.13046v1", "categories": ["math.OC", "stat.ML", "90C25, 90C06, 90-08"], "primary_category": "math.OC"}
{"title": "Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set of Experts", "abstract": "Learning-to-Defer (L2D) enables decision-making systems to improve\nreliability by selectively deferring uncertain predictions to more competent\nagents. However, most existing approaches focus exclusively on single-agent\ndeferral, which is often inadequate in high-stakes scenarios that require\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\nthe classical two-stage L2D framework that allocates each query to the $k$ most\nconfident agents instead of a single one. To further enhance flexibility and\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\nextension that learns the optimal number of agents to consult for each query,\nbased on input complexity, agent competency distributions, and consultation\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\nBayes-consistent and $(\\mathcal{R}, \\mathcal{G})$-consistent, ensuring\nconvergence to the Bayes-optimal allocation. Notably, we show that the\nwell-established model cascades paradigm arises as a restricted instance of our\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\nbenchmarks demonstrate the effectiveness of our framework on both\nclassification and regression tasks.", "published": "2025-04-17 14:50:40", "link": "http://arxiv.org/abs/2504.12988v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When do Random Forests work?", "abstract": "We study the effectiveness of randomizing split-directions in random forests.\nPrior literature has shown that, on the one hand, randomization can reduce\nvariance through decorrelation, and, on the other hand, randomization\nregularizes and works in low signal-to-noise ratio (SNR) environments. First,\nwe bring together and revisit decorrelation and regularization by presenting a\nsystematic analysis of out-of-sample mean-squared error (MSE) for different SNR\nscenarios based on commonly-used data-generating processes. We find that\nvariance reduction tends to increase with the SNR and forests outperform\nbagging when the SNR is low because, in low SNR cases, variance dominates bias\nfor both methods. Second, we show that the effectiveness of randomization is a\nquestion that goes beyond the SNR. We present a simulation study with fixed and\nmoderate SNR, in which we examine the effectiveness of randomization for other\ndata characteristics. In particular, we find that (i) randomization can\nincrease bias in the presence of fat tails in the distribution of covariates;\n(ii) in the presence of irrelevant covariates randomization is ineffective\nbecause bias dominates variance; and (iii) when covariates are mutually\ncorrelated randomization tends to be effective because variance dominates bias.\nBeyond randomization, we find that, for both bagging and random forests, bias\ncan be significantly reduced in the presence of correlated covariates. This\nlast finding goes beyond the prevailing view that averaging mostly works by\nvariance reduction. Given that in practice covariates are often correlated, our\nfindings on correlated covariates could open the way for a better understanding\nof why random forests work well in many applications.", "published": "2025-04-17 11:38:17", "link": "http://arxiv.org/abs/2504.12860v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "ALT: A Python Package for Lightweight Feature Representation in Time Series Classification", "abstract": "We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.", "published": "2025-04-17 10:57:29", "link": "http://arxiv.org/abs/2504.12841v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MS", "stat.ML", "62M10, 62H30, 68T05, 68T10", "I.5.1; I.2.6; G.3; D.2.13"], "primary_category": "cs.LG"}
{"title": "Cluster weighted models with multivariate skewed distributions for functional data", "abstract": "We propose a clustering method, funWeightClustSkew, based on mixtures of\nfunctional linear regression models and three skewed multivariate\ndistributions: the variance-gamma distribution, the skew-t distribution, and\nthe normal-inverse Gaussian distribution. Our approach follows the framework of\nthe functional high dimensional data clustering (funHDDC) method, and we extend\nto functional data the cluster weighted models based on skewed distributions\nused for finite dimensional multivariate data. We consider several parsimonious\nmodels, and to estimate the parameters we construct an expectation maximization\n(EM) algorithm. We illustrate the performance of funWeightClustSkew for\nsimulated data and for the Air Quality dataset.", "published": "2025-04-17 06:17:06", "link": "http://arxiv.org/abs/2504.12683v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Spectral Algorithms under Covariate Shift", "abstract": "Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under distribution shifts, specifically within the\nframework of reproducing kernel Hilbert spaces. Our study focuses on the case\nof covariate shift. In this scenario, the marginal distributions of the input\ndata differ between the training and test datasets, while the conditional\ndistribution of the output given the input remains unchanged. Under this\nsetting, we analyze the generalization error of spectral algorithms and show\nthat they achieve minimax optimality when the density ratios between the\ntraining and test distributions are uniformly bounded. However, we also\nidentify a critical limitation: when the density ratios are unbounded, the\nspectral algorithms may become suboptimal. To address this limitation, we\npropose a weighted spectral algorithm that incorporates density ratio\ninformation into the learning process. Our theoretical analysis shows that this\nweighted approach achieves optimal capacity-independent convergence rates.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm can approach the\noptimal capacity-dependent convergence rates arbitrarily closely. This\nimprovement resolves the suboptimality issue in unbounded density ratio\nscenarios and advances the state-of-the-art by refining existing theoretical\nresults.", "published": "2025-04-17 04:02:06", "link": "http://arxiv.org/abs/2504.12625v1", "categories": ["stat.ML", "cs.LG", "68Q32, 68T05, 62J02"], "primary_category": "stat.ML"}
{"title": "Bayesian Density-Density Regression with Application to Cell-Cell Communications", "abstract": "We introduce a scalable framework for regressing multivariate distributions\nonto multivariate distributions, motivated by the application of inferring\ncell-cell communication from population-scale single-cell data. The observed\ndata consist of pairs of multivariate distributions for ligands from one cell\ntype and corresponding receptors from another. For each ordered pair $e=(l,r)$\nof cell types $(l \\neq r)$ and each sample $i = 1, \\ldots, n$, we observe a\npair of distributions $(F_{ei}, G_{ei})$ of gene expressions for ligands and\nreceptors of cell types $l$ and $r$, respectively. The aim is to set up a\nregression of receptor distributions $G_{ei}$ given ligand distributions\n$F_{ei}$. A key challenge is that these distributions reside in distinct spaces\nof differing dimensions. We formulate the regression of multivariate densities\non multivariate densities using a generalized Bayes framework with the sliced\nWasserstein distance between fitted and observed distributions. Finally, we use\ninference under such regressions to define a directed graph for cell-cell\ncommunications.", "published": "2025-04-17 03:46:03", "link": "http://arxiv.org/abs/2504.12617v1", "categories": ["stat.ME", "stat.AP", "stat.CO", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Meta-Dependence in Conditional Independence Testing", "abstract": "Constraint-based causal discovery algorithms utilize many statistical tests\nfor conditional independence to uncover networks of causal dependencies. These\napproaches to causal discovery rely on an assumed correspondence between the\ngraphical properties of a causal structure and the conditional independence\nproperties of observed variables, known as the causal Markov condition and\nfaithfulness. Finite data yields an empirical distribution that is \"close\" to\nthe actual distribution. Across these many possible empirical distributions,\nthe correspondence to the graphical properties can break down for different\nconditional independencies, and multiple violations can occur at the same time.\nWe study this \"meta-dependence\" between conditional independence properties\nusing the following geometric intuition: each conditional independence property\nconstrains the space of possible joint distributions to a manifold. The\n\"meta-dependence\" between conditional independences is informed by the position\nof these manifolds relative to the true probability distribution. We provide a\nsimple-to-compute measure of this meta-dependence using information projections\nand consolidate our findings empirically using both synthetic and real-world\ndata.", "published": "2025-04-17 02:41:22", "link": "http://arxiv.org/abs/2504.12594v1", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition", "abstract": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.", "published": "2025-04-17 17:11:32", "link": "http://arxiv.org/abs/2504.13102v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can Masked Autoencoders Also Listen to Birds?", "abstract": "Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\nfine-grained acoustic characteristics of specialized domains such as\nbioacoustic monitoring. Bird sound classification is critical for assessing\nenvironmental health, yet general-purpose models inadequately address its\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\ndownstream tasks, substantially improving multi-label classification\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\nwe propose prototypical probing, a parameter-efficient method for leveraging\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\nprobing by up to 37\\% in MAP and narrow the gap to fine-tuning to approximately\n3\\% on average on BirdSet.", "published": "2025-04-17 12:13:25", "link": "http://arxiv.org/abs/2504.12880v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CST-former: Multidimensional Attention-based Transformer for Sound Event Localization and Detection in Real Scenes", "abstract": "Sound event localization and detection (SELD) is a task for the\nclassification of sound events and the identification of direction of arrival\n(DoA) utilizing multichannel acoustic signals. For effective classification and\nlocalization, a channel-spectro-temporal transformer (CST-former) was\nsuggested. CST-former employs multidimensional attention mechanisms across the\nspatial, spectral, and temporal domains to enlarge the model's capacity to\nlearn the domain information essential for event detection and DoA estimation\nover time. In this work, we present an enhanced version of CST-former with\nmultiscale unfolded local embedding (MSULE) developed to capture and aggregate\ndomain information over multiple time-frequency scales. Also, we propose\nfinetuning and post-processing techniques beneficial for conducting the SELD\ntask over limited training datasets. In-depth ablation studies of the proposed\narchitecture and detailed analysis on the proposed modules are carried out to\nvalidate the efficacy of multidimensional attentions on the SELD task.\nEmpirical validation through experimentation on STARSS22 and STARSS23 datasets\ndemonstrates the remarkable performance of CST-former and post-processing\ntechniques without using external data.", "published": "2025-04-17 11:56:13", "link": "http://arxiv.org/abs/2504.12870v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "abstract": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.", "published": "2025-04-17 11:50:04", "link": "http://arxiv.org/abs/2504.12867v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "A Survey on Cross-Modal Interaction Between Music and Multimodal Data", "abstract": "Multimodal learning has driven innovation across various industries,\nparticularly in the field of music. By enabling more intuitive interaction\nexperiences and enhancing immersion, it not only lowers the entry barriers to\nthe music but also increases its overall appeal. This survey aims to provide a\ncomprehensive review of multimodal tasks related to music, outlining how music\ncontributes to multimodal learning and offering insights for researchers\nseeking to expand the boundaries of computational music. Unlike text and\nimages, which are often semantically or visually intuitive, music primarily\ninteracts with humans through auditory perception, making its data\nrepresentation inherently less intuitive. Therefore, this paper first\nintroduces the representations of music and provides an overview of music\ndatasets. Subsequently, we categorize cross-modal interactions between music\nand multimodal data into three types: music-driven cross-modal interactions,\nmusic-oriented cross-modal interactions, and bidirectional music cross-modal\ninteractions. For each category, we systematically trace the development of\nrelevant sub-tasks, analyze existing limitations, and discuss emerging trends.\nFurthermore, we provide a comprehensive summary of datasets and evaluation\nmetrics used in multimodal tasks related to music, offering benchmark\nreferences for future research. Finally, we discuss the current challenges in\ncross-modal interactions involving music and propose potential directions for\nfuture research.", "published": "2025-04-17 09:58:38", "link": "http://arxiv.org/abs/2504.12796v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Temporal Attention Pooling for Frequency Dynamic Convolution in Sound Event Detection", "abstract": "Recent advances in deep learning, particularly frequency dynamic convolution\n(FDY conv), have significantly improved sound event detection (SED) by enabling\nfrequency-adaptive feature extraction. However, FDY conv relies on temporal\naverage pooling, which treats all temporal frames equally, limiting its ability\nto capture transient sound events such as alarm bells, door knocks, and speech\nplosives. To address this limitation, we propose temporal attention pooling\nfrequency dynamic convolution (TFD conv) to replace temporal average pooling\nwith temporal attention pooling (TAP). TAP adaptively weights temporal features\nthrough three complementary mechanisms: time attention pooling (TA) for\nemphasizing salient features, velocity attention pooling (VA) for capturing\ntransient changes, and conventional average pooling for robustness to\nstationary signals. Ablation studies show that TFD conv improves average PSDS1\nby 3.02% over FDY conv with only a 14.8% increase in parameter count. Classwise\nANOVA and Tukey HSD analysis further demonstrate that TFD conv significantly\nenhances detection performance for transient-heavy events, outperforming\nexisting FDY conv models. Notably, TFD conv achieves a maximum PSDS1 score of\n0.456, surpassing previous state-of-the-art SED systems. We also explore the\ncompatibility of TAP with other FDY conv variants, including dilated FDY conv\n(DFD conv), partial FDY conv (PFD conv), and multi-dilated FDY conv (MDFD\nconv). Among these, the integration of TAP with MDFD conv achieves the best\nresult with a PSDS1 score of 0.459, validating the complementary strengths of\ntemporal attention and multi-scale frequency adaptation. These findings\nestablish TFD conv as a powerful and generalizable framework for enhancing both\ntransient sensitivity and overall feature robustness in SED.", "published": "2025-04-17 06:03:43", "link": "http://arxiv.org/abs/2504.12670v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Degrees of Freedom of Holographic MIMO -- Fundamental Theory and Analytical Methods", "abstract": "Holographic multiple-input multiple-output (MIMO) is envisioned as one of the\nmost promising technology enablers for future sixth-generation (6G) networks.\nThe use of electrically large holographic surface (HoloS) antennas has the\npotential to significantly boost the spatial multiplexing gain by increasing\nthe number of degrees of freedom (DoF), even in line-of-sight (LoS) channels.\nIn this context, the research community has shown a growing interest in\ncharacterizing the fundamental limits of this technology. In this paper, we\ncompare the two analytical methods commonly utilized in the literature for this\npurpose: the cut-set integral and the self-adjoint operator. We provide a\ndetailed description of both methods and discuss their advantages and\nlimitations.", "published": "2025-04-17 15:41:28", "link": "http://arxiv.org/abs/2504.13031v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
=======
{"title": "Variance-Reduced Fast Operator Splitting Methods for Stochastic Generalized Equations", "abstract": "We develop two classes of variance-reduced fast operator splitting methods to\napproximate solutions of both finite-sum and stochastic generalized equations.\nOur approach integrates recent advances in accelerated fixed-point methods,\nco-hypomonotonicity, and variance reduction. First, we introduce a class of\nvariance-reduced estimators and establish their variance-reduction bounds. This\nclass covers both unbiased and biased instances and comprises common estimators\nas special cases, including SVRG, SAGA, SARAH, and Hybrid-SGD. Next, we design\na novel accelerated variance-reduced forward-backward splitting (FBS) algorithm\nusing these estimators to solve finite-sum and stochastic generalized\nequations. Our method achieves both $\\mathcal{O}(1/k^2)$ and $o(1/k^2)$\nconvergence rates on the expected squared norm $\\mathbb{E}[ \\|\nG_{\\lambda}x^k\\|^2]$ of the FBS residual $G_{\\lambda}$, where $k$ is the\niteration counter. Additionally, we establish, for the first time, almost sure\nconvergence rates and almost sure convergence of iterates to a solution in\nstochastic accelerated methods. Unlike existing stochastic fixed-point\nalgorithms, our methods accommodate co-hypomonotone operators, which\npotentially include nonmonotone problems arising from recent applications. We\nfurther specify our method to derive an appropriate variant for each stochastic\nestimator -- SVRG, SAGA, SARAH, and Hybrid-SGD -- demonstrating that they\nachieve the best-known complexity for each without relying on enhancement\ntechniques. Alternatively, we propose an accelerated variance-reduced\nbackward-forward splitting (BFS) method, which attains similar convergence\nrates and oracle complexity as our FBS method. Finally, we validate our results\nthrough several numerical experiments and compare their performance.", "published": "2025-04-17 16:02:20", "link": "http://arxiv.org/abs/2504.13046v1", "categories": ["math.OC", "stat.ML", "90C25, 90C06, 90-08"], "primary_category": "math.OC"}
{"title": "Bayesian Density-Density Regression with Application to Cell-Cell Communications", "abstract": "We introduce a scalable framework for regressing multivariate distributions\nonto multivariate distributions, motivated by the application of inferring\ncell-cell communication from population-scale single-cell data. The observed\ndata consist of pairs of multivariate distributions for ligands from one cell\ntype and corresponding receptors from another. For each ordered pair $e=(l,r)$\nof cell types $(l \\neq r)$ and each sample $i = 1, \\ldots, n$, we observe a\npair of distributions $(F_{ei}, G_{ei})$ of gene expressions for ligands and\nreceptors of cell types $l$ and $r$, respectively. The aim is to set up a\nregression of receptor distributions $G_{ei}$ given ligand distributions\n$F_{ei}$. A key challenge is that these distributions reside in distinct spaces\nof differing dimensions. We formulate the regression of multivariate densities\non multivariate densities using a generalized Bayes framework with the sliced\nWasserstein distance between fitted and observed distributions. Finally, we use\ninference under such regressions to define a directed graph for cell-cell\ncommunications.", "published": "2025-04-17 03:46:03", "link": "http://arxiv.org/abs/2504.12617v1", "categories": ["stat.ME", "stat.AP", "stat.CO", "stat.ML"], "primary_category": "stat.ME"}
{"title": "CST-former: Multidimensional Attention-based Transformer for Sound Event Localization and Detection in Real Scenes", "abstract": "Sound event localization and detection (SELD) is a task for the\nclassification of sound events and the identification of direction of arrival\n(DoA) utilizing multichannel acoustic signals. For effective classification and\nlocalization, a channel-spectro-temporal transformer (CST-former) was\nsuggested. CST-former employs multidimensional attention mechanisms across the\nspatial, spectral, and temporal domains to enlarge the model's capacity to\nlearn the domain information essential for event detection and DoA estimation\nover time. In this work, we present an enhanced version of CST-former with\nmultiscale unfolded local embedding (MSULE) developed to capture and aggregate\ndomain information over multiple time-frequency scales. Also, we propose\nfinetuning and post-processing techniques beneficial for conducting the SELD\ntask over limited training datasets. In-depth ablation studies of the proposed\narchitecture and detailed analysis on the proposed modules are carried out to\nvalidate the efficacy of multidimensional attentions on the SELD task.\nEmpirical validation through experimentation on STARSS22 and STARSS23 datasets\ndemonstrates the remarkable performance of CST-former and post-processing\ntechniques without using external data.", "published": "2025-04-17 11:56:13", "link": "http://arxiv.org/abs/2504.12870v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "A Survey on Cross-Modal Interaction Between Music and Multimodal Data", "abstract": "Multimodal learning has driven innovation across various industries,\nparticularly in the field of music. By enabling more intuitive interaction\nexperiences and enhancing immersion, it not only lowers the entry barriers to\nthe music but also increases its overall appeal. This survey aims to provide a\ncomprehensive review of multimodal tasks related to music, outlining how music\ncontributes to multimodal learning and offering insights for researchers\nseeking to expand the boundaries of computational music. Unlike text and\nimages, which are often semantically or visually intuitive, music primarily\ninteracts with humans through auditory perception, making its data\nrepresentation inherently less intuitive. Therefore, this paper first\nintroduces the representations of music and provides an overview of music\ndatasets. Subsequently, we categorize cross-modal interactions between music\nand multimodal data into three types: music-driven cross-modal interactions,\nmusic-oriented cross-modal interactions, and bidirectional music cross-modal\ninteractions. For each category, we systematically trace the development of\nrelevant sub-tasks, analyze existing limitations, and discuss emerging trends.\nFurthermore, we provide a comprehensive summary of datasets and evaluation\nmetrics used in multimodal tasks related to music, offering benchmark\nreferences for future research. Finally, we discuss the current challenges in\ncross-modal interactions involving music and propose potential directions for\nfuture research.", "published": "2025-04-17 09:58:38", "link": "http://arxiv.org/abs/2504.12796v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Temporal Attention Pooling for Frequency Dynamic Convolution in Sound Event Detection", "abstract": "Recent advances in deep learning, particularly frequency dynamic convolution\n(FDY conv), have significantly improved sound event detection (SED) by enabling\nfrequency-adaptive feature extraction. However, FDY conv relies on temporal\naverage pooling, which treats all temporal frames equally, limiting its ability\nto capture transient sound events such as alarm bells, door knocks, and speech\nplosives. To address this limitation, we propose temporal attention pooling\nfrequency dynamic convolution (TFD conv) to replace temporal average pooling\nwith temporal attention pooling (TAP). TAP adaptively weights temporal features\nthrough three complementary mechanisms: time attention pooling (TA) for\nemphasizing salient features, velocity attention pooling (VA) for capturing\ntransient changes, and conventional average pooling for robustness to\nstationary signals. Ablation studies show that TFD conv improves average PSDS1\nby 3.02% over FDY conv with only a 14.8% increase in parameter count. Classwise\nANOVA and Tukey HSD analysis further demonstrate that TFD conv significantly\nenhances detection performance for transient-heavy events, outperforming\nexisting FDY conv models. Notably, TFD conv achieves a maximum PSDS1 score of\n0.456, surpassing previous state-of-the-art SED systems. We also explore the\ncompatibility of TAP with other FDY conv variants, including dilated FDY conv\n(DFD conv), partial FDY conv (PFD conv), and multi-dilated FDY conv (MDFD\nconv). Among these, the integration of TAP with MDFD conv achieves the best\nresult with a PSDS1 score of 0.459, validating the complementary strengths of\ntemporal attention and multi-scale frequency adaptation. These findings\nestablish TFD conv as a powerful and generalizable framework for enhancing both\ntransient sensitivity and overall feature robustness in SED.", "published": "2025-04-17 06:03:43", "link": "http://arxiv.org/abs/2504.12670v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
>>>>>>> 33f049834fead0e168638dee581a3663e744788f
{"title": "ORIS allocation to minimize the outage probability in a multi-user VLC scenario", "abstract": "Visible Light Communication (VLC) is a promising solution to address the\ngrowing demand for wireless data, leveraging the widespread use of\nlight-emitting diodes (LEDs) as transmitters. However, its deployment is\nchallenged by link blockages that cause connectivity outages. Optical\nreconfigurable intelligent surfaces (ORISs) have recently emerged as a solution\nto mitigate these disruptions. This work considers a multi-user VLC system and\ninvestigates the optimal association of ORISs to LEDs and users to minimize the\noutage probability while limiting the number of ORISs used. Numerical results\nfrom our proposed optimization algorithm demonstrate that using ORISs can\nreduce the outage probability by up to 85% compared to a no-ORIS scenario.", "published": "2025-04-17 15:24:25", "link": "http://arxiv.org/abs/2504.13016v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Simultaneous Polysomnography and Cardiotocography Reveal Temporal Correlation Between Maternal Obstructive Sleep Apnea and Fetal Hypoxia", "abstract": "Background: Obstructive sleep apnea syndrome (OSAS) during pregnancy is\ncommon and can negatively affect fetal outcomes. However, studies on the\nimmediate effects of maternal hypoxia on fetal heart rate (FHR) changes are\nlacking. Methods: We used time-synchronized polysomnography (PSG) and\ncardiotocography (CTG) data from two cohorts to analyze the correlation between\nmaternal hypoxia and FHR changes (accelerations or decelerations). Maternal\nhypoxic event characteristics were analyzed using generalized linear modeling\n(GLM) to assess their associations with different FHR changes. Results: A total\nof 118 pregnant women participated. FHR changes were significantly associated\nwith maternal hypoxia, primarily characterized by accelerations. A longer\nhypoxic duration correlated with more significant FHR accelerations (P < 0.05),\nwhile prolonged hypoxia and greater SpO2 drop were linked to FHR decelerations\n(P < 0.05). Both cohorts showed a transient increase in FHR during maternal\nhypoxia, which returned to baseline after the event resolved. Conclusion:\nMaternal hypoxia significantly affects FHR, suggesting that maternal OSAS may\ncontribute to fetal hypoxia. These findings highlight the importance of\nmaternal-fetal interactions and provide insights for future interventions.", "published": "2025-04-17 15:17:14", "link": "http://arxiv.org/abs/2504.13010v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Optic Fingerprint(OFP): Enhancing Security in Li-Fi Networks", "abstract": "We present a hardware-integrated security framework for LiFi networks through\ndevice fingerprint extraction within the IEEE 802.15.7 protocol. Our Optic\nFingerprint (OFP) model utilizes inherent LED nonlinearities to generate\namplitude-based feature vectors in time and frequency domains, specifically\ndesigned for optical wireless systems. Experimental results with 39 commercial\nLEDs demonstrate 90.36% classification accuracy across SNR 10-30 dB while\nmaintaining standard compliance, offering a practical physical-layer\nauthentication solution for visible light communication.", "published": "2025-04-17 14:01:02", "link": "http://arxiv.org/abs/2504.12956v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "RIS-Assisted Beamfocusing in Near-Field IoT Communication Systems: A Transformer-Based Approach", "abstract": "The massive number of antennas in extremely large aperture array (ELAA)\nsystems shifts the propagation regime of signals in internet of things (IoT)\ncommunication systems towards near-field spherical wave propagation. We propose\na reconfigurable intelligent surfaces (RIS)-assisted beamfocusing mechanism,\nwhere the design of the two-dimensional beam codebook that contains both the\nangular and distance domains is challenging. To address this issue, we\nintroduce a novel Transformer-based two-stage beam training algorithm, which\nincludes the coarse and fine search phases. The proposed mechanism provides a\nfine-grained codebook with enhanced spatial resolution, enabling precise\nbeamfocusing. Specifically, in the first stage, the beam training is performed\nto estimate the approximate location of the device by using a simple codebook,\ndetermining whether it is within the beamfocusing range (BFR) or the\nnone-beamfocusing range (NBFR). In the second stage, by using a more precise\ncodebook, a fine-grained beam search strategy is conducted. Experimental\nresults unveil that the precision of the RIS-assisted beamfocusing is greatly\nimproved. The proposed method achieves beam selection accuracy up to 97% at\nsignal-to-noise ratio (SNR) of 20 dB, and improves 10% to 50% over the baseline\nmethod at different SNRs.", "published": "2025-04-17 12:29:10", "link": "http://arxiv.org/abs/2504.12889v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
<<<<<<< HEAD
{"title": "Optimizing Movable Antennas in Wideband Multi-User MIMO With Hardware Impairments", "abstract": "Movable antennas represent an emerging field in telecommunication research\nand a potential approach to achieving higher data rates in multiple-input\nmultiple-output (MIMO) communications when the total number of antennas is\nlimited. Most solutions and analyses to date have been limited to\n\\emph{narrowband} setups. This work complements the prior studies by\nquantifying the benefit of using movable antennas in \\emph{wideband} MIMO\ncommunication systems. First, we derive a novel uplink wideband system model\nthat also accounts for distortion from transceiver hardware impairments. We\nthen formulate and solve an optimization task to maximize the average sum rate\nby adjusting the antenna positions using particle swarm optimization. Finally,\nthe performance with movable antennas is compared with fixed uniform arrays and\nthe derived theoretical upper bound. The numerical study concludes that the\ndata rate improvement from movable antennas over other arrays heavily depends\non the level of hardware impairments, the richness of the multi-path\nenvironments, and the number of subcarriers. The present study provides vital\ninsights into the most suitable use cases for movable antennas in future\nwideband systems.", "published": "2025-04-17 12:20:46", "link": "http://arxiv.org/abs/2504.12885v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based on 3D Conditional GAN", "abstract": "The advancement of advanced air mobility (AAM) in recent years has given rise\nto the concept of low-altitude economy (LAE). However, the diverse flight\nactivities associated with the emerging LAE applications in urban scenarios\nconfront complex physical environments, which urgently necessitates ubiquitous\nand reliable communication to guarantee the operation safety of the\nlow-altitude aircraft. As one of promising technologies for the sixth\ngeneration (6G) mobile networks, channel knowledge map (CKM) enables the\nenvironment-aware communication by constructing a site-specific dataset,\nthereby providing a priori on-site information for the aircraft to obtain the\nchannel state information (CSI) at arbitrary locations with much reduced online\noverhead. Diverse base station (BS) deployments in the three-dimensional (3D)\nurban low-altitude environment require efficient 3D CKM construction to capture\nspatial channel characteristics with less overhead. Towards this end, this\npaper proposes a 3D channel gain map (CGM) inference method based on a 3D\nconditional generative adversarial network (3D-CGAN). Specifically, we first\nanalyze the potential deployment types of BSs in urban low-altitude scenario,\nand investigate the CGM representation with the corresponding 3D channel gain\nmodel. The framework of the proposed 3D-CGAN is then discussed, which is\ntrained by a dataset consisting of existing CGMs. Consequently, the trained\n3D-CGAN is capable of inferring the corresponding CGM only based on the BS\ncoordinate without additional measurement. The simulation results demonstrate\nthat the CGMs inferred by the proposed 3D-CGAN outperform those of the\nbenchmark schemes, which can accurately reflect the radio propagation condition\nin 3D environment.", "published": "2025-04-17 09:55:03", "link": "http://arxiv.org/abs/2504.12794v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Distributed Intelligent Sensing and Communications for 6G: Architecture and Use Cases", "abstract": "The Distributed Intelligent Sensing and Communication (DISAC) framework\nredefines Integrated Sensing and Communication (ISAC) for 6G by leveraging\ndistributed architectures to enhance scalability, adaptability, and resource\nefficiency. This paper presents key architectural enablers, including advanced\ndata representation, seamless target handover, support for heterogeneous\ndevices, and semantic integration. Two use cases illustrate the transformative\npotential of DISAC: smart factory shop floors and Vulnerable Road User (VRU)\nprotection at smart intersections. These scenarios demonstrate significant\nimprovements in precision, safety, and operational efficiency compared to\ntraditional ISAC systems. The preliminary DISAC architecture incorporates\nintelligent data processing, distributed coordination, and emerging\ntechnologies such as Reconfigurable Intelligent Surfaces (RIS) to meet 6G's\nstringent requirements. By addressing critical challenges in sensing accuracy,\nlatency, and real-time decision-making, DISAC positions itself as a cornerstone\nfor next-generation wireless networks, advancing innovation in dynamic and\ncomplex environments.", "published": "2025-04-17 09:02:36", "link": "http://arxiv.org/abs/2504.12765v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Universal Approximation with XL MIMO Systems: OTA Classification via Trainable Analog Combining", "abstract": "In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the XL MIMO channel\ncoefficients as the random nodes of a hidden layer, and the receiver's analog\ncombiner as a trainable output layer, we cast the end-to-end system to the\nExtreme Learning Machine (ELM) framework, leading to a novel formulation for\nOver-The-Air (OTA) edge inference without requiring traditional digital\nprocessing nor pre-processing at the transmitter. Through theoretical analysis\nand numerical evaluation, we showcase that XL-MIMO-ELM enables\nnear-instantaneous training and efficient classification, suggesting the\nparadigm shift of beyond massive MIMO systems as neural networks alongside\ntheir profound communications role. Compared to deep learning approaches and\nconventional ELMs, the proposed framework achieves on par performance with\norders of magnitude lower complexity, making it highly attractive for ultra low\npower wireless devices.", "published": "2025-04-17 08:53:30", "link": "http://arxiv.org/abs/2504.12758v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations", "abstract": "Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.", "published": "2025-04-17 07:54:26", "link": "http://arxiv.org/abs/2504.12721v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
=======
{"title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based on 3D Conditional GAN", "abstract": "The advancement of advanced air mobility (AAM) in recent years has given rise\nto the concept of low-altitude economy (LAE). However, the diverse flight\nactivities associated with the emerging LAE applications in urban scenarios\nconfront complex physical environments, which urgently necessitates ubiquitous\nand reliable communication to guarantee the operation safety of the\nlow-altitude aircraft. As one of promising technologies for the sixth\ngeneration (6G) mobile networks, channel knowledge map (CKM) enables the\nenvironment-aware communication by constructing a site-specific dataset,\nthereby providing a priori on-site information for the aircraft to obtain the\nchannel state information (CSI) at arbitrary locations with much reduced online\noverhead. Diverse base station (BS) deployments in the three-dimensional (3D)\nurban low-altitude environment require efficient 3D CKM construction to capture\nspatial channel characteristics with less overhead. Towards this end, this\npaper proposes a 3D channel gain map (CGM) inference method based on a 3D\nconditional generative adversarial network (3D-CGAN). Specifically, we first\nanalyze the potential deployment types of BSs in urban low-altitude scenario,\nand investigate the CGM representation with the corresponding 3D channel gain\nmodel. The framework of the proposed 3D-CGAN is then discussed, which is\ntrained by a dataset consisting of existing CGMs. Consequently, the trained\n3D-CGAN is capable of inferring the corresponding CGM only based on the BS\ncoordinate without additional measurement. The simulation results demonstrate\nthat the CGMs inferred by the proposed 3D-CGAN outperform those of the\nbenchmark schemes, which can accurately reflect the radio propagation condition\nin 3D environment.", "published": "2025-04-17 09:55:03", "link": "http://arxiv.org/abs/2504.12794v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Distributed Intelligent Sensing and Communications for 6G: Architecture and Use Cases", "abstract": "The Distributed Intelligent Sensing and Communication (DISAC) framework\nredefines Integrated Sensing and Communication (ISAC) for 6G by leveraging\ndistributed architectures to enhance scalability, adaptability, and resource\nefficiency. This paper presents key architectural enablers, including advanced\ndata representation, seamless target handover, support for heterogeneous\ndevices, and semantic integration. Two use cases illustrate the transformative\npotential of DISAC: smart factory shop floors and Vulnerable Road User (VRU)\nprotection at smart intersections. These scenarios demonstrate significant\nimprovements in precision, safety, and operational efficiency compared to\ntraditional ISAC systems. The preliminary DISAC architecture incorporates\nintelligent data processing, distributed coordination, and emerging\ntechnologies such as Reconfigurable Intelligent Surfaces (RIS) to meet 6G's\nstringent requirements. By addressing critical challenges in sensing accuracy,\nlatency, and real-time decision-making, DISAC positions itself as a cornerstone\nfor next-generation wireless networks, advancing innovation in dynamic and\ncomplex environments.", "published": "2025-04-17 09:02:36", "link": "http://arxiv.org/abs/2504.12765v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "High-Resolution Multipath Angle Estimation Based on Power-Angle-Delay Profile for Directional Scanning Sounding", "abstract": "Directional scanning sounding (DSS) has become widely adopted for\nhigh-frequency channel measurements because it effectively compensates for\nsevere path loss. However, the resolution of existing multipath component (MPC)\nangle estimation methods is constrained by the DSS angle sampling interval.\nTherefore, this communication proposes a high-resolution MPC angle estimation\nmethod based on power-angle-delay profile (PADP) for DSS. By exploiting the\nmapping relationship between the power difference of adjacent angles in the\nPADP and MPC offset angle, the resolution of MPC angle estimation is refined,\nsignificantly enhancing the accuracy of MPC angle and amplitude estimation\nwithout increasing measurement complexity. Numerical simulation results\ndemonstrate that the proposed method reduces the mean squared estimation errors\nof angle and amplitude by one order of magnitude compared to traditional\nomnidirectional synthesis methods. Furthermore, the estimation errors approach\nthe Cram\\'er-Rao Lower Bounds (CRLBs) derived for wideband DSS, thereby\nvalidating its superior performance in MPC angle and amplitude estimation.\nFinally, experiments conducted in an indoor scenario at 37.5 GHz validate the\nexcellent performance of the proposed method in practical applications.", "published": "2025-04-17 06:56:34", "link": "http://arxiv.org/abs/2504.12698v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Sub-Scalp Brain-Computer Interface Device Design and Fabrication", "abstract": "Current brain-computer interfaces (BCI) face limitations in signal\nacquisition. While sub-scalp EEG offers a potential solution, existing devices\nprioritize chronic seizure monitoring and lack features suited for BCI\napplications. This work addresses this gap by outlining key specifications for\nsub-scalp BCI devices, focusing on channel count, sampling rate, power\nefficiency, and form factor. We present the Set-And-Forget EEG (SAFE) system, a\ncustom-built amplifier and wireless transmitter meeting these criteria. This\ncompact (12x12 mm), six-channel device offers 1024 Hz sampling and Bluetooth\nLow Energy data transmission. Validation using generated sinusoids and\nelectrocorticography recordings of visual evoked potentials in sheep models\ndemonstrated low noise recording. Future animal studies will assess sub-scalp\nEEG signal quality for BCI applications. This data lays the groundwork for\nhuman trials, ultimately paving the way for chronic, in-home BCIs that empower\nindividuals with physical disabilities.", "published": "2025-04-17 01:51:16", "link": "http://arxiv.org/abs/2504.12578v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "AI for CSI Prediction in 5G-Advanced and Beyond", "abstract": "Artificial intelligence (AI) is pivotal in advancing fifth-generation\n(5G)-Advanced and sixth-generation systems, capturing substantial research\ninterest. Both the 3rd Generation Partnership Project (3GPP) and leading\ncorporations champion AI's standardization in wireless communication. This\npiece delves into AI's role in channel state information (CSI) prediction, a\nsub-use case acknowledged in 5G-Advanced by the 3GPP. We offer an exhaustive\nsurvey of AI-driven CSI prediction, highlighting crucial elements like\naccuracy, generalization, and complexity. Further, we touch on the practical\nside of model management, encompassing training, monitoring, and data\ngathering. Moreover, we explore prospects for CSI prediction in future wireless\ncommunication systems, entailing integrated design with feedback, multitasking\nsynergy, and predictions in rapid scenarios. This article seeks to be a\ntouchstone for subsequent research in this burgeoning domain.", "published": "2025-04-17 01:39:19", "link": "http://arxiv.org/abs/2504.12571v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Fast Computation of the Discrete Fourier Transform Rectangular Index Coefficients", "abstract": "In~\\cite{sic-magazine-2025}, the authors show that the square index\ncoefficients (SICs) of the \\(N\\)-point discrete Fourier transform (DFT) -- that\nis, the coefficients \\(X_{k\\sqrt{N}}\\) for \\(k = 0, 1, \\ldots, \\sqrt{N} - 1\\)\n-- can be losslessly compressed from \\(N\\) to \\(\\sqrt{N}\\) points, thereby\naccelerating the computation of these specific DFT coefficients accordingly.\nFollowing up on that, in this article we generalize SICs into what we refer to\nas rectangular index coefficients (RICs) of the DFT, formalized as $X_{kL},\nk=0,1,\\cdots,C-1$, in which the integers $C$ and $L$ are generic roots of $N$\nsuch that $N=LC$. We present an algorithm to compress the $N$-point input\nsignal $\\mathbf{x}$ into a $C$-point signal $\\mathbf{\\hat{x}}$ at the expense\nof $\\mathcal{O}(N)$ complex sums and no complex multiplication. We show that a\nDFT on $\\mathbf{\\hat{x}}$ is equivalent to a DFT on the RICs of $\\mathbf{x}$.\nIn cases where specific frequencies of \\(\\mathbf{x}\\) are of interest -- as in\nharmonic analysis -- one can conveniently adjust the signal parameters (e.g.,\nfrequency resolution) to align the RICs with those frequencies, and use the\nproposed algorithm to compute them significantly faster. If $N$ is a power of\ntwo -- as required by the fast Fourier transform (FFT) algorithm -- then $C$\ncan be any power of two in the range $[2, N/2]$ and one can use our algorithm\nalong with FFT to compute all RICs in $\\mathcal{O}(C\\log C)$ time complexity.", "published": "2025-04-17 00:44:22", "link": "http://arxiv.org/abs/2504.12551v1", "categories": ["eess.SP", "cs.CC", "cs.DS"], "primary_category": "eess.SP"}
>>>>>>> 33f049834fead0e168638dee581a3663e744788f
