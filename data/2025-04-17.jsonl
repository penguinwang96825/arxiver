{"title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "abstract": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.", "published": "2025-04-17 17:59:56", "link": "http://arxiv.org/abs/2504.13180v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Aligning Constraint Generation with Design Intent in Parametric CAD", "abstract": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.", "published": "2025-04-17 17:59:54", "link": "http://arxiv.org/abs/2504.13178v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization", "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.", "published": "2025-04-17 17:59:33", "link": "http://arxiv.org/abs/2504.13173v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MIB: A Mechanistic Interpretability Benchmark", "abstract": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.", "published": "2025-04-17 17:55:45", "link": "http://arxiv.org/abs/2504.13151v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction", "abstract": "Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.", "published": "2025-04-17 17:51:38", "link": "http://arxiv.org/abs/2504.13142v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo", "abstract": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.", "published": "2025-04-17 17:49:40", "link": "http://arxiv.org/abs/2504.13139v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Energy-Based Reward Models for Robust Language Model Alignment", "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "published": "2025-04-17 17:47:15", "link": "http://arxiv.org/abs/2504.13134v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis", "abstract": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.", "published": "2025-04-17 17:44:19", "link": "http://arxiv.org/abs/2504.13129v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "abstract": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.", "published": "2025-04-17 17:42:02", "link": "http://arxiv.org/abs/2504.13125v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models", "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.", "published": "2025-04-17 17:39:41", "link": "http://arxiv.org/abs/2504.13122v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?", "abstract": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.", "published": "2025-04-17 14:07:11", "link": "http://arxiv.org/abs/2504.12961v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis", "abstract": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.", "published": "2025-04-17 09:18:04", "link": "http://arxiv.org/abs/2504.12777v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems", "abstract": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.", "published": "2025-04-17 08:21:28", "link": "http://arxiv.org/abs/2504.12735v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric", "abstract": "This paper presents an adversary model and a simulation framework\nspecifically tailored for analyzing attacks on distributed systems composed of\nmultiple distributed protocols, with a focus on assessing the security of\nblockchain networks. Our model classifies and constrains adversarial actions\nbased on the assumptions of the target protocols, defined by failure models,\ncommunication models, and the fault tolerance thresholds of Byzantine Fault\nTolerant (BFT) protocols. The goal is to study not only the intended effects of\nadversarial strategies but also their unintended side effects on critical\nsystem properties. We apply this framework to analyze fairness properties in a\nHyperledger Fabric (HF) blockchain network. Our focus is on novel fairness\nattacks that involve coordinated adversarial actions across various HF\nservices. Simulations show that even a constrained adversary can violate\nfairness with respect to specific clients (client fairness) and impact related\nguarantees (order fairness), which relate the reception order of transactions\nto their final order in the blockchain. This paper significantly extends our\nprevious work by introducing and evaluating a mitigation mechanism specifically\ndesigned to counter transaction reordering attacks. We implement and integrate\nthis defense into our simulation environment, demonstrating its effectiveness\nunder diverse conditions.", "published": "2025-04-17 08:17:27", "link": "http://arxiv.org/abs/2504.12733v1", "categories": ["cs.CR", "cs.DC", "cs.MA"], "primary_category": "cs.CR"}
{"title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination", "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.", "published": "2025-04-17 07:41:25", "link": "http://arxiv.org/abs/2504.12714v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance", "abstract": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.", "published": "2025-04-17 03:23:17", "link": "http://arxiv.org/abs/2504.12612v1", "categories": ["cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A generalized energy-based modeling framework with application to field/circuit coupled problems", "abstract": "This paper presents a generalized energy-based modeling framework extending\nrecent formulations tailored for differential-algebraic equations. The proposed\nstructure, inspired by the port-Hamiltonian formalism, ensures passivity,\npreserves the power balance, and facilitates the consistent interconnection of\nsubsystems. A particular focus is put on low-frequency power applications in\nelectrical engineering. Stranded, solid, and foil conductor models are\ninvestigated in the context of the eddy current problem. Each conductor model\nis shown to fit into the generalized energy-based structure, which allows their\nstructure-preserving coupling with electrical circuits described by modified\nnodal analysis. Theoretical developments are validated through a numerical\nsimulation of an oscillator circuit, demonstrating energy conservation in\nlossless scenarios and controlled dissipation when eddy currents are present.", "published": "2025-04-17 15:45:20", "link": "http://arxiv.org/abs/2504.13036v1", "categories": ["math.NA", "cs.NA", "35Q61, 65M60, 65L80, 78M10"], "primary_category": "math.NA"}
{"title": "Efficient Chebyshev Reconstruction for the Anisotropic Equilibrium Model in Magnetic Particle Imaging", "abstract": "Magnetic Particle Imaging (MPI) is a tomographic imaging modality capable of\nreal-time, high-sensitivity mapping of superparamagnetic iron oxide\nnanoparticles. Model-based image reconstruction provides an alternative to\nconventional methods that rely on a measured system matrix, eliminating the\nneed for laborious calibration measurements. Nevertheless, model-based\napproaches must account for the complexities of the imaging chain to maintain\nhigh image quality. A recently proposed direct reconstruction method leverages\nweighted Chebyshev polynomials in the frequency domain, removing the need for a\nsimulated system matrix. However, the underlying model neglects key physical\neffects, such as nanoparticle anisotropy, leading to distortions in\nreconstructed images. To mitigate these artifacts, an adapted direct Chebyshev\nreconstruction (DCR) method incorporates a spatially variant deconvolution\nstep, significantly improving reconstruction accuracy at the cost of increased\ncomputational demands. In this work, we evaluate the adapted DCR on six\nexperimental phantoms, demonstrating enhanced reconstruction quality in real\nmeasurements and achieving image fidelity comparable to or exceeding that of\nsimulated system matrix reconstruction. Furthermore, we introduce an efficient\napproximation for the spatially variable deconvolution, reducing both runtime\nand memory consumption while maintaining accuracy. This method achieves\ncomputational complexity of O(N log N ), making it particularly beneficial for\nhigh-resolution and three-dimensional imaging. Our results highlight the\npotential of the adapted DCR approach for improving model-based MPI\nreconstruction in practical applications.", "published": "2025-04-17 14:37:49", "link": "http://arxiv.org/abs/2504.12981v1", "categories": ["physics.med-ph", "cs.NA", "eess.IV", "math.NA"], "primary_category": "physics.med-ph"}
{"title": "RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs", "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs). However, their performance\nheavily relies on the strategy used to select training points. Conventional\nadaptive sampling methods, such as residual-based refinement, often require\nmulti-round sampling and repeated retraining of PINNs, leading to computational\ninefficiency due to redundant points and costly gradient\ncomputations-particularly in high-dimensional or high-order derivative\nscenarios. To address these limitations, we propose RL-PINNs, a reinforcement\nlearning(RL)-driven adaptive sampling framework that enables efficient training\nwith only a single round of sampling. Our approach formulates adaptive sampling\nas a Markov decision process, where an RL agent dynamically selects optimal\ntraining points by maximizing a long-term utility metric. Critically, we\nreplace gradient-dependent residual metrics with a computationally efficient\nfunction variation as the reward signal, eliminating the overhead of derivative\ncalculations. Furthermore, we employ a delayed reward mechanism to prioritize\nlong-term training stability over short-term gains. Extensive experiments\nacross diverse PDE benchmarks, including low-regular, nonlinear,\nhigh-dimensional, and high-order problems, demonstrate that RL-PINNs\nsignificantly outperforms existing residual-driven adaptive methods in\naccuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,\nmaking them scalable to high-dimensional and high-order problems.", "published": "2025-04-17 13:50:55", "link": "http://arxiv.org/abs/2504.12949v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Optimal analysis of penalized lowest-order mixed FEMs for the Stokes-Darcy model", "abstract": "This paper is concerned with non-uniform fully-mixed FEMs for dynamic coupled\nStokes-Darcy model with the well-known Beavers-Joseph-Saffman (BJS) interface\ncondition. In particular, a decoupled algorithm with the lowest-order mixed\nnon-uniform FE approximations (MINI for the Stokes equation and RT0-DG0 for the\nDarcy equation) and the classical Nitsche-type penalty is studied. The method\nwith the combined approximation of different orders is commonly used in\npractical simulations. However, the optimal error analysis of methods with\nnon-uniform approximations for the coupled Stokes-Darcy flow model has remained\nchallenging, although the analysis for uniform approximations has been well\ndone. The key question is how the lower-order approximation to the Darcy flow\ninfluences the accuracy of the Stokes solution through the interface condition.\nIn this paper, we prove that the decoupled algorithm provides a truly optimal\nconvergence rate in L^2-norm in spatial direction: O(h^2) for Stokes velocity\nand O(h) for Darcy flow in the coupled Stokes-Darcy model. This implies that\nthe lower-order approximation to the Darcy flow does not pollute the accuracy\nof numerical velocity for Stokes flow. The analysis presented in this paper is\nbased on a well-designed Stokes-Darcy Ritz projection and given for a dynamic\ncoupled model. The optimal error estimate holds for more general combined\napproximations and more general coupled models, including the corresponding\nmodel of steady-state Stokes-Darcy flows and the model of coupled dynamic\nStokes and steady-state Darcy flows. Numerical results confirm our theoretical\nanalysis and show that the decoupled algorithm is efficient.", "published": "2025-04-17 13:37:39", "link": "http://arxiv.org/abs/2504.12938v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Manifold-valued function approximation from multiple tangent spaces", "abstract": "Approximating a manifold-valued function from samples of input-output pairs\nconsists of modeling the relationship between an input from a vector space and\nan output on a Riemannian manifold. We propose a function approximation method\nthat leverages and unifies two prior techniques: (i) approximating a pullback\nto the tangent space, and (ii) the Riemannian moving least squares method. The\ncore idea of the new scheme is to combine pullbacks to multiple tangent spaces\nwith a weighted Fr\\'echet mean. The effectiveness of this approach is\nillustrated with numerical experiments on model problems from parametric model\norder reduction.", "published": "2025-04-17 12:33:30", "link": "http://arxiv.org/abs/2504.12892v1", "categories": ["math.NA", "cs.NA", "65D15, 65D40, 65J99, 46T20, 53B20, 58C25"], "primary_category": "math.NA"}
{"title": "Inverse iteration method for higher eigenvalues of the $p$-Laplacian", "abstract": "We propose a characterization of a $p$-Laplace higher eigenvalue based on the\ninverse iteration method with balancing the Rayleigh quotients of the positive\nand negative parts of solutions to consecutive $p$-Poisson equations. The\napproach relies on the second eigenvalue's minimax properties, but the actual\nlimiting eigenvalue depends on the choice of initial function. The\nwell-posedness and convergence of the iterative scheme are proved. Moreover, we\nprovide the corresponding numerical computations. As auxiliary results, which\nalso have an independent interest, we provide several properties of certain\n$p$-Poisson problems.", "published": "2025-04-17 10:52:42", "link": "http://arxiv.org/abs/2504.12836v1", "categories": ["math.AP", "cs.NA", "math.NA", "math.SP", "35P30, 35J92, 46-08, 47J10, 47J25, 49R05, 65N25"], "primary_category": "math.AP"}
{"title": "Efficient Primal-dual Forward-backward Splitting Method for Wasserstein-like Gradient Flows with General Nonlinear Mobilities", "abstract": "We construct an efficient primal-dual forward-backward (PDFB) splitting\nmethod for computing a class of minimizing movement schemes with nonlinear\nmobility transport distances, and apply it to computing Wasserstein-like\ngradient flows. This approach introduces a novel saddle point formulation for\nthe minimizing movement schemes, leveraging a support function form from the\nBenamou-Brenier dynamical formulation of optimal transport. The resulting\nframework allows for flexible computation of Wasserstein-like gradient flows by\nsolving the corresponding saddle point problem at the fully discrete level, and\ncan be easily extended to handle general nonlinear mobilities. We also provide\na detailed convergence analysis of the PDFB splitting method, along with\npractical remarks on its implementation and application. The effectiveness of\nthe method is demonstrated through several challenging numerical examples.", "published": "2025-04-17 07:37:08", "link": "http://arxiv.org/abs/2504.12713v1", "categories": ["math.NA", "cs.NA", "math.OC", "35A15, 47J25, 47J35, 49M29, 65K10, 76M30"], "primary_category": "math.NA"}
{"title": "Tangent Space Parametrization for Stochastic Differential Equations on SO(n)", "abstract": "In this paper, we study the numerical simulation of stochastic differential\nequations (SDEs) on the special orthogonal Lie group $\\text{SO}(n)$. We propose\na geometry-preserving numerical scheme based on the stochastic tangent space\nparametrization (S-TaSP) method for state-dependent multiplicative SDEs on\n$\\text{SO}(n)$. The convergence analysis of the S-TaSP scheme establishes a\nstrong convergence order of $\\mathcal{O}(\\delta^{\\frac{1-\\epsilon}{2}})$, which\nmatches the convergence order of the previous stochastic Lie Euler-Maruyama\nscheme while avoiding the computational cost of the exponential map. Numerical\nsimulation illustrates the theoretical results.", "published": "2025-04-17 05:22:04", "link": "http://arxiv.org/abs/2504.12650v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Geometry-preserving Numerical Scheme for Riemannian Stochastic Differential Equations", "abstract": "Stochastic differential equations (SDEs) on Riemannian manifolds have\nnumerous applications in system identification and control. However,\ngeometry-preserving numerical methods for simulating Riemannian SDEs remain\nrelatively underdeveloped. In this paper, we propose the Exponential\nEuler-Maruyama (Exp-EM) scheme for approximating solutions of SDEs on\nRiemannian manifolds. The Exp-EM scheme is both geometry-preserving and\ncomputationally tractable. We establish a strong convergence rate of\n$\\mathcal{O}(\\delta^{\\frac{1 - \\epsilon}{2}})$ for the Exp-EM scheme, which\nextends previous results obtained for specific manifolds to a more general\nsetting. Numerical simulations are provided to illustrate our theoretical\nfindings.", "published": "2025-04-17 04:14:00", "link": "http://arxiv.org/abs/2504.12631v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "The existence of explicit symplectic integrators for general nonseparable Hamiltonian systems", "abstract": "The existence of explicit symplectic integrators for general nonseparable\nHamiltonian systems is an open and important problem in both numerical analysis\nand computing in science and engineering, as explicit integrators are usually\nmore efficient than the implicit integrators of the same order of accuracy. Up\nto now, all responses to this problem are negative. That is, there exist\nexplicit symplectic integrators only for some special nonseparable Hamiltonian\nsystems, whereas the universal design involving explicit symplectic integrators\nfor general nonseparable Hamiltonian systems has not yet been studied\nsufficiently. In this paper, we present a constructive proof for the existence\nof explicit symplectic integrators for general nonseparable Hamiltonian systems\nvia finding explicit symplectic mappings under which the special submanifold of\nthe extended phase space is invariant. It turns out that the proposed explicit\nintegrators are symplectic in both the extended phase space and the original\nphase space. Moreover, on the basis of the global modified Hamiltonians of the\nproposed integrators, the backward error analysis is made via a parameter\nrelaxation and restriction technique to show the linear growth of global errors\nand the near-preservation of first integrals. In particular, the effective\nestimated time interval is nearly the same as classical implicit symplectic\nintegrators when applied to (near-) integrable Hamiltonian systems. Numerical\nexperiments with a completely integrable nonseparable Hamiltonian and a\nnonintegrable nonseparable Hamiltonian illustrate the good long-term behavior\nand high efficiency of the explicit symplectic integrators proposed and\nanalyzed in this paper.", "published": "2025-04-17 01:32:43", "link": "http://arxiv.org/abs/2504.12567v1", "categories": ["math.NA", "cs.NA", "65P10, 37M15"], "primary_category": "math.NA"}
{"title": "Symmetry classification and invariant solutions of the classical geometric mean reversion process", "abstract": "Based on the Lie symmetry method, we investigate a Feynman-Kac formula for\nthe classical geometric mean reversion process, which effectively describing\nthe dynamics of short-term interest rates. The Lie algebra of infinitesimal\nsymmetries and the corresponding one-parameter symmetry groups of the equation\nare obtained. An optimal system of invariant solutions are constructed by a\nderived optimal system of one-dimensional subalgebras. Because of taking into\naccount a supply response to price rises, this equation provides for a more\nrealistic assumption than the geometric Brownian motion in many investment\nscenarios.", "published": "2025-04-17 16:59:55", "link": "http://arxiv.org/abs/2504.13094v1", "categories": ["math.DS", "math.AP", "math.PR", "q-fin.MF"], "primary_category": "math.DS"}
{"title": "Optimal Capital Structure for Life Insurance Companies Offering Surplus Participation", "abstract": "We adapt Leland's dynamic capital structure model to the context of an\ninsurance company selling participating life insurance contracts explaining the\nexistence of life insurance contracts which provide both a guaranteed payment\nand surplus participation to the policyholders. Our derivation of the optimal\nparticipation rate reveals its pronounced sensitivity to the contract duration\nand the associated tax rate. Moreover, the asset substitution effect, which\ndescribes the tendency of equity holders to increase the riskiness of a\ncompany's investment decisions, decreases when adding surplus participation.", "published": "2025-04-17 11:19:17", "link": "http://arxiv.org/abs/2504.12851v1", "categories": ["q-fin.MF", "90B50, 91B06, 91B50, 91G05, 91G10, 91G50"], "primary_category": "q-fin.MF"}
{"title": "Classification-Based Analysis of Price Pattern Differences Between Cryptocurrencies and Stocks", "abstract": "Cryptocurrencies are digital tokens built on blockchain technology, with\nthousands actively traded on centralized exchanges (CEXs). Unlike stocks, which\nare backed by real businesses, cryptocurrencies are recognized as a distinct\nclass of assets by researchers. How do investors treat this new category of\nasset in trading? Are they similar to stocks as an investment tool for\ninvestors? We answer these questions by investigating cryptocurrencies' and\nstocks' price time series which can reflect investors' attitudes towards the\ntargeted assets. Concretely, we use different machine learning models to\nclassify cryptocurrencies' and stocks' price time series in the same period and\nget an extremely high accuracy rate, which reflects that cryptocurrency\ninvestors behave differently in trading from stock investors. We then extract\nfeatures from these price time series to explain the price pattern difference,\nincluding mean, variance, maximum, minimum, kurtosis, skewness, and first to\nthird-order autocorrelation, etc., and then use machine learning methods\nincluding logistic regression (LR), random forest (RF), support vector machine\n(SVM), etc. for classification. The classification results show that these\nextracted features can help to explain the price time series pattern difference\nbetween cryptocurrencies and stocks.", "published": "2025-04-17 09:12:27", "link": "http://arxiv.org/abs/2504.12771v1", "categories": ["q-fin.ST"], "primary_category": "q-fin.ST"}
{"title": "Propagation of Chaos in One-hidden-layer Neural Networks beyond Logarithmic Time", "abstract": "We study the approximation gap between the dynamics of a polynomial-width\nneural network and its infinite-width counterpart, both trained using projected\ngradient descent in the mean-field scaling regime. We demonstrate how to\ntightly bound this approximation gap through a differential equation governed\nby the mean-field dynamics. A key factor influencing the growth of this ODE is\nthe local Hessian of each particle, defined as the derivative of the particle's\nvelocity in the mean-field dynamics with respect to its position. We apply our\nresults to the canonical feature learning problem of estimating a\nwell-specified single-index model; we permit the information exponent to be\narbitrarily large, leading to convergence times that grow polynomially in the\nambient dimension $d$. We show that, due to a certain ``self-concordance''\nproperty in these problems -- where the local Hessian of a particle is bounded\nby a constant times the particle's velocity -- polynomially many neurons are\nsufficient to closely approximate the mean-field dynamics throughout training.", "published": "2025-04-17 17:24:38", "link": "http://arxiv.org/abs/2504.13110v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "An Empirically Grounded Identifiability Theory Will Accelerate Self-Supervised Learning Research", "abstract": "Self-Supervised Learning (SSL) powers many current AI systems. As research\ninterest and investment grow, the SSL design space continues to expand. The\nPlatonic view of SSL, following the Platonic Representation Hypothesis (PRH),\nsuggests that despite different methods and engineering approaches, all\nrepresentations converge to the same Platonic ideal. However, this phenomenon\nlacks precise theoretical explanation. By synthesizing evidence from\nIdentifiability Theory (IT), we show that the PRH can emerge in SSL. However,\ncurrent IT cannot explain SSL's empirical success. To bridge the gap between\ntheory and practice, we propose expanding IT into what we term Singular\nIdentifiability Theory (SITh), a broader theoretical framework encompassing the\nentire SSL pipeline. SITh would allow deeper insights into the implicit data\nassumptions in SSL and advance the field towards learning more interpretable\nand generalizable representations. We highlight three critical directions for\nfuture research: 1) training dynamics and convergence properties of SSL; 2) the\nimpact of finite samples, batch size, and data diversity; and 3) the role of\ninductive biases in architecture, augmentations, initialization schemes, and\noptimizers.", "published": "2025-04-17 17:10:33", "link": "http://arxiv.org/abs/2504.13101v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Variance-Reduced Fast Operator Splitting Methods for Stochastic Generalized Equations", "abstract": "We develop two classes of variance-reduced fast operator splitting methods to\napproximate solutions of both finite-sum and stochastic generalized equations.\nOur approach integrates recent advances in accelerated fixed-point methods,\nco-hypomonotonicity, and variance reduction. First, we introduce a class of\nvariance-reduced estimators and establish their variance-reduction bounds. This\nclass covers both unbiased and biased instances and comprises common estimators\nas special cases, including SVRG, SAGA, SARAH, and Hybrid-SGD. Next, we design\na novel accelerated variance-reduced forward-backward splitting (FBS) algorithm\nusing these estimators to solve finite-sum and stochastic generalized\nequations. Our method achieves both $\\mathcal{O}(1/k^2)$ and $o(1/k^2)$\nconvergence rates on the expected squared norm $\\mathbb{E}[ \\|\nG_{\\lambda}x^k\\|^2]$ of the FBS residual $G_{\\lambda}$, where $k$ is the\niteration counter. Additionally, we establish, for the first time, almost sure\nconvergence rates and almost sure convergence of iterates to a solution in\nstochastic accelerated methods. Unlike existing stochastic fixed-point\nalgorithms, our methods accommodate co-hypomonotone operators, which\npotentially include nonmonotone problems arising from recent applications. We\nfurther specify our method to derive an appropriate variant for each stochastic\nestimator -- SVRG, SAGA, SARAH, and Hybrid-SGD -- demonstrating that they\nachieve the best-known complexity for each without relying on enhancement\ntechniques. Alternatively, we propose an accelerated variance-reduced\nbackward-forward splitting (BFS) method, which attains similar convergence\nrates and oracle complexity as our FBS method. Finally, we validate our results\nthrough several numerical experiments and compare their performance.", "published": "2025-04-17 16:02:20", "link": "http://arxiv.org/abs/2504.13046v1", "categories": ["math.OC", "stat.ML", "90C25, 90C06, 90-08"], "primary_category": "math.OC"}
{"title": "Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to a Set of Experts", "abstract": "Learning-to-Defer (L2D) enables decision-making systems to improve\nreliability by selectively deferring uncertain predictions to more competent\nagents. However, most existing approaches focus exclusively on single-agent\ndeferral, which is often inadequate in high-stakes scenarios that require\ncollective expertise. We propose Top-$k$ Learning-to-Defer, a generalization of\nthe classical two-stage L2D framework that allocates each query to the $k$ most\nconfident agents instead of a single one. To further enhance flexibility and\ncost-efficiency, we introduce Top-$k(x)$ Learning-to-Defer, an adaptive\nextension that learns the optimal number of agents to consult for each query,\nbased on input complexity, agent competency distributions, and consultation\ncosts. For both settings, we derive a novel surrogate loss and prove that it is\nBayes-consistent and $(\\mathcal{R}, \\mathcal{G})$-consistent, ensuring\nconvergence to the Bayes-optimal allocation. Notably, we show that the\nwell-established model cascades paradigm arises as a restricted instance of our\nTop-$k$ and Top-$k(x)$ formulations. Extensive experiments across diverse\nbenchmarks demonstrate the effectiveness of our framework on both\nclassification and regression tasks.", "published": "2025-04-17 14:50:40", "link": "http://arxiv.org/abs/2504.12988v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "When do Random Forests work?", "abstract": "We study the effectiveness of randomizing split-directions in random forests.\nPrior literature has shown that, on the one hand, randomization can reduce\nvariance through decorrelation, and, on the other hand, randomization\nregularizes and works in low signal-to-noise ratio (SNR) environments. First,\nwe bring together and revisit decorrelation and regularization by presenting a\nsystematic analysis of out-of-sample mean-squared error (MSE) for different SNR\nscenarios based on commonly-used data-generating processes. We find that\nvariance reduction tends to increase with the SNR and forests outperform\nbagging when the SNR is low because, in low SNR cases, variance dominates bias\nfor both methods. Second, we show that the effectiveness of randomization is a\nquestion that goes beyond the SNR. We present a simulation study with fixed and\nmoderate SNR, in which we examine the effectiveness of randomization for other\ndata characteristics. In particular, we find that (i) randomization can\nincrease bias in the presence of fat tails in the distribution of covariates;\n(ii) in the presence of irrelevant covariates randomization is ineffective\nbecause bias dominates variance; and (iii) when covariates are mutually\ncorrelated randomization tends to be effective because variance dominates bias.\nBeyond randomization, we find that, for both bagging and random forests, bias\ncan be significantly reduced in the presence of correlated covariates. This\nlast finding goes beyond the prevailing view that averaging mostly works by\nvariance reduction. Given that in practice covariates are often correlated, our\nfindings on correlated covariates could open the way for a better understanding\nof why random forests work well in many applications.", "published": "2025-04-17 11:38:17", "link": "http://arxiv.org/abs/2504.12860v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "ALT: A Python Package for Lightweight Feature Representation in Time Series Classification", "abstract": "We introduce ALT, an open-source Python package created for efficient and\naccurate time series classification (TSC). The package implements the adaptive\nlaw-based transformation (ALT) algorithm, which transforms raw time series data\ninto a linearly separable feature space using variable-length shifted time\nwindows. This adaptive approach enhances its predecessor, the linear law-based\ntransformation (LLT), by effectively capturing patterns of varying temporal\nscales. The software is implemented for scalability, interpretability, and ease\nof use, achieving state-of-the-art performance with minimal computational\noverhead. Extensive benchmarking on real-world datasets demonstrates the\nutility of ALT for diverse TSC tasks in physics and related domains.", "published": "2025-04-17 10:57:29", "link": "http://arxiv.org/abs/2504.12841v1", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MS", "stat.ML", "62M10, 62H30, 68T05, 68T10", "I.5.1; I.2.6; G.3; D.2.13"], "primary_category": "cs.LG"}
{"title": "Cluster weighted models with multivariate skewed distributions for functional data", "abstract": "We propose a clustering method, funWeightClustSkew, based on mixtures of\nfunctional linear regression models and three skewed multivariate\ndistributions: the variance-gamma distribution, the skew-t distribution, and\nthe normal-inverse Gaussian distribution. Our approach follows the framework of\nthe functional high dimensional data clustering (funHDDC) method, and we extend\nto functional data the cluster weighted models based on skewed distributions\nused for finite dimensional multivariate data. We consider several parsimonious\nmodels, and to estimate the parameters we construct an expectation maximization\n(EM) algorithm. We illustrate the performance of funWeightClustSkew for\nsimulated data and for the Air Quality dataset.", "published": "2025-04-17 06:17:06", "link": "http://arxiv.org/abs/2504.12683v1", "categories": ["stat.ME", "cs.LG", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Spectral Algorithms under Covariate Shift", "abstract": "Spectral algorithms leverage spectral regularization techniques to analyze\nand process data, providing a flexible framework for addressing supervised\nlearning problems. To deepen our understanding of their performance in\nreal-world scenarios where the distributions of training and test data may\ndiffer, we conduct a rigorous investigation into the convergence behavior of\nspectral algorithms under distribution shifts, specifically within the\nframework of reproducing kernel Hilbert spaces. Our study focuses on the case\nof covariate shift. In this scenario, the marginal distributions of the input\ndata differ between the training and test datasets, while the conditional\ndistribution of the output given the input remains unchanged. Under this\nsetting, we analyze the generalization error of spectral algorithms and show\nthat they achieve minimax optimality when the density ratios between the\ntraining and test distributions are uniformly bounded. However, we also\nidentify a critical limitation: when the density ratios are unbounded, the\nspectral algorithms may become suboptimal. To address this limitation, we\npropose a weighted spectral algorithm that incorporates density ratio\ninformation into the learning process. Our theoretical analysis shows that this\nweighted approach achieves optimal capacity-independent convergence rates.\nFurthermore, by introducing a weight clipping technique, we demonstrate that\nthe convergence rates of the weighted spectral algorithm can approach the\noptimal capacity-dependent convergence rates arbitrarily closely. This\nimprovement resolves the suboptimality issue in unbounded density ratio\nscenarios and advances the state-of-the-art by refining existing theoretical\nresults.", "published": "2025-04-17 04:02:06", "link": "http://arxiv.org/abs/2504.12625v1", "categories": ["stat.ML", "cs.LG", "68Q32, 68T05, 62J02"], "primary_category": "stat.ML"}
{"title": "Bayesian Density-Density Regression with Application to Cell-Cell Communications", "abstract": "We introduce a scalable framework for regressing multivariate distributions\nonto multivariate distributions, motivated by the application of inferring\ncell-cell communication from population-scale single-cell data. The observed\ndata consist of pairs of multivariate distributions for ligands from one cell\ntype and corresponding receptors from another. For each ordered pair $e=(l,r)$\nof cell types $(l \\neq r)$ and each sample $i = 1, \\ldots, n$, we observe a\npair of distributions $(F_{ei}, G_{ei})$ of gene expressions for ligands and\nreceptors of cell types $l$ and $r$, respectively. The aim is to set up a\nregression of receptor distributions $G_{ei}$ given ligand distributions\n$F_{ei}$. A key challenge is that these distributions reside in distinct spaces\nof differing dimensions. We formulate the regression of multivariate densities\non multivariate densities using a generalized Bayes framework with the sliced\nWasserstein distance between fitted and observed distributions. Finally, we use\ninference under such regressions to define a directed graph for cell-cell\ncommunications.", "published": "2025-04-17 03:46:03", "link": "http://arxiv.org/abs/2504.12617v1", "categories": ["stat.ME", "stat.AP", "stat.CO", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Meta-Dependence in Conditional Independence Testing", "abstract": "Constraint-based causal discovery algorithms utilize many statistical tests\nfor conditional independence to uncover networks of causal dependencies. These\napproaches to causal discovery rely on an assumed correspondence between the\ngraphical properties of a causal structure and the conditional independence\nproperties of observed variables, known as the causal Markov condition and\nfaithfulness. Finite data yields an empirical distribution that is \"close\" to\nthe actual distribution. Across these many possible empirical distributions,\nthe correspondence to the graphical properties can break down for different\nconditional independencies, and multiple violations can occur at the same time.\nWe study this \"meta-dependence\" between conditional independence properties\nusing the following geometric intuition: each conditional independence property\nconstrains the space of possible joint distributions to a manifold. The\n\"meta-dependence\" between conditional independences is informed by the position\nof these manifolds relative to the true probability distribution. We provide a\nsimple-to-compute measure of this meta-dependence using information projections\nand consolidate our findings empirically using both synthetic and real-world\ndata.", "published": "2025-04-17 02:41:22", "link": "http://arxiv.org/abs/2504.12594v1", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A Multi-task Learning Balanced Attention Convolutional Neural Network Model for Few-shot Underwater Acoustic Target Recognition", "abstract": "Underwater acoustic target recognition (UATR) is of great significance for\nthe protection of marine diversity and national defense security. The\ndevelopment of deep learning provides new opportunities for UATR, but faces\nchallenges brought by the scarcity of reference samples and complex\nenvironmental interference. To address these issues, we proposes a multi-task\nbalanced channel attention convolutional neural network (MT-BCA-CNN). The\nmethod integrates a channel attention mechanism with a multi-task learning\nstrategy, constructing a shared feature extractor and multi-task classifiers to\njointly optimize target classification and feature reconstruction tasks. The\nchannel attention mechanism dynamically enhances discriminative acoustic\nfeatures such as harmonic structures while suppressing noise. Experiments on\nthe Watkins Marine Life Dataset demonstrate that MT-BCA-CNN achieves 97\\%\nclassification accuracy and 95\\% $F1$-score in 27-class few-shot scenarios,\nsignificantly outperforming traditional CNN and ACNN models, as well as popular\nstate-of-the-art UATR methods. Ablation studies confirm the synergistic\nbenefits of multi-task learning and attention mechanisms, while a dynamic\nweighting adjustment strategy effectively balances task contributions. This\nwork provides an efficient solution for few-shot underwater acoustic\nrecognition, advancing research in marine bioacoustics and sonar signal\nprocessing.", "published": "2025-04-17 17:11:32", "link": "http://arxiv.org/abs/2504.13102v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Can Masked Autoencoders Also Listen to Birds?", "abstract": "Masked Autoencoders (MAEs) pretrained on AudioSet fail to capture the\nfine-grained acoustic characteristics of specialized domains such as\nbioacoustic monitoring. Bird sound classification is critical for assessing\nenvironmental health, yet general-purpose models inadequately address its\nunique acoustic challenges. To address this, we introduce Bird-MAE, a\ndomain-specialized MAE pretrained on the large-scale BirdSet dataset. We\nexplore adjustments to pretraining, fine-tuning and utilizing frozen\nrepresentations. Bird-MAE achieves state-of-the-art results across all BirdSet\ndownstream tasks, substantially improving multi-label classification\nperformance compared to the general-purpose Audio-MAE baseline. Additionally,\nwe propose prototypical probing, a parameter-efficient method for leveraging\nMAEs' frozen representations. Bird-MAE's prototypical probes outperform linear\nprobing by up to 37\\% in MAP and narrow the gap to fine-tuning to approximately\n3\\% on average on BirdSet.", "published": "2025-04-17 12:13:25", "link": "http://arxiv.org/abs/2504.12880v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "CST-former: Multidimensional Attention-based Transformer for Sound Event Localization and Detection in Real Scenes", "abstract": "Sound event localization and detection (SELD) is a task for the\nclassification of sound events and the identification of direction of arrival\n(DoA) utilizing multichannel acoustic signals. For effective classification and\nlocalization, a channel-spectro-temporal transformer (CST-former) was\nsuggested. CST-former employs multidimensional attention mechanisms across the\nspatial, spectral, and temporal domains to enlarge the model's capacity to\nlearn the domain information essential for event detection and DoA estimation\nover time. In this work, we present an enhanced version of CST-former with\nmultiscale unfolded local embedding (MSULE) developed to capture and aggregate\ndomain information over multiple time-frequency scales. Also, we propose\nfinetuning and post-processing techniques beneficial for conducting the SELD\ntask over limited training datasets. In-depth ablation studies of the proposed\narchitecture and detailed analysis on the proposed modules are carried out to\nvalidate the efficacy of multidimensional attentions on the SELD task.\nEmpirical validation through experimentation on STARSS22 and STARSS23 datasets\ndemonstrates the remarkable performance of CST-former and post-processing\ntechniques without using external data.", "published": "2025-04-17 11:56:13", "link": "http://arxiv.org/abs/2504.12870v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting", "abstract": "Human speech goes beyond the mere transfer of information; it is a profound\nexchange of emotions and a connection between individuals. While Text-to-Speech\n(TTS) models have made huge progress, they still face challenges in controlling\nthe emotional expression in the generated speech. In this work, we propose\nEmoVoice, a novel emotion-controllable TTS model that exploits large language\nmodels (LLMs) to enable fine-grained freestyle natural language emotion\ncontrol, and a phoneme boost variant design that makes the model output phoneme\ntokens and audio tokens in parallel to enhance content consistency, inspired by\nchain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we\nintroduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring\nexpressive speech and fine-grained emotion labels with natural language\ndescriptions. EmoVoice achieves state-of-the-art performance on the English\nEmoVoice-DB test set using only synthetic training data, and on the Chinese\nSecap test set using our in-house data. We further investigate the reliability\nof existing emotion evaluation metrics and their alignment with human\nperceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and\nGemini to assess emotional speech. Demo samples are available at\nhttps://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints\nwill be released.", "published": "2025-04-17 11:50:04", "link": "http://arxiv.org/abs/2504.12867v1", "categories": ["eess.AS", "cs.AI", "cs.CL"], "primary_category": "eess.AS"}
{"title": "A Survey on Cross-Modal Interaction Between Music and Multimodal Data", "abstract": "Multimodal learning has driven innovation across various industries,\nparticularly in the field of music. By enabling more intuitive interaction\nexperiences and enhancing immersion, it not only lowers the entry barriers to\nthe music but also increases its overall appeal. This survey aims to provide a\ncomprehensive review of multimodal tasks related to music, outlining how music\ncontributes to multimodal learning and offering insights for researchers\nseeking to expand the boundaries of computational music. Unlike text and\nimages, which are often semantically or visually intuitive, music primarily\ninteracts with humans through auditory perception, making its data\nrepresentation inherently less intuitive. Therefore, this paper first\nintroduces the representations of music and provides an overview of music\ndatasets. Subsequently, we categorize cross-modal interactions between music\nand multimodal data into three types: music-driven cross-modal interactions,\nmusic-oriented cross-modal interactions, and bidirectional music cross-modal\ninteractions. For each category, we systematically trace the development of\nrelevant sub-tasks, analyze existing limitations, and discuss emerging trends.\nFurthermore, we provide a comprehensive summary of datasets and evaluation\nmetrics used in multimodal tasks related to music, offering benchmark\nreferences for future research. Finally, we discuss the current challenges in\ncross-modal interactions involving music and propose potential directions for\nfuture research.", "published": "2025-04-17 09:58:38", "link": "http://arxiv.org/abs/2504.12796v1", "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "cs.MM"}
{"title": "Temporal Attention Pooling for Frequency Dynamic Convolution in Sound Event Detection", "abstract": "Recent advances in deep learning, particularly frequency dynamic convolution\n(FDY conv), have significantly improved sound event detection (SED) by enabling\nfrequency-adaptive feature extraction. However, FDY conv relies on temporal\naverage pooling, which treats all temporal frames equally, limiting its ability\nto capture transient sound events such as alarm bells, door knocks, and speech\nplosives. To address this limitation, we propose temporal attention pooling\nfrequency dynamic convolution (TFD conv) to replace temporal average pooling\nwith temporal attention pooling (TAP). TAP adaptively weights temporal features\nthrough three complementary mechanisms: time attention pooling (TA) for\nemphasizing salient features, velocity attention pooling (VA) for capturing\ntransient changes, and conventional average pooling for robustness to\nstationary signals. Ablation studies show that TFD conv improves average PSDS1\nby 3.02% over FDY conv with only a 14.8% increase in parameter count. Classwise\nANOVA and Tukey HSD analysis further demonstrate that TFD conv significantly\nenhances detection performance for transient-heavy events, outperforming\nexisting FDY conv models. Notably, TFD conv achieves a maximum PSDS1 score of\n0.456, surpassing previous state-of-the-art SED systems. We also explore the\ncompatibility of TAP with other FDY conv variants, including dilated FDY conv\n(DFD conv), partial FDY conv (PFD conv), and multi-dilated FDY conv (MDFD\nconv). Among these, the integration of TAP with MDFD conv achieves the best\nresult with a PSDS1 score of 0.459, validating the complementary strengths of\ntemporal attention and multi-scale frequency adaptation. These findings\nestablish TFD conv as a powerful and generalizable framework for enhancing both\ntransient sensitivity and overall feature robustness in SED.", "published": "2025-04-17 06:03:43", "link": "http://arxiv.org/abs/2504.12670v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Degrees of Freedom of Holographic MIMO -- Fundamental Theory and Analytical Methods", "abstract": "Holographic multiple-input multiple-output (MIMO) is envisioned as one of the\nmost promising technology enablers for future sixth-generation (6G) networks.\nThe use of electrically large holographic surface (HoloS) antennas has the\npotential to significantly boost the spatial multiplexing gain by increasing\nthe number of degrees of freedom (DoF), even in line-of-sight (LoS) channels.\nIn this context, the research community has shown a growing interest in\ncharacterizing the fundamental limits of this technology. In this paper, we\ncompare the two analytical methods commonly utilized in the literature for this\npurpose: the cut-set integral and the self-adjoint operator. We provide a\ndetailed description of both methods and discuss their advantages and\nlimitations.", "published": "2025-04-17 15:41:28", "link": "http://arxiv.org/abs/2504.13031v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "ORIS allocation to minimize the outage probability in a multi-user VLC scenario", "abstract": "Visible Light Communication (VLC) is a promising solution to address the\ngrowing demand for wireless data, leveraging the widespread use of\nlight-emitting diodes (LEDs) as transmitters. However, its deployment is\nchallenged by link blockages that cause connectivity outages. Optical\nreconfigurable intelligent surfaces (ORISs) have recently emerged as a solution\nto mitigate these disruptions. This work considers a multi-user VLC system and\ninvestigates the optimal association of ORISs to LEDs and users to minimize the\noutage probability while limiting the number of ORISs used. Numerical results\nfrom our proposed optimization algorithm demonstrate that using ORISs can\nreduce the outage probability by up to 85% compared to a no-ORIS scenario.", "published": "2025-04-17 15:24:25", "link": "http://arxiv.org/abs/2504.13016v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Simultaneous Polysomnography and Cardiotocography Reveal Temporal Correlation Between Maternal Obstructive Sleep Apnea and Fetal Hypoxia", "abstract": "Background: Obstructive sleep apnea syndrome (OSAS) during pregnancy is\ncommon and can negatively affect fetal outcomes. However, studies on the\nimmediate effects of maternal hypoxia on fetal heart rate (FHR) changes are\nlacking. Methods: We used time-synchronized polysomnography (PSG) and\ncardiotocography (CTG) data from two cohorts to analyze the correlation between\nmaternal hypoxia and FHR changes (accelerations or decelerations). Maternal\nhypoxic event characteristics were analyzed using generalized linear modeling\n(GLM) to assess their associations with different FHR changes. Results: A total\nof 118 pregnant women participated. FHR changes were significantly associated\nwith maternal hypoxia, primarily characterized by accelerations. A longer\nhypoxic duration correlated with more significant FHR accelerations (P < 0.05),\nwhile prolonged hypoxia and greater SpO2 drop were linked to FHR decelerations\n(P < 0.05). Both cohorts showed a transient increase in FHR during maternal\nhypoxia, which returned to baseline after the event resolved. Conclusion:\nMaternal hypoxia significantly affects FHR, suggesting that maternal OSAS may\ncontribute to fetal hypoxia. These findings highlight the importance of\nmaternal-fetal interactions and provide insights for future interventions.", "published": "2025-04-17 15:17:14", "link": "http://arxiv.org/abs/2504.13010v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Optic Fingerprint(OFP): Enhancing Security in Li-Fi Networks", "abstract": "We present a hardware-integrated security framework for LiFi networks through\ndevice fingerprint extraction within the IEEE 802.15.7 protocol. Our Optic\nFingerprint (OFP) model utilizes inherent LED nonlinearities to generate\namplitude-based feature vectors in time and frequency domains, specifically\ndesigned for optical wireless systems. Experimental results with 39 commercial\nLEDs demonstrate 90.36% classification accuracy across SNR 10-30 dB while\nmaintaining standard compliance, offering a practical physical-layer\nauthentication solution for visible light communication.", "published": "2025-04-17 14:01:02", "link": "http://arxiv.org/abs/2504.12956v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "RIS-Assisted Beamfocusing in Near-Field IoT Communication Systems: A Transformer-Based Approach", "abstract": "The massive number of antennas in extremely large aperture array (ELAA)\nsystems shifts the propagation regime of signals in internet of things (IoT)\ncommunication systems towards near-field spherical wave propagation. We propose\na reconfigurable intelligent surfaces (RIS)-assisted beamfocusing mechanism,\nwhere the design of the two-dimensional beam codebook that contains both the\nangular and distance domains is challenging. To address this issue, we\nintroduce a novel Transformer-based two-stage beam training algorithm, which\nincludes the coarse and fine search phases. The proposed mechanism provides a\nfine-grained codebook with enhanced spatial resolution, enabling precise\nbeamfocusing. Specifically, in the first stage, the beam training is performed\nto estimate the approximate location of the device by using a simple codebook,\ndetermining whether it is within the beamfocusing range (BFR) or the\nnone-beamfocusing range (NBFR). In the second stage, by using a more precise\ncodebook, a fine-grained beam search strategy is conducted. Experimental\nresults unveil that the precision of the RIS-assisted beamfocusing is greatly\nimproved. The proposed method achieves beam selection accuracy up to 97% at\nsignal-to-noise ratio (SNR) of 20 dB, and improves 10% to 50% over the baseline\nmethod at different SNRs.", "published": "2025-04-17 12:29:10", "link": "http://arxiv.org/abs/2504.12889v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
{"title": "Optimizing Movable Antennas in Wideband Multi-User MIMO With Hardware Impairments", "abstract": "Movable antennas represent an emerging field in telecommunication research\nand a potential approach to achieving higher data rates in multiple-input\nmultiple-output (MIMO) communications when the total number of antennas is\nlimited. Most solutions and analyses to date have been limited to\n\\emph{narrowband} setups. This work complements the prior studies by\nquantifying the benefit of using movable antennas in \\emph{wideband} MIMO\ncommunication systems. First, we derive a novel uplink wideband system model\nthat also accounts for distortion from transceiver hardware impairments. We\nthen formulate and solve an optimization task to maximize the average sum rate\nby adjusting the antenna positions using particle swarm optimization. Finally,\nthe performance with movable antennas is compared with fixed uniform arrays and\nthe derived theoretical upper bound. The numerical study concludes that the\ndata rate improvement from movable antennas over other arrays heavily depends\non the level of hardware impairments, the richness of the multi-path\nenvironments, and the number of subcarriers. The present study provides vital\ninsights into the most suitable use cases for movable antennas in future\nwideband systems.", "published": "2025-04-17 12:20:46", "link": "http://arxiv.org/abs/2504.12885v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Supporting Urban Low-Altitude Economy: Channel Gain Map Inference Based on 3D Conditional GAN", "abstract": "The advancement of advanced air mobility (AAM) in recent years has given rise\nto the concept of low-altitude economy (LAE). However, the diverse flight\nactivities associated with the emerging LAE applications in urban scenarios\nconfront complex physical environments, which urgently necessitates ubiquitous\nand reliable communication to guarantee the operation safety of the\nlow-altitude aircraft. As one of promising technologies for the sixth\ngeneration (6G) mobile networks, channel knowledge map (CKM) enables the\nenvironment-aware communication by constructing a site-specific dataset,\nthereby providing a priori on-site information for the aircraft to obtain the\nchannel state information (CSI) at arbitrary locations with much reduced online\noverhead. Diverse base station (BS) deployments in the three-dimensional (3D)\nurban low-altitude environment require efficient 3D CKM construction to capture\nspatial channel characteristics with less overhead. Towards this end, this\npaper proposes a 3D channel gain map (CGM) inference method based on a 3D\nconditional generative adversarial network (3D-CGAN). Specifically, we first\nanalyze the potential deployment types of BSs in urban low-altitude scenario,\nand investigate the CGM representation with the corresponding 3D channel gain\nmodel. The framework of the proposed 3D-CGAN is then discussed, which is\ntrained by a dataset consisting of existing CGMs. Consequently, the trained\n3D-CGAN is capable of inferring the corresponding CGM only based on the BS\ncoordinate without additional measurement. The simulation results demonstrate\nthat the CGMs inferred by the proposed 3D-CGAN outperform those of the\nbenchmark schemes, which can accurately reflect the radio propagation condition\nin 3D environment.", "published": "2025-04-17 09:55:03", "link": "http://arxiv.org/abs/2504.12794v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Distributed Intelligent Sensing and Communications for 6G: Architecture and Use Cases", "abstract": "The Distributed Intelligent Sensing and Communication (DISAC) framework\nredefines Integrated Sensing and Communication (ISAC) for 6G by leveraging\ndistributed architectures to enhance scalability, adaptability, and resource\nefficiency. This paper presents key architectural enablers, including advanced\ndata representation, seamless target handover, support for heterogeneous\ndevices, and semantic integration. Two use cases illustrate the transformative\npotential of DISAC: smart factory shop floors and Vulnerable Road User (VRU)\nprotection at smart intersections. These scenarios demonstrate significant\nimprovements in precision, safety, and operational efficiency compared to\ntraditional ISAC systems. The preliminary DISAC architecture incorporates\nintelligent data processing, distributed coordination, and emerging\ntechnologies such as Reconfigurable Intelligent Surfaces (RIS) to meet 6G's\nstringent requirements. By addressing critical challenges in sensing accuracy,\nlatency, and real-time decision-making, DISAC positions itself as a cornerstone\nfor next-generation wireless networks, advancing innovation in dynamic and\ncomplex environments.", "published": "2025-04-17 09:02:36", "link": "http://arxiv.org/abs/2504.12765v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Universal Approximation with XL MIMO Systems: OTA Classification via Trainable Analog Combining", "abstract": "In this paper, we demonstrate that an eXtremely Large (XL) Multiple-Input\nMultiple-Output (MIMO) wireless system with appropriate analog combining\ncomponents exhibits the properties of a universal function approximator,\nsimilar to a feedforward neural network. By treating the XL MIMO channel\ncoefficients as the random nodes of a hidden layer, and the receiver's analog\ncombiner as a trainable output layer, we cast the end-to-end system to the\nExtreme Learning Machine (ELM) framework, leading to a novel formulation for\nOver-The-Air (OTA) edge inference without requiring traditional digital\nprocessing nor pre-processing at the transmitter. Through theoretical analysis\nand numerical evaluation, we showcase that XL-MIMO-ELM enables\nnear-instantaneous training and efficient classification, suggesting the\nparadigm shift of beyond massive MIMO systems as neural networks alongside\ntheir profound communications role. Compared to deep learning approaches and\nconventional ELMs, the proposed framework achieves on par performance with\norders of magnitude lower complexity, making it highly attractive for ultra low\npower wireless devices.", "published": "2025-04-17 08:53:30", "link": "http://arxiv.org/abs/2504.12758v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "TimeCapsule: Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations", "abstract": "Recent deep learning models for Long-term Time Series Forecasting (LTSF)\noften emphasize complex, handcrafted designs, while simpler architectures like\nlinear models or MLPs have often outperformed these intricate solutions. In\nthis paper, we revisit and organize the core ideas behind several key\ntechniques, such as redundancy reduction and multi-scale modeling, which are\nfrequently employed in advanced LTSF models. Our goal is to streamline these\nideas for more efficient deep learning utilization. To this end, we introduce\nTimeCapsule, a model built around the principle of high-dimensional information\ncompression that unifies these techniques in a generalized yet simplified\nframework. Specifically, we model time series as a 3D tensor, incorporating\ntemporal, variate, and level dimensions, and leverage mode production to\ncapture multi-mode dependencies while achieving dimensionality compression. We\npropose an internal forecast within the compressed representation domain,\nsupported by the Joint-Embedding Predictive Architecture (JEPA), to monitor the\nlearning of predictive representations. Extensive experiments on challenging\nbenchmarks demonstrate the versatility of our method, showing that TimeCapsule\ncan achieve state-of-the-art performance.", "published": "2025-04-17 07:54:26", "link": "http://arxiv.org/abs/2504.12721v1", "categories": ["cs.LG", "cs.AI", "eess.SP"], "primary_category": "cs.LG"}
