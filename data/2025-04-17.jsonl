{"title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding", "abstract": "Vision-language models are integral to computer vision research, yet many\nhigh-performing models remain closed-source, obscuring their data, design and\ntraining recipe. The research community has responded by using distillation\nfrom black-box models to label training data, achieving strong benchmark\nresults, at the cost of measurable scientific progress. However, without\nknowing the details of the teacher model and its data sources, scientific\nprogress remains difficult to measure. In this paper, we study building a\nPerception Language Model (PLM) in a fully open and reproducible framework for\ntransparent research in image and video understanding. We analyze standard\ntraining pipelines without distillation from proprietary models and explore\nlarge-scale synthetic data to identify critical data gaps, particularly in\ndetailed video understanding. To bridge these gaps, we release 2.8M\nhuman-labeled instances of fine-grained video question-answer pairs and\nspatio-temporally grounded video captions. Additionally, we introduce\nPLM-VideoBench, a suite for evaluating challenging video understanding tasks\nfocusing on the ability to reason about \"what\", \"where\", \"when\", and \"how\" of a\nvideo. We make our work fully reproducible by providing data, training recipes,\ncode & models.", "published": "2025-04-17 17:59:56", "link": "http://arxiv.org/abs/2504.13180v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Aligning Constraint Generation with Design Intent in Parametric CAD", "abstract": "We adapt alignment techniques from reasoning LLMs to the task of generating\nengineering sketch constraints found in computer-aided design (CAD) models.\nEngineering sketches consist of geometric primitives (e.g. points, lines)\nconnected by constraints (e.g. perpendicular, tangent) that define the\nrelationships between them. For a design to be easily editable, the constraints\nmust effectively capture design intent, ensuring the geometry updates\npredictably when parameters change. Although current approaches can generate\nCAD designs, an open challenge remains to align model outputs with design\nintent, we label this problem `design alignment'. A critical first step towards\naligning generative CAD models is to generate constraints which fully-constrain\nall geometric primitives, without over-constraining or distorting sketch\ngeometry. Using alignment techniques to train an existing constraint generation\nmodel with feedback from a constraint solver, we are able to fully-constrain\n93% of sketches compared to 34% when using a na\\\"ive supervised fine-tuning\n(SFT) baseline and only 8.9% without alignment. Our approach can be applied to\nany existing constraint generation model and sets the stage for further\nresearch bridging alignment strategies between the language and design domains.", "published": "2025-04-17 17:59:54", "link": "http://arxiv.org/abs/2504.13178v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization", "abstract": "Designing efficient and effective architectural backbones has been in the\ncore of research efforts to enhance the capability of foundation models.\nInspired by the human cognitive phenomenon of attentional bias-the natural\ntendency to prioritize certain events or stimuli-we reconceptualize neural\narchitectures, including Transformers, Titans, and modern linear recurrent\nneural networks as associative memory modules that learn a mapping of keys and\nvalues using an internal objective, referred to as attentional bias.\nSurprisingly, we observed that most existing sequence models leverage either\n(1) dot-product similarity, or (2) L2 regression objectives as their\nattentional bias. Going beyond these objectives, we present a set of\nalternative attentional bias configurations along with their effective\napproximations to stabilize their training procedure. We then reinterpret\nforgetting mechanisms in modern deep learning architectures as a form of\nretention regularization, providing a novel set of forget gates for sequence\nmodels. Building upon these insights, we present Miras, a general framework to\ndesign deep learning architectures based on four choices of: (i) associative\nmemory architecture, (ii) attentional bias objective, (iii) retention gate, and\n(iv) memory learning algorithm. We present three novel sequence models-Moneta,\nYaad, and Memora-that go beyond the power of existing linear RNNs while\nmaintaining a fast parallelizable training process. Our experiments show\ndifferent design choices in Miras yield models with varying strengths. For\nexample, certain instances of Miras achieve exceptional performance in special\ntasks such as language modeling, commonsense reasoning, and recall intensive\ntasks, even outperforming Transformers and other modern linear recurrent\nmodels.", "published": "2025-04-17 17:59:33", "link": "http://arxiv.org/abs/2504.13173v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MIB: A Mechanistic Interpretability Benchmark", "abstract": "How can we know whether new mechanistic interpretability methods achieve real\nimprovements? In pursuit of meaningful and lasting evaluation standards, we\npropose MIB, a benchmark with two tracks spanning four tasks and five models.\nMIB favors methods that precisely and concisely recover relevant causal\npathways or specific causal variables in neural language models. The circuit\nlocalization track compares methods that locate the model components - and\nconnections between them - most important for performing a task (e.g.,\nattribution patching or information flow routes). The causal variable\nlocalization track compares methods that featurize a hidden vector, e.g.,\nsparse autoencoders (SAEs) or distributed alignment search (DAS), and locate\nmodel features for a causal variable relevant to the task. Using MIB, we find\nthat attribution and mask optimization methods perform best on circuit\nlocalization. For causal variable localization, we find that the supervised DAS\nmethod performs best, while SAE features are not better than neurons, i.e.,\nstandard dimensions of hidden vectors. These findings illustrate that MIB\nenables meaningful comparisons of methods, and increases our confidence that\nthere has been real progress in the field.", "published": "2025-04-17 17:55:45", "link": "http://arxiv.org/abs/2504.13151v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Transfer Learning via Auxiliary Labels with Application to Cold-Hardiness Prediction", "abstract": "Cold temperatures can cause significant frost damage to fruit crops depending\non their resilience, or cold hardiness, which changes throughout the dormancy\nseason. This has led to the development of predictive cold-hardiness models,\nwhich help farmers decide when to deploy expensive frost-mitigation measures.\nUnfortunately, cold-hardiness data for model training is only available for\nsome fruit cultivars due to the need for specialized equipment and expertise.\nRather, farmers often do have years of phenological data (e.g. date of\nbudbreak) that they regularly collect for their crops. In this work, we\nintroduce a new transfer-learning framework, Transfer via Auxiliary Labels\n(TAL), that allows farmers to leverage the phenological data to produce more\naccurate cold-hardiness predictions, even when no cold-hardiness data is\navailable for their specific crop. The framework assumes a set of source tasks\n(cultivars) where each has associated primary labels (cold hardiness) and\nauxiliary labels (phenology). However, the target task (new cultivar) is\nassumed to only have the auxiliary labels. The goal of TAL is to predict\nprimary labels for the target task via transfer from the source tasks.\nSurprisingly, despite the vast literature on transfer learning, to our\nknowledge, the TAL formulation has not been previously addressed. Thus, we\npropose several new TAL approaches based on model selection and averaging that\ncan leverage recent deep multi-task models for cold-hardiness prediction. Our\nresults on real-world cold-hardiness and phenological data for multiple grape\ncultivars demonstrate that TAL can leverage the phenological data to improve\ncold-hardiness predictions in the absence of cold-hardiness data.", "published": "2025-04-17 17:51:38", "link": "http://arxiv.org/abs/2504.13142v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo", "abstract": "A wide range of LM applications require generating text that conforms to\nsyntactic or semantic constraints. Imposing such constraints can be naturally\nframed as probabilistic conditioning, but exact generation from the resulting\ndistribution -- which can differ substantially from the LM's base distribution\n-- is generally intractable. In this work, we develop an architecture for\ncontrolled LM generation based on sequential Monte Carlo (SMC). Our SMC\nframework allows us to flexibly incorporate domain- and problem-specific\nconstraints at inference time, and efficiently reallocate computational\nresources in light of new information during the course of generation. By\ncomparing to a number of alternatives and ablations on four challenging domains\n-- Python code generation for data science, text-to-SQL, goal inference, and\nmolecule synthesis -- we demonstrate that, with little overhead, our approach\nallows small open-source language models to outperform models over 8x larger,\nas well as closed-source, fine-tuned ones. In support of the probabilistic\nperspective, we show that these performance improvements are driven by better\napproximation to the posterior distribution. Our system builds on the framework\nof Lew et al. (2023) and integrates with its language model probabilistic\nprogramming language, giving users a simple, programmable way to apply SMC to a\nbroad variety of controlled generation problems.", "published": "2025-04-17 17:49:40", "link": "http://arxiv.org/abs/2504.13139v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Energy-Based Reward Models for Robust Language Model Alignment", "abstract": "Reward models (RMs) are essential for aligning Large Language Models (LLMs)\nwith human preferences. However, they often struggle with capturing complex\nhuman preferences and generalizing to unseen data. To address these challenges,\nwe introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc\nrefinement framework that enhances RM robustness and generalization. EBRM\nmodels the reward distribution explicitly, capturing uncertainty in human\npreferences and mitigating the impact of noisy or misaligned annotations. It\nachieves this through conflict-aware data filtering, label-noise-aware\ncontrastive training, and hybrid initialization. Notably, EBRM enhances RMs\nwithout retraining, making it computationally efficient and adaptable across\ndifferent models and tasks. Empirical evaluations on RM benchmarks demonstrate\nsignificant improvements in both robustness and generalization, achieving up to\na 5.97% improvement in safety-critical alignment tasks compared to standard\nRMs. Furthermore, reinforcement learning experiments confirm that our refined\nrewards enhance alignment quality, effectively delaying reward hacking. These\nresults demonstrate our approach as a scalable and effective enhancement for\nexisting RMs and alignment pipelines. The code is available at EBRM.", "published": "2025-04-17 17:47:15", "link": "http://arxiv.org/abs/2504.13134v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis", "abstract": "We present a novel approach to integrating scientific knowledge into\ngenerative models, enhancing their realism and consistency in image synthesis.\nFirst, we introduce Science-T2I, an expert-annotated adversarial dataset\ncomprising adversarial 20k image pairs with 9k prompts, covering wide distinct\nscientific knowledge categories. Leveraging Science-T2I, we present SciScore,\nan end-to-end reward model that refines the assessment of generated images\nbased on scientific knowledge, which is achieved by augmenting both the\nscientific comprehension and visual capabilities of pre-trained CLIP model.\nAdditionally, based on SciScore, we propose a two-stage training framework,\ncomprising a supervised fine-tuning phase and a masked online fine-tuning\nphase, to incorporate scientific knowledge into existing generative models.\nThrough comprehensive experiments, we demonstrate the effectiveness of our\nframework in establishing new standards for evaluating the scientific realism\nof generated content. Specifically, SciScore attains performance comparable to\nhuman-level, demonstrating a 5% improvement similar to evaluations conducted by\nexperienced human evaluators. Furthermore, by applying our proposed fine-tuning\nmethod to FLUX, we achieve a performance enhancement exceeding 50% on SciScore.", "published": "2025-04-17 17:44:19", "link": "http://arxiv.org/abs/2504.13129v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard", "abstract": "This paper investigates the application of large language models (LLMs) to\nfinancial tasks. We fine-tuned foundation models using the Open FinLLM\nLeaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed\ntechniques including supervised fine-tuning (SFT), direct preference\noptimization (DPO), and reinforcement learning (RL) to enhance their financial\ncapabilities. The fine-tuned models demonstrated substantial performance gains\nacross a wide range of financial tasks. Moreover, we measured the data scaling\nlaw in the financial domain. Our work demonstrates the potential of large\nlanguage models (LLMs) in financial applications.", "published": "2025-04-17 17:42:02", "link": "http://arxiv.org/abs/2504.13125v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models", "abstract": "Large Video Models (LVMs) built upon Large Language Models (LLMs) have shown\npromise in video understanding but often suffer from misalignment with human\nintuition and video hallucination issues. To address these challenges, we\nintroduce VistaDPO, a novel framework for Video Hierarchical Spatial-Temporal\nDirect Preference Optimization. VistaDPO enhances text-video preference\nalignment across three hierarchical levels: i) Instance Level, aligning overall\nvideo content with responses; ii) Temporal Level, aligning video temporal\nsemantics with event descriptions; and iii) Perceptive Level, aligning spatial\nobjects with language tokens. Given the lack of datasets for fine-grained\nvideo-language preference alignment, we construct VistaDPO-7k, a dataset of\n7.2K QA pairs annotated with chosen and rejected responses, along with\nspatial-temporal grounding information such as timestamps, keyframes, and\nbounding boxes. Extensive experiments on benchmarks such as Video\nHallucination, Video QA, and Captioning performance tasks demonstrate that\nVistaDPO significantly improves the performance of existing LVMs, effectively\nmitigating video-language misalignment and hallucination. The code and data are\navailable at https://github.com/HaroldChen19/VistaDPO.", "published": "2025-04-17 17:39:41", "link": "http://arxiv.org/abs/2504.13122v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "QLLM: Do We Really Need a Mixing Network for Credit Assignment in Multi-Agent Reinforcement Learning?", "abstract": "Credit assignment has remained a fundamental challenge in multi-agent\nreinforcement learning (MARL). Previous studies have primarily addressed this\nissue through value decomposition methods under the centralized training with\ndecentralized execution paradigm, where neural networks are utilized to\napproximate the nonlinear relationship between individual Q-values and the\nglobal Q-value. Although these approaches have achieved considerable success in\nvarious benchmark tasks, they still suffer from several limitations, including\nimprecise attribution of contributions, limited interpretability, and poor\nscalability in high-dimensional state spaces. To address these challenges, we\npropose a novel algorithm, \\textbf{QLLM}, which facilitates the automatic\nconstruction of credit assignment functions using large language models (LLMs).\nSpecifically, the concept of \\textbf{TFCAF} is introduced, wherein the credit\nallocation process is represented as a direct and expressive nonlinear\nfunctional formulation. A custom-designed \\textit{coder-evaluator} framework is\nfurther employed to guide the generation, verification, and refinement of\nexecutable code by LLMs, significantly mitigating issues such as hallucination\nand shallow reasoning during inference. Extensive experiments conducted on\nseveral standard MARL benchmarks demonstrate that the proposed method\nconsistently outperforms existing state-of-the-art baselines. Moreover, QLLM\nexhibits strong generalization capability and maintains compatibility with a\nwide range of MARL algorithms that utilize mixing networks, positioning it as a\npromising and versatile solution for complex multi-agent scenarios.", "published": "2025-04-17 14:07:11", "link": "http://arxiv.org/abs/2504.12961v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis", "abstract": "Climate policy development faces significant challenges due to deep\nuncertainty, complex system dynamics, and competing stakeholder interests.\nClimate simulation methods, such as Earth System Models, have become valuable\ntools for policy exploration. However, their typical use is for evaluating\npotential polices, rather than directly synthesizing them. The problem can be\ninverted to optimize for policy pathways, but the traditional optimization\napproaches often struggle with non-linear dynamics, heterogeneous agents, and\ncomprehensive uncertainty quantification. We propose a framework for augmenting\nclimate simulations with Multi-Agent Reinforcement Learning (MARL) to address\nthese limitations. We identify key challenges at the interface between climate\nsimulations and the application of MARL in the context of policy synthesis,\nincluding reward definition, scalability with increasing agents and state\nspaces, uncertainty propagation across linked systems, and solution validation.\nAdditionally, we discuss challenges in making MARL-derived solutions\ninterpretable and useful for policy-makers. Our framework provides a foundation\nfor more sophisticated climate policy exploration while acknowledging important\nlimitations and areas for future research.", "published": "2025-04-17 09:18:04", "link": "http://arxiv.org/abs/2504.12777v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "The Athenian Academy: A Seven-Layer Architecture Model for Multi-Agent Systems", "abstract": "This paper proposes the \"Academy of Athens\" multi-agent seven-layer\nframework, aimed at systematically addressing challenges in multi-agent systems\n(MAS) within artificial intelligence (AI) art creation, such as collaboration\nefficiency, role allocation, environmental adaptation, and task parallelism.\nThe framework divides MAS into seven layers: multi-agent collaboration,\nsingle-agent multi-role playing, single-agent multi-scene traversal,\nsingle-agent multi-capability incarnation, different single agents using the\nsame large model to achieve the same target agent, single-agent using different\nlarge models to achieve the same target agent, and multi-agent synthesis of the\nsame target agent. Through experimental validation in art creation, the\nframework demonstrates its unique advantages in task collaboration, cross-scene\nadaptation, and model fusion. This paper further discusses current challenges\nsuch as collaboration mechanism optimization, model stability, and system\nsecurity, proposing future exploration through technologies like meta-learning\nand federated learning. The framework provides a structured methodology for\nmulti-agent collaboration in AI art creation and promotes innovative\napplications in the art field.", "published": "2025-04-17 08:21:28", "link": "http://arxiv.org/abs/2504.12735v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Adversary-Augmented Simulation for Fairness Evaluation and Defense in Hyperledger Fabric", "abstract": "This paper presents an adversary model and a simulation framework\nspecifically tailored for analyzing attacks on distributed systems composed of\nmultiple distributed protocols, with a focus on assessing the security of\nblockchain networks. Our model classifies and constrains adversarial actions\nbased on the assumptions of the target protocols, defined by failure models,\ncommunication models, and the fault tolerance thresholds of Byzantine Fault\nTolerant (BFT) protocols. The goal is to study not only the intended effects of\nadversarial strategies but also their unintended side effects on critical\nsystem properties. We apply this framework to analyze fairness properties in a\nHyperledger Fabric (HF) blockchain network. Our focus is on novel fairness\nattacks that involve coordinated adversarial actions across various HF\nservices. Simulations show that even a constrained adversary can violate\nfairness with respect to specific clients (client fairness) and impact related\nguarantees (order fairness), which relate the reception order of transactions\nto their final order in the blockchain. This paper significantly extends our\nprevious work by introducing and evaluating a mitigation mechanism specifically\ndesigned to counter transaction reordering attacks. We implement and integrate\nthis defense into our simulation environment, demonstrating its effectiveness\nunder diverse conditions.", "published": "2025-04-17 08:17:27", "link": "http://arxiv.org/abs/2504.12733v1", "categories": ["cs.CR", "cs.DC", "cs.MA"], "primary_category": "cs.CR"}
{"title": "Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination", "abstract": "Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.", "published": "2025-04-17 07:41:25", "link": "http://arxiv.org/abs/2504.12714v1", "categories": ["cs.MA", "cs.AI", "cs.LG"], "primary_category": "cs.MA"}
{"title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance", "abstract": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.", "published": "2025-04-17 03:23:17", "link": "http://arxiv.org/abs/2504.12612v1", "categories": ["cs.AI", "cs.CR", "cs.MA"], "primary_category": "cs.AI"}
{"title": "A generalized energy-based modeling framework with application to field/circuit coupled problems", "abstract": "This paper presents a generalized energy-based modeling framework extending\nrecent formulations tailored for differential-algebraic equations. The proposed\nstructure, inspired by the port-Hamiltonian formalism, ensures passivity,\npreserves the power balance, and facilitates the consistent interconnection of\nsubsystems. A particular focus is put on low-frequency power applications in\nelectrical engineering. Stranded, solid, and foil conductor models are\ninvestigated in the context of the eddy current problem. Each conductor model\nis shown to fit into the generalized energy-based structure, which allows their\nstructure-preserving coupling with electrical circuits described by modified\nnodal analysis. Theoretical developments are validated through a numerical\nsimulation of an oscillator circuit, demonstrating energy conservation in\nlossless scenarios and controlled dissipation when eddy currents are present.", "published": "2025-04-17 15:45:20", "link": "http://arxiv.org/abs/2504.13036v1", "categories": ["math.NA", "cs.NA", "35Q61, 65M60, 65L80, 78M10"], "primary_category": "math.NA"}
{"title": "Efficient Chebyshev Reconstruction for the Anisotropic Equilibrium Model in Magnetic Particle Imaging", "abstract": "Magnetic Particle Imaging (MPI) is a tomographic imaging modality capable of\nreal-time, high-sensitivity mapping of superparamagnetic iron oxide\nnanoparticles. Model-based image reconstruction provides an alternative to\nconventional methods that rely on a measured system matrix, eliminating the\nneed for laborious calibration measurements. Nevertheless, model-based\napproaches must account for the complexities of the imaging chain to maintain\nhigh image quality. A recently proposed direct reconstruction method leverages\nweighted Chebyshev polynomials in the frequency domain, removing the need for a\nsimulated system matrix. However, the underlying model neglects key physical\neffects, such as nanoparticle anisotropy, leading to distortions in\nreconstructed images. To mitigate these artifacts, an adapted direct Chebyshev\nreconstruction (DCR) method incorporates a spatially variant deconvolution\nstep, significantly improving reconstruction accuracy at the cost of increased\ncomputational demands. In this work, we evaluate the adapted DCR on six\nexperimental phantoms, demonstrating enhanced reconstruction quality in real\nmeasurements and achieving image fidelity comparable to or exceeding that of\nsimulated system matrix reconstruction. Furthermore, we introduce an efficient\napproximation for the spatially variable deconvolution, reducing both runtime\nand memory consumption while maintaining accuracy. This method achieves\ncomputational complexity of O(N log N ), making it particularly beneficial for\nhigh-resolution and three-dimensional imaging. Our results highlight the\npotential of the adapted DCR approach for improving model-based MPI\nreconstruction in practical applications.", "published": "2025-04-17 14:37:49", "link": "http://arxiv.org/abs/2504.12981v1", "categories": ["physics.med-ph", "cs.NA", "eess.IV", "math.NA"], "primary_category": "physics.med-ph"}
{"title": "RL-PINNs: Reinforcement Learning-Driven Adaptive Sampling for Efficient Training of PINNs", "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving partial differential equations (PDEs). However, their performance\nheavily relies on the strategy used to select training points. Conventional\nadaptive sampling methods, such as residual-based refinement, often require\nmulti-round sampling and repeated retraining of PINNs, leading to computational\ninefficiency due to redundant points and costly gradient\ncomputations-particularly in high-dimensional or high-order derivative\nscenarios. To address these limitations, we propose RL-PINNs, a reinforcement\nlearning(RL)-driven adaptive sampling framework that enables efficient training\nwith only a single round of sampling. Our approach formulates adaptive sampling\nas a Markov decision process, where an RL agent dynamically selects optimal\ntraining points by maximizing a long-term utility metric. Critically, we\nreplace gradient-dependent residual metrics with a computationally efficient\nfunction variation as the reward signal, eliminating the overhead of derivative\ncalculations. Furthermore, we employ a delayed reward mechanism to prioritize\nlong-term training stability over short-term gains. Extensive experiments\nacross diverse PDE benchmarks, including low-regular, nonlinear,\nhigh-dimensional, and high-order problems, demonstrate that RL-PINNs\nsignificantly outperforms existing residual-driven adaptive methods in\naccuracy. Notably, RL-PINNs achieve this with negligible sampling overhead,\nmaking them scalable to high-dimensional and high-order problems.", "published": "2025-04-17 13:50:55", "link": "http://arxiv.org/abs/2504.12949v1", "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "cs.LG"}
{"title": "Optimal analysis of penalized lowest-order mixed FEMs for the Stokes-Darcy model", "abstract": "This paper is concerned with non-uniform fully-mixed FEMs for dynamic coupled\nStokes-Darcy model with the well-known Beavers-Joseph-Saffman (BJS) interface\ncondition. In particular, a decoupled algorithm with the lowest-order mixed\nnon-uniform FE approximations (MINI for the Stokes equation and RT0-DG0 for the\nDarcy equation) and the classical Nitsche-type penalty is studied. The method\nwith the combined approximation of different orders is commonly used in\npractical simulations. However, the optimal error analysis of methods with\nnon-uniform approximations for the coupled Stokes-Darcy flow model has remained\nchallenging, although the analysis for uniform approximations has been well\ndone. The key question is how the lower-order approximation to the Darcy flow\ninfluences the accuracy of the Stokes solution through the interface condition.\nIn this paper, we prove that the decoupled algorithm provides a truly optimal\nconvergence rate in L^2-norm in spatial direction: O(h^2) for Stokes velocity\nand O(h) for Darcy flow in the coupled Stokes-Darcy model. This implies that\nthe lower-order approximation to the Darcy flow does not pollute the accuracy\nof numerical velocity for Stokes flow. The analysis presented in this paper is\nbased on a well-designed Stokes-Darcy Ritz projection and given for a dynamic\ncoupled model. The optimal error estimate holds for more general combined\napproximations and more general coupled models, including the corresponding\nmodel of steady-state Stokes-Darcy flows and the model of coupled dynamic\nStokes and steady-state Darcy flows. Numerical results confirm our theoretical\nanalysis and show that the decoupled algorithm is efficient.", "published": "2025-04-17 13:37:39", "link": "http://arxiv.org/abs/2504.12938v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Manifold-valued function approximation from multiple tangent spaces", "abstract": "Approximating a manifold-valued function from samples of input-output pairs\nconsists of modeling the relationship between an input from a vector space and\nan output on a Riemannian manifold. We propose a function approximation method\nthat leverages and unifies two prior techniques: (i) approximating a pullback\nto the tangent space, and (ii) the Riemannian moving least squares method. The\ncore idea of the new scheme is to combine pullbacks to multiple tangent spaces\nwith a weighted Fr\\'echet mean. The effectiveness of this approach is\nillustrated with numerical experiments on model problems from parametric model\norder reduction.", "published": "2025-04-17 12:33:30", "link": "http://arxiv.org/abs/2504.12892v1", "categories": ["math.NA", "cs.NA", "65D15, 65D40, 65J99, 46T20, 53B20, 58C25"], "primary_category": "math.NA"}
{"title": "Inverse iteration method for higher eigenvalues of the $p$-Laplacian", "abstract": "We propose a characterization of a $p$-Laplace higher eigenvalue based on the\ninverse iteration method with balancing the Rayleigh quotients of the positive\nand negative parts of solutions to consecutive $p$-Poisson equations. The\napproach relies on the second eigenvalue's minimax properties, but the actual\nlimiting eigenvalue depends on the choice of initial function. The\nwell-posedness and convergence of the iterative scheme are proved. Moreover, we\nprovide the corresponding numerical computations. As auxiliary results, which\nalso have an independent interest, we provide several properties of certain\n$p$-Poisson problems.", "published": "2025-04-17 10:52:42", "link": "http://arxiv.org/abs/2504.12836v1", "categories": ["math.AP", "cs.NA", "math.NA", "math.SP", "35P30, 35J92, 46-08, 47J10, 47J25, 49R05, 65N25"], "primary_category": "math.AP"}
{"title": "Efficient Primal-dual Forward-backward Splitting Method for Wasserstein-like Gradient Flows with General Nonlinear Mobilities", "abstract": "We construct an efficient primal-dual forward-backward (PDFB) splitting\nmethod for computing a class of minimizing movement schemes with nonlinear\nmobility transport distances, and apply it to computing Wasserstein-like\ngradient flows. This approach introduces a novel saddle point formulation for\nthe minimizing movement schemes, leveraging a support function form from the\nBenamou-Brenier dynamical formulation of optimal transport. The resulting\nframework allows for flexible computation of Wasserstein-like gradient flows by\nsolving the corresponding saddle point problem at the fully discrete level, and\ncan be easily extended to handle general nonlinear mobilities. We also provide\na detailed convergence analysis of the PDFB splitting method, along with\npractical remarks on its implementation and application. The effectiveness of\nthe method is demonstrated through several challenging numerical examples.", "published": "2025-04-17 07:37:08", "link": "http://arxiv.org/abs/2504.12713v1", "categories": ["math.NA", "cs.NA", "math.OC", "35A15, 47J25, 47J35, 49M29, 65K10, 76M30"], "primary_category": "math.NA"}
{"title": "Tangent Space Parametrization for Stochastic Differential Equations on SO(n)", "abstract": "In this paper, we study the numerical simulation of stochastic differential\nequations (SDEs) on the special orthogonal Lie group $\\text{SO}(n)$. We propose\na geometry-preserving numerical scheme based on the stochastic tangent space\nparametrization (S-TaSP) method for state-dependent multiplicative SDEs on\n$\\text{SO}(n)$. The convergence analysis of the S-TaSP scheme establishes a\nstrong convergence order of $\\mathcal{O}(\\delta^{\\frac{1-\\epsilon}{2}})$, which\nmatches the convergence order of the previous stochastic Lie Euler-Maruyama\nscheme while avoiding the computational cost of the exponential map. Numerical\nsimulation illustrates the theoretical results.", "published": "2025-04-17 05:22:04", "link": "http://arxiv.org/abs/2504.12650v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Geometry-preserving Numerical Scheme for Riemannian Stochastic Differential Equations", "abstract": "Stochastic differential equations (SDEs) on Riemannian manifolds have\nnumerous applications in system identification and control. However,\ngeometry-preserving numerical methods for simulating Riemannian SDEs remain\nrelatively underdeveloped. In this paper, we propose the Exponential\nEuler-Maruyama (Exp-EM) scheme for approximating solutions of SDEs on\nRiemannian manifolds. The Exp-EM scheme is both geometry-preserving and\ncomputationally tractable. We establish a strong convergence rate of\n$\\mathcal{O}(\\delta^{\\frac{1 - \\epsilon}{2}})$ for the Exp-EM scheme, which\nextends previous results obtained for specific manifolds to a more general\nsetting. Numerical simulations are provided to illustrate our theoretical\nfindings.", "published": "2025-04-17 04:14:00", "link": "http://arxiv.org/abs/2504.12631v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "The existence of explicit symplectic integrators for general nonseparable Hamiltonian systems", "abstract": "The existence of explicit symplectic integrators for general nonseparable\nHamiltonian systems is an open and important problem in both numerical analysis\nand computing in science and engineering, as explicit integrators are usually\nmore efficient than the implicit integrators of the same order of accuracy. Up\nto now, all responses to this problem are negative. That is, there exist\nexplicit symplectic integrators only for some special nonseparable Hamiltonian\nsystems, whereas the universal design involving explicit symplectic integrators\nfor general nonseparable Hamiltonian systems has not yet been studied\nsufficiently. In this paper, we present a constructive proof for the existence\nof explicit symplectic integrators for general nonseparable Hamiltonian systems\nvia finding explicit symplectic mappings under which the special submanifold of\nthe extended phase space is invariant. It turns out that the proposed explicit\nintegrators are symplectic in both the extended phase space and the original\nphase space. Moreover, on the basis of the global modified Hamiltonians of the\nproposed integrators, the backward error analysis is made via a parameter\nrelaxation and restriction technique to show the linear growth of global errors\nand the near-preservation of first integrals. In particular, the effective\nestimated time interval is nearly the same as classical implicit symplectic\nintegrators when applied to (near-) integrable Hamiltonian systems. Numerical\nexperiments with a completely integrable nonseparable Hamiltonian and a\nnonintegrable nonseparable Hamiltonian illustrate the good long-term behavior\nand high efficiency of the explicit symplectic integrators proposed and\nanalyzed in this paper.", "published": "2025-04-17 01:32:43", "link": "http://arxiv.org/abs/2504.12567v1", "categories": ["math.NA", "cs.NA", "65P10, 37M15"], "primary_category": "math.NA"}
{"title": "Symmetry classification and invariant solutions of the classical geometric mean reversion process", "abstract": "Based on the Lie symmetry method, we investigate a Feynman-Kac formula for\nthe classical geometric mean reversion process, which effectively describing\nthe dynamics of short-term interest rates. The Lie algebra of infinitesimal\nsymmetries and the corresponding one-parameter symmetry groups of the equation\nare obtained. An optimal system of invariant solutions are constructed by a\nderived optimal system of one-dimensional subalgebras. Because of taking into\naccount a supply response to price rises, this equation provides for a more\nrealistic assumption than the geometric Brownian motion in many investment\nscenarios.", "published": "2025-04-17 16:59:55", "link": "http://arxiv.org/abs/2504.13094v1", "categories": ["math.DS", "math.AP", "math.PR", "q-fin.MF"], "primary_category": "math.DS"}
{"title": "Optimal Capital Structure for Life Insurance Companies Offering Surplus Participation", "abstract": "We adapt Leland's dynamic capital structure model to the context of an\ninsurance company selling participating life insurance contracts explaining the\nexistence of life insurance contracts which provide both a guaranteed payment\nand surplus participation to the policyholders. Our derivation of the optimal\nparticipation rate reveals its pronounced sensitivity to the contract duration\nand the associated tax rate. Moreover, the asset substitution effect, which\ndescribes the tendency of equity holders to increase the riskiness of a\ncompany's investment decisions, decreases when adding surplus participation.", "published": "2025-04-17 11:19:17", "link": "http://arxiv.org/abs/2504.12851v1", "categories": ["q-fin.MF", "90B50, 91B06, 91B50, 91G05, 91G10, 91G50"], "primary_category": "q-fin.MF"}
