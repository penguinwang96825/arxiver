{"title": "Boxicity of Zero Divisor Graphs", "abstract": "A $d$-dimensional box is the cartesian product $R_i\\times\\cdots\\times R_d$\nwhere each $R_i$ is a closed interval on the real line. The boxicity of a\ngraph, denoted as $box(G)$, is the minimum integer $d\\geq 0$ such that $G$ is\nthe intersection graph of a collection of $d$-dimensional boxes. The study of\ngraph classes associated with algebraic structures is a fascinating area where\ngraph theory and algebra meet. A well-known class of graphs associated with\nrings is the class of zero divisor graphs introduced by Beck in 1988. Since\nthen, this graph class has been studied extensively by several researchers.\nDenote by $Z(R)$ the set of zero divisors of a ring $R$. The zero divisor graph\n$\\Gamma(R)$ for a ring $R$ is defined as the graph with the vertex set\n$V(\\Gamma(R))=Z(R)$ and $E(\\Gamma(R))=\\{\\{a_i,a_j\\}:a_ia_j\\in Z(R)\\text{ and\n}a_ia_j=0 \\}$. Let $N=\\Pi_{i=1}^ap_i^{n_i}$ be the prime factorization of $N$.\nIn Discrete Applied Mathematics 365 (2025), pp. 260-269, it was shown that\n$box(\\Gamma(\\mathbb{Z}_N))\\leq\\Pi_{i=1}^a(n_i+1)-\\Pi_{i=1}^a(\\lfloor\nn_i/2\\rfloor+1)-1$. In this paper we exactly determine the boxicity of\n$\\Gamma(\\mathbb{Z}_N)$: We show that when $N\\equiv 2\\pmod 4$ and $N$ is not\ndivisible by $p^3$ for any prime divisor $p$, we have\n$box(\\Gamma(\\mathbb{Z}_N))=a-1$. Otherwise $box(\\Gamma(\\mathbb{Z}_N))=a$.\nSuppose $R$ is a non-zero commutative ring with identity that is also a reduced\nring and let $k$ be the size of the set of minimal prime ideals of $R$. In the\nsame paper, it was showed that $box(\\Gamma(R))\\leq 2^k-2$. We improve this\nresult by showing $\\lfloor k/2\\rfloor\\leq box(\\Gamma(R))\\leq k$ with the same\nassumption on $R$. In this paper we also show that\n$a-1\\leq\\dim_{TH}(\\Gamma(\\mathbb{Z}_N))\\leq a$ and $\\lfloor\nk/2\\rfloor\\leq\\dim_{TH}(\\Gamma(R))\\leq k$, where $\\dim_{TH}$ is another\ndimensional parameter associated with graphs known as the threshold dimension.", "published": "2025-05-18 11:41:10", "link": "http://arxiv.org/abs/2505.12376v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "PoisonArena: Uncovering Competing Poisoning Attacks in Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions.\nProject page: https://github.com/yxf203/PoisonArena.", "published": "2025-05-18 23:22:53", "link": "http://arxiv.org/abs/2505.12574v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Batched Self-Consistency Improves LLM Relevance Assessment and Ranking", "abstract": "Given some information need, Large Language Models (LLMs) are increasingly\nused for candidate text relevance assessment, typically using a one-by-one\npointwise (PW) strategy where each LLM call evaluates one candidate at a time.\nMeanwhile, it has been shown that LLM performance can be improved through\nself-consistency: prompting the LLM to do the same task multiple times\n(possibly in perturbed ways) and then aggregating the responses. To take\nadvantage of self-consistency, we hypothesize that batched PW strategies, where\nmultiple passages are judged in one LLM call, are better suited than one-by-one\nPW methods since a larger input context can induce more diverse LLM sampling\nacross self-consistency calls. We first propose several candidate batching\nstrategies to create prompt diversity across self-consistency calls through\nsubset reselection and permutation. We then test our batched PW methods on\nrelevance assessment and ranking tasks against one-by-one PW and listwise LLM\nranking baselines with and without self-consistency, using three passage\nretrieval datasets and GPT-4o, Claude Sonnet 3, and Amazon Nova Pro. We find\nthat batched PW methods outperform all baselines, and show that batching can\ngreatly amplify the positive effects of self-consistency. For instance, on our\nlegal search dataset, GPT-4o one-by-one PW ranking NDCG@10 improves only from\n44.9% to 46.8% without self-consistency vs. with 15 self consistency calls,\nwhile batched PW ranking improves from 43.8% to 51.3%, respectively.", "published": "2025-05-18 23:12:10", "link": "http://arxiv.org/abs/2505.12570v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval", "abstract": "Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.", "published": "2025-05-18 17:18:06", "link": "http://arxiv.org/abs/2505.12499v1", "categories": ["cs.CV", "cs.IR", "cs.MM"], "primary_category": "cs.CV"}
{"title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "abstract": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized.", "published": "2025-05-18 15:04:02", "link": "http://arxiv.org/abs/2505.12452v1", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization", "abstract": "Graph neural networks (GNNs) have advanced recommender systems by modeling\ninteraction relationships. However, existing graph-based recommenders rely on\nsparse ID features and do not fully exploit textual information, resulting in\nlow information density within representations. Furthermore, graph contrastive\nlearning faces challenges. Random negative sampling can introduce false\nnegative samples, while fixed temperature coefficients cannot adapt to the\nheterogeneity of different nodes. In addition, current efforts to enhance\nrecommendations with large language models (LLMs) have not fully utilized their\nChain-of-Thought (CoT) reasoning capabilities to guide representation learning.\nTo address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph\nNeural Recommendation with Harmonized Group Policy Optimization). This\nframework leverages the CoT reasoning ability of LLMs to generate semantic IDs,\nenriching reasoning processes and improving information density and semantic\nquality of representations. Moreover, we design a reinforcement learning\nalgorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative\nsampling strategies and temperature coefficients in contrastive learning. This\napproach enhances long-tail recommendation performance and ensures optimization\nconsistency across different groups. Experimental results on three datasets\ndemonstrate that LGHRec improves representation quality through semantic IDs\ngenerated by LLM's CoT reasoning and effectively boosts contrastive learning\nwith HGPO. Our method outperforms several baseline models. The code is\navailable at: https://anonymous.4open.science/r/LLM-Rec.", "published": "2025-05-18 12:50:36", "link": "http://arxiv.org/abs/2505.12396v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Addressing Missing Data Issue for Diffusion-based Recommendation", "abstract": "Diffusion models have shown significant potential in generating oracle items\nthat best match user preference with guidance from user historical interaction\nsequences. However, the quality of guidance is often compromised by\nunpredictable missing data in observed sequence, leading to suboptimal item\ngeneration. Since missing data is uncertain in both occurrence and content,\nrecovering it is impractical and may introduce additional errors. To tackle\nthis challenge, we propose a novel dual-side Thompson sampling-based Diffusion\nModel (TDM), which simulates extra missing data in the guidance signals and\nallows diffusion models to handle existing missing data through extrapolation.\nTo preserve user preference evolution in sequences despite extra missing data,\nwe introduce Dual-side Thompson Sampling to implement simulation with two\nprobability models, sampling by exploiting user preference from both item\ncontinuity and sequence stability. TDM strategically removes items from\nsequences based on dual-side Thompson sampling and treats these edited\nsequences as guidance for diffusion models, enhancing models' robustness to\nmissing data through consistency regularization. Additionally, to enhance the\ngeneration efficiency, TDM is implemented under the denoising diffusion\nimplicit models to accelerate the reverse process. Extensive experiments and\ntheoretical analysis validate the effectiveness of TDM in addressing missing\ndata in sequential recommendations.", "published": "2025-05-18 07:45:46", "link": "http://arxiv.org/abs/2505.12283v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "A Survey on Side Information-driven Session-based Recommendation: From a Data-centric Perspective", "abstract": "Session-based recommendation is gaining increasing attention due to its\npractical value in predicting the intents of anonymous users based on limited\nbehaviors. Emerging efforts incorporate various side information to alleviate\ninherent data scarcity issues in this task, leading to impressive performance\nimprovements. The core of side information-driven session-based recommendation\nis the discovery and utilization of diverse data. In this survey, we provide a\ncomprehensive review of this task from a data-centric perspective.\nSpecifically, this survey commences with a clear formulation of the task. This\nis followed by a detailed exploration of various benchmarks rich in side\ninformation that are pivotal for advancing research in this field. Afterwards,\nwe delve into how different types of side information enhance the task,\nunderscoring data characteristics and utility. Moreover, we discuss the usage\nof various side information, including data encoding, data injection, and\ninvolved techniques. A systematic review of research progress is then\npresented, with the taxonomy by the types of side information. Finally, we\nsummarize the current limitations and present the future prospects of this\nvibrant topic.", "published": "2025-05-18 07:36:43", "link": "http://arxiv.org/abs/2505.12279v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "abstract": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance.", "published": "2025-05-18 06:51:21", "link": "http://arxiv.org/abs/2505.12260v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "abstract": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels.", "published": "2025-05-18 23:20:01", "link": "http://arxiv.org/abs/2505.12572v1", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "primary_category": "cs.CL"}
{"title": "Resolving the Double Near-Far Problem via Wireless Powered Pinching-Antenna Networks", "abstract": "This letter introduces a novel wireless powered communication system,\nreferred to as a wireless powered pinching-antenna network (WPPAN), utilizing a\nsingle waveguide with pinching antennas to address the double near-far problem\ninherent in wireless powered networks. In the proposed WPPAN, users harvest\nenergy from spatially distributed pinching antennas in the downlink and use the\ncollected power to transmit messages in the uplink. Furthermore, to manage the\ncombinatorial complexity associated with activating the pinching antennas, we\npropose three approaches of varying complexity to simplify the original\nresource allocation problem and then solve it efficiently using convex\noptimization methods. Simulation results confirm that the proposed WPPAN system\neffectively mitigates the double near-far problem by providing antenna\nresources closer to the users, thereby enhancing both downlink energy\nharvesting and uplink data transmission.", "published": "2025-05-18 13:13:42", "link": "http://arxiv.org/abs/2505.12403v1", "categories": ["eess.SP", "cs.IT", "math.IT"], "primary_category": "eess.SP"}
{"title": "Generative Diffusion Model Driven Massive Random Access in Massive MIMO Systems", "abstract": "Massive random access is an important technology for achieving ultra-massive\nconnectivity in next-generation wireless communication systems. It aims to\naddress key challenges during the initial access phase, including active user\ndetection (AUD), channel estimation (CE), and data detection (DD). This paper\nexamines massive access in massive multiple-input multiple-output (MIMO)\nsystems, where deep learning is used to tackle the challenging AUD, CE, and DD\nfunctions. First, we introduce a Transformer-AUD scheme tailored for variable\npilot-length access. This approach integrates pilot length information and a\nspatial correlation module into a Transformer-based detector, enabling a single\nmodel to generalize across various pilot lengths and antenna numbers. Next, we\npropose a generative diffusion model (GDM)-driven iterative CE and DD\nframework. The GDM employs a score function to capture the posterior\ndistributions of massive MIMO channels and data symbols. Part of the score\nfunction is learned from the channel dataset via neural networks, while the\nremaining score component is derived in a closed form by applying the symbol\nprior constellation distribution and known transmission model. Utilizing these\nposterior scores, we design an asynchronous alternating CE and DD framework\nthat employs a predictor-corrector sampling technique to iteratively generate\nchannel estimation and data detection results during the reverse diffusion\nprocess. Simulation results demonstrate that our proposed approaches\nsignificantly outperform baseline methods with respect to AUD, CE, and DD.", "published": "2025-05-18 11:55:17", "link": "http://arxiv.org/abs/2505.12382v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Modeling and Performance Analysis of IoT-over-LEO Satellite Systems under Realistic Operational Constraints: A Stochastic Geometry Approach", "abstract": "Current theoretical studies on IoT-over-LEO satellite systems often rely on\nunrealistic assumptions, such as infinite terrestrial areas and omnidirectional\nsatellite coverage, leaving significant gaps in theoretical analysis for more\nrealistic operational constraints. These constraints involve finite terrestrial\narea, limited satellite coverage, Earth curvature effect, integral uplink and\ndownlink analysis, and link-dependent interference. To address these gaps, this\npaper proposes a novel stochastic geometry based model to rigorously analyze\nthe performance of IoT-over-LEO satellite systems. By adopting a binomial point\nprocess (BPP) instead of the conventional Poisson point process (PPP), our\nmodel accurately characterizes the geographical distribution of a fixed number\nof IoT devices in a finite terrestrial region. This modeling framework enables\nthe derivation of distance distribution functions for both the links from the\nterrestrial IoT devices to the satellites (T-S) and from the satellites to the\nEarth station (S-ES), while also accounting for limited satellite coverage and\nEarth curvature effects. To realistically represent channel conditions, the\nNakagami fading model is employed for the T-S links to characterize diverse\nsmall-scale fading environments, while the shadowed-Rician fading model is used\nfor the S-ES links to capture the combined effects of shadowing and dominant\nline-of-sight paths. Furthermore, the analysis incorporates uplink and downlink\ninterference, ensuring a comprehensive evaluation of system performance. The\naccuracy and effectiveness of our theoretical framework are validated through\nextensive Monte Carlo simulations. These results provide insights into key\nperformance metrics, such as coverage probability and average ergodic rate, for\nboth individual links and the overall system.", "published": "2025-05-18 10:00:43", "link": "http://arxiv.org/abs/2505.12336v1", "categories": ["cs.NI", "cs.IT", "math.IT"], "primary_category": "cs.NI"}
{"title": "An Information-Theoretic Framework for Receiver Quantization in Communication", "abstract": "We investigate information-theoretic limits and design of communication under\nreceiver quantization. Unlike most existing studies, this work is more focused\non the impact of resolution reduction from high to low. We consider a standard\ntransceiver architecture, which includes i.i.d. complex Gaussian codebook at\nthe transmitter, and a symmetric quantizer cascaded with a nearest neighbor\ndecoder at the receiver. Employing the generalized mutual information (GMI), an\nachievable rate under general quantization rules is obtained in an analytical\nform, which shows that the rate loss due to quantization is\n$\\log\\left(1+\\gamma\\mathsf{SNR}\\right)$, where $\\gamma$ is determined by\nthresholds and levels of the quantizer. Based on this result, the performance\nunder uniform receiver quantization is analyzed comprehensively. We show that\nthe front-end gain control, which determines the loading factor of\nquantization, has an increasing impact on performance as the resolution\ndecreases. In particular, we prove that the unique loading factor that\nminimizes the MSE also maximizes the GMI, and the corresponding irreducible\nrate loss is given by $\\log\\left(1+\\mathsf {mmse}\\cdot\\mathsf{SNR}\\right)$,\nwhere mmse is the minimum MSE normalized by the variance of quantizer input,\nand is equal to the minimum of $\\gamma$. A geometrical interpretation for the\noptimal uniform quantization at the receiver is further established. Moreover,\nby asymptotic analysis, we characterize the impact of biased gain control,\nincluding how small rate losses decay to zero and achievable rate\napproximations under large bias. From asymptotic expressions of the optimal\nloading factor and mmse, approximations and several per-bit rules for\nperformance are also provided. Finally we discuss more types of receiver\nquantization and show that the consistency between achievable rate maximization\nand MSE minimization does not hold in general.", "published": "2025-05-18 06:37:52", "link": "http://arxiv.org/abs/2505.12258v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Behavior Synthesis via Contact-Aware Fisher Information Maximization", "abstract": "Contact dynamics hold immense amounts of information that can improve a\nrobot's ability to characterize and learn about objects in their environment\nthrough interactions. However, collecting information-rich contact data is\nchallenging due to its inherent sparsity and non-smooth nature, requiring an\nactive approach to maximize the utility of contacts for learning. In this work,\nwe investigate an optimal experimental design approach to synthesize robot\nbehaviors that produce contact-rich data for learning. Our approach derives a\ncontact-aware Fisher information measure that characterizes information-rich\ncontact behaviors that improve parameter learning. We observe emergent robot\nbehaviors that are able to excite contact interactions that efficiently learns\nobject parameters across a range of parameter learning examples. Last, we\ndemonstrate the utility of contact-awareness for learning parameters through\ncontact-seeking behaviors on several robotic experiments.", "published": "2025-05-18 03:15:31", "link": "http://arxiv.org/abs/2505.12214v1", "categories": ["cs.RO", "cs.IT", "math.IT"], "primary_category": "cs.RO"}
{"title": "Event-Driven Simulation for Rapid Iterative Development of Distributed Space Flight Software", "abstract": "This paper presents the design, development, and application of a novel space\nsimulation environment for rapidly prototyping and testing flight software for\ndistributed space systems. The environment combines the flexibility,\ndeterminism, and observability of software-only simulation with the fidelity\nand depth normally attained only by real-time hardware-in-the-loop testing.\nUltimately, this work enables an engineering process in which flight software\nis continuously improved and delivered in its final, flight-ready form, and\nwhich reduces the cost of design changes and software revisions with respect to\na traditional linear development process. Three key methods not found in\nexisting tools enable this environment's novel capabilities: first, a hybrid\nevent-driven simulation architecture that combines continuous-time and\ndiscrete-event simulation paradigms; second, a lightweight application-layer\nsoftware virtualization design that allows executing compiled flight software\nbinaries while modeling process scheduling, input/output, and memory use; and\nthird, high-fidelity models for the multi-spacecraft space environment,\nincluding for wireless communication, relative sensing such as differential GPS\nand cameras, and flight computer health metrics like heap exhaustion and\nfragmentation. The simulation environment's capabilities are applied to the\niterative development and testing of two flight-ready software packages: the\nguidance, navigation, and control software for the VISORS mission, and the\nStanford Space Rendezvous Laboratory software kit for rendezvous and proximity\noperations. Results from 33 months of flight software development demonstrate\nthe use of this simulation environment to rapidly and reliably identify and\nresolve defects, characterize navigation and control performance, and\nscrutinize implementation details like memory allocation and inter-spacecraft\nnetwork protocols.", "published": "2025-05-18 17:32:40", "link": "http://arxiv.org/abs/2505.12502v1", "categories": ["cs.SE", "cs.MA", "cs.RO"], "primary_category": "cs.SE"}
{"title": "Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems", "abstract": "Multi-agent collaboration has emerged as a pivotal paradigm for addressing\ncomplex, distributed tasks in large language model (LLM)-driven applications.\nWhile prior research has focused on high-level architectural frameworks, the\ngranular mechanisms governing agents, critical to performance and scalability,\nremain underexplored. This study systematically investigates four dimensions of\ncollaboration strategies: (1) agent governance, (2) participation control, (3)\ninteraction dynamics, and (4) dialogue history management. Through rigorous\nexperimentation under two context-dependent scenarios: Distributed Evidence\nIntegration (DEI) and Structured Evidence Synthesis (SES), we quantify the\nimpact of these strategies on both task accuracy and computational efficiency.\nOur findings reveal that centralized governance, instructor-led participation,\nordered interaction patterns, and instructor-curated context summarization\ncollectively optimize the trade-off between decision quality and resource\nutilization with the support of the proposed Token-Accuracy Ratio (TAR). This\nwork establishes a foundation for designing adaptive, scalable multi-agent\nsystems, shifting the focus from structural novelty to strategic interaction\nmechanics.", "published": "2025-05-18 15:46:14", "link": "http://arxiv.org/abs/2505.12467v1", "categories": ["cs.MA", "cs.AI"], "primary_category": "cs.MA"}
{"title": "Steady-State Strategy Synthesis for Swarms of Autonomous Agents", "abstract": "Steady-state synthesis aims to construct a policy for a given MDP $D$ such\nthat the long-run average frequencies of visits to the vertices of $D$ satisfy\ngiven numerical constraints. This problem is solvable in polynomial time, and\nmemoryless policies are sufficient for approximating an arbitrary frequency\nvector achievable by a general (infinite-memory) policy.\n  We study the steady-state synthesis problem for multiagent systems, where\nmultiple autonomous agents jointly strive to achieve a suitable frequency\nvector. We show that the problem for multiple agents is computationally hard\n(PSPACE or NP hard, depending on the variant), and memoryless strategy profiles\nare insufficient for approximating achievable frequency vectors. Furthermore,\nwe prove that even evaluating the frequency vector achieved by a given\nmemoryless profile is computationally hard. This reveals a severe barrier to\nconstructing an efficient synthesis algorithm, even for memoryless profiles.\nNevertheless, we design an efficient and scalable synthesis algorithm for a\nsubclass of full memoryless profiles, and we evaluate this algorithm on a large\nclass of randomly generated instances. The experimental results demonstrate a\nsignificant improvement against a naive algorithm based on strategy sharing.", "published": "2025-05-18 13:16:45", "link": "http://arxiv.org/abs/2505.12406v1", "categories": ["cs.MA"], "primary_category": "cs.MA"}
{"title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "abstract": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.", "published": "2025-05-18 11:28:17", "link": "http://arxiv.org/abs/2505.12371v1", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "cs.AI"}
{"title": "Stability and convergence of multi-product expansion splitting methods with negative weights for semilinear parabolic equations", "abstract": "The operator splitting method has been widely used to solve differential\nequations by splitting the equation into more manageable parts. In this work,\nwe resolves a long-standing problem -- how to establish the stability of\nmulti-product expansion (MPE) splitting methods with negative weights. The\ndifficulty occurs because negative weights in high-order MPE method cause the\nsum of the absolute values of weights larger than one, making standard\nstability proofs fail. In particular, we take the semilinear parabolic equation\nas a typical model and establish the stability of arbitrarily high-order MPE\nsplitting methods with positive time steps but possibly negative weights.\nRigorous convergence analysis is subsequently obtained from the stability\nresult. Extensive numerical experiments validate the stability and accuracy of\nvarious high-order MPE splitting methods, highlighting their efficiency and\nrobustness.", "published": "2025-05-18 15:56:24", "link": "http://arxiv.org/abs/2505.12481v1", "categories": ["math.NA", "cs.NA", "65M12, 65M15"], "primary_category": "math.NA"}
{"title": "High-dimensional Optimization with Low Rank Tensor Sampling and Local Search", "abstract": "We present a novel method called TESALOCS (TEnsor SAmpling and LOCal Search)\nfor multidimensional optimization, combining the strengths of gradient-free\ndiscrete methods and gradient-based approaches. The discrete optimization in\nour method is based on low-rank tensor techniques, which, thanks to their\nlow-parameter representation, enable efficient optimization of high-dimensional\nproblems. For the second part, i.e., local search, any effective gradient-based\nmethod can be used, whether existing (such as quasi-Newton methods) or any\nother developed in the future. Our approach addresses the limitations of\ngradient-based methods, such as getting stuck in local optima; the limitations\nof discrete methods, which cannot be directly applied to continuous functions;\nand limitations of gradient-free methods that require large computational\nbudgets. Note that we are not limited to a single type of low-rank tensor\ndecomposition for discrete optimization, but for illustrative purposes, we\nconsider a specific efficient low-rank tensor train decomposition. For 20\nchallenging 100-dimensional functions, we demonstrate that our method can\nsignificantly outperform results obtained with gradient-based methods like\nConjugate Gradient, BFGS, SLSQP, and other methods, improving them by orders of\nmagnitude with the same computing budget.", "published": "2025-05-18 11:55:40", "link": "http://arxiv.org/abs/2505.12383v1", "categories": ["math.OC", "cs.NA", "math.NA"], "primary_category": "math.OC"}
{"title": "BOLT: Block-Orthonormal Lanczos for Trace estimation of matrix functions", "abstract": "Efficient matrix trace estimation is essential for scalable computation of\nlog-determinants, matrix norms, and distributional divergences. In many\nlarge-scale applications, the matrices involved are too large to store or\naccess in full, making even a single matrix-vector (mat-vec) product\ninfeasible. Instead, one often has access only to small subblocks of the matrix\nor localized matrix-vector products on restricted index sets. Hutch++ achieves\noptimal convergence rate but relies on randomized SVD and assumes full mat-vec\naccess, making it difficult to apply in these constrained settings. We propose\nthe Block-Orthonormal Stochastic Lanczos Quadrature (BOLT), which matches\nHutch++ accuracy with a simpler implementation based on orthonormal block\nprobes and Lanczos iterations. BOLT builds on the Stochastic Lanczos Quadrature\n(SLQ) framework, which combines random probing with Krylov subspace methods to\nefficiently approximate traces of matrix functions, and performs better than\nHutch++ in near flat-spectrum regimes. To address memory limitations and\npartial access constraints, we introduce Subblock SLQ, a variant of BOLT that\noperates only on small principal submatrices. As a result, this framework\nyields a proxy KL divergence estimator and an efficient method for computing\nthe Wasserstein-2 distance between Gaussians - both compatible with low-memory\nand partial-access regimes. We provide theoretical guarantees and demonstrate\nstrong empirical performance across a range of high-dimensional settings.", "published": "2025-05-18 08:04:05", "link": "http://arxiv.org/abs/2505.12289v1", "categories": ["math.NA", "cs.DS", "cs.LG", "cs.NA"], "primary_category": "math.NA"}
{"title": "Kernel Interpolation on Sparse Grids", "abstract": "We consider scattered data approximation on product regions of equal and\ndifferent dimensionality. On each of these regions, we assume quasi-uniform but\nunstructured data sites and construct optimal sparse grids for scattered data\ninterpolation on the product region. For this, we derive new improved error\nestimates for the respective kernel interpolation error by invoking duality\narguments. An efficient algorithm to solve the underlying linear system of\nequations is proposed. The algorithm is based on the sparse grid combination\ntechnique, where a sparse direct solver is used for the elementary anisotropic\ntensor product kernel interpolation problems. The application of the sparse\ndirect solver is facilitated by applying a samplet matrix compression to each\nunivariate kernel matrix, resulting in an essentially sparse representation of\nthe latter. In this way, we obtain a method that is able to deal with large\nproblems up to billions of interpolation points, especially in case of\nreproducing kernels of nonlocal nature. Numerical results are presented to\nqualify and quantify the approach.", "published": "2025-05-18 07:43:19", "link": "http://arxiv.org/abs/2505.12282v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Stacked conformal prediction", "abstract": "We consider the conformalization of a stacked ensemble of predictive models,\nshowing that the potentially simple form of the meta-learner at the top of the\nstack enables a procedure with manageable computational cost that achieves\napproximate marginal validity without requiring the use of a separate\ncalibration sample. Empirical results indicate that the method compares\nfavorably to a standard inductive alternative.", "published": "2025-05-18 23:45:48", "link": "http://arxiv.org/abs/2505.12578v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time", "abstract": "We study the Hamiltonian flow for optimization (HF-opt), which simulates the\nHamiltonian dynamics for some integration time and resets the velocity to $0$\nto decrease the objective function; this is the optimization analogue of the\nHamiltonian Monte Carlo algorithm for sampling. For short integration time,\nHF-opt has the same convergence rates as gradient descent for minimizing\nstrongly and weakly convex functions. We show that by randomizing the\nintegration time in HF-opt, the resulting randomized Hamiltonian flow (RHF)\nachieves accelerated convergence rates in continuous time, similar to the rates\nfor the accelerated gradient flow. We study a discrete-time implementation of\nRHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove\nthat RHGD achieves the same accelerated convergence rates as Nesterov's\naccelerated gradient descent (AGD) for minimizing smooth strongly and weakly\nconvex functions. We provide numerical experiments to demonstrate that RHGD is\ncompetitive with classical accelerated methods such as AGD across all settings\nand outperforms them in certain regimes.", "published": "2025-05-18 21:45:59", "link": "http://arxiv.org/abs/2505.12553v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "primary_category": "math.OC"}
{"title": "Modeling Nonstationary Extremal Dependence via Deep Spatial Deformations", "abstract": "Modeling nonstationarity that often prevails in extremal dependence of\nspatial data can be challenging, and typically requires bespoke or complex\nspatial models that are difficult to estimate. Inference for stationary and\nisotropic models is considerably easier, but the assumptions that underpin\nthese models are rarely met by data observed over large or topographically\ncomplex domains. A possible approach for accommodating nonstationarity in a\nspatial model is to warp the spatial domain to a latent space where\nstationarity and isotropy can be reasonably assumed. Although this approach is\nvery flexible, estimating the warping function can be computationally\nexpensive, and the transformation is not always guaranteed to be bijective,\nwhich may lead to physically unrealistic transformations when the domain folds\nonto itself. We overcome these challenges by developing deep compositional\nspatial models to capture nonstationarity in extremal dependence. Specifically,\nwe focus on modeling high threshold exceedances of process functionals by\nleveraging efficient inference methods for limiting $r$-Pareto processes. A\ndetailed high-dimensional simulation study demonstrates the superior\nperformance of our model in estimating the warped space. We illustrate our\nmethod by modeling UK precipitation extremes and show that we can efficiently\nestimate the extremal dependence structure of data observed at thousands of\nlocations.", "published": "2025-05-18 21:22:00", "link": "http://arxiv.org/abs/2505.12548v1", "categories": ["stat.ME", "stat.ML"], "primary_category": "stat.ME"}
{"title": "Private Statistical Estimation via Truncation", "abstract": "We introduce a novel framework for differentially private (DP) statistical\nestimation via data truncation, addressing a key challenge in DP estimation\nwhen the data support is unbounded. Traditional approaches rely on\nproblem-specific sensitivity analysis, limiting their applicability. By\nleveraging techniques from truncated statistics, we develop computationally\nefficient DP estimators for exponential family distributions, including\nGaussian mean and covariance estimation, achieving near-optimal sample\ncomplexity. Previous works on exponential families only consider bounded or\none-dimensional families. Our approach mitigates sensitivity through truncation\nwhile carefully correcting for the introduced bias using maximum likelihood\nestimation and DP stochastic gradient descent. Along the way, we establish\nimproved uniform convergence guarantees for the log-likelihood function of\nexponential families, which may be of independent interest. Our results provide\na general blueprint for DP algorithm design via truncated statistics.", "published": "2025-05-18 20:38:38", "link": "http://arxiv.org/abs/2505.12541v1", "categories": ["cs.LG", "cs.CR", "cs.DS", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Enforcing Fairness Where It Matters: An Approach Based on Difference-of-Convex Constraints", "abstract": "Fairness in machine learning has become a critical concern, particularly in\nhigh-stakes applications. Existing approaches often focus on achieving full\nfairness across all score ranges generated by predictive models, ensuring\nfairness in both high and low-scoring populations. However, this stringent\nrequirement can compromise predictive performance and may not align with the\npractical fairness concerns of stakeholders. In this work, we propose a novel\nframework for building partially fair machine learning models, which enforce\nfairness within a specific score range of interest, such as the middle range\nwhere decisions are most contested, while maintaining flexibility in other\nregions. We introduce two statistical metrics to rigorously evaluate partial\nfairness within a given score range, such as the top 20%-40% of scores. To\nachieve partial fairness, we propose an in-processing method by formulating the\nmodel training problem as constrained optimization with difference-of-convex\nconstraints, which can be solved by an inexact difference-of-convex algorithm\n(IDCA). We provide the complexity analysis of IDCA for finding a nearly KKT\npoint. Through numerical experiments on real-world datasets, we demonstrate\nthat our framework achieves high predictive performance while enforcing partial\nfairness where it matters most.", "published": "2025-05-18 19:50:01", "link": "http://arxiv.org/abs/2505.12530v1", "categories": ["cs.LG", "math.OC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Nonlinear Laplacians: Tunable principal component analysis under directional prior information", "abstract": "We introduce a new family of algorithms for detecting and estimating a\nrank-one signal from a noisy observation under prior information about that\nsignal's direction, focusing on examples where the signal is known to have\nentries biased to be positive. Given a matrix observation $\\mathbf{Y}$, our\nalgorithms construct a nonlinear Laplacian, another matrix of the form\n$\\mathbf{Y} + \\mathrm{diag}(\\sigma(\\mathbf{Y}\\mathbf{1}))$ for a nonlinear\n$\\sigma: \\mathbb{R} \\to \\mathbb{R}$, and examine the top eigenvalue and\neigenvector of this matrix. When $\\mathbf{Y}$ is the (suitably normalized)\nadjacency matrix of a graph, our approach gives a class of algorithms that\nsearch for unusually dense subgraphs by computing a spectrum of the graph\n\"deformed\" by the degree profile $\\mathbf{Y}\\mathbf{1}$. We study the\nperformance of such algorithms compared to direct spectral algorithms (the case\n$\\sigma = 0$) on models of sparse principal component analysis with biased\nsignals, including the Gaussian planted submatrix problem. For such models, we\nrigorously characterize the critical threshold strength of rank-one signal, as\na function of the nonlinearity $\\sigma$, at which an outlier eigenvalue appears\nin the spectrum of a nonlinear Laplacian. While identifying the $\\sigma$ that\nminimizes this critical signal strength in closed form seems intractable, we\nexplore three approaches to design $\\sigma$ numerically: exhaustively searching\nover simple classes of $\\sigma$, learning $\\sigma$ from datasets of problem\ninstances, and tuning $\\sigma$ using black-box optimization of the critical\nsignal strength. We find both theoretically and empirically that, if $\\sigma$\nis chosen appropriately, then nonlinear Laplacian spectral algorithms\nsubstantially outperform direct spectral algorithms, while avoiding the\ncomplexity of broader classes of algorithms like approximate message passing or\ngeneral first order methods.", "published": "2025-05-18 19:37:47", "link": "http://arxiv.org/abs/2505.12528v1", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.PR", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Stereographic Multi-Try Metropolis Algorithms for Heavy-tailed Sampling", "abstract": "Markov chain Monte Carlo (MCMC) methods for sampling from heavy-tailed\ndistributions present unique challenges, particularly in high dimensions.\nMulti-proposal MCMC algorithms have recently gained attention for their\npotential to improve performance, especially through parallel implementation on\nmodern hardware. This paper introduces a novel family of gradient-free MCMC\nalgorithms that combine the multi-try Metropolis (MTM) with stereographic MCMC\nframework, specifically designed for efficient sampling from heavy-tailed\ntargets. The proposed stereographic multi-try Metropolis (SMTM) algorithm not\nonly outperforms traditional Euclidean MTM and existing stereographic\nrandom-walk Metropolis methods, but also avoids the pathological convergence\nbehavior often observed in MTM and demonstrates strong robustness to tuning.\nThese properties are supported by scaling analysis and extensive simulation\nstudies.", "published": "2025-05-18 16:21:23", "link": "http://arxiv.org/abs/2505.12487v1", "categories": ["stat.CO", "stat.ME", "stat.ML"], "primary_category": "stat.CO"}
{"title": "Multi-modal contrastive learning adapts to intrinsic dimensions of shared latent variables", "abstract": "Multi-modal contrastive learning as a self-supervised representation learning\ntechnique has achieved great success in foundation model training, such as\nCLIP~\\citep{radford2021learning}. In this paper, we study the theoretical\nproperties of the learned representations from multi-modal contrastive learning\nbeyond linear representations and specific data distributions. Our analysis\nreveals that, enabled by temperature optimization, multi-modal contrastive\nlearning not only maximizes mutual information between modalities but also\nadapts to intrinsic dimensions of data, which can be much lower than\nuser-specified dimensions for representation vectors. Experiments on both\nsynthetic and real-world datasets demonstrate the ability of contrastive\nlearning to learn low-dimensional and informative representations, bridging\ntheoretical insights and practical performance.", "published": "2025-05-18 15:49:53", "link": "http://arxiv.org/abs/2505.12473v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Wasserstein Barycenter Gaussian Process based Bayesian Optimization", "abstract": "Gaussian Process based Bayesian Optimization is a widely applied algorithm to\nlearn and optimize under uncertainty, well-known for its sample efficiency.\nHowever, recently -- and more frequently -- research studies have empirically\ndemonstrated that the Gaussian Process fitting procedure at its core could be\nits most relevant weakness. Fitting a Gaussian Process means tuning its\nkernel's hyperparameters to a set of observations, but the common Maximum\nLikelihood Estimation technique, usually appropriate for learning tasks, has\nshown different criticalities in Bayesian Optimization, making theoretical\nanalysis of this algorithm an open challenge. Exploiting the analogy between\nGaussian Processes and Gaussian Distributions, we present a new approach which\nuses a prefixed set of hyperparameters values to fit as many Gaussian Processes\nand then combines them into a unique model as a Wasserstein Barycenter of\nGaussian Processes. We considered both \"easy\" test problems and others known to\nundermine the \\textit{vanilla} Bayesian Optimization algorithm. The new method,\nnamely Wasserstein Barycenter Gausssian Process based Bayesian Optimization\n(WBGP-BO), resulted promising and able to converge to the optimum, contrary to\nvanilla Bayesian Optimization, also on the most \"tricky\" test problems.", "published": "2025-05-18 15:48:18", "link": "http://arxiv.org/abs/2505.12471v1", "categories": ["stat.ML", "cs.LG", "math.OC"], "primary_category": "stat.ML"}
{"title": "A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning", "abstract": "Robust reinforcement learning (RL) under the average-reward criterion is\ncrucial for long-term decision making under potential environment mismatches,\nyet its finite-sample complexity study remains largely unexplored. Existing\nworks offer algorithms with asymptotic guarantees, but the absence of\nfinite-sample analysis hinders its principled understanding and practical\ndeployment, especially in data-limited settings. We close this gap by proposing\nRobust Halpern Iteration (RHI), the first algorithm with provable finite-sample\ncomplexity guarantee. Under standard uncertainty sets -- including\ncontamination sets and $\\ell_p$-norm balls -- RHI attains an $\\epsilon$-optimal\npolicy with near-optimal sample complexity of $\\tilde{\\mathcal\nO}\\left(\\frac{SA\\mathcal H^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote\nthe numbers of states and actions, and $\\mathcal H$ is the robust optimal bias\nspan. This result gives the first polynomial sample complexity guarantee for\nrobust average-reward RL. Moreover, our RHI's independence from prior knowledge\ndistinguishes it from many previous average-reward RL studies. Our work thus\nconstitutes a significant advancement in enhancing the practical applicability\nof robust average-reward methods to complex, real-world problems.", "published": "2025-05-18 15:34:45", "link": "http://arxiv.org/abs/2505.12462v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "High-Dimensional Dynamic Covariance Models with Random Forests", "abstract": "This paper introduces a novel nonparametric method for estimating\nhigh-dimensional dynamic covariance matrices with multiple conditioning\ncovariates, leveraging random forests and supported by robust theoretical\nguarantees. Unlike traditional static methods, our dynamic nonparametric\ncovariance models effectively capture distributional heterogeneity.\nFurthermore, unlike kernel-smoothing methods, which are restricted to a single\nconditioning covariate, our approach accommodates multiple covariates in a\nfully nonparametric framework. To the best of our knowledge, this is the first\nmethod to use random forests for estimating high-dimensional dynamic covariance\nmatrices. In high-dimensional settings, we establish uniform consistency\ntheory, providing nonasymptotic error rates and model selection properties,\neven when the response dimension grows sub-exponentially with the sample size.\nThese results hold uniformly across a range of conditioning variables. The\nmethod's effectiveness is demonstrated through simulations and a stock dataset\nanalysis, highlighting its ability to model complex dynamics in\nhigh-dimensional scenarios.", "published": "2025-05-18 14:33:33", "link": "http://arxiv.org/abs/2505.12444v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Opening the Black Box of Local Projections", "abstract": "Local projections (LPs) are widely used in empirical macroeconomics to\nestimate impulse responses to policy interventions. Yet, in many ways, they are\nblack boxes. It is often unclear what mechanism or historical episodes drive a\nparticular estimate. We introduce a new decomposition of LP estimates into the\nsum of contributions of historical events, which is the product, for each time\nstamp, of a weight and the realization of the response variable. In the least\nsquares case, we show that these weights admit two interpretations. First, they\nrepresent purified and standardized shocks. Second, they serve as proximity\nscores between the projected policy intervention and past interventions in the\nsample. Notably, this second interpretation extends naturally to machine\nlearning methods, many of which yield impulse responses that, while nonlinear\nin predictors, still aggregate past outcomes linearly via proximity-based\nweights. Applying this framework to shocks in monetary and fiscal policy,\nglobal temperature, and the excess bond premium, we find that easily\nidentifiable events-such as Nixon's interference with the Fed, stagflation,\nWorld War II, and the Mount Agung volcanic eruption-emerge as dominant drivers\nof often heavily concentrated impulse response estimates.", "published": "2025-05-18 13:46:35", "link": "http://arxiv.org/abs/2505.12422v1", "categories": ["econ.EM", "stat.ML"], "primary_category": "econ.EM"}
{"title": "Embedding principle of homogeneous neural network for classification problem", "abstract": "Understanding the convergence points and optimization landscape of neural\nnetworks is crucial, particularly for homogeneous networks where\nKarush-Kuhn-Tucker (KKT) points of the associated maximum-margin problem often\ncharacterize solutions. This paper investigates the relationship between such\nKKT points across networks of different widths generated via neuron splitting.\nWe introduce and formalize the \\textbf{KKT point embedding principle},\nestablishing that KKT points of a homogeneous network's max-margin problem\n($P_{\\Phi}$) can be embedded into the KKT points of a larger network's problem\n($P_{\\tilde{\\Phi}}$) via specific linear isometric transformations\ncorresponding to neuron splitting. We rigorously prove this principle holds for\nneuron splitting in both two-layer and deep homogeneous networks. Furthermore,\nwe connect this static embedding to the dynamics of gradient flow training with\nsmooth losses. We demonstrate that trajectories initiated from appropriately\nmapped points remain mapped throughout training and that the resulting\n$\\omega$-limit sets of directions are correspondingly mapped ($T(L(\\theta(0)))\n= L(\\boldsymbol{\\eta}(0))$), thereby preserving the alignment with KKT\ndirections dynamically when directional convergence occurs. Our findings offer\ninsights into the effects of network width, parameter redundancy, and the\nstructural connections between solutions found via optimization in homogeneous\nnetworks of varying sizes.", "published": "2025-05-18 13:43:22", "link": "http://arxiv.org/abs/2505.12419v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Training Latent Diffusion Models with Interacting Particle Algorithms", "abstract": "We introduce a novel particle-based algorithm for end-to-end training of\nlatent diffusion models. We reformulate the training task as minimizing a free\nenergy functional and obtain a gradient flow that does so. By approximating the\nlatter with a system of interacting particles, we obtain the algorithm, which\nwe underpin it theoretically by providing error guarantees. The novel algorithm\ncompares favorably in experiments with previous particle-based methods and\nvariational inference analogues.", "published": "2025-05-18 13:29:07", "link": "http://arxiv.org/abs/2505.12412v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning", "abstract": "With the rapid discovery of emergent phenomena in deep learning and large\nlanguage models, explaining and understanding their cause has become an urgent\nneed. Here, we propose a rigorous entropic-force theory for understanding the\nlearning dynamics of neural networks trained with stochastic gradient descent\n(SGD) and its variants. Building on the theory of parameter symmetries and an\nentropic loss landscape, we show that representation learning is crucially\ngoverned by emergent entropic forces arising from stochasticity and\ndiscrete-time updates. These forces systematically break continuous parameter\nsymmetries and preserve discrete ones, leading to a series of gradient balance\nphenomena that resemble the equipartition property of thermal systems. These\nphenomena, in turn, (a) explain the universal alignment of neural\nrepresentations between AI models and lead to a proof of the Platonic\nRepresentation Hypothesis, and (b) reconcile the seemingly contradictory\nobservations of sharpness- and flatness-seeking behavior of deep learning\noptimization. Our theory and experiments demonstrate that a combination of\nentropic forces and symmetry breaking is key to understanding emergent\nphenomena in deep learning.", "published": "2025-05-18 12:25:42", "link": "http://arxiv.org/abs/2505.12387v1", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "math-ph", "math.MP", "q-bio.NC", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Efficient Optimization with Orthogonality Constraint: a Randomized Riemannian Submanifold Method", "abstract": "Optimization with orthogonality constraints frequently arises in various\nfields such as machine learning. Riemannian optimization offers a powerful\nframework for solving these problems by equipping the constraint set with a\nRiemannian manifold structure and performing optimization intrinsically on the\nmanifold. This approach typically involves computing a search direction in the\ntangent space and updating variables via a retraction operation. However, as\nthe size of the variables increases, the computational cost of the retraction\ncan become prohibitively high, limiting the applicability of Riemannian\noptimization to large-scale problems. To address this challenge and enhance\nscalability, we propose a novel approach that restricts each update on a random\nsubmanifold, thereby significantly reducing the per-iteration complexity. We\nintroduce two sampling strategies for selecting the random submanifolds and\ntheoretically analyze the convergence of the proposed methods. We provide\nconvergence results for general nonconvex functions and functions that satisfy\nRiemannian Polyak-Lojasiewicz condition as well as for stochastic optimization\nsettings. Additionally, we demonstrate how our approach can be generalized to\nquotient manifolds derived from the orthogonal manifold. Extensive experiments\nverify the benefits of the proposed method, across a wide variety of problems.", "published": "2025-05-18 11:46:44", "link": "http://arxiv.org/abs/2505.12378v1", "categories": ["math.OC", "cs.LG", "stat.ML"], "primary_category": "math.OC"}
{"title": "Importance Sampling for Nonlinear Models", "abstract": "While norm-based and leverage-score-based methods have been extensively\nstudied for identifying \"important\" data points in linear models, analogous\ntools for nonlinear models remain significantly underdeveloped. By introducing\nthe concept of the adjoint operator of a nonlinear map, we address this gap and\ngeneralize norm-based and leverage-score-based importance sampling to nonlinear\nsettings. We demonstrate that sampling based on these generalized notions of\nnorm and leverage scores provides approximation guarantees for the underlying\nnonlinear mapping, similar to linear subspace embeddings. As direct\napplications, these nonlinear scores not only reduce the computational\ncomplexity of training nonlinear models by enabling efficient sampling over\nlarge datasets but also offer a novel mechanism for model explainability and\noutlier detection. Our contributions are supported by both theoretical analyses\nand experimental results across a variety of supervised learning scenarios.", "published": "2025-05-18 10:34:39", "link": "http://arxiv.org/abs/2505.12353v1", "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "abstract": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains.", "published": "2025-05-18 04:00:35", "link": "http://arxiv.org/abs/2505.12225v1", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning", "abstract": "Distributionally robust reinforcement learning (DR-RL) has recently gained\nsignificant attention as a principled approach that addresses discrepancies\nbetween training and testing environments. To balance robustness, conservatism,\nand computational traceability, the literature has introduced DR-RL models with\nSA-rectangular and S-rectangular adversaries. While most existing statistical\nanalyses focus on SA-rectangular models, owing to their algorithmic simplicity\nand the optimality of deterministic policies, S-rectangular models more\naccurately capture distributional discrepancies in many real-world applications\nand often yield more effective robust randomized policies. In this paper, we\nstudy the empirical value iteration algorithm for divergence-based\nS-rectangular DR-RL and establish near-optimal sample complexity bounds of\n$\\widetilde{O}(|\\mathcal{S}||\\mathcal{A}|(1-\\gamma)^{-4}\\varepsilon^{-2})$,\nwhere $\\varepsilon$ is the target accuracy, $|\\mathcal{S}|$ and $|\\mathcal{A}|$\ndenote the cardinalities of the state and action spaces, and $\\gamma$ is the\ndiscount factor. To the best of our knowledge, these are the first sample\ncomplexity results for divergence-based S-rectangular models that achieve\noptimal dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, and $\\varepsilon$\nsimultaneously. We further validate this theoretical dependence through\nnumerical experiments on a robust inventory control problem and a theoretical\nworst-case example, demonstrating the fast learning performance of our proposed\nalgorithm.", "published": "2025-05-18 02:35:39", "link": "http://arxiv.org/abs/2505.12202v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Acoustic Field Reconstruction in Tubes via Physics-Informed Neural Networks", "abstract": "This study investigates the application of Physics-Informed Neural Networks\n(PINNs) to inverse problems in acoustic tube analysis, focusing on\nreconstructing acoustic fields from noisy and limited observation data.\nSpecifically, we address scenarios where the radiation model is unknown, and\npressure data is only available at the tube's radiation end. A PINNs framework\nis proposed to reconstruct the acoustic field, along with the PINN Fine-Tuning\nMethod (PINN-FTM) and a traditional optimization method (TOM) for predicting\nradiation model coefficients. The results demonstrate that PINNs can\neffectively reconstruct the tube's acoustic field under noisy conditions, even\nwith unknown radiation parameters. PINN-FTM outperforms TOM by delivering\nbalanced and reliable predictions and exhibiting robust noise-tolerance\ncapabilities.", "published": "2025-05-18 22:07:44", "link": "http://arxiv.org/abs/2505.12557v1", "categories": ["eess.AS", "cs.SD", "eess.SP", "physics.app-ph"], "primary_category": "eess.AS"}
{"title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/.", "published": "2025-05-18 09:58:48", "link": "http://arxiv.org/abs/2505.12332v1", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Unified Architecture and Unsupervised Speech Disentanglement for Speaker Embedding-Free Enrollment in Personalized Speech Enhancement", "abstract": "Conventional speech enhancement (SE) aims to improve speech perception and\nintelligibility by suppressing noise without requiring enrollment speech as\nreference, whereas personalized SE (PSE) addresses the cocktail party problem\nby extracting a target speaker's speech using enrollment speech. While these\ntwo tasks tackle different yet complementary challenges in speech signal\nprocessing, they often share similar model architectures, with PSE\nincorporating an additional branch to process enrollment speech. This suggests\ndeveloping a unified model capable of efficiently handling both SE and PSE\ntasks, thereby simplifying deployment while maintaining high performance.\nHowever, PSE performance is sensitive to variations in enrollment speech, like\nemotional tone, which limits robustness in real-world applications. To address\nthese challenges, we propose two novel models, USEF-PNet and DSEF-PNet, both\nextending our previous SEF-PNet framework. USEF-PNet introduces a unified\narchitecture for processing enrollment speech, integrating SE and PSE into a\nsingle framework to enhance performance and streamline deployment. Meanwhile,\nDSEF-PNet incorporates an unsupervised speech disentanglement approach by\npairing a mixture speech with two different enrollment utterances and enforcing\nconsistency in the extracted target speech. This strategy effectively isolates\nhigh-quality speaker identity information from enrollment speech, reducing\ninterference from factors such as emotion and content, thereby improving PSE\nrobustness. Additionally, we explore a long-short enrollment pairing (LSEP)\nstrategy to examine the impact of enrollment speech duration during both\ntraining and evaluation. Extensive experiments on the Libri2Mix and VoiceBank\nDEMAND demonstrate that our proposed USEF-PNet, DSEF-PNet all achieve\nsubstantial performance improvements, with random enrollment duration\nperforming slightly better.", "published": "2025-05-18 08:01:12", "link": "http://arxiv.org/abs/2505.12288v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis", "abstract": "We propose a shallow flow matching (SFM) mechanism to enhance flow matching\n(FM)-based text-to-speech (TTS) models within a coarse-to-fine generation\nparadigm. SFM constructs intermediate states along the FM paths using coarse\noutput representations. During training, we introduce an orthogonal projection\nmethod to adaptively determine the temporal position of these states, and apply\na principled construction strategy based on a single-segment piecewise flow.\nThe SFM inference starts from the intermediate state rather than pure noise and\nfocuses computation on the latter stages of the FM paths. We integrate SFM into\nmultiple TTS models with a lightweight SFM head. Experiments show that SFM\nconsistently improves the naturalness of synthesized speech in both objective\nand subjective evaluations, while significantly reducing inference when using\nadaptive-step ODE solvers. Demo and codes are available at\nhttps://ydqmkkx.github.io/SFMDemo/.", "published": "2025-05-18 04:15:08", "link": "http://arxiv.org/abs/2505.12226v1", "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "eess.AS"}
{"title": "BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech", "abstract": "Parkinson's disease (PD) poses a growing global health challenge, with\nBangladesh experiencing a notable rise in PD-related mortality. Early detection\nof PD remains particularly challenging in resource-constrained settings, where\nvoice-based analysis has emerged as a promising non-invasive and cost-effective\nalternative. However, existing studies predominantly focus on English or other\nmajor languages; notably, no voice dataset for PD exists for Bengali - posing a\nsignificant barrier to culturally inclusive and accessible healthcare\nsolutions. Moreover, most prior studies employed only a narrow set of acoustic\nfeatures, with limited or no hyperparameter tuning and feature selection\nstrategies, and little attention to model explainability. This restricts the\ndevelopment of a robust and generalizable machine learning model. To address\nthis gap, we present BenSparX, the first Bengali conversational speech dataset\nfor PD detection, along with a robust and explainable machine learning\nframework tailored for early diagnosis. The proposed framework incorporates\ndiverse acoustic feature categories, systematic feature selection methods, and\nstate-of-the-art machine learning algorithms with extensive hyperparameter\noptimization. Furthermore, to enhance interpretability and trust in model\npredictions, the framework incorporates SHAP (SHapley Additive exPlanations)\nanalysis to quantify the contribution of individual acoustic features toward PD\ndetection. Our framework achieves state-of-the-art performance, yielding an\naccuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further\nexternally validated our approach by applying the framework to existing PD\ndatasets in other languages, where it consistently outperforms state-of-the-art\napproaches. To facilitate further research and reproducibility, the dataset has\nbeen made publicly available at https://github.com/Riad071/BenSParX.", "published": "2025-05-18 01:58:36", "link": "http://arxiv.org/abs/2505.12192v1", "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.LG"}
{"title": "Bistatic Sensing in 5G NR", "abstract": "In this work, we propose and evaluate the performance of a 5th generation\n(5G) New Radio (NR) bistatic Integrated Sensing and Communication (ISaC)\nsystem. Unlike the full-duplex monostatic ISaC systems, the bistatic approach\nenables sensing in the current cellular networks without significantly\nmodifying the transceiver design. The sensing utilizes data channels, such as\nthe Physical Uplink Shared Channel (PUSCH), which carries information on the\nair interface. We provide the maximum likelihood estimator for the delay and\nDoppler parameters and derive a lower bound on the Mean Square Error (MSE) for\na single target scenario. Link-level simulations show that it is possible to\nachieve significant throughput while accurately estimating the sensing\nparameters with PUSCH. Moreover, the results reveal an interesting tradeoff\nbetween the number of reference symbols, sensing performance, and throughput in\nthe proposed 5G NR bistatic ISaC system.", "published": "2025-05-18 21:57:53", "link": "http://arxiv.org/abs/2505.12555v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "abstract": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity.", "published": "2025-05-18 20:20:32", "link": "http://arxiv.org/abs/2505.12532v1", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "primary_category": "cs.CV"}
{"title": "Time-Continuous Frequency Allocation for Feeder Links of Mega Constellations with Multi-Antenna Gateway Stations", "abstract": "With the recent rapid advancement of mega low earth orbit (LEO) satellite\nconstellations, multi-antenna gateway station (MAGS) has emerged as a key\nenabler to support extremely high system capacity via massive feeder links.\nHowever, the densification of both space and ground segment leads to reduced\nspatial separation between links, posing unprecedented challenges of\ninterference exacerbation. This paper investigates graph coloring-based\nfrequency allocation methods for interference mitigation (IM) of mega LEO\nsystems. We first reveal the characteristics of MAGS interference pattern and\nformulate the IM problem into a $K$-coloring problem using an adaptive\nthreshold method. Then we propose two tailored graph coloring algorithms,\nnamely Generalized Global (GG) and Clique-Based Tabu Search (CTS), to solve\nthis problem. GG employs a low-complexity greedy conflict avoidance strategy,\nwhile CTS leverages the unique clique structure brought by MAGSs to enhance IM\nperformance. Subsequently, we innovatively modify them to achieve\ntime-continuous frequency allocation, which is crucial to ensure the stability\nof feeder links. Moreover, we further devise two mega constellation\ndecomposition methods to alleviate the complexity burden of satellite\noperators. Finally, we propose a list coloring-based vacant subchannel\nutilization method to further improve spectrum efficiency and system capacity.\nSimulation results on Starlink constellation of the first and second\ngenerations with 34396 satellites demonstrate the effectiveness and superiority\nof the proposed methodology.", "published": "2025-05-18 13:55:13", "link": "http://arxiv.org/abs/2505.12429v1", "categories": ["eess.SY", "cs.SY", "eess.SP"], "primary_category": "eess.SY"}
{"title": "Toward Near-Space Communication Network in the 6G and Beyond Era", "abstract": "Near-space communication network (NS-ComNet), as an indispensable component\nof sixth-generation (6G) and beyond mobile communication systems and the\nspace-air-ground-sea integrated network (SAGSIN), demonstrates unique\nadvantages in wide-area coverage, long-endurance high-altitude operation, and\nhighly flexible deployment. This paper presents a comprehensive review of\nNS-ComNet for 6G and beyond era. Specifically, by contrasting satellite,\nlow-altitude unmanned-aerial-vehicle (UAV), and terrestrial communications, we\nfirst elucidate the background and motivation for integrating NS-ComNet into 6G\nnetwork architectures. Subsequently, we review the developmental status of\nnear-space platforms, including high-altitude balloons, solar-powered UAVs, and\nstratospheric airships, and analyze critical challenges faced by NS-ComNet. To\naddress these challenges, the research focuses on key enabling technologies\nsuch as topology design, resource and handover management, multi-objective\njoint optimization, etc., with particular emphasis on artificial intelligence\ntechniques for NS-ComNet. Finally, envisioning future intelligent collaborative\nnetworks that integrate NS-ComNet with satellite-UAV-terrestrial systems, we\nexplore promising directions. This paper aims to provide technical insights and\nresearch foundations for the systematic construction of NS-ComNet and its deep\ndeployment in the 6G and beyond era.", "published": "2025-05-18 11:48:59", "link": "http://arxiv.org/abs/2505.12379v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Simultaneously Exposing and Jamming Covert Communications via Disco Reconfigurable Intelligent Surfaces", "abstract": "Covert communications provide a stronger privacy protection than cryptography\nand physical-layer security (PLS). However, previous works on covert\ncommunications have implicitly assumed the validity of channel reciprocity,\ni.e., wireless channels remain constant or approximately constant during their\ncoherence time. In this work, we investigate covert communications in the\npresence of a disco RIS (DRIS) deployed by the warden Willie, where the DRIS\nwith random and time-varying reflective coefficients acts as a \"disco ball\",\nintroducing timevarying fully-passive jamming (FPJ). Consequently, the channel\nreciprocity assumption no longer holds. The DRIS not only jams the covert\ntransmissions between Alice and Bob, but also decreases the error probabilities\nof Willie's detections, without either Bob's channel knowledge or additional\njamming power. To quantify the impact of the DRIS on covert communications, we\nfirst design a detection rule for the warden Willie in the presence of\ntime-varying FPJ introduced by the DRIS. Then, we define the detection error\nprobabilities, i.e., the false alarm rate (FAR) and the missed detection rate\n(MDR), as the monitoring performance metrics for Willie's detections, and the\nsignal-to-jamming-plusnoise ratio (SJNR) as a communication performance metric\nfor the covert transmissions between Alice and Bob. Based on the detection\nrule, we derive the detection threshold for the warden Willie to detect whether\ncommunications between Alice and Bob is ongoing, considering the time-varying\nDRIS-based FPJ. Moreover, we conduct theoretical analyses of the FAR and the\nMDR at the warden Willie, as well as SJNR at Bob, and then present unique\nproperties of the DRIS-based FPJ in covert communications. We present numerical\nresults to validate the derived theoretical analyses and evaluate the impact of\nDRIS on covert communications.", "published": "2025-05-18 03:11:42", "link": "http://arxiv.org/abs/2505.12213v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "UAV-Enabled Joint Sensing, Communication, Powering and Backhaul Transmission in Maritime Monitoring Networks", "abstract": "This paper addresses the challenge of energy-constrained maritime monitoring\nnetworks by proposing an unmanned aerial vehicle (UAV)-enabled integrated\nsensing, communication, powering and backhaul transmission scheme with a\ntailored time-division duplex frame structure. Within each time slot, the UAV\nsequentially implements sensing, wireless charging and uplink receiving with\nbuoys, and lastly forwards part of collected data to the central ship via\nbackhaul links. Considering the tight coupling among these functions, we\njointly optimize time allocation, UAV trajectory, UAV-buoy association, and\npower scheduling to maximize the performance of data collection, with the\npractical consideration of sea clutter effects during UAV sensing. A novel\noptimization framework combining alternating optimization, quadratic transform\nand augmented first-order Taylor approximation is developed, which demonstrates\ngood convergence behavior and robustness. Simulation results show that under\nsensing quality-of-service constraint, buoys are able to achieve an average\ndata rate over 22bps/Hz using around 2mW harvested power per active time slot,\nvalidating the scheme's effectiveness for open-sea monitoring. Additionally, it\nis found that under the influence of sea clutters, the optimal UAV trajectory\nalways keeps a certain distance with buoys to strike a balance between sensing\nand other multi-functional transmissions.", "published": "2025-05-18 01:35:06", "link": "http://arxiv.org/abs/2505.12190v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
