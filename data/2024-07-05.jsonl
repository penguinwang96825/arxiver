{"title": "Unwinding Toxic Flow with Partial Information", "abstract": "We consider a central trading desk which aggregates the inflow of clients'\norders with unobserved toxicity, i.e. persistent adverse directionality. The\ndesk chooses either to internalise the inflow or externalise it to the market\nin a cost effective manner. In this model, externalising the order flow creates\nboth price impact costs and an additional market feedback reaction for the\ninflow of trades. The desk's objective is to maximise the daily trading P&L\nsubject to end of the day inventory penalization. We formulate this setting as\na partially observable stochastic control problem and solve it in two steps.\nFirst, we derive the filtered dynamics of the inventory and toxicity, projected\nto the observed filtration, which turns the stochastic control problem into a\nfully observed problem. Then we use a variational approach in order to derive\nthe unique optimal trading strategy. We illustrate our results for various\nscenarios in which the desk is facing momentum and mean-reverting toxicity. Our\nimplementation shows that the P&L performance gap between the partially\nobservable problem and the full information case are of order $0.01\\%$ in all\ntested scenarios.", "published": "2024-07-05 13:52:17", "link": "http://arxiv.org/abs/2407.04510v1", "categories": ["q-fin.TR", "q-fin.MF", "91G10, 49N10, 49N90, 93E20, 93E11, 60G35"], "primary_category": "q-fin.TR"}
{"title": "From 'Showgirls' to 'Performers': Fine-tuning with Gender-inclusive\n  Language for Bias Reduction in LLMs", "abstract": "Gender bias is not only prevalent in Large Language Models (LLMs) and their\ntraining data, but also firmly ingrained into the structural aspects of\nlanguage itself. Therefore, adapting linguistic structures within LLM training\ndata to promote gender-inclusivity can make gender representations within the\nmodel more inclusive. The focus of our work are gender-exclusive affixes in\nEnglish, such as in 'show-girl' or 'man-cave', which can perpetuate gender\nstereotypes and binary conceptions of gender. We use an LLM training dataset to\ncompile a catalogue of 692 gender-exclusive terms along with gender-neutral\nvariants and from this, develop a gender-inclusive fine-tuning dataset, the\n'Tiny Heap'. Fine-tuning three different LLMs with this dataset, we observe an\noverall reduction in gender-stereotyping tendencies across the models. Our\napproach provides a practical method for enhancing gender inclusivity in LLM\ntraining data and contributes to incorporating queer-feminist linguistic\nactivism in bias mitigation research in NLP.", "published": "2024-07-05 11:31:30", "link": "http://arxiv.org/abs/2407.04434v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Generalists vs. Specialists: Evaluating Large Language Models for Urdu", "abstract": "In this paper, we compare general-purpose models, GPT-4-Turbo and Llama-3-8b,\nwith special-purpose models--XLM-Roberta-large, mT5-large, and Llama-3-8b--that\nhave been fine-tuned on specific tasks. We focus on seven classification and\nseven generation tasks to evaluate the performance of these models on Urdu\nlanguage. Urdu has 70 million native speakers, yet it remains underrepresented\nin Natural Language Processing (NLP). Despite the frequent advancements in\nLarge Language Models (LLMs), their performance in low-resource languages,\nincluding Urdu, still needs to be explored. We also conduct a human evaluation\nfor the generation tasks and compare the results with the evaluations performed\nby GPT-4-Turbo, Llama-3-8b and Claude 3.5 Sonnet. We find that special-purpose\nmodels consistently outperform general-purpose models across various tasks. We\nalso find that the evaluation done by GPT-4-Turbo for generation tasks aligns\nmore closely with human evaluation compared to the evaluation the evaluation\ndone by Llama-3-8b. This paper contributes to the NLP community by providing\ninsights into the effectiveness of general and specific-purpose LLMs for\nlow-resource languages.", "published": "2024-07-05 12:09:40", "link": "http://arxiv.org/abs/2407.04459v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Using LLMs to label medical papers according to the CIViC evidence model", "abstract": "We introduce the sequence classification problem CIViC Evidence to the field\nof medical NLP. CIViC Evidence denotes the multi-label classification problem\nof assigning labels of clinical evidence to abstracts of scientific papers\nwhich have examined various combinations of genomic variants, cancer types, and\ntreatment approaches. We approach CIViC Evidence using different language\nmodels: We fine-tune pretrained checkpoints of BERT and RoBERTa on the CIViC\nEvidence dataset and challenge their performance with models of the same\narchitecture which have been pretrained on domain-specific text. In this\ncontext, we find that BiomedBERT and BioLinkBERT can outperform BERT on CIViC\nEvidence (+0.8% and +0.9% absolute improvement in class-support weighted F1\nscore). All transformer-based models show a clear performance edge when\ncompared to a logistic regression trained on bigram tf-idf scores (+1.5 - 2.7%\nimproved F1 score). We compare the aforementioned BERT-like models to OpenAI's\nGPT-4 in a few-shot setting (on a small subset of our original test dataset),\ndemonstrating that, without additional prompt-engineering or fine-tuning, GPT-4\nperforms worse on CIViC Evidence than our six fine-tuned models (66.1% weighted\nF1 score compared to 71.8% for the best fine-tuned model). However, performance\ngets reasonably close to the benchmark of a logistic regression model trained\non bigram tf-idf scores (67.7% weighted F1 score).", "published": "2024-07-05 12:30:01", "link": "http://arxiv.org/abs/2407.04466v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Strengthening Structural Inductive Biases by Pre-training to Perform\n  Syntactic Transformations", "abstract": "Models need appropriate inductive biases to effectively learn from small\namounts of data and generalize systematically outside of the training\ndistribution. While Transformers are highly versatile and powerful, they can\nstill benefit from enhanced structural inductive biases for seq2seq tasks,\nespecially those involving syntactic transformations, such as converting active\nto passive voice or semantic parsing. In this paper, we propose to strengthen\nthe structural inductive bias of a Transformer by intermediate pre-training to\nperform synthetically generated syntactic transformations of dependency trees\ngiven a description of the transformation. Our experiments confirm that this\nhelps with few-shot learning of syntactic tasks such as chunking, and also\nimproves structural generalization for semantic parsing. Our analysis shows\nthat the intermediate pre-training leads to attention heads that keep track of\nwhich syntactic transformation needs to be applied to which token, and that the\nmodel can leverage these attention heads on downstream tasks.", "published": "2024-07-05 14:29:44", "link": "http://arxiv.org/abs/2407.04543v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Testing learning hypotheses using neural networks by manipulating\n  learning data", "abstract": "Although passivization is productive in English, it is not completely general\n-- some exceptions exist (e.g. *One hour was lasted by the meeting). How do\nEnglish speakers learn these exceptions to an otherwise general pattern? Using\nneural network language models as theories of acquisition, we explore the\nsources of indirect evidence that a learner can leverage to learn whether a\nverb can passivize. We first characterize English speakers' judgments of\nexceptions to the passive, confirming that speakers find some verbs more\npassivizable than others. We then show that a neural network language model can\nlearn restrictions to the passive that are similar to those displayed by\nhumans, suggesting that evidence for these exceptions is available in the\nlinguistic input. We test the causal role of two hypotheses for how the\nlanguage model learns these restrictions by training models on modified\ntraining corpora, which we create by altering the existing training corpora to\nremove features of the input implicated by each hypothesis. We find that while\nthe frequency with which a verb appears in the passive significantly affects\nits passivizability, the semantics of the verb does not. This study highlight\nthe utility of altering a language model's training data for answering\nquestions where complete control over a learner's input is vital.", "published": "2024-07-05 15:41:30", "link": "http://arxiv.org/abs/2407.04593v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Efficient Controlled Language Generation with Low-Rank Autoregressive\n  Reward Models", "abstract": "Language models trained on large amounts of data are known to produce\ninappropriate content in some cases and require careful tuning to be used in\nthe real world. We revisit the reward augmented decoding (RAD) approach to\ncontrol the generation from a language model using the scores from a\ntask-specific reward model. We investigate the training objective of RAD, and\nreformulate it as a task of learning a reward matrix. We show that RAD is\ndesigned to support high flexibility when representing the reward matrices,\nwhich leads to a higher computational costs during decoding. However, we\ndemonstrate that RAD does not use its full flexibility. Motivated by this, we\npropose a simpler but more efficient low-rank parametrization of the reward\nmodel enabling fast and effective guided decoding. For the detoxification and\nsentiment control tasks, we show that our low-rank reward model performs on par\nwith the more flexible RAD parametrization, while requiring only a single\nreward model call per generated token.", "published": "2024-07-05 16:11:03", "link": "http://arxiv.org/abs/2407.04615v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toucan: Many-to-Many Translation for 150 African Language Pairs", "abstract": "We address a notable gap in Natural Language Processing (NLP) by introducing\na collection of resources designed to improve Machine Translation (MT) for\nlow-resource languages, with a specific focus on African languages. First, we\nintroduce two language models (LMs), Cheetah-1.2B and Cheetah-3.7B, with 1.2\nbillion and 3.7 billion parameters respectively. Next, we finetune the\naforementioned models to create toucan, an Afrocentric machine translation\nmodel designed to support 156 African language pairs. To evaluate Toucan, we\ncarefully develop an extensive machine translation benchmark, dubbed\nAfroLingu-MT, tailored for evaluating machine translation. Toucan significantly\noutperforms other models, showcasing its remarkable performance on MT for\nAfrican languages. Finally, we train a new model, spBLEU-1K, to enhance\ntranslation evaluation metrics, covering 1K languages, including 614 African\nlanguages. This work aims to advance the field of NLP, fostering cross-cultural\nunderstanding and knowledge exchange, particularly in regions with limited\nlanguage resources such as Africa. The GitHub repository for the Toucan project\nis available at https://github.com/UBC-NLP/Toucan.", "published": "2024-07-05 18:12:19", "link": "http://arxiv.org/abs/2407.04796v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Statistical investigations into the geometry and homology of random\n  programs", "abstract": "AI-supported programming has taken giant leaps with tools such as Meta's\nLlama and openAI's chatGPT. These are examples of stochastic sources of\nprograms and have already greatly influenced how we produce code and teach\nprogramming. If we consider input to such models as a stochastic source, a\nnatural question is, what is the relation between the input and the output\ndistributions, between the chatGPT prompt and the resulting program?\n  In this paper, we will show how the relation between random Python programs\ngenerated from chatGPT can be described geometrically and topologically using\nTree-edit distances between the program's syntax trees and without explicit\nmodeling of the underlying space. A popular approach to studying\nhigh-dimensional samples in a metric space is to use low-dimensional embedding\nusing, e.g., multidimensional scaling. Such methods imply errors depending on\nthe data and dimension of the embedding space. In this article, we propose to\nrestrict such projection methods to purely visualization purposes and instead\nuse geometric summary statistics, methods from spatial point statistics, and\ntopological data analysis to characterize the configurations of random programs\nthat do not rely on embedding approximations. To demonstrate their usefulness,\nwe compare two publicly available models: ChatGPT-4 and TinyLlama, on a simple\nproblem related to image processing.\n  Application areas include understanding how questions should be asked to\nobtain useful programs; measuring how consistently a given large language model\nanswers; and comparing the different large language models as a programming\nassistant. Finally, we speculate that our approach may in the future give new\ninsights into the structure of programming languages.", "published": "2024-07-05 20:25:02", "link": "http://arxiv.org/abs/2407.04854v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Question Answering with Texts and Tables through Deep Reinforcement\n  Learning", "abstract": "This paper proposes a novel architecture to generate multi-hop answers to\nopen domain questions that require information from texts and tables, using the\nOpen Table-and-Text Question Answering dataset for validation and training. One\nof the most common ways to generate answers in this setting is to retrieve\ninformation sequentially, where a selected piece of data helps searching for\nthe next piece. As different models can have distinct behaviors when called in\nthis sequential information search, a challenge is how to select models at each\nstep. Our architecture employs reinforcement learning to choose between\ndifferent state-of-the-art tools sequentially until, in the end, a desired\nanswer is generated. This system achieved an F1-score of 19.03, comparable to\niterative systems in the literature.", "published": "2024-07-05 20:44:01", "link": "http://arxiv.org/abs/2407.04858v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Unified Interpretation of Smoothing Methods for Negative Sampling Loss\n  Functions in Knowledge Graph Embedding", "abstract": "Knowledge Graphs (KGs) are fundamental resources in knowledge-intensive tasks\nin NLP. Due to the limitation of manually creating KGs, KG Completion (KGC) has\nan important role in automatically completing KGs by scoring their links with\nKG Embedding (KGE). To handle many entities in training, KGE relies on Negative\nSampling (NS) loss that can reduce the computational cost by sampling. Since\nthe appearance frequencies for each link are at most one in KGs, sparsity is an\nessential and inevitable problem. The NS loss is no exception. As a solution,\nthe NS loss in KGE relies on smoothing methods like Self-Adversarial Negative\nSampling (SANS) and subsampling. However, it is uncertain what kind of\nsmoothing method is suitable for this purpose due to the lack of theoretical\nunderstanding. This paper provides theoretical interpretations of the smoothing\nmethods for the NS loss in KGE and induces a new NS loss, Triplet Adaptive\nNegative Sampling (TANS), that can cover the characteristics of the\nconventional smoothing methods. Experimental results of TransE, DistMult,\nComplEx, RotatE, HAKE, and HousE on FB15k-237, WN18RR, and YAGO3-10 datasets\nand their sparser subsets show the soundness of our interpretation and\nperformance improvement by our TANS.", "published": "2024-07-05 04:38:17", "link": "http://arxiv.org/abs/2407.04251v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Crafting Large Language Models for Enhanced Interpretability", "abstract": "We introduce the Concept Bottleneck Large Language Model (CB-LLM), a\npioneering approach to creating inherently interpretable Large Language Models\n(LLMs). Unlike traditional black-box LLMs that rely on post-hoc interpretation\nmethods with limited neuron function insights, CB-LLM sets a new standard with\nits built-in interpretability, scalability, and ability to provide clear,\naccurate explanations. This innovation not only advances transparency in\nlanguage models but also enhances their effectiveness. Our unique Automatic\nConcept Correction (ACC) strategy successfully narrows the performance gap with\nconventional black-box LLMs, positioning CB-LLM as a model that combines the\nhigh accuracy of traditional LLMs with the added benefit of clear\ninterpretability -- a feature markedly absent in existing LLMs.", "published": "2024-07-05 07:22:44", "link": "http://arxiv.org/abs/2407.04307v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Leveraging Graph Structures to Detect Hallucinations in Large Language\n  Models", "abstract": "Large language models are extensively applied across a wide range of tasks,\nsuch as customer support, content creation, educational tutoring, and providing\nfinancial guidance. However, a well-known drawback is their predisposition to\ngenerate hallucinations. This damages the trustworthiness of the information\nthese models provide, impacting decision-making and user confidence. We propose\na method to detect hallucinations by looking at the structure of the latent\nspace and finding associations within hallucinated and non-hallucinated\ngenerations. We create a graph structure that connects generations that lie\nclosely in the embedding space. Moreover, we employ a Graph Attention Network\nwhich utilizes message passing to aggregate information from neighboring nodes\nand assigns varying degrees of importance to each neighbor based on their\nrelevance. Our findings show that 1) there exists a structure in the latent\nspace that differentiates between hallucinated and non-hallucinated\ngenerations, 2) Graph Attention Networks can learn this structure and\ngeneralize it to unseen generations, and 3) the robustness of our method is\nenhanced when incorporating contrastive learning. When evaluated against\nevidence-based benchmarks, our model performs similarly without access to\nsearch-based methods.", "published": "2024-07-05 13:08:58", "link": "http://arxiv.org/abs/2407.04485v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Spontaneous Reward Hacking in Iterative Self-Refinement", "abstract": "Language models are capable of iteratively improving their outputs based on\nnatural language feedback, thus enabling in-context optimization of user\npreference. In place of human users, a second language model can be used as an\nevaluator, providing feedback along with numerical ratings which the generator\nattempts to optimize. However, because the evaluator is an imperfect proxy of\nuser preference, this optimization can lead to reward hacking, where the\nevaluator's ratings improve while the generation quality remains stagnant or\neven decreases as judged by actual user preference. The concern of reward\nhacking is heightened in iterative self-refinement where the generator and the\nevaluator use the same underlying language model, in which case the\noptimization pressure can drive them to exploit shared vulnerabilities. Using\nan essay editing task, we show that iterative self-refinement leads to\ndeviation between the language model evaluator and human judgment,\ndemonstrating that reward hacking can occur spontaneously in-context with the\nuse of iterative self-refinement. In addition, we study conditions under which\nreward hacking occurs and observe two factors that affect reward hacking\nseverity: model size and context sharing between the generator and the\nevaluator.", "published": "2024-07-05 14:34:50", "link": "http://arxiv.org/abs/2407.04549v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "VRSD: Rethinking Similarity and Diversity for Retrieval in Large\n  Language Models", "abstract": "Vector retrieval algorithms are essential for semantic queries within the\nrapidly evolving landscape of Large Language Models (LLMs). The ability to\nretrieve vectors that satisfy both similarity and diversity criteria\nsubstantially enhances the performance of LLMs. Although Maximal Marginal\nRelevance (MMR) is widely employed in retrieval scenarios requiring relevance\nand diversity, variations in the parameter $\\lambda$ lead to fluctuations that\ncomplicate the optimization trajectory in vector spaces. This obscures the\ndirection of improvement and highlights the lack of a robust theoretical\nanalysis regarding similarity and diversity constraints in retrieval processes.\nTo address these challenges, this paper introduces a novel approach that\ncharacterizes both constraints through the relationship between the sum vector\nand the query vector. The proximity of these vectors ensures the similarity\nconstraint, while requiring individual vectors within the sum vector to diverge\nin their alignment with the query vector satisfies the diversity constraint. We\nfirst formulate a new combinatorial optimization problem, selecting k vectors\nfrom a candidate set such that their sum vector maximally aligns with the query\nvector, and demonstrate that this problem is NP-complete. This result\nunderscores the inherent difficulty of simultaneously achieving similarity and\ndiversity in vector retrieval, thereby providing a theoretical foundation for\nfuture research. Subsequently, we present the heuristic algorithm Vectors\nRetrieval with Similarity and Diversity, VRSD, which features a clear\noptimization objective and eliminates the need for preset parameters. VRSD also\nachieves a modest reduction in time complexity compared to MMR. Empirical\nvalidation confirms that VRSD significantly outperforms MMR across various\ndatasets.", "published": "2024-07-05 15:08:44", "link": "http://arxiv.org/abs/2407.04573v2", "categories": ["cs.IR", "cs.CL"], "primary_category": "cs.IR"}
{"title": "Written Term Detection Improves Spoken Term Detection", "abstract": "End-to-end (E2E) approaches to keyword search (KWS) are considerably simpler\nin terms of training and indexing complexity when compared to approaches which\nuse the output of automatic speech recognition (ASR) systems. This\nsimplification however has drawbacks due to the loss of modularity. In\nparticular, where ASR-based KWS systems can benefit from external unpaired text\nvia a language model, current formulations of E2E KWS systems have no such\nmechanism. Therefore, in this paper, we propose a multitask training objective\nwhich allows unpaired text to be integrated into E2E KWS without complicating\nindexing and search. In addition to training an E2E KWS model to retrieve text\nqueries from spoken documents, we jointly train it to retrieve text queries\nfrom masked written documents. We show empirically that this approach can\neffectively leverage unpaired text for KWS, with significant improvements in\nsearch performance across a wide variety of languages. We conduct analysis\nwhich indicates that these improvements are achieved because the proposed\nmethod improves document representations for words in the unpaired text.\nFinally, we show that the proposed method can be used for domain adaptation in\nsettings where in-domain paired data is scarce or nonexistent.", "published": "2024-07-05 15:50:47", "link": "http://arxiv.org/abs/2407.04601v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity\n  Recognition Framework", "abstract": "Clinical named entity recognition (NER) aims to retrieve important entities\nwithin clinical narratives. Recent works have demonstrated that large language\nmodels (LLMs) can achieve strong performance in this task. While previous works\nfocus on proprietary LLMs, we investigate how open NER LLMs, trained\nspecifically for entity recognition, perform in clinical NER. Our initial\nexperiment reveals significant contrast in performance for some clinical\nentities and how a simple exploitment on entity types can alleviate this issue.\nIn this paper, we introduce a novel framework, entity decomposition with\nfiltering, or EDF. Our key idea is to decompose the entity recognition task\ninto several retrievals of entity sub-types and then filter them. Our\nexperimental results demonstrate the efficacies of our framework and the\nimprovements across all metrics, models, datasets, and entity types. Our\nanalysis also reveals substantial improvement in recognizing previously missed\nentities using entity decomposition. We further provide a comprehensive\nevaluation of our framework and an in-depth error analysis to pave future\nworks.", "published": "2024-07-05 16:38:23", "link": "http://arxiv.org/abs/2407.04629v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Speculative Speech Recognition by Audio-Prefixed Low-Rank Adaptation of\n  Language Models", "abstract": "This paper explores speculative speech recognition (SSR), where we empower\nconventional automatic speech recognition (ASR) with speculation capabilities,\nallowing the recognizer to run ahead of audio. We introduce a metric for\nmeasuring SSR performance and we propose a model which does SSR by combining a\nRNN-Transducer-based ASR system with an audio-prefixed language model (LM). The\nASR system transcribes ongoing audio and feeds the resulting transcripts, along\nwith an audio-dependent prefix, to the LM, which speculates likely completions\nfor the transcriptions. We experiment with a variety of ASR datasets on which\nshow the efficacy our method and the feasibility of SSR as a method of reducing\nASR latency.", "published": "2024-07-05 16:52:55", "link": "http://arxiv.org/abs/2407.04641v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Pretraining End-to-End Keyword Search with Automatically Discovered\n  Acoustic Units", "abstract": "End-to-end (E2E) keyword search (KWS) has emerged as an alternative and\ncomplimentary approach to conventional keyword search which depends on the\noutput of automatic speech recognition (ASR) systems. While E2E methods greatly\nsimplify the KWS pipeline, they generally have worse performance than their\nASR-based counterparts, which can benefit from pretraining with untranscribed\ndata. In this work, we propose a method for pretraining E2E KWS systems with\nuntranscribed data, which involves using acoustic unit discovery (AUD) to\nobtain discrete units for untranscribed data and then learning to locate\nsequences of such units in the speech. We conduct experiments across languages\nand AUD systems: we show that finetuning such a model significantly outperforms\na model trained from scratch, and the performance improvements are generally\ncorrelated with the quality of the AUD system used for pretraining.", "published": "2024-07-05 17:07:58", "link": "http://arxiv.org/abs/2407.04652v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for\n  Interpreting Neural Networks", "abstract": "Interpretability research takes counterfactual theories of causality for\ngranted. Most causal methods rely on counterfactual interventions to inputs or\nthe activations of particular model components, followed by observations of the\nchange in models' output logits or behaviors. While this yields more faithful\nevidence than correlational methods, counterfactuals nonetheless have key\nproblems that bias our findings in specific and predictable ways. Specifically,\n(i) counterfactual theories do not effectively capture multiple independently\nsufficient causes of the same effect, which leads us to miss certain causes\nentirely; and (ii) counterfactual dependencies in neural networks are generally\nnot transitive, which complicates methods for extracting and interpreting\ncausal graphs from neural networks. We discuss the implications of these\nchallenges for interpretability researchers and propose concrete suggestions\nfor future work.", "published": "2024-07-05 17:53:03", "link": "http://arxiv.org/abs/2407.04690v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language\n  Models", "abstract": "Large language models (LLMs) exhibit hallucinations in long-form\nquestion-answering tasks across various domains and wide applications. Current\nhallucination detection and mitigation datasets are limited in domains and\nsizes, which struggle to scale due to prohibitive labor costs and insufficient\nreliability of existing hallucination annotators. To facilitate the scalable\noversight of LLM hallucinations, this paper introduces an iterative\nself-training framework that simultaneously and progressively scales up the\nhallucination annotation dataset and improves the accuracy of the hallucination\nannotator. Based on the Expectation Maximization (EM) algorithm, in each\niteration, the framework first applies a hallucination annotation pipeline to\nannotate a scaled dataset and then trains a more accurate hallucination\nannotator on the dataset. This new hallucination annotator is adopted in the\nhallucination annotation pipeline used for the next iteration. Extensive\nexperimental results demonstrate that the finally obtained hallucination\nannotator with only 7B parameters surpasses the performance of GPT-4 and\nobtains new state-of-the-art hallucination detection results on HaluEval and\nHalluQA by zero-shot inference. Such an annotator can not only evaluate the\nhallucination levels of various LLMs on the large-scale dataset but also help\nto mitigate the hallucination of LLMs generations, with the Natural Language\nInference (NLI) metric increasing from 25% to 37% on HaluEval.", "published": "2024-07-05 17:56:38", "link": "http://arxiv.org/abs/2407.04693v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Revisiting Structured Sentiment Analysis as Latent Dependency Graph\n  Parsing", "abstract": "Structured Sentiment Analysis (SSA) was cast as a problem of bi-lexical\ndependency graph parsing by prior studies. Multiple formulations have been\nproposed to construct the graph, which share several intrinsic drawbacks: (1)\nThe internal structures of spans are neglected, thus only the boundary tokens\nof spans are used for relation prediction and span recognition, thus hindering\nthe model's expressiveness; (2) Long spans occupy a significant proportion in\nthe SSA datasets, which further exacerbates the problem of internal structure\nneglect. In this paper, we treat the SSA task as a dependency parsing task on\npartially-observed dependency trees, regarding flat spans without determined\ntree annotations as latent subtrees to consider internal structures of spans.\nWe propose a two-stage parsing method and leverage TreeCRFs with a novel\nconstrained inside algorithm to model latent structures explicitly, which also\ntakes advantages of joint scoring graph arcs and headed spans for global\noptimization and inference. Results of extensive experiments on five benchmark\ndatasets reveal that our method performs significantly better than all previous\nbi-lexical methods, achieving new state-of-the-art.", "published": "2024-07-05 18:18:50", "link": "http://arxiv.org/abs/2407.04801v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Towards Enhancing Coherence in Extractive Summarization: Dataset and\n  Experiments with LLMs", "abstract": "Extractive summarization plays a pivotal role in natural language processing\ndue to its wide-range applications in summarizing diverse content efficiently,\nwhile also being faithful to the original content. Despite significant\nadvancement achieved in extractive summarization by Large Language Models\n(LLMs), these summaries frequently exhibit incoherence. An important aspect of\nthe coherent summary is its readability for intended users. Although there have\nbeen many datasets and benchmarks proposed for creating coherent extractive\nsummaries, none of them currently incorporate user intent to improve coherence\nin extractive summarization. Motivated by this, we propose a systematically\ncreated human-annotated dataset consisting of coherent summaries for five\npublicly available datasets and natural language user feedback, offering\nvaluable insights into how to improve coherence in extractive summaries. We\nutilize this dataset for aligning LLMs through supervised fine-tuning with\nnatural language human feedback to enhance the coherence of their generated\nsummaries. Preliminary experiments with Falcon-40B and Llama-2-13B show\nsignificant performance improvements (~10% Rouge-L) in terms of producing\ncoherent summaries. We further utilize human feedback to benchmark results over\ninstruction-tuned models such as FLAN-T5 which resulted in several interesting\nfindings. Data and source code are available at\nhttps://github.com/Mihir3009/Extract-AI.", "published": "2024-07-05 20:25:04", "link": "http://arxiv.org/abs/2407.04855v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Automating Venture Capital: Founder assessment using LLM-powered\n  segmentation, feature engineering and automated labeling techniques", "abstract": "This study explores the application of large language models (LLMs) in\nventure capital (VC) decision-making, focusing on predicting startup success\nbased on founder characteristics. We utilize LLM prompting techniques, like\nchain-of-thought, to generate features from limited data, then extract insights\nthrough statistics and machine learning. Our results reveal potential\nrelationships between certain founder characteristics and success, as well as\ndemonstrate the effectiveness of these characteristics in prediction. This\nframework for integrating ML techniques and LLMs has vast potential for\nimproving startup success prediction, with important implications for VC firms\nseeking to optimize their investment strategies.", "published": "2024-07-05 22:54:13", "link": "http://arxiv.org/abs/2407.04885v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Identifying the Source of Generation for Large Language Models", "abstract": "Large language models (LLMs) memorize text from several sources of documents.\nIn pretraining, LLM trains to maximize the likelihood of text but neither\nreceives the source of the text nor memorizes the source. Accordingly, LLM can\nnot provide document information on the generated content, and users do not\nobtain any hint of reliability, which is crucial for factuality or privacy\ninfringement. This work introduces token-level source identification in the\ndecoding step, which maps the token representation to the reference document.\nWe propose a bi-gram source identifier, a multi-layer perceptron with two\nsuccessive token representations as input for better generalization. We conduct\nextensive experiments on Wikipedia and PG19 datasets with several LLMs, layer\nlocations, and identifier sizes. The overall results show a possibility of\ntoken-level source identifiers for tracing the document, a crucial problem for\nthe safe use of LLMs.", "published": "2024-07-05 08:52:15", "link": "http://arxiv.org/abs/2407.12846v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "ArAIEval Shared Task: Propagandistic Techniques Detection in Unimodal\n  and Multimodal Arabic Content", "abstract": "We present an overview of the second edition of the ArAIEval shared task,\norganized as part of the ArabicNLP 2024 conference co-located with ACL 2024. In\nthis edition, ArAIEval offers two tasks: (i) detection of propagandistic\ntextual spans with persuasion techniques identification in tweets and news\narticles, and (ii) distinguishing between propagandistic and non-propagandistic\nmemes. A total of 14 teams participated in the final evaluation phase, with 6\nand 9 teams participating in Tasks 1 and 2, respectively. Finally, 11 teams\nsubmitted system description papers. Across both tasks, we observed that\nfine-tuning transformer models such as AraBERT was at the core of the majority\nof the participating systems. We provide a description of the task setup,\nincluding a description of the dataset construction and the evaluation setup.\nWe further provide a brief overview of the participating systems. All datasets\nand evaluation scripts are released to the research community\n(https://araieval.gitlab.io/). We hope this will enable further research on\nthese important tasks in Arabic.", "published": "2024-07-05 04:28:46", "link": "http://arxiv.org/abs/2407.04247v1", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "F.2.2; I.2.7"], "primary_category": "cs.CL"}
{"title": "BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks", "abstract": "In the Emotion Recognition in Conversation task, recent investigations have\nutilized attention mechanisms exploring relationships among utterances from\nintra- and inter-speakers for modeling emotional interaction between them.\nHowever, attributes such as speaker personality traits remain unexplored and\npresent challenges in terms of their applicability to other tasks or\ncompatibility with diverse model architectures. Therefore, this work introduces\na novel framework named BiosERC, which investigates speaker characteristics in\na conversation. By employing Large Language Models (LLMs), we extract the\n\"biographical information\" of the speaker within a conversation as\nsupplementary knowledge injected into the model to classify emotional labels\nfor each utterance. Our proposed method achieved state-of-the-art (SOTA)\nresults on three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP,\ndemonstrating the effectiveness and generalization of our model and showcasing\nits potential for adaptation to various conversation analysis tasks. Our source\ncode is available at https://github.com/yingjie7/BiosERC.", "published": "2024-07-05 06:25:34", "link": "http://arxiv.org/abs/2407.04279v1", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous\n  Speech", "abstract": "Prevalent ungrammatical expressions and disfluencies in spontaneous speech\nfrom second language (L2) learners pose unique challenges to Automatic Speech\nRecognition (ASR) systems. However, few datasets are tailored to L2 learner\nspeech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours\nof audio and transcriptions of L2 learners' spontaneous speech. Our linguistic\nanalysis reveals that transcriptions in our dataset contain L2S (L2 learner's\nSpontaneous speech) features, consisting of ungrammatical expressions and\ndisfluencies (e.g., filler words, word repetitions, self-repairs, false\nstarts), significantly more than native speech datasets. Fine-tuning\nwhisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than\nvanilla whisper-small.en. Furthermore, our qualitative analysis indicates that\n54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S\nfeatures, with 48.1% of them being reduced in the fine-tuned model.", "published": "2024-07-05 06:25:54", "link": "http://arxiv.org/abs/2407.04280v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Systematic Evaluation of Online Speaker Diarization Systems Regarding\n  their Latency", "abstract": "In this paper, different online speaker diarization systems are evaluated on\nthe same hardware with the same test data with regard to their latency. The\nlatency is the time span from audio input to the output of the corresponding\nspeaker label. As part of the evaluation, various model combinations within the\nDIART framework, a diarization system based on the online clustering algorithm\nUIS-RNN-SML, and the end-to-end online diarization system FS-EEND are compared.\nThe lowest latency is achieved for the DIART-pipeline with the embedding model\npyannote/embedding and the segmentation model pyannote/segmentation. The\nFS-EEND system shows a similarly good latency. In general there is currently no\npublished research that compares several online diarization systems in terms of\ntheir latency. This makes this work even more relevant.", "published": "2024-07-05 06:54:27", "link": "http://arxiv.org/abs/2407.04293v1", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey", "abstract": "Large Language Models (LLMs) have performed exceptionally in various\ntext-generative tasks, including question answering, translation, code\ncompletion, etc. However, the over-assistance of LLMs has raised the challenge\nof \"jailbreaking\", which induces the model to generate malicious responses\nagainst the usage policy and society by designing adversarial prompts. With the\nemergence of jailbreak attack methods exploiting different vulnerabilities in\nLLMs, the corresponding safety alignment measures are also evolving. In this\npaper, we propose a comprehensive and detailed taxonomy of jailbreak attack and\ndefense methods. For instance, the attack methods are divided into black-box\nand white-box attacks based on the transparency of the target model. Meanwhile,\nwe classify defense methods into prompt-level and model-level defenses.\nAdditionally, we further subdivide these attack and defense methods into\ndistinct sub-classes and present a coherent diagram illustrating their\nrelationships. We also conduct an investigation into the current evaluation\nmethods and compare them from different perspectives. Our findings aim to\ninspire future research and practical implementations in safeguarding LLMs\nagainst adversarial attacks. Above all, although jailbreak remains a\nsignificant concern within the community, we believe that our work enhances the\nunderstanding of this domain and provides a foundation for developing more\nsecure LLMs.", "published": "2024-07-05 06:57:30", "link": "http://arxiv.org/abs/2407.04295v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Romanization Encoding For Multilingual ASR", "abstract": "We introduce romanization encoding for script-heavy languages to optimize\nmultilingual and code-switching Automatic Speech Recognition (ASR) systems. By\nadopting romanization encoding alongside a balanced concatenated tokenizer\nwithin a FastConformer-RNNT framework equipped with a Roman2Char module, we\nsignificantly reduce vocabulary and output dimensions, enabling larger training\nbatches and reduced memory consumption. Our method decouples acoustic modeling\nand language modeling, enhancing the flexibility and adaptability of the\nsystem. In our study, applying this method to Mandarin-English ASR resulted in\na remarkable 63.51% vocabulary reduction and notable performance gains of\n13.72% and 15.03% on SEAME code-switching benchmarks. Ablation studies on\nMandarin-Korean and Mandarin-Japanese highlight our method's strong capability\nto address the complexities of other script-heavy languages, paving the way for\nmore versatile and effective multilingual ASR systems.", "published": "2024-07-05 09:13:24", "link": "http://arxiv.org/abs/2407.04368v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Waterfall: Framework for Robust and Scalable Text Watermarking and\n  Provenance for LLMs", "abstract": "Protecting intellectual property (IP) of text such as articles and code is\nincreasingly important, especially as sophisticated attacks become possible,\nsuch as paraphrasing by large language models (LLMs) or even unauthorized\ntraining of LLMs on copyrighted text to infringe such IP. However, existing\ntext watermarking methods are not robust enough against such attacks nor\nscalable to millions of users for practical implementation. In this paper, we\npropose Waterfall, the first training-free framework for robust and scalable\ntext watermarking applicable across multiple text types (e.g., articles, code)\nand languages supportable by LLMs, for general text and LLM data provenance.\nWaterfall comprises several key innovations, such as being the first to use LLM\nas paraphrasers for watermarking along with a novel combination of techniques\nthat are surprisingly effective in achieving robust verifiability and\nscalability. We empirically demonstrate that Waterfall achieves significantly\nbetter scalability, robust verifiability, and computational efficiency compared\nto SOTA article-text watermarking methods, and showed how it could be directly\napplied to the watermarking of code. We also demonstrated that Waterfall can be\nused for LLM data provenance, where the watermarks of LLM training data can be\ndetected in LLM output, allowing for detection of unauthorized use of data for\nLLM training and potentially enabling model-centric watermarking of\nopen-sourced LLMs which has been a limitation of existing LLM watermarking\nworks. Our code is available at https://github.com/aoi3142/Waterfall.", "published": "2024-07-05 10:51:33", "link": "http://arxiv.org/abs/2407.04411v2", "categories": ["cs.CR", "cs.AI", "cs.CL"], "primary_category": "cs.CR"}
{"title": "TokenVerse: Towards Unifying Speech and NLP Tasks via Transducer-based\n  ASR", "abstract": "In traditional conversational intelligence from speech, a cascaded pipeline\nis used, involving tasks such as voice activity detection, diarization,\ntranscription, and subsequent processing with different NLP models for tasks\nlike semantic endpointing and named entity recognition (NER). Our paper\nintroduces TokenVerse, a single Transducer-based model designed to handle\nmultiple tasks. This is achieved by integrating task-specific tokens into the\nreference text during ASR model training, streamlining the inference and\neliminating the need for separate NLP models. In addition to ASR, we conduct\nexperiments on 3 different tasks: speaker change detection, endpointing, and\nNER. Our experiments on a public and a private dataset show that the proposed\nmethod improves ASR by up to 7.7% in relative WER while outperforming the\ncascaded pipeline approach in individual task performance. Our code is publicly\navailable: https://github.com/idiap/tokenverse-unifying-speech-nlp", "published": "2024-07-05 11:54:38", "link": "http://arxiv.org/abs/2407.04444v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Are Large Language Models Strategic Decision Makers? A Study of\n  Performance and Bias in Two-Player Non-Zero-Sum Games", "abstract": "Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic decision-making abilities remain largely\nunexplored. To fully benefit from the potential of LLMs, it's essential to\nunderstand their ability to function in complex social scenarios. Game theory,\nwhich is already used to understand real-world interactions, provides a good\nframework for assessing these abilities. This work investigates the performance\nand merits of LLMs in canonical game-theoretic two-player non-zero-sum games,\nStag Hunt and Prisoner Dilemma. Our structured evaluation of GPT-3.5,\nGPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making\ndecisions in these games, are affected by at least one of the following\nsystematic biases: positional bias, payoff bias, or behavioural bias. This\nindicates that LLMs do not fully rely on logical reasoning when making these\nstrategic decisions. As a result, it was found that the LLMs' performance drops\nwhen the game configuration is misaligned with the affecting biases. When\nmisaligned, GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B show an average\nperformance drop of 32\\%, 25\\%, 34\\%, and 29\\% respectively in Stag Hunt, and\n28\\%, 16\\%, 34\\%, and 24\\% respectively in Prisoner's Dilemma. Surprisingly,\nGPT-4o (a top-performing LLM across standard benchmarks) suffers the most\nsubstantial performance drop, suggesting that newer models are not addressing\nthese issues. Interestingly, we found that a commonly used method of improving\nthe reasoning capabilities of LLMs, chain-of-thought (CoT) prompting, reduces\nthe biases in GPT-3.5, GPT-4o, and Llama-3-8B but increases the effect of the\nbias in GPT-4-Turbo, indicating that CoT alone cannot fully serve as a robust\nsolution to this problem. We perform several additional experiments, which\nprovide further insight into these observed behaviours.", "published": "2024-07-05 12:30:02", "link": "http://arxiv.org/abs/2407.04467v3", "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "cs.AI"}
{"title": "EventChat: Implementation and user-centric evaluation of a large\n  language model-driven conversational recommender system for exploring leisure\n  events in an SME context", "abstract": "Large language models (LLMs) present an enormous evolution in the strategic\npotential of conversational recommender systems (CRS). Yet to date, research\nhas predominantly focused upon technical frameworks to implement LLM-driven\nCRS, rather than end-user evaluations or strategic implications for firms,\nparticularly from the perspective of a small to medium enterprises (SME) that\nmakeup the bedrock of the global economy. In the current paper, we detail the\ndesign of an LLM-driven CRS in an SME setting, and its subsequent performance\nin the field using both objective system metrics and subjective user\nevaluations. While doing so, we additionally outline a short-form revised\nResQue model for evaluating LLM-driven CRS, enabling replicability in a rapidly\nevolving field. Our results reveal good system performance from a user\nexperience perspective (85.5% recommendation accuracy) but underscore latency,\ncost, and quality issues challenging business viability. Notably, with a median\ncost of $0.04 per interaction and a latency of 5.7s, cost-effectiveness and\nresponse time emerge as crucial areas for achieving a more user-friendly and\neconomically viable LLM-driven CRS for SME settings. One major driver of these\ncosts is the use of an advanced LLM as a ranker within the retrieval-augmented\ngeneration (RAG) technique. Our results additionally indicate that relying\nsolely on approaches such as Prompt-based learning with ChatGPT as the\nunderlying LLM makes it challenging to achieve satisfying quality in a\nproduction environment. Strategic considerations for SMEs deploying an\nLLM-driven CRS are outlined, particularly considering trade-offs in the current\ntechnical landscape.", "published": "2024-07-05 12:42:31", "link": "http://arxiv.org/abs/2407.04472v3", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50", "I.2.7; H.5.2"], "primary_category": "cs.IR"}
{"title": "Controlling Whisper: Universal Acoustic Adversarial Attacks to Control\n  Speech Foundation Models", "abstract": "Speech enabled foundation models, either in the form of flexible speech\nrecognition based systems or audio-prompted large language models (LLMs), are\nbecoming increasingly popular. One of the interesting aspects of these models\nis their ability to perform tasks other than automatic speech recognition (ASR)\nusing an appropriate prompt. For example, the OpenAI Whisper model can perform\nboth speech transcription and speech translation. With the development of\naudio-prompted LLMs there is the potential for even greater control options. In\nthis work we demonstrate that with this greater flexibility the systems can be\nsusceptible to model-control adversarial attacks. Without any access to the\nmodel prompt it is possible to modify the behaviour of the system by\nappropriately changing the audio input. To illustrate this risk, we demonstrate\nthat it is possible to prepend a short universal adversarial acoustic segment\nto any input speech signal to override the prompt setting of an ASR foundation\nmodel. Specifically, we successfully use a universal adversarial acoustic\nsegment to control Whisper to always perform speech translation, despite being\nset to perform speech transcription. Overall, this work demonstrates a new form\nof adversarial attack on multi-tasking speech enabled foundation models that\nneeds to be considered prior to the deployment of this form of model.", "published": "2024-07-05 13:04:31", "link": "http://arxiv.org/abs/2407.04482v2", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "GPT vs RETRO: Exploring the Intersection of Retrieval and\n  Parameter-Efficient Fine-Tuning", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation\n(RAG) have become popular methods for adapting large language models while\nminimizing compute requirements. In this paper, we apply PEFT methods\n(P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer\n(RETRO) and a baseline GPT model across several sizes, ranging from 823 million\nto 48 billion parameters. We show that RETRO models outperform GPT models in\nzero-shot settings due to their unique pre-training process but GPT models have\nhigher performance potential with PEFT. Additionally, our study indicates that\n8B parameter models strike an optimal balance between cost and performance and\nP-tuning lags behind other PEFT techniques. We further provide a comparative\nanalysis between applying PEFT to an Instruction-tuned RETRO model and base\nRETRO model. This work presents the first comprehensive comparison of various\nPEFT methods integrated with RAG, applied to both GPT and RETRO models,\nhighlighting their relative performance.", "published": "2024-07-05 14:16:47", "link": "http://arxiv.org/abs/2407.04528v4", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Performance Analysis of Speech Encoders for Low-Resource SLU and ASR in\n  Tunisian Dialect", "abstract": "Speech encoders pretrained through self-supervised learning (SSL) have\ndemonstrated remarkable performance in various downstream tasks, including\nSpoken Language Understanding (SLU) and Automatic Speech Recognition (ASR). For\ninstance, fine-tuning SSL models for such tasks has shown significant\npotential, leading to improvements in the SOTA performance across challenging\ndatasets. In contrast to existing research, this paper contributes by comparing\nthe effectiveness of SSL approaches in the context of (i) the low-resource\nspoken Tunisian Arabic dialect and (ii) its combination with a low-resource SLU\nand ASR scenario, where only a few semantic annotations are available for\nfine-tuning. We conduct experiments using many SSL speech encoders on the\nTARIC-SLU dataset. We use speech encoders that were pre-trained on either\nmonolingual or multilingual speech data. Some of them have also been refined\nwithout in-domain nor Tunisian data through multimodal supervised\nteacher-student paradigm. This study yields numerous significant findings that\nwe are discussing in this paper.", "published": "2024-07-05 14:21:36", "link": "http://arxiv.org/abs/2407.04533v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "PoPreRo: A New Dataset for Popularity Prediction of Romanian Reddit\n  Posts", "abstract": "We introduce PoPreRo, the first dataset for Popularity Prediction of Romanian\nposts collected from Reddit. The PoPreRo dataset includes a varied compilation\nof post samples from five distinct subreddits of Romania, totaling 28,107 data\nsamples. Along with our novel dataset, we introduce a set of competitive models\nto be used as baselines for future research. Interestingly, the top-scoring\nmodel achieves an accuracy of 61.35% and a macro F1 score of 60.60% on the test\nset, indicating that the popularity prediction task on PoPreRo is very\nchallenging. Further investigations based on few-shot prompting the Falcon-7B\nLarge Language Model also point in the same direction. We thus believe that\nPoPreRo is a valuable resource that can be used to evaluate models on\npredicting the popularity of social media posts in Romanian. We release our\ndataset at https://github.com/ana-rogoz/PoPreRo.", "published": "2024-07-05 14:28:12", "link": "http://arxiv.org/abs/2407.04541v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Not (yet) the whole story: Evaluating Visual Storytelling Requires More\n  than Measuring Coherence, Grounding, and Repetition", "abstract": "Visual storytelling consists in generating a natural language story given a\ntemporally ordered sequence of images. This task is not only challenging for\nmodels, but also very difficult to evaluate with automatic metrics since there\nis no consensus about what makes a story 'good'. In this paper, we introduce a\nnovel method that measures story quality in terms of human likeness regarding\nthree key aspects highlighted in previous work: visual grounding, coherence,\nand repetitiveness. We then use this method to evaluate the stories generated\nby several models, showing that the foundation model LLaVA obtains the best\nresult, but only slightly so compared to TAPM, a 50-times smaller visual\nstorytelling model. Upgrading the visual and language components of TAPM\nresults in a model that yields competitive performance with a relatively low\nnumber of parameters. Finally, we carry out a human evaluation study, whose\nresults suggest that a 'good' story may require more than a human-like level of\nvisual grounding, coherence, and repetition.", "published": "2024-07-05 14:48:15", "link": "http://arxiv.org/abs/2407.04559v4", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning to (Learn at Test Time): RNNs with Expressive Hidden States", "abstract": "Self-attention performs well in long context but has quadratic complexity.\nExisting RNN layers have linear complexity, but their performance in long\ncontext is limited by the expressive power of their hidden states. We present a\npractical framework for instantiating sequence modeling layers with linear\ncomplexity and expressive hidden states. The key idea is to make the hidden\nstate a machine learning model itself, and the update rule a step of\nself-supervised learning. Since the hidden state is updated by training even on\ntest sequences, our layers are called Test-Time Training (TTT) layers. We\nconsider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a\nlinear model and a two-layer MLP respectively. We evaluate our instantiations\nat the scale of 125M to 1.3B parameters, comparing with a strong Transformer\nand Mamba, a modern RNN. Similar to Transformer, TTT-Linear and TTT-MLP can\nkeep reducing perplexity by conditioning on more tokens, while Mamba cannot\nafter 16k context. TTT-MLP still faces challenges in memory I/O, but shows\nlarger potential in long context, pointing to a promising direction for future\nresearch.", "published": "2024-07-05 16:23:20", "link": "http://arxiv.org/abs/2407.04620v3", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Lost in Translation: The Algorithmic Gap Between LMs and the Brain", "abstract": "Language Models (LMs) have achieved impressive performance on various\nlinguistic tasks, but their relationship to human language processing in the\nbrain remains unclear. This paper examines the gaps and overlaps between LMs\nand the brain at different levels of analysis, emphasizing the importance of\nlooking beyond input-output behavior to examine and compare the internal\nprocesses of these systems. We discuss how insights from neuroscience, such as\nsparsity, modularity, internal states, and interactive learning, can inform the\ndevelopment of more biologically plausible language models. Furthermore, we\nexplore the role of scaling laws in bridging the gap between LMs and human\ncognition, highlighting the need for efficiency constraints analogous to those\nin biological systems. By developing LMs that more closely mimic brain\nfunction, we aim to advance both artificial intelligence and our understanding\nof human cognition.", "published": "2024-07-05 17:43:16", "link": "http://arxiv.org/abs/2407.04680v1", "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "primary_category": "q-bio.NC"}
{"title": "Rethinking Visual Prompting for Multimodal Large Language Models with\n  External Knowledge", "abstract": "In recent years, multimodal large language models (MLLMs) have made\nsignificant strides by training on vast high-quality image-text datasets,\nenabling them to generally understand images well. However, the inherent\ndifficulty in explicitly conveying fine-grained or spatially dense information\nin text, such as masks, poses a challenge for MLLMs, limiting their ability to\nanswer questions requiring an understanding of detailed or localized visual\nelements. Drawing inspiration from the Retrieval-Augmented Generation (RAG)\nconcept, this paper proposes a new visual prompt approach to integrate\nfine-grained external knowledge, gleaned from specialized vision models (e.g.,\ninstance segmentation/OCR models), into MLLMs. This is a promising yet\nunderexplored direction for enhancing MLLMs' performance. Our approach diverges\nfrom concurrent works, which transform external knowledge into additional text\nprompts, necessitating the model to indirectly learn the correspondence between\nvisual content and text coordinates. Instead, we propose embedding fine-grained\nknowledge information directly into a spatial embedding map as a visual prompt.\nThis design can be effortlessly incorporated into various MLLMs, such as LLaVA\nand Mipha, considerably improving their visual understanding performance.\nThrough rigorous experiments, we demonstrate that our method can enhance MLLM\nperformance across nine benchmarks, amplifying their fine-grained context-aware\ncapabilities.", "published": "2024-07-05 17:43:30", "link": "http://arxiv.org/abs/2407.04681v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs", "abstract": "AI assistants such as ChatGPT are trained to respond to users by saying, \"I\nam a large language model\". This raises questions. Do such models know that\nthey are LLMs and reliably act on this knowledge? Are they aware of their\ncurrent circumstances, such as being deployed to the public? We refer to a\nmodel's knowledge of itself and its circumstances as situational awareness. To\nquantify situational awareness in LLMs, we introduce a range of behavioral\ntests, based on question answering and instruction following. These tests form\nthe $\\textbf{Situational Awareness Dataset (SAD)}$, a benchmark comprising 7\ntask categories and over 13,000 questions. The benchmark tests numerous\nabilities, including the capacity of LLMs to (i) recognize their own generated\ntext, (ii) predict their own behavior, (iii) determine whether a prompt is from\ninternal evaluation or real-world deployment, and (iv) follow instructions that\ndepend on self-knowledge.\n  We evaluate 16 LLMs on SAD, including both base (pretrained) and chat models.\nWhile all models perform better than chance, even the highest-scoring model\n(Claude 3 Opus) is far from a human baseline on certain tasks. We also observe\nthat performance on SAD is only partially predicted by metrics of general\nknowledge (e.g. MMLU). Chat models, which are finetuned to serve as AI\nassistants, outperform their corresponding base models on SAD but not on\ngeneral knowledge tasks. The purpose of SAD is to facilitate scientific\nunderstanding of situational awareness in LLMs by breaking it down into\nquantitative abilities. Situational awareness is important because it enhances\na model's capacity for autonomous planning and action. While this has potential\nbenefits for automation, it also introduces novel risks related to AI safety\nand control. Code and latest results available at\nhttps://situational-awareness-dataset.org .", "published": "2024-07-05 17:57:02", "link": "http://arxiv.org/abs/2407.04694v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking", "abstract": "Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs.", "published": "2024-07-05 08:37:17", "link": "http://arxiv.org/abs/2407.04752v3", "categories": ["cs.LG", "cs.CL", "cs.NE"], "primary_category": "cs.LG"}
{"title": "Re-Tuning: Overcoming the Compositionality Limits of Large Language\n  Models with Recursive Tuning", "abstract": "We present a new method for large language models to solve compositional\ntasks. Although they have shown strong performance on traditional language\nunderstanding tasks, large language models struggle to solve compositional\ntasks, where the solution depends on solving smaller instances of the same\nproblem. We propose a natural approach to solve compositional tasks\nrecursively. Our method, Re-Tuning, tunes models to break down a problem into\nsubproblems, solve those subproblems, and combine the results. We show that our\nmethod significantly improves model performance on three representative\ncompositional tasks: integer addition, dynamic programming, and parity.\nCompared to state-of-the-art methods that keep intermediate steps towards\nsolving the problems, Re-Tuning achieves significantly higher accuracy and is\nmore GPU memory efficient.", "published": "2024-07-05 18:02:28", "link": "http://arxiv.org/abs/2407.04787v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On Evaluating The Performance of Watermarked Machine-Generated Texts\n  Under Adversarial Attacks", "abstract": "Large Language Models (LLMs) excel in various applications, including text\ngeneration and complex tasks. However, the misuse of LLMs raises concerns about\nthe authenticity and ethical implications of the content they produce, such as\ndeepfake news, academic fraud, and copyright infringement. Watermarking\ntechniques, which embed identifiable markers in machine-generated text, offer a\npromising solution to these issues by allowing for content verification and\norigin tracing. Unfortunately, the robustness of current LLM watermarking\nschemes under potential watermark removal attacks has not been comprehensively\nexplored.\n  In this paper, to fill this gap, we first systematically comb the mainstream\nwatermarking schemes and removal attacks on machine-generated texts, and then\nwe categorize them into pre-text (before text generation) and post-text (after\ntext generation) classes so that we can conduct diversified analyses. In our\nexperiments, we evaluate eight watermarks (five pre-text, three post-text) and\ntwelve attacks (two pre-text, ten post-text) across 87 scenarios. Evaluation\nresults indicate that (1) KGW and Exponential watermarks offer high text\nquality and watermark retention but remain vulnerable to most attacks; (2)\nPost-text attacks are found to be more efficient and practical than pre-text\nattacks; (3) Pre-text watermarks are generally more imperceptible, as they do\nnot alter text fluency, unlike post-text watermarks; (4) Additionally, combined\nattack methods can significantly increase effectiveness, highlighting the need\nfor more robust watermarking solutions. Our study underscores the\nvulnerabilities of current techniques and the necessity for developing more\nresilient schemes.", "published": "2024-07-05 18:09:06", "link": "http://arxiv.org/abs/2407.04794v2", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Associative Recurrent Memory Transformer", "abstract": "This paper addresses the challenge of creating a neural architecture for very\nlong sequences that requires constant time for processing new information at\neach time step. Our approach, Associative Recurrent Memory Transformer (ARMT),\nis based on transformer self-attention for local context and segment-level\nrecurrence for storage of task specific information distributed over a long\ncontext. We demonstrate that ARMT outperfors existing alternatives in\nassociative retrieval tasks and sets a new performance record in the recent\nBABILong multi-task long-context benchmark by answering single-fact questions\nover 50 million tokens with an accuracy of 79.9%. The source code for training\nand evaluation is available on github.", "published": "2024-07-05 19:57:49", "link": "http://arxiv.org/abs/2407.04841v2", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "cs.CL"}
{"title": "MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for\n  Text-to-Image Generation?", "abstract": "While text-to-image models like DALLE-3 and Stable Diffusion are rapidly\nproliferating, they often encounter challenges such as hallucination, bias, and\nthe production of unsafe, low-quality output. To effectively address these\nissues, it is crucial to align these models with desired behaviors based on\nfeedback from a multimodal judge. Despite their significance, current\nmultimodal judges frequently undergo inadequate evaluation of their\ncapabilities and limitations, potentially leading to misalignment and unsafe\nfine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel\nbenchmark which incorporates a comprehensive preference dataset to evaluate\nmultimodal judges in providing feedback for image generation models across four\nkey perspectives: alignment, safety, image quality, and bias. Specifically, we\nevaluate a large variety of multimodal judges including smaller-sized\nCLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and\nclose-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our\npreference dataset. Experiments reveal that close-source VLMs generally provide\nbetter feedback, with GPT-4o outperforming other judges in average. Compared\nwith open-source VLMs, smaller-sized scoring models can provide better feedback\nregarding text-image alignment and image quality, while VLMs provide more\naccurate feedback regarding safety and generation bias due to their stronger\nreasoning capabilities. Further studies in feedback scale reveal that VLM\njudges can generally provide more accurate and stable feedback in natural\nlanguage (Likert-scale) than numerical scales. Notably, human evaluations on\nend-to-end fine-tuned models using separate feedback from these multimodal\njudges provide similar conclusions, further confirming the effectiveness of\nMJ-Bench. All data, code, models are available at\nhttps://huggingface.co/MJ-Bench.", "published": "2024-07-05 20:03:16", "link": "http://arxiv.org/abs/2407.04842v1", "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Aligning Model Evaluations with Human Preferences: Mitigating Token\n  Count Bias in Language Model Assessments", "abstract": "The SLAM paper demonstrated that on-device Small Language Models (SLMs) are a\nviable and cost-effective alternative to API-based Large Language Models\n(LLMs), such as OpenAI's GPT-4, offering comparable performance and stability.\nHowever, SLAM also identified discrepancies between human preferences and\ntraditional auto-evaluators. This follow-up paper explores methods to align LLM\nevaluator preferences with human evaluations by addressing biases, particularly\ntoward higher token counts. We employed Bayesian statistics and a t-test to\nquantify this bias and developed a recalibration procedure to adjust the\nGPTScorer. Our findings significantly improve aligning the recalibrated LLM\nevaluator with human evaluations across multiple use cases. For instance,\nspearman's ranking correlation score in the Recommendation use case improved\nfrom -27.27 to 44.55. These results highlight the importance of accounting for\nbiases in automated evaluations to ensure fair and accurate model assessments.\nThe recalibration process enhances the reliability of automated evaluators,\nleading to better AI models that align with human values and expectations. This\nstudy provides a robust methodology for future research into bias correction\nand emphasizes the feasibility and benefits of developing human-aligned AI\nevaluation systems.", "published": "2024-07-05 09:26:40", "link": "http://arxiv.org/abs/2407.12847v1", "categories": ["cs.CL", "cs.AI", "cs.HC"], "primary_category": "cs.CL"}
{"title": "Towards Automated Functional Equation Proving: A Benchmark Dataset and A\n  Domain-Specific In-Context Agent", "abstract": "Automated Theorem Proving (ATP) faces challenges due to its complexity and\ncomputational demands. Recent work has explored using Large Language Models\n(LLMs) for ATP action selection, but these methods can be resource-intensive.\nThis study introduces FEAS, an agent that enhances the COPRA in-context\nlearning framework within Lean. FEAS refines prompt generation, response\nparsing, and incorporates domain-specific heuristics for functional equations.\nIt introduces FunEq, a curated dataset of functional equation problems with\nvarying difficulty. FEAS outperforms baselines on FunEq, particularly with the\nintegration of domain-specific heuristics. The results demonstrate FEAS's\neffectiveness in generating and formalizing high-level proof strategies into\nLean proofs, showcasing the potential of tailored approaches for specific ATP\nchallenges.", "published": "2024-07-05 15:59:16", "link": "http://arxiv.org/abs/2407.14521v1", "categories": ["cs.AI", "cs.CL", "cs.SC"], "primary_category": "cs.AI"}
{"title": "Challenges and Considerations in Annotating Legal Data: A Comprehensive\n  Overview", "abstract": "The process of annotating data within the legal sector is filled with\ndistinct challenges that differ from other fields, primarily due to the\ninherent complexities of legal language and documentation. The initial task\nusually involves selecting an appropriate raw dataset that captures the\nintricate aspects of legal texts. Following this, extracting text becomes a\ncomplicated task, as legal documents often have complex structures, footnotes,\nreferences, and unique terminology. The importance of data cleaning is\nmagnified in this context, ensuring that redundant information is eliminated\nwhile maintaining crucial legal details and context. Creating comprehensive yet\nstraightforward annotation guidelines is imperative, as these guidelines serve\nas the road map for maintaining uniformity and addressing the subtle nuances of\nlegal terminology. Another critical aspect is the involvement of legal\nprofessionals in the annotation process. Their expertise is valuable in\nensuring that the data not only remains contextually accurate but also adheres\nto prevailing legal standards and interpretations. This paper provides an\nexpanded view of these challenges and aims to offer a foundational\nunderstanding and guidance for researchers and professionals engaged in legal\ndata annotation projects. In addition, we provide links to our created and\nfine-tuned datasets and language models. These resources are outcomes of our\ndiscussed projects and solutions to challenges faced while working on them.", "published": "2024-07-05 21:56:28", "link": "http://arxiv.org/abs/2407.17503v1", "categories": ["cs.IR", "cs.CL", "cs.LG"], "primary_category": "cs.IR"}
{"title": "Semi-supervised Learning for Code-Switching ASR with Large Language\n  Model Filter", "abstract": "Code-switching (CS) phenomenon occurs when words or phrases from different\nlanguages are alternated in a single sentence. Due to data scarcity, building\nan effective CS Automatic Speech Recognition (ASR) system remains challenging.\nIn this paper, we propose to enhance CS-ASR systems by utilizing rich\nunsupervised monolingual speech data within a semi-supervised learning\nframework, particularly when access to CS data is limited. To achieve this, we\nestablish a general paradigm for applying noisy student training (NST) to the\nCS-ASR task. Specifically, we introduce the LLM-Filter, which leverages\nwell-designed prompt templates to activate the correction capability of large\nlanguage models (LLMs) for monolingual data selection and pseudo-labels\nrefinement during NST. Our experiments on the supervised ASRU-CS and\nunsupervised AISHELL-2 and LibriSpeech datasets show that our method not only\nachieves significant improvements over supervised and semi-supervised learning\nbaselines for the CS task, but also attains better performance compared with\nthe fully-supervised oracle upper-bound on the CS English part. Additionally,\nwe further investigate the influence of accent on AESRC dataset and demonstrate\nthat our method can get achieve additional benefits when the monolingual data\ncontains relevant linguistic characteristic.", "published": "2024-07-05 02:14:28", "link": "http://arxiv.org/abs/2407.04219v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Sound Field Estimation Using Deep Kernel Learning Regularized by the\n  Wave Equation", "abstract": "In this work, we introduce a spatio-temporal kernel for Gaussian process (GP)\nregression-based sound field estimation. Notably, GPs have the attractive\nproperty that the sound field is a linear function of the measurements,\nallowing the field to be estimated efficiently from distributed microphone\nmeasurements. However, to ensure analytical tractability, most existing kernels\nfor sound field estimation have been formulated in the frequency domain, formed\nindependently for each frequency. To address the analytical intractability of\nspatio-temporal kernels, we here propose to instead learn the kernel directly\nfrom data by the means of deep kernel learning. Furthermore, to improve the\ngeneralization of the deep kernel, we propose a method for regularizing the\nlearning process using the wave equation. The representational advantages of\nthe deep kernel and the improved generalization obtained by using the wave\nequation regularization are illustrated using numerical simulations.", "published": "2024-07-05 11:08:19", "link": "http://arxiv.org/abs/2407.04417v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "XLSR-Transducer: Streaming ASR for Self-Supervised Pretrained Models", "abstract": "Self-supervised pretrained models exhibit competitive performance in\nautomatic speech recognition on finetuning, even with limited in-domain\nsupervised data. However, popular pretrained models are not suitable for\nstreaming ASR because they are trained with full attention context. In this\npaper, we introduce XLSR-Transducer, where the XLSR-53 model is used as encoder\nin transducer setup. Our experiments on the AMI dataset reveal that the\nXLSR-Transducer achieves 4% absolute WER improvement over Whisper large-v2 and\n8% over a Zipformer transducer model trained from scratch. To enable streaming\ncapabilities, we investigate different attention masking patterns in the\nself-attention computation of transformer layers within the XLSR-53 model. We\nvalidate XLSR-Transducer on AMI and 5 languages from CommonVoice under\nlow-resource scenarios. Finally, with the introduction of attention sinks, we\nreduce the left context by half while achieving a relative 12% improvement in\nWER.", "published": "2024-07-05 11:41:11", "link": "http://arxiv.org/abs/2407.04439v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "From Audio Encoders to Piano Judges: Benchmarking Performance\n  Understanding for Solo Piano", "abstract": "Our study investigates an approach for understanding musical performances\nthrough the lens of audio encoding models, focusing on the domain of solo\nWestern classical piano music. Compared to composition-level attribute\nunderstanding such as key or genre, we identify a knowledge gap in\nperformance-level music understanding, and address three critical tasks:\nexpertise ranking, difficulty estimation, and piano technique detection,\nintroducing a comprehensive Pianism-Labelling Dataset (PLD) for this purpose.\nWe leverage pre-trained audio encoders, specifically Jukebox, Audio-MAE, MERT,\nand DAC, demonstrating varied capabilities in tackling downstream tasks, to\nexplore whether domain-specific fine-tuning enhances capability in capturing\nperformance nuances. Our best approach achieved 93.6\\% accuracy in expertise\nranking, 33.7\\% in difficulty estimation, and 46.7\\% in technique detection,\nwith Audio-MAE as the overall most effective encoder. Finally, we conducted a\ncase study on Chopin Piano Competition data using trained models for expertise\nranking, which highlights the challenge of accurately assessing top-tier\nperformances.", "published": "2024-07-05 14:01:27", "link": "http://arxiv.org/abs/2407.04518v2", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "FA-GAN: Artifacts-free and Phase-aware High-fidelity GAN-based Vocoder", "abstract": "Generative adversarial network (GAN) based vocoders have achieved significant\nattention in speech synthesis with high quality and fast inference speed.\nHowever, there still exist many noticeable spectral artifacts, resulting in the\nquality decline of synthesized speech. In this work, we adopt a novel GAN-based\nvocoder designed for few artifacts and high fidelity, called FA-GAN. To\nsuppress the aliasing artifacts caused by non-ideal upsampling layers in\nhigh-frequency components, we introduce the anti-aliased twin deconvolution\nmodule in the generator. To alleviate blurring artifacts and enrich the\nreconstruction of spectral details, we propose a novel fine-grained\nmulti-resolution real and imaginary loss to assist in the modeling of phase\ninformation. Experimental results reveal that FA-GAN outperforms the compared\napproaches in promoting audio quality and alleviating spectral artifacts, and\nexhibits superior performance when applied to unseen speaker scenarios.", "published": "2024-07-05 15:10:11", "link": "http://arxiv.org/abs/2407.04575v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Who Finds This Voice Attractive? A Large-Scale Experiment Using\n  In-the-Wild Data", "abstract": "This paper introduces CocoNut-Humoresque, an open-source large-scale speech\nlikability corpus that includes speech segments and their per-listener\nlikability scores. Evaluating voice likability is essential to designing\npreferable voices for speech systems, such as dialogue or announcement systems.\nIn this study, we let 885 listeners rate 1800 speech segments of a wide range\nof speakers regarding their likability. When constructing the corpus, we also\ncollected the multiple speaker attributes: genders, ages, and favorite YouTube\nvideos. Therefore, the corpus enables the large-scale statistical analysis of\nvoice likability regarding both speaker and listener factors. This paper\ndescribes the construction methodology and preliminary data analysis to reveal\nthe gender and age biases in voice likability. In addition, the relationship\nbetween the likability and two acoustic features, the fundamental frequencies\nand the x-vectors of given utterances, is also investigated.", "published": "2024-07-05 05:51:58", "link": "http://arxiv.org/abs/2407.04270v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "We Need Variations in Speech Synthesis: Sub-center Modelling for Speaker\n  Embeddings", "abstract": "In speech synthesis, modeling of rich emotions and prosodic variations\npresent in human voice are crucial to synthesize natural speech. Although\nspeaker embeddings have been widely used in personalized speech synthesis as\nconditioning inputs, they are designed to lose variation to optimize speaker\nrecognition accuracy. Thus, they are suboptimal for speech synthesis in terms\nof modeling the rich variations at the output speech distribution. In this\nwork, we propose a novel speaker embedding network which utilizes multiple\nclass centers in the speaker classification training rather than a single class\ncenter as traditional embeddings. The proposed approach introduces variations\nin the speaker embedding while retaining the speaker recognition performance\nsince model does not have to map all of the utterances of a speaker into a\nsingle class center. We apply our proposed embedding in voice conversion task\nand show that our method provides better naturalness and prosody in synthesized\nspeech.", "published": "2024-07-05 06:54:24", "link": "http://arxiv.org/abs/2407.04291v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "PAGURI: a user experience study of creative interaction with\n  text-to-music models", "abstract": "In recent years, text-to-music models have been the biggest breakthrough in\nautomatic music generation. While they are unquestionably a showcase of\ntechnological progress, it is not clear yet how they can be realistically\nintegrated into the artistic practice of musicians and music practitioners.\nThis paper aims to address this question via Prompt Audio Generation User\nResearch Investigation (PAGURI), a user experience study where we leverage\nrecent text-to-music developments to study how musicians and practitioners\ninteract with these systems, evaluating their satisfaction levels. We developed\nan online tool through which users can generate music samples and/or apply\nrecently proposed personalization techniques, based on fine-tuning, to allow\nthe text-to-music model to generate sounds closer to their needs and\npreferences. Using questionnaires, we analyzed how participants interacted with\nthe proposed tool, to understand the effectiveness of text-to-music models in\nenhancing users' creativity. Results show that even if the audio samples\ngenerated and their quality may not always meet user expectations, the majority\nof the participants would incorporate the tool in their creative process.\nFurthermore, they provided insights into potential enhancements for the system\nand its integration into their music practice.", "published": "2024-07-05 08:15:55", "link": "http://arxiv.org/abs/2407.04333v2", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Multitaper mel-spectrograms for keyword spotting", "abstract": "Keyword spotting (KWS) is one of the speech recognition tasks most sensitive\nto the quality of the feature representation. However, the research on KWS has\ntraditionally focused on new model topologies, putting little emphasis on other\naspects like feature extraction. This paper investigates the use of the\nmultitaper technique to create improved features for KWS. The experimental\nstudy is carried out for different test scenarios, windows and parameters,\ndatasets, and neural networks commonly used in embedded KWS applications.\nExperiment results confirm the advantages of using the proposed improved\nfeatures.", "published": "2024-07-05 17:18:25", "link": "http://arxiv.org/abs/2407.04662v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based\n  Speech Recognition", "abstract": "Modern automatic speech recognition (ASR) model is required to accurately\ntranscribe diverse speech signals (from different domains, languages, accents,\netc) given the specific contextual information in various application\nscenarios. Classic end-to-end models fused with extra language models perform\nwell, but mainly in data matching scenarios and are gradually approaching a\nbottleneck. In this work, we introduce Seed-ASR, a large language model (LLM)\nbased speech recognition model. Seed-ASR is developed based on the framework of\naudio conditioned LLM (AcLLM), leveraging the capabilities of LLMs by inputting\ncontinuous speech representations together with contextual information into the\nLLM. Through stage-wise large-scale training and the elicitation of\ncontext-aware capabilities in LLM, Seed-ASR demonstrates significant\nimprovement over end-to-end models on comprehensive evaluation sets, including\nmultiple domains, accents/dialects and languages. Additionally, Seed-ASR can be\nfurther deployed to support specific needs in various scenarios without\nrequiring extra language models. Compared to recently released large ASR\nmodels, Seed-ASR achieves 10%-40% reduction in word (or character, for Chinese)\nerror rates on Chinese and English public test sets, further demonstrating its\npowerful performance.", "published": "2024-07-05 17:38:03", "link": "http://arxiv.org/abs/2407.04675v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "All Neural Low-latency Directional Speech Extraction", "abstract": "We introduce a novel all neural model for low-latency directional speech\nextraction. The model uses direction of arrival (DOA) embeddings from a\npredefined spatial grid, which are transformed and fused into a recurrent\nneural network based speech extraction model. This process enables the model to\neffectively extract speech from a specified DOA. Unlike previous methods that\nrelied on hand-crafted directional features, the proposed model trains DOA\nembeddings from scratch using speech enhancement loss, making it suitable for\nlow-latency scenarios. Additionally, it operates at a high frame rate, taking\nin DOA with each input frame, which brings in the capability of quickly\nadapting to changing scene in highly dynamic real-world scenarios. We provide\nextensive evaluation to demonstrate the model's efficacy in directional speech\nextraction, robustness to DOA mismatch, and its capability to quickly adapt to\nabrupt changes in DOA.", "published": "2024-07-05 22:17:11", "link": "http://arxiv.org/abs/2407.04879v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "MuseBarControl: Enhancing Fine-Grained Control in Symbolic Music\n  Generation through Pre-Training and Counterfactual Loss", "abstract": "Automatically generating symbolic music-music scores tailored to specific\nhuman needs-can be highly beneficial for musicians and enthusiasts. Recent\nstudies have shown promising results using extensive datasets and advanced\ntransformer architectures. However, these state-of-the-art models generally\noffer only basic control over aspects like tempo and style for the entire\ncomposition, lacking the ability to manage finer details, such as control at\nthe level of individual bars. While fine-tuning a pre-trained symbolic music\ngeneration model might seem like a straightforward method for achieving this\nfiner control, our research indicates challenges in this approach. The model\noften fails to respond adequately to new, fine-grained bar-level control\nsignals. To address this, we propose two innovative solutions. First, we\nintroduce a pre-training task designed to link control signals directly with\ncorresponding musical tokens, which helps in achieving a more effective\ninitialization for subsequent fine-tuning. Second, we implement a novel\ncounterfactual loss that promotes better alignment between the generated music\nand the control prompts. Together, these techniques significantly enhance our\nability to control music generation at the bar level, showing a 13.06\\%\nimprovement over conventional methods. Our subjective evaluations also confirm\nthat this enhanced control does not compromise the musical quality of the\noriginal pre-trained generative model.", "published": "2024-07-05 08:08:22", "link": "http://arxiv.org/abs/2407.04331v1", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Mapping Strategy for Interacting with Latent Audio Synthesis Using\n  Artistic Materials", "abstract": "This paper presents a mapping strategy for interacting with the latent spaces\nof generative AI models. Our approach involves using unsupervised feature\nlearning to encode a human control space and mapping it to an audio synthesis\nmodel's latent space. To demonstrate how this mapping strategy can turn\nhigh-dimensional sensor data into control mechanisms of a deep generative\nmodel, we present a proof-of-concept system that uses visual sketches to\ncontrol an audio synthesis model. We draw on emerging discourses in XAIxArts to\ndiscuss how this approach can contribute to XAI in artistic and creative\ncontexts, we also discuss its current limitations and propose future research\ndirections.", "published": "2024-07-05 09:32:44", "link": "http://arxiv.org/abs/2407.04379v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Sound-VECaps: Improving Audio Generation with Visual Enhanced Captions", "abstract": "Generative models have shown significant achievements in audio generation\ntasks. However, existing models struggle with complex and detailed prompts,\nleading to potential performance degradation. We hypothesize that this problem\nstems from the simplicity and scarcity of the training data. This work aims to\ncreate a large-scale audio dataset with rich captions for improving audio\ngeneration models. We first develop an automated pipeline to generate detailed\ncaptions by transforming predicted visual captions, audio captions, and tagging\nlabels into comprehensive descriptions using a Large Language Model (LLM). The\nresulting dataset, Sound-VECaps, comprises 1.66M high-quality audio-caption\npairs with enriched details including audio event orders, occurred places and\nenvironment information. We then demonstrate that training the text-to-audio\ngeneration models with Sound-VECaps significantly improves the performance on\ncomplex prompts. Furthermore, we conduct ablation studies of the models on\nseveral downstream audio-language tasks, showing the potential of Sound-VECaps\nin advancing audio-text representation learning. Our dataset and models are\navailable online from here https://yyua8222.github.io/Sound-VECaps-demo/.", "published": "2024-07-05 11:07:13", "link": "http://arxiv.org/abs/2407.04416v4", "categories": ["cs.SD", "cs.MM", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Resource-Efficient Speech Quality Prediction through Quantization Aware\n  Training and Binary Activation Maps", "abstract": "As speech processing systems in mobile and edge devices become more\ncommonplace, the demand for unintrusive speech quality monitoring increases.\nDeep learning methods provide high-quality estimates of objective and\nsubjective speech quality metrics. However, their significant computational\nrequirements are often prohibitive on resource-constrained devices. To address\nthis issue, we investigated binary activation maps (BAMs) for speech quality\nprediction on a convolutional architecture based on DNSMOS. We show that the\nbinary activation model with quantization aware training matches the predictive\nperformance of the baseline model. It further allows using other compression\ntechniques. Combined with 8-bit weight quantization, our approach results in a\n25-fold memory reduction during inference, while replacing almost all dot\nproducts with summations. Our findings show a path toward substantial resource\nsavings by supporting mixed-precision binary multiplication in hard- and\nsoftware.", "published": "2024-07-05 15:15:00", "link": "http://arxiv.org/abs/2407.04578v1", "categories": ["cs.SD", "cs.NE", "eess.AS"], "primary_category": "cs.SD"}
{"title": "YourMT3+: Multi-instrument Music Transcription with Enhanced Transformer\n  Architectures and Cross-dataset Stem Augmentation", "abstract": "Multi-instrument music transcription aims to convert polyphonic music\nrecordings into musical scores assigned to each instrument. This task is\nchallenging for modeling as it requires simultaneously identifying multiple\ninstruments and transcribing their pitch and precise timing, and the lack of\nfully annotated data adds to the training difficulties. This paper introduces\nYourMT3+, a suite of models for enhanced multi-instrument music transcription\nbased on the recent language token decoding approach of MT3. We enhance its\nencoder by adopting a hierarchical attention transformer in the time-frequency\ndomain and integrating a mixture of experts. To address data limitations, we\nintroduce a new multi-channel decoding method for training with incomplete\nannotations and propose intra- and cross-stem augmentation for dataset mixing.\nOur experiments demonstrate direct vocal transcription capabilities,\neliminating the need for voice separation pre-processors. Benchmarks across ten\npublic datasets show our models' competitiveness with, or superiority to,\nexisting transcription models. Further testing on pop music recordings\nhighlights the limitations of current models. Fully reproducible code and\ndatasets are available with demos at \\url{https://github.com/mimbres/YourMT3}.", "published": "2024-07-05 19:18:33", "link": "http://arxiv.org/abs/2407.04822v3", "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Real-time Timbre Remapping with Differentiable DSP", "abstract": "Timbre is a primary mode of expression in diverse musical contexts. However,\nprevalent audio-driven synthesis methods predominantly rely on pitch and\nloudness envelopes, effectively flattening timbral expression from the input.\nOur approach draws on the concept of timbre analogies and investigates how\ntimbral expression from an input signal can be mapped onto controls for a\nsynthesizer. Leveraging differentiable digital signal processing, our method\nfacilitates direct optimization of synthesizer parameters through a novel\nfeature difference loss. This loss function, designed to learn relative timbral\ndifferences between musical events, prioritizes the subtleties of graded timbre\nmodulations within phrases, allowing for meaningful translations in a timbre\nspace. Using snare drum performances as a case study, where timbral expression\nis central, we demonstrate real-time timbre remapping from acoustic snare drums\nto a differentiable synthesizer modeled after the Roland TR-808.", "published": "2024-07-05 14:32:52", "link": "http://arxiv.org/abs/2407.04547v1", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "cs.SD"}
