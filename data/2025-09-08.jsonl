{"title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts", "abstract": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans.", "published": "2025-09-08 17:59:32", "link": "http://arxiv.org/abs/2509.06952v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models", "abstract": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL", "published": "2025-09-08 17:58:06", "link": "http://arxiv.org/abs/2509.06949v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning", "abstract": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency.", "published": "2025-09-08 17:58:02", "link": "http://arxiv.org/abs/2509.06948v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Interleaving Reasoning for Better Text-to-Image Generation", "abstract": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .", "published": "2025-09-08 17:56:23", "link": "http://arxiv.org/abs/2509.06945v1", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Outcome-based Exploration for LLM Reasoning", "abstract": "Reinforcement learning (RL) has emerged as a powerful method for improving\nthe reasoning abilities of large language models (LLMs). Outcome-based RL,\nwhich rewards policies solely for the correctness of the final answer, yields\nsubstantial accuracy gains but also induces a systematic loss in generation\ndiversity. This collapse undermines real-world performance, where diversity is\ncritical for test-time scaling. We analyze this phenomenon by viewing RL\npost-training as a sampling process and show that, strikingly, RL can reduce\neffective diversity even on the training set relative to the base model. Our\nstudy highlights two central findings: (i) a transfer of diversity degradation,\nwhere reduced diversity on solved problems propagates to unsolved ones, and\n(ii) the tractability of the outcome space, since reasoning tasks admit only a\nlimited set of distinct answers. Motivated by these insights, we propose\noutcome-based exploration, which assigns exploration bonuses according to final\noutcomes. We introduce two complementary algorithms: historical exploration,\nwhich encourages rarely observed answers via UCB-style bonuses, and batch\nexploration, which penalizes within-batch repetition to promote test-time\ndiversity. Experiments on standard competition math with Llama and Qwen models\ndemonstrate that both methods improve accuracy while mitigating diversity\ncollapse. On the theoretical side, we formalize the benefit of outcome-based\nexploration through a new model of outcome-based bandits. Together, these\ncontributions chart a practical path toward RL methods that enhance reasoning\nwithout sacrificing the diversity essential for scalable deployment.", "published": "2025-09-08 17:52:56", "link": "http://arxiv.org/abs/2509.06941v1", "categories": ["cs.LG", "cs.CL"], "primary_category": "cs.LG"}
{"title": "An Ethically Grounded LLM-Based Approach to Insider Threat Synthesis and Detection", "abstract": "Insider threats are a growing organizational problem due to the complexity of\nidentifying their technical and behavioral elements. A large research body is\ndedicated to the study of insider threats from technological, psychological,\nand educational perspectives. However, research in this domain has been\ngenerally dependent on datasets that are static and limited access which\nrestricts the development of adaptive detection models. This study introduces a\nnovel, ethically grounded approach that uses the large language model (LLM)\nClaude Sonnet 3.7 to dynamically synthesize syslog messages, some of which\ncontain indicators of insider threat scenarios. The messages reflect real-world\ndata distributions by being highly imbalanced (1% insider threats). The syslogs\nwere analyzed for insider threats by both Claude Sonnet 3.7 and GPT-4o, with\ntheir performance evaluated through statistical metrics including precision,\nrecall, MCC, and ROC AUC. Sonnet 3.7 consistently outperformed GPT-4o across\nnearly all metrics, particularly in reducing false alarms and improving\ndetection accuracy. The results show strong promise for the use of LLMs in\nsynthetic dataset generation and insider threat detection.", "published": "2025-09-08 17:32:17", "link": "http://arxiv.org/abs/2509.06920v1", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "C.2.0; I.2.7; K.4.1; H.3.3"], "primary_category": "cs.CR"}
{"title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents", "abstract": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.", "published": "2025-09-08 17:28:42", "link": "http://arxiv.org/abs/2509.06917v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "abstract": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.", "published": "2025-09-08 17:20:16", "link": "http://arxiv.org/abs/2509.06902v1", "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "primary_category": "cs.CL"}
{"title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "abstract": "Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages.", "published": "2025-09-08 17:08:42", "link": "http://arxiv.org/abs/2509.06888v1", "categories": ["cs.CL", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction", "abstract": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.", "published": "2025-09-08 17:02:34", "link": "http://arxiv.org/abs/2509.06883v1", "categories": ["cs.CL", "cs.AI", "cs.IR"], "primary_category": "cs.CL"}
{"title": "The Majority is not always right: RL training for solution aggregation", "abstract": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.", "published": "2025-09-08 16:39:38", "link": "http://arxiv.org/abs/2509.06870v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet", "abstract": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge", "published": "2025-09-08 16:28:25", "link": "http://arxiv.org/abs/2509.06861v1", "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "cs.AI"}
{"title": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models", "abstract": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.", "published": "2025-09-08 16:08:31", "link": "http://arxiv.org/abs/2509.06838v1", "categories": ["cs.CL", "cs.CR"], "primary_category": "cs.CL"}
{"title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens", "abstract": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency.", "published": "2025-09-08 16:07:06", "link": "http://arxiv.org/abs/2509.06836v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems", "abstract": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review.", "published": "2025-09-08 15:57:14", "link": "http://arxiv.org/abs/2509.06822v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs", "abstract": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis.", "published": "2025-09-08 15:48:17", "link": "http://arxiv.org/abs/2509.06813v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem", "abstract": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1", "published": "2025-09-08 15:43:29", "link": "http://arxiv.org/abs/2509.06809v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security", "abstract": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications.", "published": "2025-09-08 15:39:17", "link": "http://arxiv.org/abs/2509.06807v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML", "abstract": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.", "published": "2025-09-08 15:38:31", "link": "http://arxiv.org/abs/2509.06806v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint", "abstract": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch.", "published": "2025-09-08 15:24:33", "link": "http://arxiv.org/abs/2509.06795v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction", "abstract": "Intelligent vehicle cockpits present unique challenges for API Agents,\nrequiring coordination across tightly-coupled subsystems that exceed typical\ntask environments' complexity. Traditional Function Calling (FC) approaches\noperate statelessly, requiring multiple exploratory calls to build\nenvironmental awareness before execution, leading to inefficiency and limited\nerror recovery. We introduce VehicleWorld, the first comprehensive environment\nfor the automotive domain, featuring 30 modules, 250 APIs, and 680 properties\nwith fully executable implementations that provide real-time state information\nduring agent execution. This environment enables precise evaluation of vehicle\nagent behaviors across diverse, challenging scenarios. Through systematic\nanalysis, we discovered that direct state prediction outperforms function\ncalling for environmental control. Building on this insight, we propose\nState-based Function Call (SFC), a novel approach that maintains explicit\nsystem state awareness and implements direct state transitions to achieve\ntarget conditions. Experimental results demonstrate that SFC significantly\noutperforms traditional FC approaches, achieving superior execution accuracy\nand reduced latency. We have made all implementation code publicly available on\nGithub https://github.com/OpenMOSS/VehicleWorld.", "published": "2025-09-08 14:28:25", "link": "http://arxiv.org/abs/2509.06736v1", "categories": ["cs.AI", "cs.CL", "cs.RO"], "primary_category": "cs.AI"}
{"title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey", "abstract": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.", "published": "2025-09-08 14:27:23", "link": "http://arxiv.org/abs/2509.06733v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments", "abstract": "Aggregating multiple annotations into a single ground truth label may hide\nvaluable insights into annotator disagreement, particularly in tasks where\nsubjectivity plays a crucial role. In this work, we explore methods for\nidentifying subjectivity in recognizing the human values that motivate\narguments. We evaluate two main approaches: inferring subjectivity through\nvalue prediction vs. directly identifying subjectivity. Our experiments show\nthat direct subjectivity identification significantly improves the model\nperformance of flagging subjective arguments. Furthermore, combining\ncontrastive loss with binary cross-entropy loss does not improve performance\nbut reduces the dependency on per-label subjectivity. Our proposed methods can\nhelp identify arguments that individuals may interpret differently, fostering a\nmore nuanced annotation process.", "published": "2025-09-08 13:59:34", "link": "http://arxiv.org/abs/2509.06704v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data", "abstract": "We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0\ncorpus, targeted at speech modeling tasks with the largest variant containing\n2,695 hours. We combined the sound recordings of the Czech parliamentary\nspeeches with the official transcripts. The recordings were processed with\nWhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our\nprocessing pipeline improves upon the ParCzech 3.0 speech recognition version\nby extracting more data with higher alignment reliability. The dataset is\noffered in three flexible variants: (1) sentence-segmented for automatic speech\nrecognition and speech synthesis tasks with clean boundaries, (2) unsegmented\npreserving original utterance flow across sentences, and (3) a raw-alignment\nfor further custom refinement for other possible tasks. All variants maintain\nthe original metadata and are released under a permissive CC-BY license. The\ndataset is available in the LINDAT repository, with the sentence-segmented and\nunsegmented variants additionally available on Hugging Face.", "published": "2025-09-08 13:35:05", "link": "http://arxiv.org/abs/2509.06675v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations", "abstract": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.", "published": "2025-09-08 13:07:35", "link": "http://arxiv.org/abs/2509.06652v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval", "abstract": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains.", "published": "2025-09-08 13:04:07", "link": "http://arxiv.org/abs/2509.06650v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "Modelling Intertextuality with N-gram Embeddings", "abstract": "Intertextuality is a central tenet in literary studies. It refers to the\nintricate links between literary texts that are created by various types of\nreferences. This paper proposes a new quantitative model of intertextuality to\nenable scalable analysis and network-based insights: perform pairwise\ncomparisons of the embeddings of n-grams from two texts and average their\nresults as the overall intertextuality. Validation on four texts with known\ndegrees of intertextuality, alongside a scalability test on 267 diverse texts,\ndemonstrates the method's effectiveness and efficiency. Network analysis\nfurther reveals centrality and community structures, affirming the approach's\nsuccess in capturing and quantifying intertextual relationships.", "published": "2025-09-08 12:54:38", "link": "http://arxiv.org/abs/2509.06637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation", "abstract": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.", "published": "2025-09-08 12:51:40", "link": "http://arxiv.org/abs/2509.06631v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models", "abstract": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.", "published": "2025-09-08 12:06:09", "link": "http://arxiv.org/abs/2509.06596v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion", "abstract": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion.", "published": "2025-09-08 10:36:49", "link": "http://arxiv.org/abs/2509.06531v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection", "abstract": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines.", "published": "2025-09-08 10:30:58", "link": "http://arxiv.org/abs/2509.06524v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training", "abstract": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.", "published": "2025-09-08 10:24:19", "link": "http://arxiv.org/abs/2509.06518v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents", "abstract": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.", "published": "2025-09-08 10:07:03", "link": "http://arxiv.org/abs/2509.06501v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models", "abstract": "Recent progress in vision-language models (VLMs) has led to impressive\nresults in document understanding tasks, but their high computational demands\nremain a challenge. To mitigate the compute burdens, we propose a lightweight\ntoken pruning framework that filters out non-informative background regions\nfrom document images prior to VLM processing. A binary patch-level classifier\nremoves non-text areas, and a max-pooling refinement step recovers fragmented\ntext regions to enhance spatial coherence. Experiments on real-world document\ndatasets demonstrate that our approach substantially lowers computational\ncosts, while maintaining comparable accuracy.", "published": "2025-09-08 08:12:26", "link": "http://arxiv.org/abs/2509.06415v1", "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "cs.CV"}
{"title": "Do LLMs exhibit the same commonsense capabilities across languages?", "abstract": "This paper explores the multilingual commonsense generation abilities of\nLarge Language Models (LLMs). To facilitate this investigation, we introduce\nMULTICOM, a novel benchmark that extends the COCOTEROS dataset to four\nlanguages: English, Spanish, Dutch, and Valencian. The task involves generating\na commonsensical sentence that includes a given triplet of words. We evaluate a\nrange of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and\nSalamandra, on this benchmark. Our evaluation combines automatic metrics,\nLLM-as-a-judge approaches (using Prometheus and JudgeLM), and human\nannotations. Results consistently show superior performance in English, with\nsignificantly lower performance in less-resourced languages. While contextual\nsupport yields mixed results, it tends to benefit underrepresented languages.\nThese findings underscore the current limitations of LLMs in multilingual\ncommonsense generation. The dataset is publicly available at\nhttps://huggingface.co/datasets/gplsi/MULTICOM.", "published": "2025-09-08 07:47:00", "link": "http://arxiv.org/abs/2509.06401v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "PL-CA: A Parametric Legal Case Augmentation Framework", "abstract": "Conventional RAG is considered one of the most effective methods for\naddressing model knowledge insufficiency and hallucination, particularly in the\njudicial domain that requires high levels of knowledge rigor, logical\nconsistency, and content integrity. However, the conventional RAG method only\ninjects retrieved documents directly into the model's context, which severely\nconstrains models due to their limited context windows and introduces\nadditional computational overhead through excessively long contexts, thereby\ndisrupting models' attention and degrading performance on downstream tasks.\nMoreover, many existing benchmarks lack expert annotation and focus solely on\nindividual downstream tasks while real-world legal scenarios consist of\nmultiple mixed legal tasks, indicating conventional benchmarks' inadequacy for\nreflecting models' true capabilities. To address these limitations, we propose\nPL-CA, which introduces a parametric RAG (P-RAG) framework to perform data\naugmentation on corpus knowledge and encode this legal knowledge into\nparametric vectors, and then integrates this parametric knowledge into the\nLLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context\npressure. Additionally, we also construct a multi-task legal dataset comprising\nmore than 2000 training and test instances, which are all expert-annotated and\nmanually verified. We conduct our experiments on our dataset, and the\nexperimental results demonstrate that our method reduces the overhead\nassociated with excessively long contexts while maintaining competitive\nperformance on downstream tasks compared to conventional RAG. Our code and\ndataset are provided in the appendix.", "published": "2025-09-08 06:08:06", "link": "http://arxiv.org/abs/2509.06356v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?", "abstract": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks.", "published": "2025-09-08 05:45:37", "link": "http://arxiv.org/abs/2509.06350v1", "categories": ["cs.CL", "cs.AI", "cs.CR"], "primary_category": "cs.CL"}
{"title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents", "abstract": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies.", "published": "2025-09-08 02:07:09", "link": "http://arxiv.org/abs/2509.06283v1", "categories": ["cs.AI", "cs.CL"], "primary_category": "cs.AI"}
{"title": "No Encore: Unlearning as Opt-Out in Music Generation", "abstract": "AI music generation is rapidly emerging in the creative industries, enabling\nintuitive music generation from textual descriptions. However, these systems\npose risks in exploitation of copyrighted creations, raising ethical and legal\nconcerns. In this paper, we present preliminary results on the first\napplication of machine unlearning techniques from an ongoing research to\nprevent inadvertent usage of creative content. Particularly, we explore\nexisting methods in machine unlearning to a pre-trained Text-to-Music (TTM)\nbaseline and analyze their efficacy in unlearning pre-trained datasets without\nharming model performance. Through our experiments, we provide insights into\nthe challenges of applying unlearning in music generation, offering a\nfoundational analysis for future works on the application of unlearning for\nmusic generative models.", "published": "2025-09-08 01:56:51", "link": "http://arxiv.org/abs/2509.06277v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers", "abstract": "Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a hierarchical plug-and-play pruning-and-recovering\nframework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient\ntransformer-based 3D human pose estimation from videos. H$_{2}$OT begins with\nprogressively pruning pose tokens of redundant frames and ends with recovering\nfull-length sequences, resulting in a few pose tokens in the intermediate\ntransformer blocks and thus improving the model efficiency. It works with two\nkey modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module\n(TRM). TPM dynamically selects a few representative tokens to eliminate the\nredundancy of video frames, while TRM restores the detailed spatio-temporal\ninformation based on the selected tokens, thereby expanding the network output\nto the original full-length temporal resolution for fast inference. Our method\nis general-purpose: it can be easily incorporated into common VPT models on\nboth seq2seq and seq2frame pipelines while effectively accommodating different\ntoken pruning and recovery strategies. In addition, our H$_{2}$OT reveals that\nmaintaining the full pose sequence is unnecessary, and a few pose tokens of\nrepresentative frames can achieve both high efficiency and estimation accuracy.\nExtensive experiments on multiple benchmark datasets demonstrate both the\neffectiveness and efficiency of the proposed method. Code and models are\navailable at https://github.com/NationalGAILab/HoT.", "published": "2025-09-08 17:59:59", "link": "http://arxiv.org/abs/2509.06956v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments", "abstract": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com", "published": "2025-09-08 17:59:35", "link": "http://arxiv.org/abs/2509.06953v1", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "cs.RO"}
{"title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference", "abstract": "Recent studies have demonstrated the effectiveness of directly aligning\ndiffusion models with human preferences using differentiable reward. However,\nthey exhibit two primary challenges: (1) they rely on multistep denoising with\ngradient computation for reward scoring, which is computationally expensive,\nthus restricting optimization to only a few diffusion steps; (2) they often\nneed continuous offline adaptation of reward models in order to achieve desired\naesthetic quality, such as photorealism or precise lighting effects. To address\nthe limitation of multistep denoising, we propose Direct-Align, a method that\npredefines a noise prior to effectively recover original images from any time\nsteps via interpolation, leveraging the equation that diffusion states are\ninterpolations between noise and target images, which effectively avoids\nover-optimization in late timesteps. Furthermore, we introduce Semantic\nRelative Preference Optimization (SRPO), in which rewards are formulated as\ntext-conditioned signals. This approach enables online adjustment of rewards in\nresponse to positive and negative prompt augmentation, thereby reducing the\nreliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model\nwith optimized denoising and online reward adjustment, we improve its\nhuman-evaluated realism and aesthetic quality by over 3x.", "published": "2025-09-08 17:54:08", "link": "http://arxiv.org/abs/2509.06942v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers", "abstract": "As generative AI systems become competent and democratized in science,\nbusiness, and government, deeper insight into their failure modes now poses an\nacute need. The occasional volatility in their behavior, such as the propensity\nof transformer models to hallucinate, impedes trust and adoption of emerging AI\nsolutions in high-stakes areas. In the present work, we establish how and when\nhallucinations arise in pre-trained transformer models through concept\nrepresentations captured by sparse autoencoders, under scenarios with\nexperimentally controlled uncertainty in the input space. Our systematic\nexperiments reveal that the number of semantic concepts used by the transformer\nmodel grows as the input information becomes increasingly unstructured. In the\nface of growing uncertainty in the input space, the transformer model becomes\nprone to activate coherent yet input-insensitive semantic features, leading to\nhallucinated output. At its extreme, for pure-noise inputs, we identify a wide\nvariety of robustly triggered and meaningful concepts in the intermediate\nactivations of pre-trained transformer models, whose functional integrity we\nconfirm through targeted steering. We also show that hallucinations in the\noutput of a transformer model can be reliably predicted from the concept\npatterns embedded in transformer layer activations. This collection of insights\non transformer internal processing mechanics has immediate consequences for\naligning AI models with human values, AI safety, opening the attack surface for\npotential adversarial attacks, and providing a basis for automatic\nquantification of a model's hallucination risk.", "published": "2025-09-08 17:50:45", "link": "http://arxiv.org/abs/2509.06938v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities", "abstract": "Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit\nfundamental limitations: inadequate conceptual grounding leading to\nnon-robustness against novel attacks; limited instructibility impeding\nanalyst-guided adaptation; and misalignment with cybersecurity objectives.\nNeuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize\ncybersecurity AI. However, there is no systematic understanding of this\nemerging approach. These hybrid systems address critical cybersecurity\nchallenges by combining neural pattern recognition with symbolic reasoning,\nenabling enhanced threat understanding while introducing concerning autonomous\noffensive capabilities that reshape threat landscapes. In this survey, we\nsystematically characterize this field by analyzing 127 publications spanning\n2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A)\nframework to evaluate these systems, focusing on both cyber defense and cyber\noffense across network security, malware analysis, and cyber operations. Our\nanalysis shows advantages of multi-agent NeSy architectures and identifies\ncritical implementation challenges including standardization gaps,\ncomputational complexity, and human-AI collaboration requirements that\nconstrain deployment. We show that causal reasoning integration is the most\ntransformative advancement, enabling proactive defense beyond correlation-based\napproaches. Our findings highlight dual-use implications where autonomous\nsystems demonstrate substantial capabilities in zero-day exploitation while\nachieving significant cost reductions, altering threat dynamics. We provide\ninsights and future research directions, emphasizing the urgent need for\ncommunity-driven standardization frameworks and responsible development\npractices that ensure advancement serves defensive cybersecurity objectives\nwhile maintaining societal alignment.", "published": "2025-09-08 17:33:59", "link": "http://arxiv.org/abs/2509.06921v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Tackling the Noisy Elephant in the Room: Label Noise-robust Out-of-Distribution Detection via Loss Correction and Low-rank Decomposition", "abstract": "Robust out-of-distribution (OOD) detection is an indispensable component of\nmodern artificial intelligence (AI) systems, especially in safety-critical\napplications where models must identify inputs from unfamiliar classes not seen\nduring training. While OOD detection has been extensively studied in the\nmachine learning literature--with both post hoc and training-based\napproaches--its effectiveness under noisy training labels remains\nunderexplored. Recent studies suggest that label noise can significantly\ndegrade OOD performance, yet principled solutions to this issue are lacking. In\nthis work, we demonstrate that directly combining existing label noise-robust\nmethods with OOD detection strategies is insufficient to address this critical\nchallenge. To overcome this, we propose a robust OOD detection framework that\nintegrates loss correction techniques from the noisy label learning literature\nwith low-rank and sparse decomposition methods from signal processing.\nExtensive experiments on both synthetic and real-world datasets demonstrate\nthat our method significantly outperforms the state-of-the-art OOD detection\ntechniques, particularly under severe noisy label settings.", "published": "2025-09-08 17:28:59", "link": "http://arxiv.org/abs/2509.06918v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers", "abstract": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin.", "published": "2025-09-08 17:05:53", "link": "http://arxiv.org/abs/2509.06885v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "AxelSMOTE: An Agent-Based Oversampling Algorithm for Imbalanced Classification", "abstract": "Class imbalance in machine learning poses a significant challenge, as skewed\ndatasets often hinder performance on minority classes. Traditional oversampling\ntechniques, which are commonly used to alleviate class imbalance, have several\ndrawbacks: they treat features independently, lack similarity-based controls,\nlimit sample diversity, and fail to manage synthetic variety effectively. To\novercome these issues, we introduce AxelSMOTE, an innovative agent-based\napproach that views data instances as autonomous agents engaging in complex\ninteractions. Based on Axelrod's cultural dissemination model, AxelSMOTE\nimplements four key innovations: (1) trait-based feature grouping to preserve\ncorrelations; (2) a similarity-based probabilistic exchange mechanism for\nmeaningful interactions; (3) Beta distribution blending for realistic\ninterpolation; and (4) controlled diversity injection to avoid overfitting.\nExperiments on eight imbalanced datasets demonstrate that AxelSMOTE outperforms\nstate-of-the-art sampling methods while maintaining computational efficiency.", "published": "2025-09-08 16:47:33", "link": "http://arxiv.org/abs/2509.06875v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL", "abstract": "A hallmark of modern large-scale machine learning techniques is the use of\ntraining objectives that provide dense supervision to intermediate\ncomputations, such as teacher forcing the next token in language models or\ndenoising step-by-step in diffusion models. This enables models to learn\ncomplex functions in a generalizable manner. Motivated by this observation, we\ninvestigate the benefits of iterative computation for temporal difference (TD)\nmethods in reinforcement learning (RL). Typically they represent value\nfunctions in a monolithic fashion, without iterative compute. We introduce floq\n(flow-matching Q-functions), an approach that parameterizes the Q-function\nusing a velocity field and trains it using techniques from flow-matching,\ntypically used in generative modeling. This velocity field underneath the flow\nis trained using a TD-learning objective, which bootstraps from values produced\nby a target velocity field, computed by running multiple steps of numerical\nintegration. Crucially, floq allows for more fine-grained control and scaling\nof the Q-function capacity than monolithic architectures, by appropriately\nsetting the number of integration steps. Across a suite of challenging offline\nRL benchmarks and online fine-tuning tasks, floq improves performance by nearly\n1.8x. floq scales capacity far better than standard TD-learning architectures,\nhighlighting the potential of iterative computation for value learning.", "published": "2025-09-08 16:31:09", "link": "http://arxiv.org/abs/2509.06863v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Disentangling Interaction and Bias Effects in Opinion Dynamics of Large Language Models", "abstract": "Large Language Models are increasingly used to simulate human opinion\ndynamics, yet the effect of genuine interaction is often obscured by systematic\nbiases. We present a Bayesian framework to disentangle and quantify three such\nbiases: (i) a topic bias toward prior opinions in the training data; (ii) an\nagreement bias favoring agreement irrespective of the question; and (iii) an\nanchoring bias toward the initiating agent's stance. Applying this framework to\nmulti-step dialogues reveals that opinion trajectories tend to quickly converge\nto a shared attractor, with the influence of the interaction fading over time,\nand the impact of biases differing between LLMs. In addition, we fine-tune an\nLLM on different sets of strongly opinionated statements (incl. misinformation)\nand demonstrate that the opinion attractor shifts correspondingly. Exposing\nstark differences between LLMs and providing quantitative tools to compare them\nto human subjects in the future, our approach highlights both chances and\npitfalls in using LLMs as proxies for human behavior.", "published": "2025-09-08 16:26:45", "link": "http://arxiv.org/abs/2509.06858v1", "categories": ["physics.soc-ph", "cs.AI", "nlin.AO"], "primary_category": "physics.soc-ph"}
{"title": "Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice", "abstract": "Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van\nDer Heijde Score (TSS) is crucial, but manual scoring is often time-consuming\nand subjective. This study introduces an Automated Radiographic Sharp Scoring\n(ARTSS) framework that leverages deep learning to analyze full-hand X-ray\nimages, aiming to reduce inter- and intra-observer variability. The research\nuniquely accommodates patients with joint disappearance and variable-length\nimage sequences. We developed ARTSS using data from 970 patients, structured\ninto four stages: I) Image pre-processing and re-orientation using ResNet50,\nII) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and\nIV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,\nEfficientNetB0, and Vision Transformer (ViT). We evaluated model performance\nwith Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute\nerror (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS\nfrom two radiologists was used as the ground truth. Model training employed\n3-fold cross-validation, with each fold consisting of 452 training and 227\nvalidation samples, and external testing included 291 unseen subjects. Our\njoint identification model achieved 99% accuracy. The best-performing model,\nViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results\ndemonstrate the potential of deep learning to automate RA scoring, which can\nsignificantly enhance clinical practice. Our approach addresses the challenge\nof joint disappearance and variable joint numbers, offers timesaving benefits,\nreduces inter- and intra-reader variability, improves radiologist accuracy, and\naids rheumatologists in making more informed decisions.", "published": "2025-09-08 16:21:45", "link": "http://arxiv.org/abs/2509.06854v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor", "abstract": "The inherent complexity of living cells as production units creates major\nchallenges for maintaining stable and optimal bioprocess conditions, especially\nin open Photobioreactors (PBRs) exposed to fluctuating environments. To address\nthis, we propose a Reinforcement Learning (RL) control approach, combined with\nBehavior Cloning (BC), for pH regulation in open PBR systems. This represents,\nto the best of our knowledge, the first application of an RL-based control\nstrategy to such a nonlinear and disturbance-prone bioprocess. Our method\nbegins with an offline training stage in which the RL agent learns from\ntrajectories generated by a nominal Proportional-Integral-Derivative (PID)\ncontroller, without direct interaction with the real system. This is followed\nby a daily online fine-tuning phase, enabling adaptation to evolving process\ndynamics and stronger rejection of fast, transient disturbances. This hybrid\noffline-online strategy allows deployment of an adaptive control policy capable\nof handling the inherent nonlinearities and external perturbations in open\nPBRs. Simulation studies highlight the advantages of our method: the Integral\nof Absolute Error (IAE) was reduced by 8% compared to PID control and by 5%\nrelative to standard off-policy RL. Moreover, control effort decreased\nsubstantially-by 54% compared to PID and 7% compared to standard RL-an\nimportant factor for minimizing operational costs. Finally, an 8-day\nexperimental validation under varying environmental conditions confirmed the\nrobustness and reliability of the proposed approach. Overall, this work\ndemonstrates the potential of RL-based methods for bioprocess control and paves\nthe way for their broader application to other nonlinear, disturbance-prone\nsystems.", "published": "2025-09-08 16:21:11", "link": "http://arxiv.org/abs/2509.06853v1", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "primary_category": "eess.SY"}
{"title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting", "abstract": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies.", "published": "2025-09-08 14:54:31", "link": "http://arxiv.org/abs/2509.06770v1", "categories": ["cs.AI", "cs.HC"], "primary_category": "cs.AI"}
{"title": "Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization", "abstract": "Large Vision-Language Models (LVLMs) or multimodal large language models\nrepresent a significant advancement in artificial intelligence, enabling\nsystems to understand and generate content across both visual and textual\nmodalities. While large-scale pretraining has driven substantial progress,\nfine-tuning these models for aligning with human values or engaging in specific\ntasks or behaviors remains a critical challenge. Deep Reinforcement Learning\n(DRL) and Direct Preference Optimization (DPO) offer promising frameworks for\nthis aligning process. While DRL enables models to optimize actions using\nreward signals instead of relying solely on supervised preference data, DPO\ndirectly aligns the policy with preferences, eliminating the need for an\nexplicit reward model. This overview explores paradigms for fine-tuning LVLMs,\nhighlighting how DRL and DPO techniques can be used to align models with human\npreferences and values, improve task performance, and enable adaptive\nmultimodal interaction. We categorize key approaches, examine sources of\npreference data, reward signals, and discuss open challenges such as\nscalability, sample efficiency, continual learning, generalization, and safety.\nThe goal is to provide a clear understanding of how DRL and DPO contribute to\nthe evolution of robust and human-aligned LVLMs.", "published": "2025-09-08 14:47:57", "link": "http://arxiv.org/abs/2509.06759v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Long-Range Graph Wavelet Networks", "abstract": "Modeling long-range interactions, the propagation of information across\ndistant parts of a graph, is a central challenge in graph machine learning.\nGraph wavelets, inspired by multi-resolution signal processing, provide a\nprincipled way to capture both local and global structures. However, existing\nwavelet-based graph neural networks rely on finite-order polynomial\napproximations, which limit their receptive fields and hinder long-range\npropagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), which\ndecompose wavelet filters into complementary local and global components. Local\naggregation is handled with efficient low-order polynomials, while long-range\ninteractions are captured through a flexible spectral domain parameterization.\nThis hybrid design unifies short- and long-distance information flow within a\nprincipled wavelet framework. Experiments show that LR-GWN achieves\nstate-of-the-art performance among wavelet-based methods on long-range\nbenchmarks, while remaining competitive on short-range datasets.", "published": "2025-09-08 14:35:30", "link": "http://arxiv.org/abs/2509.06743v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture", "abstract": "Brain tumors are serious health problems that require early diagnosis due to\ntheir high mortality rates. Diagnosing tumors by examining Magnetic Resonance\nImaging (MRI) images is a process that requires expertise and is prone to\nerror. Therefore, the need for automated diagnosis systems is increasing day by\nday. In this context, a robust and explainable Deep Learning (DL) model for the\nclassification of brain tumors is proposed. In this study, a publicly available\nFigshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI\nimages of three tumor types was used. First, the classification performance of\nnine well-known CNN architectures was evaluated to determine the most effective\nbackbone. Among these, EfficientNetV2 demonstrated the best performance and was\nselected as the backbone for further development. Subsequently, an\nattention-based MLP-Mixer architecture was integrated into EfficientNetV2 to\nenhance its classification capability. The performance of the final model was\ncomprehensively compared with basic CNNs and the methods in the literature.\nAdditionally, Grad-CAM visualization was used to interpret and validate the\ndecision-making process of the proposed model. The proposed model's performance\nwas evaluated using the five-fold cross-validation method. The proposed model\ndemonstrated superior performance with 99.50% accuracy, 99.47% precision,\n99.52% recall and 99.49% F1 score. The results obtained show that the model\noutperforms the studies in the literature. Moreover, Grad-CAM visualizations\ndemonstrate that the model effectively focuses on relevant regions of MRI\nimages, thus improving interpretability and clinical reliability. A robust deep\nlearning model for clinical decision support systems has been obtained by\ncombining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy\nand interpretability in brain tumor classification.", "published": "2025-09-08 14:08:21", "link": "http://arxiv.org/abs/2509.06713v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "Probabilistic Modeling of Latent Agentic Substructures in Deep Neural Networks", "abstract": "We develop a theory of intelligent agency grounded in probabilistic modeling\nfor neural models. Agents are represented as outcome distributions with\nepistemic utility given by log score, and compositions are defined through\nweighted logarithmic pooling that strictly improves every member's welfare. We\nprove that strict unanimity is impossible under linear pooling or in binary\noutcome spaces, but possible with three or more outcomes. Our framework admits\nrecursive structure via cloning invariance, continuity, and openness, while\ntilt-based analysis rules out trivial duplication. Finally, we formalize an\nagentic alignment phenomenon in LLMs using our theory: eliciting a benevolent\npersona (\"Luigi'\") induces an antagonistic counterpart (\"Waluigi\"), while a\nmanifest-then-suppress Waluigi strategy yields strictly larger first-order\nmisalignment reduction than pure Luigi reinforcement alone. These results\nclarify how developing a principled mathematical framework for how subagents\ncan coalesce into coherent higher-level entities provides novel implications\nfor alignment in agentic AI systems.", "published": "2025-09-08 13:55:01", "link": "http://arxiv.org/abs/2509.06701v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Barycentric Neural Networks and Length-Weighted Persistent Entropy Loss: A Green Geometric and Topological Framework for Function Approximation", "abstract": "While it is well-established that artificial neural networks are\n\\emph{universal approximators} for continuous functions on compact domains,\nmany modern approaches rely on deep or overparameterized architectures that\nincur high computational costs. In this paper, a new type of \\emph{small\nshallow} neural network, called the \\emph{Barycentric Neural Network} ($\\BNN$),\nis proposed, which leverages a fixed set of \\emph{base points} and their\n\\emph{barycentric coordinates} to define both its structure and its parameters.\nWe demonstrate that our $\\BNN$ enables the exact representation of\n\\emph{continuous piecewise linear functions} ($\\CPLF$s), ensuring strict\ncontinuity across segments. Since any continuous function over a compact domain\ncan be approximated arbitrarily well by $\\CPLF$s, the $\\BNN$ naturally emerges\nas a flexible and interpretable tool for \\emph{function approximation}. Beyond\nthe use of this representation, the main contribution of the paper is the\nintroduction of a new variant of \\emph{persistent entropy}, a topological\nfeature that is stable and scale invariant, called the \\emph{length-weighted\npersistent entropy} ($\\LWPE$), which is weighted by the lifetime of topological\nfeatures. Our framework, which combines the $\\BNN$ with a loss function based\non our $\\LWPE$, aims to provide flexible and geometrically interpretable\napproximations of nonlinear continuous functions in resource-constrained\nsettings, such as those with limited base points for $\\BNN$ design and few\ntraining epochs. Instead of optimizing internal weights, our approach directly\n\\emph{optimizes the base points that define the $\\BNN$}. Experimental results\nshow that our approach achieves \\emph{superior and faster approximation\nperformance} compared to classical loss functions such as MSE, RMSE, MAE, and\nlog-cosh.", "published": "2025-09-08 13:47:21", "link": "http://arxiv.org/abs/2509.06694v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring", "abstract": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems.", "published": "2025-09-08 13:44:55", "link": "http://arxiv.org/abs/2509.06690v1", "categories": ["cs.CV", "cs.AI", "cs.AR", "N/A", "I.2.9; I.2.10; I.4.6"], "primary_category": "cs.CV"}
{"title": "TrajAware: Graph Cross-Attention and Trajectory-Aware for Generalisable VANETs under Partial Observations", "abstract": "Vehicular ad hoc networks (VANETs) are a crucial component of intelligent\ntransportation systems; however, routing remains challenging due to dynamic\ntopologies, incomplete observations, and the limited resources of edge devices.\nExisting reinforcement learning (RL) approaches often assume fixed graph\nstructures and require retraining when network conditions change, making them\nunsuitable for deployment on constrained hardware. We present TrajAware, an\nRL-based framework designed for edge AI deployment in VANETs. TrajAware\nintegrates three components: (i) action space pruning, which reduces redundant\nneighbour options while preserving two-hop reachability, alleviating the curse\nof dimensionality; (ii) graph cross-attention, which maps pruned neighbours to\nthe global graph context, producing features that generalise across diverse\nnetwork sizes; and (iii) trajectory-aware prediction, which uses historical\nroutes and junction information to estimate real-time positions under partial\nobservations. We evaluate TrajAware in the open-source SUMO simulator using\nreal-world city maps with a leave-one-city-out setup. Results show that\nTrajAware achieves near-shortest paths and high delivery ratios while\nmaintaining efficiency suitable for constrained edge devices, outperforming\nstate-of-the-art baselines in both full and partial observation scenarios.", "published": "2025-09-08 13:24:21", "link": "http://arxiv.org/abs/2509.06665v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "AnalysisGNN: Unified Music Analysis with Graph Neural Networks", "abstract": "Recent years have seen a boom in computational approaches to music analysis,\nyet each one is typically tailored to a specific analytical domain. In this\nwork, we introduce AnalysisGNN, a novel graph neural network framework that\nleverages a data-shuffling strategy with a custom weighted multi-task loss and\nlogit fusion between task-specific classifiers to integrate heterogeneously\nannotated symbolic datasets for comprehensive score analysis. We further\nintegrate a Non-Chord-Tone prediction module, which identifies and excludes\npassing and non-functional notes from all tasks, thereby improving the\nconsistency of label signals. Experimental evaluations demonstrate that\nAnalysisGNN achieves performance comparable to traditional static-dataset\napproaches, while showing increased resilience to domain shifts and annotation\ninconsistencies across multiple heterogeneous corpora.", "published": "2025-09-08 13:11:54", "link": "http://arxiv.org/abs/2509.06654v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning", "abstract": "Targeting the issues of \"shortcuts\" and insufficient contextual understanding\nin complex cross-modal reasoning of multimodal large models, this paper\nproposes a zero-shot multimodal reasoning component guided by human-like\ncognitive strategies centered on an \"intent sketch\". The component comprises a\nplug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and\nStrategy Selector-that explicitly constructs a \"understand-plan-select\"\ncognitive process. By generating and filtering \"intent sketch\" strategies to\nguide the final reasoning, it requires no parameter fine-tuning and achieves\ncross-model transfer solely through in-context engineering.\nInformation-theoretic analysis shows that this process can reduce conditional\nentropy and improve information utilization efficiency, thereby suppressing\nunintended shortcut reasoning. Experiments on IntentBench, WorldSense, and\nDaily-Omni validate the method's generality and robust gains; compared with\ntheir respective baselines, the complete \"three-module\" scheme yields\nconsistent improvements across different reasoning engines and pipeline\ncombinations, with gains up to approximately 9.51 percentage points,\ndemonstrating the practical value and portability of the \"intent sketch\"\nreasoning component in zero-shot scenarios.", "published": "2025-09-08 12:57:02", "link": "http://arxiv.org/abs/2509.06641v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "The First Voice Timbre Attribute Detection Challenge", "abstract": "The first voice timbre attribute detection challenge is featured in a special\nsession at NCMMSC 2025. It focuses on the explainability of voice timbre and\ncompares the intensity of two speech utterances in a specified timbre\ndescriptor dimension. The evaluation was conducted on the VCTK-RVA dataset.\nParticipants developed their systems and submitted their outputs to the\norganizer, who evaluated the performance and sent feedback to them. Six teams\nsubmitted their outputs, with five providing descriptions of their\nmethodologies.", "published": "2025-09-08 12:54:28", "link": "http://arxiv.org/abs/2509.06635v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework", "abstract": "Plants in their natural habitats endure an array of interacting stresses,\nboth biotic and abiotic, that rarely occur in isolation. Nutrient\nstress-particularly nitrogen deficiency-becomes even more critical when\ncompounded with drought and weed competition, making it increasingly difficult\nto distinguish and address its effects. Early detection of nitrogen stress is\ntherefore crucial for protecting plant health and implementing effective\nmanagement strategies. This study proposes a novel deep learning framework to\naccurately classify nitrogen stress severity in a combined stress environment.\nOur model uses a unique blend of four imaging modalities-RGB, multispectral,\nand two infrared wavelengths-to capture a wide range of physiological plant\nresponses from canopy images. These images, provided as time-series data,\ndocument plant health across three levels of nitrogen availability (low,\nmedium, and high) under varying water stress and weed pressures. The core of\nour approach is a spatio-temporal deep learning pipeline that merges a\nConvolutional Neural Network (CNN) for extracting spatial features from images\nwith a Long Short-Term Memory (LSTM) network to capture temporal dependencies.\nWe also devised and evaluated a spatial-only CNN pipeline for comparison. Our\nCNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively\nsurpassing the spatial-only model's 80.45% and other previously reported\nmachine learning method's 76%. These results bring actionable insights based on\nthe power of our CNN-LSTM approach in effectively capturing the subtle and\ncomplex interactions between nitrogen deficiency, water stress, and weed\npressure. This robust platform offers a promising tool for the timely and\nproactive identification of nitrogen stress severity, enabling better crop\nmanagement and improved plant health.", "published": "2025-09-08 12:41:45", "link": "http://arxiv.org/abs/2509.06625v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "BEAM: Brainwave Empathy Assessment Model for Early Childhood", "abstract": "Empathy in young children is crucial for their social and emotional\ndevelopment, yet predicting it remains challenging. Traditional methods often\nonly rely on self-reports or observer-based labeling, which are susceptible to\nbias and fail to objectively capture the process of empathy formation. EEG\noffers an objective alternative; however, current approaches primarily extract\nstatic patterns, neglecting temporal dynamics. To overcome these limitations,\nwe propose a novel deep learning framework, the Brainwave Empathy Assessment\nModel (BEAM), to predict empathy levels in children aged 4-6 years. BEAM\nleverages multi-view EEG signals to capture both cognitive and emotional\ndimensions of empathy. The framework comprises three key components: 1) a\nLaBraM-based encoder for effective spatio-temporal feature extraction, 2) a\nfeature fusion module to integrate complementary information from multi-view\nsignals, and 3) a contrastive learning module to enhance class separation.\nValidated on the CBCP dataset, BEAM outperforms state-of-the-art methods across\nmultiple metrics, demonstrating its potential for objective empathy assessment\nand providing a preliminary insight into early interventions in children's\nprosocial development.", "published": "2025-09-08 12:39:09", "link": "http://arxiv.org/abs/2509.06620v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Demo: Healthcare Agent Orchestrator (HAO) for Patient Summarization in Molecular Tumor Boards", "abstract": "Molecular Tumor Boards (MTBs) are multidisciplinary forums where oncology\nspecialists collaboratively assess complex patient cases to determine optimal\ntreatment strategies. A central element of this process is the patient summary,\ntypically compiled by a medical oncologist, radiation oncologist, or surgeon,\nor their trained medical assistant, who distills heterogeneous medical records\ninto a concise narrative to facilitate discussion. This manual approach is\noften labor-intensive, subjective, and prone to omissions of critical\ninformation. To address these limitations, we introduce the Healthcare Agent\nOrchestrator (HAO), a Large Language Model (LLM)-driven AI agent that\ncoordinates a multi-agent clinical workflow to generate accurate and\ncomprehensive patient summaries for MTBs. Evaluating predicted patient\nsummaries against ground truth presents additional challenges due to stylistic\nvariation, ordering, synonym usage, and phrasing differences, which complicate\nthe measurement of both succinctness and completeness. To overcome these\nevaluation hurdles, we propose TBFact, a ``model-as-a-judge'' framework\ndesigned to assess the comprehensiveness and succinctness of generated\nsummaries. Using a benchmark dataset derived from de-identified tumor board\ndiscussions, we applied TBFact to evaluate our Patient History agent. Results\nshow that the agent captured 94% of high-importance information (including\npartial entailments) and achieved a TBFact recall of 0.84 under strict\nentailment criteria. We further demonstrate that TBFact enables a data-free\nevaluation framework that institutions can deploy locally without sharing\nsensitive clinical data. Together, HAO and TBFact establish a robust foundation\nfor delivering reliable and scalable support to MTBs.", "published": "2025-09-08 12:15:53", "link": "http://arxiv.org/abs/2509.06602v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Integrating Spatial and Semantic Embeddings for Stereo Sound Event Localization in Videos", "abstract": "In this study, we address the multimodal task of stereo sound event\nlocalization and detection with source distance estimation (3D SELD) in regular\nvideo content. 3D SELD is a complex task that combines temporal event\nclassification with spatial localization, requiring reasoning across spatial,\ntemporal, and semantic dimensions. The last is arguably the most challenging to\nmodel. Traditional SELD approaches typically rely on multichannel input,\nlimiting their capacity to benefit from large-scale pre-training due to data\nconstraints. To overcome this, we enhance a standard SELD architecture with\nsemantic information by integrating pre-trained, contrastive language-aligned\nmodels: CLAP for audio and OWL-ViT for visual inputs. These embeddings are\nincorporated into a modified Conformer module tailored for multimodal fusion,\nwhich we refer to as the Cross-Modal Conformer. We perform an ablation study on\nthe development set of the DCASE2025 Task3 Stereo SELD Dataset to assess the\nindividual contributions of the language-aligned models and benchmark against\nthe DCASE Task 3 baseline systems. Additionally, we detail the curation process\nof large synthetic audio and audio-visual datasets used for model pre-training.\nThese datasets were further expanded through left-right channel swapping\naugmentation. Our approach, combining extensive pre-training, model ensembling,\nand visual post-processing, achieved second rank in the DCASE 2025 Challenge\nTask 3 (Track B), underscoring the effectiveness of our method. Future work\nwill explore the modality-specific contributions and architectural refinements.", "published": "2025-09-08 12:07:32", "link": "http://arxiv.org/abs/2509.06598v1", "categories": ["eess.AS", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Integrated Detection and Tracking Based on Radar Range-Doppler Feature", "abstract": "Detection and tracking are the basic tasks of radar systems. Current joint\ndetection tracking methods, which focus on dynamically adjusting detection\nthresholds from tracking results, still present challenges in fully utilizing\nthe potential of radar signals. These are mainly reflected in the limited\ncapacity of the constant false-alarm rate model to accurately represent\ninformation, the insufficient depiction of complex scenes, and the limited\ninformation acquired by the tracker. We introduce the Integrated Detection and\nTracking based on radar feature (InDT) method, which comprises a network\narchitecture for radar signal detection and a tracker that leverages detection\nassistance. The InDT detector extracts feature information from each\nRange-Doppler (RD) matrix and then returns the target position through the\nfeature enhancement module and the detection head. The InDT tracker adaptively\nupdates the measurement noise covariance of the Kalman filter based on\ndetection confidence. The similarity of target RD features is measured by\ncosine distance, which enhances the data association process by combining\nlocation and feature information. Finally, the efficacy of the proposed method\nwas validated through testing on both simulated data and publicly available\ndatasets.", "published": "2025-09-08 11:32:58", "link": "http://arxiv.org/abs/2509.06569v1", "categories": ["eess.SP", "cs.AI"], "primary_category": "eess.SP"}
{"title": "Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs", "abstract": "Network intrusion detection remains a critical challenge in cybersecurity.\nWhile supervised machine learning models achieve state-of-the-art performance,\ntheir reliance on large labelled datasets makes them impractical for many\nreal-world applications. Anomaly detection methods, which train exclusively on\nbenign traffic to identify malicious activity, suffer from high false positive\nrates, limiting their usability. Recently, self-supervised learning techniques\nhave demonstrated improved performance with lower false positive rates by\nlearning discriminative latent representations of benign traffic. In\nparticular, contrastive self-supervised models achieve this by minimizing the\ndistance between similar (positive) views of benign traffic while maximizing it\nbetween dissimilar (negative) views. Existing approaches generate positive\nviews through data augmentation and treat other samples as negative. In\ncontrast, this work introduces Contrastive Learning using Augmented Negative\npairs (CLAN), a novel paradigm for network intrusion detection where augmented\nsamples are treated as negative views - representing potentially malicious\ndistributions - while other benign samples serve as positive views. This\napproach enhances both classification accuracy and inference efficiency after\npretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset\ndemonstrates that the proposed method surpasses existing self-supervised and\nanomaly detection techniques in a binary classification task. Furthermore, when\nfine-tuned on a limited labelled dataset, the proposed approach achieves\nsuperior multi-class classification performance compared to existing\nself-supervised models.", "published": "2025-09-08 11:04:10", "link": "http://arxiv.org/abs/2509.06550v1", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI", "I.2.6; K.6.5"], "primary_category": "cs.LG"}
{"title": "Signal-Based Malware Classification Using 1D CNNs", "abstract": "Malware classification is a contemporary and ongoing challenge in\ncyber-security: modern obfuscation techniques are able to evade traditional\nstatic analysis, while dynamic analysis is too resource intensive to be\ndeployed at a large scale. One prominent line of research addresses these\nlimitations by converting malware binaries into 2D images by heuristically\nreshaping them into a 2D grid before resizing using Lanczos resampling. These\nimages can then be classified based on their textural information using\ncomputer vision approaches. While this approach can detect obfuscated malware\nmore effectively than static analysis, the process of converting files into 2D\nimages results in significant information loss due to both quantisation noise,\ncaused by rounding to integer pixel values, and the introduction of 2D\ndependencies which do not exist in the original data. This loss of signal\nlimits the classification performance of the downstream model. This work\naddresses these weaknesses by instead resizing the files into 1D signals which\navoids the need for heuristic reshaping, and additionally these signals do not\nsuffer from quantisation noise due to being stored in a floating-point format.\nIt is shown that existing 2D CNN architectures can be readily adapted to\nclassify these 1D signals for improved performance. Furthermore, a bespoke 1D\nconvolutional neural network, based on the ResNet architecture and\nsqueeze-and-excitation layers, was developed to classify these signals and\nevaluated on the MalNet dataset. It was found to achieve state-of-the-art\nperformance on binary, type, and family level classification with F1 scores of\n0.874, 0.503, and 0.507, respectively, paving the way for future models to\noperate on the proposed signal modality.", "published": "2025-09-08 11:03:48", "link": "http://arxiv.org/abs/2509.06548v1", "categories": ["cs.CR", "cs.AI", "cs.CV", "cs.LG", "I.2.6; K.6.5"], "primary_category": "cs.CR"}
{"title": "Learning Optimal Defender Strategies for CAGE-2 using a POMDP Model", "abstract": "CAGE-2 is an accepted benchmark for learning and evaluating defender\nstrategies against cyberattacks. It reflects a scenario where a defender agent\nprotects an IT infrastructure against various attacks. Many defender methods\nfor CAGE-2 have been proposed in the literature. In this paper, we construct a\nformal model for CAGE-2 using the framework of Partially Observable Markov\nDecision Process (POMDP). Based on this model, we define an optimal defender\nstrategy for CAGE-2 and introduce a method to efficiently learn this strategy.\nOur method, called BF-PPO, is based on PPO, and it uses particle filter to\nmitigate the computational complexity due to the large state space of the\nCAGE-2 model. We evaluate our method in the CAGE-2 CybORG environment and\ncompare its performance with that of CARDIFF, the highest ranked method on the\nCAGE-2 leaderboard. We find that our method outperforms CARDIFF regarding the\nlearned defender strategy and the required training time.", "published": "2025-09-08 10:51:43", "link": "http://arxiv.org/abs/2509.06539v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''", "abstract": "We investigated the reproducibility of FairCLIP, proposed by Luo et al.\n(2024), for improving the group fairness of CLIP (Radford et al., 2021) by\nminimizing image-text similarity score disparities across sensitive groups\nusing the Sinkhorn distance. The experimental setup of Luo et al. (2024) was\nreproduced to primarily investigate the research findings for FairCLIP. The\nmodel description by Luo et al. (2024) was found to differ from the original\nimplementation. Therefore, a new implementation, A-FairCLIP, is introduced to\nexamine specific design choices. Furthermore, FairCLIP+ is proposed to extend\nthe FairCLIP objective to include multiple attributes. Additionally, the impact\nof the distance minimization on FairCLIP's fairness and performance was\nexplored. In alignment with the original authors, CLIP was found to be biased\ntowards certain demographics when applied to zero-shot glaucoma classification\nusing medical scans and clinical notes from the Harvard-FairVLMed dataset.\nHowever, the experimental results on two datasets do not support their claim\nthat FairCLIP improves the performance and fairness of CLIP. Although the\nregularization objective reduces Sinkhorn distances, both the official\nimplementation and the aligned implementation, A-FairCLIP, were not found to\nimprove performance nor fairness in zero-shot glaucoma classification.", "published": "2025-09-08 10:41:10", "link": "http://arxiv.org/abs/2509.06535v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients", "abstract": "Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded in\nintesive care unit (ICU) and operating room (OR). However, the high incidence\nof poor, incomplete, and inconsistent signal quality, can lead to false alarms\nor diagnostic inaccuracies. The methods explored so far suffer from limited\ngeneralizability, reliance on extensive labeled data, and poor cross-task\ntransferability. To overcome these challenges, we introduce QualityFM, a novel\nmultimodal foundation model for these physiological signals, designed to\nacquire a general-purpose understanding of signal quality. Our model is\npre-trained on an large-scale dataset comprising over 21 million 30-second\nwaveforms and 179,757 hours of data. Our approach involves a dual-track\narchitecture that processes paired physiological signals of differing quality,\nleveraging a self-distillation strategy where an encoder for high-quality\nsignals is used to guide the training of an encoder for low-quality signals. To\nefficiently handle long sequential signals and capture essential local\nquasi-periodic patterns, we integrate a windowed sparse attention mechanism\nwithin our Transformer-based model. Furthermore, a composite loss function,\nwhich combines direct distillation loss on encoder outputs with indirect\nreconstruction loss based on power and phase spectra, ensures the preservation\nof frequency-domain characteristics of the signals. We pre-train three models\nwith varying parameter counts (9.6 M to 319 M) and demonstrate their efficacy\nand practical value through transfer learning on three distinct clinical tasks:\nfalse alarm of ventricular tachycardia detection, the identification of atrial\nfibrillation and the estimation of arterial blood pressure (ABP) from PPG and\nECG signals.", "published": "2025-09-08 10:20:56", "link": "http://arxiv.org/abs/2509.06516v1", "categories": ["cs.LG", "cs.AI", "J.3"], "primary_category": "cs.LG"}
{"title": "An AI system to help scientists write expert-level empirical software", "abstract": "The cycle of scientific discovery is frequently bottlenecked by the slow,\nmanual creation of software to support computational experiments. To address\nthis, we present an AI system that creates expert-level scientific software\nwhose goal is to maximize a quality metric. The system uses a Large Language\nModel (LLM) and Tree Search (TS) to systematically improve the quality metric\nand intelligently navigate the large space of possible solutions. The system\nachieves expert-level results when it explores and integrates complex research\nideas from external sources. The effectiveness of tree search is demonstrated\nacross a wide range of benchmarks. In bioinformatics, it discovered 40 novel\nmethods for single-cell data analysis that outperformed the top human-developed\nmethods on a public leaderboard. In epidemiology, it generated 14 models that\noutperformed the CDC ensemble and all other individual models for forecasting\nCOVID-19 hospitalizations. Our method also produced state-of-the-art software\nfor geospatial analysis, neural activity prediction in zebrafish, time series\nforecasting and numerical solution of integrals. By devising and implementing\nnovel solutions to diverse tasks, the system represents a significant step\ntowards accelerating scientific progress.", "published": "2025-09-08 10:08:36", "link": "http://arxiv.org/abs/2509.06503v1", "categories": ["cs.AI", "q-bio.QM"], "primary_category": "cs.AI"}
{"title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers", "abstract": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.", "published": "2025-09-08 09:54:18", "link": "http://arxiv.org/abs/2509.06493v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization", "abstract": "In supply chain management, decision-making often involves balancing multiple\nconflicting objectives, such as cost reduction, service level improvement, and\nenvironmental sustainability. Traditional multi-objective optimization methods,\nsuch as linear programming and evolutionary algorithms, struggle to adapt in\nreal-time to the dynamic nature of supply chains. In this paper, we propose an\napproach that combines Reinforcement Learning (RL) and Multi-Objective\nEvolutionary Algorithms (MOEAs) to address these challenges for dynamic\nmulti-objective optimization under uncertainty. Our method leverages MOEAs to\nsearch the parameter space of policy neural networks, generating a Pareto front\nof policies. This provides decision-makers with a diverse population of\npolicies that can be dynamically switched based on the current system\nobjectives, ensuring flexibility and adaptability in real-time decision-making.\nWe also introduce Conditional Value-at-Risk (CVaR) to incorporate\nrisk-sensitive decision-making, enhancing resilience in uncertain environments.\nWe demonstrate the effectiveness of our approach through case studies,\nshowcasing its ability to respond to supply chain dynamics and outperforming\nstate-of-the-art methods in an inventory management case study. The proposed\nstrategy not only improves decision-making efficiency but also offers a more\nrobust framework for managing uncertainty and optimizing performance in supply\nchains.", "published": "2025-09-08 09:51:24", "link": "http://arxiv.org/abs/2509.06490v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "DyC-STG: Dynamic Causal Spatio-Temporal Graph Network for Real-time Data Credibility Analysis in IoT", "abstract": "The wide spreading of Internet of Things (IoT) sensors generates vast\nspatio-temporal data streams, but ensuring data credibility is a critical yet\nunsolved challenge for applications like smart homes. While spatio-temporal\ngraph (STG) models are a leading paradigm for such data, they often fall short\nin dynamic, human-centric environments due to two fundamental limitations: (1)\ntheir reliance on static graph topologies, which fail to capture physical,\nevent-driven dynamics, and (2) their tendency to confuse spurious correlations\nwith true causality, undermining robustness in human-centric environments. To\naddress these gaps, we propose the Dynamic Causal Spatio-Temporal Graph Network\n(DyC-STG), a novel framework designed for real-time data credibility analysis\nin IoT. Our framework features two synergistic contributions: an event-driven\ndynamic graph module that adapts the graph topology in real-time to reflect\nphysical state changes, and a causal reasoning module to distill causally-aware\nrepresentations by strictly enforcing temporal precedence. To facilitate the\nresearch in this domain we release two new real-world datasets. Comprehensive\nexperiments show that DyC-STG establishes a new state-of-the-art, outperforming\nthe strongest baselines by 1.4 percentage points and achieving an F1-Score of\nup to 0.930.", "published": "2025-09-08 09:46:58", "link": "http://arxiv.org/abs/2509.06483v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents", "abstract": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.", "published": "2025-09-08 09:43:48", "link": "http://arxiv.org/abs/2509.06477v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems", "abstract": "AI-based recommender systems increasingly influence recruitment decisions.\nThus, transparency and responsible adoption in Human Resource Management (HRM)\nare critical. This study examines how HR managers' AI literacy influences their\nsubjective perception and objective understanding of explainable AI (XAI)\nelements in recruiting recommender dashboards. In an online experiment, 410\nGerman-based HR managers compared baseline dashboards to versions enriched with\nthree XAI styles: important features, counterfactuals, and model criteria. Our\nresults show that the dashboards used in practice do not explain AI results and\neven keep AI elements opaque. However, while adding XAI features improves\nsubjective perceptions of helpfulness and trust among users with moderate or\nhigh AI literacy, it does not increase their objective understanding. It may\neven reduce accurate understanding, especially with complex explanations. Only\noverlays of important features significantly aided the interpretations of\nhigh-literacy users. Our findings highlight that the benefits of XAI in\nrecruitment depend on users' AI literacy, emphasizing the need for tailored\nexplanation strategies and targeted literacy training in HRM to ensure fair,\ntransparent, and effective adoption of AI.", "published": "2025-09-08 09:40:49", "link": "http://arxiv.org/abs/2509.06475v1", "categories": ["cs.HC", "cs.AI", "cs.CY", "A.0; H.5.2; I.2; J.1; K.4.2; K.4.3"], "primary_category": "cs.HC"}
{"title": "Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading", "abstract": "We analyze Decentralized Online Optimization algorithms using the Performance\nEstimation Problem approach which allows, to automatically compute exact\nworst-case performance of optimization algorithms. Our analysis shows that\nseveral available performance guarantees are very conservative, sometimes by\nmultiple orders of magnitude, and can lead to misguided choices of algorithm.\nMoreover, at least in terms of worst-case performance, some algorithms appear\nnot to benefit from inter-agent communications for a significant period of\ntime. We show how to improve classical methods by tuning their step-sizes, and\nfind that we can save up to 20% on their actual worst-case performance regret.", "published": "2025-09-08 09:28:36", "link": "http://arxiv.org/abs/2509.06466v1", "categories": ["math.OC", "cs.AI", "cs.DC", "cs.MA"], "primary_category": "math.OC"}
{"title": "Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set", "abstract": "With the growing demand for applying large language models to downstream\ntasks, improving model alignment performance and efficiency has become crucial.\nSuch a process involves selecting informative instructions from a candidate\npool. However, due to the complexity of instruction set distributions, the key\nfactors driving the performance of aligned models remain unclear. As a result,\ncurrent instruction set refinement methods fail to improve performance as the\ninstruction pool expands continuously. To address this issue, we first\ninvestigate the key factors that influence the relationship between instruction\ndataset distribution and aligned model performance. Based on these insights, we\npropose a novel instruction data selection method. We identify that the depth\nof instructions and the coverage of the semantic space are the crucial factors\ndetermining downstream performance, which could explain over 70\\% of the model\nloss on the development set. We then design an instruction selection algorithm\nto simultaneously maximize the depth and semantic coverage of the selected\ninstructions. Experimental results demonstrate that, compared to\nstate-of-the-art baseline methods, it can sustainably improve model performance\nat a faster pace and thus achieve \\emph{``Accelerated Scaling''}.", "published": "2025-09-08 09:22:57", "link": "http://arxiv.org/abs/2509.06463v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.", "published": "2025-09-08 09:20:04", "link": "http://arxiv.org/abs/2509.06461v1", "categories": ["cs.CV", "cs.AI"], "primary_category": "cs.CV"}
{"title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data", "abstract": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.", "published": "2025-09-08 08:44:24", "link": "http://arxiv.org/abs/2509.06444v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning", "abstract": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.", "published": "2025-09-08 08:34:02", "link": "http://arxiv.org/abs/2509.06436v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "HECATE: An ECS-based Framework for Teaching and Developing Multi-Agent Systems", "abstract": "This paper introduces HECATE, a novel framework based on the\nEntity-Component-System (ECS) architectural pattern that bridges the gap\nbetween distributed systems engineering and MAS development. HECATE is built\nusing the Entity-Component-System architectural pattern, leveraging\ndata-oriented design to implement multiagent systems. This approach involves\nengineering multiagent systems (MAS) from a distributed systems (DS)\nperspective, integrating agent concepts directly into the DS domain. This\napproach simplifies MAS development by (i) reducing the need for specialized\nagent knowledge and (ii) leveraging familiar DS patterns and standards to\nminimize the agent-specific knowledge required for engineering MAS. We present\nthe framework's architecture, core components, and implementation approach,\ndemonstrating how it supports different agent models.", "published": "2025-09-08 08:26:01", "link": "http://arxiv.org/abs/2509.06431v1", "categories": ["cs.MA", "cs.AI", "C.2.4, I.2.11"], "primary_category": "cs.MA"}
{"title": "Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster", "abstract": "Computational models are critical to advance our understanding of how neural,\nbiomechanical, and physical systems interact to orchestrate animal behaviors.\nDespite the availability of near-complete reconstructions of the Drosophila\nmelanogaster central nervous system, musculature, and exoskeleton, anatomically\nand physically grounded models of fly leg muscles are still missing. These\nmodels provide an indispensable bridge between motor neuron activity and joint\nmovements. Here, we introduce the first 3D, data-driven musculoskeletal model\nof Drosophila legs, implemented in both OpenSim and MuJoCo simulation\nenvironments. Our model incorporates a Hill-type muscle representation based on\nhigh-resolution X-ray scans from multiple fixed specimens. We present a\npipeline for constructing muscle models using morphological imaging data and\nfor optimizing unknown muscle parameters specific to the fly. We then combine\nour musculoskeletal models with detailed 3D pose estimation data from behaving\nflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of\nmuscle activity across diverse walking and grooming behaviors predict\ncoordinated muscle synergies that can be tested experimentally. Furthermore, by\ntraining imitation learning policies in MuJoCo, we test the effect of different\npassive joint properties on learning speed and find that damping and stiffness\nfacilitate learning. Overall, our model enables the investigation of motor\ncontrol in an experimentally tractable model organism, providing insights into\nhow biomechanics contribute to generation of complex limb movements. Moreover,\nour model can be used to control embodied artificial agents to generate\nnaturalistic and compliant locomotion in simulated environments.", "published": "2025-09-08 08:21:14", "link": "http://arxiv.org/abs/2509.06426v1", "categories": ["q-bio.NC", "cs.AI", "cs.LG", "cs.RO"], "primary_category": "q-bio.NC"}
{"title": "CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup", "abstract": "Time series anomaly detection (TSAD) is a vital yet challenging task,\nparticularly in scenarios where labeled anomalies are scarce and temporal\ndependencies are complex. Recent anomaly assumption (AA) approaches alleviate\nthe lack of anomalies by injecting synthetic samples and training\ndiscriminative models. Despite promising results, these methods often suffer\nfrom two fundamental limitations: patchy generation, where scattered anomaly\nknowledge leads to overly simplistic or incoherent anomaly injection, and\nAnomaly Shift, where synthetic anomalies either resemble normal data too\nclosely or diverge unrealistically from real anomalies, thereby distorting\nclassification boundaries. In this paper, we propose CAPMix, a controllable\nanomaly augmentation framework that addresses both issues. First, we design a\nCutAddPaste mechanism to inject diverse and complex anomalies in a targeted\nmanner, avoiding patchy generation. Second, we introduce a label revision\nstrategy to adaptively refine anomaly labels, reducing the risk of anomaly\nshift. Finally, we employ dual-space mixup within a temporal convolutional\nnetwork to enforce smoother and more robust decision boundaries. Extensive\nexperiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and\nESA, demonstrate that CAPMix achieves significant improvements over\nstate-of-the-art baselines, with enhanced robustness against contaminated\ntraining data. The code is available at https://github.com/alsike22/CAPMix.", "published": "2025-09-08 08:15:12", "link": "http://arxiv.org/abs/2509.06419v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning", "abstract": "This study presents DiagCoT, a multi-stage framework that applies supervised\nfine-tuning to general-purpose vision-language models (VLMs) to emulate\nradiologists' stepwise diagnostic reasoning using only free-text reports.\nDiagCoT combines contrastive image-report tuning for domain alignment,\nchain-of-thought supervision to capture inferential logic, and reinforcement\ntuning with clinical reward signals to enhance factual accuracy and fluency. On\nthe MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC\nfrom 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08\nto 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33\n(absolute gain of 0.22). It outperformed state-of-the-art models including\nLLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By\nconverting unstructured clinical narratives into structured supervision,\nDiagCoT offers a scalable approach for developing interpretable and\ndiagnostically competent AI systems for radiology.", "published": "2025-09-08 08:01:26", "link": "http://arxiv.org/abs/2509.06409v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation", "abstract": "A key challenge in synthesizing audios from silent videos is the inherent\ntrade-off between synthesis quality and inference efficiency in existing\nmethods. For instance, flow matching based models rely on modeling\ninstantaneous velocity, inherently require an iterative sampling process,\nleading to slow inference speeds. To address this efficiency bottleneck, we\nintroduce a MeanFlow-accelerated model that characterizes flow fields using\naverage velocity, enabling one-step generation and thereby significantly\naccelerating multimodal video-to-audio (VTA) synthesis while preserving audio\nquality, semantic alignment, and temporal synchronization. Furthermore, a\nscalar rescaling mechanism is employed to balance conditional and unconditional\npredictions when classifier-free guidance (CFG) is applied, effectively\nmitigating CFG-induced distortions in one step generation. Since the audio\nsynthesis network is jointly trained with multimodal conditions, we further\nevaluate it on text-to-audio (TTA) synthesis task. Experimental results\ndemonstrate that incorporating MeanFlow into the network significantly improves\ninference speed without compromising perceptual quality on both VTA and TTA\nsynthesis tasks.", "published": "2025-09-08 07:15:21", "link": "http://arxiv.org/abs/2509.06389v1", "categories": ["cs.SD", "cs.AI"], "primary_category": "cs.SD"}
{"title": "Beyond the Pre-Service Horizon: Infusing In-Service Behavior for Improved Financial Risk Forecasting", "abstract": "Typical financial risk management involves distinct phases for pre-service\nrisk assessment and in-service default detection, often modeled separately.\nThis paper proposes a novel framework, Multi-Granularity Knowledge Distillation\n(abbreviated as MGKD), aimed at improving pre-service risk prediction through\nthe integration of in-service user behavior data. MGKD follows the idea of\nknowledge distillation, where the teacher model, trained on historical\nin-service data, guides the student model, which is trained on pre-service\ndata. By using soft labels derived from in-service data, the teacher model\nhelps the student model improve its risk prediction prior to service\nactivation. Meanwhile, a multi-granularity distillation strategy is introduced,\nincluding coarse-grained, fine-grained, and self-distillation, to align the\nrepresentations and predictions of the teacher and student models. This\napproach not only reinforces the representation of default cases but also\nenables the transfer of key behavioral patterns associated with defaulters from\nthe teacher to the student model, thereby improving the overall performance of\npre-service risk assessment. Moreover, we adopt a re-weighting strategy to\nmitigate the model's bias towards the minority class. Experimental results on\nlarge-scale real-world datasets from Tencent Mobile Payment demonstrate the\neffectiveness of our proposed approach in both offline and online scenarios.", "published": "2025-09-08 07:09:18", "link": "http://arxiv.org/abs/2509.06385v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification", "abstract": "Drought stress is a major threat to global crop productivity, making its\nearly and precise detection essential for sustainable agricultural management.\nTraditional approaches, though useful, are often time-consuming and\nlabor-intensive, which has motivated the adoption of deep learning methods. In\nrecent years, Convolutional Neural Network (CNN) and Vision Transformer\narchitectures have been widely explored for drought stress identification;\nhowever, these models generally rely on a large number of trainable parameters,\nrestricting their use in resource-limited and real-time agricultural settings.\nTo address this challenge, we propose a novel lightweight hybrid CNN framework\ninspired by ResNet, DenseNet, and MobileNet architectures. The framework\nachieves a remarkable 15-fold reduction in trainable parameters compared to\nconventional CNN and Vision Transformer models, while maintaining competitive\naccuracy. In addition, we introduce a machine unlearning mechanism based on a\ngradient norm-based influence function, which enables targeted removal of\nspecific training data influence, thereby improving model adaptability. The\nmethod was evaluated on an aerial image dataset of potato fields with\nexpert-annotated healthy and drought-stressed regions. Experimental results\nshow that our framework achieves high accuracy while substantially lowering\ncomputational costs. These findings highlight its potential as a practical,\nscalable, and adaptive solution for drought stress monitoring in precision\nagriculture, particularly under resource-constrained conditions.", "published": "2025-09-08 06:46:35", "link": "http://arxiv.org/abs/2509.06367v1", "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "cs.CV"}
{"title": "A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research", "abstract": "Modern simulation environments for complex multi-agent interactions must\nbalance high-fidelity detail with computational efficiency. We present DECOY, a\nnovel multi-agent simulator that abstracts strategic, long-horizon planning in\n3D terrains into high-level discretized simulation while preserving low-level\nenvironmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a\ntestbed, our framework accurately simulates gameplay using only movement\ndecisions as tactical positioning -- without explicitly modeling low-level\nmechanics such as aiming and shooting. Central to our approach is a waypoint\nsystem that simplifies and discretizes continuous states and actions, paired\nwith neural predictive and generative models trained on real CS:GO tournament\ndata to reconstruct event outcomes. Extensive evaluations show that replays\ngenerated from human data in DECOY closely match those observed in the original\ngame. Our publicly available simulation environment provides a valuable tool\nfor advancing research in strategic multi-agent planning and behavior\ngeneration.", "published": "2025-09-08 06:02:59", "link": "http://arxiv.org/abs/2509.06355v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "Ban&Pick: Achieving Free Performance Gains and Inference Speedup via Smarter Routing in MoE-LLMs", "abstract": "Sparse Mixture-of-Experts (MoE) has become a key architecture for scaling\nlarge language models (LLMs) efficiently. Recent fine-grained MoE designs\nintroduce hundreds of experts per layer, with multiple experts activated per\ntoken, enabling stronger specialization. However, during pre-training, routers\nare optimized mainly for stability and robustness: they converge prematurely\nand enforce balanced usage, limiting the full potential of model performance\nand efficiency. In this work, we uncover two overlooked issues: (i) a few\nhighly influential experts are underutilized due to premature and balanced\nrouting decisions; and (ii) enforcing a fixed number of active experts per\ntoken introduces substantial redundancy. Instead of retraining models or\nredesigning MoE architectures, we introduce Ban&Pick, a post-training,\nplug-and-play strategy for smarter MoE routing. Pick discovers and reinforces\nkey experts-a small group with outsized impact on performance-leading to\nnotable accuracy gains across domains. Ban complements this by dynamically\npruning redundant experts based on layer and token sensitivity, delivering\nfaster inference with minimal accuracy loss. Experiments on fine-grained\nMoE-LLMs (DeepSeek, Qwen3) across math, code, and general reasoning benchmarks\ndemonstrate that Ban&Pick delivers free performance gains and inference\nacceleration without retraining or architectural changes. For instance, on\nQwen3-30B-A3B, it improves accuracy from 80.67 to 84.66 on AIME2024 and from\n65.66 to 68.18 on GPQA-Diamond, while accelerating inference by 1.25x under the\nvLLM.", "published": "2025-09-08 05:38:10", "link": "http://arxiv.org/abs/2509.06346v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent", "abstract": "In online second-hand marketplaces, multi-turn bargaining is a crucial part\nof seller-buyer interactions. Large Language Models (LLMs) can act as seller\nagents, negotiating with buyers on behalf of sellers under given business\nconstraints. A critical ability for such agents is to track and accurately\ninterpret cumulative buyer intents across long negotiations, which directly\nimpacts bargaining effectiveness. We introduce a multi-turn evaluation\nframework for measuring the bargaining ability of seller agents in e-commerce\ndialogues. The framework tests whether an agent can extract and track buyer\nintents. Our contributions are: (1) a large-scale e-commerce bargaining\nbenchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a\nturn-level evaluation framework grounded in Theory of Mind (ToM) with annotated\nbuyer intents, moving beyond outcome-only metrics; and (3) an automated\npipeline that extracts reliable intent from massive dialogue data.", "published": "2025-09-08 05:12:03", "link": "http://arxiv.org/abs/2509.06341v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation", "abstract": "Questionnaire-based surveys are foundational to social science research and\npublic policymaking, yet traditional survey methods remain costly,\ntime-consuming, and often limited in scale. This paper explores a new paradigm:\nsimulating virtual survey respondents using Large Language Models (LLMs). We\nintroduce two novel simulation settings, namely Partial Attribute Simulation\n(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the\nability of LLMs to generate accurate and demographically coherent responses. In\nPAS, the model predicts missing attributes based on partial respondent\nprofiles, whereas FAS involves generating complete synthetic datasets under\nboth zero-context and context-enhanced conditions. We curate a comprehensive\nbenchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey\nSimulation), that spans 11 real-world public datasets across four sociological\ndomains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA\n3.0/3.1-8B) reveals consistent trends in prediction performance, highlights\nfailure modes, and demonstrates how context and prompt design impact simulation\nfidelity. This work establishes a rigorous foundation for LLM-driven survey\nsimulations, offering scalable and cost-effective tools for sociological\nresearch and policy evaluation. Our code and dataset are available at:\nhttps://github.com/dart-lab-research/LLM-S-Cube-Benchmark", "published": "2025-09-08 04:59:00", "link": "http://arxiv.org/abs/2509.06337v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing", "abstract": "Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain\nperformance by employing vision-language models like CLIP. However, existing\nCLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,\nfailing to detect critical spoofing clues. Moreover, these models rely on a\nsingle text prompt per class (e.g., 'live' or 'fake'), which limits\ngeneralization. To address these issues, we propose MVP-FAS, a novel framework\nincorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text\nPatch Alignment (MTPA). Both modules utilize multiple paraphrased texts to\ngenerate generalized features and reduce dependence on domain-specific text.\nMVS extracts local detailed spatial features and global context from patch\nembeddings by leveraging diverse texts with multiple perspectives. MTPA aligns\npatches with multiple text representations to improve semantic robustness.\nExtensive experiments demonstrate that MVP-FAS achieves superior generalization\nperformance, outperforming previous state-of-the-art methods on cross-domain\ndatasets. Code: https://github.com/Elune001/MVP-FAS.", "published": "2025-09-08 04:53:46", "link": "http://arxiv.org/abs/2509.06336v1", "categories": ["cs.CV", "cs.AI", "cs.CR"], "primary_category": "cs.CV"}
{"title": "A Fragile Number Sense: Probing the Elemental Limits of Numerical Reasoning in LLMs", "abstract": "Large Language Models (LLMs) have demonstrated remarkable emergent\ncapabilities, yet the robustness of their numerical reasoning remains an open\nquestion. While standard benchmarks evaluate LLM reasoning on complex problem\nsets using aggregated metrics, they often obscure foundational weaknesses. In\nthis work, we probe LLM mathematical numeracy by evaluating performance on\nproblems of escalating complexity, from constituent operations to combinatorial\npuzzles. We test several state-of-the-art LLM-based agents on a 100-problem\nchallenge comprising four categories: (1) basic arithmetic, (2) advanced\noperations, (3) primality checking, and (4) the Game of 24 number puzzle. Our\nresults show that while the agents achieved high accuracy on the first three\ncategories, which require deterministic algorithmic execution, they\nconsistently failed at the number puzzle, underlining its demand for a\nheuristic search over a large combinatorial space to be a significant\nbottleneck. These findings reveal that the agents' proficiency is largely\nconfined to recalling and executing known algorithms, rather than performing\ngenerative problem-solving. This suggests their apparent numerical reasoning is\nmore akin to sophisticated pattern-matching than flexible, analytical thought,\nlimiting their potential for tasks that require novel or creative numerical\ninsights.", "published": "2025-09-08 04:31:12", "link": "http://arxiv.org/abs/2509.06332v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs", "abstract": "As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to\nreduce network dependency, improve privacy, and enhance responsiveness,\nverifying the legitimacy of models running on local devices becomes critical.\nExisting attestation techniques are not suitable for billion-parameter Large\nLanguage Models (LLMs), struggling to remain both time- and memory-efficient\nwhile addressing emerging threats in the LLM era. In this paper, we present\nAttestLLM, the first-of-its-kind attestation framework to protect the\nhardware-level intellectual property (IP) of device vendors by ensuring that\nonly authorized LLMs can execute on target platforms. AttestLLM leverages an\nalgorithm/software/hardware co-design approach to embed robust watermarking\nsignatures onto the activation distributions of LLM building blocks. It also\noptimizes the attestation protocol within the Trusted Execution Environment\n(TEE), providing efficient verification without compromising inference\nthroughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen,\nand Phi families for on-device use cases demonstrate AttestLLM's attestation\nreliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model\nlegitimacy and exhibits resilience against model replacement and forgery\nattacks.", "published": "2025-09-08 04:17:02", "link": "http://arxiv.org/abs/2509.06326v1", "categories": ["cs.CR", "cs.AI"], "primary_category": "cs.CR"}
{"title": "Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models", "abstract": "Conventional approaches to building energy retrofit decision making suffer\nfrom limited generalizability and low interpretability, hindering adoption in\ndiverse residential contexts. With the growth of Smart and Connected\nCommunities, generative AI, especially large language models (LLMs), may help\nby processing contextual information and producing practitioner readable\nrecommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,\nLlama, and Claude) on residential retrofit decisions under two objectives:\nmaximizing CO2 reduction (technical) and minimizing payback period\n(sociotechnical). Performance is assessed on four dimensions: accuracy,\nconsistency, sensitivity, and reasoning, using a dataset of 400 homes across 49\nUS states. LLMs generate effective recommendations in many cases, reaching up\nto 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.\nPerformance is stronger for the technical objective, while sociotechnical\ndecisions are limited by economic trade offs and local context. Agreement\nacross models is low, and higher performing models tend to diverge from others.\nLLMs are sensitive to location and building geometry but less sensitive to\ntechnology and occupant behavior. Most models show step by step, engineering\nstyle reasoning, but it is often simplified and lacks deeper contextual\nawareness. Overall, LLMs are promising assistants for energy retrofit decision\nmaking, but improvements in accuracy, consistency, and context handling are\nneeded for reliable practice.", "published": "2025-09-08 03:13:47", "link": "http://arxiv.org/abs/2509.06307v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion", "abstract": "Traditional RL-based locomotion controllers often suffer from low data\nefficiency, requiring extensive interaction to achieve robust performance. We\npresent a model-based reinforcement learning (MBRL) framework that improves\nsample efficiency for quadrupedal locomotion by appending synthetic data to the\nend of standard rollouts in PPO-based controllers, following the Dyna-Style\nparadigm. A predictive model, trained alongside the policy, generates\nshort-horizon synthetic transitions that are gradually integrated using a\nscheduling strategy based on the policy update iterations. Through an ablation\nstudy, we identified a strong correlation between sample efficiency and rollout\nlength, which guided the design of our experiments. We validated our approach\nin simulation on the Unitree Go1 robot and showed that replacing part of the\nsimulated steps with synthetic ones not only mimics extended rollouts but also\nimproves policy return and reduces variance. Finally, we demonstrate that this\nimprovement transfers to the ability to track a wide range of locomotion\ncommands using fewer simulated steps.", "published": "2025-09-08 02:48:23", "link": "http://arxiv.org/abs/2509.06296v1", "categories": ["cs.RO", "cs.AI"], "primary_category": "cs.RO"}
{"title": "Statistical Inference for Misspecified Contextual Bandits", "abstract": "Contextual bandit algorithms have transformed modern experimentation by\nenabling real-time adaptation for personalized treatment and efficient use of\ndata. Yet these advantages create challenges for statistical inference due to\nadaptivity. A fundamental property that supports valid inference is policy\nconvergence, meaning that action-selection probabilities converge in\nprobability given the context. Convergence ensures replicability of adaptive\nexperiments and stability of online algorithms. In this paper, we highlight a\npreviously overlooked issue: widely used algorithms such as LinUCB may fail to\nconverge when the reward model is misspecified, and such non-convergence\ncreates fundamental obstacles for statistical inference. This issue is\npractically important, as misspecified models -- such as linear approximations\nof complex dynamic system -- are often employed in real-world adaptive\nexperiments to balance bias and variance.\n  Motivated by this insight, we propose and analyze a broad class of algorithms\nthat are guaranteed to converge even under model misspecification. Building on\nthis guarantee, we develop a general inference framework based on an\ninverse-probability-weighted Z-estimator (IPW-Z) and establish its asymptotic\nnormality with a consistent variance estimator. Simulation studies confirm that\nthe proposed method provides robust and data-efficient confidence intervals,\nand can outperform existing approaches that exist only in the special case of\noffline policy evaluation. Taken together, our results underscore the\nimportance of designing adaptive algorithms with built-in convergence\nguarantees to enable stable experimentation and valid statistical inference in\npractice.", "published": "2025-09-08 02:19:37", "link": "http://arxiv.org/abs/2509.06287v1", "categories": ["math.ST", "cs.AI", "stat.TH"], "primary_category": "math.ST"}
{"title": "From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs", "abstract": "Large language models (LLMs) have advanced general-purpose reasoning, showing\nstrong performance across diverse tasks. However, existing methods often rely\non implicit exploration, where the model follows stochastic and unguided\nreasoning paths-like walking without a map. This leads to unstable reasoning\npaths, lack of error correction, and limited learning from past experience. To\naddress these issues, we propose a framework that shifts from implicit\nexploration to structured reasoning through guideline and refinement. First, we\nextract structured reasoning patterns from successful trajectories and\nreflective signals from failures. During inference, the model follows these\nguidelines step-by-step, with refinement applied after each step to correct\nerrors and stabilize the reasoning process. Experiments on BBH and four\nadditional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method\nconsistently outperforms strong baselines across diverse reasoning tasks.\nStructured reasoning with stepwise execution and refinement improves stability\nand generalization, while guidelines transfer well across domains and flexibly\nsupport cross-model collaboration, matching or surpassing supervised\nfine-tuning in effectiveness and scalability.", "published": "2025-09-08 02:11:49", "link": "http://arxiv.org/abs/2509.06284v1", "categories": ["cs.AI", "cs.LG"], "primary_category": "cs.AI"}
{"title": "TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning", "abstract": "Table reasoning is crucial for leveraging structured data in domains such as\nfinance, healthcare, and scientific research. While large language models\n(LLMs) show promise in multi-step reasoning, purely text-based methods often\nstruggle with the complex numerical computations and fine-grained operations\ninherently required in this task. Tool-integrated reasoning improves\ncomputational accuracy via explicit code execution, yet existing systems\nfrequently rely on rigid patterns, supervised imitation, and lack true\nautonomous adaptability. In this paper, we present TableMind, an LLM-driven\ntable reasoning agent that (i) autonomously performs multi-turn tool\ninvocation, (ii) writes and executes data-analyzing code in a secure sandbox\nenvironment for data analysis and precise numerical reasoning, and (iii)\nexhibits high-level capabilities such as planning and self-reflection to adapt\nstrategies. To realize these capabilities, we adopt a two-stage fine-tuning\nparadigm built on top of a powerful pre-trained language model: supervised\nfine-tuning on high-quality reasoning trajectories to establish effective tool\nusage patterns, followed by reinforcement fine-tuning to optimize\nmulti-objective strategies. In particular, we propose Rank-Aware Policy\nOptimization (RAPO), which increases the update weight of high-quality\ntrajectories when their output probabilities are lower than those of\nlow-quality ones, thereby guiding the model more consistently toward better and\nmore accurate answers. Extensive experiments on several mainstream benchmarks\ndemonstrate that TableMind achieves superior performance compared to\ncompetitive baselines, yielding substantial gains in both reasoning accuracy\nand computational precision.", "published": "2025-09-08 02:00:31", "link": "http://arxiv.org/abs/2509.06278v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks", "abstract": "Sixth generation (6G) systems require environment-aware communication, driven\nby native artificial intelligence (AI) and integrated sensing and communication\n(ISAC). Radio maps (RMs), providing spatially continuous channel information,\nare key enablers. However, generating high-fidelity RM ground truth via\nelectromagnetic (EM) simulations is computationally intensive, motivating\nmachine learning (ML)-based RM construction. The effectiveness of these\ndata-driven methods depends on large-scale, high-quality training data. Current\npublic datasets often focus on single-input single-output (SISO) and limited\ninformation, such as path loss, which is insufficient for advanced multi-input\nmulti-output (MIMO) systems requiring detailed channel state information (CSI).\nTo address this gap, this paper presents UrbanMIMOMap, a novel large-scale\nurban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap\noffers comprehensive complex CSI matrices across a dense spatial grid, going\nbeyond traditional path loss data. This rich CSI is vital for constructing\nhigh-fidelity RMs and serves as a fundamental resource for data-driven RM\ngeneration, including deep learning. We demonstrate the dataset's utility\nthrough baseline performance evaluations of representative ML methods for RM\nconstruction. This work provides a crucial dataset and reference for research\nin high-precision RM generation, MIMO spatial performance, and ML for 6G\nenvironment awareness. The code and data for this work are available at:\nhttps://github.com/UNIC-Lab/UrbanMIMOMap.", "published": "2025-09-08 01:23:46", "link": "http://arxiv.org/abs/2509.06270v1", "categories": ["cs.LG", "cs.AI"], "primary_category": "cs.LG"}
{"title": "REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents", "abstract": "Personalized AI assistants often struggle to incorporate complex personal\ndata and causal knowledge, leading to generic advice that lacks explanatory\npower. We propose REMI, a Causal Schema Memory architecture for a multimodal\nlifestyle agent that integrates a personal causal knowledge graph, a causal\nreasoning engine, and a schema based planning module. The idea is to deliver\nexplainable, personalized recommendations in domains like fashion, personal\nwellness, and lifestyle planning. Our architecture uses a personal causal graph\nof the user's life events and habits, performs goal directed causal traversals\nenriched with external knowledge and hypothetical reasoning, and retrieves\nadaptable plan schemas to generate tailored action plans. A Large Language\nModel orchestrates these components, producing answers with transparent causal\nexplanations. We outline the CSM system design and introduce new evaluation\nmetrics for personalization and explainability, including Personalization\nSalience Score and Causal Reasoning Accuracy, to rigorously assess its\nperformance. Results indicate that CSM based agents can provide more context\naware, user aligned recommendations compared to baseline LLM agents. This work\ndemonstrates a novel approach to memory augmented, causal reasoning in\npersonalized agents, advancing the development of transparent and trustworthy\nAI lifestyle assistants.", "published": "2025-09-08 01:17:46", "link": "http://arxiv.org/abs/2509.06269v1", "categories": ["cs.AI"], "primary_category": "cs.AI"}
{"title": "On Synthesis of Timed Regular Expressions", "abstract": "Timed regular expressions serve as a formalism for specifying real-time\nbehaviors of Cyber-Physical Systems. In this paper, we consider the synthesis\nof timed regular expressions, focusing on generating a timed regular expression\nconsistent with a given set of system behaviors including positive and negative\nexamples, i.e., accepting all positive examples and rejecting all negative\nexamples. We first prove the decidability of the synthesis problem through an\nexploration of simple timed regular expressions. Subsequently, we propose our\nmethod of generating a consistent timed regular expression with minimal length,\nwhich unfolds in two steps. The first step is to enumerate and prune candidate\nparametric timed regular expressions. In the second step, we encode the\nrequirement that a candidate generated by the first step is consistent with the\ngiven set into a Satisfiability Modulo Theories (SMT) formula, which is\nconsequently solved to determine a solution to parametric time constraints.\nFinally, we evaluate our approach on benchmarks, including randomly generated\nbehaviors from target timed models and a case study.", "published": "2025-09-08 00:59:04", "link": "http://arxiv.org/abs/2509.06262v1", "categories": ["cs.FL", "cs.AI"], "primary_category": "cs.FL"}
{"title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions", "abstract": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.", "published": "2025-09-08 17:58:30", "link": "http://arxiv.org/abs/2509.06951v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data", "abstract": "Large transformer-based models have made significant progress in\ngeneralizable novel view synthesis (NVS) from sparse input views, generating\nnovel viewpoints without the need for test-time optimization. However, these\nmodels are constrained by the limited diversity of publicly available scene\ndatasets, making most real-world (in-the-wild) scenes out-of-distribution. To\novercome this, we incorporate synthetic training data generated from diffusion\nmodels, which improves generalization across unseen domains. While synthetic\ndata offers scalability, we identify artifacts introduced during data\ngeneration as a key bottleneck affecting reconstruction quality. To address\nthis, we propose a token disentanglement process within the transformer\narchitecture, enhancing feature separation and ensuring more effective\nlearning. This refinement not only improves reconstruction quality over\nstandard transformers but also enables scalable training with synthetic data.\nAs a result, our method outperforms existing models on both in-dataset and\ncross-dataset evaluations, achieving state-of-the-art results across multiple\nbenchmarks while significantly reducing computational costs. Project page:\nhttps://scaling3dnvs.github.io/", "published": "2025-09-08 17:58:06", "link": "http://arxiv.org/abs/2509.06950v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "LLaDA-VLA: Vision Language Diffusion Action Models", "abstract": "The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.", "published": "2025-09-08 17:45:40", "link": "http://arxiv.org/abs/2509.06932v1", "categories": ["cs.RO", "cs.CV"], "primary_category": "cs.RO"}
{"title": "FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data", "abstract": "Vision-driven field monitoring is central to digital agriculture, yet models\nbuilt on general-domain pretrained backbones often fail to generalize across\ntasks, owing to the interaction of fine, variable canopy structures with\nfluctuating field conditions. We present FoMo4Wheat, one of the first\ncrop-domain vision foundation model pretrained with self-supervision on\nImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5\nmillion high-resolution images collected over a decade at 30 global sites,\nspanning >2,000 genotypes and >500 environmental conditions). This\nwheat-specific pretraining yields representations that are robust for wheat and\ntransferable to other crops and weeds. Across ten in-field vision tasks at\ncanopy and organ levels, FoMo4Wheat models consistently outperform\nstate-of-the-art models pretrained on general-domain dataset. These results\ndemonstrate the value of crop-specific foundation models for reliable in-field\nperception and chart a path toward a universal crop foundation model with\ncross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat\ndataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat\nand https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:\nhttps://fomo4wheat.phenix-lab.com/.", "published": "2025-09-08 17:23:28", "link": "http://arxiv.org/abs/2509.06907v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration", "abstract": "This paper introduces BIR-Adapter, a low-complexity blind image restoration\nadapter for diffusion models. The BIR-Adapter enables the utilization of the\nprior of pre-trained large-scale diffusion models on blind image restoration\nwithout training any auxiliary feature extractor. We take advantage of the\nrobustness of pretrained models. We extract features from degraded images via\nthe model itself and extend the self-attention mechanism with these degraded\nfeatures. We introduce a sampling guidance mechanism to reduce hallucinations.\nWe perform experiments on synthetic and real-world degradations and demonstrate\nthat BIR-Adapter achieves competitive or better performance compared to\nstate-of-the-art methods while having significantly lower complexity.\nAdditionally, its adapter-based design enables integration into other diffusion\nmodels, enabling broader applications in image restoration tasks. We showcase\nthis by extending a super-resolution-only model to perform better under\nadditional unknown degradations.", "published": "2025-09-08 17:22:18", "link": "http://arxiv.org/abs/2509.06904v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization", "abstract": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with\nreal-time 2D radiographs, enabling accurate localization of instruments and\nimplants. A recent fully differentiable similarity learning framework\napproximates geodesic distances on SE(3), expanding the capture range of\nregistration and mitigating the effects of substantial disturbances, but\nexisting Euclidean approximations distort manifold structure and slow\nconvergence. To address these limitations, we explore similarity learning in\nnon-Euclidean spherical feature spaces to better capture and fit complex\nmanifold structure. We extract feature embeddings using a CNN-Transformer\nencoder, project them into spherical space, and approximate their geodesic\ndistances with Riemannian distances in the bi-invariant SO(4) space. This\nenables a more expressive and geometrically consistent deep similarity metric,\nenhancing the ability to distinguish subtle pose differences. During inference,\nwe replace gradient descent with fully differentiable Levenberg-Marquardt\noptimization to accelerate convergence. Experiments on real and synthetic\ndatasets show superior accuracy in both patient-specific and patient-agnostic\nscenarios.", "published": "2025-09-08 17:10:43", "link": "http://arxiv.org/abs/2509.06890v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition", "abstract": "Automatic License-Plate Recognition (ALPR) plays a pivotal role in\nIntelligent Transportation Systems (ITS) as a fundamental element of Smart\nCities. However, due to its high variability, ALPR faces challenging issues\nmore efficiently addressed by deep learning techniques. In this paper, a\nselective Generative Adversarial Network (GAN) is proposed for deblurring in\nthe preprocessing step, coupled with the state-of-the-art You-Only-Look-Once\n(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and\nthe integrated Character Segmentation (CS) and Character Recognition (CR)\nsteps. The selective preprocessing bypasses unnecessary and sometimes\ncounter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high\naccuracy and low computing cost. As a result, YOLOv5 achieves a detection time\nof 0.026 seconds for both LP and CR detection stages, facilitating real-time\napplications with exceptionally rapid responsiveness. Moreover, the proposed\nmodel achieves accuracy rates of 95\\% and 97\\% in the LPD and CR detection\nphases, respectively. Furthermore, the inclusion of the Deblur-GAN\npre-processor significantly improves detection accuracy by nearly 40\\%,\nespecially when encountering blurred License Plates (LPs).To train and test the\nlearning components, we generated and publicly released our blur and ALPR\ndatasets (using Iranian license plates as a use-case), which are more\nrepresentative of close-to-real-life ad-hoc situations. The findings\ndemonstrate that employing the state-of-the-art YOLO model results in excellent\noverall precision and detection time, making it well-suited for portable\napplications. Additionally, integrating the Deblur-GAN model as a preliminary\nprocessing step enhances the overall effectiveness of our comprehensive model,\nparticularly when confronted with blurred scenes captured by the camera as\ninput.", "published": "2025-09-08 16:34:54", "link": "http://arxiv.org/abs/2509.06868v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Matching Shapes Under Different Topologies: A Topology-Adaptive Deformation Guided Approach", "abstract": "Non-rigid 3D mesh matching is a critical step in computer vision and computer\ngraphics pipelines. We tackle matching meshes that contain topological\nartefacts which can break the assumption made by current approaches. While\nFunctional Maps assume the deformation induced by the ground truth\ncorrespondences to be near-isometric, ARAP-like deformation-guided approaches\nassume the latter to be ARAP. Neither assumption holds in certain topological\nconfigurations of the input shapes. We are motivated by real-world scenarios\nsuch as per-frame multi-view reconstructions, often suffering from topological\nartefacts. To this end, we propose a topology-adaptive deformation model\nallowing changes in shape topology to align shape pairs under ARAP and\nbijective association constraints. Using this model, we jointly optimise for a\ntemplate mesh with adequate topology and for its alignment with the shapes to\nbe matched to extract correspondences. We show that, while not relying on any\ndata-driven prior, our approach applies to highly non-isometric shapes and\nshapes with topological artefacts, including noisy per-frame multi-view\nreconstructions, even outperforming methods trained on large datasets in 3D\nalignment quality.", "published": "2025-09-08 16:29:44", "link": "http://arxiv.org/abs/2509.06862v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "ToonOut: Fine-tuned Background-Removal for Anime Characters", "abstract": "While state-of-the-art background removal models excel at realistic imagery,\nthey frequently underperform in specialized domains such as anime-style\ncontent, where complex features like hair and transparency present unique\nchallenges. To address this limitation, we collected and annotated a custom\ndataset of 1,228 high-quality anime images of characters and objects, and\nfine-tuned the open-sourced BiRefNet model on this dataset. This resulted in\nmarked improvements in background removal accuracy for anime-style images,\nincreasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric.\nWe are open-sourcing the code, the fine-tuned model weights, as well as the\ndataset at: https://github.com/MatteoKartoon/BiRefNet.", "published": "2025-09-08 16:08:56", "link": "http://arxiv.org/abs/2509.06839v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset", "abstract": "Adversarial attacks pose significant threats to machine learning models by\nintroducing carefully crafted perturbations that cause misclassification. While\nprior work has primarily focused on MNIST and similar datasets, this paper\ninvestigates the vulnerability of traffic sign classifiers using the LISA\nTraffic Sign dataset. We train a convolutional neural network to classify 47\ndifferent traffic signs and evaluate its robustness against Fast Gradient Sign\nMethod (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a\nsharp decline in classification accuracy as the perturbation magnitude\nincreases, highlighting the models susceptibility to adversarial examples. This\nstudy lays the groundwork for future exploration into defense mechanisms\ntailored for real-world traffic sign recognition systems.", "published": "2025-09-08 16:06:41", "link": "http://arxiv.org/abs/2509.06835v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis", "abstract": "We investigate how both the adaptation of a generic foundation model via\ntransfer learning and the integration of complementary modalities from the\noperating room (OR) can support surgical data science. To this end, we use\nV-JEPA as the single-modality foundation of a multimodal model for minimally\ninvasive surgery support. We analyze how the model's downstream performance can\nbenefit (a) from finetuning on unlabeled surgical video data and (b) from\nproviding additional time-resolved data streams from the OR in a multimodal\nsetup.\n  In an in-house dataset of liver surgery videos, we analyze the tasks of\npredicting hospital length of stay and postoperative complications. In videos\nof the public HeiCo dataset, we analyze the task of surgical phase recognition.\nAs a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on\nunlabeled, held-out videos to investigate its change in performance after\ndomain adaptation. Following the idea of modular decision support networks, we\nintegrate additional data streams from the OR by training a separate encoder to\nform a shared representation space with V-JEPA's embeddings.\n  Our experiments show that finetuning on domain-specific data increases model\nperformance. On the in-house data, integrating additional time-resolved data\nlikewise benefits the model. On the HeiCo data, accuracy of the pretrained\nvideo-only, single-modality baseline setup is on par with the top-performing\nsubmissions of the EndoVis2017 challenge, while finetuning on domain-specific\ndata increases accuracy further. Our results thus demonstrate how surgical data\nscience can leverage public, generic foundation models. Likewise, they indicate\nthe potential of domain adaptation and of integrating suitable complementary\ndata streams from the OR. To support further research, we release our code and\nmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.", "published": "2025-09-08 16:04:19", "link": "http://arxiv.org/abs/2509.06831v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Curia: A Multi-Modal Foundation Model for Radiology", "abstract": "AI-assisted radiological interpretation is based on predominantly narrow,\nsingle-task models. This approach is impractical for covering the vast spectrum\nof imaging modalities, diseases, and radiological findings. Foundation models\n(FMs) hold the promise of broad generalization across modalities and in\nlow-data settings. However, this potential has remained largely unrealized in\nradiology. We introduce Curia, a foundation model trained on the entire\ncross-sectional imaging output of a major hospital over several years, which to\nour knowledge is the largest such corpus of real-world data-encompassing\n150,000 exams (130 TB). On a newly curated 19-task external validation\nbenchmark, Curia accurately identifies organs, detects conditions like brain\nhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.\nCuria meets or surpasses the performance of radiologists and recent foundation\nmodels, and exhibits clinically significant emergent properties in\ncross-modality, and low-data regimes. To accelerate progress, we release our\nbase model's weights at https://huggingface.co/raidium/curia.", "published": "2025-09-08 16:04:12", "link": "http://arxiv.org/abs/2509.06830v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning", "abstract": "The rapid growth of visual content consumption across platforms necessitates\nautomated video classification for age-suitability standards like the MPAA\nrating system (G, PG, PG-13, R). Traditional methods struggle with large\nlabeled data requirements, poor generalization, and inefficient feature\nlearning. To address these challenges, we employ contrastive learning for\nimproved discrimination and adaptability, exploring three frameworks: Instance\nDiscrimination, Contextual Contrastive Learning, and Multi-View Contrastive\nLearning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a\nBahdanau attention mechanism, achieving state-of-the-art performance in the\nContextual Contrastive Learning framework, with 88% accuracy and an F1 score of\n0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,\nand attention mechanisms for dynamic frame prioritization, the model excels in\nfine-grained borderline distinctions, such as differentiating PG-13 and R-rated\ncontent. We evaluate the model's performance across various contrastive loss\nfunctions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating\nthe robustness of our proposed architecture. To ensure practical application,\nthe model is deployed as a web application for real-time MPAA rating\nclassification, offering an efficient solution for automated content compliance\nacross streaming platforms.", "published": "2025-09-08 16:01:02", "link": "http://arxiv.org/abs/2509.06826v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward", "abstract": "Recent advancements in image customization exhibit a wide range of\napplication prospects due to stronger customization capabilities. However,\nsince we humans are more sensitive to faces, a significant challenge remains in\npreserving consistent identity while avoiding identity confusion with\nmulti-reference images, limiting the identity scalability of customization\nmodels. To address this, we present UMO, a Unified Multi-identity Optimization\nframework, designed to maintain high-fidelity identity preservation and\nalleviate identity confusion with scalability. With \"multi-to-multi matching\"\nparadigm, UMO reformulates multi-identity generation as a global assignment\noptimization problem and unleashes multi-identity consistency for existing\nimage customization methods generally through reinforcement learning on\ndiffusion models. To facilitate the training of UMO, we develop a scalable\ncustomization dataset with multi-reference images, consisting of both\nsynthesised and real parts. Additionally, we propose a new metric to measure\nidentity confusion. Extensive experiments demonstrate that UMO not only\nimproves identity consistency significantly, but also reduces identity\nconfusion on several image customization methods, setting a new\nstate-of-the-art among open-source methods along the dimension of identity\npreserving. Code and model: https://github.com/bytedance/UMO", "published": "2025-09-08 15:54:55", "link": "http://arxiv.org/abs/2509.06818v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration", "abstract": "We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address\ncritical limitations in current motion restoration benchmarks. Designed with\nhigh-frame-rate (1000 FPS) acquisition and professional-grade optics, our\ndatasets capture a broad spectrum of motion scenarios, which include complex\nego-camera movements, dynamic multi-subject interactions, and depth-dependent\nblur effects. By adaptively averaging frames based on computed optical flow\nmetrics, MIORe generates consistent motion blur, and preserves sharp inputs for\nvideo frame interpolation and optical flow estimation. VAR-MIORe further\nextends by spanning a variable range of motion magnitudes, from minimal to\nextreme, establishing the first benchmark to offer explicit control over motion\namplitude. We provide high-resolution, scalable ground truths that challenge\nexisting algorithms under both controlled and adverse conditions, paving the\nway for next-generation research of various image and video restoration tasks.", "published": "2025-09-08 15:34:31", "link": "http://arxiv.org/abs/2509.06803v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis", "abstract": "In the field of autonomous driving, sensor simulation is essential for\ngenerating rare and diverse scenarios that are difficult to capture in\nreal-world environments. Current solutions fall into two categories: 1)\nCG-based methods, such as CARLA, which lack diversity and struggle to scale to\nthe vast array of rare cases required for robust perception training; and 2)\nlearning-based approaches, such as NeuSim, which are limited to specific object\ncategories (vehicles) and require extensive multi-sensor data, hindering their\napplicability to generic objects. To address these limitations, we propose a\nscalable real2sim2real system that leverages 3D generation to automate asset\nmining, generation, and rare-case data synthesis.", "published": "2025-09-08 15:29:49", "link": "http://arxiv.org/abs/2509.06798v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results", "abstract": "This paper presents a comprehensive review of the AIM 2025 High FPS\nNon-Uniform Motion Deblurring Challenge, highlighting the proposed solutions\nand final results. The objective of this challenge is to identify effective\nnetworks capable of producing clearer and visually compelling images in diverse\nand challenging conditions, by learning representative visual cues for complex\naggregations of motion types. A total of 68 participants registered for the\ncompetition, and 9 teams ultimately submitted valid entries. This paper\nthoroughly evaluates the state-of-the-art advances in high-FPS single image\nmotion deblurring, showcasing the significant progress in the field, while\nleveraging samples of the novel dataset, MIORe, that introduces challenging\nexamples of movement patterns.", "published": "2025-09-08 15:22:35", "link": "http://arxiv.org/abs/2509.06793v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "P3-SAM: Native 3D Part Segmentation", "abstract": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.", "published": "2025-09-08 15:12:17", "link": "http://arxiv.org/abs/2509.06784v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets", "abstract": "This article presents UrbanTwin datasets - high-fidelity, realistic replicas\nof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.\nEach UrbanTwin dataset contains 10K annotated frames corresponding to one of\nthe public datasets. Annotations include 3D bounding boxes, instance\nsegmentation labels, and tracking IDs for six object classes, along with\nsemantic segmentation labels for nine classes. These datasets are synthesized\nusing emulated lidar sensors within realistic digital twins, modeled based on\nsurrounding geometry, road alignment at lane level, and the lane topology and\nvehicle movement patterns at intersections of the actual locations\ncorresponding to each real dataset. Due to the precise digital twin modeling,\nthe synthetic datasets are well aligned with their real counterparts, offering\nstrong standalone and augmentative value for training deep learning models on\ntasks such as 3D object detection, tracking, and semantic and instance\nsegmentation. We evaluate the alignment of the synthetic replicas through\nstatistical and structural similarity analysis with real data, and further\ndemonstrate their utility by training 3D object detection models solely on\nsynthetic data and testing them on real, unseen data. The high similarity\nscores and improved detection performance, compared to the models trained on\nreal data, indicate that the UrbanTwin datasets effectively enhance existing\nbenchmark datasets by increasing sample size and scene diversity. In addition,\nthe digital twins can be adapted to test custom scenarios by modifying the\ndesign and dynamics of the simulations. To our knowledge, these are the first\ndigitally synthesized datasets that can replace in-domain real-world datasets\nfor lidar perception tasks. UrbanTwin datasets are publicly available at\nhttps://dataverse.harvard.edu/dataverse/ucf-ut.", "published": "2025-09-08 15:06:02", "link": "http://arxiv.org/abs/2509.06781v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning", "abstract": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning", "published": "2025-09-08 14:55:16", "link": "http://arxiv.org/abs/2509.06771v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Raw2Event: Converting Raw Frame Camera into Event Camera", "abstract": "Event cameras offer unique advantages such as high temporal resolution, low\nlatency, and high dynamic range, making them more and more popular for vision\ntasks under challenging light conditions. However, their high cost, limited\nresolution, and lack of features such as autofocus hinder their broad adoption,\nparticularly for early-stage development and prototyping. In this work, we\npresent Raw2Event, a complete hardware-software system that enables real-time\nevent generation from low-cost raw frame-based cameras. By leveraging direct\naccess to raw Bayer data and bypassing traditional image signal processors\n(ISP), our system is able to utilize the full potential of camera hardware,\ndelivering higher dynamic range, higher resolution, and more faithful output\nthan RGB-based frame-to-event converters.\n  Built upon the DVS-Voltmeter model, Raw2Event features a configurable\nsimulation framework optimized for deployment on embedded platforms. We further\ndesign a data acquisition pipeline that supports synchronized recording of raw,\nRGB, and event streams, facilitating downstream evaluation and dataset\ncreation. Experimental results show that Raw2Event can generate event streams\nclosely resembling those from real event cameras, while benefiting from higher\nresolution and autofocus capabilities. The system also supports user-intuitive\nparameter tuning, enabling flexible adaptation to various application\nrequirements. Finally, we deploy the system on a Raspberry Pi for real-time\noperation, providing a scalable and cost-effective solution for event-based\nvision research and early-stage system development.\n  The codes are available online:\nhttps://anonymous.4open.science/r/raw2event-BFF2/README.md.", "published": "2025-09-08 14:53:01", "link": "http://arxiv.org/abs/2509.06767v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Pothole Detection and Recognition based on Transfer Learning", "abstract": "With the rapid development of computer vision and machine learning, automated\nmethods for pothole detection and recognition based on image and video data\nhave received significant attention. It is of great significance for social\ndevelopment to conduct an in-depth analysis of road images through feature\nextraction, thereby achieving automatic identification of the pothole condition\nin new images. Consequently, this is the main issue addressed in this study.\nBased on preprocessing techniques such as standardization, normalization, and\ndata augmentation applied to the collected raw dataset, we continuously\nimproved the network model based on experimental results. Ultimately, we\nconstructed a deep learning feature extraction network\nResNet50-EfficientNet-RegNet model based on transfer learning. This model\nexhibits high classification accuracy and computational efficiency. In terms of\nmodel evaluation, this study employed a comparative evaluation approach by\ncomparing the performance of the proposed transfer learning model with other\nmodels, including Random Forest, MLP, SVM, and LightGBM. The comparison\nanalysis was conducted based on metrics such as Accuracy, Recall, Precision,\nF1-score, and FPS, to assess the classification performance of the transfer\nlearning model proposed in this paper. The results demonstrate that our model\nexhibits high performance in terms of recognition speed and accuracy,\nsurpassing the performance of other models. Through careful parameter selection\nand model optimization, our transfer learning model achieved a classification\naccuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%\n(890/900) on the expanded test set.", "published": "2025-09-08 14:40:16", "link": "http://arxiv.org/abs/2509.06750v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light", "abstract": "Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest\nenvironments for tasks such as environmental monitoring and search and rescue,\nwhich require safe navigation through dense foliage and precise data\ncollection. Traditional sensing approaches, including passive multispectral and\nRGB imaging, suffer from latency, poor depth resolution, and strong dependence\non ambient light - especially under forest canopies. In this work, we present a\nnovel event spectroscopy system that simultaneously enables high-resolution,\nlow-latency depth reconstruction and multispectral imaging using a single\nsensor. Depth is reconstructed using structured light, and by modulating the\nwavelength of the projected structured light, our system captures spectral\ninformation in controlled bands between 650 nm and 850 nm. We demonstrate up to\n$60\\%$ improvement in RMSE over commercial depth sensors and validate the\nspectral accuracy against a reference spectrometer and commercial multispectral\ncameras, demonstrating comparable performance. A portable version limited to\nRGB (3 wavelengths) is used to collect real-world depth and spectral data from\na Masoala Rainforest. We demonstrate the use of this prototype for color image\nreconstruction and material differentiation between leaves and branches using\nspectral and depth data. Our results show that adding depth (available at no\nextra effort with our setup) to material differentiation improves the accuracy\nby over $30\\%$ compared to color-only method. Our system, tested in both lab\nand real-world rainforest environments, shows strong performance in depth\nestimation, RGB reconstruction, and material differentiation - paving the way\nfor lightweight, integrated, and robust UAV perception and data collection in\ncomplex natural environments.", "published": "2025-09-08 14:34:55", "link": "http://arxiv.org/abs/2509.06741v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation", "abstract": "Histopathology image analysis is critical yet challenged by the demand of\nsegmenting tissue regions and nuclei instances for tumor microenvironment and\ncellular morphology analysis. Existing studies focused on tissue semantic\nsegmentation or nuclei instance segmentation separately, but ignored the\ninherent relationship between these two tasks, resulting in insufficient\nhistopathology understanding. To address this issue, we propose a Co-Seg\nframework for collaborative tissue and nuclei segmentation. Specifically, we\nintroduce a novel co-segmentation paradigm, allowing tissue and nuclei\nsegmentation tasks to mutually enhance each other. To this end, we first devise\na region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and\ninstance region prompts as prior constraints. Moreover, we design a mutual\nprompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen\nthe contextual consistency of both tasks, collaboratively computing semantic\nand instance segmentation masks. Extensive experiments on the PUMA dataset\ndemonstrate that the proposed Co-Seg surpasses state-of-the-arts in the\nsemantic, instance and panoptic segmentation of tumor tissues and nuclei\ninstances. The source code is available at https://github.com/xq141839/Co-Seg.", "published": "2025-09-08 14:34:54", "link": "http://arxiv.org/abs/2509.06740v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training", "abstract": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches.", "published": "2025-09-08 14:21:45", "link": "http://arxiv.org/abs/2509.06723v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention", "abstract": "We present Cortex Synth, a novel end-to-end differentiable framework for\njoint 3D skeleton geometry and topology synthesis from single 2D images. Our\narchitecture introduces three key innovations: (1) A hierarchical graph\nattention mechanism with multi-scale skeletal refinement, (2) Differentiable\nspectral topology optimization via Laplacian eigen decomposition, and (3)\nAdversarial geometric consistency training for pose structure alignment. The\nframework integrates four synergistic modules: a pseudo 3D point cloud\ngenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a\nnovel Differentiable Graph Construction Network (DGCN). Our experiments\ndemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and\n27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological\nerrors by 42 percent compared to previous approaches. The model's end-to-end\ndifferentiability enables applications in robotic manipulation, medical\nimaging, and automated character rigging.", "published": "2025-09-08 14:03:13", "link": "http://arxiv.org/abs/2509.06705v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment", "abstract": "Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal\nrole in enhancing the performance of downstream anomaly segmentation, as it\nprovides an effective means of expanding abnormal data. However, existing SIAS\nmethods face several critical limitations: (i) the synthesized anomalies often\nlack intricate texture details and fail to align precisely with the surrounding\nbackground, and (ii) they struggle to generate fine-grained, pixel-level\nanomalies. To address these challenges, we propose Segmentation-oriented\nAnomaly synthesis via Graded diffusion with Explicit mask alignment, termed\nSTAGE. STAGE introduces a novel anomaly inference strategy that incorporates\nclean background information as a prior to guide the denoising distribution,\nenabling the model to more effectively distinguish and highlight abnormal\nforegrounds. Furthermore, it employs a graded diffusion framework with an\nanomaly-only branch to explicitly record local anomalies during both the\nforward and reverse processes, ensuring that subtle anomalies are not\noverlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)\nstrategy to progressively align the synthesized anomalies with the background,\nresulting in context-consistent and structurally coherent generations.\nExtensive experiments on the MVTec and BTAD datasets demonstrate that STAGE\nachieves state-of-the-art performance in SIAS, which in turn enhances\ndownstream anomaly segmentation.", "published": "2025-09-08 13:47:01", "link": "http://arxiv.org/abs/2509.06693v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes", "abstract": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for\nnovel-view synthesis (NVS) in large scenes. GS typically requires accurate\ndepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited\ndepth sensing range makes it difficult for GS to work in large scenes.\nMonocular images, however, lack depth to guide the learning and lead to\ninferior NVS results. Although large foundation models (LFMs) for monocular\ndepth estimation are available, they suffer from cross-frame inconsistency,\ninaccuracy for distant scenes, and ambiguity in deceptive texture cues. This\npaper aims to generate dense, accurate depth images from monocular RGB inputs\nfor high-definite GS rendering. The key idea is to leverage the accurate but\nsparse depth from visual-inertial Structure-from-Motion (SfM) to refine the\ndense but coarse depth from LFMs. To bridge the sparse input and dense output,\nwe propose an object-segmented depth propagation algorithm that renders the\ndepth of pixels of structured objects. Then we develop a dynamic depth\nrefinement module to handle the crippled SfM depth of dynamic objects and\nrefine the coarse LFM depth. Experiments using public and customized datasets\ndemonstrate the superior rendering quality of VIM-GS in large scenes.", "published": "2025-09-08 13:41:10", "link": "http://arxiv.org/abs/2509.06685v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations", "abstract": "As long-endurance and seafloor-resident AUVs become more capable, there is an\nincreasing need for extended, real-time interpretation of seafloor imagery to\nenable adaptive missions and optimise communication efficiency. Although\noffline image analysis methods are well established, they rely on access to\ncomplete datasets and human-labelled examples to manage the strong influence of\nenvironmental and operational conditions on seafloor image\nappearance-requirements that cannot be met in real-time settings. To address\nthis, we introduce an online clustering framework (OCF) capable of interpreting\nseafloor imagery without supervision, which is designed to operate in real-time\non continuous data streams in a scalable, adaptive, and self-consistent manner.\nThe method enables the efficient review and consolidation of common patterns\nacross the entire data history in constant time by identifying and maintaining\na set of representative samples that capture the evolving feature distribution,\nsupporting dynamic cluster merging and splitting without reprocessing the full\nimage history. We evaluate the framework on three diverse seafloor image\ndatasets, analysing the impact of different representative sampling strategies\non both clustering accuracy and computational cost. The OCF achieves the\nhighest average F1 score of 0.68 across the three datasets among all\ncomparative online clustering approaches, with a standard deviation of 3%\nacross three distinct survey trajectories, demonstrating its superior\nclustering capability and robustness to trajectory variation. In addition, it\nmaintains consistently lower and bounded computational time as the data volume\nincreases. These properties are beneficial for generating survey data summaries\nand supporting informative path planning in long-term, persistent autonomous\nmarine exploration.", "published": "2025-09-08 13:36:27", "link": "http://arxiv.org/abs/2509.06678v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery", "abstract": "High-throughput interpretation of robotically gathered seafloor visual\nimagery can increase the efficiency of marine monitoring and exploration.\nAlthough recent research has suggested that location metadata can enhance\nself-supervised feature learning (SSL), its benefits across different SSL\nstrategies, models and seafloor image datasets are underexplored. This study\nevaluates the impact of location-based regularisation on six state-of-the-art\nSSL frameworks, which include Convolutional Neural Network (CNN) and Vision\nTransformer (ViT) models with varying latent-space dimensionality. Evaluation\nacross three diverse seafloor image datasets finds that location-regularisation\nconsistently improves downstream classification performance over standard SSL,\nwith average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for\nViTs, respectively. While CNNs pretrained on generic datasets benefit from\nhigh-dimensional latent representations, dataset-optimised SSL achieves similar\nperformance across the high (512) and low (128) dimensional latent\nrepresentations. Location-regularised SSL improves CNN performance over\npre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and\nlow-dimensional latent representations, respectively. For ViTs,\nhigh-dimensionality benefits both pre-trained and dataset-optimised SSL.\nAlthough location-regularisation improves SSL performance compared to standard\nSSL methods, pre-trained ViTs show strong generalisation, matching the\nbest-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$\nand $0.795 \\pm 0.077$, respectively. The findings highlight the value of\nlocation metadata for SSL regularisation, particularly when using\nlow-dimensional latent representations, and demonstrate strong generalisation\nof high-dimensional ViTs for seafloor image analysis.", "published": "2025-09-08 13:19:04", "link": "http://arxiv.org/abs/2509.06660v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis", "abstract": "Vision foundation models like DINOv2 demonstrate remarkable potential in\nmedical imaging despite their origin in natural image domains. However, their\ndesign inherently works best for uni-modal image analysis, limiting their\neffectiveness for multi-modal imaging tasks that are common in many medical\nfields, such as neurology and oncology. While supervised models perform well in\nthis setting, they fail to leverage unlabeled datasets and struggle with\nmissing modalities, a frequent challenge in clinical settings. To bridge these\ngaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the\npre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our\napproach incorporates multi-modal patch embeddings, enabling vision foundation\nmodels to effectively process multi-modal imaging data. To address missing\nmodalities, we employ full-modality masking, which encourages the model to\nlearn robust cross-modality relationships. Furthermore, we leverage\nsemi-supervised learning to harness large unlabeled datasets, enhancing both\nthe accuracy and reliability of medical predictions. Applied to glioma subtype\nclassification from multi-sequence brain MRI, our method achieves a Matthews\nCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassing\nstate-of-the-art supervised approaches by +11.1%. Our work establishes a\nscalable and robust solution for multi-modal medical imaging tasks, leveraging\npowerful vision foundation models pre-trained on natural images while\naddressing real-world clinical challenges such as missing data and limited\nannotations.", "published": "2025-09-08 12:34:15", "link": "http://arxiv.org/abs/2509.06617v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Towards In-Air Ultrasonic QR Codes: Deep Learning for Classification of Passive Reflector Constellations", "abstract": "In environments where visual sensors falter, in-air sonar provides a reliable\nalternative for autonomous systems. While previous research has successfully\nclassified individual acoustic landmarks, this paper takes a step towards\nincreasing information capacity by introducing reflector constellations as\nencoded tags. Our primary contribution is a multi-label Convolutional Neural\nNetwork (CNN) designed to simultaneously identify multiple, closely spaced\nreflectors from a single in-air 3D sonar measurement. Our initial findings on a\nsmall dataset confirm the feasibility of this approach, validating the ability\nto decode these complex acoustic patterns. Secondly, we investigated using\nadaptive beamforming with null-steering to isolate individual reflectors for\nsingle-label classification. Finally, we discuss the experimental results and\nlimitations, offering key insights and future directions for developing\nacoustic landmark systems with significantly increased information entropy and\ntheir accurate and robust detection and classification.", "published": "2025-09-08 12:33:19", "link": "http://arxiv.org/abs/2509.06615v1", "categories": ["eess.SP", "cs.CV"], "primary_category": "eess.SP"}
{"title": "From Skin to Skeleton: Towards Biomechanically Accurate 3D Digital Humans", "abstract": "Great progress has been made in estimating 3D human pose and shape from\nimages and video by training neural networks to directly regress the parameters\nof parametric human models like SMPL. However, existing body models have\nsimplified kinematic structures that do not correspond to the true joint\nlocations and articulations in the human skeletal system, limiting their\npotential use in biomechanics. On the other hand, methods for estimating\nbiomechanically accurate skeletal motion typically rely on complex motion\ncapture systems and expensive optimization methods. What is needed is a\nparametric 3D human model with a biomechanically accurate skeletal structure\nthat can be easily posed. To that end, we develop SKEL, which re-rigs the SMPL\nbody model with a biomechanics skeleton. To enable this, we need training data\nof skeletons inside SMPL meshes in diverse poses.\n  We build such a dataset by optimizing biomechanically accurate skeletons\ninside SMPL meshes from AMASS sequences. We then learn a regressor from SMPL\nmesh vertices to the optimized joint locations and bone rotations. Finally, we\nre-parametrize the SMPL mesh with the new kinematic parameters. The resulting\nSKEL model is animatable like SMPL but with fewer, and\nbiomechanically-realistic, degrees of freedom. We show that SKEL has more\nbiomechanically accurate joint locations than SMPL, and the bones fit inside\nthe body surface better than previous methods. By fitting SKEL to SMPL meshes\nwe are able to \"upgrade\" existing human pose and shape datasets to include\nbiomechanical parameters. SKEL provides a new tool to enable biomechanics in\nthe wild, while also providing vision and graphics researchers with a better\nconstrained and more realistic model of human articulation. The model, code,\nand data are available for research at https://skel.is.tue.mpg.de..", "published": "2025-09-08 12:24:27", "link": "http://arxiv.org/abs/2509.06607v1", "categories": ["cs.GR", "cs.CV"], "primary_category": "cs.GR"}
{"title": "Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method", "abstract": "Magnetic resonance imaging (MRI) is an invaluable tool for clinical and\nresearch applications. Yet, variations in scanners and acquisition parameters\ncause inconsistencies in image contrast, hindering data comparability and\nreproducibility across datasets and clinical studies. Existing scanner\nharmonization methods, designed to address this challenge, face limitations,\nsuch as requiring traveling subjects or struggling to generalize to unseen\ndomains. We propose a novel approach using a conditioned diffusion autoencoder\nwith a contrastive loss and domain-agnostic contrast augmentation to harmonize\nMR images across scanners while preserving subject-specific anatomy. Our method\nenables brain MRI synthesis from a single reference image. It outperforms\nbaseline techniques, achieving a +7% PSNR improvement on a traveling subjects\ndataset and +18% improvement on age regression in unseen. Our model provides\nrobust, effective harmonization of brain MRIs to target scanners without\nrequiring fine-tuning. This advancement promises to enhance comparability,\nreproducibility, and generalizability in multi-site and longitudinal clinical\nstudies, ultimately contributing to improved healthcare outcomes.", "published": "2025-09-08 12:03:34", "link": "http://arxiv.org/abs/2509.06592v1", "categories": ["eess.IV", "cs.CV"], "primary_category": "eess.IV"}
{"title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising", "abstract": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.", "published": "2025-09-08 12:02:38", "link": "http://arxiv.org/abs/2509.06591v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Detection of trade in products derived from threatened species using machine learning and a smartphone", "abstract": "Unsustainable trade in wildlife is a major threat to biodiversity and is now\nincreasingly prevalent in digital marketplaces and social media. With the sheer\nvolume of digital content, the need for automated methods to detect wildlife\ntrade listings is growing. These methods are especially needed for the\nautomatic identification of wildlife products, such as ivory. We developed\nmachine learning-based object recognition models that can identify wildlife\nproducts within images and highlight them. The data consists of images of\nelephant, pangolin, and tiger products that were identified as being sold\nillegally or that were confiscated by authorities. Specifically, the wildlife\nproducts included elephant ivory and skins, pangolin scales, and claws (raw and\ncrafted), and tiger skins and bones. We investigated various combinations of\ntraining strategies and two loss functions to identify the best model to use in\nthe automatic detection of these wildlife products. Models were trained for\neach species while also developing a single model to identify products from all\nthree species. The best model showed an overall accuracy of 84.2% with\naccuracies of 71.1%, 90.2% and 93.5% in detecting products derived from\nelephants, pangolins, and tigers, respectively. We further demonstrate that the\nmachine learning model can be made easily available to stakeholders, such as\ngovernment authorities and law enforcement agencies, by developing a\nsmartphone-based application that had an overall accuracy of 91.3%. The\napplication can be used in real time to click images and help identify\npotentially prohibited products of target species. Thus, the proposed method is\nnot only applicable for monitoring trade on the web but can also be used e.g.\nin physical markets for monitoring wildlife trade.", "published": "2025-09-08 11:56:26", "link": "http://arxiv.org/abs/2509.06585v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis", "abstract": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.", "published": "2025-09-08 11:49:51", "link": "http://arxiv.org/abs/2509.06579v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Approximating Condorcet Ordering for Vector-valued Mathematical Morphology", "abstract": "Mathematical morphology provides a nonlinear framework for image and spatial\ndata processing and analysis. Although there have been many successful\napplications of mathematical morphology to vector-valued images, such as color\nand hyperspectral images, there is still no consensus on the most suitable\nvector ordering for constructing morphological operators. This paper addresses\nthis issue by examining a reduced ordering approximating the Condorcet ranking\nderived from a set of vector orderings. Inspired by voting problems, the\nCondorcet ordering ranks elements from most to least voted, with voters\nrepresenting different orderings. In this paper, we develop a machine learning\napproach that learns a reduced ordering that approximates the Condorcet\nordering. Preliminary computational experiments confirm the effectiveness of\nlearning the reduced mapping to define vector-valued morphological operators\nfor color images.", "published": "2025-09-08 11:47:11", "link": "http://arxiv.org/abs/2509.06577v1", "categories": ["cs.CV", "cs.LG", "cs.NE"], "primary_category": "cs.CV"}
{"title": "Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition", "abstract": "Existing open set recognition (OSR) methods are typically designed for static\nscenarios, where models aim to classify known classes and identify unknown ones\nwithin fixed scopes. This deviates from the expectation that the model should\nincrementally identify newly emerging unknown classes from continuous data\nstreams and acquire corresponding knowledge. In such evolving scenarios, the\ndiscriminability of OSR decision boundaries is hard to maintain due to\nrestricted access to former training data, causing severe inter-class\nconfusion. To solve this problem, we propose retentive angular representation\nlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknown\nrepresentations are encouraged to align around inactive prototypes within an\nangular space constructed under the equiangular tight frame, thereby mitigating\nexcessive representation drift during knowledge updates. Specifically, we adopt\na virtual-intrinsic interactive (VII) training strategy, which compacts known\nrepresentations by enforcing clear inter-class margins through\nboundary-proximal virtual classes. Furthermore, a stratified rectification\nstrategy is designed to refine decision boundaries, mitigating representation\nbias and feature space distortion caused by imbalances between old/new and\npositive/negative class samples. We conduct thorough evaluations on CIFAR100\nand TinyImageNet datasets and establish a new benchmark for IOSR. Experimental\nresults across various task setups demonstrate that the proposed method\nachieves state-of-the-art performance.", "published": "2025-09-08 11:35:12", "link": "http://arxiv.org/abs/2509.06570v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval", "abstract": "The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural\nimages matching the overall semantics and spatial layout of a free-hand sketch.\nUnlike prior work focused on architectural augmentations of retrieval models,\nwe emphasize the inherent ambiguity and noise present in real-world sketches.\nThis insight motivates a training objective that is explicitly designed to be\nrobust to sketch variability. We show that with an appropriate combination of\npre-training, encoder architecture, and loss formulation, it is possible to\nachieve state-of-the-art performance without the introduction of additional\ncomplexity. Extensive experiments on a challenging FS-COCO and widely-used\nSketchyCOCO datasets confirm the effectiveness of our approach and underline\nthe critical role of training design in cross-modal retrieval tasks, as well as\nthe need to improve the evaluation scenarios of scene-level SBIR.", "published": "2025-09-08 11:26:40", "link": "http://arxiv.org/abs/2509.06566v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in Panoramic Radiographs using Federated, Centralized and Local Learning", "abstract": "Objectives: Federated learning (FL) may mitigate privacy constraints,\nheterogeneous data quality, and inconsistent labeling in dental diagnostic AI.\nWe compared FL with centralized (CL) and local learning (LL) for tooth\nsegmentation in panoramic radiographs across multiple data corruption\nscenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six\ninstitutions across four settings: baseline (unaltered data); label\nmanipulation (dilated/missing annotations); image-quality manipulation\n(additive Gaussian noise); and exclusion of a faulty client with corrupted\ndata. FL was implemented via the Flower AI framework. Per-client training- and\nvalidation-loss trajectories were monitored for anomaly detection and a set of\nmetrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.\nFrom these metrics significance results were reported through Wilcoxon\nsigned-rank test. CL and LL served as comparators. Results: Baseline: FL\nachieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at\n0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).\nLabel manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:\n1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:\n1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL\nscored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:\n1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:\n1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring\nreliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and\noutperforms LL across corruption scenarios while preserving privacy. Per-client\nloss trajectories provide an effective anomaly-detection mechanism and support\nFL as a practical, privacy-preserving approach for scalable clinical AI\ndeployment.", "published": "2025-09-08 11:07:47", "link": "http://arxiv.org/abs/2509.06553v1", "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "eess.IV"}
{"title": "Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing", "abstract": "The on-device real-time data distribution shift on devices challenges the\ngeneralization of lightweight on-device models. This critical issue is often\noverlooked in current research, which predominantly relies on data-intensive\nand computationally expensive fine-tuning approaches. To tackle this, we\nintroduce Persona, a novel personalized method using a prototype-based,\nbackpropagation-free parameter editing framework to enhance model\ngeneralization without post-deployment retraining. Persona employs a neural\nadapter in the cloud to generate a parameter editing matrix based on real-time\ndevice data. This matrix adeptly adapts on-device models to the prevailing data\ndistributions, efficiently clustering them into prototype models. The\nprototypes are dynamically refined via the parameter editing matrix,\nfacilitating efficient evolution. Furthermore, the integration of cross-layer\nknowledge transfer ensures consistent and context-aware multi-layer parameter\nchanges and prototype assignment. Extensive experiments on vision task and\nrecommendation task on multiple datasets confirm Persona's effectiveness and\ngenerality.", "published": "2025-09-08 11:06:50", "link": "http://arxiv.org/abs/2509.06552v1", "categories": ["cs.LG", "cs.CV", "cs.DC", "cs.IR"], "primary_category": "cs.LG"}
{"title": "Benchmarking EfficientTAM on FMO datasets", "abstract": "Fast and tiny object tracking remains a challenge in computer vision and in\nthis paper we first introduce a JSON metadata file associated with four open\nsource datasets of Fast Moving Objects (FMOs) image sequences. In addition, we\nextend the description of the FMOs datasets with additional ground truth\ninformation in JSON format (called FMOX) with object size information. Finally\nwe use our FMOX file to test a recently proposed foundational model for\ntracking (called EfficientTAM) showing that its performance compares well with\nthe pipelines originally taylored for these FMO datasets. Our comparison of\nthese state-of-the-art techniques on FMOX is provided with Trajectory\nIntersection of Union (TIoU) scores. The code and JSON is shared open source\nallowing FMOX to be accessible and usable for other machine learning pipelines\naiming to process FMO datasets.", "published": "2025-09-08 10:41:26", "link": "http://arxiv.org/abs/2509.06536v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach", "abstract": "Accurate evaluation of the response of glioblastoma to therapy is crucial for\nclinical decision-making and patient management. The Response Assessment in\nNeuro-Oncology (RANO) criteria provide a standardized framework to assess\npatients' clinical response, but their application can be complex and subject\nto observer variability. This paper presents an automated method for\nclassifying the intervention response from longitudinal MRI scans, developed to\npredict tumor response during therapy as part of the BraTS 2025 challenge. We\npropose a novel hybrid framework that combines deep learning derived feature\nextraction and an extensive set of radiomics and clinically chosen features.\nOur approach utilizes a fine-tuned ResNet-18 model to extract features from 2D\nregions of interest across four MRI modalities. These deep features are then\nfused with a rich set of more than 4800 radiomic and clinically driven\nfeatures, including 3D radiomics of tumor growth and shrinkage masks,\nvolumetric changes relative to the nadir, and tumor centroid shift. Using the\nfused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a\nMacro F1 score of 0.50 in the 4-class response prediction task (Complete\nResponse, Partial Response, Stable Disease, Progressive Disease). Our results\nhighlight that synergizing learned image representations with domain-targeted\nradiomic features provides a robust and effective solution for automated\ntreatment response assessment in neuro-oncology.", "published": "2025-09-08 10:15:23", "link": "http://arxiv.org/abs/2509.06511v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement", "abstract": "Subject-driven image generation (SDIG) aims to manipulate specific subjects\nwithin images while adhering to textual instructions, a task crucial for\nadvancing text-to-image diffusion models. SDIG requires reconciling the tension\nbetween maintaining subject identity and complying with dynamic edit\ninstructions, a challenge inadequately addressed by existing methods. In this\npaper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,\nwhich resolves this tension through target supervision and preference learning\nwithout test-time fine-tuning. TIDE pioneers target-supervised triplet\nalignment, modelling subject adaptation dynamics using a (reference image,\ninstruction, target images) triplet. This approach leverages the Direct Subject\nDiffusion (DSD) objective, training the model with paired \"winning\" (balanced\npreservation-compliance) and \"losing\" (distorted) targets, systematically\ngenerated and evaluated via quantitative metrics. This enables implicit reward\nmodelling for optimal preservation-compliance balance. Experimental results on\nstandard benchmarks demonstrate TIDE's superior performance in generating\nsubject-faithful outputs while maintaining instruction compliance,\noutperforming baseline methods across multiple quantitative metrics. TIDE's\nversatility is further evidenced by its successful application to diverse\ntasks, including structural-conditioned generation, image-to-image generation,\nand text-image interpolation. Our code is available at\nhttps://github.com/KomJay520/TIDE.", "published": "2025-09-08 10:06:37", "link": "http://arxiv.org/abs/2509.06499v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting", "abstract": "In industrial quality control, to visually recognize unwanted items within a\nmoving heterogeneous stream, human operators are often still indispensable.\nWaste-sorting stands as a significant example, where operators on multiple\nconveyor belts manually remove unwanted objects to select specific materials.\nTo automate this recognition problem, computer vision systems offer great\npotential in accurately identifying and segmenting unwanted items in such\nsettings. Unfortunately, considering the multitude and the variety of sorting\ntasks, fully supervised approaches are not a viable option to address this\nchallange, as they require extensive labeling efforts. Surprisingly, weakly\nsupervised alternatives that leverage the implicit supervision naturally\nprovided by the operator in his removal action are relatively unexplored. In\nthis paper, we define the concept of Before-After Supervision, illustrating how\nto train a segmentation network by leveraging only the visual differences\nbetween images acquired \\textit{before} and \\textit{after} the operator. To\npromote research in this direction, we introduce WS$^2$ (Weakly Supervised\nsegmentation for Waste-Sorting), the first multiview dataset consisting of more\nthan 11 000 high-resolution video frames captured on top of a conveyor belt,\nincluding \"before\" and \"after\" images. We also present a robust end-to-end\npipeline, used to benchmark several state-of-the-art weakly supervised\nsegmentation methods on WS$^2$.", "published": "2025-09-08 09:48:37", "link": "http://arxiv.org/abs/2509.06485v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection", "abstract": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.", "published": "2025-09-08 09:46:33", "link": "http://arxiv.org/abs/2509.06482v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Does DINOv3 Set a New Medical Vision Standard?", "abstract": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.", "published": "2025-09-08 09:28:57", "link": "http://arxiv.org/abs/2509.06467v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Statistical 3D Stomach Shape Model for Anatomical Analysis", "abstract": "Realistic and parameterized 3D models of human anatomy have become invaluable\nin research, diagnostics, and surgical planning. However, the development of\ndetailed models for internal organs, such as the stomach, has been limited by\ndata availability and methodological challenges. In this paper, we propose a\nnovel pipeline for the generation of synthetic 3D stomach models, enabling the\ncreation of anatomically diverse morphologies informed by established studies\non stomach shape variability. Using this pipeline, we construct a dataset of\nsynthetic stomachs. Building on this dataset, we develop a 3D statistical shape\nmodel of the stomach, trained to capture natural anatomical variability in a\nlow-dimensional shape space. The model is further refined using CT meshes\nderived from publicly available datasets through a semi-supervised alignment\nprocess, enhancing its ability to generalize to unseen anatomical variations.\nWe evaluated the model on a held-out test set of real stomach CT scans,\ndemonstrating robust generalization and fit accuracy. We make the statistical\nshape model along with the synthetic dataset publicly available on GitLab:\nhttps://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.\nThis work introduces the first statistical 3D shape model of the stomach, with\napplications ranging from surgical simulation and pre-operative planning to\nmedical education and computational modeling. By combining synthetic data\ngeneration, parametric modeling, and real-world validation, our approach\nrepresents a significant advancement in organ modeling and opens new\npossibilities for personalized healthcare solutions.", "published": "2025-09-08 09:23:11", "link": "http://arxiv.org/abs/2509.06464v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks", "abstract": "Deep neural networks currently dominate many fields of the artificial\nintelligence landscape, achieving state-of-the-art results on numerous tasks\nwhile remaining hard to understand and exhibiting surprising weaknesses. An\nactive area of research focuses on adversarial attacks, which aim to generate\ninputs that uncover these weaknesses. However, this proves challenging,\nespecially in the black-box scenario where model details are inaccessible. This\npaper explores in detail the impact of such adversarial algorithms on\nResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network\narchitectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101\ndatasets, we benchmark two novel black-box iterative adversarial algorithms\nbased on affine transformations and genetic algorithms: 1) Affine\nTransformation Attack (ATA), an iterative algorithm maximizing our attack score\nfunction using random affine transformations, and 2) Affine Genetic Attack\n(AGA), a genetic algorithm that involves random noise and affine\ntransformations. We evaluate the performance of the models in the algorithm\nparameter variation, data augmentation, and global and targeted attack\nconfigurations. We also compare our algorithms with two black-box adversarial\nalgorithms, Pixle and Square Attack. Our experiments yield better results on\nthe image classification task than similar methods in the literature, achieving\nan accuracy improvement of up to 8.82%. We provide noteworthy insights into\nsuccessful adversarial defenses and attacks at both global and targeted levels,\nand demonstrate adversarial robustness through algorithm parameter variation.", "published": "2025-09-08 09:12:27", "link": "http://arxiv.org/abs/2509.06459v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark", "abstract": "Cross-source point cloud registration, which aims to align point cloud data\nfrom different sensors, is a fundamental task in 3D vision. However, compared\nto the same-source point cloud registration, cross-source registration faces\ntwo core challenges: the lack of publicly available large-scale real-world\ndatasets for training the deep registration models, and the inherent\ndifferences in point clouds captured by multiple sensors. The diverse patterns\ninduced by the sensors pose great challenges in robust and accurate point cloud\nfeature extraction and matching, which negatively influence the registration\naccuracy. To advance research in this field, we construct Cross3DReg, the\ncurrently largest and real-world multi-modal cross-source point cloud\nregistration dataset, which is collected by a rotating mechanical lidar and a\nhybrid semi-solid-state lidar, respectively. Moreover, we design an\noverlap-based cross-source registration framework, which utilizes unaligned\nimages to predict the overlapping region between source and target point\nclouds, effectively filtering out redundant points in the irrelevant regions\nand significantly mitigating the interference caused by noise in\nnon-overlapping areas. Then, a visual-geometric attention guided matching\nmodule is proposed to enhance the consistency of cross-source point cloud\nfeatures by fusing image and geometric information to establish reliable\ncorrespondences and ultimately achieve accurate and robust registration.\nExtensive experiments show that our method achieves state-of-the-art\nregistration performance. Our framework reduces the relative rotation error\n(RRE) and relative translation error (RTE) by $63.2\\%$ and $40.2\\%$,\nrespectively, and improves the registration recall (RR) by $5.4\\%$, which\nvalidates its effectiveness in achieving accurate cross-source registration.", "published": "2025-09-08 09:01:13", "link": "http://arxiv.org/abs/2509.06456v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment", "abstract": "Many super-resolution (SR) algorithms have been proposed to increase image\nresolution. However, full-reference (FR) image quality assessment (IQA) metrics\nfor comparing and evaluating different SR algorithms are limited. In this work,\nwe propose the Perception-oriented Bidirectional Attention Network (PBAN) for\nimage SR FR-IQA, which is composed of three modules: an image encoder module, a\nperception-oriented bidirectional attention (PBA) module, and a quality\nprediction module. First, we encode the input images for feature\nrepresentations. Inspired by the characteristics of the human visual system, we\nthen construct the perception-oriented PBA module. Specifically, different from\nexisting attention-based SR IQA methods, we conceive a Bidirectional Attention\nto bidirectionally construct visual attention to distortion, which is\nconsistent with the generation and evaluation processes of SR images. To\nfurther guide the quality assessment towards the perception of distorted\ninformation, we propose Grouped Multi-scale Deformable Convolution, enabling\nthe proposed method to adaptively perceive distortion. Moreover, we design\nSub-information Excitation Convolution to direct visual perception to both\nsub-pixel and sub-channel attention. Finally, the quality prediction module is\nexploited to integrate quality-aware features and regress quality scores.\nExtensive experiments demonstrate that our proposed PBAN outperforms\nstate-of-the-art quality assessment methods.", "published": "2025-09-08 08:39:45", "link": "http://arxiv.org/abs/2509.06442v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection", "abstract": "Muzzle patterns are among the most effective biometric traits for cattle\nidentification. Fast and accurate detection of the muzzle region as the region\nof interest is critical to automatic visual cattle identification.. Earlier\napproaches relied on manual detection, which is labor-intensive and\ninconsistent. Recently, automated methods using supervised models like YOLO\nhave become popular for muzzle detection. Although effective, these methods\nrequire extensive annotated datasets and tend to be trained data-dependent,\nlimiting their performance on new or unseen cattle. To address these\nlimitations, this study proposes a zero-shot muzzle detection framework based\non Grounding DINO, a vision-language model capable of detecting muzzles without\nany task-specific training or annotated data. This approach leverages natural\nlanguage prompts to guide detection, enabling scalable and flexible muzzle\nlocalization across diverse breeds and environments. Our model achieves a mean\nAverage Precision (mAP)@0.5 of 76.8\\%, demonstrating promising performance\nwithout requiring annotated data. To our knowledge, this is the first research\nto provide a real-world, industry-oriented, and annotation-free solution for\ncattle muzzle detection. The framework offers a practical alternative to\nsupervised methods, promising improved adaptability and ease of deployment in\nlivestock monitoring applications.", "published": "2025-09-08 08:21:34", "link": "http://arxiv.org/abs/2509.06427v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM", "abstract": "Video camouflaged object detection (VCOD) is challenging due to dynamic\nenvironments. Existing methods face two main issues: (1) SAM-based methods\nstruggle to separate camouflaged object edges due to model freezing, and (2)\nMLLM-based methods suffer from poor object separability as large language\nmodels merge foreground and background. To address these issues, we propose a\nnovel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the\nseparability of object edge details, we represent video sequences with temporal\nand spatial clues and perform feature fusion via LLM to increase information\ndensity. Next, multiple cues are generated through the dynamic foreground\nvisual token scoring module and the prompt network to adaptively guide and\nfine-tune the SAM model, enabling it to adapt to subtle textures. To enhance\nthe separability of objects and background, we propose a decoupled\nforeground-background learning strategy. By generating foreground and\nbackground cues separately and performing decoupled training, the visual token\ncan effectively integrate foreground and background information independently,\nenabling SAM to more accurately segment camouflaged objects in the video.\nExperiments on the MoCA-Mask dataset show that Phantom-Insight achieves\nstate-of-the-art performance across various metrics. Additionally, its ability\nto detect unseen camouflaged objects on the CAD2016 dataset highlights its\nstrong generalization ability.", "published": "2025-09-08 08:17:47", "link": "http://arxiv.org/abs/2509.06422v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results", "abstract": "This paper presents the ISRGC-Q Challenge, built upon the Image\nSuper-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and\norganized as part of the Visual Quality Assessment (VQualA) Competition at the\nICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment\n(SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated\nby the latest generative approaches, including Generative Adversarial Networks\n(GANs) and diffusion models. The primary goal of this challenge is to analyze\nthe unique artifacts introduced by modern super-resolution techniques and to\nevaluate their perceptual quality effectively. A total of 108 participants\nregistered for the challenge, with 4 teams submitting valid solutions and fact\nsheets for the final testing phase. These submissions demonstrated\nstate-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is\npublicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.", "published": "2025-09-08 08:07:50", "link": "http://arxiv.org/abs/2509.06413v1", "categories": ["cs.CV", "eess.IV"], "primary_category": "cs.CV"}
{"title": "3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom", "abstract": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene\nreconstruction. With a number of views of a given object or scene, the\nalgorithm trains a model composed of 3D gaussians, which enables the production\nof novel views from arbitrary points of view. This freedom of movement is\nreferred to as 6DoF for 6 degrees of freedom: a view is produced for any\nposition (3 degrees), orientation of camera (3 other degrees). On large scenes,\nthough, the input views are acquired from a limited zone in space, and the\nreconstruction is valuable for novel views from the same zone, even if the\nscene itself is almost unlimited in size. We refer to this particular case as\n3DoF+, meaning that the 3 degrees of freedom of camera position are limited to\nsmall offsets around the central position. Considering the problem of\ncoordinate quantization, the impact of position error on the projection error\nin pixels is studied. It is shown that the projection error is proportional to\nthe squared inverse distance of the point being projected. Consequently, a new\nquantization scheme based on spherical coordinates is proposed. Rate-distortion\nperformance of the proposed method are illustrated on the well-known Garden\nscene.", "published": "2025-09-08 07:44:28", "link": "http://arxiv.org/abs/2509.06400v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery", "abstract": "Brain Metastases (BM) are a large contributor to mortality of patients with\ncancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored\nwith Magnetic Resonance Imaging (MRI) at regular follow-up intervals according\nto treatment guidelines. Analyzing and quantifying this longitudinal imaging\nrepresents an intractable workload for clinicians. As a result, follow-up\nimages are not annotated and merely assessed by observation. Response to\ntreatment in longitudinal imaging is being studied, to better understand growth\ntrajectories and ultimately predict treatment success or toxicity as early as\npossible. In this study, we implement an automated pipeline to curate a large\nlongitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in\n177 patients who were monitored for >360 days at approximately two-month\nintervals at Lausanne University Hospital (CHUV). We use a data-driven\nclustering to identify characteristic trajectories. In addition, we predict 12\nmonths lesion-level response using classical as well as graph machine learning\nGraph Machine Learning (GML). Clustering revealed 5 dominant growth\ntrajectories with distinct final response categories. Response prediction\nreaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first\nfollow-up MRI with gradient boosting. Similarly, robust predictive performance\nof up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more\nflexibility with a single model for multiple input time-points configurations.\nOur results suggest potential automation and increased precision for the\ncomprehensive assessment and prediction of BM response to SRS in longitudinal\nMRI. The proposed pipeline facilitates scalable data curation for the\ninvestigation of BM growth patterns, and lays the foundation for clinical\ndecision support systems aiming at optimizing personalized care.", "published": "2025-09-08 07:29:45", "link": "http://arxiv.org/abs/2509.06396v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Your Super Resolution Model is not Enough for Tackling Real-World Scenarios", "abstract": "Despite remarkable progress in Single Image Super-Resolution (SISR),\ntraditional models often struggle to generalize across varying scale factors,\nlimiting their real-world applicability. To address this, we propose a plug-in\nScale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR\nmodels with the ability to perform arbitrary-scale SR. SAAM employs\nlightweight, scale-adaptive feature extraction and upsampling, incorporating\nthe Simple parameter-free Attention Module (SimAM) for efficient guidance and\ngradient variance loss to enhance sharpness in image details. Our method\nintegrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet,\nHiT-SR, OverNet), delivering competitive or superior performance across a wide\nrange of integer and non-integer scale factors. Extensive experiments on\nbenchmark datasets demonstrate that our approach enables robust multi-scale\nupscaling with minimal computational overhead, offering a practical solution\nfor real-world scenarios.", "published": "2025-09-08 07:13:58", "link": "http://arxiv.org/abs/2509.06387v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study", "abstract": "Colorectal diseases, including inflammatory conditions and neoplasms, require\nquick, accurate care to be effectively treated. Traditional diagnostic\npipelines require extensive preparation and rely on separate, individual\nevaluations on histological images and colonoscopy footage, introducing\npossible variability and inefficiencies. This pilot study proposes a unified\ndeep learning network that uses convolutional neural networks (CN N s) to\nclassify both histopathological slides and colonoscopy video frames in one\npipeline. The pipeline integrates class-balancing learning, robust\naugmentation, and calibration methods to ensure accurate results. Static colon\nhistology images were taken from the PathMNIST dataset, and the lower\ngastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.\nThe CNN architecture used was ResNet-50. This study demonstrates an\ninterpretable and reproducible diagnostic pipeline that unifies multiple\ndiagnostic modalities to advance and ease the detection of colorectal diseases.", "published": "2025-09-08 05:54:03", "link": "http://arxiv.org/abs/2509.06351v1", "categories": ["cs.CV", "cs.LG"], "primary_category": "cs.CV"}
{"title": "Harnessing Object Grounding for Time-Sensitive Video Understanding", "abstract": "We propose to improve the time-sensitive video understanding (TSV) capability\nof video large language models (Video-LLMs) with grounded objects (GO). We\nhypothesize that TSV tasks can benefit from GO within frames, which is\nsupported by our preliminary experiments on LITA, a state-of-the-art Video-LLM\nfor reasoning temporal localization. While augmenting prompts with textual\ndescription of these object annotations improves the performance of LITA, it\nalso introduces extra token length and susceptibility to the noise in object\nlevel information. To address this, we propose GO-Tokenizer, a lightweight\nadd-on module for Video-LLMs leveraging off-the-shelf object detectors to\nencode compact object information on the fly. Experimental results demonstrate\nthat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its\ncounterpart utilizing textual description of objects in the prompt. The gain\ngeneralizes across different models, datasets and video understanding tasks\nsuch as reasoning temporal localization and dense captioning.", "published": "2025-09-08 04:52:00", "link": "http://arxiv.org/abs/2509.06335v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Multi-Modal Camera-Based Detection of Vulnerable Road Users", "abstract": "Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists\nrepresent more than half of global traffic deaths, yet their detection remains\nchallenging in poor lighting, adverse weather, and unbalanced data sets. This\npaper presents a multimodal detection framework that integrates RGB and thermal\ninfrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,\nBDD100K, and Teledyne FLIR datasets, with class re-weighting and light\naugmentations to improve minority-class performance and robustness, experiments\nshow that 640-pixel resolution and partial backbone freezing optimise accuracy\nand efficiency, while class-weighted losses enhance recall for rare VRUs.\nResults highlight that thermal models achieve the highest precision, and\nRGB-to-thermal augmentation boosts recall, demonstrating the potential of\nmultimodal detection to improve VRU safety at intersections.", "published": "2025-09-08 04:39:07", "link": "http://arxiv.org/abs/2509.06333v1", "categories": ["cs.CV", "cs.RO"], "primary_category": "cs.CV"}
{"title": "Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users", "abstract": "Currency recognition systems often overlook usability and authenticity\nassessment, especially in low-resource environments where visually impaired\nusers and offline validation are common. While existing methods focus on\ndenomination classification, they typically ignore physical degradation and\nforgery, limiting their applicability in real-world conditions. This paper\npresents a unified framework for currency evaluation that integrates three\nmodules: denomination classification using lightweight CNN models, damage\nquantification through a novel Unified Currency Damage Index (UCDI), and\ncounterfeit detection using feature-based template matching. The dataset\nconsists of over 82,000 annotated images spanning clean, damaged, and\ncounterfeit notes. Our Custom_CNN model achieves high classification\nperformance with low parameter count. The UCDI metric provides a continuous\nusability score based on binary mask loss, chromatic distortion, and structural\nfeature loss. The counterfeit detection module demonstrates reliable\nidentification of forged notes across varied imaging conditions. The framework\nsupports real-time, on-device inference and addresses key deployment challenges\nin constrained environments. Results show that accurate, interpretable, and\ncompact solutions can support inclusive currency evaluation in practical\nsettings.", "published": "2025-09-08 04:24:31", "link": "http://arxiv.org/abs/2509.06331v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap", "abstract": "The precise characterization of plant morphology provides valuable insights\ninto plant environment interactions and genetic evolution. A key technology for\nextracting this information is 3D segmentation, which delineates individual\nplant organs from complex point clouds. Despite significant progress in general\n3D computer vision domains, the adoption of 3D segmentation for plant\nphenotyping remains limited by three major challenges: i) the scarcity of\nlarge-scale annotated datasets, ii) technical difficulties in adapting advanced\ndeep neural networks to plant point clouds, and iii) the lack of standardized\nbenchmarks and evaluation protocols tailored to plant science. This review\nsystematically addresses these barriers by: i) providing an overview of\nexisting 3D plant datasets in the context of general 3D segmentation domains,\nii) systematically summarizing deep learning-based methods for point cloud\nsemantic and instance segmentation, iii) introducing Plant Segmentation Studio\n(PSS), an open-source framework for reproducible benchmarking, and iv)\nconducting extensive quantitative experiments to evaluate representative\nnetworks and sim-to-real learning strategies. Our findings highlight the\nefficacy of sparse convolutional backbones and transformer-based instance\nsegmentation, while also emphasizing the complementary role of modeling-based\nand augmentation-based synthetic data generation for sim-to-real learning in\nreducing annotation demands. In general, this study bridges the gap between\nalgorithmic advances and practical deployment, providing immediate tools for\nresearchers and a roadmap for developing data-efficient and generalizable deep\nlearning solutions in 3D plant phenotyping. Data and code are available at\nhttps://github.com/perrydoremi/PlantSegStudio.", "published": "2025-09-08 04:21:27", "link": "http://arxiv.org/abs/2509.06329v1", "categories": ["cs.CV", "q-bio.QM"], "primary_category": "cs.CV"}
{"title": "Text4Seg++: Advancing Image Segmentation via Generative Language Modeling", "abstract": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities\nin vision-language tasks. However, effectively integrating image segmentation\ninto these models remains a significant challenge. In this work, we propose a\nnovel text-as-mask paradigm that casts image segmentation as a text generation\nproblem, eliminating the need for additional decoders and significantly\nsimplifying the segmentation process. Our key innovation is semantic\ndescriptors, a new textual representation of segmentation masks where each\nimage patch is mapped to its corresponding text label. We first introduce\nimage-wise semantic descriptors, a patch-aligned textual representation of\nsegmentation masks that integrates naturally into the language modeling\npipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding\n(R-RLE), which compresses redundant text sequences, reducing the length of\nsemantic descriptors by 74% and accelerating inference by $3\\times$, without\ncompromising performance. Building upon this, our initial framework Text4Seg\nachieves strong segmentation performance across a wide range of vision tasks.\nTo further improve granularity and compactness, we propose box-wise semantic\ndescriptors, which localizes regions of interest using bounding boxes and\nrepresents region masks via structured mask tokens called semantic bricks. This\nleads to our refined model, Text4Seg++, which formulates segmentation as a\nnext-brick prediction task, combining precision, scalability, and generative\nefficiency. Comprehensive experiments on natural and remote sensing datasets\nshow that Text4Seg++ consistently outperforms state-of-the-art models across\ndiverse benchmarks without any task-specific fine-tuning, while remaining\ncompatible with existing MLLM backbones. Our work highlights the effectiveness,\nscalability, and generalizability of text-driven image segmentation within the\nMLLM framework.", "published": "2025-09-08 04:07:14", "link": "http://arxiv.org/abs/2509.06321v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix", "abstract": "A central challenge in representation learning is constructing latent\nembeddings that are both expressive and efficient. In practice, deep networks\noften produce redundant latent spaces where multiple coordinates encode\noverlapping information, reducing effective capacity and hindering\ngeneralization. Standard metrics such as accuracy or reconstruction loss\nprovide only indirect evidence of such redundancy and cannot isolate it as a\nfailure mode. We introduce a redundancy index, denoted rho(C), that directly\nquantifies inter-dimensional dependencies by analyzing coupling matrices\nderived from latent representations and comparing their off-diagonal statistics\nagainst a normal distribution via energy distance. The result is a compact,\ninterpretable, and statistically grounded measure of representational quality.\nWe validate rho(C) across discriminative and generative settings on MNIST\nvariants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiple\narchitectures and hyperparameter optimization strategies. Empirically, low\nrho(C) reliably predicts high classification accuracy or low reconstruction\nerror, while elevated redundancy is associated with performance collapse.\nEstimator reliability grows with latent dimension, yielding natural lower\nbounds for reliable analysis. We further show that Tree-structured Parzen\nEstimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)\ncan guide neural architecture search and serve as a redundancy-aware\nregularization target. By exposing redundancy as a universal bottleneck across\nmodels and tasks, rho(C) offers both a theoretical lens and a practical tool\nfor evaluating and improving the efficiency of learned representations.", "published": "2025-09-08 03:36:47", "link": "http://arxiv.org/abs/2509.06314v1", "categories": ["cs.LG", "cs.CV"], "primary_category": "cs.LG"}
{"title": "Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning", "abstract": "Generalized Category Discovery (GCD) is an emerging and challenging\nopen-world problem that has garnered increasing attention in recent years. Most\nexisting GCD methods focus on discovering categories in static images. However,\nrelying solely on static visual content is often insufficient to reliably\ndiscover novel categories. To bridge this gap, we extend the GCD problem to the\nvideo domain and introduce a new setting, termed Video-GCD. Thus, effectively\nintegrating multi-perspective information across time is crucial for accurate\nVideo-GCD. To tackle this challenge, we propose a novel Memory-guided\nConsistency-aware Contrastive Learning (MCCL) framework, which explicitly\ncaptures temporal-spatial cues and incorporates them into contrastive learning\nthrough a consistency-guided voting mechanism. MCCL consists of two core\ncomponents: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided\nRepresentation Enhancement (MGRE). CACL exploits multiperspective temporal\nfeatures to estimate consistency scores between unlabeled instances, which are\nthen used to weight the contrastive loss accordingly. MGRE introduces a\ndual-level memory buffer that maintains both feature-level and logit-level\nrepresentations, providing global context to enhance intra-class compactness\nand inter-class separability. This in turn refines the consistency estimation\nin CACL, forming a mutually reinforcing feedback loop between representation\nlearning and consistency modeling. To facilitate a comprehensive evaluation, we\nconstruct a new and challenging Video-GCD benchmark, which includes action\nrecognition and bird classification video datasets. Extensive experiments\ndemonstrate that our method significantly outperforms competitive GCD\napproaches adapted from image-based settings, highlighting the importance of\ntemporal information for discovering novel categories in videos. The code will\nbe publicly available.", "published": "2025-09-08 03:12:57", "link": "http://arxiv.org/abs/2509.06306v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding", "abstract": "Visual Grounding (VG) aims to utilize given natural language queries to\nlocate specific target objects within images. While current transformer-based\napproaches demonstrate strong localization performance in standard scene (i.e,\nscenarios without any novel objects), they exhibit notable limitations in\nopen-vocabulary scene (i.e, both familiar and novel object categories during\ntesting). These limitations primarily stem from three key factors: (1)\nimperfect alignment between visual and linguistic modalities, (2) insufficient\ncross-modal feature fusion, and (3) ineffective utilization of semantic\nprototype information. To overcome these challenges, we present Prototype-Aware\nMultimodal Learning (PAML), an innovative framework that systematically\naddresses these issues through several key components: First, we leverage ALBEF\nto establish robust cross-modal alignment during initial feature encoding.\nSubsequently, our Visual Discriminative Feature Encoder selectively enhances\nsalient object representations while suppressing irrelevant visual context. The\nframework then incorporates a novel prototype discovering and inheriting\nmechanism that extracts and aggregates multi-neighbor semantic prototypes to\nfacilitate open-vocabulary recognition. These enriched features undergo\ncomprehensive multimodal integration through our Multi-stage Decoder before\nfinal bounding box regression. Extensive experiments across five benchmark\ndatasets validate our approach, showing competitive performance in standard\nscene while achieving state-of-the-art results in open-vocabulary scene. Our\ncode is available at https://github.com/plankXie/PAML.", "published": "2025-09-08 02:27:10", "link": "http://arxiv.org/abs/2509.06291v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution", "abstract": "Skin health and disease resistance are closely linked to the skin barrier\nfunction, which protects against environmental factors and water loss. Two key\nphysiological indicators can quantitatively represent this barrier function:\nskin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH\nand TEWL is valuable for the public to monitor skin conditions regularly,\ndiagnose dermatological issues, and personalize their skincare regimens.\nHowever, these measurements are not easily accessible to general users unless\nthey visit a dermatology clinic with specialized instruments. To tackle this\nproblem, we propose a systematic solution to estimate SH and TEWL from selfie\nfacial images remotely with smartphones. Our solution encompasses multiple\nstages, including SH/TEWL data collection, data preprocessing, and formulating\na novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression.\nThrough experiments, we identified the annotation imbalance of the SH/TEWL data\nand proposed a symmetric-based contrastive regularization to reduce the model\nbias due to the imbalance effectively. This work is the first study to explore\nskin assessment from selfie facial images without physical measurements. It\nbridges the gap between computer vision and skin care research, enabling\nAI-driven accessible skin analysis for broader real-world applications.", "published": "2025-09-08 02:06:37", "link": "http://arxiv.org/abs/2509.06282v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "abstract": "Understanding 3D spatial relationships remains a major limitation of current\nVision-Language Models (VLMs). Prior work has addressed this issue by creating\nspatial question-answering (QA) datasets based on single images or indoor\nvideos. However, real-world embodied AI agents such as robots and self-driving\ncars typically rely on ego-centric, multi-view observations. To this end, we\nintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatial\nreasoning abilities of VLMs using ego-centric, multi-view outdoor data.\nEgo3D-Bench comprises over 8,600 QA pairs, created with significant involvement\nfrom human annotators to ensure quality and diversity. We benchmark 16 SOTA\nVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results\nreveal a notable performance gap between human level scores and VLM\nperformance, highlighting that current VLMs still fall short of human level\nspatial understanding. To bridge this gap, we propose Ego3D-VLM, a\npost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM\ngenerates cognitive map based on estimated global 3D coordinates, resulting in\n12% average improvement on multi-choice QA and 56% average improvement on\nabsolute distance estimation. Ego3D-VLM is modular and can be integrated with\nany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for\nadvancing toward human level spatial understanding in real-world, multi-view\nenvironments.", "published": "2025-09-08 01:08:41", "link": "http://arxiv.org/abs/2509.06266v1", "categories": ["cs.CV"], "primary_category": "cs.CV"}
{"title": "Codes Correcting Transpositions of Consecutive Symbols", "abstract": "The problem of correcting transpositions (or swaps) of consecutive symbols in\n$ q $-ary strings is studied. A family of codes correcting a transposition at\nan arbitrary location is described and proved to have asymptotically optimal\nredundancy. Additionally, an improved construction is given over a binary\nalphabet. Bounds on the cardinality of codes correcting $ t = \\textrm{const} $\ntranspositions are obtained. A lower bound on the achievable asymptotic rate of\noptimal codes correcting $ t = \\tau n $ transpositions is derived. Finally, a\nconstruction of codes correcting all possible patterns of transpositions is\npresented, and the corresponding lower bound on the zero-error capacity of the\n$ q $-ary transposition channel is stated.", "published": "2025-09-08 13:46:50", "link": "http://arxiv.org/abs/2509.06692v1", "categories": ["cs.IT", "cs.DM", "math.IT", "94B25, 94B50, 94B65, 68P30, 68R05"], "primary_category": "cs.IT"}
{"title": "Relational Algebras for Subset Selection and Optimisation", "abstract": "The database community lacks a unified relational query language for subset\nselection and optimisation queries, limiting both user expression and query\noptimiser reasoning about such problems. Decades of research (latterly under\nthe rubric of prescriptive analytics) have produced powerful evaluation\nalgorithms with incompatible, ad-hoc SQL extensions that specify and filter\nthrough distinct mechanisms. We present the first unified algebraic foundation\nfor these queries, introducing relational exponentiation to complete the\nfundamental algebraic operations alongside union (addition) and cross product\n(multiplication). First, we extend relational algebra to complete domain\nrelations-relations defined by characteristic functions rather than explicit\nextensions-achieving the expressiveness of NP-complete/hard problems, while\nsimultaneously providing query safety for finite inputs. Second, we introduce\nsolution sets, a higher-order relational algebra over sets of relations that\nnaturally expresses search spaces as functions f: Base to Decision, yielding\n|Decision|^|Base| candidate relations. Third, we provide structure-preserving\ntranslation semantics from solution sets to standard relational algebra,\nenabling mechanical translation to existing evaluation algorithms. This\nframework achieves the expressiveness of the most powerful prior approaches\nwhile providing the theoretical clarity and compositional properties absent in\nprevious work. We demonstrate the capabilities these algebras open up through a\npolymorphic SQL where standard clauses seamlessly express data management,\nsubset selection, and optimisation queries within a single paradigm.", "published": "2025-09-08 08:35:46", "link": "http://arxiv.org/abs/2509.06439v1", "categories": ["cs.DB", "cs.DM", "cs.MS"], "primary_category": "cs.DB"}
{"title": "Verifying Sampling Algorithms via Distributional Invariants", "abstract": "This paper develops a verification framework aimed at establishing the\ncorrectness of discrete sampling algorithms. We do so by considering\nprobabilistic programs as distribution transformers. Inspired by recent work on\ndistributional verification of Markov models, we introduce the notion of\n(inductive) distributional loop invariants for discrete probabilistic programs.\nThese invariants are embedded in a Hoare-like verification framework that\nincludes proof rules for total and partial correctness. To illustrate the\napplicability of our framework, we prove the correctness of two discrete\nsampling algorithms: the Fast Dice Roller and the Fast Loaded Dice Roller.", "published": "2025-09-08 08:01:42", "link": "http://arxiv.org/abs/2509.06410v1", "categories": ["cs.LO", "cs.DM"], "primary_category": "cs.LO"}
{"title": "Optimal Average Disk-Inspection via Fermat's Principle", "abstract": "This work resolves the optimal average-case cost of the Disk-Inspection\nproblem, a variant of Bellman's 1955 lost-in-a-forest problem. In\nDisk-Inspection, a mobile agent starts at the center of a unit disk and follows\na trajectory that inspects perimeter points whenever the disk does not obstruct\nvisibility. The worst-case cost was solved optimally in 1957 by Isbell, but the\naverage-case version remained open, with heuristic upper bounds proposed by\nGluss in 1961 and improved only recently.\n  Our approach applies Fermat's Principle of Least Time to a recently proposed\ndiscretization framework, showing that optimal solutions are captured by a\none-parameter family of recurrences independent of the discretization size. In\nthe continuum limit these recurrences give rise to a single-parameter optimal\ncontrol problem, whose trajectories coincide with limiting solutions of the\noriginal Disk-Inspection problem. A crucial step is proving that the optimal\ninitial condition generates a trajectory that avoids the unit disk, thereby\nvalidating the optics formulation and reducing the many-variable optimization\nto a rigorous one-parameter problem. In particular, this disproves Gluss's\nconjecture that optimal trajectories must touch the disk.\n  Our analysis determines the exact optimal average-case inspection cost, equal\nto $3.549259\\ldots$ and certified to at least six digits of accuracy.", "published": "2025-09-08 04:43:28", "link": "http://arxiv.org/abs/2509.06334v1", "categories": ["cs.DM"], "primary_category": "cs.DM"}
{"title": "UniSearch: Rethinking Search System with a Unified Generative Architecture", "abstract": "Modern search systems play a crucial role in facilitating information\nacquisition. Traditional search engines typically rely on a cascaded\narchitecture, where results are retrieved through recall, pre-ranking, and\nranking stages. The complexity of designing and maintaining multiple modules\nmakes it difficult to achieve holistic performance gains. Recent advances in\ngenerative recommendation have motivated the exploration of unified generative\nsearch as an alternative. However, existing approaches are not genuinely\nend-to-end: they typically train an item encoder to tokenize candidates first\nand then optimize a generator separately, leading to objective inconsistency\nand limited generalization. To address these limitations, we propose UniSearch,\na unified generative search framework for Kuaishou Search. UniSearch replaces\nthe cascaded pipeline with an end-to-end architecture that integrates a Search\nGenerator and a Video Encoder. The Generator produces semantic identifiers of\nrelevant items given a user query, while the Video Encoder learns latent item\nembeddings and provides their tokenized representations. A unified training\nframework jointly optimizes both components, enabling mutual enhancement and\nimproving representation quality and generation accuracy. Furthermore, we\nintroduce Search Preference Optimization (SPO), which leverages a reward model\nand real user feedback to better align generation with user preferences.\nExtensive experiments on industrial-scale datasets, together with online A/B\ntesting in both short-video and live search scenarios, demonstrate the strong\neffectiveness and deployment potential of UniSearch. Notably, its deployment in\nlive search yields the largest single-experiment improvement in recent years of\nour product's history, highlighting its practical value for real-world\napplications.", "published": "2025-09-08 17:08:26", "link": "http://arxiv.org/abs/2509.06887v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Unveiling the Listener Structure Underlying K-pop's Global Success: A Large-Scale Listening Data Analysis", "abstract": "From the mid-2000s to the 2010s, K-pop moved beyond its status as a\nregionally popular genre in Asia and established itself as a global music genre\nwith enthusiastic fans around the world. However, little is known about how the\nvast number of music listeners across the globe have listened to and perceived\nK-pop. This study addresses this question by analyzing a large-scale listening\ndataset from Last.fm. An analysis of the distribution of play counts reveals\nthat K-pop experienced a significant increase in plays between 2005 and 2019,\nlargely supported by a small group of heavy listeners. The Gini coefficient in\nplay counts is notably greater than that of existing mainstream genres and\nother growing niche genres. Furthermore, an analysis based on user-assigned\ngenre tags quantitatively demonstrates that between 2005 and 2010, K-pop shed\nits status as a local Asian genre and established itself as a distinct music\ngenre in its own right.", "published": "2025-09-08 12:21:15", "link": "http://arxiv.org/abs/2509.06606v1", "categories": ["cs.SI", "cs.IR", "cs.SD"], "primary_category": "cs.SI"}
{"title": "Reasoning-enhanced Query Understanding through Decomposition and Interpretation", "abstract": "Accurate inference of user intent is crucial for enhancing document retrieval\nin modern search engines. While large language models (LLMs) have made\nsignificant strides in this area, their effectiveness has predominantly been\nassessed with short, keyword-based queries. As AI-driven search evolves,\nlong-form queries with intricate intents are becoming more prevalent, yet they\nremain underexplored in the context of LLM-based query understanding (QU). To\nbridge this gap, we introduce ReDI: a Reasoning-enhanced approach for query\nunderstanding through Decomposition and Interpretation. ReDI leverages the\nreasoning and comprehension capabilities of LLMs in a three-stage pipeline: (i)\nit breaks down complex queries into targeted sub-queries to accurately capture\nuser intent; (ii) it enriches each sub-query with detailed semantic\ninterpretations to improve the query-document matching; and (iii) it\nindependently retrieves documents for each sub-query and employs a fusion\nstrategy to aggregate the results for the final ranking. We compiled a\nlarge-scale dataset of real-world complex queries from a major search engine\nand distilled the query understanding capabilities of teacher models into\nsmaller models for practical application. Experiments on BRIGHT and BEIR\ndemonstrate that ReDI consistently surpasses strong baselines in both sparse\nand dense retrieval paradigms, affirming its effectiveness.", "published": "2025-09-08 10:58:42", "link": "http://arxiv.org/abs/2509.06544v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "Rethinking LLM Parametric Knowledge as Post-retrieval Confidence for Dynamic Retrieval and Reranking", "abstract": "Large Language Models (LLMs) often generate inaccurate responses\n(hallucinations) when faced with questions beyond their knowledge scope.\nRetrieval-Augmented Generation (RAG) addresses this by leveraging external\nknowledge, but a critical challenge remains: determining whether retrieved\ncontexts effectively enhance the model`s ability to answer specific queries.\nThis challenge underscores the importance of knowledge boundary awareness,\nwhich current methods-relying on discrete labels or limited signals-fail to\naddress adequately, as they overlook the rich information in LLMs` continuous\ninternal hidden states. To tackle this, we propose a novel post-retrieval\nknowledge filtering approach. First, we construct a confidence detection model\nbased on LLMs` internal hidden states to quantify how retrieved contexts\nenhance the model`s confidence. Using this model, we build a preference dataset\n(NQ_Rerank) to fine-tune a reranker, enabling it to prioritize contexts\npreferred by the downstream LLM during reranking. Additionally, we introduce\nConfidence-Based Dynamic Retrieval (CBDR), which adaptively triggers retrieval\nbased on the LLM`s initial confidence in the original question, reducing\nknowledge conflicts and improving efficiency. Experimental results demonstrate\nsignificant improvements in accuracy for context screening and end-to-end RAG\nperformance, along with a notable reduction in retrieval costs while\nmaintaining competitive accuracy.", "published": "2025-09-08 09:37:20", "link": "http://arxiv.org/abs/2509.06472v1", "categories": ["cs.IR"], "primary_category": "cs.IR"}
{"title": "AudioBoost: Increasing Audiobook Retrievability in Spotify Search with Synthetic Query Generation", "abstract": "Spotify has recently introduced audiobooks as part of its catalog,\ncomplementing its music and podcast offering. Search is often the first entry\npoint for users to access new items, and an important goal for Spotify is to\nsupport users in the exploration of the audiobook catalog. More specifically,\nwe would like to enable users without a specific item in mind to broadly search\nby topic, genre, story tropes, decade, and discover audiobooks, authors and\npublishers they may like. To do this, we need to 1) inspire users to type more\nexploratory queries for audiobooks and 2) augment our retrieval systems to\nbetter deal with exploratory audiobook queries. This is challenging in a\ncold-start scenario, where we have a retrievabiliy bias due to the little\namount of user interactions with audiobooks compared to previously available\nitems such as music and podcast content. To address this, we propose\nAudioBoost, a system to boost audiobook retrievability in Spotify's Search via\nsynthetic query generation. AudioBoost leverages Large Language Models (LLMs)\nto generate synthetic queries conditioned on audiobook metadata. The synthetic\nqueries are indexed both in the Query AutoComplete (QAC) and in the Search\nRetrieval engine to improve query formulation and retrieval at the same time.\nWe show through offline evaluation that synthetic queries increase\nretrievability and are of high quality. Moreover, results from an online A/B\ntest show that AudioBoost leads to a +0.7% in audiobook impressions, +1.22% in\naudiobook clicks, and +1.82% in audiobook exploratory query completions.", "published": "2025-09-08 08:57:03", "link": "http://arxiv.org/abs/2509.06452v1", "categories": ["cs.IR", "cs.SD"], "primary_category": "cs.IR"}
{"title": "Compare: A Framework for Scientific Comparisons", "abstract": "Navigating the vast and rapidly increasing sea of academic publications to\nidentify institutional synergies, benchmark research contributions and pinpoint\nkey research contributions has become an increasingly daunting task, especially\nwith the current exponential increase in new publications. Existing tools\nprovide useful overviews or single-document insights, but none supports\nstructured, qualitative comparisons across institutions or publications.\n  To address this, we demonstrate Compare, a novel framework that tackles this\nchallenge by enabling sophisticated long-context comparisons of scientific\ncontributions. Compare empowers users to explore and analyze research overlaps\nand differences at both the institutional and publication granularity, all\ndriven by user-defined questions and automatic retrieval over online resources.\nFor this we leverage on Retrieval-Augmented Generation over evolving data\nsources to foster long context knowledge synthesis. Unlike traditional\nscientometric tools, Compare goes beyond quantitative indicators by providing\nqualitative, citation-supported comparisons.", "published": "2025-09-08 08:05:26", "link": "http://arxiv.org/abs/2509.06412v1", "categories": ["cs.DL", "cs.IR"], "primary_category": "cs.DL"}
{"title": "Row-Column Twisted Reed-Solomon codes", "abstract": "In this article, we present a new class of codes known as row-column twisted\nReed-Solomon codes (abbreviated as RCTRS), motivated by the works of\n\\cite{beelen2017twisted} and \\cite{liu2025column}. We explicitly provide\nconditions for such codes to be MDS and also ensure their existence. By\ndetermining the dimensions of their Schur squares, we prove that these MDS\ncodes are not equivalent to Reed-Solomon codes, thus presenting a new family of\nnon-RS MDS codes. Additionally, we prove that these MDS codes are also not\nequivalent to column twisted Reed-Solomon codes described in\n\\cite{liu2025column}, showing the novelty of our construction.", "published": "2025-09-08 17:30:03", "link": "http://arxiv.org/abs/2509.06919v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Rate-Optimal Streaming Codes over Three-Node Relay Networks with Burst Erasures", "abstract": "This paper investigates streaming codes over three-node relay networks under\nburst packet erasures with a delay constraint $T$. In any sliding window of\n$T+1$ consecutive packets, the source-to-relay and relay-to-destination\nchannels may introduce burst erasures of lengths at most $b_1$ and $b_2$,\nrespectively. Singhvi et al. proposed a construction achieving the optimal code\nrate when $\\max\\{b_1,b_2\\}\\mid (T-b_1-b_2)$. We construct streaming codes with\nthe optimal rate under the condition\n  $T\\geq b_1+b_2+\\frac{b_1b_2}{|b_1-b_2|}$, thereby enriching the family of\nrate-optimal streaming codes for three-node relay networks.", "published": "2025-09-08 17:25:37", "link": "http://arxiv.org/abs/2509.06912v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Enhancing Fault-Tolerant Surface Code Decoding with Iterative Lattice Reweighting", "abstract": "Efficient and realistic error decoding is crucial for fault-tolerant quantum\ncomputation (FTQC) on near-term devices. While decoding is a classical\npost-processing task, its effectiveness depends on accurately modeling quantum\nnoise, which is hardware-dependent. In particular, correlated bit-flip ($X$)\nand phase-flip ($Z$) errors often arise under circuit-level noise. We introduce\nthe Iterative Reweighting Minimum-Weight Perfect Matching (IRMWPM) decoder,\nwhich systematically incorporates such correlations to enhance quantum error\ncorrection. Our method leverages fault-detection patterns to guide reweighting:\ncorrelated $X$ and $Z$ detection events are identified, and their conditional\nprobabilities update weights on the primal and dual lattices. This iterative\nprocedure improves handling of realistic error propagation in a\nhardware-agnostic yet noise-aware manner. We prove that IRMWPM converges in\nfinite time while preserving the distance guarantee of MWPM. Numerical results\nunder circuit-level noise show substantial improvements. For distances $\\geq\n17$ and physical error rates $\\leq 0.001$, IRMWPM reduces logical error rates\nby over 20x with only a few iterations. It also raises the accuracy threshold\nfrom 1% to 1.16%, making it practical for near-term real-time decoding.\nExtrapolated estimates suggest that to reach logical error rate $10^{-16}$,\nIRMWPM requires distance $d=31$, while standard MWPM needs $d=50$, implying a\nmajor reduction in qubit overhead.", "published": "2025-09-08 14:45:59", "link": "http://arxiv.org/abs/2509.06756v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "On catastrophicity of convolutional codes and their encoders over $\\Z_{p^r}$", "abstract": "This paper investigates the existence of minimal $p$-encoders for\nconvolutional codes over $\\mathbb{Z}_{p^r}$, where $p$ is a prime. This\naddresses a conjecture from \\cite{k}, which posits that every such code admits\na minimal $p$-encoder, implying that all convolutional codes over\n$\\mathbb{Z}_{p^r}$ are noncatastrophic when input sequences are restricted to\ncoefficients in $\\{0, \\dots, p-1\\}$. Our contributions include the introduction\nof a new polynomial invariant that characterizes free codes, which enables us\nto establish a necessary and sufficient condition for a free code over\n$\\mathbb{Z}_{p^r}$ to be noncatastrophic in the usual sense (where input\ncoefficients are from $\\mathbb{Z}_{p^r}$). Based on these findings, we affirm\nthe conjecture by providing a constructive method for obtaining a minimal\n$p$-encoder for any convolutional code over $\\mathbb{Z}_{p^r}$.", "published": "2025-09-08 13:29:19", "link": "http://arxiv.org/abs/2509.06670v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data", "abstract": "The generative adversarial network (GAN) aims to approximate an unknown\ndistribution via a parameterized neural network (NN). While GANs have been\nwidely applied in reinforcement and semisupervised learning as well as computer\nvision tasks, selecting their parameters often needs an exhaustive search and\nonly a few selection methods can be proved to be theoretically optimal. One of\nthe most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on\noptimal parameters for WGAN is limited to the linear-quadratic-Gaussian (LQG)\nsetting, where the NN is linear and the data is Gaussian. In this paper, we\nfocus on the characterization of optimal WGAN parameters beyond the LQG\nsetting. We derive closed-form optimal parameters for one-dimensional WGANs\nwhen the NN has non-linear activation functions and the data is non-Gaussian.\nTo extend this to high-dimensional WGANs, we adopt the sliced Wasserstein\nframework and replace the constraint on marginal distributions of the randomly\nprojected data by a constraint on the joint distribution of the original\n(unprojected) data. We show that the linear generator can be asymptotically\noptimal for sliced WGAN with non-Gaussian data. Empirical studies show that our\nclosed-form WGAN parameters have good convergence behavior with data under both\nGaussian and Laplace distributions. Also, compared to the r principal component\nanalysis (r-PCA) solution, our proposed solution for sliced WGAN can achieve\nthe same performance while requiring less computational resources.", "published": "2025-09-08 10:10:37", "link": "http://arxiv.org/abs/2509.06505v1", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Trace Repair Never Loses to Classical Repair: Exact and Explicit Helper Nodes Selection", "abstract": "We study the repair of Reed--Solomon codes over $\\mathbb{F}=\\mathbb{B}^t$\nusing traces over $\\mathbb{B}$. Building on the trace framework of\nGuruswami--Wootters (2017), recent work of Liu--Wan--Xing (2024) reduced repair\nbandwidth by studying a related subspace $\\mathcal{W}_k$. In this work, we\ndetermine the dimension of $\\mathcal{W}_k$ exactly using cyclotomic cosets and\nprovide an explicit set of helper nodes that attains bandwidth $(n-d-1)\\log\n|\\mathbb{B}|$ bits with $d=\\text{dim}(\\mathcal{W}_k)$. Moreover, we show that\n$(n-d-1)\\le kt$, and so, trace repair never loses to the classical repair.", "published": "2025-09-08 09:54:12", "link": "http://arxiv.org/abs/2509.06492v1", "categories": ["cs.IT", "math.IT"], "primary_category": "cs.IT"}
{"title": "Beyond Diagonal IRS Aided OFDM: Rate Maximization under Frequency-Dependent Reflection", "abstract": "This paper studies a broadband orthogonal frequency division multiplexing\n(OFDM) system aided by a beyond diagonal intelligent reflecting surface\n(BD-IRS), where inter-connections exist among different elements such that the\nreflection matrix can exhibit a beyond diagonal structure. Under practical\ncircuit structures, the reflection matrix of the BD-IRS is generally dependent\non the circuit parameters (e.g., capacitance matrix for all tunable capacitors)\nas well as the operating frequency, which leads to couplings among the BD-IRS\nreflection matrices over different sub-carriers and consequently new challenges\nin the BD-IRS design. Motivated by this, we first model the relationship\nbetween the BD-IRS reflection matrices over different sub-carriers and the\ntunable capacitance matrix, and then formulate the joint optimization problem\nof the tunable capacitance matrix and power allocation over OFDM sub-carriers\nto maximize the achievable rate of the OFDM system. Despite the non-convexity\nof the problem, we propose an effective algorithm for finding a high-quality\nfeasible solution via leveraging alternating optimization and successive convex\napproximation. Numerical results show the superiority of our proposed design\nover benchmark designs.", "published": "2025-09-08 07:03:17", "link": "http://arxiv.org/abs/2509.06378v1", "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "cs.IT"}
{"title": "Single-Shot Decoding of Biased-Tailored Quantum LDPC Codes", "abstract": "Quantum processors are often affected by biased noise and noisy readout,\nwhich reduce reliability and reproducibility. This work combines two\ncomplementary strategies to address these challenges. The first is bias\ntailoring, which aligns stabilizers with the dominant error type. The second is\nsingle-shot (SS) decoding, which uses metachecks to identify measurement faults\nfrom just one noisy round. We implement these ideas in a four-dimensional\nlifted hypergraph product (4D-LHP) code constructed from quasi-cyclic\nprotograph seeds. Simulation results show that bias tailoring lowers the\nword-error rate (WER) by 20-60 percent across realistic Z:X bias ratios (from\n1:1 up to 1000:1), with the largest improvements at moderate bias. When\nmeasurement noise is present, a single SS round recovers more than one third of\nthe performance lost to readout errors. Moreover, metachecks identify over 99.8\npercent of faulty syndromes, providing near-complete fault visibility even with\nlimited correction power. Together, these findings demonstrate that 4D-LHP\ncodes maintain strong resilience under realistic noise, making them promising\ncandidates for integration into orchestrated QPU-CPU workflows.", "published": "2025-09-08 03:43:22", "link": "http://arxiv.org/abs/2509.06316v1", "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "quant-ph"}
{"title": "On the recognition problem for limits of entropy functions", "abstract": "We prove that there is no algorithm to decide whether a given integer vector\nis in the closure of the entropic cone $\\overline{\\Gamma_{n}^{*}}$.\nEquivalently, there is no decision procedure to determine whether a given\ninteger-valued function\n$h:\\mathcal{P}(\\{1,\\ldots,n\\})\\rightarrow\\mathbb{Z}_{\\ge 0}$ is a pointwise\nlimit of joint entropy functions. In other words, given such an $h$, it is\nundecidable whether for all $\\varepsilon > 0$ there exists a finite probability\nspace $(\\Omega,P)$ with random variables $X_{1},\\ldots,X_{n}$ such that their\njoint entropy $H$ satisfies\n$\\max_{I\\subseteq\\{1,\\ldots,n\\}}\\left|H\\left(X_{I}\\right)-h\\left(I\\right)\\right|<\\varepsilon$.\nThis settles the last open case in a sequence of related undecidability results\nproved by L. K\\\"{u}hne and the author, with applications in algorithmic\ninformation theory. The main new tool is a Desargues'-type theorem for almost\nentropic polymatroids.", "published": "2025-09-08 03:05:48", "link": "http://arxiv.org/abs/2509.06302v1", "categories": ["math.CO", "cs.IT", "math.IT", "05B35, 68P30, 20F10"], "primary_category": "math.CO"}
{"title": "Learning words in groups: fusion algebras, tensor ranks and grokking", "abstract": "In this work, we demonstrate that a simple two-layer neural network with\nstandard activation functions can learn an arbitrary word operation in any\nfinite group, provided sufficient width is available and exhibits grokking\nwhile doing so. To explain the mechanism by which this is achieved, we reframe\nthe problem as that of learning a particular $3$-tensor, which we show is\ntypically of low rank. A key insight is that low-rank implementations of this\ntensor can be obtained by decomposing it along triplets of basic self-conjugate\nrepresentations of the group and leveraging the fusion structure to rule out\nmany components. Focusing on a phenomenologically similar but more tractable\nsurrogate model, we show that the network is able to find such low-rank\nimplementations (or approximations thereof), thereby using limited width to\napproximate the word-tensor in a generalizable way. In the case of the simple\nmultiplication word, we further elucidate the form of these low-rank\nimplementations, showing that the network effectively implements efficient\nmatrix multiplication in the sense of Strassen. Our work also sheds light on\nthe mechanism by which a network reaches such a solution under gradient\ndescent.", "published": "2025-09-08 17:43:45", "link": "http://arxiv.org/abs/2509.06931v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Data-driven solar forecasting enables near-optimal economic decisions", "abstract": "Solar energy adoption is critical to achieving net-zero emissions. However,\nit remains difficult for many industrial and commercial actors to decide on\nwhether they should adopt distributed solar-battery systems, which is largely\ndue to the unavailability of fast, low-cost, and high-resolution irradiance\nforecasts. Here, we present SunCastNet, a lightweight data-driven forecasting\nsystem that provides 0.05$^\\circ$, 10-minute resolution predictions of surface\nsolar radiation downwards (SSRD) up to 7 days ahead. SunCastNet, coupled with\nreinforcement learning (RL) for battery scheduling, reduces operational regret\nby 76--93\\% compared to robust decision making (RDM). In 25-year investment\nbacktests, it enables up to five of ten high-emitting industrial sectors per\nregion to cross the commercial viability threshold of 12\\% Internal Rate of\nReturn (IRR). These results show that high-resolution, long-horizon solar\nforecasts can directly translate into measurable economic gains, supporting\nnear-optimal energy operations and accelerating renewable deployment.", "published": "2025-09-08 17:38:05", "link": "http://arxiv.org/abs/2509.06925v1", "categories": ["physics.geo-ph", "cs.LG"], "primary_category": "physics.geo-ph"}
{"title": "Neutron Reflectometry by Gradient Descent", "abstract": "Neutron reflectometry (NR) is a powerful technique to probe surfaces and\ninterfaces. NR is inherently an indirect measurement technique, access to the\nphysical quantities of interest (layer thickness, scattering length density,\nroughness), necessitate the solution of an inverse modelling problem, that is\ninefficient for large amounts of data or complex multiplayer structures (e.g.\nlithium batteries / electrodes). Recently, surrogate machine learning models\nhave been proposed as an alternative to existing optimisation routines.\nAlthough such approaches have been successful, physical intuition is lost when\nreplacing governing equations with fast neural networks. Instead, we propose a\nnovel and efficient approach; to optimise reflectivity data analysis by\nperforming gradient descent on the forward reflection model itself. Herein,\nautomatic differentiation techniques are used to evaluate exact gradients of\nthe error function with respect to the parameters of interest. Access to these\nquantities enables users of neutron reflectometry to harness a host of powerful\nmodern optimisation and inference techniques that remain thus far unexploited\nin the context of neutron reflectometry. This paper presents two benchmark case\nstudies; demonstrating state-of-the-art performance on a thick oxide quartz\nfilm, and robust co-fitting performance in the high complexity regime of\norganic LED multilayer devices. Additionally, we provide an open-source library\nof differentiable reflectometry kernels in the python programming language so\nthat gradient based approaches can readily be applied to other NR datasets.", "published": "2025-09-08 17:38:01", "link": "http://arxiv.org/abs/2509.06924v1", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "primary_category": "cs.LG"}
{"title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via Capability-Adaptive Hint Scaffolding", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks.", "published": "2025-09-08 17:36:21", "link": "http://arxiv.org/abs/2509.06923v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection", "abstract": "We propose HyGLAD, a novel algorithm that automatically builds a set of\ninterpretable patterns that model event data. These patterns can then be used\nto detect event-based anomalies in a stationary system, where any deviation\nfrom past behavior may indicate malicious activity. The algorithm infers\nequivalence classes of entities with similar behavior observed from the events,\nand then builds regular expressions that capture the values of those entities.\nAs opposed to deep-learning approaches, the regular expressions are directly\ninterpretable, which also translates to interpretable anomalies. We evaluate\nHyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five\ndatasets from real-world systems. The experimental results show that on average\nHyGLAD outperforms existing deep-learning methods while being an order of\nmagnitude more efficient in training and inference (single CPU vs GPU).\nPrecision improved by 1.2x and recall by 1.3x compared to the second-best\nbaseline.", "published": "2025-09-08 17:25:23", "link": "http://arxiv.org/abs/2509.06911v1", "categories": ["cs.SE", "cs.LG"], "primary_category": "cs.SE"}
{"title": "Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning", "abstract": "Targeted data poisoning attacks pose an increasingly serious threat due to\ntheir ease of deployment and high success rates. These attacks aim to\nmanipulate the prediction for a single test sample in classification models.\nUnlike indiscriminate attacks that aim to decrease overall test performance,\ntargeted attacks present a unique threat to individual test instances. This\nthreat model raises a fundamental question: what factors make certain test\nsamples more susceptible to successful poisoning than others? We investigate\nhow attack difficulty varies across different test instances and identify key\ncharacteristics that influence vulnerability. This paper introduces three\npredictive criteria for targeted data poisoning difficulty: ergodic prediction\naccuracy (analyzed through clean training dynamics), poison distance, and\npoison budget. Our experimental results demonstrate that these metrics\neffectively predict the varying difficulty of real-world targeted poisoning\nattacks across diverse scenarios, offering practitioners valuable insights for\nvulnerability assessment and understanding data poisoning attacks.", "published": "2025-09-08 17:14:55", "link": "http://arxiv.org/abs/2509.06896v1", "categories": ["cs.LG", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Learning from one graph: transductive learning guarantees via the geometry of small random worlds", "abstract": "Since their introduction by Kipf and Welling in $2017$, a primary use of\ngraph convolutional networks is transductive node classification, where missing\nlabels are inferred within a single observed graph and its feature matrix.\nDespite the widespread use of the network model, the statistical foundations of\ntransductive learning remain limited, as standard inference frameworks\ntypically rely on multiple independent samples rather than a single graph. In\nthis work, we address these gaps by developing new concentration-of-measure\ntools that leverage the geometric regularities of large graphs via\nlow-dimensional metric embeddings. The emergent regularities are captured using\na random graph model; however, the methods remain applicable to deterministic\ngraphs once observed. We establish two principal learning results. The first\nconcerns arbitrary deterministic $k$-vertex graphs, and the second addresses\nrandom graphs that share key geometric properties with an Erd\\H{o}s-R\\'{e}nyi\ngraph $\\mathbf{G}=\\mathbf{G}(k,p)$ in the regime $p \\in \\mathcal{O}((\\log\n(k)/k)^{1/2})$. The first result serves as the basis for and illuminates the\nsecond. We then extend these results to the graph convolutional network\nsetting, where additional challenges arise. Lastly, our learning guarantees\nremain informative even with a few labelled nodes $N$ and achieve the optimal\nnonparametric rate $\\mathcal{O}(N^{-1/2})$ as $N$ grows.", "published": "2025-09-08 17:13:28", "link": "http://arxiv.org/abs/2509.06894v1", "categories": ["stat.ML", "cs.LG", "math.MG", "math.PR", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "Learning spatially structured open quantum dynamics with regional-attention transformers", "abstract": "Simulating the dynamics of open quantum systems with spatial structure and\nexternal control is an important challenge in quantum information science.\nClassical numerical solvers for such systems require integrating coupled master\nand field equations, which is computationally demanding for simulation and\noptimization tasks and often precluding real-time use in network-scale\nsimulations or feedback control. We introduce a regional attention-based neural\narchitecture that learns the spatiotemporal dynamics of structured open quantum\nsystems. The model incorporates translational invariance of physical laws as an\ninductive bias to achieve scalable complexity, and supports conditioning on\ntime-dependent global control parameters. We demonstrate learning on two\nrepresentative systems: a driven dissipative single qubit and an\nelectromagnetically induced transparency (EIT) quantum memory. The model\nachieves high predictive fidelity under both in-distribution and\nout-of-distribution control protocols, and provides substantial acceleration up\nto three orders of magnitude over numerical solvers. These results demonstrate\nthat the architecture establishes a general surrogate modeling framework for\nspatially structured open quantum dynamics, with immediate relevance to\nlarge-scale quantum network simulation, quantum repeater and protocol design,\nreal-time experimental optimization, and scalable device modeling across\ndiverse light-matter platforms.", "published": "2025-09-08 16:40:32", "link": "http://arxiv.org/abs/2509.06871v1", "categories": ["quant-ph", "cs.LG", "physics.atom-ph"], "primary_category": "quant-ph"}
{"title": "Concolic Testing on Individual Fairness of Neural Network Models", "abstract": "This paper introduces PyFair, a formal framework for evaluating and verifying\nindividual fairness of Deep Neural Networks (DNNs). By adapting the concolic\ntesting tool PyCT, we generate fairness-specific path constraints to\nsystematically explore DNN behaviors. Our key innovation is a dual network\narchitecture that enables comprehensive fairness assessments and provides\ncompleteness guarantees for certain network types. We evaluate PyFair on 25\nbenchmark models, including those enhanced by existing bias mitigation\ntechniques. Results demonstrate PyFair's efficacy in detecting discriminatory\ninstances and verifying fairness, while also revealing scalability challenges\nfor complex models. This work advances algorithmic fairness in critical domains\nby offering a rigorous, systematic method for fairness testing and verification\nof pre-trained DNNs.", "published": "2025-09-08 16:31:14", "link": "http://arxiv.org/abs/2509.06864v1", "categories": ["cs.LG", "cs.SE"], "primary_category": "cs.LG"}
{"title": "Sequential Least-Squares Estimators with Fast Randomized Sketching for Linear Statistical Models", "abstract": "We propose a novel randomized framework for the estimation problem of\nlarge-scale linear statistical models, namely Sequential Least-Squares\nEstimators with Fast Randomized Sketching (SLSE-FRS), which integrates\nSketch-and-Solve and Iterative-Sketching methods for the first time. By\niteratively constructing and solving sketched least-squares (LS) subproblems\nwith increasing sketch sizes to achieve better precisions, SLSE-FRS gradually\nrefines the estimators of the true parameter vector, ultimately producing\nhigh-precision estimators. We analyze the convergence properties of SLSE-FRS,\nand provide its efficient implementation. Numerical experiments show that\nSLSE-FRS outperforms the state-of-the-art methods, namely the Preconditioned\nConjugate Gradient (PCG) method, and the Iterative Double Sketching (IDS)\nmethod.", "published": "2025-09-08 16:23:58", "link": "http://arxiv.org/abs/2509.06856v1", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "primary_category": "stat.ML"}
{"title": "Green Learning for STAR-RIS mmWave Systems with Implicit CSI", "abstract": "In this paper, a green learning (GL)-based precoding framework is proposed\nfor simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS)-aided millimeter-wave (mmWave) MIMO broadcasting systems.\nMotivated by the growing emphasis on environmental sustainability in future 6G\nnetworks, this work adopts a broadcasting transmission architecture for\nscenarios where multiple users share identical information, improving spectral\nefficiency and reducing redundant transmissions and power consumption.\nDifferent from conventional optimization methods, such as block coordinate\ndescent (BCD) that require perfect channel state information (CSI) and\niterative computation, the proposed GL framework operates directly on received\nuplink pilot signals without explicit CSI estimation. Unlike deep learning (DL)\napproaches that require CSI-based labels for training, the proposed GL approach\nalso avoids deep neural networks and backpropagation, leading to a more\nlightweight design. Although the proposed GL framework is trained with\nsupervision generated by BCD under full CSI, inference is performed in a fully\nCSI-free manner. The proposed GL integrates subspace approximation with\nadjusted bias (Saab), relevant feature test (RFT)-based supervised feature\nselection, and eXtreme gradient boosting (XGBoost)-based decision learning to\njointly predict the STAR-RIS coefficients and transmit precoder. Simulation\nresults show that the proposed GL approach achieves competitive spectral\nefficiency compared to BCD and DL-based models, while reducing floating-point\noperations (FLOPs) by over four orders of magnitude. These advantages make the\nproposed GL approach highly suitable for real-time deployment in energy- and\nhardware-constrained broadcasting scenarios.", "published": "2025-09-08 15:56:06", "link": "http://arxiv.org/abs/2509.06820v1", "categories": ["eess.SP", "cs.LG"], "primary_category": "eess.SP"}
{"title": "Reward function compression facilitates goal-dependent reinforcement learning", "abstract": "Reinforcement learning agents learn from rewards, but humans can uniquely\nassign value to novel, abstract outcomes in a goal-dependent manner. However,\nthis flexibility is cognitively costly, making learning less efficient. Here,\nwe propose that goal-dependent learning is initially supported by a\ncapacity-limited working memory system. With consistent experience, learners\ncreate a \"compressed\" reward function (a simplified rule defining the goal)\nwhich is then transferred to long-term memory and applied automatically upon\nreceiving feedback. This process frees up working memory resources, boosting\nlearning efficiency. We test this theory across six experiments. Consistent\nwith our predictions, our findings demonstrate that learning is parametrically\nimpaired by the size of the goal space, but improves when the goal space\nstructure allows for compression. We also find faster reward processing to\ncorrelate with better learning performance, supporting the idea that as goal\nvaluation becomes more automatic, more resources are available for learning. We\nleverage computational modeling to support this interpretation. Our work\nsuggests that efficient goal-directed learning relies on compressing complex\ngoal information into a stable reward function, shedding light on the cognitive\nmechanisms of human motivation. These findings generate new insights into the\nneuroscience of intrinsic motivation and could help improve behavioral\ntechniques that support people in achieving their goals.", "published": "2025-09-08 15:43:40", "link": "http://arxiv.org/abs/2509.06810v1", "categories": ["q-bio.NC", "cs.LG"], "primary_category": "q-bio.NC"}
{"title": "Imitative Membership Inference Attack", "abstract": "A Membership Inference Attack (MIA) assesses how much a target machine\nlearning model reveals about its training data by determining whether specific\nquery instances were part of the training set. State-of-the-art MIAs rely on\ntraining hundreds of shadow models that are independent of the target model,\nleading to significant computational overhead. In this paper, we introduce\nImitative Membership Inference Attack (IMIA), which employs a novel imitative\ntraining technique to strategically construct a small number of target-informed\nimitative models that closely replicate the target model's behavior for\ninference. Extensive experimental results demonstrate that IMIA substantially\noutperforms existing MIAs in various attack settings while only requiring less\nthan 5% of the computational cost of state-of-the-art approaches.", "published": "2025-09-08 15:27:35", "link": "http://arxiv.org/abs/2509.06796v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Dato: A Task-Based Programming Model for Dataflow Accelerators", "abstract": "Recent deep learning workloads increasingly push computational demand beyond\nwhat current memory systems can sustain, with many kernels stalling on data\nmovement rather than computation. While modern dataflow accelerators\nincorporate on-chip streaming to mitigate off-chip bandwidth limitations,\nexisting programming models struggle to harness these capabilities effectively.\nLow-level interfaces provide fine-grained control but impose significant\ndevelopment overhead, whereas high-level tile-based languages abstract away\ncommunication details, restricting optimization and forcing compilers to\nreconstruct the intended dataflow. We present Dato, a Python-embedded,\ntask-based programming model for dataflow accelerators that elevates data\ncommunication and sharding to first-class type constructs. Developers write\nprograms as a graph of tasks connected via explicit stream types, with sharded\ninputs specified using layout types. These tasks are first mapped virtually\nonto the accelerator's spatial fabric, and the compiler then generates a\nphysical mapping that respects hardware constraints. Experimental results on\nboth AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves\nhigh performance while significantly reducing the burden of writing optimized\ncode. On the NPU, Dato attains up to 84% hardware utilization for GEMM and\ndelivers a 2.81x speedup on attention kernels compared to a state-of-the-art\ncommercial framework. On the FPGA, Dato surpasses leading frameworks in\nperformance when generating custom systolic arrays, achieving 98% of the\ntheoretical peak performance.", "published": "2025-09-08 15:22:51", "link": "http://arxiv.org/abs/2509.06794v1", "categories": ["cs.PL", "cs.AR", "cs.LG"], "primary_category": "cs.PL"}
{"title": "\\texttt{R$^\\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World", "abstract": "In this position paper, we address the persistent gap between rapidly growing\nAI capabilities and lagging safety progress. Existing paradigms divide into\n``Make AI Safe'', which applies post-hoc alignment and guardrails but remains\nbrittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety\nbut struggles to address unforeseen risks in open-ended environments. We\ntherefore propose \\textit{safe-by-coevolution} as a new formulation of the\n``Make Safe AI'' paradigm, inspired by biological immunity, in which safety\nbecomes a dynamic, adversarial, and ongoing learning process. To operationalize\nthis vision, we introduce \\texttt{R$^2$AI} -- \\textit{Resistant and Resilient\nAI} -- as a practical framework that unites resistance against known threats\nwith resilience to unforeseen risks. \\texttt{R$^2$AI} integrates \\textit{fast\nand slow safe models}, adversarial simulation and verification through a\n\\textit{safety wind tunnel}, and continual feedback loops that guide safety and\ncapability to coevolve. We argue that this framework offers a scalable and\nproactive path to maintain continual safety in dynamic environments, addressing\nboth near-term vulnerabilities and long-term existential risks as AI advances\ntoward AGI and ASI.", "published": "2025-09-08 15:13:23", "link": "http://arxiv.org/abs/2509.06786v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning", "abstract": "Offline Goal-Conditioned Reinforcement Learning (GCRL) holds great promise\nfor domains such as autonomous navigation and locomotion, where collecting\ninteractive data is costly and unsafe. However, it remains challenging in\npractice due to the need to learn from datasets with limited coverage of the\nstate-action space and to generalize across long-horizon tasks. To improve on\nthese challenges, we propose a Physics-informed (Pi) regularized loss for value\nlearning, derived from the Eikonal Partial Differential Equation (PDE) and\nwhich induces a geometric inductive bias in the learned value function. Unlike\ngeneric gradient penalties that are primarily used to stabilize training, our\nformulation is grounded in continuous-time optimal control and encourages value\nfunctions to align with cost-to-go structures. The proposed regularizer is\nbroadly compatible with temporal-difference-based value learning and can be\nintegrated into existing Offline GCRL algorithms. When combined with\nHierarchical Implicit Q-Learning (HIQL), the resulting method, Physics-informed\nHIQL (Pi-HIQL), yields significant improvements in both performance and\ngeneralization, with pronounced gains in stitching regimes and large-scale\nnavigation tasks.", "published": "2025-09-08 15:08:42", "link": "http://arxiv.org/abs/2509.06782v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs when\ntasks require long-range interactions. The problem arises from the presence of\nbottlenecks that limit the propagation of messages among distant nodes.\nRecently, graph rewiring methods modify edge connectivity and are expected to\nperform well on long-range tasks. Yet, graph rewiring compromises the inductive\nbias, incurring significant information loss in solving the downstream task.\nFurthermore, increasing channel capacity may overcome information bottlenecks\nbut enhance the parameter complexity of the model. To alleviate these\nshortcomings, we propose an efficient model-agnostic framework that\nasynchronously updates node features, unlike traditional synchronous message\npassing GNNs. Our framework creates node batches in every layer based on the\nnode centrality values. The features of the nodes belonging to these batches\nwill only get updated. Asynchronous message updates process information\nsequentially across layers, avoiding simultaneous compression into\nfixed-capacity channels. We also theoretically establish that our proposed\nframework maintains higher feature sensitivity bounds compared to standard\nsynchronous approaches. Our framework is applied to six standard graph datasets\nand two long-range datasets to perform graph classification and achieves\nimpressive performances with a $5\\%$ and $4\\%$ improvements on REDDIT-BINARY\nand Peptides-struct, respectively.", "published": "2025-09-08 15:03:05", "link": "http://arxiv.org/abs/2509.06777v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "RT-HCP: Dealing with Inference Delays and Sample Efficiency to Learn Directly on Robotic Platforms", "abstract": "Learning a controller directly on the robot requires extreme sample\nefficiency. Model-based reinforcement learning (RL) methods are the most sample\nefficient, but they often suffer from a too long inference time to meet the\nrobot control frequency requirements. In this paper, we address the sample\nefficiency and inference time challenges with two contributions. First, we\ndefine a general framework to deal with inference delays where the slow\ninference robot controller provides a sequence of actions to feed the\ncontrol-hungry robotic platform without execution gaps. Then, we compare\nseveral RL algorithms in the light of this framework and propose RT-HCP, an\nalgorithm that offers an excellent trade-off between performance, sample\nefficiency and inference time. We validate the superiority of RT-HCP with\nexperiments where we learn a controller directly on a simple but high frequency\nFURUTA pendulum platform. Code: github.com/elasriz/RTHCP", "published": "2025-09-08 14:09:33", "link": "http://arxiv.org/abs/2509.06714v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "When Secure Isn't: Assessing the Security of Machine Learning Model Sharing", "abstract": "The rise of model-sharing through frameworks and dedicated hubs makes Machine\nLearning significantly more accessible. Despite their benefits, these tools\nexpose users to underexplored security risks, while security awareness remains\nlimited among both practitioners and developers. To enable a more\nsecurity-conscious culture in Machine Learning model sharing, in this paper we\nevaluate the security posture of frameworks and hubs, assess whether\nsecurity-oriented mechanisms offer real protection, and survey how users\nperceive the security narratives surrounding model sharing. Our evaluation\nshows that most frameworks and hubs address security risks partially at best,\noften by shifting responsibility to the user. More concerningly, our analysis\nof frameworks advertising security-oriented settings and complete model sharing\nuncovered six 0-day vulnerabilities enabling arbitrary code execution. Through\nthis analysis, we debunk the misconceptions that the model-sharing problem is\nlargely solved and that its security can be guaranteed by the file format used\nfor sharing. As expected, our survey shows that the surrounding security\nnarrative leads users to consider security-oriented settings as trustworthy,\ndespite the weaknesses shown in this work. From this, we derive takeaways and\nsuggestions to strengthen the security of model-sharing ecosystems.", "published": "2025-09-08 13:55:54", "link": "http://arxiv.org/abs/2509.06703v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Nested Optimal Transport Distances", "abstract": "Simulating realistic financial time series is essential for stress testing,\nscenario generation, and decision-making under uncertainty. Despite advances in\ndeep generative models, there is no consensus metric for their evaluation. We\nfocus on generative AI for financial time series in decision-making\napplications and employ the nested optimal transport distance, a time-causal\nvariant of optimal transport distance, which is robust to tasks such as\nhedging, optimal stopping, and reinforcement learning. Moreover, we propose a\nstatistically consistent, naturally parallelizable algorithm for its\ncomputation, achieving substantial speedups over existing approaches.", "published": "2025-09-08 13:55:18", "link": "http://arxiv.org/abs/2509.06702v1", "categories": ["cs.LG", "q-fin.CP", "91G60, 60G07, 65C60"], "primary_category": "cs.LG"}
{"title": "Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties", "abstract": "Accurate forecasting of exchange rates remains a persistent challenge,\nparticularly for emerging economies such as Brazil, Russia, India, and China\n(BRIC). These series exhibit long memory, nonlinearity, and non-stationarity\nproperties that conventional time series models struggle to capture.\nAdditionally, there exist several key drivers of exchange rate dynamics,\nincluding global economic policy uncertainty, US equity market volatility, US\nmonetary policy uncertainty, oil price growth rates, and country-specific\nshort-term interest rate differentials. These empirical complexities underscore\nthe need for a flexible modeling framework that can jointly accommodate long\nmemory, nonlinearity, and the influence of external drivers. To address these\nchallenges, we propose a Neural AutoRegressive Fractionally Integrated Moving\nAverage (NARFIMA) model that combines the long-memory representation of ARFIMA\nwith the nonlinear learning capacity of neural networks, while flexibly\nincorporating exogenous causal variables. We establish theoretical properties\nof the model, including asymptotic stationarity of the NARFIMA process using\nMarkov chains and nonlinear time series techniques. We quantify forecast\nuncertainty using conformal prediction intervals within the NARFIMA framework.\nEmpirical results across six forecast horizons show that NARFIMA consistently\noutperforms various state-of-the-art statistical and machine learning models in\nforecasting BRIC exchange rates. These findings provide new insights for\npolicymakers and market participants navigating volatile financial conditions.\nThe \\texttt{narfima} \\textbf{R} package provides an implementation of our\napproach.", "published": "2025-09-08 13:49:48", "link": "http://arxiv.org/abs/2509.06697v1", "categories": ["econ.EM", "cs.LG", "stat.AP", "stat.ML"], "primary_category": "econ.EM"}
{"title": "Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives", "abstract": "Understanding and modeling individual travel behavior responses is crucial\nfor urban mobility regulation and policy evaluation. The Markov decision\nprocess (MDP) provides a structured framework for dynamic travel behavior\nmodeling at the individual level. However, solving an MDP in this context is\nhighly data-intensive and faces challenges of data quantity, spatial-temporal\ncoverage, and situational diversity. To address these, we propose a\ngroup-effect-enhanced generative adversarial imitation learning (gcGAIL) model\nthat improves the individual behavior modeling efficiency by leveraging shared\nbehavioral patterns among passenger groups. We validate the gcGAIL model using\na public transport fare-discount case study and compare against\nstate-of-the-art benchmarks, including adversarial inverse reinforcement\nlearning (AIRL), baseline GAIL, and conditional GAIL. Experimental results\ndemonstrate that gcGAIL outperforms these methods in learning individual travel\nbehavior responses to incentives over time in terms of accuracy,\ngeneralization, and pattern demonstration efficiency. Notably, gcGAIL is robust\nto spatial variation, data sparsity, and behavioral diversity, maintaining\nstrong performance even with partial expert demonstrations and underrepresented\npassenger groups. The gcGAIL model predicts the individual behavior response at\nany time, providing the basis for personalized incentives to induce sustainable\nbehavior changes (better timing of incentive injections).", "published": "2025-09-08 13:14:28", "link": "http://arxiv.org/abs/2509.06656v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Knowledge-Guided Machine Learning for Stabilizing Near-Shortest Path Routing", "abstract": "We propose a simple algorithm that needs only a few data samples from a\nsingle graph for learning local routing policies that generalize across a rich\nclass of geometric random graphs in Euclidean metric spaces. We thus solve the\nall-pairs near-shortest path problem by training deep neural networks (DNNs)\nthat let each graph node efficiently and scalably route (i.e., forward) packets\nby considering only the node's state and the state of the neighboring nodes.\nOur algorithm design exploits network domain knowledge in the selection of\ninput features and design of the policy function for learning an approximately\noptimal policy. Domain knowledge also provides theoretical assurance that the\nchoice of a ``seed graph'' and its node data sampling suffices for\ngeneralizable learning. Remarkably, one of these DNNs we train -- using\ndistance-to-destination as the only input feature -- learns a policy that\nexactly matches the well-known Greedy Forwarding policy, which forwards packets\nto the neighbor with the shortest distance to the destination. We also learn a\nnew policy, which we call GreedyTensile routing -- using both\ndistance-to-destination and node stretch as the input features -- that almost\nalways outperforms greedy forwarding. We demonstrate the explainability and\nultra-low latency run-time operation of Greedy Tensile routing by symbolically\ninterpreting its DNN in low-complexity terms of two linear actions.", "published": "2025-09-08 12:56:42", "link": "http://arxiv.org/abs/2509.06640v1", "categories": ["cs.LG", "cs.NI"], "primary_category": "cs.LG"}
{"title": "A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models", "abstract": "Graph anomaly detection (GAD) has attracted increasing attention in recent\nyears for identifying malicious samples in a wide range of graph-based\napplications, such as social media and e-commerce. However, most GAD methods\nassume identical training and testing distributions and are tailored to\nspecific tasks, resulting in limited adaptability to real-world scenarios such\nas shifting data distributions and scarce training samples in new applications.\nTo address the limitations, recent work has focused on improving the\ngeneralization capability of GAD models through transfer learning that\nleverages knowledge from related domains to enhance detection performance, or\ndeveloping \"one-for-all\" GAD foundation models that generalize across multiple\napplications. Since a systematic understanding of generalization in GAD is\nstill lacking, in this paper, we provide a comprehensive review of\ngeneralization in GAD. We first trace the evolution of generalization in GAD\nand formalize the problem settings, which further leads to our systematic\ntaxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensive\nreview is conducted for the existing generalized GAD methods. Finally, we\nidentify current open challenges and suggest future directions to inspire\nfuture research in this emerging field.", "published": "2025-09-08 12:26:32", "link": "http://arxiv.org/abs/2509.06609v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Small Vectors, Big Effects: A Mechanistic Study of RL-Induced Reasoning via Steering Vectors", "abstract": "The mechanisms by which reasoning training reshapes language-model\ncomputations remain poorly understood. We study lightweight steering vectors\ninserted into the base model's residual stream and trained with a\nreinforcement-learning objective, which can match full fine-tuning performance\nwhile retaining the interpretability of small, additive interventions. Using\nlogit-lens readouts, path patching, and circuit analyses, we analyze two models\nand find: (i) the last-layer steering vector behaves like a token-substitution\nbias concentrated on the first generated token, consistently boosting tokens\nsuch as \"To\" and \"Step\"; and (ii) the penultimate-layer steering vector leaves\nattention patterns largely unchanged and instead acts through the MLP and\nunembedding, preferentially up-weighting process words and structure symbols.\nThese results establish a principled framework for interpreting the behavioral\nchanges induced by reasoning training.", "published": "2025-09-08 12:26:31", "link": "http://arxiv.org/abs/2509.06608v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification", "abstract": "Graph neural networks (GNNs) have achieved remarkable success in processing\ngraph-structured data across various applications. A critical aspect of\nreal-world graphs is their dynamic nature, where new nodes are continually\nadded and existing connections may change over time. Previous theoretical\nstudies, largely based on the transductive learning framework, fail to\nadequately model such temporal evolution and structural dynamics. In this\npaper, we presents a PAC-Bayesian theoretical analysis of graph convolutional\nnetworks (GCNs) for inductive node classification, treating nodes as dependent\nand non-identically distributed data points. We derive novel generalization\nbounds for one-layer GCNs that explicitly incorporate the effects of data\ndependency and non-stationarity, and establish sufficient conditions under\nwhich the generalization gap converges to zero as the number of nodes\nincreases. Furthermore, we extend our analysis to two-layer GCNs, and reveal\nthat it requires stronger assumptions on graph topology to guarantee\nconvergence. This work establishes a theoretical foundation for understanding\nand improving GNN generalization in dynamic graph environments.", "published": "2025-09-08 12:10:54", "link": "http://arxiv.org/abs/2509.06600v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems", "abstract": "Dynamic nonlinear systems exhibit distortions arising from coupled static and\ndynamic effects. Their intertwined nature poses major challenges for\ndata-driven modeling. This paper presents a theoretical framework grounded in\nstructured decomposition, variance analysis, and task-centric complexity\nbounds.\n  The framework employs a directional lower bound on interactions between\nmeasurable system components, extending orthogonality in inner product spaces\nto structurally asymmetric settings. This bound supports variance inequalities\nfor decomposed systems. Key behavioral indicators are introduced along with a\nmemory finiteness index. A rigorous power-based condition establishes a\nmeasurable link between finite memory in realizable systems and the First Law\nof Thermodynamics. This offers a more foundational perspective than classical\nbounds based on the Second Law.\n  Building on this foundation, we formulate a `Behavioral Uncertainty\nPrinciple,' demonstrating that static and dynamic distortions cannot be\nminimized simultaneously. We identify that real-world systems seem to resist\ncomplete deterministic decomposition due to entangled static and dynamic\neffects. We also present two general-purpose theorems linking function variance\nto mean-squared Lipschitz continuity and learning complexity. This yields a\nmodel-agnostic, task-aware complexity metric, showing that lower-variance\ncomponents are inherently easier to learn.\n  These insights explain the empirical benefits of structured residual\nlearning, including improved generalization, reduced parameter count, and lower\ntraining cost, as previously observed in power amplifier linearization\nexperiments. The framework is broadly applicable and offers a scalable,\ntheoretically grounded approach to modeling complex dynamic nonlinear systems.", "published": "2025-09-08 12:08:02", "link": "http://arxiv.org/abs/2509.06599v1", "categories": ["cs.LG", "cs.CC", "cs.SY", "eess.SP", "eess.SY", "math.ST", "stat.TH"], "primary_category": "cs.LG"}
{"title": "AI for Scientific Discovery is a Social Problem", "abstract": "Artificial intelligence promises to accelerate scientific discovery, yet its\nbenefits remain unevenly distributed. While technical obstacles such as scarce\ndata, fragmented standards, and unequal access to computation are significant,\nwe argue that the primary barriers are social and institutional. Narratives\nthat defer progress to speculative \"AI scientists,\" the undervaluing of data\nand infrastructure contributions, misaligned incentives, and gaps between\ndomain experts and machine learning researchers all constrain impact. We\nhighlight four interconnected challenges: community dysfunction, research\npriorities misaligned with upstream needs, data fragmentation, and\ninfrastructure inequities. We argue that their roots lie in cultural and\norganizational practices. Addressing them requires not only technical\ninnovation but also intentional community-building, cross-disciplinary\neducation, shared benchmarks, and accessible infrastructure. We call for\nreframing AI for science as a collective social project, where sustainable\ncollaboration and equitable participation are treated as prerequisites for\ntechnical progress.", "published": "2025-09-08 11:49:52", "link": "http://arxiv.org/abs/2509.06580v1", "categories": ["cs.LG", "cs.CY"], "primary_category": "cs.LG"}
{"title": "Automated Hierarchical Graph Construction for Multi-source Electronic Health Records", "abstract": "Electronic Health Records (EHRs), comprising diverse clinical data such as\ndiagnoses, medications, and laboratory results, hold great promise for\ntranslational research. EHR-derived data have advanced disease prevention,\nimproved clinical trial recruitment, and generated real-world evidence.\nSynthesizing EHRs across institutions enables large-scale, generalizable\nstudies that capture rare diseases and population diversity, but remains\nhindered by the heterogeneity of medical codes, institution-specific\nterminologies, and the absence of standardized data structures. These barriers\nlimit the interpretability, comparability, and scalability of EHR-based\nanalyses, underscoring the need for robust methods to harmonize and extract\nmeaningful insights from distributed, heterogeneous data. To address this, we\npropose MASH (Multi-source Automated Structured Hierarchy), a fully automated\nframework that aligns medical codes across institutions using neural optimal\ntransport and constructs hierarchical graphs with learned hyperbolic\nembeddings. During training, MASH integrates information from pre-trained\nlanguage models, co-occurrence patterns, textual descriptions, and supervised\nlabels to capture semantic and hierarchical relationships among medical\nconcepts more effectively. Applied to real-world EHR data, including diagnosis,\nmedication, and laboratory codes, MASH produces interpretable hierarchical\ngraphs that facilitate the navigation and understanding of heterogeneous\nclinical data. Notably, it generates the first automated hierarchies for\nunstructured local laboratory codes, establishing foundational references for\ndownstream applications.", "published": "2025-09-08 11:45:59", "link": "http://arxiv.org/abs/2509.06576v1", "categories": ["stat.ML", "cs.LG"], "primary_category": "stat.ML"}
{"title": "Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination", "abstract": "Representation-based multi-task learning (MTL) improves efficiency by\nlearning a shared structure across tasks, but its practical application is\noften hindered by contamination, outliers, or adversarial tasks. Most existing\nmethods and theories assume a clean or near-clean setting, failing when\ncontamination is significant. This paper tackles representation MTL with an\nunknown and potentially large contamination proportion, while also allowing for\nheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectral\nmethod (RAS) that can distill the shared inlier representation effectively and\nefficiently, while requiring no prior knowledge of the contamination level or\nthe true representation dimension. Theoretically, we provide non-asymptotic\nerror bounds for both the learned representation and the per-task parameters.\nThese bounds adapt to inlier task similarity and outlier structure, and\nguarantee that RAS performs at least as well as single-task learning, thus\npreventing negative transfer. We also extend our framework to transfer learning\nwith corresponding theoretical guarantees for the target task. Extensive\nexperiments confirm our theory, showcasing the robustness and adaptivity of\nRAS, and its superior performance in regimes with up to 80\\% task\ncontamination.", "published": "2025-09-08 11:41:30", "link": "http://arxiv.org/abs/2509.06575v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "Topological Regularization for Force Prediction in Active Particle Suspension with EGNN and Persistent Homology", "abstract": "Capturing the dynamics of active particles, i.e., small self-propelled agents\nthat both deform and are deformed by a fluid in which they move is a formidable\nproblem as it requires coupling fine scale hydrodynamics with large scale\ncollective effects. So we present a multi-scale framework that combines the\nthree learning-driven tools to learn in concert within one pipeline. We use\nhigh-resolution Lattice Boltzmann snapshots of fluid velocity and particle\nstresses in a periodic box as input to the learning pipeline. the second step\ntakes the morphology and positions orientations of particles to predict\npairwise interaction forces between them with a E(2)-equivariant graph neural\nnetwork that necessarily respect flat symmetries. Then, a physics-informed\nneural network further updates these local estimates by summing over them with\na stress data using Fourier feature mappings and residual blocks that is\nadditionally regularized with a topological term (introduced by persistent\nhomology) to penalize unrealistically tangled or spurious connections. In\nconcert, these stages deliver an holistic highly-data driven full force network\nprediction empathizing on the physical underpinnings together with emerging\nmulti-scale structure typical for active matter.", "published": "2025-09-08 11:39:42", "link": "http://arxiv.org/abs/2509.06574v1", "categories": ["cond-mat.soft", "cs.LG"], "primary_category": "cond-mat.soft"}
{"title": "Robustness and accuracy of mean opinion scores with hard and soft outlier detection", "abstract": "In subjective assessment of image and video quality, observers rate or\ncompare selected stimuli. Before calculating the mean opinion scores (MOS) for\nthese stimuli from the ratings, it is recommended to identify and deal with\noutliers that may have given unreliable ratings. Several methods are available\nfor this purpose, some of which have been standardized. These methods are\ntypically based on statistics and sometimes tested by introducing synthetic\nratings from artificial outliers, such as random clickers. However, a reliable\nand comprehensive approach is lacking for comparative performance analysis of\noutlier detection methods. To fill this gap, this work proposes and applies an\nempirical worst-case analysis as a general solution. Our method involves\nevolutionary optimization of an adversarial black-box attack on outlier\ndetection algorithms, where the adversary maximizes the distortion of scale\nvalues with respect to ground truth. We apply our analysis to several hard and\nsoft outlier detection methods for absolute category ratings and show their\ndiffering performance in this stress test. In addition, we propose two new\noutlier detection methods with low complexity and excellent worst-case\nperformance. Software for adversarial attacks and data analysis is available.", "published": "2025-09-08 11:09:14", "link": "http://arxiv.org/abs/2509.06554v1", "categories": ["eess.IV", "cs.LG", "cs.MM"], "primary_category": "eess.IV"}
{"title": "Predicting Fetal Outcomes from Cardiotocography Signals Using a Supervised Variational Autoencoder", "abstract": "Objective: To develop and interpret a supervised variational autoencoder\n(VAE) model for classifying cardiotocography (CTG) signals based on pregnancy\noutcomes, addressing interpretability limits of current deep learning\napproaches. Methods: The OxMat CTG dataset was used to train a VAE on\nfive-minute fetal heart rate (FHR) segments, labeled with postnatal outcomes.\nThe model was optimised for signal reconstruction and outcome prediction,\nincorporating Kullback-Leibler divergence and total correlation (TC)\nconstraints to structure the latent space. Performance was evaluated using area\nunder the receiver operating characteristic curve (AUROC) and mean squared\nerror (MSE). Interpretability was assessed using coefficient of determination,\nlatent traversals and unsupervised component analyses. Results: The model\nachieved an AUROC of 0.752 at the segment level and 0.779 at the CTG level,\nwhere predicted scores were aggregated. Relaxing TC constraints improved both\nreconstruction and classification. Latent analysis showed that baseline-related\nfeatures (e.g., FHR baseline, baseline shift) were well represented and aligned\nwith model scores, while metrics like short- and long-term variability were\nless strongly encoded. Traversals revealed clear signal changes for baseline\nfeatures, while other properties were entangled or subtle. Unsupervised\ndecompositions corroborated these patterns. Findings: This work demonstrates\nthat supervised VAEs can achieve competitive fetal outcome prediction while\npartially encoding clinically meaningful CTG features. The irregular,\nmulti-timescale nature of FHR signals poses challenges for disentangling\nphysiological components, distinguishing CTG from more periodic signals such as\nECG. Although full interpretability was not achieved, the model supports\nclinically useful outcome prediction and provides a basis for future\ninterpretable, generative models.", "published": "2025-09-08 10:54:04", "link": "http://arxiv.org/abs/2509.06540v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Lane Change Intention Prediction of two distinct Populations using a Transformer", "abstract": "As a result of the growing importance of lane change intention prediction for\na safe and efficient driving experience in complex driving scenarios,\nresearchers have in recent years started to train novel machine learning\nalgorithms on available datasets with promising results. A shortcoming of this\nrecent research effort, though, is that the vast majority of the proposed\nalgorithms are trained on a single datasets. In doing so, researchers failed to\ntest if their algorithm would be as effective if tested on a different dataset\nand, by extension, on a different population with respect to the one on which\nthey were trained. In this article we test a transformer designed for lane\nchange intention prediction on two datasets collected by LevelX in Germany and\nHong Kong. We found that the transformer's accuracy plummeted when tested on a\npopulation different to the one it was trained on with accuracy values as low\nas 39.43%, but that when trained on both populations simultaneously it could\nachieve an accuracy as high as 86.71%. - This work has been submitted to the\nIEEE for possible publication. Copyright may be transferred without notice,\nafter which this version may no longer be accessible.", "published": "2025-09-08 10:35:26", "link": "http://arxiv.org/abs/2509.06529v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A machine-learned expression for the excess Gibbs energy", "abstract": "The excess Gibbs energy plays a central role in chemical engineering and\nchemistry, providing a basis for modeling the thermodynamic properties of\nliquid mixtures. Predicting the excess Gibbs energy of multi-component mixtures\nsolely from the molecular structures of their components is a long-standing\nchallenge. In this work, we address this challenge by integrating physical laws\nas hard constraints within a flexible neural network. The resulting model,\nHANNA, was trained end-to-end on an extensive experimental dataset for binary\nmixtures from the Dortmund Data Bank, guaranteeing thermodynamically consistent\npredictions. A novel surrogate solver developed in this work enabled the\ninclusion of liquid-liquid equilibrium data in the training process.\nFurthermore, a geometric projection method was applied to enable robust\nextrapolations to multi-component mixtures, without requiring additional\nparameters. We demonstrate that HANNA delivers excellent predictions, clearly\noutperforming state-of-the-art benchmark methods in accuracy and scope. The\ntrained model and corresponding code are openly available, and an interactive\ninterface is provided on our website, MLPROP.", "published": "2025-09-08 09:47:03", "link": "http://arxiv.org/abs/2509.06484v1", "categories": ["cs.LG", "cs.CE"], "primary_category": "cs.LG"}
{"title": "CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction", "abstract": "Antibody binding site prediction plays a pivotal role in computational\nimmunology and therapeutic antibody design. Existing sequence or structure\nmethods rely on single-view features and fail to identify antibody-specific\nbinding sites on the antigens-a dual limitation in representation and\nprediction. In this paper, we propose CAME-AB, a novel Cross-modality Attention\nframework with a Mixture-of-Experts (MoE) backbone for robust antibody binding\nsite prediction. CAME-AB integrates five biologically grounded modalities,\nincluding raw amino acid encodings, BLOSUM substitution profiles, pretrained\nlanguage model embeddings, structure-aware features, and GCN-refined\nbiochemical graphs-into a unified multimodal representation. To enhance\nadaptive cross-modal reasoning, we propose an adaptive modality fusion module\nthat learns to dynamically weight each modality based on its global relevance\nand input-specific contribution. A Transformer encoder combined with an MoE\nmodule further promotes feature specialization and capacity expansion. We\nadditionally incorporate a supervised contrastive learning objective to\nexplicitly shape the latent space geometry, encouraging intra-class compactness\nand inter-class separability. To improve optimization stability and\ngeneralization, we apply stochastic weight averaging during training. Extensive\nexperiments on benchmark antibody-antigen datasets demonstrate that CAME-AB\nconsistently outperforms strong baselines on multiple metrics, including\nPrecision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies further\nvalidate the effectiveness of each architectural component and the benefit of\nmultimodal feature integration. The model implementation details and the codes\nare available on https://anonymous.4open.science/r/CAME-AB-C525", "published": "2025-09-08 09:24:09", "link": "http://arxiv.org/abs/2509.06465v1", "categories": ["cs.LG", "cs.CE", "q-bio.BM"], "primary_category": "cs.LG"}
{"title": "NeuroDeX: Unlocking Diverse Support in Decompiling Deep Neural Network Executables", "abstract": "On-device deep learning models have extensive real world demands. Deep\nlearning compilers efficiently compile models into executables for deployment\non edge devices, but these executables may face the threat of reverse\nengineering. Previous studies have attempted to decompile DNN executables, but\nthey face challenges in handling compilation optimizations and analyzing\nquantized compiled models. In this paper, we present NeuroDeX to unlock diverse\nsupport in decompiling DNN executables. NeuroDeX leverages the semantic\nunderstanding capabilities of LLMs along with dynamic analysis to accurately\nand efficiently perform operator type recognition, operator attribute recovery\nand model reconstruction. NeuroDeX can recover DNN executables into high-level\nmodels towards compilation optimizations, different architectures and quantized\ncompiled models. We conduct experiments on 96 DNN executables across 12 common\nDNN models. Extensive experimental results demonstrate that NeuroDeX can\ndecompile non-quantized executables into nearly identical high-level models.\nNeuroDeX can recover functionally similar high-level models for quantized\nexecutables, achieving an average top-1 accuracy of 72%. NeuroDeX offers a more\ncomprehensive and effective solution compared to previous DNN executables\ndecompilers.", "published": "2025-09-08 07:47:58", "link": "http://arxiv.org/abs/2509.06402v1", "categories": ["cs.LG", "cs.CR"], "primary_category": "cs.LG"}
{"title": "Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints", "abstract": "Meeting minimum data rate constraints is a significant challenge in wireless\ncommunication systems, particularly as network complexity grows. Traditional\ndeep learning approaches often address these constraints by incorporating\npenalty terms into the loss function and tuning hyperparameters empirically.\nHowever, this heuristic treatment offers no theoretical convergence guarantees\nand frequently fails to satisfy QoS requirements in practical scenarios.\nBuilding upon the structure of the WMMSE algorithm, we first extend it to a\nmulti-channel setting with QoS constraints, resulting in the enhanced WMMSE\n(eWMMSE) algorithm, which is provably convergent to a locally optimal solution\nwhen the problem is feasible. To further reduce computational complexity and\nimprove scalability, we develop a GNN-based algorithm, JCPGNN-M, capable of\nsupporting simultaneous multi-channel allocation per user. To overcome the\nlimitations of traditional deep learning methods, we propose a principled\nframework that integrates GNN with a Lagrangian-based primal-dual optimization\nmethod. By training the GNN within the Lagrangian framework, we ensure\nsatisfaction of QoS constraints and convergence to a stationary point.\nExtensive simulations demonstrate that JCPGNN-M matches the performance of\neWMMSE while offering significant gains in inference speed, generalization to\nlarger networks, and robustness under imperfect channel state information. This\nwork presents a scalable and theoretically grounded solution for constrained\nresource allocation in future wireless networks.", "published": "2025-09-08 07:28:10", "link": "http://arxiv.org/abs/2509.06395v1", "categories": ["cs.LG", "eess.SP"], "primary_category": "cs.LG"}
{"title": "Variational Garrote for Statistical Physics-based Sparse and Robust Variable Selection", "abstract": "Selecting key variables from high-dimensional data is increasingly important\nin the era of big data. Sparse regression serves as a powerful tool for this\npurpose by promoting model simplicity and explainability. In this work, we\nrevisit a valuable yet underutilized method, the statistical physics-based\nVariational Garrote (VG), which introduces explicit feature selection spin\nvariables and leverages variational inference to derive a tractable loss\nfunction. We enhance VG by incorporating modern automatic differentiation\ntechniques, enabling scalable and efficient optimization. We evaluate VG on\nboth fully controllable synthetic datasets and complex real-world datasets. Our\nresults demonstrate that VG performs especially well in highly sparse regimes,\noffering more consistent and robust variable selection than Ridge and LASSO\nregression across varying levels of sparsity. We also uncover a sharp\ntransition: as superfluous variables are admitted, generalization degrades\nabruptly and the uncertainty of the selection variables increases. This\ntransition point provides a practical signal for estimating the correct number\nof relevant variables, an insight we successfully apply to identify key\npredictors in real-world data. We expect that VG offers strong potential for\nsparse modeling across a wide range of applications, including compressed\nsensing and model pruning in machine learning.", "published": "2025-09-08 07:06:10", "link": "http://arxiv.org/abs/2509.06383v1", "categories": ["cs.LG", "physics.data-an"], "primary_category": "cs.LG"}
{"title": "Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment", "abstract": "Due to hardware and software improvements, an increasing number of AI models\nare deployed on-device. This shift enhances privacy and reduces latency, but\nalso introduces security risks distinct from traditional software. In this\narticle, we examine these risks through the real-world case study of\nSafetyCore, an Android system service incorporating sensitive image content\ndetection. We demonstrate how the on-device AI model can be extracted and\nmanipulated to bypass detection, effectively rendering the protection\nineffective. Our analysis exposes vulnerabilities of on-device AI models and\nprovides a practical demonstration of how adversaries can exploit them.", "published": "2025-09-08 06:53:13", "link": "http://arxiv.org/abs/2509.06371v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Embedding Poisoning: Bypassing Safety Alignment via Embedding Semantic Shift", "abstract": "The widespread distribution of Large Language Models (LLMs) through public\nplatforms like Hugging Face introduces significant security challenges. While\nthese platforms perform basic security scans, they often fail to detect subtle\nmanipulations within the embedding layer. This work identifies a novel class of\ndeployment phase attacks that exploit this vulnerability by injecting\nimperceptible perturbations directly into the embedding layer outputs without\nmodifying model weights or input text. These perturbations, though\nstatistically benign, systematically bypass safety alignment mechanisms and\ninduce harmful behaviors during inference. We propose Search based Embedding\nPoisoning(SEP), a practical, model agnostic framework that introduces carefully\noptimized perturbations into embeddings associated with high risk tokens. SEP\nleverages a predictable linear transition in model responses, from refusal to\nharmful output to semantic deviation to identify a narrow perturbation window\nthat evades alignment safeguards. Evaluated across six aligned LLMs, SEP\nachieves an average attack success rate of 96.43% while preserving benign task\nperformance and evading conventional detection mechanisms. Our findings reveal\na critical oversight in deployment security and emphasize the urgent need for\nembedding level integrity checks in future LLM defense strategies.", "published": "2025-09-08 05:00:58", "link": "http://arxiv.org/abs/2509.06338v1", "categories": ["cs.CR", "cs.LG"], "primary_category": "cs.CR"}
{"title": "Exploring approaches to computational representation and classification of user-generated meal logs", "abstract": "This study examined the use of machine learning and domain specific\nenrichment on patient generated health data, in the form of free text meal\nlogs, to classify meals on alignment with different nutritional goals. We used\na dataset of over 3000 meal records collected by 114 individuals from a\ndiverse, low income community in a major US city using a mobile app. Registered\ndietitians provided expert judgement for meal to goal alignment, used as gold\nstandard for evaluation. Using text embeddings, including TFIDF and BERT, and\ndomain specific enrichment information, including ontologies, ingredient\nparsers, and macronutrient contents as inputs, we evaluated the performance of\nlogistic regression and multilayer perceptron classifiers using accuracy,\nprecision, recall, and F1 score against the gold standard and self assessment.\nEven without enrichment, ML outperformed self assessments of individuals who\nlogged meals, and the best performing combination of ML classifier with\nenrichment achieved even higher accuracies. In general, ML classifiers with\nenrichment of Parsed Ingredients, Food Entities, and Macronutrients information\nperformed well across multiple nutritional goals, but there was variability in\nthe impact of enrichment and classification algorithm on accuracy of\nclassification for different nutritional goals. In conclusion, ML can utilize\nunstructured free text meal logs and reliably classify whether meals align with\nspecific nutritional goals, exceeding self assessments, especially when\nincorporating nutrition domain knowledge. Our findings highlight the potential\nof ML analysis of patient generated health data to support patient centered\nnutrition guidance in precision healthcare.", "published": "2025-09-08 04:23:48", "link": "http://arxiv.org/abs/2509.06330v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics", "abstract": "Large language models (LLMs) have demonstrated emergent in-context learning\n(ICL) capabilities across a range of tasks, including zero-shot time-series\nforecasting. We show that text-trained foundation models can accurately\nextrapolate spatiotemporal dynamics from discretized partial differential\nequation (PDE) solutions without fine-tuning or natural language prompting.\nPredictive accuracy improves with longer temporal contexts but degrades at\nfiner spatial discretizations. In multi-step rollouts, where the model\nrecursively predicts future spatial states over multiple time steps, errors\ngrow algebraically with the time horizon, reminiscent of global error\naccumulation in classical finite-difference solvers. We interpret these trends\nas in-context neural scaling laws, where prediction quality varies predictably\nwith both context length and output length. To better understand how LLMs are\nable to internally process PDE solutions so as to accurately roll them out, we\nanalyze token-level output distributions and uncover a consistent ICL\nprogression: beginning with syntactic pattern imitation, transitioning through\nan exploratory high-entropy phase, and culminating in confident, numerically\ngrounded predictions.", "published": "2025-09-08 04:08:50", "link": "http://arxiv.org/abs/2509.06322v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent Recognition", "abstract": "The rapid development of the low-altitude economy emphasizes the critical\nneed for effective perception and intent recognition of non-cooperative\nunmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities\nof multimodal large language models (MLLMs) present a promising approach in\nsuch tasks. In this paper, we focus on the combination of UAV intent\nrecognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV\nintent recognition architecture, where the multimodal perception system is\nutilized to obtain real-time payload and motion information of UAVs, generating\nstructured input information, and MLLM outputs intent recognition results by\nincorporating environmental information, prior knowledge, and tactical\npreferences. Subsequently, we review the related work and demonstrate their\nprogress within the proposed architecture. Then, a use case for low-altitude\nconfrontation is conducted to demonstrate the feasibility of our architecture\nand offer valuable insights for practical system design. Finally, the future\nchallenges are discussed, followed by corresponding strategic recommendations\nfor further applications.", "published": "2025-09-08 03:34:56", "link": "http://arxiv.org/abs/2509.06312v1", "categories": ["eess.SY", "cs.LG", "cs.SY", "68T07, 68T45, 93C85, 94A12", "I.2.10; I.2.6; I.2.9; C.2.1"], "primary_category": "eess.SY"}
{"title": "WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting", "abstract": "High-quality wind power forecasting is crucial for the operation of modern\npower grids. However, prevailing data-driven paradigms either train a\nsite-specific model which cannot generalize to other locations or rely on\nfine-tuning of general-purpose time series foundation models which are\ndifficult to incorporate domain-specific data in the energy sector. This paper\nintroduces WindFM, a lightweight and generative Foundation Model designed\nspecifically for probabilistic wind power forecasting. WindFM employs a\ndiscretize-and-generate framework. A specialized time-series tokenizer first\nconverts continuous multivariate observations into discrete, hierarchical\ntokens. Subsequently, a decoder-only Transformer learns a universal\nrepresentation of wind generation dynamics by autoregressively pre-training on\nthese token sequences. Using the comprehensive WIND Toolkit dataset comprising\napproximately 150 billion time steps from more than 126,000 sites, WindFM\ndevelops a foundational understanding of the complex interplay between\natmospheric conditions and power output. Extensive experiments demonstrate that\nour compact 8.1M parameter model achieves state-of-the-art zero-shot\nperformance on both deterministic and probabilistic tasks, outperforming\nspecialized models and larger foundation models without any fine-tuning. In\nparticular, WindFM exhibits strong adaptiveness under out-of-distribution data\nfrom a different continent, demonstrating the robustness and transferability of\nits learned representations. Our pre-trained model is publicly available at\nhttps://github.com/shiyu-coder/WindFM.", "published": "2025-09-08 03:26:18", "link": "http://arxiv.org/abs/2509.06311v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Minimax optimal transfer learning for high-dimensional additive regression", "abstract": "This paper studies high-dimensional additive regression under the transfer\nlearning framework, where one observes samples from a target population\ntogether with auxiliary samples from different but potentially related\nregression models. We first introduce a target-only estimation procedure based\non the smooth backfitting estimator with local linear smoothing. In contrast to\nprevious work, we establish general error bounds under sub-Weibull($\\alpha$)\nnoise, thereby accommodating heavy-tailed error distributions. In the\nsub-exponential case ($\\alpha=1$), we show that the estimator attains the\nminimax lower bound under regularity conditions, which requires a substantial\ndeparture from existing proof strategies. We then develop a novel two-stage\nestimation method within a transfer learning framework, and provide theoretical\nguarantees at both the population and empirical levels. Error bounds are\nderived for each stage under general tail conditions, and we further\ndemonstrate that the minimax optimal rate is achieved when the auxiliary and\ntarget distributions are sufficiently close. All theoretical results are\nsupported by simulation studies and real data analysis.", "published": "2025-09-08 03:16:05", "link": "http://arxiv.org/abs/2509.06308v1", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "stat.ML"}
{"title": "MOSAIC: Minimax-Optimal Sparsity-Adaptive Inference for Change Points in Dynamic Networks", "abstract": "We propose a new inference framework, named MOSAIC, for change-point\ndetection in dynamic networks with the simultaneous low-rank and sparse-change\nstructure. We establish the minimax rate of detection boundary, which relies on\nthe sparsity of changes. We then develop an eigen-decomposition-based test with\nscreened signals that approaches the minimax rate in theory, with only a minor\nlogarithmic loss. For practical implementation of MOSAIC, we adjust the\ntheoretical test by a novel residual-based technique, resulting in a pivotal\nstatistic that converges to a standard normal distribution via the martingale\ncentral limit theorem under the null hypothesis and achieves full power under\nthe alternative hypothesis. We also analyze the minimax rate of testing\nboundary for dynamic networks without the low-rank structure, which almost\naligns with the results in high-dimensional mean-vector change-point inference.\nWe showcase the effectiveness of MOSAIC and verify our theoretical results with\nseveral simulation examples and a real data application.", "published": "2025-09-08 03:09:50", "link": "http://arxiv.org/abs/2509.06303v1", "categories": ["stat.ML", "cs.LG", "stat.ME"], "primary_category": "stat.ML"}
{"title": "LoaQ: Layer-wise Output Approximation Quantization", "abstract": "A natural and intuitive idea in model quantization is to approximate each\ncomponent's quantized output to match its original. Layer-wise post-training\nquantization (PTQ), though based on this idea, adopts a strictly local view and\ncan achieve, at best, only activation-aware approximations of weights. As a\nresult, it often leads to insufficient approximations and practical deviations\nfrom this guiding intuition. Recent work has achieved a more accurate\napproximation of linear-layer outputs within the framework of layer-wise PTQ,\nbut such refinements remain inadequate for achieving alignment with the full\nmodel output. Based on a deeper understanding of the structural characteristics\nof mainstream LLMs, we propose $LoaQ$, an output-approximation method for\nlayer-wise PTQ that explicitly targets output-level consistency. It better\naligns with this intuition and can feature a simple closed-form solution,\nmaking it orthogonal to existing techniques and readily integrable into\nexisting quantization pipelines. Experiments on the LLaMA and Qwen model\nfamilies demonstrate that LoaQ performs effectively in both weight-only and\nweight-activation joint quantization. By integrating seamlessly with existing\nquantization strategies, it further enhances overall quantization quality and\nshows strong potential to advance the frontier of post-training quantization.", "published": "2025-09-08 02:50:11", "link": "http://arxiv.org/abs/2509.06297v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults", "abstract": "Silent Data Errors (SDEs) from time-zero defects and aging degrade\nsafety-critical systems. Functional testing detects SDE-related faults but is\nexpensive to simulate. We present a unified spatio-temporal graph convolutional\nnetwork (ST-GCN) for fast, accurate prediction of long-cycle fault impact\nprobabilities (FIPs) in large sequential circuits, supporting quantitative risk\nassessment. Gate-level netlists are modeled as spatio-temporal graphs to\ncapture topology and signal timing; dedicated spatial and temporal encoders\npredict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the method\nreduces simulation time by more than 10x while maintaining high accuracy (mean\nabsolute error 0.024 for 5-cycle predictions). The framework accepts features\nfrom testability metrics or fault simulation, allowing efficiency-accuracy\ntrade-offs. A test-point selection study shows that choosing observation points\nby predicted FIPs improves detection of long-cycle, hard-to-detect faults. The\napproach scales to SoC-level test strategy optimization and fits downstream\nelectronic design automation flows.", "published": "2025-09-08 02:23:51", "link": "http://arxiv.org/abs/2509.06289v1", "categories": ["cs.LG", "cs.AR", "cs.ET", "B.7.3"], "primary_category": "cs.LG"}
{"title": "RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations", "abstract": "Personalization is a core capability across consumer technologies, streaming,\nshopping, wearables, and voice, yet it remains challenged by sparse\ninteractions, fast content churn, and heterogeneous textual signals. We present\nRecMind, an LLM-enhanced graph recommender that treats the language model as a\npreference prior rather than a monolithic ranker. A frozen LLM equipped with\nlightweight adapters produces text-conditioned user/item embeddings from\ntitles, attributes, and reviews; a LightGCN backbone learns collaborative\nembeddings from the user-item graph. We align the two views with a symmetric\ncontrastive objective and fuse them via intra-layer gating, allowing language\nto dominate in cold/long-tail regimes and graph structure to stabilize rankings\nelsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on\nall eight reported metrics, with relative improvements up to +4.53\\%\n(Recall@40) and +4.01\\% (NDCG@40) over strong baselines. Ablations confirm both\nthe necessity of cross-view alignment and the advantage of gating over late\nfusion and LLM-only variants.", "published": "2025-09-08 02:15:55", "link": "http://arxiv.org/abs/2509.06286v1", "categories": ["cs.LG"], "primary_category": "cs.LG"}
{"title": "Nanobot Algorithms for Treatment of Diffuse Cancer", "abstract": "Motile nanosized particles, or \"nanobots\", promise more effective and less\ntoxic targeted drug delivery because of their unique scale and precision. We\nconsider the case in which the cancer is \"diffuse\", dispersed such that there\nare multiple distinct cancer sites. We investigate the problem of a swarm of\nnanobots locating these sites and treating them by dropping drug payloads at\nthe sites. To improve the success of the treatment, the drug payloads must be\nallocated between sites according to their \"demands\"; this requires extra\nnanobot coordination. We present a mathematical model of the behavior of the\nnanobot agents and of their colloidal environment. This includes a movement\nmodel for agents based upon experimental findings from actual nanoparticles in\nwhich bots noisily ascend and descend chemical gradients. We present three\nalgorithms: The first algorithm, called KM, is the most representative of\nreality, with agents simply following naturally existing chemical signals that\nsurround each cancer site. The second algorithm, KMA, includes an additional\nchemical payload which amplifies the existing natural signals. The third\nalgorithm, KMAR, includes another additional chemical payload which counteracts\nthe other signals, instead inducing negative chemotaxis in agents such that\nthey are repelled from sites that are already sufficiently treated. We present\nsimulation results for all algorithms across different types of cancer\narrangements. For KM, we show that the treatment is generally successful unless\nthe natural chemical signals are weak, in which case the treatment progresses\ntoo slowly. For KMA, we demonstrate a significant improvement in treatment\nspeed but a drop in eventual success, except for concentrated cancer patterns.\nFor KMAR, our results show great performance across all types of cancer\npatterns, demonstrating robustness and adaptability.", "published": "2025-09-08 17:11:59", "link": "http://arxiv.org/abs/2509.06893v1", "categories": ["cs.MA", "cs.RO", "q-bio.QM"], "primary_category": "cs.MA"}
{"title": "MAPF-HD: Multi-Agent Path Finding in High-Density Environments", "abstract": "Multi-agent path finding (MAPF) involves planning efficient paths for\nmultiple agents to move simultaneously while avoiding collisions. In typical\nwarehouse environments, agents are often sparsely distributed along aisles.\nHowever, increasing the agent density can improve space efficiency. When the\nagent density is high, we must optimize the paths not only for goal-assigned\nagents but also for those obstructing them. This study proposes a novel MAPF\nframework for high-density environments (MAPF-HD). Several studies have\nexplored MAPF in similar settings using integer linear programming (ILP).\nHowever, ILP-based methods require substantial computation time to optimize all\nagent paths simultaneously. Even in small grid-based environments with fewer\nthan $100$ cells, these computations can incur tens to hundreds of seconds.\nThese high computational costs render these methods impractical for large-scale\napplications such as automated warehouses and valet parking. To address these\nlimitations, we introduce the phased null-agent swapping (PHANS) method. PHANS\nemploys a heuristic approach to incrementally swap positions between agents and\nempty vertices. This method solves the MAPF-HD problem within seconds to tens\nof seconds, even in large environments containing more than $700$ cells. The\nproposed method can potentially improve efficiency in various real-world\napplications such as warehouse logistics, traffic management, or crowd control.\nCode is available at https://github.com/ToyotaCRDL/MAPF-in-High-Density-Envs.", "published": "2025-09-08 06:59:46", "link": "http://arxiv.org/abs/2509.06374v1", "categories": ["cs.MA", "cs.RO"], "primary_category": "cs.MA"}
{"title": "Chebyshev smoothing with adaptive block-FSAI preconditioners for the multilevel solution of higher-order problems", "abstract": "In this paper, we assess the performance of adaptive and nested factorized\nsparse approximate inverses as smoothers in multilevel V-cycles, when smoothing\nis performed following the Chebyshev iteration of the fourth kind. For our test\nproblems, we rely on the partition of unity method to discretize the biharmonic\nand triharmonic equations in a multilevel manner. Inspired by existing\nalgorithms, we introduce a new adaptive algorithm for the construction of\nsparse approximate inverses, based on the block structure of matrices arising\nin the partition of unity method. Additionally, we also present a new (and\narguably simpler) formulation of the Chebyshev iteration of the fourth kind.", "published": "2025-09-08 14:36:39", "link": "http://arxiv.org/abs/2509.06744v1", "categories": ["math.NA", "cs.NA", "65F08, 65F10, 65F50, 65N22, 65N55"], "primary_category": "math.NA"}
{"title": "A Parallel Solver with Multiphysics Finite Element Method for Poroelasticity Coupled with Elasticity Model", "abstract": "In this paper, we propose a parallel solver for solving the quasi-static\nlinear poroelasticity coupled with linear elasticity model in the Lagrange\nmultiplier framework. Firstly, we reformulate the model into a coupling of the\nnearly incompressible elasticity and an unsteady affection-diffusion equations\nby setting new variable ``elastic pressure\" and ``volumetric fluid content\".\nAnd we introduce a Lagrange multiplier to guarantee the normal stress\ncontinuity on the interface. Then, we give the variational formulations in each\nsubdomain and choose the $\\boldsymbol{P}_k$-$P_1$-$P_1$ mixed finite element\ntuple for poroelasticity subdomain, and $\\boldsymbol{P}_k$-$P_1$ finite element\npair ($k=1,2$) for elasticity subdomain and the backward Euler scheme for time.\nAlso, we propose a parallel solver for solving the fully discrete scheme at\neach time step-- the FETI method with a classical FETI preconditioner for\nsolving the Lagrange multiplier and calculating the subproblems in each\nsubdomain in parallel. And we show several numerical tests to validate the\ncomputational efficiency and the convergence error order, and we consider\nBarry-Mercer's model as the benchmark test to show that there no oscillation in\nthe computed pressure. Finally, we draw conclusions to summarize the main\nresults of this paper.", "published": "2025-09-08 13:31:40", "link": "http://arxiv.org/abs/2509.06673v1", "categories": ["math.NA", "cs.CE", "cs.NA", "65N30"], "primary_category": "math.NA"}
{"title": "Quadrature rules with few nodes supported on algebraic curves", "abstract": "We investigate quadrature rules for measures supported on real algebraic and\nrational curves, focusing on the {odd-degree} case \\(2s-1\\). Adopting an\noptimization viewpoint, we minimize suitable penalty functions over the space\nof quadrature rules of strength \\(2s-1\\), so that optimal solutions yield rules\nwith the minimal number of nodes. For plane algebraic curves of degree \\(d\\),\nwe derive explicit node bounds depending on \\(d\\) and the number of places at\ninfinity, improving results of Riener--Schweighofer, and Zalar. For rational\ncurves in arbitrary dimension of degree \\(d\\), we further refine these bounds\nusing the geometry of the parametrization and recover the classical Gaussian\nquadrature bound when \\(d=1\\). Our results reveal a direct link between the\nalgebraic complexity of the supporting curve and the minimal size of quadrature\nformulas, providing a unified framework that connects real algebraic geometry,\npolynomial optimization, and moment theory.", "published": "2025-09-08 12:59:26", "link": "http://arxiv.org/abs/2509.06643v1", "categories": ["math.AG", "cs.NA", "math.NA", "math.OC", "65D32, 14H50, 14P05"], "primary_category": "math.AG"}
{"title": "Fisher entropic Fokker-Planck model of monatomic rarefied gases", "abstract": "Particle-based stochastic approximations of the Boltzmann equation are\npopular tools for simulations of non-equilibrium gas flows, for which the\nNavier-Stokes-Fourier equations fail to provide accurate description. However,\nthese numerical methods are computationally demanding, especially in the\nnear-continuum regime, where the collisions become overwhelming. On the other\nhand, the Fokker-Planck kinetic models offer an efficient alternative, as the\nbinary collisions are described by a diffusive process. Despite the intuitive\nadvantage, rigorous and efficient Fokker-Planck approximations of the Boltzmann\nequation remain an open problem. On one hand, the moment projection of the\nFokker-Planck operator should be consistent with that of the Boltzmann\noperator. On the other hand, the Fokker-Planck model should be constructed in\nsuch a way that the H-theorem is satisfied. The central aim of this study is\nfulfilling these two categorically different constraints, i.e. moment matching\nand entropy dissipation, within a flexible and tractable Fokker-Planck\nframework. To this end, we introduce a Fisher information-based entropic\nconstraint and demonstrate that, with a suitable polynomial expansion of the\ndrift term, it is possible to simultaneously achieve weak moment matching while\nhonouring the H-theorem. We support our theoretical result by numerical\nexperiments on the shock problem, validating our Fisher Entropic Fokker-Planck\nframework.", "published": "2025-09-08 12:27:09", "link": "http://arxiv.org/abs/2509.06610v1", "categories": ["math.NA", "cs.NA", "82C40, 60J60, 65C20"], "primary_category": "math.NA"}
{"title": "A novel time integration scheme for linear parabolic PDEs", "abstract": "This paper presents a class of Crank-Nicolson (CN) type schemes enhanced by\nradial basis function (RBF) interpolation for the time integration of linear\nparabolic partial differential equations (PDEs). The resulting RBF-CN schemes\npreserve the structural simplicity of the classical CN method while enabling\nhigher-order temporal accuracy through the optimization of the shape parameter.\nConsistency analysis shows that the schemes are always at least second-order\naccurate and a simple choice of the shape parameter increases the accuracy by\ntwo orders over the standard CN scheme. Further optimization can reduce the\nnext leading error term to attain even higher-order accuracy. A von Neumann\nstability analysis confirms that the stability conditions are essentially the\nsame as those of the standard CN scheme. Several numerical experiments in 1D\nare carried out to verify the theoretical results under different boundary\nconditions. Particular focus is given to the startup stage, where several\nstrategies for computing the initial steps are examined, and Gauss-Legendre\nimplicit Runge-Kutta (IRK) methods are found to be the most effective. The\nexperiments further demonstrate that, with optimal initialization, the proposed\nschemes deliver accuracy comparable to implicit Runge-Kutta methods while\nachieving nearly two orders of magnitude reduction in computational cost.", "published": "2025-09-08 11:14:42", "link": "http://arxiv.org/abs/2509.06556v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Fractal Based Rational Cubic Trigonometric Zipper Interpolation with Positivity Constraints", "abstract": "We propose a novel fractal based interpolation scheme termed Rational Cubic\nTrigonometric Zipper Fractal Interpolation Functions (RCTZFIFs) designed to\nmodel and preserve the inherent geometric property, positivity, in given\ndatasets. The method employs a combination of rational cubic trigonometric\nfunctions within a zipper fractal framework, offering enhanced flexibility\nthrough shape parameters and scaling factors. Rigorous error analysis is\npresented to establish the convergence of the proposed zipper fractal\ninterpolants to the underlying classical fractal functions, and subsequently,\nto the data-generating function. We derive necessary constraints on the scaling\nfactors and shape parameters to ensure positivity preservation. By carefully\nselecting the signature, shape parameters, and scaling factors within these\nbounds, we construct a class of RCTZFIFs that effectively preserve the positive\nnature of the data, as compared to a reference interpolant that may violate\nthis property. Numerical experiments and visualisations demonstrate the\nefficacy and robustness of our approach in preserving positivity while offering\nfractal flexibility.", "published": "2025-09-08 10:37:44", "link": "http://arxiv.org/abs/2509.06532v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Solving PDEs on Surfaces of Pipe Geometries Using New Coordinate Transformations and High-order Compact Finite Differences", "abstract": "We introduce suitable coordinate systems for pipes and their variants that\nallow us to transform partial differential equations (PDEs) on the pipe\nsurfaces or in the solid pipes into computational domains with fixed\nlimits/ranges. Such a notion is reminiscent of the polar and cylindrical\ncoordinates for their advantageous geometries. The new curvilinear coordinates\nare non-orthogonal in two directions, so the Laplace--Beltrami operators\ninvolve mixed derivatives. To deal with the variable coefficients arising from\ncoordinate transformations and diverse surface geometries, we develop efficient\nfourth-order compact finite difference methods adaptable to various scenarios.\nWe then rigorously prove the convergence of the proposed method for some model\nproblem, and apply the solver to several other types of PDEs. We further\ndemonstrate the efficiency and accuracy of our approach with ample numerical\nresults. Here, we only consider the compact finite differences for the\ntransformed PDEs for simplicity, but one can employ the spectral-collocation\nmethods to efficiently handle such variable coefficient problems.", "published": "2025-09-08 10:12:18", "link": "http://arxiv.org/abs/2509.06507v1", "categories": ["math.NA", "cs.NA", "65N06, 65N12, 35J25, 65D25, 58J90"], "primary_category": "math.NA"}
{"title": "Sequential symmetric interior penalty discontinuous Galerkin method for fully coupled quasi-static thermo-poroelasticity problems", "abstract": "In this paper, we investigate a sequentially decoupled numerical method for\nsolving the fully coupled quasi-static thermo-poroelasticity problems with\nnonlinear convective transport. The symmetric interior penalty discontinuous\nGalerkin method is employed for spatial discretization and the backward Euler\nmethod for temporal discretization. Unlike other splitting algorithms, this\ntype of sequential method does not require any internal iterations and the\ncomputational efficiency is higher than that of the fully implicit nonlinear\nnumerical scheme. In the theoretical analysis, a cut-off operator is introduced\nto prove the existence and uniqueness of numerical solution and the stability\nanalysis of numerical scheme is conducted. Then, we derive the optimal\nconvergence order estimates in space and time. Finally, several numerical\nexamples are presented to illustrate the accuracy and efficiency of our\nproposed method.", "published": "2025-09-08 09:44:46", "link": "http://arxiv.org/abs/2509.06480v1", "categories": ["math.NA", "cs.NA", "65M15, 65M60, 76S99"], "primary_category": "math.NA"}
{"title": "Approximations of the mean curvature, and the Buet-Rumpf approximate mean curvature flow", "abstract": "The aim of this paper is to generalize the work of B. Buet and M. Rumpf on\nsome definition of the approximate mean curvature vector for varifolds, and its\nassociated mean curvature motions for points clouds. We propose a\ngeneralization of the definition of the approximate mean curvature vector in\ntwo terms: in terms of linear operators and in terms of regularity of the\nvarifold. We then extend the results to the approximate second fundamental\nform. Finally, we prove some additional comparison principles satisfied by the\nmotion of points cloud by mean curvature (in the discrete and the continuous\ncases).", "published": "2025-09-08 08:35:43", "link": "http://arxiv.org/abs/2509.06438v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "A Geometric Multigrid-Accelerated Compact Gas-Kinetic Scheme for Fast Convergence in High-Speed Flows on GPUs", "abstract": "Implicit methods and GPU parallelization are two distinct yet powerful\nstrategies for accelerating high-order CFD algorithms. However, few studies\nhave successfully integrated both approaches within high-speed flow solvers.\nThe core challenge lies in preserving the robustness of implicit algorithms in\nthe presence of strong discontinuities, while simultaneously enabling massive\nthread parallelism under the constraints of limited GPU memory. To address\nthis, we propose a GPU-optimized, geometric multigrid-accelerated, high-order\ncompact gas kinetic scheme (CGKS) that incorporates three key innovations:\n  (1) a multi-color lower-upper symmetric Gauss-Seidel scheme that eliminates\nthread conflicts and preserves memory efficiency, serving as an implicit\nsmoother on coarse grids; (2) a discontinuity-adaptive relaxation technique and\na multigrid prolongation process, based on a discontinuous feedback factor,\nwhich dynamically stabilize shock regions without compromising convergence in\nsmooth zones; and (3) a three-layer V-cycle geometric parallel multigrid\nstrategy specifically tailored for unstructured meshes. Extensive tests on\nmulti-dimensional subsonic to hypersonic flows demonstrate that our GPU-based\nhigh-performance solver achieves one to two orders of magnitude faster\nconvergence compared to previous explicit solvers. More importantly, it\npreserves the shock-capturing robustness of the explicit CGKS and exhibits\nstrong scalability on GPU architectures. This work presents a unified framework\nthat synergistically leverages implicit acceleration and GPU optimization for\nhigh-speed flow simulations, effectively overcoming traditional trade-offs\nbetween parallelism, memory constraints, and numerical stability in high-order\nmethods.", "published": "2025-09-08 05:40:57", "link": "http://arxiv.org/abs/2509.06347v1", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "primary_category": "math.NA"}
{"title": "A High-order Backpropagation Algorithm for Neural Stochastic Differential Equation Model", "abstract": "Neural stochastic differential equation model with a Brownian motion term can\ncapture epistemic uncertainty of deep neural network from the perspective of a\ndynamical system. The goal of this paper is to improve the convergence rate of\nthe sample-wise backpropagation algorithm in neural stochastic differential\nequation model which has been proposed in [Archibald et al., SIAM Journal on\nNumerical Analysis, 62 (2024), pp. 593-621]. It is necessary to emphasize that,\nimproving the convergence order of the algorithm consisting of forward backward\nstochastic differential equations remains challenging, due to the loss of\ninformation of Z term in backward equations under sample-wise approximation and\nthe limitations of the forward network form. In this paper, we develop a\nhigh-order backpropagation algorithm to improve the training accuracy. Under\nthe convexity assumption, the result indicates that the first-order convergence\nis achieved when the number of training steps is proportional to the cubic\nnumber of layers. Finally, numerical examples illustrate our theoretical\nresults.", "published": "2025-09-08 02:29:37", "link": "http://arxiv.org/abs/2509.06292v1", "categories": ["math.NA", "cs.NA"], "primary_category": "math.NA"}
{"title": "Optimal Exit Time for Liquidity Providers in Automated Market Makers", "abstract": "We study the problem of optimal liquidity withdrawal for a representative\nliquidity provider (LP) in an automated market maker (AMM). LPs earn fees from\ntrading activity but are exposed to impermanent loss (IL) due to price\nfluctuations. While existing work has focused on static provision and exogenous\nexit strategies, we characterise the optimal exit time as the solution to a\nstochastic control problem with an endogenous stopping time. Mathematically,\nthe LP's value function is shown to satisfy a Hamilton-Jacobi-Bellman\nquasi-variational inequality, for which we establish uniqueness in the\nviscosity sense. To solve the problem numerically, we develop two complementary\napproaches: a Euler scheme based on operator splitting and a Longstaff-Schwartz\nregression method. Calibrated simulations highlight how the LP's optimal exit\nstrategy depends on the oracle price volatility, fee levels, and the behaviour\nof arbitrageurs and noise traders. Our results show that while arbitrage\ngenerates both fees and IL, the LP's optimal decision balances these opposing\neffects based on the pool state variables and price misalignments. This work\ncontributes to a deeper understanding of dynamic liquidity provision in AMMs\nand provides insights into the sustainability of passive LP strategies under\ndifferent market regimes.", "published": "2025-09-08 10:15:23", "link": "http://arxiv.org/abs/2509.06510v1", "categories": ["q-fin.TR", "q-fin.MF"], "primary_category": "q-fin.TR"}
{"title": "The use of financial and sustainability ratios to map a sector. An approach using compositional data", "abstract": "Purpose: The article aims to visualise in a single graph fish and meat\nprocessing company groups in Spain with respect to long-term solvency, energy,\nwaste and water intensity and gender employment gap.\n  Design/methodology/approach: The selected financial, environmental and social\nindicators are ratios, which require specific statistical analysis methods to\nprevent severe skewness and outliers. We use the compositional data methodology\nand the principal-component analysis biplot.\n  Findings: Fish-processing companies have more homogeneous financial,\nenvironmental and social performance than their meat-processing counterparts.\nSpecific company groups in both sectors can be identified as poor performers in\nsome of the indicators. Firms with higher solvency tend to be less efficient in\nenergy and water use. Two clusters of company groups with similar performances\nare identified.\n  Research limitations/implications: As of now, few firms publish reports\naccording to the EU Corporate Sustainability Reporting Directive. In future\nresearch larger samples will be available.\n  Social Implications: Firm groups can visually see their areas of improvement\nin their financial, environmental and social performance compared to their\ncompetitors in the sector.\n  Originality/value: This is the first time in which visualization tools have\ncombined financial, environmental and social indicators. All individual firms\ncan be visually ordered along all indicators simultaneously.", "published": "2025-09-08 09:29:13", "link": "http://arxiv.org/abs/2509.06468v1", "categories": ["q-fin.ST", "stat.AP", "62H25, 62P05, 91B76"], "primary_category": "q-fin.ST"}
{"title": "Benchmarking Music Autotagging with MGPHot Expert Annotations vs. Generic Tag Datasets", "abstract": "Music autotagging aims to automatically assign descriptive tags, such as\ngenre, mood, or instrumentation, to audio recordings. Due to its challenges,\ndiversity of semantic descriptions, and practical value in various\napplications, it has become a common downstream task for evaluating the\nperformance of general-purpose music representations learned from audio data.\nWe introduce a new benchmarking dataset based on the recently published MGPHot\ndataset, which includes expert musicological annotations, allowing for\nadditional insights and comparisons with results obtained on common generic tag\ndatasets. While MGPHot annotations have been shown to be useful for\ncomputational musicology, the original dataset neither includes audio nor\nprovides evaluation setups for its use as a standardized autotagging benchmark.\nTo address this, we provide a curated set of YouTube URLs with retrievable\naudio, and propose a train/val/test split for standardized evaluation, and\nprecomputed representations for seven state-of-the-art models. Using these\nresources, we evaluated these models in MGPHot and standard reference tag\ndatasets, highlighting key differences between expert and generic tag\nannotations. Altogether, our contributions provide a more advanced benchmarking\nframework for future research in music understanding.", "published": "2025-09-08 17:47:56", "link": "http://arxiv.org/abs/2509.06936v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Continuous Audio Language Models", "abstract": "Audio Language Models (ALM) have emerged as the dominant paradigm for speech\nand music generation by representing audio as sequences of discrete tokens.\nYet, unlike text tokens, which are invertible, audio tokens are extracted from\nlossy codecs with a limited bitrate. As a consequence, increasing audio quality\nrequires generating more tokens, which imposes a trade-off between fidelity and\ncomputational cost. We address this issue by studying Continuous Audio Language\nModels (CALM). These models instantiate a large Transformer backbone that\nproduces a contextual embedding at every timestep. This sequential information\nthen conditions an MLP that generates the next continuous frame of an audio VAE\nthrough consistency modeling. By avoiding lossy compression, CALM achieves\nhigher quality at lower computational cost than their discrete counterpart.\nExperiments on speech and music demonstrate improved efficiency and fidelity\nover state-of-the-art discrete audio language models, facilitating lightweight,\nhigh-quality audio generation. Samples are available at\nhttps://continuous-audio-language-models.github.io", "published": "2025-09-08 17:38:13", "link": "http://arxiv.org/abs/2509.06926v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Speaker Privacy and Security in the Big Data Era: Protection and Defense against Deepfake", "abstract": "In the era of big data, remarkable advancements have been achieved in\npersonalized speech generation techniques that utilize speaker attributes,\nincluding voice and speaking style, to generate deepfake speech. This has also\namplified global security risks from deepfake speech misuse, resulting in\nconsiderable societal costs worldwide. To address the security threats posed by\ndeepfake speech, techniques have been developed focusing on both the protection\nof voice attributes and the defense against deepfake speech. Among them, the\nvoice anonymization technique has been developed to protect voice attributes\nfrom extraction for deepfake generation, while deepfake detection and\nwatermarking have been utilized to defend against the misuse of deepfake\nspeech. This paper provides a short and concise overview of the three\ntechniques, describing the methodologies, advancements, and challenges. A\ncomprehensive version, offering additional discussions, will be published in\nthe near future.", "published": "2025-09-08 06:22:36", "link": "http://arxiv.org/abs/2509.06361v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "BatStation: Toward In-Situ Radar Sensing on 5G Base Stations with Zero-Shot Template Generation", "abstract": "The coexistence between incumbent radar signals and commercial 5G signals\nnecessitates a versatile and ubiquitous radar sensing for efficient and\nadaptive spectrum sharing. In this context, leveraging the densely deployed 5G\nbase stations (BS) for radar sensing is particularly promising, offering both\nwide coverage and immediate feedback to 5G scheduling. However, the targeting\nradar signals are superimposed with concurrent 5G uplink transmissions received\nby the BS, and practical deployment also demands a lightweight, portable radar\nsensing model. This paper presents BatStation, a lightweight, in-situ radar\nsensing framework seamlessly integrated into 5G BSs. BatStation leverages\nuplink resource grids to extract radar signals through three key components:\n(i) radar signal separation to cancel concurrent 5G transmissions and reveal\nthe radar signals, (ii) resource grid reshaping to align time-frequency\nresolution with radar pulse characteristics, and (iii) zero-shot template\ncorrelation based on a portable model trained purely on synthetic data that\nsupports detection, classification, and localization of radar pulses without\nfine-tuning using experimental data. We implement BatStation on a\nsoftware-defined radio (SDR) testbed and evaluate its performance with real 5G\ntraffic in the CBRS band. Results show robust performance across diverse radar\ntypes, achieving detection probabilities of 97.02% (PUCCH) and 79.23% (PUSCH),\nclassification accuracy up to 97.00%, and median localization errors of\n2.68-6.20 MHz (frequency) and 24.6-32.4 microseconds (time). Notably,\nBatStation achieves this performance with a runtime latency of only 0.11/0.94\nms on GPU/CPU, meeting the real-time requirement of 5G networks.", "published": "2025-09-08 17:17:12", "link": "http://arxiv.org/abs/2509.06898v1", "categories": ["cs.NI", "eess.SP"], "primary_category": "cs.NI"}
{"title": "RadHARSimulator V1: Model-Based FMCW Radar Human Activity Recognition Simulator", "abstract": "Radar-based human activity recognition (HAR) is a pivotal research area for\napplications requiring non-invasive monitoring. However, the acquisition of\ndiverse and high-fidelity radar datasets for robust algorithm development\nremains a significant challenge. To overcome this bottleneck, a model-based\nfrequency-modulated continuous wave (FMCW) radar HAR simulator is developed.\nThe simulator integrates an anthropometrically scaled $13$-scatterer kinematic\nmodel to simulate $12$ distinct activities. The FMCW radar echo model is\nemployed, which incorporates dynamic radar cross-section (RCS), free-space or\nthrough-the-wall propagation, and a calibrated noise floor to ensure signal\nfidelity. The simulated raw data is then processed through a complete pipeline,\nincluding moving target indication (MTI), bulk Doppler compensation, and\nSavitzky-Golay denoising, culminating in the generation of high-resolution\nrange-time map (RTM) and Doppler-time maps (DTMs) via both short-time Fourier\ntransform (STFT) and Fourier synchrosqueezed transform (FSST). Finally, a novel\nneural network method is proposed to validate the effectiveness of the radar\nHAR. Numerical experiments demonstrate that the simulator successfully\ngenerates high-fidelity and distinct micro-Doppler signature, which provides a\nvaluable tool for radar HAR algorithm design and validation. The installer of\nthis simulator is released at:\n\\href{https://github.com/JoeyBGOfficial/RadHARSimulatorV1-Model-Based-FMCW-Radar-Human-Activity-Recognition-Simulator}{Github/JoeyBGOfficial/RadHARSimulatorV1}.", "published": "2025-09-08 14:40:29", "link": "http://arxiv.org/abs/2509.06751v1", "categories": ["eess.SP", "68T45", "I.5.4"], "primary_category": "eess.SP"}
{"title": "ISAC Imaging by Channel State Information using Ray Tracing for Next Generation 6G", "abstract": "Integrated sensing and communications (ISAC) is emerging as a cornerstone\ntechnology for sixth generation (6G) wireless systems, unifying connectivity\nand environmental mapping through shared hardware, spectrum, and waveforms. The\nfollowing paper presents an ISAC imaging framework utilizing channel state\ninformation (CSI) per-path components, transmitter (TX) positions, and receiver\n(RX) positions obtained from the calibrated NYURay ray tracer at 6.75 GHz in\nthe upper mid-band. Our work shows how each resolvable multipath component can\nbe extracted from CSI estimation and cast into an equivalent three-dimensional\nreflection point by fusing its angle and delay information, which is useful and\nchallenging for multi-bounce reflections. The primary contribution of the paper\nis the two-segment reflection point optimization algorithm, which independently\nestimates the path lengths from the TX position and RX position to an\nequivalent reflection point (ERP) on the object surface, thus enabling precise\ngeometric reconstruction. Subsequently, we aggregate the ERPs derived from\nmultiple pairs of TX and RX positions, generating dense three dimensional point\nclouds representing the objects in the channel. Experimental results validate\nthat the proposed ISAC imaging framework accurately reconstructs object\nsurfaces, edges, and curved features. To the best of our knowledge, this paper\nprovides the first demonstration of multi bounce ISAC imaging using wireless\nray tracing at 6.75 GHz.", "published": "2025-09-08 13:30:39", "link": "http://arxiv.org/abs/2509.06672v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "SE and EE Tradeoff in Active STAR-RIS Assisted Systems With Hardware Impairments", "abstract": "This paper investigates the problem of resource efficiency maximization in an\nactive simultaneously transmitting and reflecting reconfigurable intelligent\nsurface (STAR-RIS) assisted communication system under practical transceiver\nhardware impairments (HWIs). We aim to obtain an optimal tradeoff between\nsystem spectral efficiency (SE) and energy efficiency (EE), by jointly\noptimizing the base station (BS) transmit beamforming and the active STAR-RIS\nbeamforming. To tackle the challenges in the fractional objective function, we\nbegin by applying the quadratic transformation method to simplify it into a\nmanageable form. An alternating optimization-based algorithm is then developed\nto iteratively update the BS and STAR-RIS beamforming coefficients. Simulation\nresults demonstrate that the proposed scheme performs better than other\nbaseline schemes in the presence of HWIs. Moreover, the variation of the\nachievable SE-EE region with different transmit power budgets is analyzed.", "published": "2025-09-08 13:20:09", "link": "http://arxiv.org/abs/2509.06662v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Near-Threshold Voltage Massive MIMO Computing", "abstract": "Massive MIMO systems have the potential to significantly enhance spectral\nefficiency, yet their widespread integration is hindered by the high power\nconsumption of the underlying computations. This paper explores the\napplicability and effectiveness of Algorithm-Based Fault Tolerance (ABFT) for\nmassive MIMO signal processing to tackle the reliability challenge of Near\nThreshold Computing (NTC). We propose modifying matrix arithmetic Newton\niteration MIMO algorithm to seamlessly integrate ABFT to detect any\ncomputational errors by inspecting the final result. The overhead from ABFT\ndepends largely on the matrix dimensions, which in this context are dictated by\nthe number of user equipments involved in the computation. NTC is a promising\nstrategy for reducing the energy consumption in digital circuits by operating\ntransistors at extremely reduced voltages. However, NTC is highly susceptible\nto variations in Process, Voltage, and Temperature (PVT) which can lead to\nincreased error rates in computations. Traditional techniques for enabling NTC,\nsuch as dynamic voltage and frequency scaling guided by circuit level timing\nerror detection methods, introduce considerable hardware complexity and are\ndifficult to implement at high clock frequencies. In this context ABFT has\nemerged as a lightweight error detection method tailored for matrix operations\nwithout requiring any modifications on circuit-level and can be implemented\npurely in software.A MIMO accelerator was implemented on a reconfigurable\nhardware platform. Experimental results demonstrate that for sufficiently large\nproblem sizes, the proposed method achieves a 36% power saving compared to\nbaseline, with only an average of 3% computational overhead, at default clock\nfrequency. These results indicate that combining ABFT with near-threshold\noperation provides a viable path toward energy-efficient and robust massive\nMIMO processors.", "published": "2025-09-08 13:07:18", "link": "http://arxiv.org/abs/2509.06651v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity", "abstract": "This paper considers automatic generation control over an information-sharing\nnetwork of communicating generators as a multi-agent system. The optimization\nsolution is distributed among the agents based on information consensus\nalgorithms, while addressing the generators' ramp-rate-limits (RRL). This is\ntypically ignored in the existing linear/nonlinear optimization solutions but\nthey exist in real-time power generation scenarios. Without addressing the RRL,\nthe generators cannot follow the assigned rate of generating power by the\noptimization algorithm; therefore, the existing solutions may not necessarily\nconverge to the exact optimal cost or may lose feasibility in practice. The\nproposed solution in this work addresses the ramp-rate-limit constraint along\nwith the box constraint (limits on the generated powers) and the\ncoupling-constraint (generation-demand balance) at all iteration times of the\nalgorithm. The latter is referred to as the anytime feasibility and implies\nthat at every termination point of the algorithm, the balance between the\ndemand and generated power holds. To improve the convergence rate of the\nalgorithm we further consider internal signum-based nonlinearity. We also show\nthat our solution can tolerate communication link removal. This follows from\nthe uniform-connectivity assumption on the communication network.", "published": "2025-09-08 11:57:44", "link": "http://arxiv.org/abs/2509.06588v1", "categories": ["eess.SY", "cs.DC", "cs.SY", "eess.SP", "math.OC"], "primary_category": "eess.SY"}
{"title": "Synesthesia of Machines (SoM)-Aided LiDAR Point Cloud Transmission for Collaborative Perception", "abstract": "Collaborative perception enables more accurate and comprehensive scene\nunderstanding by learning how to share information between agents, with LiDAR\npoint clouds providing essential precise spatial data. Due to the substantial\ndata volume generated by LiDAR sensors, efficient point cloud transmission is\nessential for low-latency multi-agent collaboration. In this work, we propose\nan efficient, robust and applicable LiDAR point cloud transmission system via\nthe Synesthesia of Machines (SoM), termed LiDAR Point Cloud Feature\nTransmission (LPC-FT), to support collaborative perception among multiple\nagents. Specifically, we employ a density-preserving deep point cloud\ncompression method that encodes the complete point cloud into a downsampled\nefficient representation. To mitigate the effects of the wireless channel, we\ndesign a channel encoder module based on self-attention to enhance LiDAR point\ncloud features and a feature fusion module based on cross-attention to\nintegrate features from transceivers. Furthermore, we utilize the nonlinear\nactivation layer and transfer learning to improve the training of deep neural\nnetworks in the presence the digital channel noise. Experimental results\ndemonstrate that the proposed LPC-FT is more robust and effective than\ntraditional octree-based compression followed by channel coding, and\noutperforms state-of-the-art deep learning-based compression techniques and\nexisting semantic communication methods, reducing the Chamfer Distance by 30%\nand improving the PSNR by 1.9 dB on average. Owing to its superior\nreconstruction performance and robustness against channel variations, LPC-FT is\nexpected to support collaborative perception tasks.", "published": "2025-09-08 10:11:27", "link": "http://arxiv.org/abs/2509.06506v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Optimal Distortion-Aware Multi-User Power Allocation for Massive MIMO Networks", "abstract": "Real-world wireless transmitter front-ends exhibit certain nonlinear\nbehavior, e.g., signal clipping by a Power Amplifier (PA). Although many\nresource allocation solutions do not consider this for simplicity, it leads to\ninaccurate results or a reduced number of degrees of freedom, not achieving the\nglobal performance. In this work, we propose an optimal PA distortion-aware\npower allocation strategy in a downlink orthogonal frequency division multiplex\n(OFDM) based massive multiple-input multiple-output (M-MIMO) system. Assuming a\nsoft-limiter PA model, where the transmission occurs under small-scale\nindependent and identically distributed (i.i.d) Rayleigh fading channel, we\nderive the wideband signal-to-noise-and-distortion ratio (SNDR) and formulate\nthe power allocation problem. Most interestingly, the distortion introduced by\nthe PA leads to an SNDR-efficient operating point without explicit transmit\npower constraints. While the optimization problem is non-convex, we decouple it\ninto a non-convex total power allocation problem and a convex power\ndistribution problem among the users (UEs). We propose an alternating\noptimization algorithm to find the optimum solution. Our simulation results\nshow significant sum-rate gains over existing distortion-neglecting solutions,\ne.g., a median 4 times increase and a median 50\\% increase for a 64-antenna and\n512-antenna base station serving 60 users, respectively.", "published": "2025-09-08 09:52:37", "link": "http://arxiv.org/abs/2509.06491v1", "categories": ["eess.SP"], "primary_category": "eess.SP"}
{"title": "Human Body Weight Estimation Through Music-Induced Bed Vibrations", "abstract": "Rapid and accurate body weight estimation is critical in emergency medical\ncare, as it directly influences treatment decisions, such as drug dosing,\ndefibrillation energy selection, and fluid resuscitation. Traditional methods\nsuch as stand-on scales, length-based tapes, or transfer-based weighing scales\nare often impractical for immobilized patients, inaccurate, or labor-intensive\nand time-consuming. This paper introduces MelodyBedScale, a non-intrusive and\nrapid on-bed weight estimation system that leverages bed vibration induced by\nmusic. The core insight is that body weight affects the vibration transfer\nfunction of the bed-body system, which is captured using vibration sensors\nplaced on opposite sides of the bed. First, we identify weight-sensitive\nfrequency bands and compose clinically acceptable soft, natural music with high\nsignal energy in these frequency bands. This music is then played through a\nspeaker mounted on the bed to induce bed vibrations. Additionally, to\nefficiently capture the complex weight-vibration relationship with limited data\nand enhance generalizability to unseen individuals and weights, we\ntheoretically analyze the weight-vibration relationship and integrate the\nresults into the activation functions of the neural network for\nphysics-informed weight regression. We evaluated MelodyBedScale on both wooden\nand steel beds across 11 participants, achieving a mean absolute error of up to\n1.55 kg.", "published": "2025-09-08 00:44:59", "link": "http://arxiv.org/abs/2509.06257v1", "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "eess.SP"}
