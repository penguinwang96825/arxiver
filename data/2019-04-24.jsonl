{"title": "PhonSenticNet: A Cognitive Approach to Microtext Normalization for\n  Concept-Level Sentiment Analysis", "abstract": "With the current upsurge in the usage of social media platforms, the trend of\nusing short text (microtext) in place of standard words has seen a significant\nrise. The usage of microtext poses a considerable performance issue in\nconcept-level sentiment analysis, since models are trained on standard words.\nThis paper discusses the impact of coupling sub-symbolic (phonetics) with\nsymbolic (machine learning) Artificial Intelligence to transform the\nout-of-vocabulary concepts into their standard in-vocabulary form. The phonetic\ndistance is calculated using the Sorensen similarity algorithm. The\nphonetically similar invocabulary concepts thus obtained are then used to\ncompute the correct polarity value, which was previously being miscalculated\nbecause of the presence of microtext. Our proposed framework increases the\naccuracy of polarity detection by 6% as compared to the earlier model. This\nalso validates the fact that microtext normalization is a necessary\npre-requisite for the sentiment analysis task.", "published": "2019-04-24 03:23:38", "link": "http://arxiv.org/abs/1905.01967v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Objective Assessment of Social Skills Using Automated Language Analysis\n  for Identification of Schizophrenia and Bipolar Disorder", "abstract": "Several studies have shown that speech and language features, automatically\nextracted from clinical interviews or spontaneous discourse, have diagnostic\nvalue for mental disorders such as schizophrenia and bipolar disorder. They\ntypically make use of a large feature set to train a classifier for\ndistinguishing between two groups of interest, i.e. a clinical and control\ngroup. However, a purely data-driven approach runs the risk of overfitting to a\nparticular data set, especially when sample sizes are limited. Here, we first\ndown-select the set of language features to a small subset that is related to a\nwell-validated test of functional ability, the Social Skills Performance\nAssessment (SSPA). This helps establish the concurrent validity of the selected\nfeatures. We use only these features to train a simple classifier to\ndistinguish between groups of interest. Linear regression reveals that a subset\nof language features can effectively model the SSPA, with a correlation\ncoefficient of 0.75. Furthermore, the same feature set can be used to build a\nstrong binary classifier to distinguish between healthy controls and a clinical\ngroup (AUC = 0.96) and also between patients within the clinical group with\nschizophrenia and bipolar I disorder (AUC = 0.83).", "published": "2019-04-24 03:09:47", "link": "http://arxiv.org/abs/1904.10622v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Better Automatic Evaluation of Open-Domain Dialogue Systems with\n  Contextualized Embeddings", "abstract": "Despite advances in open-domain dialogue systems, automatic evaluation of\nsuch systems is still a challenging problem. Traditional reference-based\nmetrics such as BLEU are ineffective because there could be many valid\nresponses for a given context that share no common words with reference\nresponses. A recent work proposed Referenced metric and Unreferenced metric\nBlended Evaluation Routine (RUBER) to combine a learning-based metric, which\npredicts relatedness between a generated response and a given query, with\nreference-based metric; it showed high correlation with human judgments. In\nthis paper, we explore using contextualized word embeddings to compute more\naccurate relatedness scores, thus better evaluation metrics. Experiments show\nthat our evaluation metrics outperform RUBER, which is trained on static\nembeddings.", "published": "2019-04-24 04:16:44", "link": "http://arxiv.org/abs/1904.10635v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Who Blames Whom in a Crisis? Detecting Blame Ties from News Articles\n  Using Neural Networks", "abstract": "Blame games tend to follow major disruptions, be they financial crises,\nnatural disasters or terrorist attacks. To study how the blame game evolves and\nshapes the dominant crisis narratives is of great significance, as sense-making\nprocesses can affect regulatory outcomes, social hierarchies, and cultural\nnorms. However, it takes tremendous time and efforts for social scientists to\nmanually examine each relevant news article and extract the blame ties (A\nblames B). In this study, we define a new task, Blame Tie Extraction, and\nconstruct a new dataset related to the United States financial crisis\n(2007-2010) from The New York Times, The Wall Street Journal and USA Today. We\nbuild a Bi-directional Long Short-Term Memory (BiLSTM) network for contexts\nwhere the entities appear in and it learns to automatically extract such blame\nties at the document level. Leveraging the large unsupervised model such as\nGloVe and ELMo, our best model achieves an F1 score of 70% on the test set for\nblame tie extraction, making it a useful tool for social scientists to extract\nblame ties more efficiently.", "published": "2019-04-24 04:31:46", "link": "http://arxiv.org/abs/1904.10637v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Detecting Machine-Translated Paragraphs by Matching Similar Words", "abstract": "Machine-translated text plays an important role in modern life by smoothing\ncommunication from various communities using different languages. However,\nunnatural translation may lead to misunderstanding, a detector is thus needed\nto avoid the unfortunate mistakes. While a previous method measured the\nnaturalness of continuous words using a N-gram language model, another method\nmatched noncontinuous words across sentences but this method ignores such words\nin an individual sentence. We have developed a method matching similar words\nthroughout the paragraph and estimating the paragraph-level coherence, that can\nidentify machine-translated text. Experiment evaluates on 2000 English\nhuman-generated and 2000 English machine-translated paragraphs from German\nshowing that the coherence-based method achieves high performance (accuracy =\n87.0%; equal error rate = 13.0%). It is efficiently better than previous\nmethods (best accuracy = 72.4%; equal error rate = 29.7%). Similar experiments\non Dutch and Japanese obtain 89.2% and 97.9% accuracy, respectively. The\nresults demonstrate the persistence of the proposed method in various languages\nwith different resource levels.", "published": "2019-04-24 05:03:28", "link": "http://arxiv.org/abs/1904.10641v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Listening between the Lines: Learning Personal Attributes from\n  Conversations", "abstract": "Open-domain dialogue agents must be able to converse about many topics while\nincorporating knowledge about the user into the conversation. In this work we\naddress the acquisition of such knowledge, for personalization in downstream\nWeb applications, by extracting personal attributes from conversations. This\nproblem is more challenging than the established task of information extraction\nfrom scientific publications or Wikipedia articles, because dialogues often\ngive merely implicit cues about the speaker. We propose methods for inferring\npersonal attributes, such as profession, age or family status, from\nconversations using deep learning. Specifically, we propose several Hidden\nAttribute Models, which are neural networks leveraging attention mechanisms and\nembeddings. Our methods are trained on a per-predicate basis to output rankings\nof object values for a given subject-predicate combination (e.g., ranking the\ndoctor and nurse professions high when speakers talk about patients, emergency\nrooms, etc). Experiments with various conversational texts including Reddit\ndiscussions, movie scripts and a collection of crowdsourced personal dialogues\ndemonstrate the viability of our methods and their superior performance\ncompared to state-of-the-art baselines.", "published": "2019-04-24 15:54:46", "link": "http://arxiv.org/abs/1904.10887v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Assessing the Tolerance of Neural Machine Translation Systems Against\n  Speech Recognition Errors", "abstract": "Machine translation systems are conventionally trained on textual resources\nthat do not model phenomena that occur in spoken language. While the evaluation\nof neural machine translation systems on textual inputs is actively researched\nin the literature , little has been discovered about the complexities of\ntranslating spoken language data with neural models. We introduce and motivate\ninteresting problems one faces when considering the translation of automatic\nspeech recognition (ASR) outputs on neural machine translation (NMT) systems.\nWe test the robustness of sentence encoding approaches for NMT encoder-decoder\nmodeling, focusing on word-based over byte-pair encoding. We compare the\ntranslation of utterances containing ASR errors in state-of-the-art NMT\nencoder-decoder systems against a strong phrase-based machine translation\nbaseline in order to better understand which phenomena present in ASR outputs\nare better represented under the NMT framework than approaches that represent\ntranslation as a linear model.", "published": "2019-04-24 18:23:05", "link": "http://arxiv.org/abs/1904.10997v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Toponym Identification in Epidemiology Articles - A Deep Learning\n  Approach", "abstract": "When analyzing the spread of viruses, epidemiologists often need to identify\nthe location of infected hosts. This information can be found in public\ndatabases, such as GenBank, however, information provided in these databases\nare usually limited to the country or state level. More fine-grained\nlocalization information requires phylogeographers to manually read relevant\nscientific articles. In this work we propose an approach to automate the\nprocess of place name identification from medical (epidemiology) articles. The\nfocus of this paper is to propose a deep learning based model for toponym\ndetection and experiment with the use of external linguistic features and\ndomain specific information. The model was evaluated using a collection of 105\nepidemiology articles from PubMed Central provided by the recent SemEval task\n12. Our best detection model achieves an F1 score of $80.13\\%$, a significant\nimprovement compared to the state of the art of $69.84\\%$. These results\nunderline the importance of domain specific embedding as well as specific\nlinguistic features in toponym detection in medical journals.", "published": "2019-04-24 19:00:13", "link": "http://arxiv.org/abs/1904.11018v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Phonetically-Oriented Word Error Alignment for Speech Recognition Error\n  Analysis in Speech Translation", "abstract": "We propose a variation to the commonly used Word Error Rate (WER) metric for\nspeech recognition evaluation which incorporates the alignment of phonemes, in\nthe absence of time boundary information. After computing the Levenshtein\nalignment on words in the reference and hypothesis transcripts, spans of\nadjacent errors are converted into phonemes with word and syllable boundaries\nand a phonetic Levenshtein alignment is performed. The aligned phonemes are\nrecombined into aligned words that adjust the word alignment labels in each\nerror region. We demonstrate that our Phonetically-Oriented Word Error Rate\n(POWER) yields similar scores to WER with the added advantages of better word\nalignments and the ability to capture one-to-many word alignments corresponding\nto homophonic errors in speech recognition hypotheses. These improved\nalignments allow us to better trace the impact of Levenshtein error types on\ndownstream tasks such as speech translation.", "published": "2019-04-24 19:09:35", "link": "http://arxiv.org/abs/1904.11024v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Condition-Transforming Variational AutoEncoder for Conversation Response\n  Generation", "abstract": "This paper proposes a new model, called condition-transforming variational\nautoencoder (CTVAE), to improve the performance of conversation response\ngeneration using conditional variational autoencoders (CVAEs). In conventional\nCVAEs , the prior distribution of latent variable z follows a multivariate\nGaussian distribution with mean and variance modulated by the input conditions.\nPrevious work found that this distribution tends to become condition\nindependent in practical application. In our proposed CTVAE model, the latent\nvariable z is sampled by performing a non-lineartransformation on the\ncombination of the input conditions and the samples from a\ncondition-independent prior distribution N (0; I). In our objective\nevaluations, the CTVAE model outperforms the CVAE model on fluency metrics and\nsurpasses a sequence-to-sequence (Seq2Seq) model on diversity metrics. In\nsubjective preference tests, our proposed CTVAE model performs significantly\nbetter than CVAE and Seq2Seq models on generating fluency, informative and\ntopic relevant responses.", "published": "2019-04-24 02:26:48", "link": "http://arxiv.org/abs/1904.10610v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Semantic Drift in Multilingual Representations", "abstract": "Multilingual representations have mostly been evaluated based on their\nperformance on specific tasks. In this article, we look beyond engineering\ngoals and analyze the relations between languages in computational\nrepresentations. We introduce a methodology for comparing languages based on\ntheir organization of semantic concepts. We propose to conduct an adapted\nversion of representational similarity analysis of a selected set of concepts\nin computational multilingual representations. Using this analysis method, we\ncan reconstruct a phylogenetic tree that closely resembles those assumed by\nlinguistic experts. These results indicate that multilingual distributional\nrepresentations which are only trained on monolingual text and bilingual\ndictionaries preserve relations between languages without the need for any\netymological information. In addition, we propose a measure to identify\nsemantic drift between language families. We perform experiments on word-based\nand sentence-based multilingual models and provide both quantitative results\nand qualitative examples. Analyses of semantic drift in multilingual\nrepresentations can serve two purposes: they can indicate unwanted\ncharacteristics of the computational models and they provide a quantitative\nmeans to study linguistic phenomena across languages. The code is available at\nhttps://github.com/beinborn/SemanticDrift.", "published": "2019-04-24 13:55:42", "link": "http://arxiv.org/abs/1904.10820v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "A Self-Attentive Emotion Recognition Network", "abstract": "Modern deep learning approaches have achieved groundbreaking performance in\nmodeling and classifying sequential data. Specifically, attention networks\nconstitute the state-of-the-art paradigm for capturing long temporal dynamics.\nThis paper examines the efficacy of this paradigm in the challenging task of\nemotion recognition in dyadic conversations. In contrast to existing\napproaches, our work introduces a novel attention mechanism capable of\ninferring the immensity of the effect of each past utterance on the current\nspeaker emotional state. The proposed attention mechanism performs this\ninference procedure without the need of a decoder network; this is achieved by\nmeans of innovative self-attention arguments. Our self-attention networks\ncapture the correlation patterns among consecutive encoder network states, thus\nallowing to robustly and effectively model temporal dynamics over arbitrary\nlong temporal horizons. Thus, we enable capturing strong affective patterns\nover the course of long discussions. We exhibit the effectiveness of our\napproach considering the challenging IEMOCAP benchmark. As we show, our devised\nmethodology outperforms state-of-the-art alternatives and commonly used\napproaches, giving rise to promising new research directions in the context of\nOnline Social Network (OSN) analysis tasks.", "published": "2019-04-24 09:46:58", "link": "http://arxiv.org/abs/1905.01972v1", "categories": ["cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Generating Token-Level Explanations for Natural Language Inference", "abstract": "The task of Natural Language Inference (NLI) is widely modeled as supervised\nsentence pair classification. While there has been a lot of work recently on\ngenerating explanations of the predictions of classifiers on a single piece of\ntext, there have been no attempts to generate explanations of classifiers\noperating on pairs of sentences. In this paper, we show that it is possible to\ngenerate token-level explanations for NLI without the need for training data\nexplicitly annotated for this purpose. We use a simple LSTM architecture and\nevaluate both LIME and Anchor explanations for this task. We compare these to a\nMultiple Instance Learning (MIL) method that uses thresholded attention make\ntoken-level predictions. The approach we present in this paper is a novel\nextension of zero-shot single-sentence tagging to sentence pairs for NLI. We\nconduct our experiments on the well-studied SNLI dataset that was recently\naugmented with manually annotation of the tokens that explain the entailment\nrelation. We find that our white-box MIL-based method, while orders of\nmagnitude faster, does not reach the same accuracy as the black-box methods.", "published": "2019-04-24 09:41:14", "link": "http://arxiv.org/abs/1904.10717v1", "categories": ["cs.LG", "cs.CL", "stat.ML"], "primary_category": "cs.LG"}
{"title": "A bag-of-concepts model improves relation extraction in a narrow\n  knowledge domain with limited data", "abstract": "This paper focuses on a traditional relation extraction task in the context\nof limited annotated data and a narrow knowledge domain. We explore this task\nwith a clinical corpus consisting of 200 breast cancer follow-up treatment\nletters in which 16 distinct types of relations are annotated. We experiment\nwith an approach to extracting typed relations called window-bounded\nco-occurrence (WBC), which uses an adjustable context window around entity\nmentions of a relevant type, and compare its performance with a more typical\nintra-sentential co-occurrence baseline. We further introduce a new\nbag-of-concepts (BoC) approach to feature engineering based on the\nstate-of-the-art word embeddings and word synonyms. We demonstrate the\ncompetitiveness of BoC by comparing with methods of higher complexity, and\nexplore its effectiveness on this small dataset.", "published": "2019-04-24 11:06:54", "link": "http://arxiv.org/abs/1904.10743v1", "categories": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "primary_category": "cs.LG"}
{"title": "Integrating Social Media into a Pan-European Flood Awareness System: A\n  Multilingual Approach", "abstract": "This paper describes a prototype system that integrates social media analysis\ninto the European Flood Awareness System (EFAS). This integration allows the\ncollection of social media data to be automatically triggered by flood risk\nwarnings determined by a hydro-meteorological model. Then, we adopt a\nmulti-lingual approach to find flood-related messages by employing two\nstate-of-the-art methodologies: language-agnostic word embeddings and\nlanguage-aligned word embeddings. Both approaches can be used to bootstrap a\nclassifier of social media messages for a new language with little or no\nlabeled data. Finally, we describe a method for selecting relevant and\nrepresentative messages and displaying them back in the interface of EFAS.", "published": "2019-04-24 15:40:14", "link": "http://arxiv.org/abs/1904.10876v1", "categories": ["cs.IR", "cs.AI", "cs.CL"], "primary_category": "cs.IR"}
{"title": "On the Contributions of Visual and Textual Supervision in Low-Resource\n  Semantic Speech Retrieval", "abstract": "Recent work has shown that speech paired with images can be used to learn\nsemantically meaningful speech representations even without any textual\nsupervision. In real-world low-resource settings, however, we often have access\nto some transcribed speech. We study whether and how visual grounding is useful\nin the presence of varying amounts of textual supervision. In particular, we\nconsider the task of semantic speech retrieval in a low-resource setting. We\nuse a previously studied data set and task, where models are trained on images\nwith spoken captions and evaluated on human judgments of semantic relevance. We\npropose a multitask learning approach to leverage both visual and textual\nmodalities, with visual supervision in the form of keyword probabilities from\nan external tagger. We find that visual grounding is helpful even in the\npresence of textual supervision, and we analyze this effect over a range of\nsizes of transcribed data sets. With ~5 hours of transcribed speech, we obtain\n23% higher average precision when also using visual supervision.", "published": "2019-04-24 17:44:06", "link": "http://arxiv.org/abs/1904.10947v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "An Attentional Neural Network Architecture for Folk Song Classification", "abstract": "In this paper we present an attentional neural network for folk song\nclassification. We introduce the concept of musical motif embedding, and show\nhow using melodic local context we are able to model monophonic folk song\nmotifs using the skipgram version of the word2vec algorithm. We use the motif\nembeddings to represent folk songs from Germany, China, and Sweden, and\nclassify them using an attentional neural network that is able to discern\nrelevant motifs in a song. The results show how the network obtains state of\nthe art accuracy in a completely unsupervised manner, and how motif embeddings\nproduce high quality motif representations from folk songs. We conjecture on\nthe advantages of this type of representation in large symbolic music corpora,\nand how it can be helpful in the musicological analysis of folk song\ncollections from different cultures and geographical areas.", "published": "2019-04-24 21:27:09", "link": "http://arxiv.org/abs/1904.11074v1", "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Context-Aware Zero-Shot Learning for Object Recognition", "abstract": "Zero-Shot Learning (ZSL) aims at classifying unlabeled objects by leveraging\nauxiliary knowledge, such as semantic representations. A limitation of previous\napproaches is that only intrinsic properties of objects, e.g. their visual\nappearance, are taken into account while their context, e.g. the surrounding\nobjects in the image, is ignored. Following the intuitive principle that\nobjects tend to be found in certain contexts but not others, we propose a new\nand challenging approach, context-aware ZSL, that leverages semantic\nrepresentations in a new way to model the conditional likelihood of an object\nto appear in a given context. Finally, through extensive experiments conducted\non Visual Genome, we show that contextual information can substantially improve\nthe standard ZSL approach and is robust to unbalanced classes.", "published": "2019-04-24 08:50:05", "link": "http://arxiv.org/abs/1904.12638v2", "categories": ["cs.CV", "cs.CL", "cs.LG", "stat.ML"], "primary_category": "cs.CV"}
{"title": "Grounding Natural Language Commands to StarCraft II Game States for\n  Narration-Guided Reinforcement Learning", "abstract": "While deep reinforcement learning techniques have led to agents that are\nsuccessfully able to learn to perform a number of tasks that had been\npreviously unlearnable, these techniques are still susceptible to the\nlongstanding problem of {\\em reward sparsity}. This is especially true for\ntasks such as training an agent to play StarCraft II, a real-time strategy game\nwhere reward is only given at the end of a game which is usually very long.\nWhile this problem can be addressed through reward shaping, such approaches\ntypically require a human expert with specialized knowledge. Inspired by the\nvision of enabling reward shaping through the more-accessible paradigm of\nnatural-language narration, we investigate to what extent we can contextualize\nthese narrations by grounding them to the goal-specific states. We present a\nmutual-embedding model using a multi-input deep-neural network that projects a\nsequence of natural language commands into the same high-dimensional\nrepresentation space as corresponding goal states. We show that using this\nmodel we can learn an embedding space with separable and distinct clusters that\naccurately maps natural-language commands to corresponding game states . We\nalso discuss how this model can allow for the use of narrations as a robust\nform of reward shaping to improve RL performance and efficiency.", "published": "2019-04-24 17:43:40", "link": "http://arxiv.org/abs/1906.02671v1", "categories": ["cs.MM", "cs.CL", "cs.LG", "cs.NE", "cs.RO"], "primary_category": "cs.MM"}
{"title": "Realizing Petabyte Scale Acoustic Modeling", "abstract": "Large scale machine learning (ML) systems such as the Alexa automatic speech\nrecognition (ASR) system continue to improve with increasing amounts of\nmanually transcribed training data. Instead of scaling manual transcription to\nimpractical levels, we utilize semi-supervised learning (SSL) to learn acoustic\nmodels (AM) from the vast firehose of untranscribed audio data. Learning an AM\nfrom 1 Million hours of audio presents unique ML and system design challenges.\nWe present the design and evaluation of a highly scalable and resource\nefficient SSL system for AM. Employing the student/teacher learning paradigm,\nwe focus on the student learning subsystem: a scalable and robust data pipeline\nthat generates features and targets from raw audio, and an efficient model\npipeline, including the distributed trainer, that builds a student model. Our\nevaluations show that, even without extensive hyper-parameter tuning, we obtain\nrelative accuracy improvements in the 10 to 20$\\%$ range, with higher gains in\nnoisier conditions. The end-to-end processing time of this SSL system was 12\ndays, and several components in this system can trivially scale linearly with\nmore compute resources.", "published": "2019-04-24 00:39:25", "link": "http://arxiv.org/abs/1904.10584v1", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "primary_category": "cs.SD"}
{"title": "Unsupervised Adversarial Domain Adaptation Based On The Wasserstein\n  Distance For Acoustic Scene Classification", "abstract": "A challenging problem in deep learning-based machine listening field is the\ndegradation of the performance when using data from unseen conditions. In this\npaper we focus on the acoustic scene classification (ASC) task and propose an\nadversarial deep learning method to allow adapting an acoustic scene\nclassification system to deal with a new acoustic channel resulting from data\ncaptured with a different recording device. We build upon the theoretical model\nof H{\\Delta}H-distance and previous adversarial discriminative deep learning\nmethod for ASC unsupervised domain adaptation, and we present an adversarial\ntraining based method using the Wasserstein distance. We improve the\nstate-of-the-art mean accuracy on the data from the unseen conditions from 32%\nto 45%, using the TUT Acoustic Scenes dataset.", "published": "2019-04-24 08:01:38", "link": "http://arxiv.org/abs/1904.10678v2", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Robust Approach for Securing Audio Classification Against Adversarial\n  Attacks", "abstract": "Adversarial audio attacks can be considered as a small perturbation\nunperceptive to human ears that is intentionally added to the audio signal and\ncauses a machine learning model to make mistakes. This poses a security concern\nabout the safety of machine learning models since the adversarial attacks can\nfool such models toward the wrong predictions. In this paper we first review\nsome strong adversarial attacks that may affect both audio signals and their 2D\nrepresentations and evaluate the resiliency of the most common machine learning\nmodel, namely deep learning models and support vector machines (SVM) trained on\n2D audio representations such as short time Fourier transform (STFT), discrete\nwavelet transform (DWT) and cross recurrent plot (CRP) against several\nstate-of-the-art adversarial attacks. Next, we propose a novel approach based\non pre-processed DWT representation of audio signals and SVM to secure audio\nsystems against adversarial attacks. The proposed architecture has several\npreprocessing modules for generating and enhancing spectrograms including\ndimension reduction and smoothing. We extract features from small patches of\nthe spectrograms using speeded up robust feature (SURF) algorithm which are\nfurther used to generate a codebook using the K-Means++ algorithm. Finally,\ncodewords are used to train a SVM on the codebook of the SURF-generated\nvectors. All these steps yield to a novel approach for audio classification\nthat provides a good trade-off between accuracy and resilience. Experimental\nresults on three environmental sound datasets show the competitive performance\nof proposed approach compared to the deep neural networks both in terms of\naccuracy and robustness against strong adversarial attacks.", "published": "2019-04-24 18:07:52", "link": "http://arxiv.org/abs/1904.10990v2", "categories": ["cs.LG", "cs.CR", "cs.SD", "eess.AS", "stat.ML"], "primary_category": "cs.LG"}
