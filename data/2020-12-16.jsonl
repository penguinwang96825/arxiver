{"title": "Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language\n  Pre-Training", "abstract": "Fitting complex patterns in the training data, such as reasoning and\ncommonsense, is a key challenge for language pre-training. According to recent\nstudies and our empirical observations, one possible reason is that some\neasy-to-fit patterns in the training data, such as frequently co-occurring word\ncombinations, dominate and harm pre-training, making it hard for the model to\nfit more complex information. We argue that mis-predictions can help locate\nsuch dominating patterns that harm language understanding. When a\nmis-prediction occurs, there should be frequently co-occurring patterns with\nthe mis-predicted word fitted by the model that lead to the mis-prediction. If\nwe can add regularization to train the model to rely less on such dominating\npatterns when a mis-prediction occurs and focus more on the rest more subtle\npatterns, more information can be efficiently fitted at pre-training. Following\nthis motivation, we propose a new language pre-training method, Mis-Predictions\nas Harm Alerts (MPA). In MPA, when a mis-prediction occurs during pre-training,\nwe use its co-occurrence information to guide several heads of the\nself-attention modules. Some self-attention heads in the Transformer modules\nare optimized to assign lower attention weights to the words in the input\nsentence that frequently co-occur with the mis-prediction while assigning\nhigher weights to the other words. By doing so, the Transformer model is\ntrained to rely less on the dominating frequently co-occurring patterns with\nmis-predictions while focus more on the rest more complex information when\nmis-predictions occur. Our experiments show that MPA expedites the pre-training\nof BERT and ELECTRA and improves their performances on downstream tasks.", "published": "2020-12-16 08:21:51", "link": "http://arxiv.org/abs/2012.08789v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "A Lightweight Neural Model for Biomedical Entity Linking", "abstract": "Biomedical entity linking aims to map biomedical mentions, such as diseases\nand drugs, to standard entities in a given knowledge base. The specific\nchallenge in this context is that the same biomedical entity can have a wide\nrange of names, including synonyms, morphological variations, and names with\ndifferent word orderings. Recently, BERT-based methods have advanced the\nstate-of-the-art by allowing for rich representations of word sequences.\nHowever, they often have hundreds of millions of parameters and require heavy\ncomputing resources, which limits their applications in resource-limited\nscenarios. Here, we propose a lightweight neural method for biomedical entity\nlinking, which needs just a fraction of the parameters of a BERT model and much\nless computing resources. Our method uses a simple alignment layer with\nattention mechanisms to capture the variations between mention and entity\nnames. Yet, we show that our model is competitive with previous work on\nstandard evaluation benchmarks.", "published": "2020-12-16 10:34:37", "link": "http://arxiv.org/abs/2012.08844v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Costs to Consider in Adopting NLP for Your Business", "abstract": "Recent advances in Natural Language Processing (NLP) have largely pushed deep\ntransformer-based models as the go-to state-of-the-art technique without much\nregard to the production and utilization cost. Companies planning to adopt\nthese methods into their business face difficulties because of the lack of\nmachine, data, and human resources to build them. We compare both the\nperformance and the cost of classical learning algorithms to the latest ones in\ncommon sequence and text labeling tasks. In our industrial datasets, we find\nthat classical models often perform on par with deep neural ones despite the\nlower cost. We show the trade-off between performance gain and the cost across\nthe models to give more insights for AI-pivoting business. Further, we call for\nmore research into low-cost models, especially for under-resourced languages.", "published": "2020-12-16 13:57:31", "link": "http://arxiv.org/abs/2012.08958v2", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6"], "primary_category": "cs.CL"}
{"title": "Show or Tell? Demonstration is More Robust to Changes in Shared\n  Perception than Explanation", "abstract": "Successful teaching entails a complex interaction between a teacher and a\nlearner. The teacher must select and convey information based on what they\nthink the learner perceives and believes. Teaching always involves misaligned\nbeliefs, but studies of pedagogy often focus on situations where teachers and\nlearners share perceptions. Nonetheless, a teacher and learner may not always\nexperience or attend to the same aspects of the environment. Here, we study how\nmisaligned perceptions influence communication. We hypothesize that the\nefficacy of different forms of communication depends on the shared perceptual\nstate between teacher and learner. We develop a cooperative teaching game to\ntest whether concrete mediums (demonstrations, or \"showing\") are more robust\nthan abstract ones (language, or \"telling\") when the teacher and learner are\nnot perceptually aligned. We find evidence that (1) language-based teaching is\nmore affected by perceptual misalignment, but (2) demonstration-based teaching\nis less likely to convey nuanced information. We discuss implications for human\npedagogy and machine learning.", "published": "2020-12-16 15:53:02", "link": "http://arxiv.org/abs/2012.09035v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Exploring Thematic Coherence in Fake News", "abstract": "The spread of fake news remains a serious global issue; understanding and\ncurtailing it is paramount. One way of differentiating between deceptive and\ntruthful stories is by analyzing their coherence. This study explores the use\nof topic models to analyze the coherence of cross-domain news shared online.\nExperimental results on seven cross-domain datasets demonstrate that fake news\nshows a greater thematic deviation between its opening sentences and its\nremainder.", "published": "2020-12-16 18:01:04", "link": "http://arxiv.org/abs/2012.09118v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Improving Multilingual Neural Machine Translation For Low-Resource\n  Languages: French,English - Vietnamese", "abstract": "Prior works have demonstrated that a low-resource language pair can benefit\nfrom multilingual machine translation (MT) systems, which rely on many language\npairs' joint training. This paper proposes two simple strategies to address the\nrare word issue in multilingual MT systems for two low-resource language pairs:\nFrench-Vietnamese and English-Vietnamese. The first strategy is about dynamical\nlearning word similarity of tokens in the shared space among source languages\nwhile another one attempts to augment the translation ability of rare words\nthrough updating their embeddings during the training. Besides, we leverage\nmonolingual data for multilingual MT systems to increase the amount of\nsynthetic parallel corpora while dealing with the data sparsity problem. We\nhave shown significant improvements of up to +1.62 and +2.54 BLEU points over\nthe bilingual baseline systems for both language pairs and released our\ndatasets for the research community.", "published": "2020-12-16 04:43:43", "link": "http://arxiv.org/abs/2012.08743v2", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building domain specific lexicon based on TikTok comment dataset", "abstract": "In the sentiment analysis task, predicting the sentiment tendency of a\nsentence is an important branch. Previous research focused more on sentiment\nanalysis in English, for example, analyzing the sentiment tendency of sentences\nbased on Valence, Arousal, Dominance of sentences. the emotional tendency is\ndifferent between the two languages. For example, the sentence order between\nChinese and English may present different emotions. This paper tried a method\nthat builds a domain-specific lexicon. In this way, the model can classify\nChinese words with emotional tendency. In this approach, based on the [13], an\nultra-dense space embedding table is trained through word embedding of Chinese\nTikTok review and emotional lexicon sources(seed words). The result of the\nmodel is a domain-specific lexicon, which presents the emotional tendency of\nwords. I collected Chinese TikTok comments as training data. By comparing The\ntraining results with the PCA method to evaluate the performance of the model\nin Chinese sentiment classification, the results show that the model has done\nwell in Chinese. The source code has released on\ngithub:https://github.com/h2222/douyin_comment_dataset", "published": "2020-12-16 07:26:43", "link": "http://arxiv.org/abs/2012.08773v1", "categories": ["cs.CL", "cs.AI", "F.2.2, I.2.7"], "primary_category": "cs.CL"}
{"title": "Clinical Temporal Relation Extraction with Probabilistic Soft Logic\n  Regularization and Global Inference", "abstract": "There has been a steady need in the medical community to precisely extract\nthe temporal relations between clinical events. In particular, temporal\ninformation can facilitate a variety of downstream applications such as case\nreport retrieval and medical question answering. Existing methods either\nrequire expensive feature engineering or are incapable of modeling the global\nrelational dependencies among the events. In this paper, we propose a novel\nmethod, Clinical Temporal ReLation Exaction with Probabilistic Soft Logic\nRegularization and Global Inference (CTRL-PG) to tackle the problem at the\ndocument level. Extensive experiments on two benchmark datasets, I2B2-2012 and\nTB-Dense, demonstrate that CTRL-PG significantly outperforms baseline methods\nfor temporal relation extraction.", "published": "2020-12-16 08:23:03", "link": "http://arxiv.org/abs/2012.08790v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "R$^2$-Net: Relation of Relation Learning Network for Sentence Semantic\n  Matching", "abstract": "Sentence semantic matching is one of the fundamental tasks in natural\nlanguage processing, which requires an agent to determine the semantic relation\namong input sentences. Recently, deep neural networks have achieved impressive\nperformance in this area, especially BERT. Despite the effectiveness of these\nmodels, most of them treat output labels as meaningless one-hot vectors,\nunderestimating the semantic information and guidance of relations that these\nlabels reveal, especially for tasks with a small number of labels. To address\nthis problem, we propose a Relation of Relation Learning Network (R2-Net) for\nsentence semantic matching. Specifically, we first employ BERT to encode the\ninput sentences from a global perspective. Then a CNN-based encoder is designed\nto capture keywords and phrase information from a local perspective. To fully\nleverage labels for better relation information extraction, we introduce a\nself-supervised relation of relation classification task for guiding R2-Net to\nconsider more about labels. Meanwhile, a triplet loss is employed to\ndistinguish the intra-class and inter-class relations in a finer granularity.\nEmpirical experiments on two sentence semantic matching tasks demonstrate the\nsuperiority of our proposed model. As a byproduct, we have released the codes\nto facilitate other researches.", "published": "2020-12-16 13:11:30", "link": "http://arxiv.org/abs/2012.08920v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Discovering New Intents with Deep Aligned Clustering", "abstract": "Discovering new intents is a crucial task in dialogue systems. Most existing\nmethods are limited in transferring the prior knowledge from known intents to\nnew intents. They also have difficulties in providing high-quality supervised\nsignals to learn clustering-friendly features for grouping unlabeled intents.\nIn this work, we propose an effective method, Deep Aligned Clustering, to\ndiscover new intents with the aid of the limited known intent data. Firstly, we\nleverage a few labeled known intent samples as prior knowledge to pre-train the\nmodel. Then, we perform k-means to produce cluster assignments as\npseudo-labels. Moreover, we propose an alignment strategy to tackle the label\ninconsistency problem during clustering assignments. Finally, we learn the\nintent representations under the supervision of the aligned pseudo-labels. With\nan unknown number of new intents, we predict the number of intent categories by\neliminating low-confidence intent-wise clusters. Extensive experiments on two\nbenchmark datasets show that our method is more robust and achieves substantial\nimprovements over the state-of-the-art methods. The codes are released at\nhttps://github.com/thuiar/DeepAligned-Clustering.", "published": "2020-12-16 14:32:06", "link": "http://arxiv.org/abs/2012.08987v7", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Using Meta-Knowledge Mined from Identifiers to Improve Intent\n  Recognition in Neuro-Symbolic Algorithms", "abstract": "In this paper we explore the use of meta-knowledge embedded in intent\nidentifiers to improve intent recognition in conversational systems. As\nevidenced by the analysis of thousands of real-world chatbots and in interviews\nwith professional chatbot curators, developers and domain experts tend to\norganize the set of chatbot intents by identifying them using proto-taxonomies,\ni.e., meta-knowledge connecting high-level, symbolic concepts shared across\ndifferent intents. By using neuro-symbolic algorithms able to incorporate such\nproto-taxonomies to expand intent representation, we show that such mined\nmeta-knowledge can improve accuracy in intent recognition. In a dataset with\nintents and example utterances from hundreds of professional chatbots, we saw\nimprovements of more than 10% in the equal error rate (EER) in almost a third\nof the chatbots when we apply those algorithms in comparison to a baseline of\nthe same algorithms without the meta-knowledge. The meta-knowledge proved to be\neven more relevant in detecting out-of-scope utterances, decreasing the false\nacceptance rate (FAR) in more than 20\\% in about half of the chatbots. The\nexperiments demonstrate that such symbolic meta-knowledge structures can be\neffectively mined and used by neuro-symbolic algorithms, apparently by\nincorporating into the learning process higher-level structures of the problem\nbeing solved. Based on these results, we also discuss how the use of mined\nmeta-knowledge can be an answer for the challenge of knowledge acquisition in\nneuro-symbolic algorithms.", "published": "2020-12-16 15:04:50", "link": "http://arxiv.org/abs/2012.09005v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Building and Using Personal Knowledge Graph to Improve Suicidal Ideation\n  Detection on Social Media", "abstract": "A large number of individuals are suffering from suicidal ideation in the\nworld. There are a number of causes behind why an individual might suffer from\nsuicidal ideation. As the most popular platform for self-expression, emotion\nrelease, and personal interaction, individuals may exhibit a number of symptoms\nof suicidal ideation on social media. Nevertheless, challenges from both data\nand knowledge aspects remain as obstacles, constraining the social media-based\ndetection performance. Data implicitness and sparsity make it difficult to\ndiscover the inner true intentions of individuals based on their posts.\nInspired by psychological studies, we build and unify a high-level\nsuicide-oriented knowledge graph with deep neural networks for suicidal\nideation detection on social media. We further design a two-layered attention\nmechanism to explicitly reason and establish key risk factors to individual's\nsuicidal ideation. The performance study on microblog and Reddit shows that: 1)\nwith the constructed personal knowledge graph, the social media-based suicidal\nideation detection can achieve over 93% accuracy; and 2) among the six\ncategories of personal factors, post, personality, and experience are the top-3\nkey indicators. Under these categories, posted text, stress level, stress\nduration, posted image, and ruminant thinking contribute to one's suicidal\nideation detection.", "published": "2020-12-16 18:09:32", "link": "http://arxiv.org/abs/2012.09123v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "LIREx: Augmenting Language Inference with Relevant Explanation", "abstract": "Natural language explanations (NLEs) are a special form of data annotation in\nwhich annotators identify rationales (most significant text tokens) when\nassigning labels to data instances, and write out explanations for the labels\nin natural language based on the rationales. NLEs have been shown to capture\nhuman reasoning better, but not as beneficial for natural language inference\n(NLI). In this paper, we analyze two primary flaws in the way NLEs are\ncurrently used to train explanation generators for language inference tasks. We\nfind that the explanation generators do not take into account the variability\ninherent in human explanation of labels, and that the current explanation\ngeneration models generate spurious explanations. To overcome these\nlimitations, we propose a novel framework, LIREx, that incorporates both a\nrationale-enabled explanation generator and an instance selector to select only\nrelevant, plausible NLEs to augment NLI models. When evaluated on the\nstandardized SNLI data set, LIREx achieved an accuracy of 91.87%, an\nimprovement of 0.32 over the baseline and matching the best-reported\nperformance on the data set. It also achieves significantly better performance\nthan previous studies when transferred to the out-of-domain MultiNLI data set.\nQualitative analysis shows that LIREx generates flexible, faithful, and\nrelevant NLEs that allow the model to be more robust to spurious explanations.\nThe code is available at https://github.com/zhaoxy92/LIREx.", "published": "2020-12-16 18:49:29", "link": "http://arxiv.org/abs/2012.09157v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "MELINDA: A Multimodal Dataset for Biomedical Experiment Method\n  Classification", "abstract": "We introduce a new dataset, MELINDA, for Multimodal biomEdicaL experImeNt\nmethoD clAssification. The dataset is collected in a fully automated distant\nsupervision manner, where the labels are obtained from an existing curated\ndatabase, and the actual contents are extracted from papers associated with\neach of the records in the database. We benchmark various state-of-the-art NLP\nand computer vision models, including unimodal models which only take either\ncaption texts or images as inputs, and multimodal models. Extensive experiments\nand analysis show that multimodal models, despite outperforming unimodal ones,\nstill need improvements especially on a less-supervised way of grounding visual\nconcepts with languages, and better transferability to low resource domains. We\nrelease our dataset and the benchmarks to facilitate future research in\nmultimodal learning, especially to motivate targeted improvements for\napplications in scientific domains.", "published": "2020-12-16 19:11:36", "link": "http://arxiv.org/abs/2012.09216v1", "categories": ["cs.CL", "cs.CV"], "primary_category": "cs.CL"}
{"title": "DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion\n  Recognition", "abstract": "This paper presents our pioneering effort for emotion recognition in\nconversation (ERC) with pre-trained language models. Unlike regular documents,\nconversational utterances appear alternately from different parties and are\nusually organized as hierarchical structures in previous work. Such structures\nare not conducive to the application of pre-trained language models such as\nXLNet. To address this issue, we propose an all-in-one XLNet model, namely\nDialogXL, with enhanced memory to store longer historical context and\ndialog-aware self-attention to deal with the multi-party structures.\nSpecifically, we first modify the recurrence mechanism of XLNet from\nsegment-level to utterance-level in order to better model the conversational\ndata. Second, we introduce dialog-aware self-attention in replacement of the\nvanilla self-attention in XLNet to capture useful intra- and inter-speaker\ndependencies. Extensive experiments are conducted on four ERC benchmarks with\nmainstream models presented for comparison. The experimental results show that\nthe proposed model outperforms the baselines on all the datasets. Several other\nexperiments such as ablation study and error analysis are also conducted and\nthe results confirm the role of the critical modules of DialogXL.", "published": "2020-12-16 01:50:46", "link": "http://arxiv.org/abs/2012.08695v1", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multi-type Disentanglement without Adversarial Training", "abstract": "Controlling the style of natural language by disentangling the latent space\nis an important step towards interpretable machine learning. After the latent\nspace is disentangled, the style of a sentence can be transformed by tuning the\nstyle representation without affecting other features of the sentence. Previous\nworks usually use adversarial training to guarantee that disentangled vectors\ndo not affect each other. However, adversarial methods are difficult to train.\nEspecially when there are multiple features (e.g., sentiment, or tense, which\nwe call style types in this paper), each feature requires a separate\ndiscriminator for extracting a disentangled style vector corresponding to that\nfeature. In this paper, we propose a unified distribution-controlling method,\nwhich provides each specific style value (the value of style types, e.g.,\npositive sentiment, or past tense) with a unique representation. This method\ncontributes a solid theoretical basis to avoid adversarial training in\nmulti-type disentanglement. We also propose multiple loss functions to achieve\na style-content disentanglement as well as a disentanglement among multiple\nstyle types. In addition, we observe that if two different style types always\nhave some specific style values that occur together in the dataset, they will\naffect each other when transferring the style values. We call this phenomenon\ntraining bias, and we propose a loss function to alleviate such training bias\nwhile disentangling multiple types. We conduct experiments on two datasets\n(Yelp service reviews and Amazon product reviews) to evaluate the\nstyle-disentangling effect and the unsupervised style transfer performance on\ntwo style types: sentiment and tense. The experimental results show the\neffectiveness of our model.", "published": "2020-12-16 11:47:18", "link": "http://arxiv.org/abs/2012.08883v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Learning from the Best: Rationalizing Prediction by Adversarial\n  Information Calibration", "abstract": "Explaining the predictions of AI models is paramount in safety-critical\napplications, such as in legal or medical domains. One form of explanation for\na prediction is an extractive rationale, i.e., a subset of features of an\ninstance that lead the model to give its prediction on the instance. Previous\nworks on generating extractive rationales usually employ a two-phase model: a\nselector that selects the most important features (i.e., the rationale)\nfollowed by a predictor that makes the prediction based exclusively on the\nselected features. One disadvantage of these works is that the main signal for\nlearning to select features comes from the comparison of the answers given by\nthe predictor and the ground-truth answers. In this work, we propose to squeeze\nmore information from the predictor via an information calibration method. More\nprecisely, we train two models jointly: one is a typical neural model that\nsolves the task at hand in an accurate but black-box manner, and the other is a\nselector-predictor model that additionally produces a rationale for its\nprediction. The first model is used as a guide to the second model. We use an\nadversarial-based technique to calibrate the information extracted by the two\nmodels such that the difference between them is an indicator of the missed or\nover-selected features. In addition, for natural language tasks, we propose to\nuse a language-model-based regularizer to encourage the extraction of fluent\nrationales. Experimental results on a sentiment analysis task as well as on\nthree tasks from the legal domain show the effectiveness of our approach to\nrationale extraction.", "published": "2020-12-16 11:54:15", "link": "http://arxiv.org/abs/2012.08884v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Multilingual Evidence Retrieval and Fact Verification to Combat Global\n  Disinformation: The Power of Polyglotism", "abstract": "This article investigates multilingual evidence retrieval and fact\nverification as a step to combat global disinformation, a first effort of this\nkind, to the best of our knowledge. The goal is building multilingual systems\nthat retrieve in evidence-rich languages to verify claims in evidence-poor\nlanguages that are more commonly targeted by disinformation. To this end, our\nEnmBERT fact verification system shows evidence of transfer learning ability\nand 400 example mixed English-Romanian dataset is made available for\ncross-lingual transfer learning evaluation.", "published": "2020-12-16 13:10:56", "link": "http://arxiv.org/abs/2012.08919v2", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "primary_category": "cs.CL"}
{"title": "You Are What You Tweet: Profiling Users by Past Tweets to Improve Hate\n  Speech Detection", "abstract": "Hate speech detection research has predominantly focused on purely\ncontent-based methods, without exploiting any additional context. We briefly\ncritique pros and cons of this task formulation. We then investigate profiling\nusers by their past utterances as an informative prior to better predict\nwhether new utterances constitute hate speech. To evaluate this, we augment\nthree Twitter hate speech datasets with additional timeline data, then embed\nthis additional context into a strong baseline model. Promising results suggest\nmerit for further investigation, though analysis is complicated by differences\nin annotation schemes and processes, as well as Twitter API limitations and\ndata sharing policies.", "published": "2020-12-16 17:17:47", "link": "http://arxiv.org/abs/2012.09090v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Synergistic Kalman- and Deep Postfiltering Approach to Acoustic Echo\n  Cancellation", "abstract": "We introduce a synergistic approach to double-talk robust acoustic echo\ncancellation combining adaptive Kalman filtering with a deep neural\nnetwork-based postfilter. The proposed algorithm overcomes the well-known\nlimitations of Kalman filter-based adaptation control in scenarios\ncharacterized by abrupt echo path changes. As the key innovation, we suggest to\nexploit the different statistical properties of the interfering signal\ncomponents for robustly estimating the adaptation step size. This is achieved\nby leveraging the postfilter near-end estimate and the estimation error of the\nKalman filter. The proposed synergistic scheme allows for rapid reconvergence\nof the adaptive filter after abrupt echo path changes without compromising the\nsteady state performance achieved by state-of-the-art approaches in static\nscenarios.", "published": "2020-12-16 11:14:57", "link": "http://arxiv.org/abs/2012.08867v3", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Automatic source localization and spectra generation from sparse\n  beamforming maps", "abstract": "Beamforming is an imaging tool for the investigation of aeroacoustic\nphenomena and results in high dimensional data that is broken down to spectra\nby integrating spatial Regions Of Interest. This paper presents two methods\nthat enable the automated identification of aeroacoustic sources in sparse\nbeamforming maps and the extraction of their corresponding spectra to overcome\nthe manual definition of Regions Of Interest. The methods are evaluated on two\nscaled airframe half-model wind-tunnel measurements and on a generic monopole\nsource. The first relies on the spatial normal distribution of aeroacoustic\nbroadband sources in sparse beamforming maps. The second uses hierarchical\nclustering methods. Both methods are robust to statistical noise and predict\nthe existence, location, and spatial probability estimation for sources based\non which Regions Of Interest are automatically determined.", "published": "2020-12-16 16:15:49", "link": "http://arxiv.org/abs/2012.09643v4", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
