{"title": "#maskUp: Selective Attribute Encryption for Sensitive Vocalization for\n  English language on Social Media Platforms", "abstract": "Social media has become a platform for people to stand up and raise their\nvoices against social and criminal acts. Vocalization of such information has\nallowed the investigation and identification of criminals. However, revealing\nsuch sensitive information may jeopardize the victim's safety. We propose\n#maskUp, a safe method for information communication in a secure fashion to the\nrelevant authorities, discouraging potential bullying of the victim. This would\nensure security by conserving their privacy through natural language processing\nsupplemented with selective encryption for sensitive attribute masking. To our\nknowledge, this is the first work that aims to protect the privacy of the\nvictims by masking their private details as well as emboldening them to come\nforward to report crimes. The use of masking technology allows only binding\nauthorities to view/un-mask this data. We construct and evaluate the proposed\nmethodology on continual learning tasks, allowing practical implementation of\nthe same in a real-world scenario. #maskUp successfully demonstrates this\nintegration on sample datasets validating the presented objective.", "published": "2022-11-16 04:10:41", "link": "http://arxiv.org/abs/2211.08653v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Parameter-Efficient Tuning on Layer Normalization for Pre-trained\n  Language Models", "abstract": "Conventional fine-tuning encounters increasing difficulties given the size of\ncurrent Pre-trained Language Models, which makes parameter-efficient tuning\nbecome the focal point of frontier research. Previous methods in this field add\ntunable adapters into MHA or/and FFN of Transformer blocks to enable PLMs\nachieve transferability. However, as an important part of Transformer\narchitecture, the power of layer normalization for parameter-efficent tuning is\nignored. In this paper, we first propose LN-tuning, by tuning the gain and bias\nterm of Layer Normalization module with only 0.03\\% parameters, which is of\nhigh time-efficency and significantly superior to baselines which are less than\n0.1\\% tunable parameters. Further, we study the unified framework of combining\nLN-tuning with previous ones and we find that: (1) the unified framework of\ncombining prefix-tuning, the adapter-based method working on MHA, and LN-tuning\nachieves SOTA performance. (2) unified framework which tunes MHA and LayerNorm\nsimultaneously can get performance improvement but those which tune FFN and\nLayerNorm simultaneous will cause performance decrease. Ablation study\nvalidates LN-tuning is of no abundant parameters and gives a further\nunderstanding of it.", "published": "2022-11-16 05:31:49", "link": "http://arxiv.org/abs/2211.08682v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Neural Unsupervised Reconstruction of Protolanguage Word Forms", "abstract": "We present a state-of-the-art neural approach to the unsupervised\nreconstruction of ancient word forms. Previous work in this domain used\nexpectation-maximization to predict simple phonological changes between ancient\nword forms and their cognates in modern languages. We extend this work with\nneural models that can capture more complicated phonological and morphological\nchanges. At the same time, we preserve the inductive biases from classical\nmethods by building monotonic alignment constraints into the model and\ndeliberately underfitting during the maximization step. We evaluate our\nperformance on the task of reconstructing Latin from a dataset of cognates\nacross five Romance languages, achieving a notable reduction in edit distance\nfrom the target word forms compared to previous methods.", "published": "2022-11-16 05:38:51", "link": "http://arxiv.org/abs/2211.08684v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Noisy Pairing and Partial Supervision for Stylized Opinion Summarization", "abstract": "Opinion summarization research has primarily focused on generating summaries\nreflecting important opinions from customer reviews without paying much\nattention to the writing style. In this paper, we propose the stylized opinion\nsummarization task, which aims to generate a summary of customer reviews in the\ndesired (e.g., professional) writing style. To tackle the difficulty in\ncollecting customer and professional review pairs, we develop a non-parallel\ntraining framework, Noisy Pairing and Partial Supervision (NAPA), which trains\na stylized opinion summarization system from non-parallel customer and\nprofessional review sets. We create a benchmark ProSum by collecting customer\nand professional reviews from Yelp and Michelin. Experimental results on ProSum\nand FewSum demonstrate that our non-parallel training framework consistently\nimproves both automatic and human evaluations, successfully building a stylized\nopinion summarization model that can generate professionally-written summaries\nfrom customer reviews. The code is available at\nhttps://github.com/megagonlabs/napa", "published": "2022-11-16 07:27:37", "link": "http://arxiv.org/abs/2211.08723v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight\n  BERT", "abstract": "With the development of deep learning and Transformer-based pre-trained\nmodels like BERT, the accuracy of many NLP tasks has been dramatically\nimproved. However, the large number of parameters and computations also pose\nchallenges for their deployment. For instance, using BERT can improve the\npredictions in the financial sentiment analysis (FSA) task but slow it down,\nwhere speed and accuracy are equally important in terms of profits. To address\nthese issues, we first propose an efficient and lightweight BERT (ELBERT) along\nwith a novel confidence-window-based (CWB) early exit mechanism. Based on\nELBERT, an innovative method to accelerate text processing on the GPU platform\nis developed, solving the difficult problem of making the early exit mechanism\nwork more effectively with a large input batch size. Afterward, a fast and\nhigh-accuracy FSA system is built. Experimental results show that the proposed\nCWB early exit mechanism achieves significantly higher accuracy than existing\nearly exit methods on BERT under the same computation cost. By using this\nacceleration method, our FSA system can boost the processing speed by nearly 40\ntimes to over 1000 texts per second with sufficient accuracy, which is nearly\ntwice as fast as FastBERT, thus providing a more powerful text processing\ncapability for modern trading systems.", "published": "2022-11-16 11:43:09", "link": "http://arxiv.org/abs/2211.08842v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "TSMind: Alibaba and Soochow University's Submission to the WMT22\n  Translation Suggestion Task", "abstract": "This paper describes the joint submission of Alibaba and Soochow University,\nTSMind, to the WMT 2022 Shared Task on Translation Suggestion (TS). We\nparticipate in the English-German and English-Chinese tasks. Basically, we\nutilize the model paradigm fine-tuning on the downstream tasks based on\nlarge-scale pre-trained models, which has recently achieved great success. We\nchoose FAIR's WMT19 English-German news translation system and MBART50 for\nEnglish-Chinese as our pre-trained models. Considering the task's condition of\nlimited use of training data, we follow the data augmentation strategies\nproposed by WeTS to boost our TS model performance. The difference is that we\nfurther involve the dual conditional cross-entropy model and GPT-2 language\nmodel to filter augmented data. The leader board finally shows that our\nsubmissions are ranked first in three of four language directions in the Naive\nTS task of the WMT22 Translation Suggestion task.", "published": "2022-11-16 15:43:31", "link": "http://arxiv.org/abs/2211.08987v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "UniRel: Unified Representation and Interaction for Joint Relational\n  Triple Extraction", "abstract": "Relational triple extraction is challenging for its difficulty in capturing\nrich correlations between entities and relations. Existing works suffer from 1)\nheterogeneous representations of entities and relations, and 2) heterogeneous\nmodeling of entity-entity interactions and entity-relation interactions.\nTherefore, the rich correlations are not fully exploited by existing works. In\nthis paper, we propose UniRel to address these challenges. Specifically, we\nunify the representations of entities and relations by jointly encoding them\nwithin a concatenated natural language sequence, and unify the modeling of\ninteractions with a proposed Interaction Map, which is built upon the\noff-the-shelf self-attention mechanism within any Transformer block. With\ncomprehensive experiments on two popular relational triple extraction datasets,\nwe demonstrate that UniRel is more effective and computationally efficient. The\nsource code is available at https://github.com/wtangdev/UniRel.", "published": "2022-11-16 16:53:13", "link": "http://arxiv.org/abs/2211.09039v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Towards Computationally Verifiable Semantic Grounding for Language\n  Models", "abstract": "The paper presents an approach to semantic grounding of language models (LMs)\nthat conceptualizes the LM as a conditional model generating text given a\ndesired semantic message formalized as a set of entity-relationship triples. It\nembeds the LM in an auto-encoder by feeding its output to a semantic parser\nwhose output is in the same representation domain as the input message.\nCompared to a baseline that generates text using greedy search, we demonstrate\ntwo techniques that improve the fluency and semantic accuracy of the generated\ntext: The first technique samples multiple candidate text sequences from which\nthe semantic parser chooses. The second trains the language model while keeping\nthe semantic parser frozen to improve the semantic accuracy of the\nauto-encoder. We carry out experiments on the English WebNLG 3.0 data set,\nusing BLEU to measure the fluency of generated text and standard parsing\nmetrics to measure semantic accuracy. We show that our proposed approaches\nsignificantly improve on the greedy search baseline. Human evaluation\ncorroborates the results of the automatic evaluation experiments.", "published": "2022-11-16 17:35:52", "link": "http://arxiv.org/abs/2211.09070v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Prompting PaLM for Translation: Assessing Strategies and Performance", "abstract": "Large language models (LLMs) that have been trained on multilingual but not\nparallel text exhibit a remarkable ability to translate between languages. We\nprobe this ability in an in-depth study of the pathways language model (PaLM),\nwhich has demonstrated the strongest machine translation (MT) performance among\nsimilarly-trained LLMs to date. We investigate various strategies for choosing\ntranslation examples for few-shot prompting, concluding that example quality is\nthe most important factor. Using optimized prompts, we revisit previous\nassessments of PaLM's MT capabilities with more recent test sets, modern MT\nmetrics, and human evaluation, and find that its performance, while impressive,\nstill lags that of state-of-the-art supervised systems. We conclude by\nproviding an analysis of PaLM's MT output which reveals some interesting\nproperties and prospects for future work.", "published": "2022-11-16 18:42:37", "link": "http://arxiv.org/abs/2211.09102v3", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "On Measuring the Intrinsic Few-Shot Hardness of Datasets", "abstract": "While advances in pre-training have led to dramatic improvements in few-shot\nlearning of NLP tasks, there is limited understanding of what drives successful\nfew-shot adaptation in datasets. In particular, given a new dataset and a\npre-trained model, what properties of the dataset make it \\emph{few-shot\nlearnable} and are these properties independent of the specific adaptation\ntechniques used? We consider an extensive set of recent few-shot learning\nmethods, and show that their performance across a large number of datasets is\nhighly correlated, showing that few-shot hardness may be intrinsic to datasets,\nfor a given pre-trained model. To estimate intrinsic few-shot hardness, we then\npropose a simple and lightweight metric called \"Spread\" that captures the\nintuition that few-shot learning is made possible by exploiting feature-space\ninvariances between training and test samples. Our metric better accounts for\nfew-shot hardness compared to existing notions of hardness, and is ~8-100x\nfaster to compute.", "published": "2022-11-16 18:53:52", "link": "http://arxiv.org/abs/2211.09113v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Artificial Disfluency Detection, Uh No, Disfluency Generation for the\n  Masses", "abstract": "Existing approaches for disfluency detection typically require the existence\nof large annotated datasets. However, current datasets for this task are\nlimited, suffer from class imbalance, and lack some types of disfluencies that\ncan be encountered in real-world scenarios. This work proposes LARD, a method\nfor automatically generating artificial disfluencies from fluent text. LARD can\nsimulate all the different types of disfluencies (repetitions, replacements and\nrestarts) based on the reparandum/interregnum annotation scheme. In addition,\nit incorporates contextual embeddings into the disfluency generation to produce\nrealistic context-aware artificial disfluencies. Since the proposed method\nrequires only fluent text, it can be used directly for training, bypassing the\nrequirement of annotated disfluent data. Our empirical evaluation demonstrates\nthat LARD can indeed be effectively used when no or only a few data are\navailable. Furthermore, our detailed analysis suggests that the proposed method\ngenerates realistic disfluencies and increases the accuracy of existing\ndisfluency detectors.", "published": "2022-11-16 22:00:02", "link": "http://arxiv.org/abs/2211.09235v1", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Task-aware Retrieval with Instructions", "abstract": "We study the problem of retrieval with instructions, where users of a\nretrieval system explicitly describe their intent along with their queries. We\naim to develop a general-purpose task-aware retrieval system using multi-task\ninstruction tuning, which can follow human-written instructions to find the\nbest documents for a given query. We introduce the first large-scale collection\nof approximately 40 retrieval datasets with instructions, BERRI, and present\nTART, a multi-task retrieval system trained on BERRI with instructions. TART\nshows strong capabilities to adapt to a new retrieval task via instructions and\nadvances the state of the art on two zero-shot retrieval benchmarks, BEIR and\nLOTTE, outperforming models up to three times larger. We further introduce a\nnew evaluation setup, X^2-Retrieval to better reflect real-world scenarios,\nwhere diverse domains and tasks are pooled and a system needs to find documents\naligning users' intents. In this setup, TART significantly outperforms\ncompetitive baselines, further demonstrating the effectiveness of guiding\nretrieval with instructions.", "published": "2022-11-16 23:13:22", "link": "http://arxiv.org/abs/2211.09260v2", "categories": ["cs.CL"], "primary_category": "cs.CL"}
{"title": "Disentangling Task Relations for Few-shot Text Classification via\n  Self-Supervised Hierarchical Task Clustering", "abstract": "Few-Shot Text Classification (FSTC) imitates humans to learn a new text\nclassifier efficiently with only few examples, by leveraging prior knowledge\nfrom historical tasks. However, most prior works assume that all the tasks are\nsampled from a single data source, which cannot adapt to real-world scenarios\nwhere tasks are heterogeneous and lie in different distributions. As such,\nexisting methods may suffer from their globally knowledge-shared mechanisms to\nhandle the task heterogeneity. On the other hand, inherent task relation are\nnot explicitly captured, making task knowledge unorganized and hard to transfer\nto new tasks. Thus, we explore a new FSTC setting where tasks can come from a\ndiverse range of data sources. To address the task heterogeneity, we propose a\nself-supervised hierarchical task clustering (SS-HTC) method. SS-HTC not only\ncustomizes cluster-specific knowledge by dynamically organizing heterogeneous\ntasks into different clusters in hierarchical levels but also disentangles\nunderlying relations between tasks to improve the interpretability. Extensive\nexperiments on five public FSTC benchmark datasets demonstrate the\neffectiveness of SS-HTC.", "published": "2022-11-16 00:19:53", "link": "http://arxiv.org/abs/2211.08588v1", "categories": ["cs.CL", "cs.LG"], "primary_category": "cs.CL"}
{"title": "MT Metrics Correlate with Human Ratings of Simultaneous Speech\n  Translation", "abstract": "There have been several meta-evaluation studies on the correlation between\nhuman ratings and offline machine translation (MT) evaluation metrics such as\nBLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate\nsimultaneous speech translation (SST) but their correlations with human ratings\nof SST, which has been recently collected as Continuous Ratings (CR), are\nunclear. In this paper, we leverage the evaluations of candidate systems\nsubmitted to the English-German SST task at IWSLT 2022 and conduct an extensive\ncorrelation analysis of CR and the aforementioned metrics. Our study reveals\nthat the offline metrics are well correlated with CR and can be reliably used\nfor evaluating machine translation in simultaneous mode, with some limitations\non the test set size. We conclude that given the current quality levels of SST,\nthese metrics can be used as proxies for CR, alleviating the need for large\nscale human evaluation. Additionally, we observe that correlations of the\nmetrics with translation as a reference is significantly higher than with\nsimultaneous interpreting, and thus we recommend the former for reliable\nevaluation.", "published": "2022-11-16 03:03:56", "link": "http://arxiv.org/abs/2211.08633v2", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Lesion Guided Explainable Few Weak-shot Medical Report Generation", "abstract": "Medical images are widely used in clinical practice for diagnosis.\nAutomatically generating interpretable medical reports can reduce radiologists'\nburden and facilitate timely care. However, most existing approaches to\nautomatic report generation require sufficient labeled data for training. In\naddition, the learned model can only generate reports for the training classes,\nlacking the ability to adapt to previously unseen novel diseases. To this end,\nwe propose a lesion guided explainable few weak-shot medical report generation\nframework that learns correlation between seen and novel classes through visual\nand semantic feature alignment, aiming to generate medical reports for diseases\nnot observed in training. It integrates a lesion-centric feature extractor and\na Transformer-based report generation module. Concretely, the lesion-centric\nfeature extractor detects the abnormal regions and learns correlations between\nseen and novel classes with multi-view (visual and lexical) embeddings. Then,\nfeatures of the detected regions and corresponding embeddings are concatenated\nas multi-view input to the report generation module for explainable report\ngeneration, including text descriptions and corresponding abnormal regions\ndetected in the images. We conduct experiments on FFA-IR, a dataset providing\nexplainable annotations, showing that our framework outperforms others on\nreport generation for novel diseases.", "published": "2022-11-16 07:47:29", "link": "http://arxiv.org/abs/2211.08732v2", "categories": ["cs.CV", "cs.CL"], "primary_category": "cs.CV"}
{"title": "RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training\n  Retrieval-Oriented Language Models", "abstract": "To better support retrieval applications such as web search and question\nanswering, growing effort is made to develop retrieval-oriented language\nmodels. Most of the existing works focus on improving the semantic\nrepresentation capability for the contextualized embedding of [CLS] token.\nHowever, recent study shows that the ordinary tokens besides [CLS] may provide\nextra information, which helps to produce a better representation effect. As\nsuch, it's necessary to extend the current methods where all contextualized\nembeddings can be jointly pre-trained for the retrieval tasks.\n  With this motivation, we propose a new pre-training method: duplex masked\nauto-encoder, a.k.a. DupMAE, which targets on improving the semantic\nrepresentation capacity for the contextualized embeddings of both [CLS] and\nordinary tokens. It introduces two decoding tasks: one is to reconstruct the\noriginal input sentence based on the [CLS] embedding, the other one is to\nminimize the bag-of-words loss (BoW) about the input sentence based on the\nentire ordinary tokens' embeddings. The two decoding losses are added up to\ntrain a unified encoding model. The embeddings from [CLS] and ordinary tokens,\nafter dimension reduction and aggregation, are concatenated as one unified\nsemantic representation for the input. DupMAE is simple but empirically\ncompetitive: with a small decoding cost, it substantially contributes to the\nmodel's representation capability and transferability, where remarkable\nimprovements are achieved on MS MARCO and BEIR benchmarks.", "published": "2022-11-16 08:57:55", "link": "http://arxiv.org/abs/2211.08769v1", "categories": ["cs.CL", "cs.IR"], "primary_category": "cs.CL"}
{"title": "CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers", "abstract": "In this paper, we present CSCD-NS, the first Chinese spelling check (CSC)\ndataset designed for native speakers, containing 40,000 samples from a Chinese\nsocial platform. Compared with existing CSC datasets aimed at Chinese learners,\nCSCD-NS is ten times larger in scale and exhibits a distinct error\ndistribution, with a significantly higher proportion of word-level errors. To\nfurther enhance the data resource, we propose a novel method that simulates the\ninput process through an input method, generating large-scale and high-quality\npseudo data that closely resembles the actual error distribution and\noutperforms existing methods. Moreover, we investigate the performance of\nvarious models in this scenario, including large language models (LLMs), such\nas ChatGPT. The result indicates that generative models underperform BERT-like\nclassification models due to strict length and pronunciation constraints. The\nhigh prevalence of word-level errors also makes CSC for native speakers\nchallenging enough, leaving substantial room for improvement.", "published": "2022-11-16 09:25:42", "link": "http://arxiv.org/abs/2211.08788v3", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Cognitive Simplification Operations Improve Text Simplification", "abstract": "Text Simplification (TS) is the task of converting a text into a form that is\neasier to read while maintaining the meaning of the original text. A sub-task\nof TS is Cognitive Simplification (CS), converting text to a form that is\nreadily understood by people with cognitive disabilities without rendering it\nchildish or simplistic. This sub-task has yet to be explored with neural\nmethods in NLP, and resources for it are scarcely available. In this paper, we\npresent a method for incorporating knowledge from the cognitive accessibility\ndomain into a TS model, by introducing an inductive bias regarding what\nsimplification operations to use. We show that by adding this inductive bias to\na TS-trained model, it is able to adapt better to CS without ever seeing CS\ndata, and outperform a baseline model on a traditional TS benchmark. In\naddition, we provide a novel test dataset for CS, and analyze the differences\nbetween CS corpora and existing TS corpora, in terms of how simplification\noperations are applied.", "published": "2022-11-16 10:51:03", "link": "http://arxiv.org/abs/2211.08825v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "L2 proficiency assessment using self-supervised speech representations", "abstract": "There has been a growing demand for automated spoken language assessment\nsystems in recent years. A standard pipeline for this process is to start with\na speech recognition system and derive features, either hand-crafted or based\non deep-learning, that exploit the transcription and audio. Though these\napproaches can yield high performance systems, they require speech recognition\nsystems that can be used for L2 speakers, and preferably tuned to the specific\nform of test being deployed. Recently a self-supervised speech representation\nbased scheme, requiring no speech recognition, was proposed. This work extends\nthe initial analysis conducted on this approach to a large scale proficiency\ntest, Linguaskill, that comprises multiple parts, each designed to assess\ndifferent attributes of a candidate's speaking proficiency. The performance of\nthe self-supervised, wav2vec 2.0, system is compared to a high performance\nhand-crafted assessment system and a BERT-based text system both of which use\nspeech transcriptions. Though the wav2vec 2.0 based system is found to be\nsensitive to the nature of the response, it can be configured to yield\ncomparable performance to systems requiring a speech transcription, and yields\ngains when appropriately combined with standard approaches.", "published": "2022-11-16 11:47:20", "link": "http://arxiv.org/abs/2211.08849v1", "categories": ["eess.AS", "cs.CL"], "primary_category": "eess.AS"}
{"title": "Consecutive Question Generation via Dynamic Multitask Learning", "abstract": "In this paper, we propose the task of consecutive question generation (CQG),\nwhich generates a set of logically related question-answer pairs to understand\na whole passage, with a comprehensive consideration of the aspects including\naccuracy, coverage, and informativeness. To achieve this, we first examine the\nfour key elements of CQG, i.e., question, answer, rationale, and context\nhistory, and propose a novel dynamic multitask framework with one main task\ngenerating a question-answer pair, and four auxiliary tasks generating other\nelements. It directly helps the model generate good questions through both\njoint training and self-reranking. At the same time, to fully explore the\nworth-asking information in a given passage, we make use of the reranking\nlosses to sample the rationales and search for the best question series\nglobally. Finally, we measure our strategy by QA data augmentation and manual\nevaluation, as well as a novel application of generated question-answer pairs\non DocNLI. We prove that our strategy can improve question generation\nsignificantly and benefit multiple related NLP tasks.", "published": "2022-11-16 11:50:36", "link": "http://arxiv.org/abs/2211.08850v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Galactica: A Large Language Model for Science", "abstract": "Information overload is a major obstacle to scientific progress. The\nexplosive growth in scientific literature and data has made it ever harder to\ndiscover useful insights in a large mass of information. Today scientific\nknowledge is accessed through search engines, but they are unable to organize\nscientific knowledge alone. In this paper we introduce Galactica: a large\nlanguage model that can store, combine and reason about scientific knowledge.\nWe train on a large scientific corpus of papers, reference material, knowledge\nbases and many other sources. We outperform existing models on a range of\nscientific tasks. On technical knowledge probes such as LaTeX equations,\nGalactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also\nperforms well on reasoning, outperforming Chinchilla on mathematical MMLU by\n41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It\nalso sets a new state-of-the-art on downstream tasks such as PubMedQA and\nMedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general\ncorpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these\nresults demonstrate the potential for language models as a new interface for\nscience. We open source the model for the benefit of the scientific community.", "published": "2022-11-16 18:06:33", "link": "http://arxiv.org/abs/2211.09085v1", "categories": ["cs.CL", "stat.ML"], "primary_category": "cs.CL"}
{"title": "Unified Question Answering in Slovene", "abstract": "Question answering is one of the most challenging tasks in language\nunderstanding. Most approaches are developed for English, while less-resourced\nlanguages are much less researched. We adapt a successful English\nquestion-answering approach, called UnifiedQA, to the less-resourced Slovene\nlanguage. Our adaptation uses the encoder-decoder transformer SloT5 and mT5\nmodels to handle four question-answering formats: yes/no, multiple-choice,\nabstractive, and extractive. We use existing Slovene adaptations of four\ndatasets, and machine translate the MCTest dataset. We show that a general\nmodel can answer questions in different formats at least as well as specialized\nmodels. The results are further improved using cross-lingual transfer from\nEnglish. While we produce state-of-the-art results for Slovene, the performance\nstill lags behind English.", "published": "2022-11-16 19:15:14", "link": "http://arxiv.org/abs/2211.09159v1", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "primary_category": "cs.CL"}
{"title": "Deep Emotion Recognition in Textual Conversations: A Survey", "abstract": "Emotion Recognition in Conversations (ERC) is a key step towards successful\nhuman-machine interaction. While the field has seen tremendous advancement in\nthe last few years, new applications and implementation scenarios present novel\nchallenges and opportunities. These range from leveraging the conversational\ncontext, speaker, and emotion dynamics modelling, to interpreting common sense\nexpressions, informal language, and sarcasm, addressing challenges of real-time\nERC, recognizing emotion causes, different taxonomies across datasets,\nmultilingual ERC, and interpretability. This survey starts by introducing ERC,\nelaborating on the challenges and opportunities of this task. It proceeds with\na description of the emotion taxonomies and a variety of ERC benchmark datasets\nemploying such taxonomies. This is followed by descriptions comparing the most\nprominent works in ERC with explanations of the neural architectures employed.\nThen, it provides advisable ERC practices towards better frameworks,\nelaborating on methods to deal with subjectivity in annotations and modelling\nand methods to deal with the typically unbalanced ERC datasets. Finally, it\npresents systematic review tables comparing several works regarding the methods\nused and their performance. Benchmarking these works highlights resorting to\npre-trained Transformer Language Models to extract utterance representations,\nusing Gated and Graph Neural Networks to model the interactions between these\nutterances, and leveraging Generative Large Language Models to tackle ERC\nwithin a generative framework. This survey emphasizes the advantage of\nleveraging techniques to address unbalanced data, the exploration of mixed\nemotions, and the benefits of incorporating annotation subjectivity in the\nlearning phase.", "published": "2022-11-16 19:42:31", "link": "http://arxiv.org/abs/2211.09172v4", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware\n  Dialog Generation", "abstract": "The development of conversational agents to interact with patients and\ndeliver clinical advice has attracted the interest of many researchers,\nparticularly in light of the COVID-19 pandemic. The training of an end-to-end\nneural based dialog system, on the other hand, is hampered by a lack of\nmulti-turn medical dialog corpus. We make the very first attempt to release a\nhigh-quality multi-turn Medical Dialog dataset relating to Covid-19 disease\nnamed CDialog, with over 1K conversations collected from the online medical\ncounselling websites. We annotate each utterance of the conversation with seven\ndifferent categories of medical entities, including diseases, symptoms, medical\ntests, medical history, remedies, medications and other aspects as additional\nlabels. Finally, we propose a novel neural medical dialog system based on the\nCDialog dataset to advance future research on developing automated medical\ndialog systems. We use pre-trained language models for dialogue generation,\nincorporating annotated medical entities, to generate a virtual doctor's\nresponse that addresses the patient's query. Experimental results show that the\nproposed dialog models perform comparably better when supplemented with entity\ninformation and hence can improve the response quality.", "published": "2022-11-16 11:07:34", "link": "http://arxiv.org/abs/2212.06049v1", "categories": ["cs.CL", "cs.AI"], "primary_category": "cs.CL"}
{"title": "Reward Gaming in Conditional Text Generation", "abstract": "To align conditional text generation model outputs with desired behaviors,\nthere has been an increasing focus on training the model using reinforcement\nlearning (RL) with reward functions learned from human annotations. Under this\nframework, we identify three common cases where high rewards are incorrectly\nassigned to undesirable patterns: noise-induced spurious correlation, naturally\noccurring spurious correlation, and covariate shift. We show that even though\nlearned metrics achieve high performance on the distribution of the data used\nto train the reward function, the undesirable patterns may be amplified during\nRL training of the text generation model. While there has been discussion about\nreward gaming in the RL or safety community, in this discussion piece, we would\nlike to highlight reward gaming in the natural language generation (NLG)\ncommunity using concrete conditional text generation examples and discuss\npotential fixes and areas for future work.", "published": "2022-11-16 07:10:02", "link": "http://arxiv.org/abs/2211.08714v3", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Streaming Joint Speech Recognition and Disfluency Detection", "abstract": "Disfluency detection has mainly been solved in a pipeline approach, as\npost-processing of speech recognition. In this study, we propose\nTransformer-based encoder-decoder models that jointly solve speech recognition\nand disfluency detection, which work in a streaming manner. Compared to\npipeline approaches, the joint models can leverage acoustic information that\nmakes disfluency detection robust to recognition errors and provide non-verbal\nclues. Moreover, joint modeling results in low-latency and lightweight\ninference. We investigate two joint model variants for streaming disfluency\ndetection: a transcript-enriched model and a multi-task model. The\ntranscript-enriched model is trained on text with special tags indicating the\nstarting and ending points of the disfluent part. However, it has problems with\nlatency and standard language model adaptation, which arise from the additional\ndisfluency tags. We propose a multi-task model to solve such problems, which\nhas two output layers at the Transformer decoder; one for speech recognition\nand the other for disfluency detection. It is modeled to be conditioned on the\ncurrently recognized token with an additional token-dependency mechanism. We\nshow that the proposed joint models outperformed a BERT-based pipeline approach\nin both accuracy and latency, on both the Switchboard and the corpus of\nspontaneous Japanese.", "published": "2022-11-16 07:34:20", "link": "http://arxiv.org/abs/2211.08726v2", "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "cs.CL"}
{"title": "Auditing Algorithmic Fairness in Machine Learning for Health with\n  Severity-Based LOGAN", "abstract": "Auditing machine learning-based (ML) healthcare tools for bias is critical to\npreventing patient harm, especially in communities that disproportionately face\nhealth inequities. General frameworks are becoming increasingly available to\nmeasure ML fairness gaps between groups. However, ML for health (ML4H) auditing\nprinciples call for a contextual, patient-centered approach to model\nassessment. Therefore, ML auditing tools must be (1) better aligned with ML4H\nauditing principles and (2) able to illuminate and characterize communities\nvulnerable to the most harm. To address this gap, we propose supplementing ML4H\nauditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs\ndetectioN), an automatic tool for capturing local biases in a clinical\nprediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs\ndetectioN), by contextualizing group bias detection in patient illness severity\nand past medical history. We investigate and compare SLOGAN's bias detection\ncapabilities to LOGAN and other clustering techniques across patient subgroups\nin the MIMIC-III dataset. On average, SLOGAN identifies larger fairness\ndisparities in over 75% of patient groups than LOGAN while maintaining\nclustering quality. Furthermore, in a diabetes case study, health disparity\nliterature corroborates the characterizations of the most biased clusters\nidentified by SLOGAN. Our results contribute to the broader discussion of how\nmachine learning biases may perpetuate existing healthcare disparities.", "published": "2022-11-16 08:04:12", "link": "http://arxiv.org/abs/2211.08742v1", "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "cs.LG"}
{"title": "Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed\n  Representations", "abstract": "Due to the huge amount of parameters, fine-tuning of pretrained language\nmodels (PLMs) is prone to overfitting in the low resource scenarios. In this\nwork, we present a novel method that operates on the hidden representations of\na PLM to reduce overfitting. During fine-tuning, our method inserts random\nautoencoders between the hidden layers of a PLM, which transform activations\nfrom the previous layers into multi-view compressed representations before\nfeeding them into the upper layers. The autoencoders are plugged out after\nfine-tuning, so our method does not add extra parameters or increase\ncomputation cost during inference. Our method demonstrates promising\nperformance improvement across a wide range of sequence- and token-level\nlow-resource NLP tasks.", "published": "2022-11-16 09:39:29", "link": "http://arxiv.org/abs/2211.08794v4", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "Holistic Evaluation of Language Models", "abstract": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.", "published": "2022-11-16 18:51:34", "link": "http://arxiv.org/abs/2211.09110v2", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "A Graph-Based Context-Aware Model to Understand Online Conversations", "abstract": "Online forums that allow for participatory engagement between users have been\ntransformative for the public discussion of many important issues. However,\nsuch conversations can sometimes escalate into full-blown exchanges of hate and\nmisinformation. Existing approaches in natural language processing (NLP), such\nas deep learning models for classification tasks, use as inputs only a single\ncomment or a pair of comments depending upon whether the task concerns the\ninference of properties of the individual comments or the replies between pairs\nof comments, respectively. But in online conversations, comments and replies\nmay be based on external context beyond the immediately relevant information\nthat is input to the model. Therefore, being aware of the conversations'\nsurrounding contexts should improve the model's performance for the inference\ntask at hand.\n  We propose GraphNLI, a novel graph-based deep learning architecture that uses\ngraph walks to incorporate the wider context of a conversation in a principled\nmanner. Specifically, a graph walk starts from a given comment and samples\n\"nearby\" comments in the same or parallel conversation threads, which results\nin additional embeddings that are aggregated together with the initial\ncomment's embedding. We then use these enriched embeddings for downstream NLP\nprediction tasks that are important for online conversations. We evaluate\nGraphNLI on two such tasks - polarity prediction and misogynistic hate speech\ndetection - and found that our model consistently outperforms all relevant\nbaselines for both tasks. Specifically, GraphNLI with a biased root-seeking\nrandom walk performs with a macro-F1 score of 3 and 6 percentage points better\nthan the best-performing BERT-based baselines for the polarity prediction and\nhate speech detection tasks, respectively.", "published": "2022-11-16 20:51:45", "link": "http://arxiv.org/abs/2211.09207v1", "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "cs.CL"}
{"title": "Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue\n  Response Quality", "abstract": "Human communication relies on common ground (CG), the mutual knowledge and\nbeliefs shared by participants, to produce coherent and interesting\nconversations. In this paper, we demonstrate that current response generation\n(RG) models produce generic and dull responses in dialogues because they act\nreflexively, failing to explicitly model CG, both due to the lack of CG in\ntraining data and the standard RG training procedure. We introduce Reflect, a\ndataset that annotates dialogues with explicit CG (materialized as inferences\napproximating shared knowledge and beliefs) and solicits 9k diverse\nhuman-generated responses each following one common ground. Using Reflect, we\nshowcase the limitations of current dialogue data and RG models: less than half\nof the responses in current data are rated as high quality (sensible, specific,\nand interesting) and models trained using this data have even lower quality,\nwhile most Reflect responses are judged high quality. Next, we analyze whether\nCG can help models produce better-quality responses by using Reflect CG to\nguide RG models. Surprisingly, we find that simply prompting GPT3 to \"think\"\nabout CG generates 30% more quality responses, showing promising benefits to\nintegrating CG into the RG process.", "published": "2022-11-16 23:50:22", "link": "http://arxiv.org/abs/2211.09267v1", "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "cs.CL"}
{"title": "On using the UA-Speech and TORGO databases to validate automatic\n  dysarthric speech classification approaches", "abstract": "Although the UA-Speech and TORGO databases of control and dysarthric speech\nare invaluable resources made available to the research community with the\nobjective of developing robust automatic speech recognition systems, they have\nalso been used to validate a considerable number of automatic dysarthric speech\nclassification approaches. Such approaches typically rely on the underlying\nassumption that recordings from control and dysarthric speakers are collected\nin the same noiseless environment using the same recording setup. In this\npaper, we show that this assumption is violated for the UA-Speech and TORGO\ndatabases. Using voice activity detection to extract speech and non-speech\nsegments, we show that the majority of state-of-the-art dysarthria\nclassification approaches achieve the same or a considerably better performance\nwhen using the non-speech segments of these databases than when using the\nspeech segments. These results demonstrate that such approaches trained and\nvalidated on the UA-Speech and TORGO databases are potentially learning\ncharacteristics of the recording environment or setup rather than dysarthric\nspeech characteristics. We hope that these results raise awareness in the\nresearch community about the importance of the quality of recordings when\ndeveloping and evaluating automatic dysarthria classification approaches.", "published": "2022-11-16 11:16:42", "link": "http://arxiv.org/abs/2211.08833v1", "categories": ["eess.AS"], "primary_category": "eess.AS"}
{"title": "Exploring Detection-based Method For Speaker Diarization @ Ego4D\n  Audio-only Diarization Challenge 2022", "abstract": "We provide the technical report for Ego4D audio-only diarization challenge in\nECCV 2022. Speaker diarization takes the audio streams as input and outputs the\nhomogeneous segments according to the speaker's identity. It aims to solve the\nproblem of \"Who spoke when.\" In this paper, we explore a Detection-based method\nto tackle the audio-only speaker diarization task. Our method first extracts\naudio features by audio backbone and then feeds the feature to a\ndetection-generate network to get the speaker proposals. Finally, after\npostprocessing, we can get the diarization results. The validation dataset\nvalidates this method, and our method achieves 53.85 DER on the test dataset.\nThese results rank 3rd on the leaderboard of Ego4D audio-only diarization\nchallenge 2022.", "published": "2022-11-16 06:48:08", "link": "http://arxiv.org/abs/2211.08708v1", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Array Configuration-Agnostic Personalized Speech Enhancement using\n  Long-Short-Term Spatial Coherence", "abstract": "Personalized speech enhancement has been a field of active research for\nsuppression of speechlike interferers such as competing speakers or TV\ndialogues. Compared with single channel approaches, multichannel PSE systems\ncan be more effective in adverse acoustic conditions by leveraging the spatial\ninformation in microphone signals. However, the implementation of multichannel\nPSEs to accommodate a wide range of array topology in household applications\ncan be challenging. To develop an array configuration agnostic PSE system, we\ndefine a spatial feature termed the long short term spatial coherence as the\ninput feature to a convolutional recurrent network to monitor the voice\nactivity of the target speaker. As another refinement, an equivalent\nrectangular bandwidth scaled LSTSC feature can be used to reduce the\ncomputational cost. Experiments were conducted to compare the proposed PSE\nsystems, including the complete and the simplified versions with two baselines\nusing unseen room responses and array configurations in the presence of TV\nnoise and competing speakers. The results demonstrated that the proposed\nmultichannel PSE network trained with the LSTSC feature achieved superior\nenhancement performance without precise knowledge of the array configurations\nand room responses.", "published": "2022-11-16 08:15:56", "link": "http://arxiv.org/abs/2211.08748v1", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Speaker Adaptation for End-To-End Speech Recognition Systems in Noisy\n  Environments", "abstract": "We analyze the impact of speaker adaptation in end-to-end automatic speech\nrecognition models based on transformers and wav2vec 2.0 under different noise\nconditions. By including speaker embeddings obtained from x-vector and\nECAPA-TDNN systems, as well as i-vectors, we achieve relative word error rate\nimprovements of up to 16.3% on LibriSpeech and up to 14.5% on Switchboard. We\nshow that the proven method of concatenating speaker vectors to the acoustic\nfeatures and supplying them as auxiliary model inputs remains a viable option\nto increase the robustness of end-to-end architectures. The effect on\ntransformer models is stronger, when more noise is added to the input speech.\nThe most substantial benefits for systems based on wav2vec 2.0 are achieved\nunder moderate or no noise conditions. Both x-vectors and ECAPA-TDNN embeddings\noutperform i-vectors as speaker representations. The optimal embedding size\ndepends on the dataset and also varies with the noise condition.", "published": "2022-11-16 09:02:41", "link": "http://arxiv.org/abs/2211.08774v3", "categories": ["cs.SD", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Structural Segmentation and Labeling of Tabla Solo Performances", "abstract": "Tabla is a North Indian percussion instrument used as an accompaniment and an\nexclusive instrument for solo performances. Tabla solo is intricate and\nelaborate, exhibiting rhythmic evolution through a sequence of homogeneous\nsections marked by shared rhythmic characteristics. Each section has a specific\nstructure and name associated with it. Tabla learning and performance in the\nIndian subcontinent is based on stylistic schools called gharana-s. Several\ncompositions by various composers from different gharana-s are played in each\nsection. This paper addresses the task of segmenting the tabla solo concert\ninto musically meaningful sections. We then assign suitable section labels and\nrecognize gharana-s from the sections. We present a diverse collection of over\n38 hours of solo tabla recordings for the task. We motivate the problem and\npresent different challenges and facets of the tasks. Inspired by the distinct\nmusical properties of tabla solo, we compute several rhythmic and timbral\nfeatures for the segmentation task. This work explores the approach of\nautomatically locating the significant changes in the rhythmic structure by\nanalyzing local self-similarity in an unsupervised manner. We also explore\nsupervised random forest and a convolutional neural network trained on\nhand-crafted features. Both supervised and unsupervised approaches are also\ntested on a set of held-out recordings. Segmentation of an audio piece into its\nstructural components and labeling is crucial to many music information\nretrieval applications like repetitive structure finding, audio summarization,\nand fast music navigation. This work helps us obtain a comprehensive musical\ndescription of the tabla solo concert.", "published": "2022-11-16 09:34:04", "link": "http://arxiv.org/abs/2211.08790v1", "categories": ["eess.AS", "cs.LG"], "primary_category": "eess.AS"}
{"title": "Delivering Speaking Style in Low-resource Voice Conversion with\n  Multi-factor Constraints", "abstract": "Conveying the linguistic content and maintaining the source speech's speaking\nstyle, such as intonation and emotion, is essential in voice conversion (VC).\nHowever, in a low-resource situation, where only limited utterances from the\ntarget speaker are accessible, existing VC methods are hard to meet this\nrequirement and capture the target speaker's timber. In this work, a novel VC\nmodel, referred to as MFC-StyleVC, is proposed for the low-resource VC task.\nSpecifically, speaker timbre constraint generated by clustering method is newly\nproposed to guide target speaker timbre learning in different stages.\nMeanwhile, to prevent over-fitting to the target speaker's limited data,\nperceptual regularization constraints explicitly maintain model performance on\nspecific aspects, including speaking style, linguistic content, and speech\nquality. Besides, a simulation mode is introduced to simulate the inference\nprocess to alleviate the mismatch between training and inference. Extensive\nexperiments performed on highly expressive speech demonstrate the superiority\nof the proposed method in low-resource VC.", "published": "2022-11-16 12:06:12", "link": "http://arxiv.org/abs/2211.08857v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "A Two-Stage Deep Representation Learning-Based Speech Enhancement Method\n  Using Variational Autoencoder and Adversarial Training", "abstract": "This paper focuses on leveraging deep representation learning (DRL) for\nspeech enhancement (SE). In general, the performance of the deep neural network\n(DNN) is heavily dependent on the learning of data representation. However, the\nDRL's importance is often ignored in many DNN-based SE algorithms. To obtain a\nhigher quality enhanced speech, we propose a two-stage DRL-based SE method\nthrough adversarial training. In the first stage, we disentangle different\nlatent variables because disentangled representations can help DNN generate a\nbetter enhanced speech. Specifically, we use the $\\beta$-variational\nautoencoder (VAE) algorithm to obtain the speech and noise posterior\nestimations and related representations from the observed signal. However,\nsince the posteriors and representations are intractable and we can only apply\na conditional assumption to estimate them, it is difficult to ensure that these\nestimations are always pretty accurate, which may potentially degrade the final\naccuracy of the signal estimation. To further improve the quality of enhanced\nspeech, in the second stage, we introduce adversarial training to reduce the\neffect of the inaccurate posterior towards signal reconstruction and improve\nthe signal estimation accuracy, making our algorithm more robust for the\npotentially inaccurate posterior estimations. As a result, better SE\nperformance can be achieved. The experimental results indicate that the\nproposed strategy can help similar DNN-based SE algorithms achieve higher\nshort-time objective intelligibility (STOI), perceptual evaluation of speech\nquality (PESQ), and scale-invariant signal-to-distortion ratio (SI-SDR) scores.\nMoreover, the proposed algorithm can also outperform recent competitive SE\nalgorithms.", "published": "2022-11-16 19:31:46", "link": "http://arxiv.org/abs/2211.09166v2", "categories": ["eess.AS", "cs.SD"], "primary_category": "eess.AS"}
{"title": "Leveraging Heteroscedastic Uncertainty in Learning Complex Spectral\n  Mapping for Single-channel Speech Enhancement", "abstract": "Most speech enhancement (SE) models learn a point estimate and do not make\nuse of uncertainty estimation in the learning process. In this paper, we show\nthat modeling heteroscedastic uncertainty by minimizing a multivariate Gaussian\nnegative log-likelihood (NLL) improves SE performance at no extra cost. During\ntraining, our approach augments a model learning complex spectral mapping with\na temporary submodel to predict the covariance of the enhancement error at each\ntime-frequency bin. Due to unrestricted heteroscedastic uncertainty, the\ncovariance introduces an undersampling effect, detrimental to SE performance.\nTo mitigate undersampling, our approach inflates the uncertainty lower bound\nand weights each loss component with their uncertainty, effectively\ncompensating severely undersampled components with more penalties. Our\nmultivariate setting reveals common covariance assumptions such as scalar and\ndiagonal matrices. By weakening these assumptions, we show that the NLL\nachieves superior performance compared to popular loss functions including the\nmean squared error (MSE), mean absolute error (MAE), and scale-invariant\nsignal-to-distortion ratio (SI-SDR).", "published": "2022-11-16 02:29:05", "link": "http://arxiv.org/abs/2211.08624v3", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Conditional variational autoencoder to improve neural audio synthesis\n  for polyphonic music sound", "abstract": "Deep generative models for audio synthesis have recently been significantly\nimproved. However, the task of modeling raw-waveforms remains a difficult\nproblem, especially for audio waveforms and music signals. Recently, the\nrealtime audio variational autoencoder (RAVE) method was developed for\nhigh-quality audio waveform synthesis. The RAVE method is based on the\nvariational autoencoder and utilizes the two-stage training strategy.\nUnfortunately, the RAVE model is limited in reproducing wide-pitch polyphonic\nmusic sound. Therefore, to enhance the reconstruction performance, we adopt the\npitch activation data as an auxiliary information to the RAVE model. To handle\nthe auxiliary information, we propose an enhanced RAVE model with a conditional\nvariational autoencoder structure and an additional fully-connected layer. To\nevaluate the proposed structure, we conducted a listening experiment based on\nmultiple stimulus tests with hidden references and an anchor (MUSHRA) with the\nMAESTRO. The obtained results indicate that the proposed model exhibits a more\nsignificant performance and stability improvement than the conventional RAVE\nmodel.", "published": "2022-11-16 07:11:56", "link": "http://arxiv.org/abs/2211.08715v1", "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Improving Speech Emotion Recognition with Unsupervised Speaking Style\n  Transfer", "abstract": "Humans can effortlessly modify various prosodic attributes, such as the\nplacement of stress and the intensity of sentiment, to convey a specific\nemotion while maintaining consistent linguistic content. Motivated by this\ncapability, we propose EmoAug, a novel style transfer model designed to enhance\nemotional expression and tackle the data scarcity issue in speech emotion\nrecognition tasks. EmoAug consists of a semantic encoder and a paralinguistic\nencoder that represent verbal and non-verbal information respectively.\nAdditionally, a decoder reconstructs speech signals by conditioning on the\naforementioned two information flows in an unsupervised fashion. Once training\nis completed, EmoAug enriches expressions of emotional speech with different\nprosodic attributes, such as stress, rhythm and intensity, by feeding different\nstyles into the paralinguistic encoder. EmoAug enables us to generate similar\nnumbers of samples for each class to tackle the data imbalance issue as well.\nExperimental results on the IEMOCAP dataset demonstrate that EmoAug can\nsuccessfully transfer different speaking styles while retaining the speaker\nidentity and semantic content. Furthermore, we train a SER model with data\naugmented by EmoAug and show that the augmented model not only surpasses the\nstate-of-the-art supervised and self-supervised methods but also overcomes\noverfitting problems caused by data imbalance. Some audio samples can be found\non our demo website.", "published": "2022-11-16 11:43:25", "link": "http://arxiv.org/abs/2211.08843v3", "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "cs.SD"}
{"title": "Annotation of Soft Onsets in String Ensemble Recordings", "abstract": "Onset detection is the process of identifying the start points of musical\nnote events within an audio recording. While the detection of percussive onsets\nis often considered a solved problem, soft onsets-as found in string instrument\nrecordings-still pose a significant challenge for state-of-the-art algorithms.\nThe problem is further exacerbated by a paucity of data containing expert\nannotations and research related to best practices for curating soft onset\nannotations for string instruments. To this end, we investigate inter-annotator\nagreement between 24 participants, extend an algorithm for determining the most\nconsistent annotator, and compare the performance of human annotators and\nstate-of-the-art onset detection algorithms. Experimental results reveal a\npositive trend between musical experience and both inter-annotator agreement\nand performance in comparison with automated systems. Additionally, onsets\nproduced by changes in fingering as well as those from the cello were found to\nbe particularly challenging for both human annotators and automatic approaches.\nTo promote research in best practices for annotation of soft onsets, we have\nmade all experimental data associated with this study publicly available. In\naddition, we publish the ARME Virtuoso Strings dataset, consisting of over 144\nrecordings of professional performances of an excerpt from Haydn's string\nquartet Op. 74 No. 1 Finale, each with corresponding individual instrumental\nonset annotations.", "published": "2022-11-16 11:46:34", "link": "http://arxiv.org/abs/2211.08848v1", "categories": ["eess.AS", "cs.IR", "cs.SD"], "primary_category": "eess.AS"}
{"title": "McNet: Fuse Multiple Cues for Multichannel Speech Enhancement", "abstract": "In multichannel speech enhancement, both spectral and spatial information are\nvital for discriminating between speech and noise. How to fully exploit these\ntwo types of information and their temporal dynamics remains an interesting\nresearch problem. As a solution to this problem, this paper proposes a\nmulti-cue fusion network named McNet, which cascades four modules to\nrespectively exploit the full-band spatial, narrow-band spatial, sub-band\nspectral, and full-band spectral information. Experiments show that each module\nin the proposed network has its unique contribution and, as a whole, notably\noutperforms other state-of-the-art methods.", "published": "2022-11-16 12:25:54", "link": "http://arxiv.org/abs/2211.08872v1", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "primary_category": "eess.AS"}
{"title": "Psychophysiology-aided Perceptually Fluent Speech Analysis of Children\n  Who Stutter", "abstract": "This first-of-its-kind paper presents a novel approach named PASAD that\ndetects changes in perceptually fluent speech acoustics of young children.\nParticularly, analysis of perceptually fluent speech enables identifying the\nspeech-motor-control factors that are considered as the underlying cause of\nstuttering disfluencies. Recent studies indicate that the speech production of\nyoung children, especially those who stutter, may get adversely affected by\nsituational physiological arousal. A major contribution of this paper is\nleveraging the speaker's situational physiological responses in real-time to\nanalyze the speech signal effectively. The presented PASAD approach adapts a\nHyper-Network structure to extract temporal speech importance information\nleveraging physiological parameters. In addition, a novel non-local acoustic\nspectrogram feature extraction network identifies meaningful acoustic\nattributes. Finally, a sequential network utilizes the acoustic attributes and\nthe extracted temporal speech importance for effective classification. We\ncollected speech and physiological sensing data from 73 preschool-age children\nwho stutter (CWS) and who don't stutter (CWNS) in different conditions. PASAD's\nunique architecture enables visualizing speech attributes distinct to a CWS's\nfluent speech and mapping them to the speaker's respective speech-motor-control\nfactors (i.e., speech articulators). Extracted knowledge can enhance\nunderstanding of children's fluent speech, speech-motor-control (SMC), and\nstuttering development. Our comprehensive evaluation shows that PASAD\noutperforms state-of-the-art multi-modal baseline approaches in different\nconditions, is expressive and adaptive to the speaker's speech and physiology,\ngeneralizable, robust, and is real-time executable on mobile and scalable\ndevices.", "published": "2022-11-16 18:12:15", "link": "http://arxiv.org/abs/2211.09089v1", "categories": ["cs.SD", "cs.HC", "eess.AS"], "primary_category": "cs.SD"}
{"title": "A Review of Intelligent Music Generation Systems", "abstract": "With the introduction of ChatGPT, the public's perception of AI-generated\ncontent (AIGC) has begun to reshape. Artificial intelligence has significantly\nreduced the barrier to entry for non-professionals in creative endeavors,\nenhancing the efficiency of content creation. Recent advancements have seen\nsignificant improvements in the quality of symbolic music generation, which is\nenabled by the use of modern generative algorithms to extract patterns implicit\nin a piece of music based on rule constraints or a musical corpus.\nNevertheless, existing literature reviews tend to present a conventional and\nconservative perspective on future development trajectories, with a notable\nabsence of thorough benchmarking of generative models. This paper provides a\nsurvey and analysis of recent intelligent music generation techniques,\noutlining their respective characteristics and discussing existing methods for\nevaluation. Additionally, the paper compares the different characteristics of\nmusic generation techniques in the East and West as well as analysing the\nfield's development prospects.", "published": "2022-11-16 13:43:16", "link": "http://arxiv.org/abs/2211.09124v3", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "68T01", "J.5"], "primary_category": "cs.SD"}
{"title": "PBSM: Backdoor attack against Keyword spotting based on pitch boosting\n  and sound masking", "abstract": "Keyword spotting (KWS) has been widely used in various speech control\nscenarios. The training of KWS is usually based on deep neural networks and\nrequires a large amount of data. Manufacturers often use third-party data to\ntrain KWS. However, deep neural networks are not sufficiently interpretable to\nmanufacturers, and attackers can manipulate third-party training data to plant\nbackdoors during the model training. An effective backdoor attack can force the\nmodel to make specified judgments under certain conditions, i.e., triggers. In\nthis paper, we design a backdoor attack scheme based on Pitch Boosting and\nSound Masking for KWS, called PBSM. Experimental results demonstrated that PBSM\nis feasible to achieve an average attack success rate close to 90% in three\nvictim models when poisoning less than 1% of the training data.", "published": "2022-11-16 06:20:47", "link": "http://arxiv.org/abs/2211.08697v1", "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "cs.SD"}
